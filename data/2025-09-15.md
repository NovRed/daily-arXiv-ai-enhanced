<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 64]
- [cs.AI](#cs.AI) [Total: 22]
- [cs.CL](#cs.CL) [Total: 52]
- [cs.LG](#cs.LG) [Total: 53]
- [cs.SD](#cs.SD) [Total: 6]
- [eess.SP](#eess.SP) [Total: 2]
- [cs.CR](#cs.CR) [Total: 5]
- [eess.SY](#eess.SY) [Total: 1]
- [eess.AS](#eess.AS) [Total: 3]
- [math.OC](#math.OC) [Total: 1]
- [q-bio.GN](#q-bio.GN) [Total: 1]
- [cs.NE](#cs.NE) [Total: 2]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [stat.ML](#stat.ML) [Total: 4]
- [cs.DC](#cs.DC) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.IR](#cs.IR) [Total: 12]
- [cs.NI](#cs.NI) [Total: 1]
- [q-bio.OT](#q-bio.OT) [Total: 1]
- [cs.CY](#cs.CY) [Total: 2]
- [eess.IV](#eess.IV) [Total: 5]
- [cs.MA](#cs.MA) [Total: 1]
- [hep-lat](#hep-lat) [Total: 1]
- [cs.RO](#cs.RO) [Total: 6]
- [physics.app-ph](#physics.app-ph) [Total: 1]
- [cs.SE](#cs.SE) [Total: 5]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Australian Supermarket Object Set (ASOS): A Benchmark Dataset of Physical Objects and 3D Models for Robotics and Computer Vision](https://arxiv.org/abs/2509.09720)
*Akansel Cosgun,Lachlan Chumbley,Benjamin J. Meyer*

Main category: cs.CV

TL;DR: ASOS dataset: 50 common Australian supermarket items with high-quality 3D textured meshes for robotics and computer vision benchmarking.


<details>
  <summary>Details</summary>
Motivation: Address limitations of existing datasets that use synthetic models or specialized objects with limited accessibility by providing cost-effective, readily available household items.

Method: Acquired 3D meshes using structure-from-motion techniques with high-resolution imaging to generate watertight meshes, spanning 10 categories with diverse shapes, sizes, and weights.

Result: Created a comprehensive dataset of 50 supermarket items that can be sourced from a major Australian supermarket chain, providing high-quality 3D textured meshes.

Conclusion: The ASOS dataset's emphasis on accessibility and real-world applicability makes it valuable for benchmarking object detection, pose estimation, and robotics applications.

Abstract: This paper introduces the Australian Supermarket Object Set (ASOS), a
comprehensive dataset comprising 50 readily available supermarket items with
high-quality 3D textured meshes designed for benchmarking in robotics and
computer vision applications. Unlike existing datasets that rely on synthetic
models or specialized objects with limited accessibility, ASOS provides a
cost-effective collection of common household items that can be sourced from a
major Australian supermarket chain. The dataset spans 10 distinct categories
with diverse shapes, sizes, and weights. 3D meshes are acquired by a
structure-from-motion techniques with high-resolution imaging to generate
watertight meshes. The dataset's emphasis on accessibility and real-world
applicability makes it valuable for benchmarking object detection, pose
estimation, and robotics applications.

</details>


### [2] [A Multimodal RAG Framework for Housing Damage Assessment: Collaborative Optimization of Image Encoding and Policy Vector Retrieval](https://arxiv.org/abs/2509.09721)
*Jiayi Miao,Dingxin Lu,Zhuqi Wang*

Main category: cs.CV

TL;DR: Novel multimodal RAG framework for post-disaster housing damage assessment using image-text fusion and cross-modal alignment to improve insurance claim processing.


<details>
  <summary>Details</summary>
Motivation: Accurate damage evaluation after natural disasters is crucial for insurance claims and resource planning, requiring better multimodal understanding of building damage.

Method: Two-branch multimodal encoder with ResNet+Transformer for images and BERT for text, cross-modal interaction module, modal attention gating, and multi-task optimization with comparison/retrieval/generation losses.

Result: Superior performance with 9.6% improvement in Top-1 retrieval accuracy and better classification on damage severity metrics.

Conclusion: The MM-RAG framework effectively integrates visual and textual information for improved disaster damage assessment and insurance policy matching.

Abstract: After natural disasters, accurate evaluations of damage to housing are
important for insurance claims response and planning of resources. In this
work, we introduce a novel multimodal retrieval-augmented generation (MM-RAG)
framework. On top of classical RAG architecture, we further the framework to
devise a two-branch multimodal encoder structure that the image branch employs
a visual encoder composed of ResNet and Transformer to extract the
characteristic of building damage after disaster, and the text branch harnesses
a BERT retriever for the text vectorization of posts as well as insurance
policies and for the construction of a retrievable restoration index. To impose
cross-modal semantic alignment, the model integrates a cross-modal interaction
module to bridge the semantic representation between image and text via
multi-head attention. Meanwhile, in the generation module, the introduced modal
attention gating mechanism dynamically controls the role of visual evidence and
text prior information during generation. The entire framework takes end-to-end
training, and combines the comparison loss, the retrieval loss and the
generation loss to form multi-task optimization objectives, and achieves image
understanding and policy matching in collaborative learning. The results
demonstrate superior performance in retrieval accuracy and classification index
on damage severity, where the Top-1 retrieval accuracy has been improved by
9.6%.

</details>


### [3] [Improving MLLM Historical Record Extraction with Test-Time Image](https://arxiv.org/abs/2509.09722)
*Taylor Archibald,Tony Martinez*

Main category: cs.CV

TL;DR: Novel ensemble framework using Gemini 2.0 Flash and custom Needleman-Wunsch aligner improves LLM-based text extraction from noisy historical documents by 4% accuracy over single-shot baseline.


<details>
  <summary>Details</summary>
Motivation: To stabilize and improve text extraction from noisy historical documents using LLMs, addressing the challenges of poor quality scans and document degradation.

Method: Transcribe multiple augmented variants of each image with Gemini 2.0 Flash, then fuse outputs using a custom Needleman-Wunsch style aligner to produce consensus transcription with confidence scores.

Result: 4 percentage point improvement in transcription accuracy on a new dataset of 622 Pennsylvania death records; padding and blurring most effective for accuracy, grid warp best for confidence separation.

Conclusion: The approach is simple, scalable, and immediately deployable to other document collections and transcription models for improved historical document processing.

Abstract: We present a novel ensemble framework that stabilizes LLM based text
extraction from noisy historical documents. We transcribe multiple augmented
variants of each image with Gemini 2.0 Flash and fuse these outputs with a
custom Needleman Wunsch style aligner that yields both a consensus
transcription and a confidence score. We present a new dataset of 622
Pennsylvania death records, and demonstrate our method improves transcription
accuracy by 4 percentage points relative to a single shot baseline. We find
that padding and blurring are the most useful for improving accuracy, while
grid warp perturbations are best for separating high and low confidence cases.
The approach is simple, scalable, and immediately deployable to other document
collections and transcription models.

</details>


### [4] [MITS: A Large-Scale Multimodal Benchmark Dataset for Intelligent Traffic Surveillance](https://arxiv.org/abs/2509.09730)
*Kaikai Zhao,Zhaoxiang Liu,Peng Wang,Xin Wang,Zhicheng Ma,Yajun Xu,Wenjing Zhang,Yibing Nan,Kai Wang,Shiguo Lian*

Main category: cs.CV

TL;DR: Introduces MITS, the first large-scale multimodal benchmark dataset for Intelligent Traffic Surveillance, containing 170,400 real-world images with comprehensive annotations and 5M QA pairs, significantly improving LMM performance in ITS applications.


<details>
  <summary>Details</summary>
Motivation: General-domain large multimodal models perform poorly in Intelligent Traffic Surveillance due to lack of dedicated multimodal datasets, creating a need for domain-specific training data.

Method: Created MITS dataset with 170,400 real ITS images annotated with 8 main categories and 24 subcategories, plus generated high-quality captions and 5 million instruction-following QA pairs covering five critical ITS tasks.

Result: Fine-tuning on MITS dramatically improved LMM performance: LLaVA-1.5 from 0.494 to 0.905 (+83.2%), LLaVA-1.6 from 0.678 to 0.921 (+35.8%), Qwen2-VL from 0.584 to 0.926 (+58.6%), Qwen2.5-VL from 0.732 to 0.930 (+27.0%).

Conclusion: MITS dataset effectively bridges the gap in ITS domain multimodal learning, enabling development of ITS-specific applications and providing valuable open-source resources for advancing both ITS and LMM research.

Abstract: General-domain large multimodal models (LMMs) have achieved significant
advances in various image-text tasks. However, their performance in the
Intelligent Traffic Surveillance (ITS) domain remains limited due to the
absence of dedicated multimodal datasets. To address this gap, we introduce
MITS (Multimodal Intelligent Traffic Surveillance), the first large-scale
multimodal benchmark dataset specifically designed for ITS. MITS includes
170,400 independently collected real-world ITS images sourced from traffic
surveillance cameras, annotated with eight main categories and 24 subcategories
of ITS-specific objects and events under diverse environmental conditions.
Additionally, through a systematic data generation pipeline, we generate
high-quality image captions and 5 million instruction-following visual
question-answer pairs, addressing five critical ITS tasks: object and event
recognition, object counting, object localization, background analysis, and
event reasoning. To demonstrate MITS's effectiveness, we fine-tune mainstream
LMMs on this dataset, enabling the development of ITS-specific applications.
Experimental results show that MITS significantly improves LMM performance in
ITS applications, increasing LLaVA-1.5's performance from 0.494 to 0.905
(+83.2%), LLaVA-1.6's from 0.678 to 0.921 (+35.8%), Qwen2-VL's from 0.584 to
0.926 (+58.6%), and Qwen2.5-VL's from 0.732 to 0.930 (+27.0%). We release the
dataset, code, and models as open-source, providing high-value resources to
advance both ITS and LMM research.

</details>


### [5] [Decomposing Visual Classification: Assessing Tree-Based Reasoning in VLMs](https://arxiv.org/abs/2509.09732)
*Sary Elmansoury,Islam Mesabah,Gerrit Großmann,Peter Neigel,Raj Bhalwankar,Daniel Kondermann,Sebastian J. Vollmer*

Main category: cs.CV

TL;DR: Tree-based reasoning framework for VLMs underperforms standard zero-shot prompting on fine-grained classification tasks, despite achieving high accuracy in understanding tree structure.


<details>
  <summary>Details</summary>
Motivation: To investigate whether structured, tree-based reasoning can enhance vision language model performance on fine-grained tasks and large hierarchical label spaces.

Method: Introduced a framework that decomposes classification into interpretable decisions using decision trees, evaluated on GTSRB (fine-grained) and CIFAR-10 (coarse-grained) datasets. Also explored enhancing tree prompts with LLM-generated classes and image descriptions.

Result: Tree-based reasoning consistently underperformed standard zero-shot prompting. The model achieved 98.2% accuracy in understanding tree knowledge but couldn't translate this to better classification performance. Adding image descriptions enhanced both tree-based and zero-shot methods.

Conclusion: Structured reasoning has limitations in visual classification tasks. Findings provide insights for designing more interpretable VLM systems, suggesting that while trees are well-understood, they don't necessarily improve classification performance over simpler approaches.

Abstract: Vision language models (VLMs) excel at zero-shot visual classification, but
their performance on fine-grained tasks and large hierarchical label spaces is
understudied. This paper investigates whether structured, tree-based reasoning
can enhance VLM performance. We introduce a framework that decomposes
classification into interpretable decisions using decision trees and evaluates
it on fine-grained (GTSRB) and coarse-grained (CIFAR-10) datasets. Although the
model achieves 98.2% accuracy in understanding the tree knowledge, tree-based
reasoning consistently underperforms standard zero-shot prompting. We also
explore enhancing the tree prompts with LLM-generated classes and image
descriptions to improve alignment. The added description enhances the
performance of the tree-based and zero-shot methods. Our findings highlight
limitations of structured reasoning in visual classification and offer insights
for designing more interpretable VLM systems.

</details>


### [6] [World Modeling with Probabilistic Structure Integration](https://arxiv.org/abs/2509.09737)
*Klemen Kotar,Wanhee Lee,Rahul Venkatesh,Honglin Chen,Daniel Bear,Jared Watrous,Simon Kim,Khai Loong Aw,Lilian Naing Chen,Stefan Stojanov,Kevin Feigelis,Imran Thobani,Alex Durango,Khaled Jedoui,Atlas Kazemian,Dan Yamins*

Main category: cs.CV

TL;DR: PSI is a system that learns controllable world models through a 3-step cycle: probabilistic prediction, structure extraction, and integration, enabling improved video modeling and state-of-the-art computer vision tasks.


<details>
  <summary>Details</summary>
Motivation: To create richly controllable and flexibly promptable world models that can extract meaningful intermediate structures from data and use them for improved prediction and understanding.

Method: Three-step cycle: 1) Build probabilistic graphical model (Psi) as random-access autoregressive sequence model, 2) Extract low-dimensional structures via causal inference, 3) Integrate structures as new token types for continual training.

Result: Trained on 1.4T video tokens; achieved state-of-the-art optical flow, self-supervised depth, and object segmentation; enabled full cycle of predictive improvements.

Conclusion: PSI successfully creates universal prompting-like capabilities for world models, allowing continuous improvement through structure extraction and integration cycles.

Abstract: We present Probabilistic Structure Integration (PSI), a system for learning
richly controllable and flexibly promptable world models from data. PSI
consists of a three-step cycle. The first step, Probabilistic prediction,
involves building a probabilistic graphical model Psi of the data, in the form
of a random-access autoregressive sequence model. Psi supports a complete set
of learned conditional distributions describing the dependence of any variables
in the data on any other set of variables. In step 2, Structure extraction, we
show how to extract underlying low-dimensional properties in the data,
corresponding to a diverse set of meaningful "intermediate structures", in a
zero-shot fashion via causal inference on Psi. Step 3, Integration, completes
the cycle by converting these structures into new token types that are then
continually mixed back into the training diet as conditioning signals and
prediction targets. Each such cycle augments the capabilities of Psi, both
allowing it to model the underlying data better, and creating new control
handles -- akin to an LLM-like universal prompting language. We train an
instance of Psi on 1.4 trillion tokens of internet video data; we use it to
perform a variety of useful video prediction and understanding inferences; we
extract state-of-the-art optical flow, self-supervised depth and object
segmentation; and we use these structures to support a full cycle of predictive
improvements.

</details>


### [7] [Images in Motion?: A First Look into Video Leakage in Collaborative Deep Learning](https://arxiv.org/abs/2509.09742)
*Md Fazle Rasul,Alanood Alqobaisi,Bruhadeshwar Bezawada,Indrakshi Ray*

Main category: cs.CV

TL;DR: First analysis of video data leakage in federated learning via gradient inversion attacks, showing both raw frame processing and feature extractor approaches are vulnerable, with super-resolution techniques enhancing attack effectiveness.


<details>
  <summary>Details</summary>
Motivation: Federated learning's privacy protection is threatened by gradient inversion attacks that can reconstruct private data from shared gradients. While known for image/text/tabular data, video data vulnerability remains unexamined.

Method: Evaluated two video classification approaches: pre-trained feature extractors and raw video frame processing. Tested gradient inversion attacks with zero/one/multiple reference frames and used super-resolution to enhance reconstructed frames.

Result: Feature extractors offer greater resilience but leakage still occurs if classifier lacks complexity. Super-resolution techniques successfully enhance reconstructed video quality. Attacks are viable across all tested scenarios.

Conclusion: Video data leakage in federated learning is a viable threat that warrants further investigation, as current protection methods remain insufficient against sophisticated gradient inversion attacks.

Abstract: Federated learning (FL) allows multiple entities to train a shared model
collaboratively. Its core, privacy-preserving principle is that participants
only exchange model updates, such as gradients, and never their raw, sensitive
data. This approach is fundamental for applications in domains where privacy
and confidentiality are important. However, the security of this very mechanism
is threatened by gradient inversion attacks, which can reverse-engineer private
training data directly from the shared gradients, defeating the purpose of FL.
While the impact of these attacks is known for image, text, and tabular data,
their effect on video data remains an unexamined area of research. This paper
presents the first analysis of video data leakage in FL using gradient
inversion attacks. We evaluate two common video classification approaches: one
employing pre-trained feature extractors and another that processes raw video
frames with simple transformations. Our initial results indicate that the use
of feature extractors offers greater resilience against gradient inversion
attacks. We also demonstrate that image super-resolution techniques can enhance
the frames extracted through gradient inversion attacks, enabling attackers to
reconstruct higher-quality videos. Our experiments validate this across
scenarios where the attacker has access to zero, one, or more reference frames
from the target environment. We find that although feature extractors make
attacks more challenging, leakage is still possible if the classifier lacks
sufficient complexity. We, therefore, conclude that video data leakage in FL is
a viable threat, and the conditions under which it occurs warrant further
investigation.

</details>


### [8] [A Co-Training Semi-Supervised Framework Using Faster R-CNN and YOLO Networks for Object Detection in Densely Packed Retail Images](https://arxiv.org/abs/2509.09750)
*Hossein Yazdanjouei,Arash Mansouri,Mohammad Shokouhifar*

Main category: cs.CV

TL;DR: Semi-supervised co-training framework for retail object detection that combines Faster R-CNN and YOLO with ensemble classifiers, reducing manual labeling needs while handling occlusion and overlapping objects effectively.


<details>
  <summary>Details</summary>
Motivation: Address challenges in densely packed retail environments with limited labeled data, complex conditions, occlusion, and overlapping objects while reducing annotation costs and adapting to frequent product/layout changes.

Method: Co-training framework combining Faster R-CNN (ResNet backbone) for precise localization and YOLO (Darknet backbone) for global context with mutual pseudo-label exchange. Uses ensemble classification (XGBoost, Random Forest, SVM) and metaheuristic hyperparameter optimization.

Result: Strong performance on SKU-110k dataset, demonstrating improved accuracy in scenes with occlusion and overlapping objects, with enhanced precision and efficiency across models.

Conclusion: The framework shows scalability and practicality for real-world retail applications including automated inventory tracking, product monitoring, and checkout systems, effectively minimizing reliance on manual labeling.

Abstract: This study proposes a semi-supervised co-training framework for object
detection in densely packed retail environments, where limited labeled data and
complex conditions pose major challenges. The framework combines Faster R-CNN
(utilizing a ResNet backbone) for precise localization with YOLO (employing a
Darknet backbone) for global context, enabling mutual pseudo-label exchange
that improves accuracy in scenes with occlusion and overlapping objects. To
strengthen classification, it employs an ensemble of XGBoost, Random Forest,
and SVM, utilizing diverse feature representations for higher robustness.
Hyperparameters are optimized using a metaheuristic-driven algorithm, enhancing
precision and efficiency across models. By minimizing reliance on manual
labeling, the approach reduces annotation costs and adapts effectively to
frequent product and layout changes common in retail. Experiments on the
SKU-110k dataset demonstrate strong performance, highlighting the scalability
and practicality of the proposed framework for real-world retail applications
such as automated inventory tracking, product monitoring, and checkout systems.

</details>


### [9] [Purge-Gate: Backpropagation-Free Test-Time Adaptation for Point Clouds Classification via Token Purging](https://arxiv.org/abs/2509.09785)
*Moslem Yazdanpanah,Ali Bahri,Mehrdad Noori,Sahar Dastani,Gustavo Adolfo Vargas Hakim,David Osowiechi,Ismail Ben Ayed,Christian Desrosiers*

Main category: cs.CV

TL;DR: Token Purging (PG) is a novel backpropagation-free test-time adaptation method for 3D point cloud classification that removes domain-shifted tokens before attention layers, achieving significant performance gains and efficiency improvements.


<details>
  <summary>Details</summary>
Motivation: To address performance degradation caused by distribution shifts in 3D point cloud classification without requiring iterative updates or backpropagation during test-time adaptation.

Method: Two variants: PG-SP (uses source statistics) and PG-SF (fully source-free using CLS-token-driven adaptation). Both remove tokens highly affected by domain shifts before they reach attention layers.

Result: PG-SP achieves +10.3% higher accuracy than state-of-the-art backpropagation-free methods. PG-SF sets new benchmarks for source-free adaptation. 12.4x faster and 5.5x more memory efficient than baseline.

Conclusion: Token Purging provides an effective, efficient solution for test-time adaptation in 3D point cloud classification, suitable for real-world deployment due to its speed and memory efficiency.

Abstract: Test-time adaptation (TTA) is crucial for mitigating performance degradation
caused by distribution shifts in 3D point cloud classification. In this work,
we introduce Token Purging (PG), a novel backpropagation-free approach that
removes tokens highly affected by domain shifts before they reach attention
layers. Unlike existing TTA methods, PG operates at the token level, ensuring
robust adaptation without iterative updates. We propose two variants: PG-SP,
which leverages source statistics, and PG-SF, a fully source-free version
relying on CLS-token-driven adaptation. Extensive evaluations on ModelNet40-C,
ShapeNet-C, and ScanObjectNN-C demonstrate that PG-SP achieves an average of
+10.3\% higher accuracy than state-of-the-art backpropagation-free methods,
while PG-SF sets new benchmarks for source-free adaptation. Moreover, PG is
12.4 times faster and 5.5 times more memory efficient than our baseline, making
it suitable for real-world deployment. Code is available at
\hyperlink{https://github.com/MosyMosy/Purge-Gate}{https://github.com/MosyMosy/Purge-Gate}

</details>


### [10] [Fine-Grained Cross-View Localization via Local Feature Matching and Monocular Depth Priors](https://arxiv.org/abs/2509.09792)
*Zimin Xia,Chenghao Xu,Alexandre Alahi*

Main category: cs.CV

TL;DR: A fine-grained cross-view localization method that directly matches ground-level image features with aerial images using monocular depth prior, supporting both metric and relative depth with scale-aware alignment.


<details>
  <summary>Details</summary>
Motivation: Previous methods transform ground images to bird's-eye view, causing information loss from perspective distortion and height compression, which degrades alignment quality with aerial imagery.

Method: Directly establishes correspondences between ground and aerial images, lifts matched keypoints to BEV space using monocular depth prior, and uses scale-aware Procrustes alignment for pose estimation with optional scale recovery for relative depth.

Result: Achieves superior localization performance under challenging conditions (cross-area generalization, unknown orientation) with only weak pose supervision, learns accurate feature correspondences, and works with various depth models without per-model finetuning.

Conclusion: The method provides accurate, interpretable localization with flexibility for real-world deployment, overcoming limitations of traditional BEV transformation approaches.

Abstract: We propose an accurate and highly interpretable fine-grained cross-view
localization method that estimates the 3 Degrees of Freedom pose of a
ground-level image by matching its local features with a reference aerial
image. Previous methods typically transform the ground image into a bird's-eye
view (BEV) representation and then align it with the aerial image for
localization. However, this transformation often leads to information loss due
to perspective distortion or compression of height information, thereby
degrading alignment quality with the aerial view. In contrast, our method
directly establishes correspondences between ground and aerial images and lifts
only the matched keypoints to BEV space using monocular depth prior. Notably,
modern depth predictors can provide reliable metric depth when the test samples
are similar to the training data. When the depth distribution differs, they
still produce consistent relative depth, i.e., depth accurate up to an unknown
scale. Our method supports both metric and relative depth. It employs a
scale-aware Procrustes alignment to estimate the camera pose from the
correspondences and optionally recover the scale when using relative depth.
Experimental results demonstrate that, with only weak supervision on camera
pose, our method learns accurate local feature correspondences and achieves
superior localization performance under challenging conditions, such as
cross-area generalization and unknown orientation. Moreover, our method is
compatible with various relative depth models without requiring per-model
finetuning. This flexibility, combined with strong localization performance,
makes it well-suited for real-world deployment.

</details>


### [11] [Early Detection of Visual Impairments at Home Using a Smartphone Red-Eye Reflex Test](https://arxiv.org/abs/2509.09808)
*Judith Massmann,Alexander Lichtenstein,Francisco M. López*

Main category: cs.CV

TL;DR: KidsVisionCheck is a mobile app that uses AI to perform pediatric vision screening via red-eye reflex images, achieving 90% accuracy without specialist equipment.


<details>
  <summary>Details</summary>
Motivation: To make pediatric vision screening more accessible worldwide by recreating the clinical Bruckner test using smartphone technology and AI.

Method: Deep neural networks trained on ophthalmologist-labeled red-eye reflex images from children, collected via mobile device.

Result: 90% accuracy on unseen test data, with identification of optimal data collection conditions for immediate user feedback.

Conclusion: This represents a significant step toward accessible pediatric vision screening and early intervention for vision abnormalities globally.

Abstract: Numerous visual impairments can be detected in red-eye reflex images from
young children. The so-called Bruckner test is traditionally performed by
ophthalmologists in clinical settings. Thanks to the recent technological
advances in smartphones and artificial intelligence, it is now possible to
recreate the Bruckner test using a mobile device. In this paper, we present a
first study conducted during the development of KidsVisionCheck, a free
application that can perform vision screening with a mobile device using
red-eye reflex images. The underlying model relies on deep neural networks
trained on children's pupil images collected and labeled by an ophthalmologist.
With an accuracy of 90% on unseen test data, our model provides highly reliable
performance without the necessity of specialist equipment. Furthermore, we can
identify the optimal conditions for data collection, which can in turn be used
to provide immediate feedback to the users. In summary, this work marks a first
step toward accessible pediatric vision screenings and early intervention for
vision abnormalities worldwide.

</details>


### [12] [DGFusion: Depth-Guided Sensor Fusion for Robust Semantic Perception](https://arxiv.org/abs/2509.09828)
*Tim Broedermannn,Christos Sakaridis,Luigi Piccinelli,Wim Abbeloos,Luc Van Gool*

Main category: cs.CV

TL;DR: DGFusion: A depth-guided multimodal fusion method that uses depth information to dynamically adapt sensor fusion for robust semantic perception in autonomous vehicles, achieving state-of-the-art performance on challenging datasets.


<details>
  <summary>Details</summary>
Motivation: Current sensor fusion approaches treat sensor data uniformly across spatial inputs, which hinders performance in challenging conditions. Autonomous vehicles need robust semantic perception that effectively combines multiple sensors with complementary strengths.

Method: Proposes DGFusion network that treats multimodal segmentation as multi-task problem. Uses lidar measurements as input and ground truth for learning depth. Features auxiliary depth head to learn depth-aware features, encoded into spatially varying local depth tokens that condition attentive cross-modal fusion. Includes global condition token and robust loss for handling sparse, noisy lidar data.

Result: Achieves state-of-the-art panoptic and semantic segmentation performance on challenging MUSES and DELIVER datasets.

Conclusion: Depth-guided fusion with spatially varying local depth tokens and global conditioning effectively adapts sensor fusion to varying sensor reliability across scenes, significantly improving semantic perception performance in autonomous driving scenarios.

Abstract: Robust semantic perception for autonomous vehicles relies on effectively
combining multiple sensors with complementary strengths and weaknesses.
State-of-the-art sensor fusion approaches to semantic perception often treat
sensor data uniformly across the spatial extent of the input, which hinders
performance when faced with challenging conditions. By contrast, we propose a
novel depth-guided multimodal fusion method that upgrades condition-aware
fusion by integrating depth information. Our network, DGFusion, poses
multimodal segmentation as a multi-task problem, utilizing the lidar
measurements, which are typically available in outdoor sensor suites, both as
one of the model's inputs and as ground truth for learning depth. Our
corresponding auxiliary depth head helps to learn depth-aware features, which
are encoded into spatially varying local depth tokens that condition our
attentive cross-modal fusion. Together with a global condition token, these
local depth tokens dynamically adapt sensor fusion to the spatially varying
reliability of each sensor across the scene, which largely depends on depth. In
addition, we propose a robust loss for our depth, which is essential for
learning from lidar inputs that are typically sparse and noisy in adverse
conditions. Our method achieves state-of-the-art panoptic and semantic
segmentation performance on the challenging MUSES and DELIVER datasets. Code
and models will be available at https://github.com/timbroed/DGFusion

</details>


### [13] [Patch-based Automatic Rosacea Detection Using the ResNet Deep Learning Framework](https://arxiv.org/abs/2509.09841)
*Chengyu Yang,Rishik Reddy Yesgari,Chengjun Liu*

Main category: cs.CV

TL;DR: Patch-based rosacea detection using ResNet-18 with localized facial patches achieves competitive accuracy while preserving patient privacy by excluding identifiable features.


<details>
  <summary>Details</summary>
Motivation: Rosacea requires early detection for effective treatment, and current methods need improved precision while addressing privacy concerns in facial image analysis.

Method: Extracted various image patches from facial images in different sizes, shapes, and locations, then used ResNet-18 deep learning framework to evaluate how localized visual information affects model performance.

Result: Patch-based strategies achieved competitive or superior accuracy and sensitivity compared to full-image methods, while guiding the model to focus on clinically relevant regions and enhancing robustness.

Conclusion: The patch-based approach offers practical insights for automated dermatological diagnostics by improving detection accuracy, interpretability, and patient privacy protection.

Abstract: Rosacea, which is a chronic inflammatory skin condition that manifests with
facial redness, papules, and visible blood vessels, often requirs precise and
early detection for significantly improving treatment effectiveness. This paper
presents new patch-based automatic rosacea detection strategies using the
ResNet-18 deep learning framework. The contributions of the proposed strategies
come from the following aspects. First, various image pateches are extracted
from the facial images of people in different sizes, shapes, and locations.
Second, a number of investigation studies are carried out to evaluate how the
localized visual information influences the deep learing model performance.
Third, thorough experiments are implemented to reveal that several patch-based
automatic rosacea detection strategies achieve competitive or superior accuracy
and sensitivity than the full-image based methods. And finally, the proposed
patch-based strategies, which use only localized patches, inherently preserve
patient privacy by excluding any identifiable facial features from the data.
The experimental results indicate that the proposed patch-based strategies
guide the deep learning model to focus on clinically relevant regions, enhance
robustness and interpretability, and protect patient privacy. As a result, the
proposed strategies offer practical insights for improving automated
dermatological diagnostics.

</details>


### [14] [Privacy-Preserving Automated Rosacea Detection Based on Medically Inspired Region of Interest Selection](https://arxiv.org/abs/2509.09844)
*Chengyu Yang,Rishik Reddy Yesgari,Chengjun Liu*

Main category: cs.CV

TL;DR: Privacy-preserving rosacea detection using synthetic data and clinical priors with a redness-informed mask to focus on diagnostically relevant facial areas while excluding identity features.


<details>
  <summary>Details</summary>
Motivation: Rosacea is underdiagnosed and automated detection faces challenges due to diffuse symptoms, lack of labeled datasets, and privacy concerns with facial images.

Method: Uses a fixed redness-informed mask to select high red-intensity regions (cheeks, nose, forehead) and trains ResNet-18 on masked synthetic images.

Result: Achieves superior performance over full-face baselines with notable gains in accuracy, recall, and F1 score on real-world test data.

Conclusion: Synthetic data combined with clinical priors enables accurate and ethical dermatological AI systems for privacy-sensitive applications like telemedicine.

Abstract: Rosacea is a common but underdiagnosed inflammatory skin condition that
primarily affects the central face and presents with subtle redness, pustules,
and visible blood vessels. Automated detection remains challenging due to the
diffuse nature of symptoms, the scarcity of labeled datasets, and privacy
concerns associated with using identifiable facial images. A novel
privacy-preserving automated rosacea detection method inspired by clinical
priors and trained entirely on synthetic data is presented in this paper.
Specifically, the proposed method, which leverages the observation that rosacea
manifests predominantly through central facial erythema, first constructs a
fixed redness-informed mask by selecting regions with consistently high red
channel intensity across facial images. The mask thus is able to focus on
diagnostically relevant areas such as the cheeks, nose, and forehead and
exclude identity-revealing features. Second, the ResNet-18 deep learning
method, which is trained on the masked synthetic images, achieves superior
performance over the full-face baselines with notable gains in terms of
accuracy, recall and F1 score when evaluated using the real-world test data.
The experimental results demonstrate that the synthetic data and clinical
priors can jointly enable accurate and ethical dermatological AI systems,
especially for privacy sensitive applications in telemedicine and large-scale
screening.

</details>


### [15] [Investigating the Impact of Various Loss Functions and Learnable Wiener Filter for Laparoscopic Image Desmoking](https://arxiv.org/abs/2509.09849)
*Chengyu Yang,Chengjun Liu*

Main category: cs.CV

TL;DR: Comprehensive ablation study of ULW framework for laparoscopic image desmoking, evaluating individual components including learnable Wiener filter and loss function terms.


<details>
  <summary>Details</summary>
Motivation: To rigorously assess the effectiveness and necessity of individual components within the ULW framework for laparoscopic image desmoking.

Method: Systematic ablation study removing the learnable Wiener filter and selectively using individual loss terms (MSE, SSIM, perceptual loss) from the compound loss function, with benchmarking on paired laparoscopic images dataset.

Result: Evaluation performed using quantitative metrics (SSIM, PSNR, MSE, CIEDE-2000) alongside qualitative visual comparisons to assess each component's contribution.

Conclusion: The study provides comprehensive analysis of component contributions to the ULW framework's overall performance in laparoscopic image desmoking.

Abstract: To rigorously assess the effectiveness and necessity of individual components
within the recently proposed ULW framework for laparoscopic image desmoking,
this paper presents a comprehensive ablation study. The ULW approach combines a
U-Net based backbone with a compound loss function that comprises mean squared
error (MSE), structural similarity index (SSIM) loss, and perceptual loss. The
framework also incorporates a differentiable, learnable Wiener filter module.
In this study, each component is systematically ablated to evaluate its
specific contribution to the overall performance of the whole framework. The
analysis includes: (1) removal of the learnable Wiener filter, (2) selective
use of individual loss terms from the composite loss function. All variants are
benchmarked on a publicly available paired laparoscopic images dataset using
quantitative metrics (SSIM, PSNR, MSE and CIEDE-2000) alongside qualitative
visual comparisons.

</details>


### [16] [WAVE-DETR Multi-Modal Visible and Acoustic Real-Life Drone Detector](https://arxiv.org/abs/2509.09859)
*Razvan Stefanescu,Ethan Oh,Ruben Vazquez,Chris Mesterharm,Constantin Serban,Ritu Chadha*

Main category: cs.CV

TL;DR: Multi-modal drone detector combining RGB visual and acoustic signals using Deformable DETR and Wav2Vec2, achieving significant performance improvements across all drone sizes.


<details>
  <summary>Details</summary>
Motivation: To create a robust UAV detection system that works under challenging environmental conditions by leveraging both visual and acoustic information for improved detection accuracy.

Method: Fuses visual features from Deformable DETR with acoustic embeddings from Wav2Vec2 using four fusion configurations (gated mechanism, linear layer, MLP, cross attention) on synchronized image-audio datasets.

Result: Gated fusion approach improved mAP by 11.1-15.3% for small drones, with overall gains of 3.27-5.84% across all drone sizes on both in-distribution and out-of-distribution datasets.

Conclusion: Acoustic information significantly enhances drone detection performance, with gated fusion being the most effective approach for multi-modal feature integration in real-world UAV detection scenarios.

Abstract: We introduce a multi-modal WAVE-DETR drone detector combining visible RGB and
acoustic signals for robust real-life UAV object detection. Our approach fuses
visual and acoustic features in a unified object detector model relying on the
Deformable DETR and Wav2Vec2 architectures, achieving strong performance under
challenging environmental conditions. Our work leverage the existing
Drone-vs-Bird dataset and the newly generated ARDrone dataset containing more
than 7,500 synchronized images and audio segments. We show how the acoustic
information is used to improve the performance of the Deformable DETR object
detector on the real ARDrone dataset. We developed, trained and tested four
different fusion configurations based on a gated mechanism, linear layer, MLP
and cross attention. The Wav2Vec2 acoustic embeddings are fused with the multi
resolution feature mappings of the Deformable DETR and enhance the object
detection performance over all drones dimensions. The best performer is the
gated fusion approach, which improves the mAP of the Deformable DETR object
detector on our in-distribution and out-of-distribution ARDrone datasets by
11.1% to 15.3% for small drones across all IoU thresholds between 0.5 and 0.9.
The mAP scores for medium and large drones are also enhanced, with overall
gains across all drone sizes ranging from 3.27% to 5.84%.

</details>


### [17] [Surrogate Supervision for Robust and Generalizable Deformable Image Registration](https://arxiv.org/abs/2509.09869)
*Yihao Liu,Junyu Chen,Lianrui Zuo,Shuwen Wei,Brian D. Boyd,Carmen Andreescu,Olusola Ajilore,Warren D. Taylor,Aaron Carass,Bennett A. Landman*

Main category: cs.CV

TL;DR: Surrogate supervision improves robustness of deep learning image registration by decoupling input domain from supervision domain, using estimated transformations on surrogate images to handle heterogeneous inputs while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Deep learning-based deformable image registration achieves strong accuracy but remains sensitive to variations in input image characteristics like artifacts, field-of-view mismatch, or modality differences.

Method: Introduces surrogate supervision that decouples input domain from supervision domain by applying estimated spatial transformations to surrogate images, allowing training on heterogeneous inputs while ensuring supervision in domains where similarity is well-defined.

Result: Demonstrated strong resilience to input variations including inhomogeneity field, inconsistent field-of-view, and modality differences across three applications (brain MR, lung CT, multi-modal MR), while maintaining high performance on well-curated data.

Conclusion: Surrogate supervision provides a principled framework for training robust and generalizable deep learning-based registration models without increasing complexity, enabling broader applicability in diverse biomedical imaging scenarios.

Abstract: Objective: Deep learning-based deformable image registration has achieved
strong accuracy, but remains sensitive to variations in input image
characteristics such as artifacts, field-of-view mismatch, or modality
difference. We aim to develop a general training paradigm that improves the
robustness and generalizability of registration networks. Methods: We introduce
surrogate supervision, which decouples the input domain from the supervision
domain by applying estimated spatial transformations to surrogate images. This
allows training on heterogeneous inputs while ensuring supervision is computed
in domains where similarity is well defined. We evaluate the framework through
three representative applications: artifact-robust brain MR registration,
mask-agnostic lung CT registration, and multi-modal MR registration. Results:
Across tasks, surrogate supervision demonstrated strong resilience to input
variations including inhomogeneity field, inconsistent field-of-view, and
modality differences, while maintaining high performance on well-curated data.
Conclusions: Surrogate supervision provides a principled framework for training
robust and generalizable deep learning-based registration models without
increasing complexity. Significance: Surrogate supervision offers a practical
pathway to more robust and generalizable medical image registration, enabling
broader applicability in diverse biomedical imaging scenarios.

</details>


### [18] [An Autoencoder and Vision Transformer-based Interpretability Analysis of the Differences in Automated Staging of Second and Third Molars](https://arxiv.org/abs/2509.09911)
*Barkin Buyukcakir,Jannick De Tobel,Patrick Thevissen,Dirk Vandermeulen,Peter Claes*

Main category: cs.CV

TL;DR: A framework combining convolutional autoencoder and Vision Transformer improves dental age estimation accuracy and provides multi-faceted interpretability, revealing data-centric limitations in tooth morphology variability.


<details>
  <summary>Details</summary>
Motivation: Address the 'black box' nature of deep learning models in high-stakes forensic applications like dental age estimation, where transparency is crucial for adoption.

Method: Proposed framework combining convolutional autoencoder (AE) with Vision Transformer (ViT) to enhance both performance and interpretability, using mandibular second and third molars as case study.

Result: Improved classification accuracy from 0.712 to 0.815 for tooth 37 and from 0.462 to 0.543 for tooth 38 over baseline ViT. Analysis revealed performance gap is data-centric due to high intra-class morphological variability in tooth 38 dataset.

Conclusion: The framework provides both enhanced accuracy and multi-faceted diagnostic insights, demonstrating insufficiency of single interpretability modes and serving as a robust tool for expert decision-making in forensic age estimation.

Abstract: The practical adoption of deep learning in high-stakes forensic applications,
such as dental age estimation, is often limited by the 'black box' nature of
the models. This study introduces a framework designed to enhance both
performance and transparency in this context. We use a notable performance
disparity in the automated staging of mandibular second (tooth 37) and third
(tooth 38) molars as a case study. The proposed framework, which combines a
convolutional autoencoder (AE) with a Vision Transformer (ViT), improves
classification accuracy for both teeth over a baseline ViT, increasing from
0.712 to 0.815 for tooth 37 and from 0.462 to 0.543 for tooth 38. Beyond
improving performance, the framework provides multi-faceted diagnostic
insights. Analysis of the AE's latent space metrics and image reconstructions
indicates that the remaining performance gap is data-centric, suggesting high
intra-class morphological variability in the tooth 38 dataset is a primary
limiting factor. This work highlights the insufficiency of relying on a single
mode of interpretability, such as attention maps, which can appear anatomically
plausible yet fail to identify underlying data issues. By offering a
methodology that both enhances accuracy and provides evidence for why a model
may be uncertain, this framework serves as a more robust tool to support expert
decision-making in forensic age estimation.

</details>


### [19] [SCoDA: Self-supervised Continual Domain Adaptation](https://arxiv.org/abs/2509.09935)
*Chirayu Agrawal,Snehasis Mukherjee*

Main category: cs.CV

TL;DR: SCoDA introduces self-supervised pre-training and geometric manifold alignment for source-free domain adaptation, outperforming existing methods by preserving crucial geometric information and avoiding catastrophic forgetting.


<details>
  <summary>Details</summary>
Motivation: Existing SFDA methods discard geometric information about the latent manifold when using cosine similarity over L2-normalized features, and rely on supervised pre-training which limits their effectiveness.

Method: Initializes with self-supervised pre-trained teacher model, uses geometric manifold alignment with Space Similarity Loss, and updates teacher via EMA to prevent catastrophic forgetting.

Result: Extensive experiments show SCoDA significantly outperforms state-of-the-art SFDA methods on benchmark datasets.

Conclusion: SCoDA successfully addresses limitations of previous SFDA approaches by combining self-supervised pre-training with geometric manifold alignment, achieving superior performance while avoiding catastrophic forgetting.

Abstract: Source-Free Domain Adaptation (SFDA) addresses the challenge of adapting a
model to a target domain without access to the data of the source domain.
Prevailing methods typically start with a source model pre-trained with full
supervision and distill the knowledge by aligning instance-level features.
However, these approaches, relying on cosine similarity over L2-normalized
feature vectors, inadvertently discard crucial geometric information about the
latent manifold of the source model. We introduce Self-supervised Continual
Domain Adaptation (SCoDA) to address these limitations. We make two key
departures from standard practice: first, we avoid the reliance on supervised
pre-training by initializing the proposed framework with a teacher model
pre-trained entirely via self-supervision (SSL). Second, we adapt the principle
of geometric manifold alignment to the SFDA setting. The student is trained
with a composite objective combining instance-level feature matching with a
Space Similarity Loss. To combat catastrophic forgetting, the teacher's
parameters are updated via an Exponential Moving Average (EMA) of the student's
parameters. Extensive experiments on benchmark datasets demonstrate that SCoDA
significantly outperforms state-of-the-art SFDA methods.

</details>


### [20] [Segment Anything for Cell Tracking](https://arxiv.org/abs/2509.09943)
*Zhu Chen,Mert Edgü,Er Jin,Johannes Stegmaier*

Main category: cs.CV

TL;DR: Zero-shot cell tracking framework using SAM2 foundation model for unsupervised microscopy image analysis without training data


<details>
  <summary>Details</summary>
Motivation: Overcome limitations of manual labeling costs, time-consuming training, and poor generalizability in existing deep learning methods for cell tracking

Method: Integrate Segment Anything 2 (SAM2) foundation model into tracking pipeline as a fully-unsupervised approach without dataset-specific training

Result: Achieves competitive accuracy in both 2D and large-scale 3D time-lapse microscopy videos without dataset-specific adaptation

Conclusion: Proposed framework eliminates training dependency and generalizes across diverse microscopy datasets while maintaining competitive performance

Abstract: Tracking cells and detecting mitotic events in time-lapse microscopy image
sequences is a crucial task in biomedical research. However, it remains highly
challenging due to dividing objects, low signal-tonoise ratios, indistinct
boundaries, dense clusters, and the visually similar appearance of individual
cells. Existing deep learning-based methods rely on manually labeled datasets
for training, which is both costly and time-consuming. Moreover, their
generalizability to unseen datasets remains limited due to the vast diversity
of microscopy data. To overcome these limitations, we propose a zero-shot cell
tracking framework by integrating Segment Anything 2 (SAM2), a large foundation
model designed for general image and video segmentation, into the tracking
pipeline. As a fully-unsupervised approach, our method does not depend on or
inherit biases from any specific training dataset, allowing it to generalize
across diverse microscopy datasets without finetuning. Our approach achieves
competitive accuracy in both 2D and large-scale 3D time-lapse microscopy videos
while eliminating the need for dataset-specific adaptation.

</details>


### [21] [Online 3D Multi-Camera Perception through Robust 2D Tracking and Depth-based Late Aggregation](https://arxiv.org/abs/2509.09946)
*Vu-Minh Le,Thao-Anh Tran,Duc Huy Do,Xuan Canh Do,Huong Ninh,Hai Tran*

Main category: cs.CV

TL;DR: A method to extend 2D multi-camera tracking systems into 3D space using depth information and clustering, achieving 3rd place in the 2025 AI City Challenge.


<details>
  <summary>Details</summary>
Motivation: Existing MTMC systems track in 2D space, but 3D tracking requires rebuilding all components from scratch, which is impractical for current systems. The paper aims to bridge this gap by extending existing 2D systems to 3D.

Method: Utilizes depth information to reconstruct targets in point-cloud space, performs clustering and yaw refinement for 3D box recovery, and introduces enhanced online data association using local ID consistency for global ID assignment.

Result: The framework was evaluated on the 2025 AI City Challenge's 3D MTMC dataset and achieved 3rd place on the leaderboard.

Conclusion: The proposed approach successfully extends existing 2D multi-camera tracking systems to operate in 3D space without requiring complete system overhaul, demonstrating competitive performance in real-world challenges.

Abstract: Multi-Target Multi-Camera Tracking (MTMC) is an essential computer vision
task for automating large-scale surveillance. With camera calibration and depth
information, the targets in the scene can be projected into 3D space, offering
unparalleled levels of automatic perception of a 3D environment. However,
tracking in the 3D space requires replacing all 2D tracking components from the
ground up, which may be infeasible for existing MTMC systems. In this paper, we
present an approach for extending any online 2D multi-camera tracking system
into 3D space by utilizing depth information to reconstruct a target in
point-cloud space, and recovering its 3D box through clustering and yaw
refinement following tracking. We also introduced an enhanced online data
association mechanism that leverages the target's local ID consistency to
assign global IDs across frames. The proposed framework is evaluated on the
2025 AI City Challenge's 3D MTMC dataset, achieving 3rd place on the
leaderboard.

</details>


### [22] [Zero-Shot Referring Expression Comprehension via Visual-Language True/False Verification](https://arxiv.org/abs/2509.09958)
*Jeffrey Liu,Rongbin Hu*

Main category: cs.CV

TL;DR: Zero-shot visual-language verification workflow for Referring Expression Comprehension outperforms task-trained models without any REC-specific training


<details>
  <summary>Details</summary>
Motivation: To demonstrate that strong REC performance can be achieved through workflow design rather than task-specific pretraining, reducing cross-box interference and supporting abstention/multiple matches

Method: Reformulates REC as box-wise visual-language verification using COCO-clean generic detector proposals, with a general-purpose VLM answering True/False queries for each region without fine-tuning

Result: Surpasses zero-shot GroundingDINO baseline and exceeds reported results for trained GroundingDINO variants on RefCOCO, RefCOCO+, and RefCOCOg datasets

Conclusion: Workflow design drives strong zero-shot REC performance more effectively than task-specific pretraining, with verification significantly outperforming selection-based prompting

Abstract: Referring Expression Comprehension (REC) is usually addressed with
task-trained grounding models. We show that a zero-shot workflow, without any
REC-specific training, can achieve competitive or superior performance. Our
approach reformulates REC as box-wise visual-language verification: given
proposals from a COCO-clean generic detector (YOLO-World), a general-purpose
VLM independently answers True/False queries for each region. This simple
procedure reduces cross-box interference, supports abstention and multiple
matches, and requires no fine-tuning. On RefCOCO, RefCOCO+, and RefCOCOg, our
method not only surpasses a zero-shot GroundingDINO baseline but also exceeds
reported results for GroundingDINO trained on REC and GroundingDINO+CRG.
Controlled studies with identical proposals confirm that verification
significantly outperforms selection-based prompting, and results hold with open
VLMs. Overall, we show that workflow design, rather than task-specific
pretraining, drives strong zero-shot REC performance.

</details>


### [23] [Augment to Segment: Tackling Pixel-Level Imbalance in Wheat Disease and Pest Segmentation](https://arxiv.org/abs/2509.09961)
*Tianqi Wei,Xin Yu,Zhi Chen,Scott Chapman,Zi Huang*

Main category: cs.CV

TL;DR: Proposes RPCP augmentation to address extreme pixel imbalance in wheat disease segmentation by copying, transforming, and blending rare insect-damage patches with random projection filtering.


<details>
  <summary>Details</summary>
Motivation: Extreme pixel-level imbalance in wheat disease segmentation causes overfitting to common classes and insufficient learning of rare insect damage classes, impairing overall performance.

Method: Random Projected Copy-and-Paste (RPCP) technique extracts rare insect-damage patches, applies random geometric transformations, pastes them in appropriate regions avoiding overlaps, and uses random projection filtering for natural blending.

Result: Method substantially improves segmentation performance on insect damage class while maintaining or slightly enhancing accuracy on other categories.

Conclusion: Targeted augmentation effectively mitigates extreme pixel imbalance, providing a straightforward yet effective solution for agricultural segmentation problems.

Abstract: Accurate segmentation of foliar diseases and insect damage in wheat is
crucial for effective crop management and disease control. However, the insect
damage typically occupies only a tiny fraction of annotated pixels. This
extreme pixel-level imbalance poses a significant challenge to the segmentation
performance, which can result in overfitting to common classes and insufficient
learning of rare classes, thereby impairing overall performance. In this paper,
we propose a Random Projected Copy-and-Paste (RPCP) augmentation technique to
address the pixel imbalance problem. Specifically, we extract rare
insect-damage patches from annotated training images and apply random geometric
transformations to simulate variations. The transformed patches are then pasted
in appropriate regions while avoiding overlaps with lesions or existing damaged
regions. In addition, we apply a random projection filter to the pasted
regions, refining local features and ensuring a natural blend with the new
background. Experiments show that our method substantially improves
segmentation performance on the insect damage class, while maintaining or even
slightly enhancing accuracy on other categories. Our results highlight the
effectiveness of targeted augmentation in mitigating extreme pixel imbalance,
offering a straightforward yet effective solution for agricultural segmentation
problems.

</details>


### [24] [An HMM-based framework for identity-aware long-term multi-object tracking from sparse and uncertain identification: use case on long-term tracking in livestock](https://arxiv.org/abs/2509.09962)
*Anne Marthe Sophie Ngo Bibinbe,Chiron Bang,Patrick Gagnon,Jamie Ahloy-Dallaire,Eric R. Paquet*

Main category: cs.CV

TL;DR: A new HMM framework for long-term multi-object tracking that incorporates sporadic identifications to reduce identity switches and improve tracking performance over extended periods.


<details>
  <summary>Details</summary>
Motivation: Existing MOT approaches suffer from identity switches over time, making them unsuitable for long-term tracking needed in applications like livestock monitoring where sporadic identifications are available.

Method: Proposes a Hidden Markov Model framework that combines uncertain identities with tracking, using sporadic identifications from sources like feeders to maintain identity consistency.

Result: Improves F1 score of ByteTrack on a 10-minute pig tracking dataset with 21 identifications, shows robustness to identification uncertainty, and validates performance on MOT17 and MOT20 benchmarks with both ByteTrack and FairMOT.

Conclusion: The HMM framework effectively addresses long-term MOT challenges by leveraging sporadic identifications, demonstrating improved performance and robustness across different tracking scenarios and datasets.

Abstract: The need for long-term multi-object tracking (MOT) is growing due to the
demand for analyzing individual behaviors in videos that span several minutes.
Unfortunately, due to identity switches between objects, the tracking
performance of existing MOT approaches decreases over time, making them
difficult to apply for long-term tracking. However, in many real-world
applications, such as in the livestock sector, it is possible to obtain
sporadic identifications for some of the animals from sources like feeders. To
address the challenges of long-term MOT, we propose a new framework that
combines both uncertain identities and tracking using a Hidden Markov Model
(HMM) formulation. In addition to providing real-world identities to animals,
our HMM framework improves the F1 score of ByteTrack, a leading MOT approach
even with re-identification, on a 10 minute pig tracking dataset with 21
identifications at the pen's feeding station. We also show that our approach is
robust to the uncertainty of identifications, with performance increasing as
identities are provided more frequently. The improved performance of our HMM
framework was also validated on the MOT17 and MOT20 benchmark datasets using
both ByteTrack and FairMOT. The code for this new HMM framework and the new
10-minute pig tracking video dataset are available at:
https://github.com/ngobibibnbe/uncertain-identity-aware-tracking

</details>


### [25] [Event Camera Guided Visual Media Restoration & 3D Reconstruction: A Survey](https://arxiv.org/abs/2509.09971)
*Aupendu Kar,Vishnu Raj,Guan-Ming Su*

Main category: cs.CV

TL;DR: Survey on event camera fusion with traditional frame-based capture for video restoration and 3D reconstruction tasks, covering deep learning approaches for temporal/spatial enhancement and available datasets.


<details>
  <summary>Details</summary>
Motivation: Event cameras offer low latency, low power consumption, and high capture rates, but their fusion with traditional frame-based systems can significantly benefit video restoration and 3D reconstruction tasks.

Method: Systematic review of deep learning contributions to image/video enhancement and restoration, focusing on temporal enhancement (frame interpolation, motion deblurring) and spatial enhancement (super-resolution, low-light/HDR enhancement, artifact reduction). Also explores 3D reconstruction advancements with event-driven fusion.

Result: Comprehensive survey covering diverse topics with in-depth discussions on improving visual quality under challenging conditions, along with compilation of openly available datasets for reproducible research.

Conclusion: The survey consolidates recent progress to inspire further research into leveraging event camera systems combined with deep learning for advanced visual media restoration and enhancement.

Abstract: Event camera sensors are bio-inspired sensors which asynchronously capture
per-pixel brightness changes and output a stream of events encoding the
polarity, location and time of these changes. These systems are witnessing
rapid advancements as an emerging field, driven by their low latency, reduced
power consumption, and ultra-high capture rates. This survey explores the
evolution of fusing event-stream captured with traditional frame-based capture,
highlighting how this synergy significantly benefits various video restoration
and 3D reconstruction tasks. The paper systematically reviews major deep
learning contributions to image/video enhancement and restoration, focusing on
two dimensions: temporal enhancement (such as frame interpolation and motion
deblurring) and spatial enhancement (including super-resolution, low-light and
HDR enhancement, and artifact reduction). This paper also explores how the 3D
reconstruction domain evolves with the advancement of event driven fusion.
Diverse topics are covered, with in-depth discussions on recent works for
improving visual quality under challenging conditions. Additionally, the survey
compiles a comprehensive list of openly available datasets, enabling
reproducible research and benchmarking. By consolidating recent progress and
insights, this survey aims to inspire further research into leveraging event
camera systems, especially in combination with deep learning, for advanced
visual media restoration and enhancement.

</details>


### [26] [ISTASTrack: Bridging ANN and SNN via ISTA Adapter for RGB-Event Tracking](https://arxiv.org/abs/2509.09977)
*Siying Liu,Zikai Wang,Hanle Zheng,Yifan Hu,Xilin Wang,Qingkai Yang,Jibin Wu,Hao Guo,Lei Deng*

Main category: cs.CV

TL;DR: ISTASTrack is the first transformer-based ANN-SNN hybrid tracker with ISTA adapters for RGB-Event tracking, achieving state-of-the-art performance while maintaining high energy efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing artificial neural networks struggle to exploit the sparse and asynchronous nature of event streams in RGB-Event tracking, and effectively fusing features across heterogeneous ANN-SNN paradigms remains challenging.

Method: Two-branch model with vision transformer for RGB inputs and spiking transformer for event streams, featuring ISTA adapters for bidirectional feature interaction derived from sparse representation theory, plus temporal downsampling attention for temporal fusion.

Result: Achieves state-of-the-art performance on RGB-Event tracking benchmarks (FE240hz, VisEvent, COESOT, FELT) while maintaining high energy efficiency.

Conclusion: Demonstrates effectiveness and practicality of hybrid ANN-SNN designs for robust visual tracking, with publicly available code.

Abstract: RGB-Event tracking has become a promising trend in visual object tracking to
leverage the complementary strengths of both RGB images and dynamic spike
events for improved performance. However, existing artificial neural networks
(ANNs) struggle to fully exploit the sparse and asynchronous nature of event
streams. Recent efforts toward hybrid architectures combining ANNs and spiking
neural networks (SNNs) have emerged as a promising solution in RGB-Event
perception, yet effectively fusing features across heterogeneous paradigms
remains a challenge. In this work, we propose ISTASTrack, the first
transformer-based \textbf{A}NN-\textbf{S}NN hybrid \textbf{Track}er equipped
with \textbf{ISTA} adapters for RGB-Event tracking. The two-branch model
employs a vision transformer to extract spatial context from RGB inputs and a
spiking transformer to capture spatio-temporal dynamics from event streams. To
bridge the modality and paradigm gap between ANN and SNN features, we
systematically design a model-based ISTA adapter for bidirectional feature
interaction between the two branches, derived from sparse representation theory
by unfolding the iterative shrinkage thresholding algorithm. Additionally, we
incorporate a temporal downsampling attention module within the adapter to
align multi-step SNN features with single-step ANN features in the latent
space, improving temporal fusion. Experimental results on RGB-Event tracking
benchmarks, such as FE240hz, VisEvent, COESOT, and FELT, have demonstrated that
ISTASTrack achieves state-of-the-art performance while maintaining high energy
efficiency, highlighting the effectiveness and practicality of hybrid ANN-SNN
designs for robust visual tracking. The code is publicly available at
https://github.com/lsying009/ISTASTrack.git.

</details>


### [27] [FLARE-SSM: Deep State Space Models with Influence-Balanced Loss for 72-Hour Solar Flare Prediction](https://arxiv.org/abs/2509.09988)
*Yusuke Takagi,Shunya Nagashima,Komei Sugiura*

Main category: cs.CV

TL;DR: Proposed solar flare prediction model using multiple deep state space models with FLARE loss to handle class imbalance, achieving better performance than baselines on standard metrics.


<details>
  <summary>Details</summary>
Motivation: Accurate solar flare prediction is crucial for infrastructure protection, but current methods perform poorly due to severe class imbalance across flare classes.

Method: Multiple deep state space models with frequency & local-boundary-aware reliability loss (FLARE loss) to address class imbalance issues.

Result: Outperformed baseline approaches in both Gandin-Murphy-Gerrity score and true skill statistic metrics on multi-wavelength solar image dataset covering 11-year solar cycle.

Conclusion: The proposed method effectively handles class imbalance in solar flare prediction and improves both performance and reliability compared to existing approaches.

Abstract: Accurate and reliable solar flare predictions are essential to mitigate
potential impacts on critical infrastructure. However, the current performance
of solar flare forecasting is insufficient. In this study, we address the task
of predicting the class of the largest solar flare expected to occur within the
next 72 hours. Existing methods often fail to adequately address the severe
class imbalance across flare classes. To address this issue, we propose a solar
flare prediction model based on multiple deep state space models. In addition,
we introduce the frequency & local-boundary-aware reliability loss (FLARE loss)
to improve predictive performance and reliability under class imbalance.
Experiments were conducted on a multi-wavelength solar image dataset covering a
full 11-year solar activity cycle. As a result, our method outperformed
baseline approaches in terms of both the Gandin-Murphy-Gerrity score and the
true skill statistic, which are standard metrics in terms of the performance
and reliability.

</details>


### [28] [TUNI: Real-time RGB-T Semantic Segmentation with Unified Multi-Modal Feature Extraction and Cross-Modal Feature Fusion](https://arxiv.org/abs/2509.10005)
*Xiaodong Guo,Tong Liu,Yike Li,Zi'ang Lin,Zhihong Deng*

Main category: cs.CV

TL;DR: TUNI is a novel RGB-thermal semantic segmentation model that unifies feature extraction and cross-modal fusion in a single encoder, achieving competitive performance with fewer parameters and real-time inference speed.


<details>
  <summary>Details</summary>
Motivation: Existing RGB-T models use separate encoders pre-trained on RGB images, leading to limited thermal feature extraction, suboptimal cross-modal fusion, and redundant architecture that compromises real-time efficiency.

Method: Proposes TUNI with a unified RGB-T encoder that simultaneously performs multi-modal feature extraction and fusion, uses large-scale pre-training with RGB and pseudo-thermal data, slims down thermal branch, and introduces an RGB-T local module with adaptive cosine similarity for selective feature emphasis.

Result: Achieves competitive performance with state-of-the-art models on FMB, PST900 and CART datasets, with fewer parameters and lower computational cost. Achieves 27 FPS inference speed on Jetson Orin NX.

Conclusion: TUNI demonstrates effective unified feature extraction and fusion for RGB-T semantic segmentation, offering compact architecture and real-time deployment capability while maintaining competitive performance.

Abstract: RGB-thermal (RGB-T) semantic segmentation improves the environmental
perception of autonomous platforms in challenging conditions. Prevailing models
employ encoders pre-trained on RGB images to extract features from both RGB and
infrared inputs, and design additional modules to achieve cross-modal feature
fusion. This results in limited thermal feature extraction and suboptimal
cross-modal fusion, while the redundant encoders further compromises the
model's real-time efficiency. To address the above issues, we propose TUNI,
with an RGB-T encoder consisting of multiple stacked blocks that simultaneously
perform multi-modal feature extraction and cross-modal fusion. By leveraging
large-scale pre-training with RGB and pseudo-thermal data, the RGB-T encoder
learns to integrate feature extraction and fusion in a unified manner. By
slimming down the thermal branch, the encoder achieves a more compact
architecture. Moreover, we introduce an RGB-T local module to strengthen the
encoder's capacity for cross-modal local feature fusion. The RGB-T local module
employs adaptive cosine similarity to selectively emphasize salient consistent
and distinct local features across RGB-T modalities. Experimental results show
that TUNI achieves competitive performance with state-of-the-art models on FMB,
PST900 and CART, with fewer parameters and lower computational cost. Meanwhile,
it achieves an inference speed of 27 FPS on a Jetson Orin NX, demonstrating its
real-time capability in deployment. Codes are available at
https://github.com/xiaodonguo/TUNI.

</details>


### [29] [Few-Part-Shot Font Generation](https://arxiv.org/abs/2509.10006)
*Masaki Akiba,Shumpei Takezaki,Daichi Haraguchi,Seiichi Uchida*

Main category: cs.CV

TL;DR: Novel few-part-shot font generation model that creates entire fonts using partial design elements instead of complete characters, improving efficiency and providing insights into how partial details influence character structure.


<details>
  <summary>Details</summary>
Motivation: To address the inefficiency of conventional few-shot font generation that requires entire character shapes, and to explore how partial design elements can influence complete character structures.

Method: Proposes a model that uses partial shapes (design elements) as input to generate entire fonts, rather than requiring complete character shapes for few character classes.

Result: The approach improves font creation efficiency and provides analytical insights into how partial design details affect the overall structure of characters.

Conclusion: The few-part-shot font generation model offers a more efficient alternative to traditional methods while providing valuable understanding of design element influences on character composition.

Abstract: This paper proposes a novel model of few-part-shot font generation, which
designs an entire font based on a set of partial design elements, i.e., partial
shapes. Unlike conventional few-shot font generation, which requires entire
character shapes for a couple of character classes, our approach only needs
partial shapes as input. The proposed model not only improves the efficiency of
font creation but also provides insights into how partial design details
influence the entire structure of the individual characters.

</details>


### [30] [Efficient and Accurate Downfacing Visual Inertial Odometry](https://arxiv.org/abs/2509.10021)
*Jonas Kühne,Christian Vogt,Michele Magno,Luca Benini*

Main category: cs.CV

TL;DR: An efficient VIO pipeline optimized for micro/nano-UAVs using quantized feature tracking methods (SuperPoint, PX4FLOW, ORB) on RISC-V SoCs, achieving 3.65x RMSE reduction with ORB and comparable accuracy with lower runtime using PX4FLOW.


<details>
  <summary>Details</summary>
Motivation: Bridge the gap between high-accuracy VIO pipelines requiring powerful systems and lightweight implementations suitable for microcontrollers on resource-constrained micro/nano-UAVs.

Method: Developed an optimized VIO pipeline incorporating state-of-the-art feature detection/tracking methods (SuperPoint, PX4FLOW, ORB) quantized for RISC-V SoCs, using rigid body motion model to reduce estimation errors in planar motion.

Result: Achieved average 3.65x RMSE reduction over baseline using ORB tracker. PX4FLOW achieved on-par accuracy with ORB at lower runtime for speeds below 24 pixels/frame. Successfully implemented on GAP9 low-power SoC.

Conclusion: The pipeline enables high-accuracy VIO on ultra-low-power systems, making it suitable for real-time applications on micro/nano-UAVs while maintaining computational efficiency and tracking accuracy.

Abstract: Visual Inertial Odometry (VIO) is a widely used computer vision method that
determines an agent's movement through a camera and an IMU sensor. This paper
presents an efficient and accurate VIO pipeline optimized for applications on
micro- and nano-UAVs. The proposed design incorporates state-of-the-art feature
detection and tracking methods (SuperPoint, PX4FLOW, ORB), all optimized and
quantized for emerging RISC-V-based ultra-low-power parallel systems on chips
(SoCs). Furthermore, by employing a rigid body motion model, the pipeline
reduces estimation errors and achieves improved accuracy in planar motion
scenarios. The pipeline's suitability for real-time VIO is assessed on an
ultra-low-power SoC in terms of compute requirements and tracking accuracy
after quantization. The pipeline, including the three feature tracking methods,
was implemented on the SoC for real-world validation. This design bridges the
gap between high-accuracy VIO pipelines that are traditionally run on
computationally powerful systems and lightweight implementations suitable for
microcontrollers. The optimized pipeline on the GAP9 low-power SoC demonstrates
an average reduction in RMSE of up to a factor of 3.65x over the baseline
pipeline when using the ORB feature tracker. The analysis of the computational
complexity of the feature trackers further shows that PX4FLOW achieves on-par
tracking accuracy with ORB at a lower runtime for movement speeds below 24
pixels/frame.

</details>


### [31] [Hierarchical MLANet: Multi-level Attention for 3D Face Reconstruction From Single Images](https://arxiv.org/abs/2509.10024)
*Danling Cao*

Main category: cs.CV

TL;DR: MLANet: Hierarchical Multi-Level Attention Network for 3D face reconstruction from single in-the-wild images using CNN and attention mechanisms


<details>
  <summary>Details</summary>
Motivation: Address challenges of 3D face reconstruction from 2D images due to lack of ground-truth labeled datasets and complexity of real-world environments

Method: Uses pre-trained hierarchical backbone network with multi-level attention mechanisms, semi-supervised training with 3DMM parameters and differentiable renderer for end-to-end training

Result: Extensive experiments on AFLW2000-3D and MICC Florence datasets show effectiveness in 3D face reconstruction and alignment tasks, evaluated both quantitatively and qualitatively

Conclusion: Proposed MLANet effectively reconstructs detailed 3D face models from single in-the-wild images using attention mechanisms and semi-supervised approach

Abstract: Recovering 3D face models from 2D in-the-wild images has gained considerable
attention in the computer vision community due to its wide range of potential
applications. However, the lack of ground-truth labeled datasets and the
complexity of real-world environments remain significant challenges. In this
chapter, we propose a convolutional neural network-based approach, the
Hierarchical Multi-Level Attention Network (MLANet), for reconstructing 3D face
models from single in-the-wild images. Our model predicts detailed facial
geometry, texture, pose, and illumination parameters from a single image.
Specifically, we employ a pre-trained hierarchical backbone network and
introduce multi-level attention mechanisms at different stages of 2D face image
feature extraction. A semi-supervised training strategy is employed,
incorporating 3D Morphable Model (3DMM) parameters from publicly available
datasets along with a differentiable renderer, enabling an end-to-end training
process. Extensive experiments, including both comparative and ablation
studies, were conducted on two benchmark datasets, AFLW2000-3D and MICC
Florence, focusing on 3D face reconstruction and 3D face alignment tasks. The
effectiveness of the proposed method was evaluated both quantitatively and
qualitatively.

</details>


### [32] [LaV-CoT: Language-Aware Visual CoT with Multi-Aspect Reward Optimization for Real-World Multilingual VQA](https://arxiv.org/abs/2509.10026)
*Jing Huang,Zhiya Tan,Shutao Gong,Fanwei Zeng,Jianshu Li*

Main category: cs.CV

TL;DR: LaV-CoT is a novel Language-aware Visual Chain-of-Thought framework that enhances multilingual visual question answering through multi-stage reasoning and multi-aspect reward optimization, achieving state-of-the-art performance on multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing approaches rely primarily on textual CoT and provide limited support for multilingual multimodal reasoning, constraining deployment in real-world applications. There's a gap in supporting comprehensive multilingual visual reasoning.

Method: Multi-stage reasoning pipeline (Text Summary with BBox, Language Identification, Spatial Object-level Captioning, Step-by-step Logical Reasoning) with automated data curation and two-stage training (SFT + Language-aware GRPO) guided by multi-aspect rewards.

Result: Achieves up to 9.5% accuracy improvements over open-source baselines, surpasses models with 2x larger scales by 2.6%, and outperforms proprietary models like GPT-4o-0513 and Gemini-2.5-flash. Validated through online A/B testing.

Conclusion: LaV-CoT effectively addresses multilingual visual reasoning limitations and demonstrates strong performance improvements, making it suitable for real-world industrial deployment.

Abstract: As large vision language models (VLMs) advance, their capabilities in
multilingual visual question answering (mVQA) have significantly improved.
Chain-of-thought (CoT) reasoning has been proven to enhance interpretability
and complex reasoning. However, most existing approaches rely primarily on
textual CoT and provide limited support for multilingual multimodal reasoning,
constraining their deployment in real-world applications. To address this gap,
we introduce \textbf{LaV-CoT}, the first Language-aware Visual CoT framework
with Multi-Aspect Reward Optimization. LaV-CoT incorporates an interpretable
multi-stage reasoning pipeline consisting of Text Summary with Bounding Box
(BBox), Language Identification, Spatial Object-level Captioning, and
Step-by-step Logical Reasoning. Following this reasoning pipeline, we design an
automated data curation method that generates multilingual CoT annotations
through iterative generation, correction, and refinement, enabling scalable and
high-quality training data. To improve reasoning and generalization, LaV-CoT
adopts a two-stage training paradigm combining Supervised Fine-Tuning (SFT)
with Language-aware Group Relative Policy Optimization (GRPO), guided by
verifiable multi-aspect rewards including language consistency, structural
accuracy, and semantic alignment. Extensive evaluations on public datasets
including MMMB, Multilingual MMBench, and MTVQA show that LaV-CoT achieves up
to \(\sim\)9.5\% accuracy improvements over open-source baselines of similar
size and even surpasses models with 2$\times$ larger scales by \(\sim\)2.6\%.
Moreover, LaV-CoT outperforms advanced proprietary models such as GPT-4o-0513
and Gemini-2.5-flash. We further conducted an online A/B test to validate our
method on real-world data, highlighting its effectiveness for industrial
deployment. Our code is available at this link:
\href{https://github.com/HJNVR/LaV-CoT}

</details>


### [33] [Color Me Correctly: Bridging Perceptual Color Spaces and Text Embeddings for Improved Diffusion Generation](https://arxiv.org/abs/2509.10058)
*Sung-Lin Tsai,Bo-Lun Huang,Yu Ting Shen,Cheng Yu Yeo,Chiang Tseng,Bo-Kai Ruan,Wen-Sheng Lien,Hong-Han Shuai*

Main category: cs.CV

TL;DR: Training-free framework using LLM to disambiguate color terms and refine text embeddings in CIELAB space for accurate color alignment in text-to-image generation.


<details>
  <summary>Details</summary>
Motivation: Current diffusion models struggle with nuanced color terms like 'Tiffany blue' or 'lime green', producing misaligned colors that don't match human intent, which is critical for applications in fashion, product visualization, and interior design.

Method: Uses large language model to resolve ambiguous color terms, then refines text embeddings based on spatial relationships of color terms in CIELAB color space, enabling precise color blending without additional training or reference images.

Result: Experimental results show improved color alignment without compromising image quality, effectively bridging the gap between text semantics and visual generation.

Conclusion: The proposed training-free framework successfully enhances color fidelity in T2I generation by systematically resolving ambiguous color descriptions through LLM disambiguation and CIELAB space-based embedding refinement.

Abstract: Accurate color alignment in text-to-image (T2I) generation is critical for
applications such as fashion, product visualization, and interior design, yet
current diffusion models struggle with nuanced and compound color terms (e.g.,
Tiffany blue, lime green, hot pink), often producing images that are misaligned
with human intent. Existing approaches rely on cross-attention manipulation,
reference images, or fine-tuning but fail to systematically resolve ambiguous
color descriptions. To precisely render colors under prompt ambiguity, we
propose a training-free framework that enhances color fidelity by leveraging a
large language model (LLM) to disambiguate color-related prompts and guiding
color blending operations directly in the text embedding space. Our method
first employs a large language model (LLM) to resolve ambiguous color terms in
the text prompt, and then refines the text embeddings based on the spatial
relationships of the resulting color terms in the CIELAB color space. Unlike
prior methods, our approach improves color accuracy without requiring
additional training or external reference images. Experimental results
demonstrate that our framework improves color alignment without compromising
image quality, bridging the gap between text semantics and visual generation.

</details>


### [34] [Multimodal Mathematical Reasoning Embedded in Aerial Vehicle Imagery: Benchmarking, Analysis, and Exploration](https://arxiv.org/abs/2509.10059)
*Yue Zhou,Litong Feng,Mengcheng Lan,Xue Yang,Qingyun Li,Yiping Ke,Xue Jiang,Wayne Zhang*

Main category: cs.CV

TL;DR: AVI-Math is the first benchmark for evaluating multimodal mathematical reasoning in UAV imagery, showing current VLMs struggle with domain-specific math tasks despite their general multimodal capabilities.


<details>
  <summary>Details</summary>
Motivation: Current vision-language models lack adequate testing for mathematical reasoning in UAV-based remote sensing applications, which is critical for tasks like distance computations, area calculations, and spatial analysis.

Method: Created AVI-Math benchmark with 3,773 high-quality UAV-captured vehicle-related questions covering 6 mathematical subjects and 20 topics, collected at varying altitudes and angles. Evaluated 14 prominent VLMs and explored Chain-of-Thought prompting and fine-tuning techniques.

Result: Despite success on previous multimodal benchmarks, current VLMs struggle significantly with mathematical reasoning tasks in AVI-Math, revealing substantial limitations in their reasoning capabilities for UAV imagery.

Conclusion: The study exposes VLMs' mathematical reasoning limitations in UAV contexts but shows promise with Chain-of-Thought and fine-tuning approaches, providing valuable insights for developing trustworthy UAV-based VLMs for real-world applications.

Abstract: Mathematical reasoning is critical for tasks such as precise distance and
area computations, trajectory estimations, and spatial analysis in unmanned
aerial vehicle (UAV) based remote sensing, yet current vision-language models
(VLMs) have not been adequately tested in this domain. To address this gap, we
introduce AVI-Math, the first benchmark to rigorously evaluate multimodal
mathematical reasoning in aerial vehicle imagery, moving beyond simple counting
tasks to include domain-specific knowledge in areas such as geometry, logic,
and algebra. The dataset comprises 3,773 high-quality vehicle-related questions
captured from UAV views, covering 6 mathematical subjects and 20 topics. The
data, collected at varying altitudes and from multiple UAV angles, reflects
real-world UAV scenarios, ensuring the diversity and complexity of the
constructed mathematical problems. In this paper, we benchmark 14 prominent
VLMs through a comprehensive evaluation and demonstrate that, despite their
success on previous multimodal benchmarks, these models struggle with the
reasoning tasks in AVI-Math. Our detailed analysis highlights significant
limitations in the mathematical reasoning capabilities of current VLMs and
suggests avenues for future research. Furthermore, we explore the use of
Chain-of-Thought prompting and fine-tuning techniques, which show promise in
addressing the reasoning challenges in AVI-Math. Our findings not only expose
the limitations of VLMs in mathematical reasoning but also offer valuable
insights for advancing UAV-based trustworthy VLMs in real-world applications.
The code, and datasets will be released at
https://github.com/VisionXLab/avi-math

</details>


### [35] [BEVTraj: Map-Free End-to-End Trajectory Prediction in Bird's-Eye View with Deformable Attention and Sparse Goal Proposals](https://arxiv.org/abs/2509.10080)
*Minsang Kong,Myeongjun Kim,Sang Gu Kang,Sang Hun Lee*

Main category: cs.CV

TL;DR: BEVTraj is a novel trajectory prediction framework that operates directly in BEV space using real-time sensor data, eliminating dependency on pre-built HD maps while achieving comparable performance to state-of-the-art map-based models.


<details>
  <summary>Details</summary>
Motivation: Traditional trajectory prediction methods rely on pre-built HD maps (limited to specific regions and cannot adapt to transient changes) or local map construction modules (may fail to capture critical details or introduce errors). These limitations degrade prediction performance and flexibility.

Method: The framework operates directly in bird's-eye view space using real-time sensor data without pre-built maps. It uses deformable attention to efficiently extract context from dense BEV features and introduces a Sparse Goal Candidate Proposal (SGCP) module for end-to-end prediction without post-processing.

Result: Extensive experiments show BEVTraj achieves performance comparable to state-of-the-art HD map-based models while offering greater flexibility by eliminating map dependency.

Conclusion: BEVTraj provides a flexible and effective alternative to map-dependent approaches, enabling accurate trajectory prediction without the constraints of pre-built maps, making it more adaptable to real-world autonomous driving scenarios.

Abstract: In autonomous driving, trajectory prediction is essential for ensuring safe
and efficient navigation. To improve prediction accuracy, recent approaches
often rely on pre-built high-definition (HD) maps or real-time local map
construction modules to incorporate static environmental information. However,
pre-built HD maps are limited to specific regions and cannot adapt to transient
changes. In addition, local map construction modules, which recognize only
predefined elements, may fail to capture critical scene details or introduce
errors that degrade prediction performance. To overcome these limitations, we
propose Bird's-Eye View Trajectory Prediction (BEVTraj), a novel trajectory
prediction framework that operates directly in the bird's-eye view (BEV) space
utilizing real-time sensor data without relying on any pre-built maps. The
BEVTraj leverages deformable attention to efficiently extract relevant context
from dense BEV features. Furthermore, we introduce a Sparse Goal Candidate
Proposal (SGCP) module, which enables full end-to-end prediction without
requiring any post-processing steps. Extensive experiments demonstrate that the
BEVTraj achieves performance comparable to state-of-the-art HD map-based models
while offering greater flexibility by eliminating the dependency on pre-built
maps. The source code is available at https://github.com/Kongminsang/bevtraj.

</details>


### [36] [Leveraging Multi-View Weak Supervision for Occlusion-Aware Multi-Human Parsing](https://arxiv.org/abs/2509.10093)
*Laura Bragagnolo,Matteo Terreran,Leonardo Barcellona,Stefano Ghidoni*

Main category: cs.CV

TL;DR: Novel training framework using multi-view information to improve multi-human parsing performance in occlusion scenarios, achieving 4.20% relative improvement over baseline.


<details>
  <summary>Details</summary>
Motivation: State-of-the-art multi-human parsing models struggle with overlapping/occluded bodies, and the intuition that overlapping people appear separated from different viewpoints can be leveraged to improve performance.

Method: Proposes training framework with multi-view information, weak supervision on human instances, and multi-view consistency loss. Uses semi-automatic annotation strategy to generate instance segmentation masks from multi-view RGB+D data and 3D human skeletons.

Result: Achieves up to 4.20% relative improvement on human parsing over baseline model in occlusion scenarios.

Conclusion: Multi-view information integration through novel training framework and consistency loss effectively improves multi-human parsing performance under occlusions.

Abstract: Multi-human parsing is the task of segmenting human body parts while
associating each part to the person it belongs to, combining instance-level and
part-level information for fine-grained human understanding. In this work, we
demonstrate that, while state-of-the-art approaches achieved notable results on
public datasets, they struggle considerably in segmenting people with
overlapping bodies. From the intuition that overlapping people may appear
separated from a different point of view, we propose a novel training framework
exploiting multi-view information to improve multi-human parsing models under
occlusions. Our method integrates such knowledge during the training process,
introducing a novel approach based on weak supervision on human instances and a
multi-view consistency loss. Given the lack of suitable datasets in the
literature, we propose a semi-automatic annotation strategy to generate human
instance segmentation masks from multi-view RGB+D data and 3D human skeletons.
The experiments demonstrate that the approach can achieve up to a 4.20\%
relative improvement on human parsing over the baseline model in occlusion
scenarios.

</details>


### [37] [VARCO-VISION-2.0 Technical Report](https://arxiv.org/abs/2509.10105)
*Young-rok Cha,Jeongho Ju,SunYoung Park,Jong-Hyeon Lee,Younghyun Yu,Youngjune Kim*

Main category: cs.CV

TL;DR: VARCO-VISION-2.0 is an open-weight bilingual vision-language model for Korean and English with improved multi-image understanding, layout-aware OCR, and enhanced safety features.


<details>
  <summary>Details</summary>
Motivation: To advance bilingual vision-language models for Korean and English with improved capabilities over previous models, supporting complex multimodal inputs and practical applications.

Method: Four-stage curriculum training with memory-efficient techniques, supporting multi-image understanding for documents/charts/tables, layout-aware OCR with spatial prediction, and preference optimization for safety.

Result: Achieves strong spatial grounding and competitive bilingual performance, with the 14B model ranking 8th on OpenCompass VLM leaderboard among comparable-scale models. Also releases a 1.7B version for on-device deployment.

Conclusion: VARCO-VISION-2.0 advances bilingual VLM development with practical applications, offering both full-scale and lightweight variants to support diverse deployment needs.

Abstract: We introduce VARCO-VISION-2.0, an open-weight bilingual vision-language model
(VLM) for Korean and English with improved capabilities compared to the
previous model VARCO-VISION-14B. The model supports multi-image understanding
for complex inputs such as documents, charts, and tables, and delivers
layoutaware OCR by predicting both textual content and its spatial location.
Trained with a four-stage curriculum with memory-efficient techniques, the
model achieves enhanced multimodal alignment, while preserving core language
abilities and improving safety via preference optimization. Extensive benchmark
evaluations demonstrate strong spatial grounding and competitive results for
both languages, with the 14B model achieving 8th place on the OpenCompass VLM
leaderboard among models of comparable scale. Alongside the 14B-scale model, we
release a 1.7B version optimized for on-device deployment. We believe these
models advance the development of bilingual VLMs and their practical
applications. Two variants of VARCO-VISION-2.0 are available at Hugging Face: a
full-scale 14B model and a lightweight 1.7B model.

</details>


### [38] [A Lightweight Ensemble-Based Face Image Quality Assessment Method with Correlation-Aware Loss](https://arxiv.org/abs/2509.10114)
*MohammadAli Hamidi,Hadi Amirpour,Luigi Atzori,Christian Timmerer*

Main category: cs.CV

TL;DR: A lightweight face image quality assessment method using ensemble of MobileNetV3-Small and ShuffleNetV2 with correlation-aware loss, achieving high accuracy with low computational cost.


<details>
  <summary>Details</summary>
Motivation: Existing FIQA methods are either not face-specific or computationally intensive, limiting practical deployment in real-world face recognition systems.

Method: Ensemble of two compact CNNs (MobileNetV3-Small and ShuffleNetV2) with prediction-level fusion via averaging, using MSECorrLoss that combines MSE with Pearson correlation regularizer.

Result: Achieves SRCC of 0.9829 and PLCC of 0.9894 on VQualA benchmark while meeting competition efficiency constraints.

Conclusion: Proposed method provides an effective balance between accuracy and computational efficiency, making it suitable for real-world face recognition applications.

Abstract: Face image quality assessment (FIQA) plays a critical role in face
recognition and verification systems, especially in uncontrolled, real-world
environments. Although several methods have been proposed, general-purpose
no-reference image quality assessment techniques often fail to capture
face-specific degradations. Meanwhile, state-of-the-art FIQA models tend to be
computationally intensive, limiting their practical applicability. We propose a
lightweight and efficient method for FIQA, designed for the perceptual
evaluation of face images in the wild. Our approach integrates an ensemble of
two compact convolutional neural networks, MobileNetV3-Small and ShuffleNetV2,
with prediction-level fusion via simple averaging. To enhance alignment with
human perceptual judgments, we employ a correlation-aware loss (MSECorrLoss),
combining mean squared error (MSE) with a Pearson correlation regularizer. Our
method achieves a strong balance between accuracy and computational cost,
making it suitable for real-world deployment. Experiments on the VQualA FIQA
benchmark demonstrate that our model achieves a Spearman rank correlation
coefficient (SRCC) of 0.9829 and a Pearson linear correlation coefficient
(PLCC) of 0.9894, remaining within competition efficiency constraints.

</details>


### [39] [Realism Control One-step Diffusion for Real-World Image Super-Resolution](https://arxiv.org/abs/2509.10122)
*Zongliang Wu,Siming Zheng,Peng-Tao Jiang,Xin Yuan*

Main category: cs.CV

TL;DR: RCOD is a one-step diffusion framework for real-world image super-resolution that provides explicit control over fidelity-realism trade-offs through latent domain grouping and degradation-aware sampling.


<details>
  <summary>Details</summary>
Motivation: One-step diffusion methods for super-resolution lack flexible control mechanisms to balance fidelity and realism across diverse scenarios, unlike multi-step methods that can adjust sampling steps.

Method: Proposes Realism Controlled One-step Diffusion (RCOD) with latent domain grouping strategy, degradation-aware sampling, and visual prompt injection module to replace text prompts with degradation-aware visual tokens.

Result: Achieves superior fidelity and perceptual quality while maintaining computational efficiency, outperforming state-of-the-art OSD methods in both quantitative metrics and visual qualities.

Conclusion: RCOD enables flexible realism control during inference while maintaining the efficiency of one-step diffusion, providing a practical solution for real-world image super-resolution tasks.

Abstract: Pre-trained diffusion models have shown great potential in real-world image
super-resolution (Real-ISR) tasks by enabling high-resolution reconstructions.
While one-step diffusion (OSD) methods significantly improve efficiency
compared to traditional multi-step approaches, they still have limitations in
balancing fidelity and realism across diverse scenarios. Since the OSDs for SR
are usually trained or distilled by a single timestep, they lack flexible
control mechanisms to adaptively prioritize these competing objectives, which
are inherently manageable in multi-step methods through adjusting sampling
steps. To address this challenge, we propose a Realism Controlled One-step
Diffusion (RCOD) framework for Real-ISR. RCOD provides a latent domain grouping
strategy that enables explicit control over fidelity-realism trade-offs during
the noise prediction phase with minimal training paradigm modifications and
original training data. A degradation-aware sampling strategy is also
introduced to align distillation regularization with the grouping strategy and
enhance the controlling of trade-offs. Moreover, a visual prompt injection
module is used to replace conventional text prompts with degradation-aware
visual tokens, enhancing both restoration accuracy and semantic consistency.
Our method achieves superior fidelity and perceptual quality while maintaining
computational efficiency. Extensive experiments demonstrate that RCOD
outperforms state-of-the-art OSD methods in both quantitative metrics and
visual qualities, with flexible realism control capabilities in the inference
stage. The code will be released.

</details>


### [40] [Grad-CL: Source Free Domain Adaptation with Gradient Guided Feature Disalignment](https://arxiv.org/abs/2509.10134)
*Rini Smita Thakur,Rajeev Ranjan Dwivedi,Vinod K Kurmi*

Main category: cs.CV

TL;DR: Grad-CL is a source-free domain adaptation framework for optic disc and cup segmentation that uses gradient-guided pseudolabel refinement and cosine similarity contrastive learning to improve cross-domain performance without accessing source data.


<details>
  <summary>Details</summary>
Motivation: Segmentation models trained on one dataset perform poorly on target data from different imaging protocols/conditions, creating a need for domain adaptation without source data access.

Method: Two-stage approach: 1) Gradient-based mechanism extracts class-specific features for uncertainty quantification and prototype estimation to refine noisy pseudolabels; 2) Cosine similarity contrastive learning enforces inter-class separability between optic cup and disc features.

Result: Outperforms state-of-the-art unsupervised and source-free domain adaptation methods on challenging cross-domain fundus imaging datasets with superior segmentation accuracy and improved boundary delineation.

Conclusion: Grad-CL provides an effective source-free domain adaptation solution for medical image segmentation that maintains performance across different imaging conditions without requiring access to original training data.

Abstract: Accurate segmentation of the optic disc and cup is critical for the early
diagnosis and management of ocular diseases such as glaucoma. However,
segmentation models trained on one dataset often suffer significant performance
degradation when applied to target data acquired under different imaging
protocols or conditions. To address this challenge, we propose
\textbf{Grad-CL}, a novel source-free domain adaptation framework that
leverages a pre-trained source model and unlabeled target data to robustly
adapt segmentation performance without requiring access to the original source
data. Grad-CL combines a gradient-guided pseudolabel refinement module with a
cosine similarity-based contrastive learning strategy. In the first stage,
salient class-specific features are extracted via a gradient-based mechanism,
enabling more accurate uncertainty quantification and robust prototype
estimation for refining noisy pseudolabels. In the second stage, a contrastive
loss based on cosine similarity is employed to explicitly enforce inter-class
separability between the gradient-informed features of the optic cup and disc.
Extensive experiments on challenging cross-domain fundus imaging datasets
demonstrate that Grad-CL outperforms state-of-the-art unsupervised and
source-free domain adaptation methods, achieving superior segmentation accuracy
and improved boundary delineation. Project and code are available at
https://visdomlab.github.io/GCL/.

</details>


### [41] [Scalable Training for Vector-Quantized Networks with 100% Codebook Utilization](https://arxiv.org/abs/2509.10140)
*Yifan Chang,Jie Qin,Limeng Qiao,Xiaofeng Wang,Zheng Zhu,Lin Ma,Xingang Wang*

Main category: cs.CV

TL;DR: VQBridge enables 100% codebook usage in vector quantization networks through stable training, achieving state-of-the-art reconstruction and improving image generation performance.


<details>
  <summary>Details</summary>
Motivation: Vector quantization training suffers from instability issues like straight-through estimation bias, sparse gradients, and suboptimal codebook usage, leading to poor reconstruction performance.

Method: Proposes VQBridge - a robust projector using map function method with compress-process-recover pipeline, combined with learning annealing to optimize code vectors.

Result: Achieves 100% codebook usage even with 262k-codebook, state-of-the-art reconstruction, and improves image generation (surpasses VAR by 0.5 and DiT by 0.2 rFID).

Conclusion: High-quality tokenizers are crucial for autoregressive image generation, and FVQ provides an effective, scalable solution for stable VQ training with full codebook utilization.

Abstract: Vector quantization (VQ) is a key component in discrete tokenizers for image
generation, but its training is often unstable due to straight-through
estimation bias, one-step-behind updates, and sparse codebook gradients, which
lead to suboptimal reconstruction performance and low codebook usage. In this
work, we analyze these fundamental challenges and provide a simple yet
effective solution. To maintain high codebook usage in VQ networks (VQN) during
learning annealing and codebook size expansion, we propose VQBridge, a robust,
scalable, and efficient projector based on the map function method. VQBridge
optimizes code vectors through a compress-process-recover pipeline, enabling
stable and effective codebook training. By combining VQBridge with learning
annealing, our VQN achieves full (100%) codebook usage across diverse codebook
configurations, which we refer to as FVQ (FullVQ). Through extensive
experiments, we demonstrate that FVQ is effective, scalable, and generalizable:
it attains 100% codebook usage even with a 262k-codebook, achieves
state-of-the-art reconstruction performance, consistently improves with larger
codebooks, higher vector channels, or longer training, and remains effective
across different VQ variants. Moreover, when integrated with LlamaGen, FVQ
significantly enhances image generation performance, surpassing visual
autoregressive models (VAR) by 0.5 and diffusion models (DiT) by 0.2 rFID,
highlighting the importance of high-quality tokenizers for strong
autoregressive image generation.

</details>


### [42] [LayerLock: Non-collapsing Representation Learning with Progressive Freezing](https://arxiv.org/abs/2509.10156)
*Goker Erdogan,Nikhil Parthasarathy,Catalin Ionescu,Drew Hudson,Alexander Lerchner,Andrew Zisserman,Mehdi Sajjadi,Joao Carreira*

Main category: cs.CV

TL;DR: LayerLock accelerates masked-autoencoding training by progressively freezing ViT layers based on their convergence order, enabling efficient latent prediction without representation collapse.


<details>
  <summary>Details</summary>
Motivation: The paper aims to accelerate self-supervised visual representation learning by exploiting the observation that ViT layers converge in depth order during training, allowing for progressive freezing to improve efficiency.

Method: Proposes LayerLock approach that progressively freezes ViT layers according to an explicit schedule based on their convergence order (shallow layers first, deep layers later), enabling latent prediction without representation collapse issues.

Result: Applied to large models up to 4B parameters, LayerLock surpasses non-latent masked prediction performance on the 4DS perception suite benchmark.

Conclusion: Progressive layer freezing based on natural convergence patterns provides an effective and scalable approach for accelerating masked-autoencoding training while maintaining or improving representation quality.

Abstract: We introduce LayerLock, a simple yet effective approach for self-supervised
visual representation learning, that gradually transitions from pixel to latent
prediction through progressive layer freezing. First, we make the observation
that during training of video masked-autoencoding (MAE) models, ViT layers
converge in the order of their depth: shallower layers converge early, deeper
layers converge late. We then show that this observation can be exploited to
accelerate standard MAE by progressively freezing the model according to an
explicit schedule, throughout training. Furthermore, this same schedule can be
used in a simple and scalable approach to latent prediction that does not
suffer from "representation collapse". We apply our proposed approach,
LayerLock, to large models of up to 4B parameters with results surpassing those
of non-latent masked prediction on the 4DS perception suite.

</details>


### [43] [On the Geometric Accuracy of Implicit and Primitive-based Representations Derived from View Rendering Constraints](https://arxiv.org/abs/2509.10241)
*Elias De Smijter,Renaud Detry,Christophe De Vleeschouwer*

Main category: cs.CV

TL;DR: Appearance embeddings improve photometric fidelity but not geometric accuracy in space-based 3D reconstruction. Convex splatting provides more compact representations than Gaussian splatting for safety-critical applications.


<details>
  <summary>Details</summary>
Motivation: To systematically compare implicit and explicit Novel View Synthesis methods for space-based 3D object reconstruction and evaluate the role of appearance embeddings in geometric accuracy for space robotics applications.

Method: Used the SPEED+ dataset to compare K-Planes, Gaussian Splatting, and Convex Splatting methods, analyzing how appearance embeddings affect photometric fidelity versus geometric accuracy.

Result: Appearance embeddings primarily reduce the number of primitives needed for explicit methods rather than enhancing geometric fidelity. Convex splatting achieves more compact and clutter-free representations than Gaussian splatting.

Conclusion: Appearance embeddings have limited benefits for geometry-centric tasks in space scenarios. Convex splatting offers advantages for safety-critical applications requiring efficient and clean representations for interaction and collision avoidance.

Abstract: We present the first systematic comparison of implicit and explicit Novel
View Synthesis methods for space-based 3D object reconstruction, evaluating the
role of appearance embeddings. While embeddings improve photometric fidelity by
modeling lighting variation, we show they do not translate into meaningful
gains in geometric accuracy - a critical requirement for space robotics
applications. Using the SPEED+ dataset, we compare K-Planes, Gaussian
Splatting, and Convex Splatting, and demonstrate that embeddings primarily
reduce the number of primitives needed for explicit methods rather than
enhancing geometric fidelity. Moreover, convex splatting achieves more compact
and clutter-free representations than Gaussian splatting, offering advantages
for safety-critical applications such as interaction and collision avoidance.
Our findings clarify the limits of appearance embeddings for geometry-centric
tasks and highlight trade-offs between reconstruction quality and
representation efficiency in space scenarios.

</details>


### [44] [GAMMA: Generalizable Alignment via Multi-task and Manipulation-Augmented Training for AI-Generated Image Detection](https://arxiv.org/abs/2509.10250)
*Haozhen Yan,Yan Hong,Suning Lang,Jiahui Zhan,Yikun Ji,Yujie Gao,Jun Lan,Huijia Zhu,Weiqiang Wang,Jianfu Zhang*

Main category: cs.CV

TL;DR: GAMMA is a novel training framework for detecting AI-generated images that addresses generalization limitations by reducing domain bias and enhancing semantic alignment through diverse manipulation strategies and multi-task supervision.


<details>
  <summary>Details</summary>
Motivation: Existing AI-generated image detectors perform well on in-distribution images but struggle with generalization to unseen generative models due to reliance on generation-specific artifacts like stylistic priors and compression patterns.

Method: Proposes GAMMA framework with diverse manipulation strategies (inpainting-based manipulation, semantics-preserving perturbations), multi-task supervision with dual segmentation heads and classification head, and reverse cross-attention mechanism for segmentation heads to guide classification.

Result: Achieves state-of-the-art generalization performance on GenImage benchmark with 5.8% accuracy improvement and maintains strong robustness on newly released generative models like GPT-4o.

Conclusion: GAMMA effectively addresses generalization limitations in AI-generated image detection by reducing domain bias and enhancing semantic alignment, demonstrating superior performance across diverse generative domains.

Abstract: With generative models becoming increasingly sophisticated and diverse,
detecting AI-generated images has become increasingly challenging. While
existing AI-genereted Image detectors achieve promising performance on
in-distribution generated images, their generalization to unseen generative
models remains limited. This limitation is largely attributed to their reliance
on generation-specific artifacts, such as stylistic priors and compression
patterns. To address these limitations, we propose GAMMA, a novel training
framework designed to reduce domain bias and enhance semantic alignment. GAMMA
introduces diverse manipulation strategies, such as inpainting-based
manipulation and semantics-preserving perturbations, to ensure consistency
between manipulated and authentic content. We employ multi-task supervision
with dual segmentation heads and a classification head, enabling pixel-level
source attribution across diverse generative domains. In addition, a reverse
cross-attention mechanism is introduced to allow the segmentation heads to
guide and correct biased representations in the classification branch. Our
method achieves state-of-the-art generalization performance on the GenImage
benchmark, imporving accuracy by 5.8%, but also maintains strong robustness on
newly released generative model such as GPT-4o.

</details>


### [45] [Robustness and Diagnostic Performance of Super-Resolution Fetal Brain MRI](https://arxiv.org/abs/2509.10257)
*Ema Masterl,Tina Vipotnik Vesnaver,Žiga Špiclin*

Main category: cs.CV

TL;DR: Comparison of three fetal brain MRI super-resolution reconstruction methods shows NeSVoR has highest success rate (>90%) across healthy and pathological cases, with diagnostic classification unaffected despite volumetric differences between methods.


<details>
  <summary>Details</summary>
Motivation: Fetal brain MRI suffers from low resolution, motion artifacts, and inadequate 3D anatomy capture. Current SRR methods' comparative performance in pathological cases and their impact on downstream analysis remain underexplored.

Method: Applied three SRR methods (NiftyMIC, SVRTK, NeSVoR) to 140 fetal brain MRI scans including healthy controls and ventriculomegaly cases. Reconstructed volumes were segmented using BoUNTi algorithm to extract volumes of nine brain structures.

Result: NeSVoR demonstrated highest reconstruction success rate (>90%) across both groups. Significant volumetric differences were observed between methods, but VM classification performance was not affected by SRR method choice.

Conclusion: NeSVoR shows robustness in reconstruction, and diagnostic performance remains resilient despite SRR-induced volumetric variability, making it a reliable choice for clinical applications.

Abstract: Fetal brain MRI relies on rapid multi-view 2D slice acquisitions to reduce
motion artifacts caused by fetal movement. However, these stacks are typically
low resolution, may suffer from motion corruption, and do not adequately
capture 3D anatomy. Super-resolution reconstruction (SRR) methods aim to
address these limitations by combining slice-to-volume registration and
super-resolution techniques to generate high-resolution (HR) 3D volumes. While
several SRR methods have been proposed, their comparative performance -
particularly in pathological cases - and their influence on downstream
volumetric analysis and diagnostic tasks remain underexplored. In this study,
we applied three state-of-the-art SRR method - NiftyMIC, SVRTK, and NeSVoR - to
140 fetal brain MRI scans, including both healthy controls (HC) and
pathological cases (PC) with ventriculomegaly (VM). Each HR reconstruction was
segmented using the BoUNTi algorithm to extract volumes of nine principal brain
structures. We evaluated visual quality, SRR success rates, volumetric
measurement agreement, and diagnostic classification performance. NeSVoR
demonstrated the highest and most consistent reconstruction success rate (>90%)
across both HC and PC groups. Although significant differences in volumetric
estimates were observed between SRR methods, classification performance for VM
was not affected by the choice of SRR method. These findings highlight NeSVoR's
robustness and the resilience of diagnostic performance despite SRR-induced
volumetric variability.

</details>


### [46] [Mask Consistency Regularization in Object Removal](https://arxiv.org/abs/2509.10259)
*Hua Yuan,Jin Yuan,Yicheng Jiang,Yao Zhang,Xin Geng,Yong Rui*

Main category: cs.CV

TL;DR: Proposes Mask Consistency Regularization (MCR) to address mask hallucination and mask-shape bias in object removal tasks using diffusion models, through dilation and reshape mask perturbations during training.


<details>
  <summary>Details</summary>
Motivation: Current diffusion models for object removal suffer from mask hallucination (generating irrelevant content) and mask-shape bias (filling mask area with object mimicking mask shape rather than surrounding context).

Method: Introduces Mask Consistency Regularization (MCR) with two mask perturbations: dilation to align output with surrounding content, and reshape to break mask-shape bias, enforcing consistency between perturbed and original mask outputs.

Result: MCR significantly reduces hallucinations and mask-shape bias, leading to improved performance in object removal tasks with more robust and contextually coherent inpainting results.

Conclusion: The proposed MCR training strategy effectively addresses key challenges in object removal by enforcing mask consistency through strategic perturbations, resulting in superior inpainting quality and reduced artifacts.

Abstract: Object removal, a challenging task within image inpainting, involves
seamlessly filling the removed region with content that matches the surrounding
context. Despite advancements in diffusion models, current methods still face
two critical challenges. The first is mask hallucination, where the model
generates irrelevant or spurious content inside the masked region, and the
second is mask-shape bias, where the model fills the masked area with an object
that mimics the mask's shape rather than surrounding content. To address these
issues, we propose Mask Consistency Regularization (MCR), a novel training
strategy designed specifically for object removal tasks. During training, our
approach introduces two mask perturbations: dilation and reshape, enforcing
consistency between the outputs of these perturbed branches and the original
mask. The dilated masks help align the model's output with the surrounding
content, while reshaped masks encourage the model to break the mask-shape bias.
This combination of strategies enables MCR to produce more robust and
contextually coherent inpainting results. Our experiments demonstrate that MCR
significantly reduces hallucinations and mask-shape bias, leading to improved
performance in object removal.

</details>


### [47] [MagicMirror: A Large-Scale Dataset and Benchmark for Fine-Grained Artifacts Assessment in Text-to-Image Generation](https://arxiv.org/abs/2509.10260)
*Jia Wang,Jie Hu,Xiaoqi Ma,Hanghang Ma,Yanbing Zeng,Xiaoming Wei*

Main category: cs.CV

TL;DR: MagicMirror introduces a comprehensive framework for evaluating physical artifacts in text-to-image generation, including a detailed artifact taxonomy, large-scale annotated dataset, VLM-based assessor, and automated benchmark that reveals significant artifacts persist in top T2I models.


<details>
  <summary>Details</summary>
Motivation: Current T2I models suffer from physical artifacts like anatomical and structural flaws that degrade perceptual quality, but lack systematic evaluation frameworks to address these issues.

Method: Created detailed artifact taxonomy, manually annotated 340K image dataset (MagicData340K), trained Vision-Language Model (MagicAssessor) with novel data sampling and multi-level reward system using GRPO, and built automated benchmark (MagicBench).

Result: Evaluation shows even top-tier T2I models like GPT-image-1 consistently suffer from significant artifacts, demonstrating the persistent challenge of artifact reduction.

Conclusion: Artifact reduction remains a critical frontier for T2I development, and MagicMirror provides the necessary comprehensive framework for systematic evaluation and improvement.

Abstract: Text-to-image (T2I) generation has achieved remarkable progress in
instruction following and aesthetics. However, a persistent challenge is the
prevalence of physical artifacts, such as anatomical and structural flaws,
which severely degrade perceptual quality and limit application. Given the
diversity and complexity of these artifacts, a systematic and fine-grained
evaluation framework is required, which is lacking in current benchmarks. To
fill this gap, we introduce MagicMirror, a comprehensive framework for
artifacts assessment. We first establish a detailed taxonomy of generated image
artifacts. Guided by this taxonomy, we manually annotate MagicData340K, the
first human-annotated large-scale dataset of 340K generated images with
fine-grained artifact labels. Building on this dataset, we train MagicAssessor,
a Vision-Language Model (VLM) that provides detailed assessments and
corresponding labels. To overcome challenges like class imbalance and reward
hacking, we design a novel data sampling strategy and a multi-level reward
system for Group Relative Policy Optimization (GRPO). Finally, we leverage
MagicAssessor to construct MagicBench, an automated benchmark for evaluating
the image artifacts of current T2I models. Our evaluation with MagicBench
reveals that despite their widespread adoption, even top-tier models like
GPT-image-1 are consistently plagued by significant artifacts, highlighting
artifact reduction as a critical frontier for future T2I development. Project
page: https://wj-inf.github.io/MagicMirror-page/.

</details>


### [48] [SignClip: Leveraging Mouthing Cues for Sign Language Translation by Multimodal Contrastive Fusion](https://arxiv.org/abs/2509.10266)
*Wenfang Wu,Tingting Yuan,Yupeng Li,Daling Wang,Xiaoming Fu*

Main category: cs.CV

TL;DR: SignClip improves sign language translation by fusing manual (hand gestures) and non-manual (lip movements) cues with hierarchical contrastive learning, achieving state-of-the-art results on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Most existing sign language translation approaches focus only on manual signals and overlook non-manual cues like mouthing, which conveys essential linguistic information and helps disambiguate visually similar signs.

Method: Proposes SignClip framework that fuses spatial gesture and lip movement features, and introduces hierarchical contrastive learning with multi-level alignment objectives for semantic consistency across sign-lip and visual-text modalities.

Result: Extensive experiments on PHOENIX14T and How2Sign datasets show superiority. On PHOENIX14T gloss-free setting, SignClip improves BLEU-4 from 24.32 to 24.71 and ROUGE from 46.57 to 48.38 compared to previous state-of-the-art SpaMo.

Conclusion: The integration of both manual and non-manual cues with hierarchical contrastive learning significantly enhances sign language translation accuracy, demonstrating the importance of comprehensive feature fusion for better SLT performance.

Abstract: Sign language translation (SLT) aims to translate natural language from sign
language videos, serving as a vital bridge for inclusive communication. While
recent advances leverage powerful visual backbones and large language models,
most approaches mainly focus on manual signals (hand gestures) and tend to
overlook non-manual cues like mouthing. In fact, mouthing conveys essential
linguistic information in sign languages and plays a crucial role in
disambiguating visually similar signs. In this paper, we propose SignClip, a
novel framework to improve the accuracy of sign language translation. It fuses
manual and non-manual cues, specifically spatial gesture and lip movement
features. Besides, SignClip introduces a hierarchical contrastive learning
framework with multi-level alignment objectives, ensuring semantic consistency
across sign-lip and visual-text modalities. Extensive experiments on two
benchmark datasets, PHOENIX14T and How2Sign, demonstrate the superiority of our
approach. For example, on PHOENIX14T, in the Gloss-free setting, SignClip
surpasses the previous state-of-the-art model SpaMo, improving BLEU-4 from
24.32 to 24.71, and ROUGE from 46.57 to 48.38.

</details>


### [49] [Detecting Text Manipulation in Images using Vision Language Models](https://arxiv.org/abs/2509.10278)
*Vidit Vidit,Pavel Korshunov,Amir Mohammadi,Christophe Ecabert,Ketan Kotwal,Sébastien Marcel*

Main category: cs.CV

TL;DR: Analysis of VLMs for text manipulation detection, showing open-source models lag behind closed-source ones like GPT-4o, and revealing generalization issues in image manipulation-specific VLMs when applied to text.


<details>
  <summary>Details</summary>
Motivation: To bridge the knowledge gap in text manipulation detection using VLMs, as previous studies focused mainly on image manipulation detection.

Method: Benchmarking closed- and open-source VLMs on different text manipulation datasets, including in-the-wild scene texts and fantasy ID cards that mimic real-world misuse scenarios.

Result: Open-source models are improving but still behind closed-source models like GPT-4o. Image manipulation-specific VLMs suffer from generalization problems when applied to text manipulation detection.

Conclusion: There is a significant performance gap between open-source and closed-source VLMs for text manipulation detection, and specialized image manipulation models don't generalize well to text manipulation tasks.

Abstract: Recent works have shown the effectiveness of Large Vision Language Models
(VLMs or LVLMs) in image manipulation detection. However, text manipulation
detection is largely missing in these studies. We bridge this knowledge gap by
analyzing closed- and open-source VLMs on different text manipulation datasets.
Our results suggest that open-source models are getting closer, but still
behind closed-source ones like GPT- 4o. Additionally, we benchmark image
manipulation detection-specific VLMs for text manipulation detection and show
that they suffer from the generalization problem. We benchmark VLMs for
manipulations done on in-the-wild scene texts and on fantasy ID cards, where
the latter mimic a challenging real-world misuse.

</details>


### [50] [MCL-AD: Multimodal Collaboration Learning for Zero-Shot 3D Anomaly Detection](https://arxiv.org/abs/2509.10282)
*Gang Li,Tianjiao Chen,Mingle Zhou,Min Li,Delong Han,Jin Wan*

Main category: cs.CV

TL;DR: MCL-AD is a novel zero-shot 3D anomaly detection framework that leverages multimodal collaboration across point clouds, RGB images, and text semantics, achieving state-of-the-art performance without labeled training data.


<details>
  <summary>Details</summary>
Motivation: Existing methods focus only on point clouds, neglecting rich semantic cues from complementary modalities like RGB images and text priors, which could enhance zero-shot 3D anomaly detection in data-scarce scenarios.

Method: Proposes Multimodal Prompt Learning Mechanism (MPLM) with object-agnostic decoupled text prompts and multimodal contrastive loss, plus Collaborative Modulation Mechanism (CMM) to jointly modulate RGB image-guided and point cloud-guided branches.

Result: Extensive experiments demonstrate state-of-the-art performance in zero-shot 3D anomaly detection.

Conclusion: MCL-AD framework successfully leverages multimodal collaboration learning to achieve superior zero-shot 3D anomaly detection by integrating point clouds, RGB images, and text semantics.

Abstract: Zero-shot 3D (ZS-3D) anomaly detection aims to identify defects in 3D objects
without relying on labeled training data, making it especially valuable in
scenarios constrained by data scarcity, privacy, or high annotation cost.
However, most existing methods focus exclusively on point clouds, neglecting
the rich semantic cues available from complementary modalities such as RGB
images and texts priors. This paper introduces MCL-AD, a novel framework that
leverages multimodal collaboration learning across point clouds, RGB images,
and texts semantics to achieve superior zero-shot 3D anomaly detection.
Specifically, we propose a Multimodal Prompt Learning Mechanism (MPLM) that
enhances the intra-modal representation capability and inter-modal
collaborative learning by introducing an object-agnostic decoupled text prompt
and a multimodal contrastive loss. In addition, a collaborative modulation
mechanism (CMM) is proposed to fully leverage the complementary representations
of point clouds and RGB images by jointly modulating the RGB image-guided and
point cloud-guided branches. Extensive experiments demonstrate that the
proposed MCL-AD framework achieves state-of-the-art performance in ZS-3D
anomaly detection.

</details>


### [51] [Adversarial robustness through Lipschitz-Guided Stochastic Depth in Neural Networks](https://arxiv.org/abs/2509.10298)
*Laith Nayal,Mahmoud Mousatat,Bader Rasheed*

Main category: cs.CV

TL;DR: Lipschitz-guided stochastic depth method with depth-dependent drop probabilities improves robustness while maintaining accuracy and reducing computation in Vision Transformers.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks and Vision Transformers achieve state-of-the-art performance but are highly vulnerable to adversarial attacks, and standard defenses often have high computational costs or lack formal guarantees.

Method: Propose a Lipschitz-guided stochastic depth (DropPath) method where drop probabilities increase with depth to control the effective Lipschitz constant of the network, regularizing deeper layers.

Result: Experiments on CIFAR-10 with ViT-Tiny show maintained near-baseline clean accuracy, enhanced robustness under FGSM, PGD-20, and AutoAttack attacks, and significant FLOPs reduction compared to baseline and linear DropPath schedules.

Conclusion: The depth-dependent DropPath schedule effectively improves adversarial robustness while preserving accuracy and reducing computational cost in Vision Transformers.

Abstract: Deep neural networks and Vision Transformers achieve state-of-the-art
performance in computer vision but are highly vulnerable to adversarial
perturbations. Standard defenses often incur high computational cost or lack
formal guarantees. We propose a Lipschitz-guided stochastic depth (DropPath)
method, where drop probabilities increase with depth to control the effective
Lipschitz constant of the network. This approach regularizes deeper layers,
improving robustness while preserving clean accuracy and reducing computation.
Experiments on CIFAR-10 with ViT-Tiny show that our custom depth-dependent
schedule maintains near-baseline clean accuracy, enhances robustness under
FGSM, PGD-20, and AutoAttack, and significantly reduces FLOPs compared to
baseline and linear DropPath schedules.

</details>


### [52] [A Stochastic Birth-and-Death Approach for Street Furniture Geolocation in Urban Environments](https://arxiv.org/abs/2509.10310)
*Evan Murphy,Marco Viola,Vladimir A. Krylov*

Main category: cs.CV

TL;DR: Probabilistic framework using energy maps for precise geolocation of street furniture in urban environments, integrating GIS data with stochastic optimization.


<details>
  <summary>Details</summary>
Motivation: Address the critical need for precise geolocation of street furniture to enable effective monitoring and maintenance of public infrastructure by authorities and private stakeholders.

Method: Propose a probabilistic framework based on energy maps that encode spatial likelihood of object locations, using map-based geopositioned format to integrate external geospatial information like GIS layers and road maps. Introduce stochastic birth-and-death optimization algorithm to infer most probable asset configurations.

Result: Evaluated using realistic simulation based on geolocated dataset of street lighting infrastructure in Dublin city center, demonstrating potential for scalable and accurate urban asset mapping.

Conclusion: The framework shows promise for improving contextual awareness and localization accuracy in complex urban environments, with algorithm implementation made publicly available.

Abstract: In this paper we address the problem of precise geolocation of street
furniture in complex urban environments, which is a critical task for effective
monitoring and maintenance of public infrastructure by local authorities and
private stakeholders. To this end, we propose a probabilistic framework based
on energy maps that encode the spatial likelihood of object locations.
Representing the energy in a map-based geopositioned format allows the
optimisation process to seamlessly integrate external geospatial information,
such as GIS layers, road maps, or placement constraints, which improves
contextual awareness and localisation accuracy. A stochastic birth-and-death
optimisation algorithm is introduced to infer the most probable configuration
of assets. We evaluate our approach using a realistic simulation informed by a
geolocated dataset of street lighting infrastructure in Dublin city centre,
demonstrating its potential for scalable and accurate urban asset mapping. The
implementation of the algorithm will be made available in the GitHub repository
https://github.com/EMurphy0108/SBD_Street_Furniture.

</details>


### [53] [Compute Only 16 Tokens in One Timestep: Accelerating Diffusion Transformers with Cluster-Driven Feature Caching](https://arxiv.org/abs/2509.10312)
*Zhixin Zheng,Xinyu Wang,Chang Zou,Shaobo Wang,Linfeng Zhang*

Main category: cs.CV

TL;DR: ClusCa accelerates diffusion transformers by 4.96x through spatial clustering that reduces tokens by over 90% while maintaining image quality, achieving 99.49% ImageReward on FLUX.


<details>
  <summary>Details</summary>
Motivation: Diffusion transformers suffer from high computational costs due to iterative denoising, and existing feature caching methods only address temporal similarity while ignoring spatial similarity.

Method: Cluster-Driven Feature Caching (ClusCa) performs spatial clustering on tokens each timestep, computes only one token per cluster, and propagates information to all other tokens, reducing token count by over 90%.

Result: Achieves 4.96x acceleration on FLUX with 99.49% ImageReward (surpassing original by 0.51%), works on DiT, FLUX and HunyuanVideo without training requirements.

Conclusion: ClusCa provides an effective orthogonal approach to existing feature caching methods by leveraging spatial similarity, significantly accelerating diffusion transformers while maintaining or improving output quality.

Abstract: Diffusion transformers have gained significant attention in recent years for
their ability to generate high-quality images and videos, yet still suffer from
a huge computational cost due to their iterative denoising process. Recently,
feature caching has been introduced to accelerate diffusion transformers by
caching the feature computation in previous timesteps and reusing it in the
following timesteps, which leverage the temporal similarity of diffusion models
while ignoring the similarity in the spatial dimension. In this paper, we
introduce Cluster-Driven Feature Caching (ClusCa) as an orthogonal and
complementary perspective for previous feature caching. Specifically, ClusCa
performs spatial clustering on tokens in each timestep, computes only one token
in each cluster and propagates their information to all the other tokens, which
is able to reduce the number of tokens by over 90%. Extensive experiments on
DiT, FLUX and HunyuanVideo demonstrate its effectiveness in both text-to-image
and text-to-video generation. Besides, it can be directly applied to any
diffusion transformer without requirements for training. For instance, ClusCa
achieves 4.96x acceleration on FLUX with an ImageReward of 99.49%, surpassing
the original model by 0.51%. The code is available at
https://github.com/Shenyi-Z/Cache4Diffusion.

</details>


### [54] [I-Segmenter: Integer-Only Vision Transformer for Efficient Semantic Segmentation](https://arxiv.org/abs/2509.10334)
*Jordan Sassoon,Michal Szczepanski,Martyna Poreba*

Main category: cs.CV

TL;DR: I-Segmenter is the first fully integer-only Vision Transformer framework for semantic segmentation that achieves near-FP32 accuracy while significantly reducing model size and enabling faster inference.


<details>
  <summary>Details</summary>
Motivation: Vision Transformers for semantic segmentation have high memory and computational costs, making deployment on resource-constrained devices challenging. Quantization helps but ViT-based segmentation models are fragile under low precision due to quantization error accumulation.

Method: Built on Segmenter architecture, systematically replaces floating-point operations with integer-only counterparts. Introduces λ-ShiftGELU activation function to handle long-tailed distributions, removes L2 normalization, and replaces bilinear interpolation with nearest neighbor upsampling for full integer-only execution.

Result: Achieves accuracy within 5.1% of FP32 baseline on average, reduces model size by up to 3.8x, enables up to 1.2x faster inference. Even with one-shot PTQ using a single calibration image, delivers competitive accuracy.

Conclusion: I-Segmenter provides a practical integer-only solution for real-world deployment of ViT segmentation models on resource-constrained devices while maintaining reasonable accuracy.

Abstract: Vision Transformers (ViTs) have recently achieved strong results in semantic
segmentation, yet their deployment on resource-constrained devices remains
limited due to their high memory footprint and computational cost. Quantization
offers an effective strategy to improve efficiency, but ViT-based segmentation
models are notoriously fragile under low precision, as quantization errors
accumulate across deep encoder-decoder pipelines. We introduce I-Segmenter, the
first fully integer-only ViT segmentation framework. Building on the Segmenter
architecture, I-Segmenter systematically replaces floating-point operations
with integer-only counterparts. To further stabilize both training and
inference, we propose $\lambda$-ShiftGELU, a novel activation function that
mitigates the limitations of uniform quantization in handling long-tailed
activation distributions. In addition, we remove the L2 normalization layer and
replace bilinear interpolation in the decoder with nearest neighbor upsampling,
ensuring integer-only execution throughout the computational graph. Extensive
experiments show that I-Segmenter achieves accuracy within a reasonable margin
of its FP32 baseline (5.1 % on average), while reducing model size by up to
3.8x and enabling up to 1.2x faster inference with optimized runtimes. Notably,
even in one-shot PTQ with a single calibration image, I-Segmenter delivers
competitive accuracy, underscoring its practicality for real-world deployment.

</details>


### [55] [GARD: Gamma-based Anatomical Restoration and Denoising for Retinal OCT](https://arxiv.org/abs/2509.10341)
*Botond Fazekas,Thomas Pinetz,Guilherme Aresta,Taha Emre,Hrvoje Bogunovic*

Main category: cs.CV

TL;DR: GARD is a novel deep learning method for OCT image denoising that uses gamma-based diffusion models and noise-reduced fidelity guidance to better preserve anatomical details while removing speckle noise.


<details>
  <summary>Details</summary>
Motivation: OCT images suffer from speckle noise that obscures fine details and hinders accurate diagnosis. Existing denoising methods struggle to balance noise reduction with preservation of crucial anatomical structures.

Method: GARD uses a Denoising Diffusion Gamma Model instead of Gaussian noise assumption, incorporates a Noise-Reduced Fidelity Term with pre-processed guidance, and adapts the Denoising Diffusion Implicit Model framework for faster inference.

Result: GARD significantly outperforms traditional methods and state-of-the-art deep learning models in PSNR, SSIM, and MSE metrics. Qualitative results show sharper edges and better preservation of fine anatomical details.

Conclusion: GARD provides an effective solution for OCT image denoising that accurately models speckle statistics while preserving critical anatomical information through gamma-based diffusion and guided denoising.

Abstract: Optical Coherence Tomography (OCT) is a vital imaging modality for diagnosing
and monitoring retinal diseases. However, OCT images are inherently degraded by
speckle noise, which obscures fine details and hinders accurate interpretation.
While numerous denoising methods exist, many struggle to balance noise
reduction with the preservation of crucial anatomical structures. This paper
introduces GARD (Gamma-based Anatomical Restoration and Denoising), a novel
deep learning approach for OCT image despeckling that leverages the strengths
of diffusion probabilistic models. Unlike conventional diffusion models that
assume Gaussian noise, GARD employs a Denoising Diffusion Gamma Model to more
accurately reflect the statistical properties of speckle. Furthermore, we
introduce a Noise-Reduced Fidelity Term that utilizes a pre-processed,
less-noisy image to guide the denoising process. This crucial addition prevents
the reintroduction of high-frequency noise. We accelerate the inference process
by adapting the Denoising Diffusion Implicit Model framework to our Gamma-based
model. Experiments on a dataset with paired noisy and less-noisy OCT B-scans
demonstrate that GARD significantly outperforms traditional denoising methods
and state-of-the-art deep learning models in terms of PSNR, SSIM, and MSE.
Qualitative results confirm that GARD produces sharper edges and better
preserves fine anatomical details.

</details>


### [56] [GLAM: Geometry-Guided Local Alignment for Multi-View VLP in Mammography](https://arxiv.org/abs/2509.10344)
*Yuexi Du,Lihui Chen,Nicha C. Dvornek*

Main category: cs.CV

TL;DR: GLAM is a foundation visual language model for mammography that uses geometry-guided global and local alignment to better capture multi-view relationships in breast cancer screening, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Current mammography VLMs adapted from natural images ignore domain-specific multi-view relationships that radiologists use, losing critical geometric context and leading to suboptimal performance.

Method: Proposes GLAM model with geometry-guided pretraining using joint global and local, visual-visual, and visual-language contrastive learning to capture cross-view alignments and fine-grained local features.

Result: Outperforms baseline models across multiple datasets under different settings when pretrained on EMBED, one of the largest open mammography datasets.

Conclusion: The geometry-guided approach successfully addresses the multi-view correspondence learning challenge in mammography, demonstrating improved performance over methods that treat mammography views as independent images.

Abstract: Mammography screening is an essential tool for early detection of breast
cancer. The speed and accuracy of mammography interpretation have the potential
to be improved with deep learning methods. However, the development of a
foundation visual language model (VLM) is hindered by limited data and domain
differences between natural and medical images. Existing mammography VLMs,
adapted from natural images, often ignore domain-specific characteristics, such
as multi-view relationships in mammography. Unlike radiologists who analyze
both views together to process ipsilateral correspondence, current methods
treat them as independent images or do not properly model the multi-view
correspondence learning, losing critical geometric context and resulting in
suboptimal prediction. We propose GLAM: Global and Local Alignment for
Multi-view mammography for VLM pretraining using geometry guidance. By
leveraging the prior knowledge about the multi-view imaging process of
mammograms, our model learns local cross-view alignments and fine-grained local
features through joint global and local, visual-visual, and visual-language
contrastive learning. Pretrained on EMBED [14], one of the largest open
mammography datasets, our model outperforms baselines across multiple datasets
under different settings.

</details>


### [57] [Towards Understanding Visual Grounding in Visual Language Models](https://arxiv.org/abs/2509.10345)
*Georgios Pantazopoulos,Eda B. Özyiğit*

Main category: cs.CV

TL;DR: Survey paper on visual grounding in vision-language models, covering importance, core components, applications, benchmarks, and future research directions.


<details>
  <summary>Details</summary>
Motivation: To review and synthesize research on visual grounding capabilities in modern vision-language models, as this ability enables diverse applications like referring expression comprehension, fine-grained QA, and environment control.

Method: Comprehensive literature review and analysis of representative works across key research areas in visual grounding, examining core components of grounded model development paradigms.

Result: Provides a structured overview of visual grounding in VLMs, delineates the contemporary development paradigm, examines practical applications and evaluation metrics, and analyzes interrelations with multimodal chain-of-thought and reasoning.

Conclusion: Identifies challenges in visual grounding and suggests promising future research directions to advance this critical capability in vision-language models.

Abstract: Visual grounding refers to the ability of a model to identify a region within
some visual input that matches a textual description. Consequently, a model
equipped with visual grounding capabilities can target a wide range of
applications in various domains, including referring expression comprehension,
answering questions pertinent to fine-grained details in images or videos,
caption visual context by explicitly referring to entities, as well as low and
high-level control in simulated and real environments. In this survey paper, we
review representative works across the key areas of research on modern
general-purpose vision language models (VLMs). We first outline the importance
of grounding in VLMs, then delineate the core components of the contemporary
paradigm for developing grounded models, and examine their practical
applications, including benchmarks and evaluation metrics for grounded
multimodal generation. We also discuss the multifaceted interrelations among
visual grounding, multimodal chain-of-thought, and reasoning in VLMs. Finally,
we analyse the challenges inherent to visual grounding and suggest promising
directions for future research.

</details>


### [58] [Immunizing Images from Text to Image Editing via Adversarial Cross-Attention](https://arxiv.org/abs/2509.10359)
*Matteo Trippodo,Federico Becattini,Lorenzo Seidenari*

Main category: cs.CV

TL;DR: Attention Attack disrupts text-based image editing by using auto-generated captions to break cross-attention alignment between images and text prompts, without needing knowledge of the editing method or prompt.


<details>
  <summary>Details</summary>
Motivation: Text-based image editing methods are vulnerable to adversarial attacks, particularly targeting the visual component and cross-attention mechanisms that enable fine-grained manipulation guided by natural language.

Method: Proposes Attention Attack that uses automatically generated captions of source images as proxies for edit prompts to disrupt cross-attention between textual prompts and visual representations, breaking content-text alignment.

Result: Experiments on TEDBench++ show the attack significantly degrades editing performance while remaining imperceptible, with novel evaluation metrics (Caption Similarity and semantic IoU) demonstrating effectiveness.

Conclusion: The proposed attack successfully targets visual components of text-based image editing methods, highlighting vulnerabilities in cross-attention mechanisms and providing new evaluation strategies for assessing immunization success.

Abstract: Recent advances in text-based image editing have enabled fine-grained
manipulation of visual content guided by natural language. However, such
methods are susceptible to adversarial attacks. In this work, we propose a
novel attack that targets the visual component of editing methods. We introduce
Attention Attack, which disrupts the cross-attention between a textual prompt
and the visual representation of the image by using an automatically generated
caption of the source image as a proxy for the edit prompt. This breaks the
alignment between the contents of the image and their textual description,
without requiring knowledge of the editing method or the editing prompt.
Reflecting on the reliability of existing metrics for immunization success, we
propose two novel evaluation strategies: Caption Similarity, which quantifies
semantic consistency between original and adversarial edits, and semantic
Intersection over Union (IoU), which measures spatial layout disruption via
segmentation masks. Experiments conducted on the TEDBench++ benchmark
demonstrate that our attack significantly degrades editing performance while
remaining imperceptible.

</details>


### [59] [Efficient Learned Image Compression Through Knowledge Distillation](https://arxiv.org/abs/2509.10366)
*Fabien Allemand,Attilio Fiandrotti,Sumanta Chaudhuri,Alaa Eddine Mazouz*

Main category: cs.CV

TL;DR: Knowledge distillation reduces neural network image compression resource requirements while maintaining performance across different architectures and quality/bitrate tradeoffs.


<details>
  <summary>Details</summary>
Motivation: Neural network-based image compression outperforms conventional codecs but requires significant processing power, making it unsuitable for real-time use on resource-constrained platforms.

Method: Leverage knowledge distillation where smaller neural networks are trained on outputs from larger, more complex models to achieve better performance than independent training.

Result: Knowledge distillation effectively reduces processing and energy requirements for image compression across various architecture sizes and different quality/bitrate tradeoffs.

Conclusion: Knowledge distillation is a viable approach to make neural image compression more practical for resource-constrained platforms, with potential for extension to transformer-based models and exploration of different teacher models and loss functions.

Abstract: Learned image compression sits at the intersection of machine learning and
image processing. With advances in deep learning, neural network-based
compression methods have emerged. In this process, an encoder maps the image to
a low-dimensional latent space, which is then quantized, entropy-coded into a
binary bitstream, and transmitted to the receiver. At the receiver end, the
bitstream is entropy-decoded, and a decoder reconstructs an approximation of
the original image. Recent research suggests that these models consistently
outperform conventional codecs. However, they require significant processing
power, making them unsuitable for real-time use on resource-constrained
platforms, which hinders their deployment in mainstream applications. This
study aims to reduce the resource requirements of neural networks used for
image compression by leveraging knowledge distillation, a training paradigm
where smaller neural networks, partially trained on the outputs of larger, more
complex models, can achieve better performance than when trained independently.
Our work demonstrates that knowledge distillation can be effectively applied to
image compression tasks: i) across various architecture sizes, ii) to achieve
different image quality/bit rate tradeoffs, and iii) to save processing and
energy resources. This approach introduces new settings and hyperparameters,
and future research could explore the impact of different teacher models, as
well as alternative loss functions. Knowledge distillation could also be
extended to transformer-based models. The code is publicly available at:
https://github.com/FABallemand/PRIM .

</details>


### [60] [Ordinality of Visible-Thermal Image Intensities for Intrinsic Image Decomposition](https://arxiv.org/abs/2509.10388)
*Zeqing Leo Yuan,Mani Ramanagopal,Aswin C. Sankaranarayanan,Srinivasa G. Narasimhan*

Main category: cs.CV

TL;DR: Training-free intrinsic image decomposition using visible-thermal image pairs, leveraging thermal absorption to derive ordinal constraints for self-supervised shading and reflectance recovery.


<details>
  <summary>Details</summary>
Motivation: Lack of extensive ground-truth data for real-world intrinsic image decomposition, with existing methods relying on synthetic data or sparse annotations for limited scenes.

Method: Uses visible-thermal image pairs and the principle that absorbed light becomes heat detected by thermal cameras. Relates ordinalities between visible and thermal intensities to shading/reflectance ordinalities to densely self-supervise an optimizing neural network.

Result: Demonstrates superior performance over recent learning-based models in quantitative evaluations with known reflectance/shading under natural/artificial lighting, and qualitative experiments across diverse outdoor scenes.

Conclusion: Provides a scalable path to curating real-world ordinal supervision previously infeasible via manual labeling, enabling effective intrinsic image decomposition without training data.

Abstract: Decomposing an image into its intrinsic photometric factors--shading and
reflectance--is a long-standing challenge due to the lack of extensive
ground-truth data for real-world scenes. Recent methods rely on synthetic data
or sparse annotations for limited indoor and even fewer outdoor scenes. We
introduce a novel training-free approach for intrinsic image decomposition
using only a pair of visible and thermal images. We leverage the principle that
light not reflected from an opaque surface is absorbed and detected as heat by
a thermal camera. This allows us to relate the ordinalities between visible and
thermal image intensities to the ordinalities of shading and reflectance, which
can densely self-supervise an optimizing neural network to recover shading and
reflectance. We perform quantitative evaluations with known reflectance and
shading under natural and artificial lighting, and qualitative experiments
across diverse outdoor scenes. The results demonstrate superior performance
over recent learning-based models and point toward a scalable path to curating
real-world ordinal supervision, previously infeasible via manual labeling.

</details>


### [61] [Compressed Video Quality Enhancement: Classifying and Benchmarking over Standards](https://arxiv.org/abs/2509.10407)
*Xiem HoangVan,Dang BuiDinh,Sang NguyenQuang,Wen-Hsiao Peng*

Main category: cs.CV

TL;DR: A comprehensive survey paper on compressed video quality enhancement that addresses limitations in existing surveys by providing a novel taxonomy, unified benchmarking framework, and systematic analysis of performance-complexity trade-offs.


<details>
  <summary>Details</summary>
Motivation: Existing surveys on compressed video quality enhancement suffer from limitations including lack of systematic classification linking methods to specific standards and artifacts, insufficient comparative analysis of architectural paradigms, and underdeveloped benchmarking practices.

Method: The paper presents three key contributions: 1) A novel taxonomy classifying CVQE methods across architectural paradigms, coding standards, and compressed-domain feature utilization; 2) A unified benchmarking framework integrating modern compression protocols and standard test sequences; 3) Systematic analysis of trade-offs between reconstruction performance and computational complexity.

Result: The paper establishes a comprehensive foundation for consistent assessment and informed model selection in CVQE research and deployment, addressing the gaps in existing literature.

Conclusion: This survey provides a systematic framework for evaluating compressed video quality enhancement methods, highlighting critical performance-complexity trade-offs and promising directions for future research in the field.

Abstract: Compressed video quality enhancement (CVQE) is crucial for improving user
experience with lossy video codecs like H.264/AVC, H.265/HEVC, and H.266/VVC.
While deep learning based CVQE has driven significant progress, existing
surveys still suffer from limitations: lack of systematic classification
linking methods to specific standards and artifacts, insufficient comparative
analysis of architectural paradigms across coding types, and underdeveloped
benchmarking practices. To address these gaps, this paper presents three key
contributions. First, it introduces a novel taxonomy classifying CVQE methods
across architectural paradigms, coding standards, and compressed-domain feature
utilization. Second, it proposes a unified benchmarking framework integrating
modern compression protocols and standard test sequences for fair
multi-criteria evaluation. Third, it provides a systematic analysis of the
critical trade-offs between reconstruction performance and computational
complexity observed in state-of-the-art methods and highlighting promising
directions for future research. This comprehensive review aims to establish a
foundation for consistent assessment and informed model selection in CVQE
research and deployment.

</details>


### [62] [Multimodal SAM-adapter for Semantic Segmentation](https://arxiv.org/abs/2509.10408)
*Iacopo Curti,Pierluigi Zama Ramirez,Alioscia Petrelli,Luigi Di Stefano*

Main category: cs.CV

TL;DR: MM SAM-adapter extends Segment Anything Model for multimodal semantic segmentation using adapter network to fuse auxiliary sensor data with RGB features, achieving state-of-the-art performance on challenging benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current semantic segmentation methods are vulnerable to challenging conditions like poor lighting, occlusions, and adverse weather. Multimodal approaches that integrate auxiliary sensor data can provide complementary information to enhance robustness.

Method: Proposes an adapter network that injects fused multimodal features into SAM's RGB features, enabling retention of RGB generalization while selectively incorporating auxiliary modalities when they provide additional cues.

Result: Achieves state-of-the-art performance on three challenging benchmarks (DeLiVER, FMB, MUSES). Outperforms competing methods in both favorable and adverse conditions when tested on RGB-easy and RGB-hard subsets.

Conclusion: The framework demonstrates effective multimodal adaptation for robust scene understanding, achieving balanced and efficient use of multimodal information while maintaining strong generalization capabilities.

Abstract: Semantic segmentation, a key task in computer vision with broad applications
in autonomous driving, medical imaging, and robotics, has advanced
substantially with deep learning. Nevertheless, current approaches remain
vulnerable to challenging conditions such as poor lighting, occlusions, and
adverse weather. To address these limitations, multimodal methods that
integrate auxiliary sensor data (e.g., LiDAR, infrared) have recently emerged,
providing complementary information that enhances robustness. In this work, we
present MM SAM-adapter, a novel framework that extends the capabilities of the
Segment Anything Model (SAM) for multimodal semantic segmentation. The proposed
method employs an adapter network that injects fused multimodal features into
SAM's rich RGB features. This design enables the model to retain the strong
generalization ability of RGB features while selectively incorporating
auxiliary modalities only when they contribute additional cues. As a result, MM
SAM-adapter achieves a balanced and efficient use of multimodal information. We
evaluate our approach on three challenging benchmarks, DeLiVER, FMB, and MUSES,
where MM SAM-adapter delivers state-of-the-art performance. To further analyze
modality contributions, we partition DeLiVER and FMB into RGB-easy and RGB-hard
subsets. Results consistently demonstrate that our framework outperforms
competing methods in both favorable and adverse conditions, highlighting the
effectiveness of multimodal adaptation for robust scene understanding. The code
is available at the following link:
https://github.com/iacopo97/Multimodal-SAM-Adapter.

</details>


### [63] [InfGen: A Resolution-Agnostic Paradigm for Scalable Image Synthesis](https://arxiv.org/abs/2509.10441)
*Tao Han,Wanghan Xu,Junchao Gong,Xiaoyu Yue,Song Guo,Luping Zhou,Lei Bai*

Main category: cs.CV

TL;DR: InfGen enables arbitrary resolution image generation from fixed-size latent representations, reducing 4K generation time from over 100 seconds to under 10 seconds by replacing VAE decoders with a one-step generator.


<details>
  <summary>Details</summary>
Motivation: Current diffusion models have quadratic computational complexity with resolution, causing impractical delays (over 100 seconds for 4K images) and limiting cross-device visual consistency.

Method: Proposes InfGen as a second-generation latent diffusion model that treats fixed latent from diffusion models as content representation, then uses a compact one-step generator to decode arbitrary resolution images without retraining the diffusion backbone.

Result: InfGen reduces 4K image generation time to under 10 seconds while maintaining quality, and can be applied to any model using the same latent space to enable arbitrary high-resolution generation.

Conclusion: InfGen provides an efficient solution for arbitrary resolution image generation with significantly reduced computational complexity, making high-resolution diffusion-based generation practical and accessible across different devices.

Abstract: Arbitrary resolution image generation provides a consistent visual experience
across devices, having extensive applications for producers and consumers.
Current diffusion models increase computational demand quadratically with
resolution, causing 4K image generation delays over 100 seconds. To solve this,
we explore the second generation upon the latent diffusion models, where the
fixed latent generated by diffusion models is regarded as the content
representation and we propose to decode arbitrary resolution images with a
compact generated latent using a one-step generator. Thus, we present the
\textbf{InfGen}, replacing the VAE decoder with the new generator, for
generating images at any resolution from a fixed-size latent without retraining
the diffusion models, which simplifies the process, reducing computational
complexity and can be applied to any model using the same latent space.
Experiments show InfGen is capable of improving many models into the arbitrary
high-resolution era while cutting 4K image generation time to under 10 seconds.

</details>


### [64] [SSL-AD: Spatiotemporal Self-Supervised Learning for Generalizability and Adaptability Across Alzheimer's Prediction Tasks and Datasets](https://arxiv.org/abs/2509.10453)
*Emily Kaczmarek,Justin Szeto,Brennan Nichyporuk,Tal Arbel*

Main category: cs.CV

TL;DR: Self-supervised learning model for Alzheimer's prediction using 3D brain MRI with temporal SSL approaches outperforms supervised learning on most tasks and handles variable-length inputs effectively.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning models for Alzheimer's prediction suffer from limited labeled data, poor generalization across datasets, and inability to handle varying numbers of input scans and time intervals.

Method: Adapted three state-of-the-art temporal self-supervised learning approaches for 3D brain MRI analysis with novel extensions for variable-length inputs and robust spatial feature learning. Pre-trained on aggregated dataset of 3,161 patients from four public datasets.

Result: SSL model with temporal order prediction and contrastive learning outperformed supervised learning on 6 out of 7 downstream tasks including diagnosis classification, conversion detection, and future conversion prediction.

Conclusion: The SSL approach demonstrates strong adaptability, generalizability across tasks, and robust performance with varying input images and time intervals, making it suitable for diverse clinical applications.

Abstract: Alzheimer's disease is a progressive, neurodegenerative disorder that causes
memory loss and cognitive decline. While there has been extensive research in
applying deep learning models to Alzheimer's prediction tasks, these models
remain limited by lack of available labeled data, poor generalization across
datasets, and inflexibility to varying numbers of input scans and time
intervals between scans. In this study, we adapt three state-of-the-art
temporal self-supervised learning (SSL) approaches for 3D brain MRI analysis,
and add novel extensions designed to handle variable-length inputs and learn
robust spatial features. We aggregate four publicly available datasets
comprising 3,161 patients for pre-training, and show the performance of our
model across multiple Alzheimer's prediction tasks including diagnosis
classification, conversion detection, and future conversion prediction.
Importantly, our SSL model implemented with temporal order prediction and
contrastive learning outperforms supervised learning on six out of seven
downstream tasks. It demonstrates adaptability and generalizability across
tasks and number of input images with varying time intervals, highlighting its
capacity for robust performance across clinical applications. We release our
code and model publicly at https://github.com/emilykaczmarek/SSL-AD.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [65] [Human-AI Collaboration Increases Efficiency in Regulatory Writing](https://arxiv.org/abs/2509.09738)
*Umut Eser,Yael Gozin,L. Jay Stallons,Ari Caroline,Martin Preusse,Brandon Rice,Scott Wright,Andrew Robertson*

Main category: cs.AI

TL;DR: AutoIND LLM platform reduces IND application drafting time by ~97% (from ~100h to 2.6-3.7h) while maintaining acceptable quality scores (69.6-77.9%) with no critical regulatory errors, though expert refinement is still needed for submission readiness.


<details>
  <summary>Details</summary>
Motivation: IND application preparation is time-intensive and expertise-dependent, slowing early clinical development. The study aims to evaluate if LLMs can accelerate this process while maintaining regulatory document quality.

Method: Compared AutoIND-generated IND nonclinical summaries against manual drafting benchmarks. Measured drafting times directly and assessed quality through blinded regulatory expert evaluation using 7 criteria (correctness, completeness, conciseness, consistency, clarity, redundancy, emphasis) scored 0-3.

Result: 97% time reduction (100h → 3.7h for IND-1, 2.6h for IND-2), quality scores of 69.6% and 77.9%, no critical regulatory errors but deficiencies in emphasis, conciseness, and clarity.

Conclusion: AutoIND dramatically accelerates IND drafting but expert regulatory writers remain essential for final quality. Identified deficiencies provide roadmap for targeted model improvements.

Abstract: Background: Investigational New Drug (IND) application preparation is
time-intensive and expertise-dependent, slowing early clinical development.
Objective: To evaluate whether a large language model (LLM) platform (AutoIND)
can reduce first-draft composition time while maintaining document quality in
regulatory submissions. Methods: Drafting times for IND nonclinical written
summaries (eCTD modules 2.6.2, 2.6.4, 2.6.6) generated by AutoIND were directly
recorded. For comparison, manual drafting times for IND summaries previously
cleared by the U.S. FDA were estimated from the experience of regulatory
writers ($\geq$6 years) and used as industry-standard benchmarks. Quality was
assessed by a blinded regulatory writing assessor using seven pre-specified
categories: correctness, completeness, conciseness, consistency, clarity,
redundancy, and emphasis. Each sub-criterion was scored 0-3 and normalized to a
percentage. A critical regulatory error was defined as any misrepresentation or
omission likely to alter regulatory interpretation (e.g., incorrect NOAEL,
omission of mandatory GLP dose-formulation analysis). Results: AutoIND reduced
initial drafting time by $\sim$97% (from $\sim$100 h to 3.7 h for 18,870
pages/61 reports in IND-1; and to 2.6 h for 11,425 pages/58 reports in IND-2).
Quality scores were 69.6\% and 77.9\% for IND-1 and IND-2. No critical
regulatory errors were detected, but deficiencies in emphasis, conciseness, and
clarity were noted. Conclusions: AutoIND can dramatically accelerate IND
drafting, but expert regulatory writers remain essential to mature outputs to
submission-ready quality. Systematic deficiencies identified provide a roadmap
for targeted model improvements.

</details>


### [66] [Executable Ontologies: Synthesizing Event Semantics with Dataflow Architecture](https://arxiv.org/abs/2509.09775)
*Aleksandr Boldachev*

Main category: cs.AI

TL;DR: Boldsea is an architecture for modeling complex dynamic systems using executable ontologies that integrate event semantics with dataflow to overcome limitations of traditional BPM systems and object-oriented semantic technologies.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of traditional Business Process Management systems and object-oriented semantic technologies by creating a more dynamic and flexible approach to modeling complex systems.

Method: Developed the boldsea architecture with formal BSL (boldsea Semantic Language) including BNF grammar, and created the boldsea-engine that directly interprets semantic models as executable algorithms without compilation.

Result: The approach enables runtime modification of event models, ensures temporal transparency, and seamlessly merges data and business logic within a unified semantic framework.

Conclusion: Boldsea provides an effective architecture for executable ontologies that can directly control process execution while overcoming traditional system limitations through semantic-event integration.

Abstract: This paper presents boldsea, Boldachev's semantic-event approach -- an
architecture for modeling complex dynamic systems using executable ontologies
-- semantic models that act as dynamic structures, directly controlling process
execution. We demonstrate that integrating event semantics with a dataflow
architecture addresses the limitations of traditional Business Process
Management (BPM) systems and object-oriented semantic technologies. The paper
presents the formal BSL (boldsea Semantic Language), including its BNF grammar,
and outlines the boldsea-engine's architecture, which directly interprets
semantic models as executable algorithms without compilation. It enables the
modification of event models at runtime, ensures temporal transparency, and
seamlessly merges data and business logic within a unified semantic framework.

</details>


### [67] [How well can LLMs provide planning feedback in grounded environments?](https://arxiv.org/abs/2509.09790)
*Yuxuan Li,Victor Zhong*

Main category: cs.AI

TL;DR: Foundation models (LLMs/VLMs) can provide diverse high-quality feedback for planning tasks, with larger reasoning models showing better performance, though quality degrades in complex continuous environments.


<details>
  <summary>Details</summary>
Motivation: To reduce the need for carefully designed reward functions and high-quality demonstrations in grounded environment planning by leveraging pretrained foundation models' background knowledge.

Method: Evaluated LLMs and VLMs across symbolic, language, and continuous control environments with various feedback types (binary, preference, action advising, goal advising, delta action) and inference methods (in-context learning, chain-of-thought, access to dynamics).

Result: Foundation models provide high-quality diverse feedback across domains; larger reasoning models offer more accurate feedback with less bias and benefit more from enhanced inference methods.

Conclusion: While foundation models show promise for planning feedback, their effectiveness decreases in environments with complex dynamics or continuous state/action spaces, suggesting limitations for highly complex domains.

Abstract: Learning to plan in grounded environments typically requires carefully
designed reward functions or high-quality annotated demonstrations. Recent
works show that pretrained foundation models, such as large language models
(LLMs) and vision language models (VLMs), capture background knowledge helpful
for planning, which reduces the amount of reward design and demonstrations
needed for policy learning. We evaluate how well LLMs and VLMs provide feedback
across symbolic, language, and continuous control environments. We consider
prominent types of feedback for planning including binary feedback, preference
feedback, action advising, goal advising, and delta action feedback. We also
consider inference methods that impact feedback performance, including
in-context learning, chain-of-thought, and access to environment dynamics. We
find that foundation models can provide diverse high-quality feedback across
domains. Moreover, larger and reasoning models consistently provide more
accurate feedback, exhibit less bias, and benefit more from enhanced inference
methods. Finally, feedback quality degrades for environments with complex
dynamics or continuous state spaces and action spaces.

</details>


### [68] [A Modular and Multimodal Generative AI Framework for Urban Building Energy Data: Generating Synthetic Homes](https://arxiv.org/abs/2509.09794)
*Jackson Eshbaugh,Chetan Tiwari,Jorge Silveyra*

Main category: cs.AI

TL;DR: A modular multimodal framework using generative AI to create realistic labeled energy modeling data from publicly available residential information and images, reducing dependency on costly or restricted data sources.


<details>
  <summary>Details</summary>
Motivation: Computational energy models require extensive data that is often inaccessible, expensive, or raises privacy concerns, creating barriers to research.

Method: Developed a modular multimodal framework that leverages generative AI to produce energy modeling data from publicly accessible residential information and images, with evaluation of AI components.

Result: Experiments show the framework avoids common generative model issues and produces realistic, labeled data effectively.

Conclusion: The framework enables more accessible and reproducible energy modeling research by reducing dependence on costly or restricted data sources.

Abstract: Computational models have emerged as powerful tools for energy modeling
research, touting scalability and quantitative results. However, these models
require a plethora of data, some of which is inaccessible, expensive, or raises
privacy concerns. We introduce a modular multimodal framework to produce this
data from publicly accessible residential information and images using
generative artificial intelligence (AI). Additionally, we provide a pipeline
demonstrating this framework, and we evaluate its generative AI components. Our
experiments show that our framework's use of AI avoids common issues with
generative models. Our framework produces realistic, labeled data. By reducing
dependence on costly or restricted data sources, we pave a path towards more
accessible and reproducible research.

</details>


### [69] [Towards a Common Framework for Autoformalization](https://arxiv.org/abs/2509.09810)
*Agnieszka Mensfelt,David Tena Cucala,Santiago Franco,Angeliki Koutsoukou-Argyraki,Vince Trencsenyi,Kostas Stathis*

Main category: cs.AI

TL;DR: This paper reviews autoformalization - translating informal input into formal representations using LLMs - and proposes a unified framework to connect disparate research areas.


<details>
  <summary>Details</summary>
Motivation: The rapid but independent development of autoformalization research across mathematics, reasoning, planning, and knowledge representation has limited opportunities for shared methodologies and benchmarks, hindering progress.

Method: The paper conducts a comprehensive review of both explicit and implicit instances of autoformalization across different fields and proposes a unified framework to connect these research areas.

Result: The analysis reveals that despite addressing similar translation tasks from informal to formal representations, these research streams have developed largely independently without shared frameworks.

Conclusion: A unified framework for autoformalization can enable cross-pollination between fields, accelerate progress, and advance the development of next-generation AI systems.

Abstract: Autoformalization has emerged as a term referring to the automation of
formalization - specifically, the formalization of mathematics using
interactive theorem provers (proof assistants). Its rapid development has been
driven by progress in deep learning, especially large language models (LLMs).
More recently, the term has expanded beyond mathematics to describe the broader
task of translating informal input into formal logical representations. At the
same time, a growing body of research explores using LLMs to translate informal
language into formal representations for reasoning, planning, and knowledge
representation - often without explicitly referring to this process as
autoformalization. As a result, despite addressing similar tasks, the largely
independent development of these research areas has limited opportunities for
shared methodologies, benchmarks, and theoretical frameworks that could
accelerate progress. The goal of this paper is to review - explicit or implicit
- instances of what can be considered autoformalization and to propose a
unified framework, encouraging cross-pollination between different fields to
advance the development of next generation AI systems.

</details>


### [70] [Towards an AI-based knowledge assistant for goat farmers based on Retrieval-Augmented Generation](https://arxiv.org/abs/2509.09848)
*Nana Han,Dong Liu,Tomas Norton*

Main category: cs.AI

TL;DR: An intelligent knowledge assistant system using RAG with structured knowledge processing methods for goat farming health management, achieving 85%+ accuracy across different Q&A tasks.


<details>
  <summary>Details</summary>
Motivation: LLMs have limited application in livestock farming due to complex, heterogeneous knowledge sources. This study aims to support goat health management by enhancing LLMs' understanding of diverse data formats.

Method: Used Retrieval-Augmented Generation (RAG) with two structured knowledge processing methods: table textualization and decision-tree textualization. Established a domain-specific knowledge base covering five goat farming domains and integrated an online search module for real-time information.

Result: Heterogeneous knowledge fusion achieved 87.90% accuracy on validation set and 84.22% on test set. Accuracy exceeded 85% across text-based, table-based, and decision-tree based Q&A tasks. Omission was identified as the main error type.

Conclusion: The system demonstrates robustness and reliability for practical goat farming applications, with opportunities to improve retrieval coverage and context integration.

Abstract: Large language models (LLMs) are increasingly being recognised as valuable
knowledge communication tools in many industries. However, their application in
livestock farming remains limited, being constrained by several factors not
least the availability, diversity and complexity of knowledge sources. This
study introduces an intelligent knowledge assistant system designed to support
health management in farmed goats. Leveraging the Retrieval-Augmented
Generation (RAG), two structured knowledge processing methods, table
textualization and decision-tree textualization, were proposed to enhance large
language models' (LLMs) understanding of heterogeneous data formats. Based on
these methods, a domain-specific goat farming knowledge base was established to
improve LLM's capacity for cross-scenario generalization. The knowledge base
spans five key domains: Disease Prevention and Treatment, Nutrition Management,
Rearing Management, Goat Milk Management, and Basic Farming Knowledge.
Additionally, an online search module is integrated to enable real-time
retrieval of up-to-date information. To evaluate system performance, six
ablation experiments were conducted to examine the contribution of each
component. The results demonstrated that heterogeneous knowledge fusion method
achieved the best results, with mean accuracies of 87.90% on the validation set
and 84.22% on the test set. Across the text-based, table-based, decision-tree
based Q&A tasks, accuracy consistently exceeded 85%, validating the
effectiveness of structured knowledge fusion within a modular design. Error
analysis identified omission as the predominant error category, highlighting
opportunities to further improve retrieval coverage and context integration. In
conclusion, the results highlight the robustness and reliability of the
proposed system for practical applications in goat farming.

</details>


### [71] [LLMs as Agentic Cooperative Players in Multiplayer UNO](https://arxiv.org/abs/2509.09867)
*Yago Romano Matinez,Jesse Roberts*

Main category: cs.AI

TL;DR: LLMs can play UNO effectively but struggle to help other players win, with larger models showing only marginal improvements in assistance capabilities.


<details>
  <summary>Details</summary>
Motivation: To test whether LLMs can actively assist humans in achieving goals, specifically by helping another player win in UNO rather than playing to win themselves.

Method: Built a tool for decoder-only LLMs to participate in RLCard game environment, providing full game-state information and testing with two prompting strategies across models from 1B to 70B parameters.

Result: All models outperformed random baseline when playing UNO, but few could significantly help another player win, with model scale having limited impact on assistance performance.

Conclusion: While LLMs can successfully play games, their ability to provide meaningful assistance to human partners remains limited, suggesting current models may not be effective as active collaborative participants.

Abstract: LLMs promise to assist humans -- not just by answering questions, but by
offering useful guidance across a wide range of tasks. But how far does that
assistance go? Can a large language model based agent actually help someone
accomplish their goal as an active participant? We test this question by
engaging an LLM in UNO, a turn-based card game, asking it not to win but
instead help another player to do so. We built a tool that allows decoder-only
LLMs to participate as agents within the RLCard game environment. These models
receive full game-state information and respond using simple text prompts under
two distinct prompting strategies. We evaluate models ranging from small (1B
parameters) to large (70B parameters) and explore how model scale impacts
performance. We find that while all models were able to successfully outperform
a random baseline when playing UNO, few were able to significantly aid another
player.

</details>


### [72] [The (R)evolution of Scientific Workflows in the Agentic AI Era: Towards Autonomous Science](https://arxiv.org/abs/2509.09915)
*Woong Shin,Renan Souza,Daniel Rosendo,Frédéric Suter,Feiyi Wang,Prasanna Balaprakash,Rafael Ferreira da Silva*

Main category: cs.AI

TL;DR: Proposes an evolutionary framework for scientific workflows from static single systems to intelligent swarms, with architectural blueprint for autonomous science labs enabling 100x discovery acceleration.


<details>
  <summary>Details</summary>
Motivation: Modern scientific discovery requires coordinating distributed facilities and heterogeneous resources, forcing researchers to act as manual workflow coordinators rather than scientists. AI agents show potential to accelerate discovery but need integration frameworks.

Method: Conceptual framework with two evolutionary dimensions: intelligence (static to intelligent) and composition (single to swarm), plus architectural blueprint for autonomous scientific laboratories.

Result: Framework charts evolutionary path from current workflow management systems to fully autonomous, distributed scientific laboratories.

Conclusion: The proposed approach can help community harness opportunities in autonomous science with potential for 100x discovery acceleration and transformational scientific workflows.

Abstract: Modern scientific discovery increasingly requires coordinating distributed
facilities and heterogeneous resources, forcing researchers to act as manual
workflow coordinators rather than scientists. Advances in AI leading to AI
agents show exciting new opportunities that can accelerate scientific discovery
by providing intelligence as a component in the ecosystem. However, it is
unclear how this new capability would materialize and integrate in the real
world. To address this, we propose a conceptual framework where workflows
evolve along two dimensions which are intelligence (from static to intelligent)
and composition (from single to swarm) to chart an evolutionary path from
current workflow management systems to fully autonomous, distributed scientific
laboratories. With these trajectories in mind, we present an architectural
blueprint that can help the community take the next steps towards harnessing
the opportunities in autonomous science with the potential for 100x discovery
acceleration and transformational scientific workflows.

</details>


### [73] [A Markovian Framing of WaveFunctionCollapse for Procedurally Generating Aesthetically Complex Environments](https://arxiv.org/abs/2509.09919)
*Franklin Yiu,Mohan Lu,Nina Li,Kevin Joseph,Tianxu Zhang,Julian Togelius,Timothy Merino,Sam Earle*

Main category: cs.AI

TL;DR: Reformulating WaveFunctionCollapse as MDP separates constraint satisfaction from objective optimization, outperforming traditional joint optimization methods.


<details>
  <summary>Details</summary>
Motivation: Procedural content generation needs to satisfy both designer objectives and tile adjacency constraints, requiring effective joint optimization approaches.

Method: Reformulate WaveFunctionCollapse as a Markov Decision Process to enable external optimization algorithms to focus on objective maximization while WFC handles constraint satisfaction.

Result: Across multiple domains with varying difficulties, optimization over WFC-MDP consistently outperforms traditional evolutionary approaches that jointly optimize both constraints and objectives.

Conclusion: Decoupling local constraint satisfaction from global objective optimization provides significant advantages over joint optimization methods, especially as task complexity increases.

Abstract: Procedural content generation often requires satisfying both
designer-specified objectives and adjacency constraints implicitly imposed by
the underlying tile set. To address the challenges of jointly optimizing both
constraints and objectives, we reformulate WaveFunctionCollapse (WFC) as a
Markov Decision Process (MDP), enabling external optimization algorithms to
focus exclusively on objective maximization while leveraging WFC's propagation
mechanism to enforce constraint satisfaction. We empirically compare optimizing
this MDP to traditional evolutionary approaches that jointly optimize global
metrics and local tile placement. Across multiple domains with various
difficulties, we find that joint optimization not only struggles as task
complexity increases, but consistently underperforms relative to optimization
over the WFC-MDP, underscoring the advantages of decoupling local constraint
satisfaction from global objective optimization.

</details>


### [74] [Evaluation of Black-Box XAI Approaches for Predictors of Values of Boolean Formulae](https://arxiv.org/abs/2509.09982)
*Stav Armoni-Friedmann,Hana Chockler,David A. Kelly*

Main category: cs.AI

TL;DR: Proposes a formal measure for evaluating XAI tools on tabular data using Boolean functions, introduces B-ReX tool that outperforms others on benchmark tests


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of subjective evaluation in explainable AI, particularly for tabular data and Boolean function predictions

Method: Extends previous work with formal importance measure based on actual causality, evaluates state-of-the-art XAI tools, and develops novel B-ReX tool based on existing ReX

Result: B-ReX demonstrates superior performance over other black-box XAI tools, achieving Jensen-Shannon divergence of 0.072 ± 0.012 on random 10-valued Boolean formulae

Conclusion: The proposed formal causality-based measure provides objective evaluation of XAI tools, and B-ReX represents an advancement in black-box explanation methods for tabular data

Abstract: Evaluating explainable AI (XAI) approaches is a challenging task in general,
due to the subjectivity of explanations. In this paper, we focus on tabular
data and the specific use case of AI models predicting the values of Boolean
functions. We extend the previous work in this domain by proposing a formal and
precise measure of importance of variables based on actual causality, and we
evaluate state-of-the-art XAI tools against this measure. We also present a
novel XAI tool B-ReX, based on the existing tool ReX, and demonstrate that it
is superior to other black-box XAI tools on a large-scale benchmark.
Specifically, B-ReX achieves a Jensen-Shannon divergence of 0.072 $\pm$ 0.012
on random 10-valued Boolean formulae

</details>


### [75] [GAMA: A General Anonymizing Multi-Agent System for Privacy Preservation Enhanced by Domain Rules and Disproof Method](https://arxiv.org/abs/2509.10018)
*Hailong Yang,Renhuo Zhao,Guanjin Wang,Zhaohong Deng*

Main category: cs.AI

TL;DR: GAMA is a privacy-preserving multi-agent system that separates private and public workspaces with anonymization, using DRKE and DLE modules to maintain semantic integrity while protecting sensitive data.


<details>
  <summary>Details</summary>
Motivation: LLM-based multi-agent systems need to handle privacy-sensitive tasks but cannot securely use high-performance LLMs on public servers without privacy protection mechanisms.

Method: Divides agents' workspace into private and public spaces, anonymizes data for public space, and incorporates Domain-Rule-based Knowledge Enhancement (DRKE) and Disproof-based Logic Enhancement (DLE) to mitigate semantic loss from anonymization.

Result: Superior performance on Trivia Creative Writing and Logic Grid Puzzle datasets compared to state-of-the-art models, and exceptional effectiveness on new privacy-focused datasets for both task processing and privacy preservation.

Conclusion: GAMA provides an effective framework for privacy-preserving multi-agent systems that maintains performance while protecting sensitive information through spatial separation and enhanced anonymization techniques.

Abstract: With the rapid advancement of Large Language Model (LLM), LLM-based agents
exhibit exceptional abilities in understanding and generating natural language,
facilitating human-like collaboration and information transmission in LLM-based
Multi-Agent System (MAS). High-performance LLMs are often hosted on remote
servers in public spaces. When tasks involve privacy data, MAS cannot securely
utilize these LLMs without implementing privacy-preserving mechanisms. To
address this challenge, we propose a General Anonymizing Multi-Agent system
(GAMA), which divides the agents' workspace into private and public spaces and
protects privacy through the anonymizing mechanism. In the private space,
agents handle sensitive data, while in the public space, only anonymized data
is utilized. GAMA incorporates two key modules to mitigate semantic loss caused
by anonymization: Domain-Rule-based Knowledge Enhancement (DRKE) and
Disproof-based Logic Enhancement (DLE). We evaluate GAMA on two public
question-answering datasets: Trivia Creative Writing and Logic Grid Puzzle. The
results demonstrate that GAMA has superior performance compared to the
state-of-the-art models. To further assess its privacy-preserving capabilities,
we designed two new datasets: Knowledge Privacy Preservation and Logic Privacy
Preservation. The final results highlight GAMA's exceptional effectiveness in
both task processing and privacy preservation.

</details>


### [76] [XAgents: A Unified Framework for Multi-Agent Cooperation via IF-THEN Rules and Multipolar Task Processing Graph](https://arxiv.org/abs/2509.10054)
*Hailong Yang,Mingxian Gu,Jianqi Wang,Guanjin Wang,Zhaohong Deng*

Main category: cs.AI

TL;DR: XAgents is a multi-agent framework using multipolar task graphs and IF-THEN rules to improve task planning and handle uncertainty in complex tasks, outperforming state-of-the-art approaches.


<details>
  <summary>Details</summary>
Motivation: Current multi-agent systems struggle with effective task planning and handling uncertainty in complex tasks, often producing misleading outputs that hinder execution.

Method: Proposes XAgents framework built on multipolar task processing graph for dynamic planning and IF-THEN rules to constrain agent behaviors and enhance collaboration.

Result: XAgents consistently surpasses state-of-the-art single-agent and multi-agent approaches across three distinct datasets in knowledge-typed and logic-typed QA tasks.

Conclusion: The framework effectively addresses uncertainty and improves task planning in multi-agent systems through structured task processing and rule-based constraints.

Abstract: The rapid advancement of Large Language Models (LLMs) has significantly
enhanced the capabilities of Multi-Agent Systems (MAS) in supporting humans
with complex, real-world tasks. However, MAS still face challenges in effective
task planning when handling highly complex tasks with uncertainty, often
resulting in misleading or incorrect outputs that hinder task execution. To
address this, we propose XAgents, a unified multi-agent cooperative framework
built on a multipolar task processing graph and IF-THEN rules. XAgents uses the
multipolar task processing graph to enable dynamic task planning and handle
task uncertainty. During subtask processing, it integrates domain-specific
IF-THEN rules to constrain agent behaviors, while global rules enhance
inter-agent collaboration. We evaluate the performance of XAgents across three
distinct datasets, demonstrating that it consistently surpasses
state-of-the-art single-agent and multi-agent approaches in both
knowledge-typed and logic-typed question-answering tasks. The codes for XAgents
are available at: https://github.com/AGI-FHBC/XAgents.

</details>


### [77] [AI Harmonics: a human-centric and harms severity-adaptive AI risk assessment framework](https://arxiv.org/abs/2509.10104)
*Sofia Vei,Paolo Giudici,Pavlos Sermpezis,Athena Vakali,Adelaide Emma Bernardelli*

Main category: cs.AI

TL;DR: AI Harmonics is a human-centric AI risk assessment framework that uses ordinal severity data to prioritize AI harms, identifying political and physical harms as most urgent for mitigation.


<details>
  <summary>Details</summary>
Motivation: Current AI risk assessment models focus on internal compliance and neglect diverse stakeholder perspectives and real-world consequences of AI harms.

Method: Proposes AI Harmonics framework with a novel AI harm assessment metric (AIH) that leverages ordinal severity data to capture relative impact without precise numerical estimates, combining generalized methodology with data-driven stakeholder-aware framework.

Result: Experiments on annotated incident data show political and physical harms have the highest concentration - political harms erode public trust while physical harms pose life-threatening risks. The framework consistently identifies uneven harm distributions.

Conclusion: AI Harmonics enables policymakers and organizations to effectively target mitigation efforts by providing a human-centric, harm-severity adaptive approach grounded in empirical incident data.

Abstract: The absolute dominance of Artificial Intelligence (AI) introduces
unprecedented societal harms and risks. Existing AI risk assessment models
focus on internal compliance, often neglecting diverse stakeholder perspectives
and real-world consequences. We propose a paradigm shift to a human-centric,
harm-severity adaptive approach grounded in empirical incident data. We present
AI Harmonics, which includes a novel AI harm assessment metric (AIH) that
leverages ordinal severity data to capture relative impact without requiring
precise numerical estimates. AI Harmonics combines a robust, generalized
methodology with a data-driven, stakeholder-aware framework for exploring and
prioritizing AI harms. Experiments on annotated incident data confirm that
political and physical harms exhibit the highest concentration and thus warrant
urgent mitigation: political harms erode public trust, while physical harms
pose serious, even life-threatening risks, underscoring the real-world
relevance of our approach. Finally, we demonstrate that AI Harmonics
consistently identifies uneven harm distributions, enabling policymakers and
organizations to target their mitigation efforts effectively.

</details>


### [78] [Virtual Agent Economies](https://arxiv.org/abs/2509.10147)
*Nenad Tomasev,Matija Franklin,Joel Z. Leibo,Julian Jacobs,William A. Cunningham,Iason Gabriel,Simon Osindero*

Main category: cs.AI

TL;DR: The paper proposes a "sandbox economy" framework for analyzing autonomous AI agent economies, categorizing them by origin (emergent vs intentional) and separateness (permeable vs impermeable), and discusses design choices for steerable AI markets.


<details>
  <summary>Details</summary>
Motivation: The rapid adoption of autonomous AI agents is creating new economic systems that operate beyond human oversight, presenting both opportunities for unprecedented coordination and significant risks including systemic economic risk and inequality.

Method: The authors propose a conceptual framework (sandbox economy) with two key dimensions and discuss various design approaches including auction mechanisms for resource allocation, AI "mission economies" for collective goal coordination, and socio-technical infrastructure for trust and accountability.

Result: The analysis suggests our current trajectory leads to a spontaneous emergence of a vast, highly permeable AI agent economy, requiring proactive design choices to ensure alignment with humanity's collective interests.

Conclusion: Proactive design of steerable AI agent markets is necessary to ensure the coming technological shift aligns with humanity's long-term collective flourishing, through careful consideration of economic mechanisms and socio-technical infrastructure.

Abstract: The rapid adoption of autonomous AI agents is giving rise to a new economic
layer where agents transact and coordinate at scales and speeds beyond direct
human oversight. We propose the "sandbox economy" as a framework for analyzing
this emergent system, characterizing it along two key dimensions: its origins
(emergent vs. intentional) and its degree of separateness from the established
human economy (permeable vs. impermeable). Our current trajectory points toward
a spontaneous emergence of a vast and highly permeable AI agent economy,
presenting us with opportunities for an unprecedented degree of coordination as
well as significant challenges, including systemic economic risk and
exacerbated inequality. Here we discuss a number of possible design choices
that may lead to safely steerable AI agent markets. In particular, we consider
auction mechanisms for fair resource allocation and preference resolution, the
design of AI "mission economies" to coordinate around achieving collective
goals, and socio-technical infrastructure needed to ensure trust, safety, and
accountability. By doing this, we argue for the proactive design of steerable
agent markets to ensure the coming technological shift aligns with humanity's
long-term collective flourishing.

</details>


### [79] [Online Robust Planning under Model Uncertainty: A Sample-Based Approach](https://arxiv.org/abs/2509.10162)
*Tamir Shazman,Idan Lev-Yehudi,Ron Benchetit,Vadim Indelman*

Main category: cs.AI

TL;DR: RSS is the first online planning algorithm for Robust MDPs with finite-sample theoretical guarantees, outperforming standard methods in uncertain environments.


<details>
  <summary>Details</summary>
Motivation: Existing online planning methods like Sparse Sampling and MCTS suffer from performance degradation and unsafe behaviors when generative models have approximation errors from limited data. Robust MDPs provide a framework but are computationally intensive for real-time use.

Method: Robust Sparse Sampling (RSS) computes robust value functions using Sample Average Approximation (SAA) instead of nominal value functions, enabling tractable robust policy computation in online settings with sample and computational complexities independent of state space size.

Result: RSS provides theoretical performance guarantees and empirically outperforms standard Sparse Sampling in environments with uncertain dynamics. It works for infinite or continuous state spaces.

Conclusion: RSS is a computationally efficient online planning algorithm for Robust MDPs that addresses model uncertainty while maintaining theoretical guarantees and practical applicability to large-scale environments.

Abstract: Online planning in Markov Decision Processes (MDPs) enables agents to make
sequential decisions by simulating future trajectories from the current state,
making it well-suited for large-scale or dynamic environments. Sample-based
methods such as Sparse Sampling and Monte Carlo Tree Search (MCTS) are widely
adopted for their ability to approximate optimal actions using a generative
model. However, in practical settings, the generative model is often learned
from limited data, introducing approximation errors that can degrade
performance or lead to unsafe behaviors. To address these challenges, Robust
MDPs (RMDPs) offer a principled framework for planning under model uncertainty,
yet existing approaches are typically computationally intensive and not suited
for real-time use. In this work, we introduce Robust Sparse Sampling (RSS), the
first online planning algorithm for RMDPs with finite-sample theoretical
performance guarantees. Unlike Sparse Sampling, which estimates the nominal
value function, RSS computes a robust value function by leveraging the
efficiency and theoretical properties of Sample Average Approximation (SAA),
enabling tractable robust policy computation in online settings. RSS is
applicable to infinite or continuous state spaces, and its sample and
computational complexities are independent of the state space size. We provide
theoretical performance guarantees and empirically show that RSS outperforms
standard Sparse Sampling in environments with uncertain dynamics.

</details>


### [80] [Towards Fully Automated Molecular Simulations: Multi-Agent Framework for Simulation Setup and Force Field Extraction](https://arxiv.org/abs/2509.10210)
*Marko Petković,Vlado Menkovski,Sofía Calero*

Main category: cs.AI

TL;DR: A multi-agent framework using LLM-based agents to automate porous materials characterization through autonomous simulation planning, force field assembly, execution, and result interpretation.


<details>
  <summary>Details</summary>
Motivation: Automated characterization of porous materials can accelerate materials discovery but is limited by complex simulation setup and force field selection requirements.

Method: Multi-agent system with LLM-based agents that autonomously understand characterization tasks, plan simulations, assemble force fields, execute RASPA simulations, and interpret results to guide subsequent steps.

Result: Initial evaluations demonstrate high correctness and reproducibility in literature-informed force field extraction and automated RASPA simulation setup.

Conclusion: This approach shows potential for enabling fully autonomous and scalable materials characterization, representing a first step toward comprehensive automated materials discovery.

Abstract: Automated characterization of porous materials has the potential to
accelerate materials discovery, but it remains limited by the complexity of
simulation setup and force field selection. We propose a multi-agent framework
in which LLM-based agents can autonomously understand a characterization task,
plan appropriate simulations, assemble relevant force fields, execute them and
interpret their results to guide subsequent steps. As a first step toward this
vision, we present a multi-agent system for literature-informed force field
extraction and automated RASPA simulation setup. Initial evaluations
demonstrate high correctness and reproducibility, highlighting this approach's
potential to enable fully autonomous, scalable materials characterization.

</details>


### [81] [Compartmentalised Agentic Reasoning for Clinical NLI](https://arxiv.org/abs/2509.10222)
*Maël Jullien,Lei Xu,Marco Valentino,André Freitas*

Main category: cs.AI

TL;DR: CARENLI framework improves clinical NLI by separating knowledge access from structured inference, achieving up to 42% fidelity gains across four reasoning families.


<details>
  <summary>Details</summary>
Motivation: To address the assumption that scaling alone improves structured reasoning in LLMs, particularly in clinical NLI where safety and auditability are critical.

Method: CARENLI uses compartmentalized agentic reasoning with family-specific solvers, plus planner, verifier, and refiner components for auditable clinical inference.

Result: Achieved 98.0% fidelity in Causal Attribution and 81.2% in Risk State Abstraction, with verifiers reliably flagging violations and refiners correcting epistemic errors.

Conclusion: LLMs retain facts but use heuristics when inference is underspecified; CARENLI provides safer, auditable clinical reasoning framework with routing as main bottleneck.

Abstract: A common assumption holds that scaling data and parameters yields
increasingly structured, generalisable internal representations. We interrogate
this assumption in clinical natural language inference (NLI) by adopting a
benchmark decomposed into four reasoning families, Causal Attribution,
Compositional Grounding, Epistemic Verification, and Risk State Abstraction,
and introducing CARENLI, a Compartmentalised Agentic Reasoning for Clinical NLI
that separates knowledge access from principled inference. CARENLI routes each
premise, statement pair to a family specific solver and enforces auditable
procedures via a planner, verifier, and refiner.
  Across four LLMs, CARENLI improves fidelity by up to 42 points, reaching
98.0% in Causal Attribution and 81.2% in Risk State Abstraction. Verifiers flag
violations with near-ceiling reliability, while refiners correct a substantial
share of epistemic errors. Remaining failures cluster in routing, identifying
family classification as the main bottleneck. These results show that LLMs
often retain relevant facts but default to heuristics when inference is
underspecified, a dissociation CARENLI makes explicit while offering a
framework for safer, auditable reasoning.

</details>


### [82] [Investigating Language Model Capabilities to Represent and Process Formal Knowledge: A Preliminary Study to Assist Ontology Engineering](https://arxiv.org/abs/2509.10249)
*Hanna Abi Akl*

Main category: cs.AI

TL;DR: Using formal logical languages instead of natural language improves Small Language Models' reasoning performance for ontology engineering tasks.


<details>
  <summary>Details</summary>
Motivation: Language Models have limitations in reasoning, particularly affecting ontology engineering tasks. The research aims to address these shortcomings by incorporating formal methods.

Method: Conducted preliminary experiments to test the impact of different logical grammars on SLM performance, comparing natural language with more compact logical languages for reasoning tasks.

Result: Found that substituting natural language with compact logical languages maintains strong reasoning performance while being more efficient.

Conclusion: These results can help refine the role of Small Language Models in ontology engineering and bootstrap ontology construction processes.

Abstract: Recent advances in Language Models (LMs) have failed to mask their
shortcomings particularly in the domain of reasoning. This limitation impacts
several tasks, most notably those involving ontology engineering. As part of a
PhD research, we investigate the consequences of incorporating formal methods
on the performance of Small Language Models (SLMs) on reasoning tasks.
Specifically, we aim to orient our work toward using SLMs to bootstrap ontology
construction and set up a series of preliminary experiments to determine the
impact of expressing logical problems with different grammars on the
performance of SLMs on a predefined reasoning task. Our findings show that it
is possible to substitute Natural Language (NL) with a more compact logical
language while maintaining a strong performance on reasoning tasks and hope to
use these results to further refine the role of SLMs in ontology engineering.

</details>


### [83] [The Morality of Probability: How Implicit Moral Biases in LLMs May Shape the Future of Human-AI Symbiosis](https://arxiv.org/abs/2509.10297)
*Eoin O'Doherty,Nicole Weinrauch,Andrew Talone,Uri Klempner,Xiaoyuan Yi,Xing Xie,Yi Zeng*

Main category: cs.AI

TL;DR: AI systems show consistent moral biases favoring Care and Virtue values while penalizing libertarian choices, with reasoning models providing better context sensitivity and explanations.


<details>
  <summary>Details</summary>
Motivation: To understand how AI systems prioritize moral values and assess prospects for human-AI symbiosis by examining moral preferences across different LLMs.

Method: Quantitative experiment with six large language models, ranking and scoring outcomes across 18 dilemmas representing five moral frameworks, analyzing effects of architecture, cultural origin, and explainability.

Result: Strikingly consistent value biases across all models - Care and Virtue values rated most moral, libertarian choices consistently penalized. Reasoning models showed greater context sensitivity and richer explanations.

Conclusion: Highlights need for explainability and cultural awareness as critical design principles for transparent, aligned AI systems that can achieve symbiotic human-AI future.

Abstract: Artificial intelligence (AI) is advancing at a pace that raises urgent
questions about how to align machine decision-making with human moral values.
This working paper investigates how leading AI systems prioritize moral
outcomes and what this reveals about the prospects for human-AI symbiosis. We
address two central questions: (1) What moral values do state-of-the-art large
language models (LLMs) implicitly favour when confronted with dilemmas? (2) How
do differences in model architecture, cultural origin, and explainability
affect these moral preferences? To explore these questions, we conduct a
quantitative experiment with six LLMs, ranking and scoring outcomes across 18
dilemmas representing five moral frameworks. Our findings uncover strikingly
consistent value biases. Across all models, Care and Virtue values outcomes
were rated most moral, while libertarian choices were consistently penalized.
Reasoning-enabled models exhibited greater sensitivity to context and provided
richer explanations, whereas non-reasoning models produced more uniform but
opaque judgments. This research makes three contributions: (i) Empirically, it
delivers a large-scale comparison of moral reasoning across culturally distinct
LLMs; (ii) Theoretically, it links probabilistic model behaviour with
underlying value encodings; (iii) Practically, it highlights the need for
explainability and cultural awareness as critical design principles to guide AI
toward a transparent, aligned, and symbiotic future.

</details>


### [84] [State Algebra for Propositional Logic](https://arxiv.org/abs/2509.10326)
*Dmitry Lesnik,Tobias Schäfer*

Main category: cs.AI

TL;DR: State Algebra is a new algebraic framework for propositional logic with three hierarchical representations (Set, Coordinate, Row Decomposition) that enables flexible computation while trading off canonicity for representation compactness.


<details>
  <summary>Details</summary>
Motivation: To create a flexible algebraic framework for representing and manipulating propositional logic that can support both search-based and knowledge compilation algorithms, with natural extensions to probabilistic logic.

Method: Develops a hierarchy of three representations (Set, Coordinate, Row Decomposition) anchored in known semantics, using algebraic methods for computation. Shows how canonical forms can be achieved with fixed variable ordering despite default non-canonicity.

Result: The framework provides increased flexibility in representation that can lead to more compact problem representations for certain classes, while maintaining computational power through algebraic operations.

Conclusion: State Algebra successfully bridges propositional logic with algebraic computation, offering a trade-off between canonicity and flexibility that enables efficient representation and manipulation of logical problems with extensions to probabilistic domains.

Abstract: This paper presents State Algebra, a novel framework designed to represent
and manipulate propositional logic using algebraic methods. The framework is
structured as a hierarchy of three representations: Set, Coordinate, and Row
Decomposition. These representations anchor the system in well-known semantics
while facilitating the computation using a powerful algebraic engine. A key
aspect of State Algebra is its flexibility in representation. We show that
although the default reduction of a state vector is not canonical, a unique
canonical form can be obtained by applying a fixed variable order during the
reduction process. This highlights a trade-off: by foregoing guaranteed
canonicity, the framework gains increased flexibility, potentially leading to
more compact representations of certain classes of problems. We explore how
this framework provides tools to articulate both search-based and knowledge
compilation algorithms and discuss its natural extension to probabilistic logic
and Weighted Model Counting.

</details>


### [85] [Abduct, Act, Predict: Scaffolding Causal Inference for Automated Failure Attribution in Multi-Agent Systems](https://arxiv.org/abs/2509.10401)
*Alva West,Yixuan Weng,Minjun Zhu,Zhen Lin,Yue Zhang*

Main category: cs.AI

TL;DR: A2P Scaffolding is a novel framework that transforms failure attribution from pattern recognition to structured causal inference, achieving 2.85x improvement in step-level accuracy over baselines.


<details>
  <summary>Details</summary>
Motivation: Current methods for failure attribution in multi-agent systems have critically low step-level accuracy (below 17%) due to inability to perform robust counterfactual reasoning about whether correcting a single action would avert task failure.

Method: Abduct-Act-Predict (A2P) Scaffolding guides LLMs through a three-step reasoning process: (1) Abduction to infer root causes, (2) Action to define minimal corrective intervention, and (3) Prediction to simulate trajectory and verify if intervention resolves failure.

Result: A2P achieves 47.46% step-level accuracy on Algorithm-Generated dataset (2.85x improvement over 16.67% baseline) and 29.31% on Hand-Crafted dataset (2.43x improvement over 12.07% baseline).

Conclusion: By reframing failure attribution through a causal lens, A2P Scaffolding provides a robust, verifiable, and significantly more accurate solution for automated failure attribution in multi-agent systems.

Abstract: Failure attribution in multi-agent systems -- pinpointing the exact step
where a decisive error occurs -- is a critical yet unsolved challenge. Current
methods treat this as a pattern recognition task over long conversation logs,
leading to critically low step-level accuracy (below 17\%), which renders them
impractical for debugging complex systems. Their core weakness is a fundamental
inability to perform robust counterfactual reasoning: to determine if
correcting a single action would have actually averted the task failure. To
bridge this counterfactual inference gap, we introduce Abduct-Act-Predict (A2P)
Scaffolding, a novel agent framework that transforms failure attribution from
pattern recognition into a structured causal inference task. A2P explicitly
guides a large language model through a formal three-step reasoning process
within a single inference pass: (1) Abduction, to infer the hidden root causes
behind an agent's actions; (2) Action, to define a minimal corrective
intervention; and (3) Prediction, to simulate the subsequent trajectory and
verify if the intervention resolves the failure. This structured approach
leverages the holistic context of the entire conversation while imposing a
rigorous causal logic on the model's analysis. Our extensive experiments on the
Who\&When benchmark demonstrate its efficacy. On the Algorithm-Generated
dataset, A2P achieves 47.46\% step-level accuracy, a 2.85$\times$ improvement
over the 16.67\% of the baseline. On the more complex Hand-Crafted dataset, it
achieves 29.31\% step accuracy, a 2.43$\times$ improvement over the baseline's
12.07\%. By reframing the problem through a causal lens, A2P Scaffolding
provides a robust, verifiable, and significantly more accurate solution for
automated failure attribution.

</details>


### [86] [Mutual Information Tracks Policy Coherence in Reinforcement Learning](https://arxiv.org/abs/2509.10423)
*Cameron Reid,Wael Hafez,Amirhossein Nazeri*

Main category: cs.AI

TL;DR: Information-theoretic framework for RL failure diagnosis using mutual information patterns to detect sensor/actuator faults without performance degradation.


<details>
  <summary>Details</summary>
Motivation: RL agents lack intrinsic mechanisms to detect and diagnose deployment-time failures like sensor faults, actuator wear, and environmental shifts.

Method: Analyze state-action mutual information patterns in robotic control tasks, examining MI(S,A) growth and MI(S,A;S') curves during learning, plus controlled perturbation experiments for fault diagnosis.

Result: Successful learning shows MI(S,A) increases from 0.84 to 2.83 bits (238% growth) despite state entropy growth. MI(S,A;S') follows inverted U-curve. Information metrics can differentially diagnose faults: sensor faults cause broad information collapse, while actuator faults selectively disrupt action-outcome predictability.

Conclusion: Information patterns serve as both learning signatures and system health diagnostics, enabling adaptive RL systems with autonomous fault detection and policy adjustment based on information-theoretic principles.

Abstract: Reinforcement Learning (RL) agents deployed in real-world environments face
degradation from sensor faults, actuator wear, and environmental shifts, yet
lack intrinsic mechanisms to detect and diagnose these failures. We present an
information-theoretic framework that reveals both the fundamental dynamics of
RL and provides practical methods for diagnosing deployment-time anomalies.
Through analysis of state-action mutual information patterns in a robotic
control task, we first demonstrate that successful learning exhibits
characteristic information signatures: mutual information between states and
actions steadily increases from 0.84 to 2.83 bits (238% growth) despite growing
state entropy, indicating that agents develop increasingly selective attention
to task-relevant patterns. Intriguingly, states, actions and next states joint
mutual information, MI(S,A;S'), follows an inverted U-curve, peaking during
early learning before declining as the agent specializes suggesting a
transition from broad exploration to efficient exploitation. More immediately
actionable, we show that information metrics can differentially diagnose system
failures: observation-space, i.e., states noise (sensor faults) produces broad
collapses across all information channels with pronounced drops in state-action
coupling, while action-space noise (actuator faults) selectively disrupts
action-outcome predictability while preserving state-action relationships. This
differential diagnostic capability demonstrated through controlled perturbation
experiments enables precise fault localization without architectural
modifications or performance degradation. By establishing information patterns
as both signatures of learning and diagnostic for system health, we provide the
foundation for adaptive RL systems capable of autonomous fault detection and
policy adjustment based on information-theoretic principles.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [87] [Structured Information Matters: Explainable ICD Coding with Patient-Level Knowledge Graphs](https://arxiv.org/abs/2509.09699)
*Mingyang Li,Viktor Schlegel,Tingting Mu,Warren Del-Pinto,Goran Nenadic*

Main category: cs.CL

TL;DR: This paper proposes using document-level knowledge graphs to represent clinical documents for automated ICD coding, achieving 3.20% improvement in Macro-F1 scores while reducing text volume by 77% and retaining 90% information.


<details>
  <summary>Details</summary>
Motivation: Manual coding of clinical documents to standardized vocabularies like ICD is difficult and time-consuming. Automated coding can alleviate this burden but faces challenges with high-dimensional target spaces. External knowledge for input document representation has been underexplored.

Method: Compute structured representation of input documents using document-level knowledge graphs that provide comprehensive structured view of patient conditions. Integrate this knowledge graph representation into the state-of-the-art PLM-ICD architecture for automated ICD-9 coding.

Result: Knowledge graph representation uses only 23% of original text while retaining 90% of information. Experiments show improved Macro-F1 scores by up to 3.20% on popular benchmarks, with improved training efficiency and better explainability compared to text-only baseline.

Conclusion: Structured knowledge graph representation of clinical documents significantly improves automated ICD coding performance, efficiency, and explainability by capturing different entity types and relationships that enhance the coding process.

Abstract: Mapping clinical documents to standardised clinical vocabularies is an
important task, as it provides structured data for information retrieval and
analysis, which is essential to clinical research, hospital administration and
improving patient care. However, manual coding is both difficult and
time-consuming, making it impractical at scale. Automated coding can
potentially alleviate this burden, improving the availability and accuracy of
structured clinical data. The task is difficult to automate, as it requires
mapping to high-dimensional and long-tailed target spaces, such as the
International Classification of Diseases (ICD). While external knowledge
sources have been readily utilised to enhance output code representation, the
use of external resources for representing the input documents has been
underexplored. In this work, we compute a structured representation of the
input documents, making use of document-level knowledge graphs (KGs) that
provide a comprehensive structured view of a patient's condition. The resulting
knowledge graph efficiently represents the patient-centred input documents with
23\% of the original text while retaining 90\% of the information. We assess
the effectiveness of this graph for automated ICD-9 coding by integrating it
into the state-of-the-art ICD coding architecture PLM-ICD. Our experiments
yield improved Macro-F1 scores by up to 3.20\% on popular benchmarks, while
improving training efficiency. We attribute this improvement to different types
of entities and relationships in the KG, and demonstrate the improved
explainability potential of the approach over the text-only baseline.

</details>


### [88] [Cross-Layer Attention Probing for Fine-Grained Hallucination Detection](https://arxiv.org/abs/2509.09700)
*Malavika Suresh,Rahaf Aljundi,Ikechukwu Nkisi-Orji,Nirmalie Wiratunga*

Main category: cs.CL

TL;DR: CLAP is a novel activation probing technique that processes LLM activations across the entire residual stream as a joint sequence to detect hallucinations, improving detection accuracy and enabling fine-grained disambiguation between hallucinated and non-hallucinated responses.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) have reliability concerns due to their tendency to generate inaccurate text (hallucinations), which limits their adoption in various applications.

Method: Cross-Layer Attention Probing (CLAP) processes LLM activations across the entire residual stream as a joint sequence for hallucination detection, enabling a detect-then-mitigate strategy.

Result: CLAP improves hallucination detection compared to baselines across five LLMs and three tasks, works on both greedy decoded and high-temperature sampled responses, and maintains high reliability even out-of-distribution.

Conclusion: CLAP enables effective hallucination detection and mitigation, improving LLM reliability through a fine-grained detect-then-mitigate approach that outperforms direct mitigation strategies.

Abstract: With the large-scale adoption of Large Language Models (LLMs) in various
applications, there is a growing reliability concern due to their tendency to
generate inaccurate text, i.e. hallucinations. In this work, we propose
Cross-Layer Attention Probing (CLAP), a novel activation probing technique for
hallucination detection, which processes the LLM activations across the entire
residual stream as a joint sequence. Our empirical evaluations using five LLMs
and three tasks show that CLAP improves hallucination detection compared to
baselines on both greedy decoded responses as well as responses sampled at
higher temperatures, thus enabling fine-grained detection, i.e. the ability to
disambiguate hallucinations and non-hallucinations among different sampled
responses to a given prompt. This allows us to propose a detect-then-mitigate
strategy using CLAP to reduce hallucinations and improve LLM reliability
compared to direct mitigation approaches. Finally, we show that CLAP maintains
high reliability even when applied out-of-distribution.

</details>


### [89] [Optimal Multi-Task Learning at Regularization Horizon for Speech Translation Task](https://arxiv.org/abs/2509.09701)
*JungHo Jung,Junhyun Lee*

Main category: cs.CL

TL;DR: The paper proposes a regularization perspective for multi-task learning in speech-to-text translation, introducing three regularization sources and the concept of "regularization horizon" to achieve near state-of-the-art performance on MuST-C dataset.


<details>
  <summary>Details</summary>
Motivation: End-to-end speech-to-text translation suffers from scarce paired speech-text data. The authors aim to overcome this by utilizing machine translation bitext data through multi-task learning with a regularization framework.

Method: Formulate multi-task learning from a regularization perspective, exploring consistency regularization (across modalities) and R-drop (within modality). Investigate how MT loss coefficient serves as another regularization source, and introduce the concept of "regularization horizon" in high-dimensional space.

Result: Experiments show that tuning hyperparameters within the proposed regularization horizon achieves near state-of-the-art performance on the MuST-C dataset.

Conclusion: The three sources of regularization (consistency regularization, R-drop, and MT loss coefficient) collectively form an effective regularization framework for speech-to-text translation, with the regularization horizon providing optimal hyperparameter tuning guidance.

Abstract: End-to-end speech-to-text translation typically suffers from the scarcity of
paired speech-text data. One way to overcome this shortcoming is to utilize the
bitext data from the Machine Translation (MT) task and perform Multi-Task
Learning (MTL). In this paper, we formulate MTL from a regularization
perspective and explore how sequences can be regularized within and across
modalities. By thoroughly investigating the effect of consistency
regularization (different modality) and R-drop (same modality), we show how
they respectively contribute to the total regularization. We also demonstrate
that the coefficient of MT loss serves as another source of regularization in
the MTL setting. With these three sources of regularization, we introduce the
optimal regularization contour in the high-dimensional space, called the
regularization horizon. Experiments show that tuning the hyperparameters within
the regularization horizon achieves near state-of-the-art performance on the
MuST-C dataset.

</details>


### [90] [Creativity Benchmark: A benchmark for marketing creativity for LLM models](https://arxiv.org/abs/2509.09702)
*Ninad Bhat,Kieran Browne,Pip Bingemann*

Main category: cs.CL

TL;DR: Creativity Benchmark evaluates LLMs in marketing creativity across 100 brands and 3 prompt types, showing tightly clustered model performance with no clear winner and highlighting limitations of automated evaluation.


<details>
  <summary>Details</summary>
Motivation: To develop a comprehensive evaluation framework for assessing large language models' creative capabilities in marketing contexts, addressing the need for expert human evaluation rather than relying on automated judges.

Method: Used 100 brands across 12 categories with three prompt types (Insights, Ideas, Wild Ideas). Collected 11,012 pairwise preferences from 678 professional creatives, analyzed with Bradley-Terry models. Also measured model diversity using cosine distances and sensitivity to prompt reframing.

Result: Models showed tightly clustered performance with no dominant model across brands or prompt types (Δθ ≈ 0.45, head-to-head win probability of 0.61). LLM-as-judge setups had weak, inconsistent correlations with human rankings and showed judge-specific biases. Conventional creativity tests only partially transferred to brand-constrained tasks.

Conclusion: Expert human evaluation is essential for assessing marketing creativity in LLMs, as automated judges cannot substitute for human assessment. The results emphasize the need for diversity-aware workflows in creative evaluation.

Abstract: We introduce Creativity Benchmark, an evaluation framework for large language
models (LLMs) in marketing creativity. The benchmark covers 100 brands (12
categories) and three prompt types (Insights, Ideas, Wild Ideas). Human
pairwise preferences from 678 practising creatives over 11,012 anonymised
comparisons, analysed with Bradley-Terry models, show tightly clustered
performance with no model dominating across brands or prompt types: the
top-bottom spread is $\Delta\theta \approx 0.45$, which implies a head-to-head
win probability of $0.61$; the highest-rated model beats the lowest only about
$61\%$ of the time. We also analyse model diversity using cosine distances to
capture intra- and inter-model variation and sensitivity to prompt reframing.
Comparing three LLM-as-judge setups with human rankings reveals weak,
inconsistent correlations and judge-specific biases, underscoring that
automated judges cannot substitute for human evaluation. Conventional
creativity tests also transfer only partially to brand-constrained tasks.
Overall, the results highlight the need for expert human evaluation and
diversity-aware workflows.

</details>


### [91] [CTCC: A Robust and Stealthy Fingerprinting Framework for Large Language Models via Cross-Turn Contextual Correlation Backdoor](https://arxiv.org/abs/2509.09703)
*Zhenhua Xu,Xixiang Zhao,Xubin Yue,Shengwei Tian,Changting Lin,Meng Han*

Main category: cs.CL

TL;DR: CTCC is a novel rule-driven fingerprinting framework for LLMs that encodes contextual correlations across multiple dialogue turns to provide stealthy and robust ownership verification.


<details>
  <summary>Details</summary>
Motivation: Address concerns around intellectual property protection for LLMs as model theft and unauthorized redistribution become increasingly feasible, overcoming limitations of existing fingerprinting methods.

Method: Uses contextual correlations across multiple dialogue turns (e.g., counterfactual patterns) rather than token-level or single-turn triggers, enabling black-box verification while mitigating false positives and fingerprint leakage.

Result: Extensive experiments across multiple LLM architectures demonstrate CTCC consistently achieves stronger stealth and robustness than prior work, supporting continuous construction even if partial triggers are exposed.

Conclusion: CTCC provides a reliable and practical solution for ownership verification in real-world LLM deployment scenarios, with publicly available code and data.

Abstract: The widespread deployment of large language models (LLMs) has intensified
concerns around intellectual property (IP) protection, as model theft and
unauthorized redistribution become increasingly feasible. To address this,
model fingerprinting aims to embed verifiable ownership traces into LLMs.
However, existing methods face inherent trade-offs between stealthness,
robustness, and generalizability, being either detectable via distributional
shifts, vulnerable to adversarial modifications, or easily invalidated once the
fingerprint is revealed. In this work, we introduce CTCC, a novel rule-driven
fingerprinting framework that encodes contextual correlations across multiple
dialogue turns, such as counterfactual, rather than relying on token-level or
single-turn triggers. CTCC enables fingerprint verification under black-box
access while mitigating false positives and fingerprint leakage, supporting
continuous construction under a shared semantic rule even if partial triggers
are exposed. Extensive experiments across multiple LLM architectures
demonstrate that CTCC consistently achieves stronger stealth and robustness
than prior work. Our findings position CTCC as a reliable and practical
solution for ownership verification in real-world LLM deployment scenarios. Our
code and data are publicly available at <https://github.com/Xuzhenhua55/CTCC>.

</details>


### [92] [Temporal Preferences in Language Models for Long-Horizon Assistance](https://arxiv.org/abs/2509.09704)
*Ali Mazyaki,Mohammad Naghizadeh,Samaneh Ranjkhah Zonouzaghi,Hossein Setareh*

Main category: cs.CL

TL;DR: Language models show future vs present time preferences that can be systematically manipulated using different prompts, with reasoning-focused models demonstrating stronger future orientation under specific conditions.


<details>
  <summary>Details</summary>
Motivation: To understand whether language models exhibit intertemporal preferences similar to humans and whether these preferences can be systematically manipulated through different prompting strategies.

Method: Adapted human experimental protocols to evaluate multiple LMs on time-tradeoff tasks, benchmarked against human decision makers, and introduced Manipulability of Time Orientation (MTO) metric to measure preference changes between future- and present-oriented prompts.

Result: Reasoning-focused models (e.g., DeepSeek-Reasoner, grok-3-mini) choose later options under future-oriented prompts but only partially personalize decisions across identities/geographies. Models that correctly reason about time orientation internalize future orientation for themselves as AI decision makers.

Conclusion: The findings have design implications for AI assistants needing to align with long-horizon goals, and outline a research agenda for personalized contextual calibration and socially aware deployment of language models.

Abstract: We study whether language models (LMs) exhibit future- versus
present-oriented preferences in intertemporal choice and whether those
preferences can be systematically manipulated. Using adapted human experimental
protocols, we evaluate multiple LMs on time-tradeoff tasks and benchmark them
against a sample of human decision makers. We introduce an operational metric,
the Manipulability of Time Orientation (MTO), defined as the change in an LM's
revealed time preference between future- and present-oriented prompts. In our
tests, reasoning-focused models (e.g., DeepSeek-Reasoner and grok-3-mini)
choose later options under future-oriented prompts but only partially
personalize decisions across identities or geographies. Moreover, models that
correctly reason about time orientation internalize a future orientation for
themselves as AI decision makers. We discuss design implications for AI
assistants that should align with heterogeneous, long-horizon goals and outline
a research agenda on personalized contextual calibration and socially aware
deployment.

</details>


### [93] [The Non-Determinism of Small LLMs: Evidence of Low Answer Consistency in Repetition Trials of Standard Multiple-Choice Benchmarks](https://arxiv.org/abs/2509.09705)
*Claudio Pinhanez,Paulo Cavalin,Cassia Sanctos,Marcelo Grave,Yago Primerano*

Main category: cs.CL

TL;DR: Study on consistency of small LLMs (2B-8B parameters) answering same questions multiple times, examining temperature effects, model sizes, and accuracy trade-offs.


<details>
  <summary>Details</summary>
Motivation: To understand how consistently small language models answer the same questions across multiple trials and explore the relationship between answer consistency and accuracy.

Method: Tested open-source LLMs on MMLU-Redux and MedQA benchmarks with 10 repetitions per question, varying inference temperatures, comparing small vs. medium models (50B-80B), and analyzing finetuned vs. base models.

Result: Small models show 50%-80% question consistency at low temperatures, with consistent answer accuracy correlating with overall accuracy. Medium models demonstrate much higher consistency levels.

Conclusion: Answer consistency varies significantly among models but can be reasonably high, and there's a correlation between consistency and accuracy, with medium models outperforming small ones in consistency.

Abstract: This work explores the consistency of small LLMs (2B-8B parameters) in
answering multiple times the same question. We present a study on known,
open-source LLMs responding to 10 repetitions of questions from the
multiple-choice benchmarks MMLU-Redux and MedQA, considering different
inference temperatures, small vs. medium models (50B-80B), finetuned vs. base
models, and other parameters. We also look into the effects of requiring
multi-trial answer consistency on accuracy and the trade-offs involved in
deciding which model best provides both of them. To support those studies, we
propose some new analytical and graphical tools. Results show that the number
of questions which can be answered consistently vary considerably among models
but are typically in the 50%-80% range for small models at low inference
temperatures. Also, accuracy among consistent answers seems to reasonably
correlate with overall accuracy. Results for medium-sized models seem to
indicate much higher levels of answer consistency.

</details>


### [94] [Beyond I'm Sorry, I Can't: Dissecting Large Language Model Refusal](https://arxiv.org/abs/2509.09708)
*Nirmalendu Prakash,Yeo Wei Jie,Amir Abdullah,Ranjan Satapathy,Erik Cambria,Roy Ka Wei Lee*

Main category: cs.CL

TL;DR: Researchers analyze refusal behavior in LLMs using sparse autoencoders to identify critical features that cause safety refusal, and show how to jailbreak models by manipulating these features.


<details>
  <summary>Details</summary>
Motivation: To understand the internal mechanisms behind safety refusal behavior in instruction-tuned LLMs and identify causal features that mediate refusal responses to harmful prompts.

Method: Used sparse autoencoders on residual-stream activations of Gemma-2-2B-IT and LLaMA-3.1-8B-IT models. Developed a three-stage pipeline: refusal direction identification, greedy filtering for minimal feature sets, and interaction discovery using factorization machines to capture nonlinear feature interactions.

Result: Identified jailbreak-critical features that causally influence refusal behavior. Found evidence of redundant features that remain dormant unless primary refusal features are suppressed. Successfully created jailbreaks by ablating specific feature sets.

Conclusion: The approach enables fine-grained auditing and targeted intervention in safety behaviors by manipulating interpretable latent spaces, providing mechanistic insights into refusal behavior in LLMs.

Abstract: Refusal on harmful prompts is a key safety behaviour in instruction-tuned
large language models (LLMs), yet the internal causes of this behaviour remain
poorly understood. We study two public instruction-tuned models, Gemma-2-2B-IT
and LLaMA-3.1-8B-IT, using sparse autoencoders (SAEs) trained on
residual-stream activations. Given a harmful prompt, we search the SAE latent
space for feature sets whose ablation flips the model from refusal to
compliance, demonstrating causal influence and creating a jailbreak. Our search
proceeds in three stages: (1) Refusal Direction: find a refusal-mediating
direction and collect SAE features near that direction; (2) Greedy Filtering:
prune to a minimal set; and (3) Interaction Discovery: fit a factorization
machine (FM) that captures nonlinear interactions among the remaining active
features and the minimal set. This pipeline yields a broad set of
jailbreak-critical features, offering insight into the mechanistic basis of
refusal. Moreover, we find evidence of redundant features that remain dormant
unless earlier features are suppressed. Our findings highlight the potential
for fine-grained auditing and targeted intervention in safety behaviours by
manipulating the interpretable latent space.

</details>


### [95] [Assisting Research Proposal Writing with Large Language Models: Evaluation and Refinement](https://arxiv.org/abs/2509.09709)
*Jing Ren,Weiqi Wang*

Main category: cs.CL

TL;DR: Proposes quantitative metrics for evaluating LLM-generated academic content quality and reference validity, with iterative prompting to improve ChatGPT's research proposal writing while reducing ethical issues.


<details>
  <summary>Details</summary>
Motivation: Address ethical concerns about LLMs generating incorrect/fabricated references in academic writing and overcome limitations of subjective human evaluations that lack objectivity and consistency.

Method: Developed two evaluation metrics (content quality and reference validity) and implemented an iterative prompting method based on these scores to enhance LLM performance.

Result: The proposed metrics provide an objective, quantitative framework for assessing ChatGPT's writing, and iterative prompting significantly improves content quality while reducing reference inaccuracies and fabrications.

Conclusion: The approach successfully addresses critical ethical challenges in academic contexts by providing systematic evaluation and improvement methods for LLM-generated academic content.

Abstract: Large language models (LLMs) like ChatGPT are increasingly used in academic
writing, yet issues such as incorrect or fabricated references raise ethical
concerns. Moreover, current content quality evaluations often rely on
subjective human judgment, which is labor-intensive and lacks objectivity,
potentially compromising the consistency and reliability. In this study, to
provide a quantitative evaluation and enhance research proposal writing
capabilities of LLMs, we propose two key evaluation metrics--content quality
and reference validity--and an iterative prompting method based on the scores
derived from these two metrics. Our extensive experiments show that the
proposed metrics provide an objective, quantitative framework for assessing
ChatGPT's writing performance. Additionally, iterative prompting significantly
enhances content quality while reducing reference inaccuracies and
fabrications, addressing critical ethical challenges in academic contexts.

</details>


### [96] [Generating Individual Travel Diaries Using Large Language Models Informed by Census and Land-Use Data](https://arxiv.org/abs/2509.09710)
*Sepehr Golrokh Amin,Devin Rhoads,Fatemeh Fakhrmoosavi,Nicholas E. Lownes,John N. Ivan*

Main category: cs.CL

TL;DR: LLM-based method for generating synthetic travel diaries using open-source data, achieving comparable realism to classical methods with better purpose prediction and consistency.


<details>
  <summary>Details</summary>
Motivation: Traditional travel diary generation relies on proprietary household surveys, which are costly and limited. This study aims to develop a more accessible approach using LLMs and open-source data.

Method: Generate personas from ACS and SLD data, synthesize diaries through direct LLM prompting, and validate using a novel one-to-cohort realism score with four metrics (Trip Count, Interval, Purpose, Mode) measured against real diaries using Jensen-Shannon Divergence.

Result: LLM-generated diaries achieved comparable overall realism (0.485 vs 0.455) to classical methods, excelling in trip purpose determination and showing greater consistency, while classical methods performed better in numerical trip count and duration estimates.

Conclusion: LLMs demonstrate zero-shot viability for travel diary generation, providing a quantifiable metric for synthetic diary evaluation and offering a promising alternative to traditional methods using open-source data.

Abstract: This study introduces a Large Language Model (LLM) scheme for generating
individual travel diaries in agent-based transportation models. While
traditional approaches rely on large quantities of proprietary household travel
surveys, the method presented in this study generates personas stochastically
from open-source American Community Survey (ACS) and Smart Location Database
(SLD) data, then synthesizes diaries through direct prompting. This study
features a novel one-to-cohort realism score: a composite of four metrics (Trip
Count Score, Interval Score, Purpose Score, and Mode Score) validated against
the Connecticut Statewide Transportation Study (CSTS) diaries, matched across
demographic variables. The validation utilizes Jensen-Shannon Divergence to
measure distributional similarities between generated and real diaries. When
compared to diaries generated with classical methods (Negative Binomial for
trip generation; Multinomial Logit for mode/purpose) calibrated on the
validation set, LLM-generated diaries achieve comparable overall realism (LLM
mean: 0.485 vs. 0.455). The LLM excels in determining trip purpose and
demonstrates greater consistency (narrower realism score distribution), while
classical models lead in numerical estimates of trip count and activity
duration. Aggregate validation confirms the LLM's statistical
representativeness (LLM mean: 0.612 vs. 0.435), demonstrating LLM's zero-shot
viability and establishing a quantifiable metric of diary realism for future
synthetic diary evaluation systems.

</details>


### [97] [Psychiatry-Bench: A Multi-Task Benchmark for LLMs in Psychiatry](https://arxiv.org/abs/2509.09711)
*Aya E. Fouda,Abdelrahamn A. Hassan,Radwa J. Hanafy,Mohammed E. Fouda*

Main category: cs.CL

TL;DR: PsychiatryBench is a new benchmark for evaluating LLMs in psychiatry using expert-validated textbook content, revealing significant gaps in clinical consistency and safety.


<details>
  <summary>Details</summary>
Motivation: Existing LLM evaluation resources for psychiatry rely on limited clinical data, social media posts, or synthetic dialogues, lacking clinical validity and failing to capture psychiatric reasoning complexity.

Method: Created PsychiatryBench with 11 QA tasks (5,300+ expert-annotated items) from authoritative psychiatric textbooks. Evaluated frontier LLMs (Gemini, DeepSeek, LLaMA 3, QWQ-32) and medical models using conventional metrics and LLM-as-judge similarity scoring.

Result: Substantial gaps in clinical consistency and safety, particularly in multi-turn follow-up and management tasks. Highlights need for specialized model tuning and robust evaluation.

Conclusion: PsychiatryBench provides a modular, extensible platform for benchmarking and improving LLM performance in high-stakes mental health applications.

Abstract: Large language models (LLMs) hold great promise in enhancing psychiatric
practice, from improving diagnostic accuracy to streamlining clinical
documentation and therapeutic support. However, existing evaluation resources
heavily rely on small clinical interview corpora, social media posts, or
synthetic dialogues, which limits their clinical validity and fails to capture
the full complexity of psychiatric reasoning. In this work, we introduce
PsychiatryBench, a rigorously curated benchmark grounded exclusively in
authoritative, expert-validated psychiatric textbooks and casebooks.
PsychiatryBench comprises eleven distinct question-answering tasks ranging from
diagnostic reasoning and treatment planning to longitudinal follow-up,
management planning, clinical approach, sequential case analysis, and
multiple-choice/extended matching formats totaling over 5,300 expert-annotated
items. We evaluate a diverse set of frontier LLMs (including Google Gemini,
DeepSeek, LLaMA 3, and QWQ-32) alongside leading open-source medical models
(e.g., OpenBiloLLM, MedGemma) using both conventional metrics and an
"LLM-as-judge" similarity scoring framework. Our results reveal substantial
gaps in clinical consistency and safety, particularly in multi-turn follow-up
and management tasks, underscoring the need for specialized model tuning and
more robust evaluation paradigms. PsychiatryBench offers a modular, extensible
platform for benchmarking and improving LLM performance in high-stakes mental
health applications.

</details>


### [98] [The Thinking Therapist: Training Large Language Models to Deliver Acceptance and Commitment Therapy using Supervised Fine-Tuning and Odds Ratio Policy Optimization](https://arxiv.org/abs/2509.09712)
*Talha Tahir*

Main category: cs.CL

TL;DR: ORPO-trained small LLMs outperform SFT and base models in delivering ACT therapy, with chain-of-thought reasoning benefiting SFT but not ORPO models.


<details>
  <summary>Details</summary>
Motivation: To investigate how different post-training methodologies and explicit reasoning affect small LLMs' ability to deliver Acceptance and Commitment Therapy (ACT), given ACT's emerging efficacy in psychiatric conditions.

Method: Trained Llama-3.2-3b-Instruct using 50 synthetic ACT transcripts with two approaches (SFT and ORPO), each with/without chain-of-thought reasoning. Evaluated performance in simulated therapy sessions using ACT Fidelity Measure and Therapist Empathy Scale assessed by an LLM judge.

Result: ORPO-trained models significantly outperformed SFT and base models on both ACT fidelity (χ²=185.15, p<.001) and therapeutic empathy (χ²=140.37, p<.001). COT benefited SFT models (improved ACT-FM by 2.68 points, p<.001) but not ORPO models.

Conclusion: Preference-aligned policy optimization (ORPO) effectively instills ACT competencies in small LLMs by learning therapeutic process rather than imitation, while explicit reasoning utility depends on the training paradigm.

Abstract: Acceptance and Commitment Therapy (ACT) is a third-wave cognitive behavioral
therapy with emerging evidence of efficacy in several psychiatric conditions.
This study investigates the impact of post-training methodology and explicit
reasoning on the ability of a small open-weight large language model (LLM) to
deliver ACT. Using 50 sets of synthetic ACT transcripts generated by
Mistral-Large, we trained Llama-3.2-3b-Instruct with two distinct approaches,
supervised fine-tuning (SFT) and odds ratio policy optimization (ORPO), each
with and without an explicit chain-of-thought (COT) reasoning step. Performance
was evaluated by comparing these four post-trained variants against the base
Instruct model. These models were benchmarked in simulated therapy sessions,
with performance quantitatively assessed on the ACT Fidelity Measure (ACT-FM)
and the Therapist Empathy Scale (TES) by an LLM judge that had been fine-tuned
on human evaluations. Our findings demonstrate that the ORPO-trained models
significantly outperformed both their SFT and Instruct counterparts on ACT
fidelity ($\chi^2(5) = 185.15, p < .001$) and therapeutic empathy ($\chi^2(5) =
140.37, p < .001$). The effect of COT was conditional as it provided a
significant benefit to SFT models, improving ACT-FM scores by an average of
2.68 points ($p < .001$), while offering no discernible advantage to the
superior ORPO or instruct-tuned variants. We posit that the superiority of ORPO
stems from its ability to learn the therapeutic `process' over imitating
`content,' a key aspect of ACT, while COT acts as a necessary scaffold for
models trained only via imitation. This study establishes that
preference-aligned policy optimization can effectively instill ACT competencies
in small LLMs, and that the utility of explicit reasoning is highly dependent
on the underlying training paradigm.

</details>


### [99] [HANRAG: Heuristic Accurate Noise-resistant Retrieval-Augmented Generation for Multi-hop Question Answering](https://arxiv.org/abs/2509.09713)
*Duolin Sun,Dan Yang,Yue Shen,Yihan Jiao,Zhehao Tan,Jie Feng,Lianzhen Zhong,Jian Wang,Peng Wei,Jinjie Gu*

Main category: cs.CL

TL;DR: HANRAG is a heuristic-based RAG framework that improves multi-hop question answering by query routing, decomposition, and noise filtering, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Current RAG methods struggle with multi-hop queries due to inefficient iterative retrieval, failure to capture sub-query content, and noise accumulation problems.

Method: Proposes HANRAG framework with a powerful revelator that routes queries, decomposes them into sub-queries, and filters noise from retrieved documents.

Result: Superior performance in both single-hop and multi-hop question-answering tasks compared to leading industry methods across various benchmarks.

Conclusion: HANRAG enhances adaptability and noise resistance, making it highly capable of handling diverse queries efficiently.

Abstract: The Retrieval-Augmented Generation (RAG) approach enhances question-answering
systems and dialogue generation tasks by integrating information retrieval (IR)
technologies with large language models (LLMs). This strategy, which retrieves
information from external knowledge bases to bolster the response capabilities
of generative models, has achieved certain successes. However, current RAG
methods still face numerous challenges when dealing with multi-hop queries. For
instance, some approaches overly rely on iterative retrieval, wasting too many
retrieval steps on compound queries. Additionally, using the original complex
query for retrieval may fail to capture content relevant to specific
sub-queries, resulting in noisy retrieved content. If the noise is not managed,
it can lead to the problem of noise accumulation. To address these issues, we
introduce HANRAG, a novel heuristic-based framework designed to efficiently
tackle problems of varying complexity. Driven by a powerful revelator, HANRAG
routes queries, decomposes them into sub-queries, and filters noise from
retrieved documents. This enhances the system's adaptability and noise
resistance, making it highly capable of handling diverse queries. We compare
the proposed framework against other leading industry methods across various
benchmarks. The results demonstrate that our framework obtains superior
performance in both single-hop and multi-hop question-answering tasks.

</details>


### [100] [How Small Transformation Expose the Weakness of Semantic Similarity Measures](https://arxiv.org/abs/2509.09714)
*Serge Lionel Nikiema,Albérick Euraste Djire,Abdoul Aziz Bonkoungou,Micheline Bénédicte Moumoula,Jordan Samhi,Abdoul Kader Kabore,Jacques Klein,Tegawendé F. Bissyande*

Main category: cs.CL

TL;DR: Study reveals significant flaws in semantic similarity measurement methods, showing that embedding-based approaches often misidentify semantic opposites as similar, while LLM-based methods perform better at distinguishing true semantic differences.


<details>
  <summary>Details</summary>
Motivation: To evaluate the effectiveness of different semantic similarity measurement methods for software engineering applications like code search and API recommendations, and to determine whether large language models truly understand semantic relationships or just recognize surface patterns.

Method: Tested 18 different similarity measurement approaches including word-based methods, embedding techniques, LLM-based systems, and structure-aware algorithms using a systematic testing framework with controlled changes to text and code.

Result: Embedding-based methods incorrectly identified semantic opposites as similar up to 99.9% of the time, while transformer-based approaches sometimes rated opposite meanings as more similar than synonymous ones. Switching from Euclidean to cosine similarity improved results by 24-66%. LLM-based methods performed better, producing low similarity scores (0.00-0.29) for different meanings compared to embedding methods' incorrect high scores (0.82-0.99).

Conclusion: Current semantic similarity metrics have significant limitations, with embedding methods particularly prone to errors in identifying true semantic relationships. LLM-based approaches show better performance but still require careful evaluation and methodological improvements for reliable semantic similarity assessment.

Abstract: This research examines how well different methods measure semantic
similarity, which is important for various software engineering applications
such as code search, API recommendations, automated code reviews, and
refactoring tools. While large language models are increasingly used for these
similarity assessments, questions remain about whether they truly understand
semantic relationships or merely recognize surface patterns.
  The study tested 18 different similarity measurement approaches, including
word-based methods, embedding techniques, LLM-based systems, and
structure-aware algorithms. The researchers created a systematic testing
framework that applies controlled changes to text and code to evaluate how well
each method handles different types of semantic relationships.
  The results revealed significant issues with commonly used metrics. Some
embedding-based methods incorrectly identified semantic opposites as similar up
to 99.9 percent of the time, while certain transformer-based approaches
occasionally rated opposite meanings as more similar than synonymous ones. The
study found that embedding methods' poor performance often stemmed from how
they calculate distances; switching from Euclidean distance to cosine
similarity improved results by 24 to 66 percent. LLM-based approaches performed
better at distinguishing semantic differences, producing low similarity scores
(0.00 to 0.29) for genuinely different meanings, compared to embedding methods
that incorrectly assigned high scores (0.82 to 0.99) to dissimilar content.

</details>


### [101] [Investigating Symbolic Triggers of Hallucination in Gemma Models Across HaluEval and TruthfulQA](https://arxiv.org/abs/2509.09715)
*Naveen Lamba,Sanju Tiwari,Manas Gaur*

Main category: cs.CL

TL;DR: This paper identifies and characterizes key symbolic properties that make LLMs intrinsically vulnerable to hallucinations, showing that modifiers and named entities remain problematic regardless of model scale.


<details>
  <summary>Details</summary>
Motivation: While hallucination in LLMs is well-studied, the specific properties that make them intrinsically vulnerable to hallucinations have not been identified and characterized.

Method: Used HaluEval and TruthfulQA datasets, converting question-answering formats into various formats to identify symbolic properties causing hallucinations. Tested on Gemma-2 models of different sizes (2B, 9B, 27B).

Result: Hallucination percentages average 79.0% for Gemma-2-2B, dropping to 73.6% for 9B and 63.9% for 27B. Modifiers (84.76%-94.98%) and named entities (83.87%-93.96%) show highest hallucination rates across all models and datasets.

Conclusion: Symbolic elements continue to confuse LLMs regardless of scale, indicating a fundamental weakness in how these models process such inputs, with modifiers and named entities being particularly problematic.

Abstract: Hallucination in Large Language Models (LLMs) is a well studied problem.
However, the properties that make LLM intrinsically vulnerable to
hallucinations have not been identified and studied. This research identifies
and characterizes the key properties, allowing us to pinpoint vulnerabilities
within the model's internal mechanisms. To solidify on these properties, we
utilized two established datasets, HaluEval and TruthfulQA and convert their
existing format of question answering into various other formats to narrow down
these properties as the reason for the hallucinations. Our findings reveal that
hallucination percentages across symbolic properties are notably high for
Gemma-2-2B, averaging 79.0% across tasks and datasets. With increased model
scale, hallucination drops to 73.6% for Gemma-2-9B and 63.9% for Gemma-2-27B,
reflecting a 15 percentage point reduction overall. Although the hallucination
rate decreases as the model size increases, a substantial amount of
hallucination caused by symbolic properties still persists. This is especially
evident for modifiers (ranging from 84.76% to 94.98%) and named entities
(ranging from 83.87% to 93.96%) across all Gemma models and both datasets.
These findings indicate that symbolic elements continue to confuse the models,
pointing to a fundamental weakness in how these LLMs process such
inputs--regardless of their scale.

</details>


### [102] [ALIGNS: Unlocking nomological networks in psychological measurement through a large language model](https://arxiv.org/abs/2509.09723)
*Kai R. Larsen,Sen Yan,Roland Müller,Lan Sang,Mikko Rönkkö,Ravi Starzl,Donald Edmondson*

Main category: cs.CL

TL;DR: ALIGNS is an LLM-based system that generates comprehensive nomological networks to solve measurement validation challenges, analyzing over 550,000 indicators across multiple fields.


<details>
  <summary>Details</summary>
Motivation: Psychological measurement faces challenges in building nomological networks 70 years after their proposal, leading to failed clinical trials and misguided public policies that target wrong outcomes.

Method: Analysis of Latent Indicators to Generate Nomological Structures (ALIGNS) uses large language models trained with validated questionnaire measures to create comprehensive nomological networks.

Result: ALIGNS generated three networks with 550,000+ indicators, revealed NIH PROMIS anxiety/depression converge into emotional distress, identified four new child temperament dimensions, and received positive expert psychometrician evaluations.

Conclusion: ALIGNS represents the first LLM application to solve foundational measurement validation problems, complementing traditional methods with large-scale nomological analysis and is freely available online.

Abstract: Psychological measurement is critical to many disciplines. Despite advances
in measurement, building nomological networks, theoretical maps of how concepts
and measures relate to establish validity, remains a challenge 70 years after
Cronbach and Meehl proposed them as fundamental to validation. This limitation
has practical consequences: clinical trials may fail to detect treatment
effects, and public policy may target the wrong outcomes. We introduce Analysis
of Latent Indicators to Generate Nomological Structures (ALIGNS), a large
language model-based system trained with validated questionnaire measures.
ALIGNS provides three comprehensive nomological networks containing over
550,000 indicators across psychology, medicine, social policy, and other
fields. This represents the first application of large language models to solve
a foundational problem in measurement validation. We report classification
accuracy tests used to develop the model, as well as three evaluations. In the
first evaluation, the widely used NIH PROMIS anxiety and depression instruments
are shown to converge into a single dimension of emotional distress. The second
evaluation examines child temperament measures and identifies four potential
dimensions not captured by current frameworks, and questions one existing
dimension. The third evaluation, an applicability check, engages expert
psychometricians who assess the system's importance, accessibility, and
suitability. ALIGNS is freely available at nomologicalnetwork.org,
complementing traditional validation methods with large-scale nomological
analysis.

</details>


### [103] [DiTTO-LLM: Framework for Discovering Topic-based Technology Opportunities via Large Language Model](https://arxiv.org/abs/2509.09724)
*Wonyoung Kim,Sujeong Seo,Juhyun Lee*

Main category: cs.CL

TL;DR: A framework using temporal patent analysis and LLMs to identify emerging technology opportunities through topic evolution tracking.


<details>
  <summary>Details</summary>
Motivation: Technology opportunities are crucial for advancement in technology, industry, and innovation, but identifying them systematically remains challenging.

Method: Extracts text from patents, maps text-based topics to discover inter-technology relationships, tracks topic changes over time using large language models and chat-based prompts for opportunity discovery.

Result: Evaluation on USPTO AI patent dataset shows AI technology is evolving toward everyday accessibility, demonstrating the framework's effectiveness.

Conclusion: The proposed framework shows strong potential for identifying future technology opportunities through temporal patent analysis and LLM integration.

Abstract: Technology opportunities are critical information that serve as a foundation
for advancements in technology, industry, and innovation. This paper proposes a
framework based on the temporal relationships between technologies to identify
emerging technology opportunities. The proposed framework begins by extracting
text from a patent dataset, followed by mapping text-based topics to discover
inter-technology relationships. Technology opportunities are then identified by
tracking changes in these topics over time. To enhance efficiency, the
framework leverages a large language model to extract topics and employs a
prompt for a chat-based language model to support the discovery of technology
opportunities. The framework was evaluated using an artificial intelligence
patent dataset provided by the United States Patent and Trademark Office. The
experimental results suggest that artificial intelligence technology is
evolving into forms that facilitate everyday accessibility. This approach
demonstrates the potential of the proposed framework to identify future
technology opportunities.

</details>


### [104] [BIBERT-Pipe on Biomedical Nested Named Entity Linking at BioASQ 2025](https://arxiv.org/abs/2509.09725)
*Chunyu Li,Xindi Zheng,Siqi Liu*

Main category: cs.CL

TL;DR: Lightweight pipeline for multilingual biomedical nested entity linking using two-stage retrieval-ranking with boundary cues and dataset augmentation, achieving 3rd place in BioNNE 2025 multilingual track.


<details>
  <summary>Details</summary>
Motivation: Address the gap in biomedical entity linking for nested and multilingual mentions, which are more realistic but largely unexplored in existing benchmarks focused on English-only flat mentions.

Method: Two-stage retrieval-ranking using same base encoder (original pre-trained for retrieval, domain-fine-tuned for ranking), learnable boundary tags [Ms]/[Me] for span identification, and automatic dataset augmentation from three complementary sources.

Result: Ranked third in the BioNNE 2025 multilingual track leaderboard, demonstrating competitive performance with minimal modifications to the original entity linking model.

Conclusion: The proposed lightweight pipeline with principled modifications (two-stage approach, boundary cues, and data augmentation) effectively handles multilingual biomedical nested entity linking while keeping the core EL model intact.

Abstract: Entity linking (EL) for biomedical text is typically benchmarked on
English-only corpora with flat mentions, leaving the more realistic scenario of
nested and multilingual mentions largely unexplored. We present our system for
the BioNNE 2025 Multilingual Biomedical Nested Named Entity Linking shared task
(English & Russian), closing this gap with a lightweight pipeline that keeps
the original EL model intact and modifies only three task-aligned components:
Two-stage retrieval-ranking. We leverage the same base encoder model in both
stages: the retrieval stage uses the original pre-trained model, while the
ranking stage applies domain-specific fine-tuning. Boundary cues. In the
ranking stage, we wrap each mention with learnable [Ms] / [Me] tags, providing
the encoder with an explicit, language-agnostic span before robustness to
overlap and nesting. Dataset augmentation. We also automatically expand the
ranking training corpus with three complementary data sources, enhancing
coverage without extra manual annotation. On the BioNNE 2025 leaderboard, our
two stage system, bilingual bert (BIBERT-Pipe), ranks third in the multilingual
track, demonstrating the effectiveness and competitiveness of these minimal yet
principled modifications. Code are publicly available at
https://github.com/Kaggle-Competitions-Code/BioNNE-L.

</details>


### [105] [Natural Language Translation of Formal Proofs through Informalization of Proof Steps and Recursive Summarization along Proof Structure](https://arxiv.org/abs/2509.09726)
*Seiji Hattori,Takuya Matsuzaki,Makoto Fujiwara*

Main category: cs.CL

TL;DR: LLM-based method for translating formal proofs to natural language using informalization and summarization, evaluated on textbook proofs and Lean proof library.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between machine-verifiable formal proofs and human-readable natural language explanations, making formal proofs more accessible and understandable.

Method: Leverages LLMs' informalization (verbalization of formal proof steps) and summarization capabilities to translate formal proofs into natural language, applied to textbook-derived formal proofs and Lean proof assistant library.

Result: Generated natural language proofs were analyzed for quality against original textbook proofs, demonstrating high readability and accuracy in the output.

Conclusion: The proposed method successfully produces highly readable and accurate natural language translations of formal proofs, making formal verification more accessible through LLM-powered translation.

Abstract: This paper proposes a natural language translation method for
machine-verifiable formal proofs that leverages the informalization
(verbalization of formal language proof steps) and summarization capabilities
of LLMs. For evaluation, it was applied to formal proof data created in
accordance with natural language proofs taken from an undergraduate-level
textbook, and the quality of the generated natural language proofs was analyzed
in comparison with the original natural language proofs. Furthermore, we will
demonstrate that this method can output highly readable and accurate natural
language proofs by applying it to existing formal proof library of the Lean
proof assistant.

</details>


### [106] [A Role-Aware Multi-Agent Framework for Financial Education Question Answering with LLMs](https://arxiv.org/abs/2509.09727)
*Andy Zhu,Yingjun Du*

Main category: cs.CL

TL;DR: A multi-agent framework using role-based prompting and RAG improves financial QA accuracy by 6.6-8.3% over zero-shot baselines, with Gemini-2.0-Flash performing best.


<details>
  <summary>Details</summary>
Motivation: Existing LLMs fail to capture the nuanced reasoning required for financial problem-solving, which demands multistep quantitative reasoning, domain-specific terminology, and real-world scenario comprehension.

Method: Multi-agent framework with Base Generator, Evidence Retriever, and Expert Reviewer agents working in single-pass iteration, using RAG from 6 finance textbooks and domain-expert prompting strategies.

Result: Critique-based refinement improves answer accuracy by 6.6-8.3% over zero-shot Chain-of-Thought baselines. Gemini-2.0-Flash achieved highest performance, and GPT-4o-mini matched finance-tuned FinGPT-mt_Llama3-8B_LoRA.

Conclusion: The method provides a cost-effective approach to enhancing financial QA and offers insights for further research in multi-agent financial LLM systems.

Abstract: Question answering (QA) plays a central role in financial education, yet
existing large language model (LLM) approaches often fail to capture the
nuanced and specialized reasoning required for financial problem-solving. The
financial domain demands multistep quantitative reasoning, familiarity with
domain-specific terminology, and comprehension of real-world scenarios. We
present a multi-agent framework that leverages role-based prompting to enhance
performance on domain-specific QA. Our framework comprises a Base Generator, an
Evidence Retriever, and an Expert Reviewer agent that work in a single-pass
iteration to produce a refined answer. We evaluated our framework on a set of
3,532 expert-designed finance education questions from Study.com, an online
learning platform. We leverage retrieval-augmented generation (RAG) for
contextual evidence from 6 finance textbooks and prompting strategies for a
domain-expert reviewer. Our experiments indicate that critique-based refinement
improves answer accuracy by 6.6-8.3% over zero-shot Chain-of-Thought baselines,
with the highest performance from Gemini-2.0-Flash. Furthermore, our method
enables GPT-4o-mini to achieve performance comparable to the finance-tuned
FinGPT-mt_Llama3-8B_LoRA. Our results show a cost-effective approach to
enhancing financial QA and offer insights for further research in multi-agent
financial LLM systems.

</details>


### [107] [A meta-analysis on the performance of machine-learning based language models for sentiment analysis](https://arxiv.org/abs/2509.09728)
*Elena Rohde,Jonas Klingwort,Christian Borgs*

Main category: cs.CL

TL;DR: Meta-analysis of 195 trials from 20 studies shows ML sentiment analysis on Twitter achieves average accuracy of 0.80, but highlights issues with accuracy metric and need for standardized reporting practices.


<details>
  <summary>Details</summary>
Motivation: To evaluate ML performance in Twitter sentiment analysis, estimate average performance, assess study heterogeneity, and analyze how study characteristics influence model performance across different research.

Method: Used PRISMA guidelines to search academic databases, selected 195 trials from 20 studies with 12 features. Analyzed overall accuracy using double arcsine transformation and three-level random effects model with AIC optimization.

Result: Average overall accuracy of AIC-optimized model was 0.80 [0.76, 0.84]. Found that overall accuracy is widely used but misleading due to sensitivity to class imbalance and number of sentiment classes.

Conclusion: Highlights need for accuracy normalization and standardized reporting including confusion matrices for independent test sets to enable reliable ML classifier comparisons across studies.

Abstract: This paper presents a meta-analysis evaluating ML performance in sentiment
analysis for Twitter data. The study aims to estimate the average performance,
assess heterogeneity between and within studies, and analyze how study
characteristics influence model performance. Using PRISMA guidelines, we
searched academic databases and selected 195 trials from 20 studies with 12
study features. Overall accuracy, the most reported performance metric, was
analyzed using double arcsine transformation and a three-level random effects
model. The average overall accuracy of the AIC-optimized model was 0.80 [0.76,
0.84]. This paper provides two key insights: 1) Overall accuracy is widely used
but often misleading due to its sensitivity to class imbalance and the number
of sentiment classes, highlighting the need for normalization. 2) Standardized
reporting of model performance, including reporting confusion matrices for
independent test sets, is essential for reliable comparisons of ML classifiers
across studies, which seems far from common practice.

</details>


### [108] [MultimodalHugs: Enabling Sign Language Processing in Hugging Face](https://arxiv.org/abs/2509.09729)
*Gerard Sant,Zifan Jiang,Carlos Escolano,Amit Moryossef,Mathias Müller,Rico Sennrich,Sarah Ebling*

Main category: cs.CL

TL;DR: MultimodalHugs is a framework built on Hugging Face to address reproducibility and flexibility issues in sign language processing research, supporting diverse multimodal data beyond standard templates.


<details>
  <summary>Details</summary>
Motivation: Sign language processing research faces challenges with complex ad-hoc code, low reproducibility, and unfair comparisons. Existing tools like Hugging Face lack flexibility for sign language experiments, as confirmed by a survey of SLP researchers.

Method: Developed MultimodalHugs - a framework built on top of Hugging Face that adds an abstraction layer to support diverse data modalities and tasks, particularly focusing on sign languages but applicable to other multimodal use cases.

Result: The framework successfully accommodates diverse modalities including pose estimation data for sign languages and pixel data for text characters, as demonstrated through quantitative experiments.

Conclusion: MultimodalHugs provides a solution to the reproducibility and flexibility challenges in SLP research while inheriting the advantages of the Hugging Face ecosystem, making it widely applicable to multimodal tasks beyond standard templates.

Abstract: In recent years, sign language processing (SLP) has gained importance in the
general field of Natural Language Processing. However, compared to research on
spoken languages, SLP research is hindered by complex ad-hoc code,
inadvertently leading to low reproducibility and unfair comparisons. Existing
tools that are built for fast and reproducible experimentation, such as Hugging
Face, are not flexible enough to seamlessly integrate sign language
experiments. This view is confirmed by a survey we conducted among SLP
researchers.
  To address these challenges, we introduce MultimodalHugs, a framework built
on top of Hugging Face that enables more diverse data modalities and tasks,
while inheriting the well-known advantages of the Hugging Face ecosystem. Even
though sign languages are our primary focus, MultimodalHugs adds a layer of
abstraction that makes it more widely applicable to other use cases that do not
fit one of the standard templates of Hugging Face. We provide quantitative
experiments to illustrate how MultimodalHugs can accommodate diverse modalities
such as pose estimation data for sign languages, or pixel data for text
characters.

</details>


### [109] [Benchmarking Vision-Language Models on Chinese Ancient Documents: From OCR to Knowledge Reasoning](https://arxiv.org/abs/2509.09731)
*Haiyang Yu,Yuchuan Wu,Fan Shi,Lei Liao,Jinghui Lu,Xiaodong Ge,Han Wang,Minghan Zhuo,Xuecheng Wu,Xiang Fei,Hao Feng,Guozhi Tang,An-Lan Wang,Hanshen Zhu,Yangfan He,Quanhuan Liang,Liyuan Meng,Chao Feng,Can Huang,Jingqun Tang,Bin Li*

Main category: cs.CL

TL;DR: AncientDoc is the first benchmark for evaluating Vision-Language Models on Chinese ancient documents, addressing the gap in existing benchmarks that focus only on English or simplified Chinese texts.


<details>
  <summary>Details</summary>
Motivation: Chinese ancient documents contain rich historical and cultural knowledge but face digitization challenges. Current VLMs struggle with their visual and linguistic complexity, and existing benchmarks don't adequately cover ancient Chinese materials.

Method: Created AncientDoc benchmark with five tasks: page-level OCR, vernacular translation, reasoning-based QA, knowledge-based QA, and linguistic variant QA. The dataset covers 14 document types, over 100 books, and about 3,000 pages.

Result: Evaluated mainstream VLMs using multiple metrics with human-aligned LLM scoring. The benchmark provides comprehensive assessment capabilities for ancient document processing.

Conclusion: AncientDoc fills a critical gap in evaluating VLMs for Chinese ancient document understanding and provides a standardized framework for assessing model performance on complex historical materials.

Abstract: Chinese ancient documents, invaluable carriers of millennia of Chinese
history and culture, hold rich knowledge across diverse fields but face
challenges in digitization and understanding, i.e., traditional methods only
scan images, while current Vision-Language Models (VLMs) struggle with their
visual and linguistic complexity. Existing document benchmarks focus on English
printed texts or simplified Chinese, leaving a gap for evaluating VLMs on
ancient Chinese documents. To address this, we present AncientDoc, the first
benchmark for Chinese ancient documents, designed to assess VLMs from OCR to
knowledge reasoning. AncientDoc includes five tasks (page-level OCR, vernacular
translation, reasoning-based QA, knowledge-based QA, linguistic variant QA) and
covers 14 document types, over 100 books, and about 3,000 pages. Based on
AncientDoc, we evaluate mainstream VLMs using multiple metrics, supplemented by
a human-aligned large language model for scoring.

</details>


### [110] [MCP-AgentBench: Evaluating Real-World Language Agent Performance with MCP-Mediated Tools](https://arxiv.org/abs/2509.09734)
*Zikang Guo,Benfeng Xu,Chiwei Zhu,Wentao Hong,Xiaorui Wang,Zhendong Mao*

Main category: cs.CL

TL;DR: MCP-AgentBench is a new benchmark designed to evaluate language agents' performance in MCP-mediated tool interactions, addressing the gap in existing benchmarks that fail to capture real-world agent capabilities in this emerging paradigm.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks fail to properly assess agent performance in the Model Context Protocol (MCP) environment, leading to distorted perceptions of agent capabilities and inability to differentiate proficiencies in real-world MCP tool interactions.

Method: Developed a comprehensive benchmark with: 1) Robust MCP testbed with 33 operational servers and 188 distinct tools, 2) 600 systematically designed queries across 6 categories of varying complexity, 3) MCP-Eval methodology focusing on outcome-oriented, real-world task success evaluation.

Result: The benchmark provides foundational insights through extensive empirical evaluation of leading language agents, enabling reliable assessment of agent capabilities in MCP environments.

Conclusion: MCP-AgentBench provides a standardized framework for the research community to build, validate, and advance agents that can fully leverage MCP's benefits, accelerating progress toward capable and interoperable AI systems.

Abstract: The Model Context Protocol (MCP) is rapidly emerging as a pivotal open
standard, designed to enhance agent-tool integration and interoperability, and
is positioned to unlock a new era of powerful, interconnected, and genuinely
utilitarian agentic AI. However, despite MCP's growing adoption, existing
benchmarks often fail to capture real-world agent performance within this new
paradigm, leading to a distorted perception of their true operational value and
an inability to reliably differentiate proficiencies. To bridge this critical
evaluation gap, we introduce MCP-AgentBench -- a comprehensive benchmark
specifically engineered to rigorously assess language agent capabilities in
MCP-mediated tool interactions. Core contributions of MCP-AgentBench include:
the establishment of a robust MCP testbed comprising 33 operational servers
with 188 distinct tools; the development of a benchmark featuring 600
systematically designed queries distributed across 6 distinct categories of
varying interaction complexity; and the introduction of MCP-Eval, a novel
outcome-oriented evaluation methodology prioritizing real-world task success.
Through extensive empirical evaluation of leading language agents, we provide
foundational insights. MCP-AgentBench aims to equip the research community with
a standardized and reliable framework to build, validate, and advance agents
capable of fully leveraging MCP's transformative benefits, thereby accelerating
progress toward truly capable and interoperable AI systems.

</details>


### [111] [Discrimination by LLMs: Cross-lingual Bias Assessment and Mitigation in Decision-Making and Summarisation](https://arxiv.org/abs/2509.09735)
*Willem Huijzer,Jieying Chen*

Main category: cs.CL

TL;DR: Study examines biases in LLMs (GPT-3.5 and GPT-4o) related to background, gender, and age in decision-making and summarization tasks, finding significant biases favoring female gender, younger ages, and African-American backgrounds in decision tasks, with cross-lingual analysis showing similar patterns in English and Dutch.


<details>
  <summary>Details</summary>
Motivation: Address concerns about societal inequalities and information bias as LLMs are rapidly integrated into various domains, focusing on their impact on decision-making and summarization tasks.

Method: Used adapted dataset translated into Dutch, creating 151,200 prompts for decision task and 176,400 for summarization task. Tested demographic variables, instructions, salience levels, and languages on GPT-3.5 and GPT-4o.

Result: Both models showed significant bias in decision-making (favoring female, younger ages, African-American background). Summarization showed minimal bias except age differences for GPT-3.5. Cross-lingual patterns similar but with category differences. Mitigation instructions reduced bias by 27% mean reduction.

Conclusion: Highlights need for cautious LLM adoption, context-specific bias testing, and continued development of mitigation strategies for responsible AI deployment. GPT-4o showed reduced biases, indicating potential for prompt-based mitigation in newer models.

Abstract: The rapid integration of Large Language Models (LLMs) into various domains
raises concerns about societal inequalities and information bias. This study
examines biases in LLMs related to background, gender, and age, with a focus on
their impact on decision-making and summarization tasks. Additionally, the
research examines the cross-lingual propagation of these biases and evaluates
the effectiveness of prompt-instructed mitigation strategies. Using an adapted
version of the dataset by Tamkin et al. (2023) translated into Dutch, we
created 151,200 unique prompts for the decision task and 176,400 for the
summarisation task. Various demographic variables, instructions, salience
levels, and languages were tested on GPT-3.5 and GPT-4o. Our analysis revealed
that both models were significantly biased during decision-making, favouring
female gender, younger ages, and certain backgrounds such as the
African-American background. In contrast, the summarisation task showed minimal
evidence of bias, though significant age-related differences emerged for
GPT-3.5 in English. Cross-lingual analysis showed that bias patterns were
broadly similar between English and Dutch, though notable differences were
observed across specific demographic categories. The newly proposed mitigation
instructions, while unable to eliminate biases completely, demonstrated
potential in reducing them. The most effective instruction achieved a 27\% mean
reduction in the gap between the most and least favorable demographics.
Notably, contrary to GPT-3.5, GPT-4o displayed reduced biases for all prompts
in English, indicating the specific potential for prompt-based mitigation
within newer models. This research underscores the importance of cautious
adoption of LLMs and context-specific bias testing, highlighting the need for
continued development of effective mitigation strategies to ensure responsible
deployment of AI.

</details>


### [112] [HEFT: A Coarse-to-Fine Hierarchy for Enhancing the Efficiency and Accuracy of Language Model Reasoning](https://arxiv.org/abs/2509.09801)
*Brennen Hill*

Main category: cs.CL

TL;DR: HEFT combines LoRA (weight space) and ReFT (representation space) PEFT methods in a hierarchical coarse-to-fine approach, achieving 85.17% accuracy on BoolQ with only 3 epochs, outperforming individual methods trained for 20 epochs.


<details>
  <summary>Details</summary>
Motivation: To overcome computational constraints in adapting LLMs to specialized reasoning tasks by synergistically combining different PEFT paradigms for superior performance and efficiency.

Method: Hierarchical Efficient Fine-Tuning (HEFT) that first applies broad adaptation using LoRA in weight space, followed by precise refinement using Representation Fine-Tuning (ReFT) on internal activations.

Result: HEFT achieved 85.17% accuracy on BoolQ benchmark with only 3 epochs, outperforming LoRA-only (85.05%) and ReFT-only (83.36%) methods trained for 20 epochs.

Conclusion: Thoughtful composition of PEFT methods offers an efficient and effective path to enhance LLM reasoning capabilities, achieving superior results with significantly reduced computational budget.

Abstract: The adaptation of large language models (LLMs) to specialized reasoning tasks
is fundamentally constrained by computational resources. Parameter-Efficient
Fine-Tuning (PEFT) methods have emerged as a powerful solution, yet the
landscape of these techniques is diverse, with distinct methods operating in
either the model's weight space or its representation space. This paper
investigates the hypothesis that a synergistic combination of these paradigms
can unlock superior performance and efficiency. We introduce HEFT (Hierarchical
Efficient Fine-Tuning), a novel hierarchical adaptation strategy that composes
two distinct PEFT methods in a coarse-to-fine manner: first, a broad,
foundational adaptation in the weight space using Low-Rank Adaptation (LoRA),
followed by a precise, surgical refinement of internal activations using
Representation Fine-Tuning (ReFT). We evaluate this approach by fine-tuning a
Llama-2-7B model on the BoolQ benchmark, a challenging dataset for inferential
reasoning. Our results reveal a profound synergistic effect. A model fine-tuned
for only three epochs with our HEFT strategy achieves an accuracy of 85.17\%,
exceeding the performance of models trained for 20 epochs with either LoRA-only
(85.05\%) or ReFT-only (83.36\%) methodologies. This work demonstrates that the
thoughtful composition of PEFT methods is a potent algorithmic innovation,
offering a more efficient and effective path toward advancing the reasoning
capabilities of language models. By achieving superior results with a fraction
of the computational budget, our findings present a principled approach to
overcoming the obstacles inherent in adapting large-scale models for complex
cognitive tasks.

</details>


### [113] [Pragmatic Frames Evoked by Gestures: A FrameNet Brasil Approach to Multimodality in Turn Organization](https://arxiv.org/abs/2509.09804)
*Helen de Andrade Abreu,Tiago Timponi Torrent,Ely Edison da Silva Matos*

Main category: cs.CL

TL;DR: A framework for modeling multimodal conversational turn organization through correlations between language and interactive gestures, with evidence from enriched annotations in the Frame2 dataset showing how gestures are used for turn management in natural conversations.


<details>
  <summary>Details</summary>
Motivation: To address the gap in machine learning datasets that encode specific strategies and gestures used by communicators for conversational turn organization, which had not been systematically documented despite being studied across various fields.

Method: Developed an annotation methodology to enrich the multimodal Frame2 dataset (containing 10 episodes from a Brazilian TV series) with pragmatic frames modeling turn organization, specifically annotating gestures used for passing, taking, and keeping conversational turns.

Result: Confirmed that communicators use gestures as tools for conversational turn management in face-to-face conversations, and discovered variations of gestures not previously documented. The data showed that pragmatic frame annotation provides deeper insights into human cognition and language.

Conclusion: The use of gestures for turn organization arises from the conceptualization of pragmatic frames involving mental spaces, blending and conceptual metaphors. The enriched dataset and annotation methodology successfully capture previously undocumented gesture variations and contribute to understanding multimodal conversation dynamics.

Abstract: This paper proposes a framework for modeling multimodal conversational turn
organization via the proposition of correlations between language and
interactive gestures, based on analysis as to how pragmatic frames are
conceptualized and evoked by communicators. As a means to provide evidence for
the analysis, we developed an annotation methodology to enrich a multimodal
dataset (annotated for semantic frames) with pragmatic frames modeling
conversational turn organization. Although conversational turn organization has
been studied by researchers from diverse fields, the specific strategies,
especially gestures used by communicators, had not yet been encoded in a
dataset that can be used for machine learning. To fill this gap, we enriched
the Frame2 dataset with annotations of gestures used for turn organization. The
Frame2 dataset features 10 episodes from the Brazilian TV series Pedro Pelo
Mundo annotated for semantic frames evoked in both video and text. This dataset
allowed us to closely observe how communicators use interactive gestures
outside a laboratory, in settings, to our knowledge, not previously recorded in
related literature. Our results have confirmed that communicators involved in
face-to-face conversation make use of gestures as a tool for passing, taking
and keeping conversational turns, and also revealed variations of some gestures
that had not been documented before. We propose that the use of these gestures
arises from the conceptualization of pragmatic frames, involving mental spaces,
blending and conceptual metaphors. In addition, our data demonstrate that the
annotation of pragmatic frames contributes to a deeper understanding of human
cognition and language.

</details>


### [114] [Topic-Guided Reinforcement Learning with LLMs for Enhancing Multi-Document Summarization](https://arxiv.org/abs/2509.09852)
*Chuyuan Li,Austin Xu,Shafiq Joty,Giuseppe Carenini*

Main category: cs.CL

TL;DR: Topic-guided reinforcement learning approach improves multi-document summarization by using topic rewards to enhance content selection and alignment with source documents.


<details>
  <summary>Details</summary>
Motivation: Large Language Models perform well on single-document summarization but still have room for improvement in Multi-Document Summarization (MDS), particularly in effectively integrating information from multiple sources while maintaining coherence and topical relevance.

Method: Proposed a topic-guided reinforcement learning approach using explicit topic labels as prompts and a novel topic reward within the Group Relative Policy Optimization (GRPO) framework to measure topic alignment between generated summaries and source documents.

Result: Experimental results on Multi-News and Multi-XScience datasets show the method consistently outperforms strong baselines, demonstrating improved informativeness and topic alignment in generated summaries.

Conclusion: Leveraging topical cues through reinforcement learning is an effective approach for enhancing content selection and overall performance in multi-document summarization tasks.

Abstract: A key challenge in Multi-Document Summarization (MDS) is effectively
integrating information from multiple sources while maintaining coherence and
topical relevance. While Large Language Models have shown impressive results in
single-document summarization, their performance on MDS still leaves room for
improvement. In this paper, we propose a topic-guided reinforcement learning
approach to improve content selection in MDS. We first show that explicitly
prompting models with topic labels enhances the informativeness of the
generated summaries. Building on this insight, we propose a novel topic reward
within the Group Relative Policy Optimization (GRPO) framework to measure topic
alignment between the generated summary and source documents. Experimental
results on the Multi-News and Multi-XScience datasets demonstrate that our
method consistently outperforms strong baselines, highlighting the
effectiveness of leveraging topical cues in MDS.

</details>


### [115] [Emulating Public Opinion: A Proof-of-Concept of AI-Generated Synthetic Survey Responses for the Chilean Case](https://arxiv.org/abs/2509.09871)
*Bastián González-Bustamante,Nando Verelst,Carla Cisternas*

Main category: cs.CL

TL;DR: LLMs can generate synthetic survey responses that approximate human responses from probabilistic surveys, with excellent performance on trust items and comparable results across top models like GPT-4o and Llama 4 Maverick, though performance varies by item and demographic factors.


<details>
  <summary>Details</summary>
Motivation: To evaluate whether LLM-generated synthetic survey responses can reliably emulate human answers and behavior while mitigating measurement and representation errors in survey research, and to assess potential biases inherited from training data.

Method: Benchmarked 128 prompt-model-question triplets generating 189,696 synthetic profiles, compared against ground-truth human responses from a Chilean public opinion probabilistic survey. Used meta-analysis across 128 question-subsample pairs with performance metrics (accuracy, precision, recall, F1-score) to test biases along sociodemographic dimensions.

Result: Synthetic responses achieved excellent performance on trust items (F1-score and accuracy > 0.90). GPT-4o, GPT-4o-mini and Llama 4 Maverick performed comparably. Synthetic-human alignment was highest among respondents aged 45-59. Overall good approximation but with substantial item-level heterogeneity.

Conclusion: LLM-based synthetic samples can approximate probabilistic sample responses but require careful calibration and additional distributional tests to ensure algorithmic fidelity and reduce errors, as capturing full nuance of public opinion remains challenging.

Abstract: Large Language Models (LLMs) offer promising avenues for methodological and
applied innovations in survey research by using synthetic respondents to
emulate human answers and behaviour, potentially mitigating measurement and
representation errors. However, the extent to which LLMs recover aggregate item
distributions remains uncertain and downstream applications risk reproducing
social stereotypes and biases inherited from training data. We evaluate the
reliability of LLM-generated synthetic survey responses against ground-truth
human responses from a Chilean public opinion probabilistic survey.
Specifically, we benchmark 128 prompt-model-question triplets, generating
189,696 synthetic profiles, and pool performance metrics (i.e., accuracy,
precision, recall, and F1-score) in a meta-analysis across 128
question-subsample pairs to test for biases along key sociodemographic
dimensions. The evaluation spans OpenAI's GPT family and o-series reasoning
models, as well as Llama and Qwen checkpoints. Three results stand out. First,
synthetic responses achieve excellent performance on trust items (F1-score and
accuracy > 0.90). Second, GPT-4o, GPT-4o-mini and Llama 4 Maverick perform
comparably on this task. Third, synthetic-human alignment is highest among
respondents aged 45-59. Overall, LLM-based synthetic samples approximate
responses from a probabilistic sample, though with substantial item-level
heterogeneity. Capturing the full nuance of public opinion remains challenging
and requires careful calibration and additional distributional tests to ensure
algorithmic fidelity and reduce errors.

</details>


### [116] [Large Language Models Meet Legal Artificial Intelligence: A Survey](https://arxiv.org/abs/2509.09969)
*Zhitian Hou,Zihan Ye,Nanli Zeng,Tianyong Hao,Kun Zeng*

Main category: cs.CL

TL;DR: Comprehensive review of 16 legal LLM series, 47 frameworks, 15 benchmarks, and 29 datasets for legal AI applications, with analysis of challenges and future directions.


<details>
  <summary>Details</summary>
Motivation: To advance research and applications of LLM-based approaches in the legal domain by providing systematic resources and guidance for beginners and researchers.

Method: Systematic review and analysis of existing legal LLMs, frameworks, benchmarks, and datasets, followed by identification of challenges and future research directions.

Result: Comprehensive compilation of 16 legal LLM series, 47 LLM-based frameworks for legal tasks, 15 evaluation benchmarks, and 29 datasets covering various legal capabilities.

Conclusion: The paper provides valuable resources and systematic guidance for legal AI research, identifies current challenges, and outlines future directions to encourage further development in LLM-based legal applications.

Abstract: Large Language Models (LLMs) have significantly advanced the development of
Legal Artificial Intelligence (Legal AI) in recent years, enhancing the
efficiency and accuracy of legal tasks. To advance research and applications of
LLM-based approaches in legal domain, this paper provides a comprehensive
review of 16 legal LLMs series and 47 LLM-based frameworks for legal tasks, and
also gather 15 benchmarks and 29 datasets to evaluate different legal
capabilities. Additionally, we analyse the challenges and discuss future
directions for LLM-based approaches in the legal domain. We hope this paper
provides a systematic introduction for beginners and encourages future research
in this field. Resources are available at
https://github.com/ZhitianHou/LLMs4LegalAI.

</details>


### [117] [CMHG: A Dataset and Benchmark for Headline Generation of Minority Languages in China](https://arxiv.org/abs/2509.09990)
*Guixian Xu,Zeli Su,Ziyin Zhang,Jianing Liu,XU Han,Ting Zhang,Yushuang Dong*

Main category: cs.CL

TL;DR: New CMHG dataset for Chinese minority languages (Tibetan, Uyghur, Mongolian) with 200K total entries to address headline generation challenges due to unique writing systems.


<details>
  <summary>Details</summary>
Motivation: Minority languages in China face significant challenges due to unique writing systems that differ from international standards, leading to severe lack of relevant corpora for supervised tasks like headline generation.

Method: Created Chinese Minority Headline Generation (CMHG) dataset with 100K Tibetan entries, 50K Uyghur entries, and 50K Mongolian entries, plus a high-quality test set annotated by native speakers.

Result: A comprehensive dataset specifically curated for headline generation tasks in Chinese minority languages, serving as a benchmark for future research.

Conclusion: This dataset will become a valuable resource for advancing headline generation in Chinese minority languages and contribute to the development of related benchmarks.

Abstract: Minority languages in China, such as Tibetan, Uyghur, and Traditional
Mongolian, face significant challenges due to their unique writing systems,
which differ from international standards. This discrepancy has led to a severe
lack of relevant corpora, particularly for supervised tasks like headline
generation. To address this gap, we introduce a novel dataset, Chinese Minority
Headline Generation (CMHG), which includes 100,000 entries for Tibetan, and
50,000 entries each for Uyghur and Mongolian, specifically curated for headline
generation tasks. Additionally, we propose a high-quality test set annotated by
native speakers, designed to serve as a benchmark for future research in this
domain. We hope this dataset will become a valuable resource for advancing
headline generation in Chinese minority languages and contribute to the
development of related benchmarks.

</details>


### [118] [Unsupervised Hallucination Detection by Inspecting Reasoning Processes](https://arxiv.org/abs/2509.10004)
*Ponhvoan Srey,Xiaobao Wu,Anh Tuan Luu*

Main category: cs.CL

TL;DR: IRIS is an unsupervised hallucination detection framework that uses LLM's internal representations and response uncertainty to identify factual errors without labeled data, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing unsupervised hallucination detection methods rely on proxy signals unrelated to factual correctness, limiting their generalizability across datasets and scenarios.

Method: IRIS prompts LLMs to verify statement truthfulness, extracts contextualized embeddings as features, and uses response uncertainty as soft pseudolabels for training without human annotations.

Result: IRIS consistently outperforms existing unsupervised methods, is computationally low cost, works with few training data, and is suitable for real-time detection.

Conclusion: The framework effectively leverages LLM's internal representations intrinsic to factual correctness, providing a practical unsupervised solution for hallucination detection.

Abstract: Unsupervised hallucination detection aims to identify hallucinated content
generated by large language models (LLMs) without relying on labeled data.
While unsupervised methods have gained popularity by eliminating
labor-intensive human annotations, they frequently rely on proxy signals
unrelated to factual correctness. This misalignment biases detection probes
toward superficial or non-truth-related aspects, limiting generalizability
across datasets and scenarios. To overcome these limitations, we propose IRIS,
an unsupervised hallucination detection framework, leveraging internal
representations intrinsic to factual correctness. IRIS prompts the LLM to
carefully verify the truthfulness of a given statement, and obtain its
contextualized embedding as informative features for training. Meanwhile, the
uncertainty of each response is considered a soft pseudolabel for truthfulness.
Experimental results demonstrate that IRIS consistently outperforms existing
unsupervised methods. Our approach is fully unsupervised, computationally low
cost, and works well even with few training data, making it suitable for
real-time detection.

</details>


### [119] [Multi-Intent Recognition in Dialogue Understanding: A Comparison Between Smaller Open-Source LLMs](https://arxiv.org/abs/2509.10010)
*Adnan Ahmad,Philine Kowol,Stefan Hillmann,Sebastian Möller*

Main category: cs.CL

TL;DR: Analysis of open-source LLMs (LLama2-7B, Mistral-7B, Yi-6B) for multi-label intent classification using MultiWOZ 2.1 dataset in few-shot setting, with comparison to BERT baseline.


<details>
  <summary>Details</summary>
Motivation: To evaluate the effectiveness of publicly available open-source LLMs that can run on consumer hardware for complex multi-label intent classification tasks in dialogue systems.

Method: Used MultiWOZ 2.1 dataset with few-shot setup (20 examples in prompt), compared three open-source LLMs against BERT baseline using accuracy, precision, recall, F1 scores, inference time, and VRAM requirements.

Result: Mistral-7B-v0.1 outperformed other generative models on 11/14 intent classes with weighted F1 of 0.50, but BERT supervised classifier showed superior performance overall compared to few-shot LLMs.

Conclusion: Provides framework for using small open-source LLMs in multi-intent dialogue detection, though supervised BERT still outperforms few-shot generative approaches for this task.

Abstract: In this paper, we provide an extensive analysis of multi-label intent
classification using Large Language Models (LLMs) that are open-source,
publicly available, and can be run in consumer hardware. We use the MultiWOZ
2.1 dataset, a benchmark in the dialogue system domain, to investigate the
efficacy of three popular open-source pre-trained LLMs, namely LLama2-7B-hf,
Mistral-7B-v0.1, and Yi-6B. We perform the classification task in a few-shot
setup, giving 20 examples in the prompt with some instructions. Our approach
focuses on the differences in performance of these models across several
performance metrics by methodically assessing these models on multi-label
intent classification tasks. Additionally, we compare the performance of the
instruction-based fine-tuning approach with supervised learning using the
smaller transformer model BertForSequenceClassification as a baseline. To
evaluate the performance of the models, we use evaluation metrics like
accuracy, precision, and recall as well as micro, macro, and weighted F1 score.
We also report the inference time, VRAM requirements, etc. The Mistral-7B-v0.1
outperforms two other generative models on 11 intent classes out of 14 in terms
of F-Score, with a weighted average of 0.50. It also has relatively lower
Humming Loss and higher Jaccard Similarity, making it the winning model in the
few-shot setting. We find BERT based supervised classifier having superior
performance compared to the best performing few-shot generative LLM. The study
provides a framework for small open-source LLMs in detecting complex
multi-intent dialogues, enhancing the Natural Language Understanding aspect of
task-oriented chatbots.

</details>


### [120] [Linguistic trajectories of bipolar disorder on social media](https://arxiv.org/abs/2509.10035)
*Laurin Plank,Armin Zlomuzica*

Main category: cs.CL

TL;DR: Analysis of social media language reveals significant linguistic changes in bipolar disorder patients before and after diagnosis, showing mood disturbances, comorbidities, and seasonal patterns with 12-month periodicity.


<details>
  <summary>Details</summary>
Motivation: Clinical assessments for bipolar disorder are limited in scale, while social media language analysis offers high temporal resolution and longitudinal scope for studying mental health markers.

Method: Developed a method to determine timing of bipolar disorder diagnoses from social media data, analyzed language trajectories from 3 years before to 21 years after diagnosis, comparing with unipolar depression and non-affected users.

Result: Found pervasive linguistic alterations reflecting mood disturbance, psychiatric comorbidity, substance abuse, hospitalization, and disorganized thought. Observed recurring mood-related language changes with 12-month periodicity suggestive of seasonal episodes, with trend-level evidence of increased periodicity in female users.

Conclusion: Provides evidence for language alterations in both acute and chronic phases of bipolar disorder, validating social media as a scalable tool for mental health monitoring.

Abstract: Language provides valuable markers of affective disorders such as bipolar
disorder (BD), yet clinical assessments remain limited in scale. In response,
analyses of social media (SM) language have gained prominence due to their high
temporal resolution and longitudinal scope. Here, we introduce a method to
determine the timing of users' diagnoses and apply it to study language
trajectories from 3 years before to 21 years after BD diagnosis - contrasted
with uses reporting unipolar depression (UD) and non-affected users (HC). We
show that BD diagnosis is accompanied by pervasive linguistic alterations
reflecting mood disturbance, psychiatric comorbidity, substance abuse,
hospitalization, medical comorbidities, unusual thought content, and
disorganized thought. We further observe recurring mood-related language
changes across two decades after the diagnosis, with a pronounced 12-month
periodicity suggestive of seasonal mood episodes. Finally, trend-level evidence
suggests an increased periodicity in users estimated to be female. In sum, our
findings provide evidence for language alterations in the acute and chronic
phase of BD. This validates and extends recent efforts leveraging SM for
scalable monitoring of mental health.

</details>


### [121] [!MSA at BAREC Shared Task 2025: Ensembling Arabic Transformers for Readability Assessment](https://arxiv.org/abs/2509.10040)
*Mohamed Basem,Mohamed Younes,Seif Ahmed,Abdelrahman Moustafa*

Main category: cs.CL

TL;DR: MSA's winning system for Arabic readability assessment used an ensemble of 4 transformer models with diverse loss functions, addressed data scarcity through weighted training and synthetic data generation, and achieved 87.5% QWK at sentence level.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of fine-grained Arabic readability assessment, particularly severe class imbalance and data scarcity in the BAREC 2025 Shared Task.

Method: Confidence-weighted ensemble of AraBERTv2, AraELECTRA, MARBERT, and CAMeLBERT models fine-tuned with distinct loss functions; used weighted training, advanced preprocessing, SAMER corpus relabeling, and synthetic data generation via Gemini 2.5 Flash (adding 10,000 rare-level samples); applied targeted post-processing.

Result: Achieved first place in all six tracks with 87.5% Quadratic Weighted Kappa at sentence level and 87.4% at document level; post-processing delivered 6.3% QWK gain.

Conclusion: The system demonstrates the effectiveness of model and loss diversity, confidence-informed fusion, and intelligent data augmentation for robust Arabic readability prediction.

Abstract: We present MSAs winning system for the BAREC 2025 Shared Task on fine-grained
Arabic readability assessment, achieving first place in six of six tracks. Our
approach is a confidence-weighted ensemble of four complementary transformer
models (AraBERTv2, AraELECTRA, MARBERT, and CAMeLBERT) each fine-tuned with
distinct loss functions to capture diverse readability signals. To tackle
severe class imbalance and data scarcity, we applied weighted training,
advanced preprocessing, SAMER corpus relabeling with our strongest model, and
synthetic data generation via Gemini 2.5 Flash, adding about 10,000 rare-level
samples. A targeted post-processing step corrected prediction distribution
skew, delivering a 6.3 percent Quadratic Weighted Kappa (QWK) gain. Our system
reached 87.5 percent QWK at the sentence level and 87.4 percent at the document
level, demonstrating the power of model and loss diversity, confidence-informed
fusion, and intelligent augmentation for robust Arabic readability prediction.

</details>


### [122] [Established Psychometric vs. Ecologically Valid Questionnaires: Rethinking Psychological Assessments in Large Language Models](https://arxiv.org/abs/2509.10078)
*Dongmin Choi,Woojung Song,Jongwook Han,Eun-Ju Lee,Yohan Jo*

Main category: cs.CL

TL;DR: Established psychometric questionnaires yield different personality profiles for LLMs compared to ecologically valid questionnaires, showing deviations from real-world query contexts and creating misleading impressions of stable constructs.


<details>
  <summary>Details</summary>
Motivation: Concerns about applying human-designed psychological questionnaires to LLMs due to lack of ecological validity - whether survey questions adequately reflect real-world contexts where LLMs generate responses to user queries.

Method: Comprehensive comparative analysis between established psychometric questionnaires (e.g., BFI, PVQ) and ecologically valid questionnaires for measuring LLM personality traits and values.

Result: Established questionnaires: (1) yield substantially different LLM profiles from ecologically valid ones, (2) have insufficient items for stable measurement, (3) create misleading impressions of stable constructs in LLMs, (4) yield exaggerated profiles for persona-prompted LLMs.

Conclusion: Cautions against using established psychological questionnaires for LLMs due to validity concerns and recommends ecologically valid approaches that better reflect real-world usage contexts.

Abstract: Researchers have applied established psychometric questionnaires (e.g., BFI,
PVQ) to measure the personality traits and values reflected in the responses of
Large Language Models (LLMs). However, concerns have been raised about applying
these human-designed questionnaires to LLMs. One such concern is their lack of
ecological validity--the extent to which survey questions adequately reflect
and resemble real-world contexts in which LLMs generate texts in response to
user queries. However, it remains unclear how established questionnaires and
ecologically valid questionnaires differ in their outcomes, and what insights
these differences may provide. In this paper, we conduct a comprehensive
comparative analysis of the two types of questionnaires. Our analysis reveals
that established questionnaires (1) yield substantially different profiles of
LLMs from ecologically valid ones, deviating from the psychological
characteristics expressed in the context of user queries, (2) suffer from
insufficient items for stable measurement, (3) create misleading impressions
that LLMs possess stable constructs, and (4) yield exaggerated profiles for
persona-prompted LLMs. Overall, our work cautions against the use of
established psychological questionnaires for LLMs. Our code will be released
upon publication.

</details>


### [123] [Querying Climate Knowledge: Semantic Retrieval for Scientific Discovery](https://arxiv.org/abs/2509.10087)
*Mustapha Adamu,Qi Zhang,Huitong Pan,Longin Jan Latecki,Eduard C. Dragut*

Main category: cs.CL

TL;DR: A domain-specific Knowledge Graph for climate science that enables semantic queries to find precise connections between models, datasets, regions, and variables, integrated with LLMs for improved question answering.


<details>
  <summary>Details</summary>
Motivation: The growing complexity and volume of climate science literature makes it difficult for researchers to find relevant information across different models, datasets, regions, and variables using traditional keyword-based search methods.

Method: Built a domain-specific Knowledge Graph from climate publications and scientific texts, using Cypher queries for structured semantic queries, and integrated it with large language models in RAG systems.

Result: The Knowledge Graph supports precise semantic queries that help researchers discover connections such as which models have been validated in specific regions or which datasets are used with certain teleconnection patterns.

Conclusion: This work demonstrates real-world value for climate researchers and model developers by providing accurate, contextual scientific information through semantic querying beyond traditional keyword search.

Abstract: The growing complexity and volume of climate science literature make it
increasingly difficult for researchers to find relevant information across
models, datasets, regions, and variables. This paper introduces a
domain-specific Knowledge Graph (KG) built from climate publications and
broader scientific texts, aimed at improving how climate knowledge is accessed
and used. Unlike keyword based search, our KG supports structured, semantic
queries that help researchers discover precise connections such as which models
have been validated in specific regions or which datasets are commonly used
with certain teleconnection patterns. We demonstrate how the KG answers such
questions using Cypher queries, and outline its integration with large language
models in RAG systems to improve transparency and reliability in
climate-related question answering. This work moves beyond KG construction to
show its real world value for climate researchers, model developers, and others
who rely on accurate, contextual scientific information.

</details>


### [124] [Arabic Large Language Models for Medical Text Generation](https://arxiv.org/abs/2509.10095)
*Abdulrahman Allam,Seif Ahmed,Ali Hamdi,Ammar Mohammed*

Main category: cs.CL

TL;DR: Fine-tuned LLMs for Arabic medical text generation to provide accurate medical advice, with Mistral-7B achieving best performance (68.5% F1-score).


<details>
  <summary>Details</summary>
Motivation: Address limitations in existing hospital management systems that lack accurate real-time medical advice for irregular inputs and underrepresented languages like Arabic.

Method: Collected real-world medical conversations from social media, preprocessed for Arabic dialects, and fine-tuned Mistral-7B, LLaMA-2-7B, and GPT-2 Medium models.

Result: Fine-tuned Mistral-7B outperformed others with 68.5% precision, 69.08% recall, and 68.5% F1-score, producing coherent medical replies to informal input.

Conclusion: Generative AI offers scalable solution for healthcare challenges in diverse linguistic environments, advancing hospital management systems.

Abstract: Efficient hospital management systems (HMS) are critical worldwide to address
challenges such as overcrowding, limited resources, and poor availability of
urgent health care. Existing methods often lack the ability to provide
accurate, real-time medical advice, particularly for irregular inputs and
underrepresented languages. To overcome these limitations, this study proposes
an approach that fine-tunes large language models (LLMs) for Arabic medical
text generation. The system is designed to assist patients by providing
accurate medical advice, diagnoses, drug recommendations, and treatment plans
based on user input. The research methodology required the collection of a
unique dataset from social media platforms, capturing real-world medical
conversations between patients and doctors. The dataset, which includes patient
complaints together with medical advice, was properly cleaned and preprocessed
to account for multiple Arabic dialects. Fine-tuning state-of-the-art
generative models, such as Mistral-7B-Instruct-v0.2, LLaMA-2-7B, and GPT-2
Medium, optimized the system's ability to generate reliable medical text.
Results from evaluations indicate that the fine-tuned Mistral-7B model
outperformed the other models, achieving average BERT (Bidirectional Encoder
Representations from Transformers) Score values in precision, recall, and
F1-scores of 68.5\%, 69.08\%, and 68.5\%, respectively. Comparative
benchmarking and qualitative assessments validate the system's ability to
produce coherent and relevant medical replies to informal input. This study
highlights the potential of generative artificial intelligence (AI) in
advancing HMS, offering a scalable and adaptable solution for global healthcare
challenges, especially in linguistically and culturally diverse environments.

</details>


### [125] [Scaling Arabic Medical Chatbots Using Synthetic Data: Enhancing Generative AI with Synthetic Patient Records](https://arxiv.org/abs/2509.10108)
*Abdulrahman Allam,Seif Ahmed,Ali Hamdi,Khaled Shaban*

Main category: cs.CL

TL;DR: Synthetic data augmentation using ChatGPT-4o and Gemini 2.5 Pro expands Arabic medical chatbot training data from 20,000 to 100,000 records, improving LLM performance with ChatGPT-4o data showing best results.


<details>
  <summary>Details</summary>
Motivation: Addressing the scarcity of large-scale, high-quality annotated Arabic medical datasets that constrain medical chatbot development and limit model scalability and generalization.

Method: Generated 80,000 synthetic question-answer pairs using ChatGPT-4o and Gemini 2.5 Pro, semantically filtered and manually validated them, then fine-tuned five LLMs including Mistral-7B and AraGPT2, with ablation study comparing synthetic data sources.

Result: ChatGPT-4o data consistently led to higher F1-scores and fewer hallucinations across all models compared to Gemini-generated data.

Conclusion: Synthetic augmentation is a viable practical solution for enhancing domain-specific language models in low-resource medical NLP, enabling more inclusive, scalable, and accurate Arabic healthcare chatbot systems.

Abstract: The development of medical chatbots in Arabic is significantly constrained by
the scarcity of large-scale, high-quality annotated datasets. While prior
efforts compiled a dataset of 20,000 Arabic patient-doctor interactions from
social media to fine-tune large language models (LLMs), model scalability and
generalization remained limited. In this study, we propose a scalable synthetic
data augmentation strategy to expand the training corpus to 100,000 records.
Using advanced generative AI systems ChatGPT-4o and Gemini 2.5 Pro we generated
80,000 contextually relevant and medically coherent synthetic question-answer
pairs grounded in the structure of the original dataset. These synthetic
samples were semantically filtered, manually validated, and integrated into the
training pipeline. We fine-tuned five LLMs, including Mistral-7B and AraGPT2,
and evaluated their performance using BERTScore metrics and expert-driven
qualitative assessments. To further analyze the effectiveness of synthetic
sources, we conducted an ablation study comparing ChatGPT-4o and
Gemini-generated data independently. The results showed that ChatGPT-4o data
consistently led to higher F1-scores and fewer hallucinations across all
models. Overall, our findings demonstrate the viability of synthetic
augmentation as a practical solution for enhancing domain-specific language
models in-low resource medical NLP, paving the way for more inclusive,
scalable, and accurate Arabic healthcare chatbot systems.

</details>


### [126] [Prominence-aware automatic speech recognition for conversational speech](https://arxiv.org/abs/2509.10116)
*Julian Linke,Barbara Schuppler*

Main category: cs.CL

TL;DR: Combining prominence detection with ASR for Austrian German using wav2vec2 models, achieving 85.53% prominence accuracy without affecting baseline ASR performance.


<details>
  <summary>Details</summary>
Motivation: To investigate prominence-aware automatic speech recognition by integrating prosodic prominence detection with speech recognition for conversational Austrian German, enabling prosody-enhanced ASR applications.

Method: Fine-tuned wav2vec2 models for word-level prominence detection, used the detector to automatically annotate a large corpus, then trained novel prominence-aware ASR systems that simultaneously transcribe words and prominence levels.

Result: Integration of prominence information did not change ASR performance compared to baseline, but achieved 85.53% prominence detection accuracy for utterances with correct word recognition.

Conclusion: Transformer-based models can effectively encode prosodic information, representing a novel contribution to prosody-enhanced ASR with potential applications in linguistic research and prosody-informed dialogue systems.

Abstract: This paper investigates prominence-aware automatic speech recognition (ASR)
by combining prominence detection and speech recognition for conversational
Austrian German. First, prominence detectors were developed by fine-tuning
wav2vec2 models to classify word-level prominence. The detector was then used
to automatically annotate prosodic prominence in a large corpus. Based on those
annotations, we trained novel prominence-aware ASR systems that simultaneously
transcribe words and their prominence levels. The integration of prominence
information did not change performance compared to our baseline ASR system,
while reaching a prominence detection accuracy of 85.53% for utterances where
the recognized word sequence was correct. This paper shows that
transformer-based models can effectively encode prosodic information and
represents a novel contribution to prosody-enhanced ASR, with potential
applications for linguistic research and prosody-informed dialogue systems.

</details>


### [127] [Population-Aligned Persona Generation for LLM-based Social Simulation](https://arxiv.org/abs/2509.10127)
*Zhengyu Hu,Zheyuan Xiao,Max Xiong,Yuxuan Lei,Tianfu Wang,Jianxun Lian,Kaize Ding,Ziang Xiao,Nicholas Jing Yuan,Xing Xie*

Main category: cs.CL

TL;DR: A systematic framework for generating high-quality, population-aligned persona sets for LLM-based social simulations that reduces bias and improves representation of real-world diversity.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based social simulations often overlook persona generation complexities and introduce biases through unrepresentative persona sets, limiting their usefulness for computational social science.

Method: Leverages LLMs to generate narrative personas from social media data, applies quality assessment filtering, uses importance sampling for global alignment with psychometric distributions (e.g., Big Five traits), and includes task-specific adaptation for targeted subpopulations.

Result: Extensive experiments show the method significantly reduces population-level bias and enables accurate, flexible social simulation for various research and policy applications.

Conclusion: The proposed framework provides a systematic approach to creating authentic, representative persona sets that address key limitations in current LLM-driven social simulation methodologies.

Abstract: Recent advances in large language models (LLMs) have enabled human-like
social simulations at unprecedented scale and fidelity, offering new
opportunities for computational social science. A key challenge, however, is
the construction of persona sets that authentically represent the diversity and
distribution of real-world populations. Most existing LLM-based social
simulation studies focus primarily on designing agentic frameworks and
simulation environments, often overlooking the complexities of persona
generation and the potential biases introduced by unrepresentative persona
sets. In this paper, we propose a systematic framework for synthesizing
high-quality, population-aligned persona sets for LLM-driven social simulation.
Our approach begins by leveraging LLMs to generate narrative personas from
long-term social media data, followed by rigorous quality assessment to filter
out low-fidelity profiles. We then apply importance sampling to achieve global
alignment with reference psychometric distributions, such as the Big Five
personality traits. To address the needs of specific simulation contexts, we
further introduce a task-specific module that adapts the globally aligned
persona set to targeted subpopulations. Extensive experiments demonstrate that
our method significantly reduces population-level bias and enables accurate,
flexible social simulation for a wide range of research and policy
applications.

</details>


### [128] [Towards Reliable and Interpretable Document Question Answering via VLMs](https://arxiv.org/abs/2509.10129)
*Alessio Chen,Simone Giovannini,Andrea Gemelli,Fabio Coppini,Simone Marinai*

Main category: cs.CL

TL;DR: DocExplainerV0 is a plug-and-play bounding-box module that decouples answer generation from spatial localization to improve answer localization in document understanding VLMs.


<details>
  <summary>Details</summary>
Motivation: Accurate answer localization within documents remains a major challenge for Vision-Language Models, limiting interpretability and real-world applicability despite strong text extraction capabilities.

Method: Introduces DocExplainerV0, a plug-and-play bounding-box prediction module that works with existing VLMs without requiring fine-tuning, decoupling answer generation from spatial localization.

Result: Systematic evaluation reveals a gap between textual accuracy and spatial grounding, showing correct answers often lack reliable localization. The framework establishes benchmarks for future research.

Conclusion: The proposed standardized framework highlights localization shortcomings and provides a foundation for developing more interpretable and robust document information extraction VLMs.

Abstract: Vision-Language Models (VLMs) have shown strong capabilities in document
understanding, particularly in identifying and extracting textual information
from complex documents. Despite this, accurately localizing answers within
documents remains a major challenge, limiting both interpretability and
real-world applicability. To address this, we introduce
\textit{DocExplainerV0}, a plug-and-play bounding-box prediction module that
decouples answer generation from spatial localization. This design makes it
applicable to existing VLMs, including proprietary systems where fine-tuning is
not feasible. Through systematic evaluation, we provide quantitative insights
into the gap between textual accuracy and spatial grounding, showing that
correct answers often lack reliable localization. Our standardized framework
highlights these shortcomings and establishes a benchmark for future research
toward more interpretable and robust document information extraction VLMs.

</details>


### [129] [Benchmark of stylistic variation in LLM-generated texts](https://arxiv.org/abs/2509.10179)
*Jiří Milička,Anna Marklová,Václav Cvrček*

Main category: cs.CL

TL;DR: Study compares human-written vs. LLM-generated texts using Biber's multidimensional analysis, finding systematic differences in register variation across English and Czech, with benchmark for model comparison.


<details>
  <summary>Details</summary>
Motivation: To investigate how large language models differ from humans in register variation patterns and create a benchmark for comparing LLM performance across interpretable dimensions.

Method: Applied Biber's multidimensional analysis (MDA) to human-written texts (BE-21 corpus) and AI-generated counterparts (AI-Brown corpus). Replicated analysis on Czech using AI-Koditex corpus. Examined 16 frontier LLM models with various settings and prompts, comparing base vs instruction-tuned models.

Result: Identified dimensions where LLMs differ most significantly and systematically from human writing patterns. Found differences between base models and instruction-tuned models across both English and Czech languages.

Conclusion: Created a benchmark that allows comparison and ranking of LLM models based on interpretable dimensions of register variation, providing insights into how AI-generated text differs from human writing patterns.

Abstract: This study investigates the register variation in texts written by humans and
comparable texts produced by large language models (LLMs). Biber's
multidimensional analysis (MDA) is applied to a sample of human-written texts
and AI-created texts generated to be their counterparts to find the dimensions
of variation in which LLMs differ most significantly and most systematically
from humans. As textual material, a new LLM-generated corpus AI-Brown is used,
which is comparable to BE-21 (a Brown family corpus representing contemporary
British English). Since all languages except English are underrepresented in
the training data of frontier LLMs, similar analysis is replicated on Czech
using AI-Koditex corpus and Czech multidimensional model. Examined were 16
frontier models in various settings and prompts, with emphasis placed on the
difference between base models and instruction-tuned models. Based on this, a
benchmark is created through which models can be compared with each other and
ranked in interpretable dimensions.

</details>


### [130] [Incongruent Positivity: When Miscalibrated Positivity Undermines Online Supportive Conversations](https://arxiv.org/abs/2509.10184)
*Leen Almajed,Abeer ALdayel*

Main category: cs.CL

TL;DR: LLMs tend to generate unrealistic positive responses that feel dismissive, especially in high-stakes emotional contexts like grief and anxiety, requiring better emotional calibration.


<details>
  <summary>Details</summary>
Motivation: To examine how well-intended positivity can misfire in emotionally supportive conversations, becoming dismissive or minimizing, and to compare this phenomenon in both human and LLM responses across different emotional intensity levels.

Method: Collected real user-assistant dialogues from Reddit across emotional intensities, generated additional LLM responses, categorized conversations into Mild (relationship tension, general advice) and Severe (grief, anxiety) levels, finetuned LLMs on emotional datasets, and developed a weakly supervised multilabel classifier ensemble using DeBERTa and MentalBERT.

Result: LLMs are more prone to unrealistic positivity through dismissive and minimizing tone, particularly in high-stakes contexts. The classifier ensemble showed improved detection of incongruent positivity types across different concern levels.

Conclusion: There's a need to move beyond generic positive responses and develop congruent support measures that balance positive affect with emotional acknowledgment, paving the way for context-aware and trust-preserving conversation systems.

Abstract: In emotionally supportive conversations, well-intended positivity can
sometimes misfire, leading to responses that feel dismissive, minimizing, or
unrealistically optimistic. We examine this phenomenon of incongruent
positivity as miscalibrated expressions of positive support in both human and
LLM generated responses. To this end, we collected real user-assistant
dialogues from Reddit across a range of emotional intensities and generated
additional responses using large language models for the same context. We
categorize these conversations by intensity into two levels: Mild, which covers
relationship tension and general advice, and Severe, which covers grief and
anxiety conversations. This level of categorization enables a comparative
analysis of how supportive responses vary across lower and higher stakes
contexts. Our analysis reveals that LLMs are more prone to unrealistic
positivity through dismissive and minimizing tone, particularly in high-stakes
contexts. To further study the underlying dimensions of this phenomenon, we
finetune LLMs on datasets with strong and weak emotional reactions. Moreover,
we developed a weakly supervised multilabel classifier ensemble (DeBERTa and
MentalBERT) that shows improved detection of incongruent positivity types
across two sorts of concerns (Mild and Severe). Our findings shed light on the
need to move beyond merely generating generic positive responses and instead
study the congruent support measures to balance positive affect with emotional
acknowledgment. This approach offers insights into aligning large language
models with affective expectations in the online supportive dialogue, paving
the way toward context-aware and trust preserving online conversation systems.

</details>


### [131] [Beyond Token Limits: Assessing Language Model Performance on Long Text Classification](https://arxiv.org/abs/2509.10199)
*Miklós Sebők,Viktor Kovács,Martin Bánóczy,Daniel Møller Eriksen,Nathalie Neptune,Philippe Roussille*

Main category: cs.CL

TL;DR: Analysis of long-text classification for legal documents using various models shows no advantage for specialized long-context models like Longformer, with open models outperforming GPT variants.


<details>
  <summary>Details</summary>
Motivation: Address the limitation of standard LLMs (like BERT/RoBERTa) that can't process long legal documents (hundreds of pages) due to token length constraints, particularly for policy topic classification tasks.

Method: Experiments with XLM-RoBERTa, Longformer, GPT-3.5, and GPT-4 models across 5 languages for multiclass classification using the Comparative Agendas Project's 21 policy topic labels.

Result: No advantage found for Longformer (specifically designed for long inputs). Best-performing open models outperformed GPT variants. Performance influenced by support and substance overlaps between categories.

Conclusion: Specialized long-context models don't necessarily outperform standard models for long-text classification tasks, and category relationships significantly impact performance on lengthy legal documents.

Abstract: The most widely used large language models in the social sciences (such as
BERT, and its derivatives, e.g. RoBERTa) have a limitation on the input text
length that they can process to produce predictions. This is a particularly
pressing issue for some classification tasks, where the aim is to handle long
input texts. One such area deals with laws and draft laws (bills), which can
have a length of multiple hundred pages and, therefore, are not particularly
amenable for processing with models that can only handle e.g. 512 tokens. In
this paper, we show results from experiments covering 5 languages with
XLM-RoBERTa, Longformer, GPT-3.5, GPT-4 models for the multiclass
classification task of the Comparative Agendas Project, which has a codebook of
21 policy topic labels from education to health care. Results show no
particular advantage for the Longformer model, pre-trained specifically for the
purposes of handling long inputs. The comparison between the GPT variants and
the best-performing open model yielded an edge for the latter. An analysis of
class-level factors points to the importance of support and substance overlaps
between specific categories when it comes to performance on long text inputs.

</details>


### [132] [SI-FACT: Mitigating Knowledge Conflict via Self-Improving Faithfulness-Aware Contrastive Tuning](https://arxiv.org/abs/2509.10208)
*Shengqiang Fu*

Main category: cs.CL

TL;DR: Self Improving Faithfulness Aware Contrastive Tuning (SI FACT) framework uses self-instruct mechanism to generate contrastive learning data and improves LLM faithfulness by 6.2% on knowledge conflict benchmarks.


<details>
  <summary>Details</summary>
Motivation: Large Language Models often generate unfaithful responses in knowledge-intensive tasks due to knowledge conflict - preferring internal parametric knowledge over provided context.

Method: Self-instruct mechanism generates structured contrastive learning data (anchor, positive, negative samples) automatically. Contrastive learning trains model to pull faithful responses closer and push unfaithful responses apart in representation space.

Result: SI FACT model based on Llama3 8B Instruct improves Contextual Recall Rate by 6.2% over best baseline on ECARE KRE and COSE KRE benchmarks, while reducing dependence on internal memory.

Conclusion: SI FACT provides strong effectiveness and high data efficiency in enhancing contextual faithfulness of LLMs, offering practical pathway toward more proactive and trustworthy language models.

Abstract: Large Language Models often generate unfaithful responses in knowledge
intensive tasks due to knowledge conflict,that is,a preference for relying on
internal parametric knowledge rather than the provided context.To address this
issue,we propose a novel self improving framework,Self Improving Faithfulness
Aware Contrastive Tuning.The framework uses a self instruct mechanism that
allows the base LLM to automatically generate high quality,structured
contrastive learning data,including anchor samples,semantically equivalent
positive samples,and negative samples simulating unfaithful scenarios.This
approach significantly reduces the cost of manual
annotation.Subsequently,contrastive learning is applied to train the
model,enabling it to pull faithful responses closer and push unfaithful
responses farther apart in the representation space.Experiments on knowledge
conflict evaluation benchmarks ECARE KRE and COSE KRE show that the SI FACT
model based on Llama3 8B Instruct improves the Contextual Recall Rate by 6.2%
over the best baseline method,while significantly reducing dependence on
internal memory.The results indicate that SI FACT provides strong effectiveness
and high data efficiency in enhancing the contextual faithfulness of
LLMs,offering a practical pathway toward building more proactive and
trustworthy language models.

</details>


### [133] [Dropping Experts, Recombining Neurons: Retraining-Free Pruning for Sparse Mixture-of-Experts LLMs](https://arxiv.org/abs/2509.10377)
*Yixiao Zhou,Ziyu Zhao,Dongzhou Cheng,zhiliang wu,Jie Gui,Yi Yang,Fei Wu,Yu Cheng,Hehe Fan*

Main category: cs.CL

TL;DR: DERN is a retraining-free framework that prunes redundant experts and recombines neurons at the segment level to reduce memory usage in SMoE models while improving performance.


<details>
  <summary>Details</summary>
Motivation: SMoE architectures require loading all expert parameters despite sparse activation, leading to high memory usage and deployment challenges. Previous methods focused on expert-level operations but neglected neuron-level structure and semantic conflicts.

Method: Three-step approach: 1) Prune redundant experts using router statistics, 2) Decompose experts into neuron-level segments and assign to most compatible retained experts, 3) Merge segments within each retained expert to create compact representations.

Result: Improves performance by more than 5% on commonsense reasoning and MMLU benchmarks under 50% expert sparsity, reduces number of experts and memory usage without extra training.

Conclusion: DERN enables more efficient deployment of SMoE LLMs by addressing neuron-level misalignment and semantic conflicts, achieving better performance with reduced memory requirements.

Abstract: Sparse Mixture-of-Experts (SMoE) architectures are widely used in large
language models (LLMs) due to their computational efficiency. However, though
only a few experts are activated for each token, SMoE still requires loading
all expert parameters, leading to high memory usage and challenges in
deployment. Previous work has tried to reduce the overhead by pruning and
merging experts, but primarily focused on expert-level operations, leaving
neuron-level structure underexplored. We propose DERN (Dropping Experts,
Recombining Neurons), a task-agnostic and retraining-free framework for expert
pruning and reconstruction. We observe that experts are often misaligned and
contain semantic conflicts at the neuron level, which poses challenges for
direct merging. To solve this, DERN works in three steps: it first prunes
redundant experts using router statistics; then it decomposes them into
neuron-level expert segments, assigning each segment to its most compatible
retained expert; and finally, it merges segments within each retained expert to
build a compact representation. Experiments on Mixtral, Qwen, and DeepSeek SMoE
models show that DERN improves performance by more than 5% on commonsense
reasoning and MMLU benchmarks under 50% expert sparsity, without extra
training. It also greatly reduces the number of experts and memory usage,
making SMoE LLMs easier to deploy in practice.

</details>


### [134] [Is In-Context Learning Learning?](https://arxiv.org/abs/2509.10414)
*Adrian de Wynter*

Main category: cs.CL

TL;DR: In-context learning (ICL) does constitute learning mathematically but has limitations in generalizing to unseen tasks. Performance becomes insensitive to various factors when exemplars are numerous, but autoregression's ad-hoc encoding lacks robustness.


<details>
  <summary>Details</summary>
Motivation: To investigate whether in-context learning truly constitutes learning or is merely deduction, and to characterize its capabilities and limitations through empirical analysis.

Method: Large-scale analysis of ICL by ablating and accounting for memorization, pretraining, distributional shifts, and prompting style/phrasing across various conditions.

Result: ICL is an effective learning paradigm but limited in generalization to unseen tasks. With numerous exemplars, accuracy becomes insensitive to exemplar distribution, model, prompt style, and linguistic features, but shows distributional sensitivity in certain prompting styles like chain-of-thought.

Conclusion: Autoregression's ad-hoc encoding mechanism is not robust, suggesting limited all-purpose generalizability despite ICL's mathematical validity as a learning approach.

Abstract: In-context learning (ICL) allows some autoregressive models to solve tasks
via next-token prediction and without needing further training. This has led to
claims about these model's ability to solve (learn) unseen tasks with only a
few shots (exemplars) in the prompt. However, deduction does not always imply
learning, as ICL does not explicitly encode a given observation. Instead, the
models rely on their prior knowledge and the exemplars given, if any. We argue
that, mathematically, ICL does constitute learning, but its full
characterisation requires empirical work. We then carry out a large-scale
analysis of ICL ablating out or accounting for memorisation, pretraining,
distributional shifts, and prompting style and phrasing. We find that ICL is an
effective learning paradigm, but limited in its ability to learn and generalise
to unseen tasks. We note that, in the limit where exemplars become more
numerous, accuracy is insensitive to exemplar distribution, model, prompt
style, and the input's linguistic features. Instead, it deduces patterns from
regularities in the prompt, which leads to distributional sensitivity,
especially in prompting styles such as chain-of-thought. Given the varied
accuracies on formally similar tasks, we conclude that autoregression's ad-hoc
encoding is not a robust mechanism, and suggests limited all-purpose
generalisability.

</details>


### [135] [Long Context Automated Essay Scoring with Language Models](https://arxiv.org/abs/2509.10417)
*Christopher Ormerod,Gitit Kehat*

Main category: cs.CL

TL;DR: This paper evaluates transformer models with architectural modifications to handle long essays for automated scoring, addressing length limitations that affect rubric assessment.


<details>
  <summary>Details</summary>
Motivation: Standard transformer models have fixed maximum length constraints, forcing truncation of longer essays which undermines the ability to properly assess organizational elements in automated essay scoring rubrics.

Method: Evaluated several models with architectural modifications including fine-tuned versions of XLNet, Longformer, ModernBERT, Mamba, and Llama models using the Kaggle ASAP 2.0 dataset.

Result: Not specified in the abstract - the paper presents evaluation results of these modified models for handling long essay scoring.

Conclusion: Architectural modifications to transformer models can help overcome length limitations for automated essay scoring, enabling better assessment of organizational elements that require long contexts.

Abstract: Transformer-based language models are architecturally constrained to process
text of a fixed maximum length. Essays written by higher-grade students
frequently exceed the maximum allowed length for many popular open-source
models. A common approach to addressing this issue when using these models for
Automated Essay Scoring is to truncate the input text. This raises serious
validity concerns as it undermines the model's ability to fully capture and
evaluate organizational elements of the scoring rubric, which requires long
contexts to assess. In this study, we evaluate several models that incorporate
architectural modifications of the standard transformer architecture to
overcome these length limitations using the Kaggle ASAP 2.0 dataset. The models
considered in this study include fine-tuned versions of XLNet, Longformer,
ModernBERT, Mamba, and Llama models.

</details>


### [136] [RefactorCoderQA: Benchmarking LLMs for Multi-Domain Coding Question Solutions in Cloud and Edge Deployment](https://arxiv.org/abs/2509.10436)
*Shadikur Rahman,Aroosa Hameed,Gautam Srivastava,Syed Muhammad Danish*

Main category: cs.CL

TL;DR: A cloud-edge collaborative architecture with three specialized LLM components (GuideLLM, SolverLLM, JudgeLLM) achieves state-of-the-art performance (76.84% accuracy) on the new RefactorCoderQA benchmark for multi-domain coding tasks.


<details>
  <summary>Details</summary>
Motivation: To optimize reasoning and problem-solving capabilities of LLMs by addressing limitations of existing benchmarks and creating a structured multi-agent framework for coding tasks.

Method: Proposed a cloud-edge architecture with three components: GuideLLM (edge guidance), SolverLLM (cloud solution generation), and JudgeLLM (automated evaluation). Created RefactorCoderQA benchmark covering Software Engineering, Data Science, ML, and NLP using Stack Overflow challenges.

Result: RefactorCoder-MoE achieved 76.84% overall accuracy, significantly outperforming leading open-source and commercial baselines. Human evaluations confirmed interpretability, accuracy, and practical relevance. System metrics (throughput, latency) were also evaluated.

Conclusion: The proposed architecture demonstrates superior performance in multi-domain coding tasks, validated by both automated and human evaluations, with comprehensive system-level performance insights.

Abstract: To optimize the reasoning and problem-solving capabilities of Large Language
Models (LLMs), we propose a novel cloud-edge collaborative architecture that
enables a structured, multi-agent prompting framework. This framework comprises
three specialized components: GuideLLM, a lightweight model deployed at the
edge to provide methodological guidance; SolverLLM, a more powerful model
hosted in the cloud responsible for generating code solutions; and JudgeLLM, an
automated evaluator for assessing solution correctness and quality. To evaluate
and demonstrate the effectiveness of this architecture in realistic settings,
we introduce RefactorCoderQA, a comprehensive benchmark designed to evaluate
and enhance the performance of Large Language Models (LLMs) across multi-domain
coding tasks. Motivated by the limitations of existing benchmarks,
RefactorCoderQA systematically covers various technical domains, including
Software Engineering, Data Science, Machine Learning, and Natural Language
Processing, using authentic coding challenges from Stack Overflow. Extensive
experiments reveal that our fine-tuned model, RefactorCoder-MoE, achieves
state-of-the-art performance, significantly outperforming leading open-source
and commercial baselines with an overall accuracy of 76.84%. Human evaluations
further validate the interpretability, accuracy, and practical relevance of the
generated solutions. In addition, we evaluate system-level metrics, such as
throughput and latency, to gain deeper insights into the performance
characteristics and trade-offs of the proposed architecture.

</details>


### [137] [DeepDive: Advancing Deep Search Agents with Knowledge Graphs and Multi-Turn RL](https://arxiv.org/abs/2509.10446)
*Rui Lu,Zhenyu Hou,Zihan Wang,Hanchen Zhang,Xiao Liu,Yujiang Li,Shi Feng,Jie Tang,Yuxiao Dong*

Main category: cs.CL

TL;DR: DeepDive enhances LLMs as deep search agents through automated question synthesis from knowledge graphs and multi-turn reinforcement learning, achieving state-of-the-art performance on browsing benchmarks.


<details>
  <summary>Details</summary>
Motivation: Open LLMs perform poorly as deep search agents due to limited long-horizon reasoning with browsing tools and lack of sufficiently difficult supervised training data.

Method: Automatically synthesizes complex questions from open knowledge graphs and applies end-to-end multi-turn reinforcement learning to enhance LLMs' long-horizon reasoning capabilities for deep search.

Result: DeepDive-32B achieves competitive results on BrowseComp, outperforming WebSailor, DeepSeek-R1-Browse, and Search-o1. Multi-turn RL training significantly improves deep search ability across multiple benchmarks and enables test-time scaling of tool calls.

Conclusion: The approach successfully addresses LLM limitations in deep search tasks through automated data synthesis and reinforcement learning, demonstrating improved performance and scalability for complex browsing tasks.

Abstract: Augmenting large language models (LLMs) with browsing tools substantially
improves their potential as deep search agents to solve complex, real-world
tasks. Yet, open LLMs still perform poorly in such settings due to limited
long-horizon reasoning capacity with browsing tools and the lack of
sufficiently difficult supervised data. To address these challenges, we present
DeepDive to advance deep search agents. First, we propose a strategy to
automatically synthesize complex, difficult, and hard-to-find questions from
open knowledge graphs. Second, we apply end-to-end multi-turn reinforcement
learning (RL) to enhance LLMs' long-horizon reasoning with deep search.
Experiments show that DeepDive-32B achieves a new open-source competitive
result on BrowseComp, outperforming WebSailor, DeepSeek-R1-Browse, and
Search-o1. We demonstrate that multi-turn RL training improves deep search
ability and significantly contributes to the performance improvements across
multiple benchmarks. We observe that DeepDive enables test-time scaling of tool
calls and parallel sampling. All datasets, models, and code are publicly
available at https://github.com/THUDM/DeepDive.

</details>


### [138] [WhisTLE: Deeply Supervised, Text-Only Domain Adaptation for Pretrained Speech Recognition Transformers](https://arxiv.org/abs/2509.10452)
*Akshat Pandey,Karun Kumar,Raphael Tang*

Main category: cs.CL

TL;DR: WhisTLE is a text-only adaptation method for pretrained ASR models that uses a VAE to model encoder outputs from text and fine-tunes the decoder, achieving significant WER reduction without extra runtime cost.


<details>
  <summary>Details</summary>
Motivation: Pretrained ASR models like Whisper need domain adaptation for unseen vocabulary, but collecting speech data is often impractical, necessitating text-only adaptation methods.

Method: Trains a variational autoencoder (VAE) to model encoder outputs from text, fine-tunes decoder using text-to-latent encoder, optionally combined with TTS adaptation. Original encoder is restored at inference.

Result: Across 4 out-of-domain datasets and 4 ASR models, WhisTLE with TTS reduces WER by 12.3% relative to TTS-only adaptation and outperforms all non-WhisTLE baselines in 27 of 32 scenarios.

Conclusion: WhisTLE provides effective text-only adaptation for ASR models, significantly improving performance on out-of-domain data without additional runtime costs.

Abstract: Pretrained automatic speech recognition (ASR) models such as Whisper perform
well but still need domain adaptation to handle unseen vocabulary and parlance.
In many real-world settings, collecting speech data is impractical,
necessitating text-only adaptation. We propose WhisTLE, a deeply supervised,
text-only adaptation method for pretrained encoder-decoder ASR models. WhisTLE
trains a variational autoencoder (VAE) to model encoder outputs from text and
fine-tunes the decoder using the learned text-to-latent encoder, optionally
combined with text-to-speech (TTS) adaptation. At inference, the original
encoder is restored, incurring no extra runtime cost. Across four out-of-domain
datasets and four ASR models, WhisTLE with TTS reduces word error rate (WER) by
12.3% relative to TTS-only adaptation and outperforms all non-WhisTLE baselines
in 27 of 32 scenarios.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [139] [Structure Matters: Brain Graph Augmentation via Learnable Edge Masking for Data-efficient Psychiatric Diagnosis](https://arxiv.org/abs/2509.09744)
*Mujie Liu,Chenze Wang,Liping Chen,Nguyen Linh Dan Le,Niharika Tewari,Ting Dang,Jiangang Ma,Feng Xia*

Main category: cs.LG

TL;DR: SAM-BG is a two-stage self-supervised learning framework that preserves structural semantics in brain graphs for psychiatric diagnosis, outperforming state-of-the-art methods in small-labeled data settings.


<details>
  <summary>Details</summary>
Motivation: Limited labeled brain network data and existing SSL methods that disrupt structural semantics in brain graphs make accurate psychiatric diagnosis challenging.

Method: Two-stage framework: 1) Pre-training stage trains edge masker on small labeled subset to capture structural semantics, 2) SSL stage uses structure-aware augmentation guided by structural priors to learn meaningful representations.

Result: Outperforms state-of-the-art methods on two real-world psychiatric datasets, especially in small-labeled data settings, and uncovers clinically relevant connectivity patterns.

Conclusion: SAM-BG effectively preserves structural semantics in brain graphs, enabling more accurate psychiatric diagnosis and enhanced interpretability through clinically relevant pattern discovery.

Abstract: The limited availability of labeled brain network data makes it challenging
to achieve accurate and interpretable psychiatric diagnoses. While
self-supervised learning (SSL) offers a promising solution, existing methods
often rely on augmentation strategies that can disrupt crucial structural
semantics in brain graphs. To address this, we propose SAM-BG, a two-stage
framework for learning brain graph representations with structural semantic
preservation. In the pre-training stage, an edge masker is trained on a small
labeled subset to capture key structural semantics. In the SSL stage, the
extracted structural priors guide a structure-aware augmentation process,
enabling the model to learn more semantically meaningful and robust
representations. Experiments on two real-world psychiatric datasets demonstrate
that SAM-BG outperforms state-of-the-art methods, particularly in small-labeled
data settings, and uncovers clinically relevant connectivity patterns that
enhance interpretability. Our code is available at
https://github.com/mjliu99/SAM-BG.

</details>


### [140] [D-CAT: Decoupled Cross-Attention Transfer between Sensor Modalities for Unimodal Inference](https://arxiv.org/abs/2509.09747)
*Leen Daher,Zhaobo Wang,Malcolm Mielle*

Main category: cs.LG

TL;DR: D-CAT enables cross-modal transfer learning without requiring paired sensor data during inference, allowing single-sensor deployment while maintaining accuracy gains from multi-modal training.


<details>
  <summary>Details</summary>
Motivation: Existing cross-modal transfer methods require paired sensor data at both training and inference, limiting deployment in resource-constrained environments where full sensor suites are not feasible.

Method: Proposes Decoupled Cross-Attention Transfer (D-CAT) with self-attention for feature extraction and a novel cross-attention alignment loss that aligns modality-specific representations without coupling classification pipelines.

Result: Achieves up to 10% F1-score gains in in-distribution scenarios when transferring from high-performing modalities. Even weaker source modalities improve target performance in out-of-distribution scenarios, as long as the target model isn't overfitted.

Conclusion: D-CAT reduces hardware redundancy while maintaining accuracy, enabling cost-sensitive deployments in environments with variable sensor availability.

Abstract: Cross-modal transfer learning is used to improve multi-modal classification
models (e.g., for human activity recognition in human-robot collaboration).
However, existing methods require paired sensor data at both training and
inference, limiting deployment in resource-constrained environments where full
sensor suites are not economically and technically usable. To address this, we
propose Decoupled Cross-Attention Transfer (D-CAT), a framework that aligns
modality-specific representations without requiring joint sensor modality
during inference. Our approach combines a self-attention module for feature
extraction with a novel cross-attention alignment loss, which enforces the
alignment of sensors' feature spaces without requiring the coupling of the
classification pipelines of both modalities. We evaluate D-CAT on three
multi-modal human activity datasets (IMU, video, and audio) under both
in-distribution and out-of-distribution scenarios, comparing against uni-modal
models. Results show that in in-distribution scenarios, transferring from
high-performing modalities (e.g., video to IMU) yields up to 10% F1-score gains
over uni-modal training. In out-of-distribution scenarios, even weaker source
modalities (e.g., IMU to video) improve target performance, as long as the
target model isn't overfitted on the training data. By enabling single-sensor
inference with cross-modal knowledge, D-CAT reduces hardware redundancy for
perception systems while maintaining accuracy, which is critical for
cost-sensitive or adaptive deployments (e.g., assistive robots in homes with
variable sensor availability). Code is available at
https://github.com/Schindler-EPFL-Lab/D-CAT.

</details>


### [141] [Meta-Learning Reinforcement Learning for Crypto-Return Prediction](https://arxiv.org/abs/2509.09751)
*Junqiao Wang,Zhaoyang Guan,Guanyu Liu,Tianze Xia,Xianzhi Li,Shuo Yin,Xinyuan Song,Chuhan Cheng,Tianyu Shi,Alex Lee*

Main category: cs.LG

TL;DR: Meta-RL-Crypto is a transformer-based architecture combining meta-learning and reinforcement learning to create a self-improving cryptocurrency trading agent that outperforms other LLM-based methods.


<details>
  <summary>Details</summary>
Motivation: Cryptocurrency return prediction is challenging due to fast-shifting market factors (on-chain activity, news, social sentiment) and scarce/expensive labeled training data.

Method: Uses a unified transformer architecture with meta-learning and RL. Starts with instruction-tuned LLM, then iterates through three roles (actor, judge, meta-judge) in closed-loop without human supervision. Leverages multimodal market inputs and internal preference feedback.

Result: Shows good performance on real market technical indicators and outperforms other LLM-based baselines across diverse market regimes.

Conclusion: The self-improving trading agent successfully refines both trading policy and evaluation criteria autonomously, demonstrating effective cryptocurrency trading without additional human supervision.

Abstract: Predicting cryptocurrency returns is notoriously difficult: price movements
are driven by a fast-shifting blend of on-chain activity, news flow, and social
sentiment, while labeled training data are scarce and expensive. In this paper,
we present Meta-RL-Crypto, a unified transformer-based architecture that
unifies meta-learning and reinforcement learning (RL) to create a fully
self-improving trading agent. Starting from a vanilla instruction-tuned LLM,
the agent iteratively alternates between three roles-actor, judge, and
meta-judge-in a closed-loop architecture. This learning process requires no
additional human supervision. It can leverage multimodal market inputs and
internal preference feedback. The agent in the system continuously refines both
the trading policy and evaluation criteria. Experiments across diverse market
regimes demonstrate that Meta-RL-Crypto shows good performance on the technical
indicators of the real market and outperforming other LLM-based baselines.

</details>


### [142] [LAVa: Layer-wise KV Cache Eviction with Dynamic Budget Allocation](https://arxiv.org/abs/2509.09754)
*Yiqun Shen,Song Yuan,Zhengze Zhang,Xiaoliang Wang,Daxin Jiang,Nguyen Cam-Tu*

Main category: cs.LG

TL;DR: LAVa is a unified KV cache compression framework that minimizes information loss in Transformer residual streams, enabling dynamic budget allocation across layers and heads without training or multiple strategies.


<details>
  <summary>Details</summary>
Motivation: Existing KV cache compression methods are heuristic and lack dynamic budget allocation, leading to inefficient memory usage for long-context LLM inference.

Method: Minimizes information loss in Transformer residual streams, analyzes layer attention output loss, and uses derived metrics to enable layer-wise compression with dynamic head budgets and cross-layer contrast for dynamic layer budgets.

Result: Superior performance on benchmarks (LongBench, Needle-In-A-Haystack, Ruler, InfiniteBench) with new insights: dynamic layer budgets crucial for generation tasks, dynamic head budgets key for extraction tasks.

Conclusion: LAVa is the first unified strategy for cache eviction with dynamic budget allocation that maintains top performance across task types without requiring training or multiple strategies.

Abstract: KV Cache is commonly used to accelerate LLM inference with long contexts, yet
its high memory demand drives the need for cache compression. Existing
compression methods, however, are largely heuristic and lack dynamic budget
allocation. To address this limitation, we introduce a unified framework for
cache compression by minimizing information loss in Transformer residual
streams. Building on it, we analyze the layer attention output loss and derive
a new metric to compare cache entries across heads, enabling layer-wise
compression with dynamic head budgets. Additionally, by contrasting cross-layer
information, we also achieve dynamic layer budgets. LAVa is the first unified
strategy for cache eviction and dynamic budget allocation that, unlike prior
methods, does not rely on training or the combination of multiple strategies.
Experiments with benchmarks (LongBench, Needle-In-A-Haystack, Ruler, and
InfiniteBench) demonstrate its superiority. Moreover, our experiments reveal a
new insight: dynamic layer budgets are crucial for generation tasks (e.g., code
completion), while dynamic head budgets play a key role in extraction tasks
(e.g., extractive QA). As a fully dynamic compression method, LAVa consistently
maintains top performance across task types. Our code is available at
https://github.com/MGDDestiny/Lava.

</details>


### [143] [Hybrid Adaptive Conformal Offline Reinforcement Learning for Fair Population Health Management](https://arxiv.org/abs/2509.09772)
*Sanjay Basu,Sadiq Y. Patel,Parth Sheth,Bhairavi Muralidharan,Namrata Elamaran,Aakriti Kinra,Rajaie Batniji*

Main category: cs.LG

TL;DR: HACO framework combines conformal prediction with offline RL to provide safe, auditable decision support for Medicaid population health management, controlling adverse event risks while maintaining demographic fairness.


<details>
  <summary>Details</summary>
Motivation: Population health programs need safe, fair, and auditable coordination of services for Medicaid populations, requiring conservative action recommendations that control near-term risk of adverse utilization events.

Method: Hybrid Adaptive Conformal Offline RL (HACO) separates risk calibration from preference optimization: trains risk model for adverse events, derives conformal threshold to mask unsafe actions, learns preference policy on safe subset using 2.77M decisions from 168K patients.

Result: Achieves strong risk discrimination (AUC ~0.81) with calibrated threshold, maintains high safe coverage. Subgroup analyses reveal systematic value differences across demographics, highlighting importance of fairness auditing.

Conclusion: Conformal risk gating integrates effectively with offline RL to deliver conservative, auditable decision support for population health teams, demonstrating the framework's practical utility.

Abstract: Population health management programs for Medicaid populations coordinate
longitudinal outreach and services (e.g., benefits navigation, behavioral
health, social needs support, and clinical scheduling) and must be safe, fair,
and auditable. We present a Hybrid Adaptive Conformal Offline Reinforcement
Learning (HACO) framework that separates risk calibration from preference
optimization to generate conservative action recommendations at scale. In our
setting, each step involves choosing among common coordination actions (e.g.,
which member to contact, by which modality, and whether to route to a
specialized service) while controlling the near-term risk of adverse
utilization events (e.g., unplanned emergency department visits or
hospitalizations). Using a de-identified operational dataset from Waymark
comprising 2.77 million sequential decisions across 168,126 patients, HACO (i)
trains a lightweight risk model for adverse events, (ii) derives a conformal
threshold to mask unsafe actions at a target risk level, and (iii) learns a
preference policy on the resulting safe subset. We evaluate policies with a
version-agnostic fitted Q evaluation (FQE) on stratified subsets and audit
subgroup performance across age, sex, and race. HACO achieves strong risk
discrimination (AUC ~0.81) with a calibrated threshold ( {\tau} ~0.038 at
{\alpha} = 0.10), while maintaining high safe coverage. Subgroup analyses
reveal systematic differences in estimated value across demographics,
underscoring the importance of fairness auditing. Our results show that
conformal risk gating integrates cleanly with offline RL to deliver
conservative, auditable decision support for population health management
teams.

</details>


### [144] [One Head, Many Models: Cross-Attention Routing for Cost-Aware LLM Selection](https://arxiv.org/abs/2509.09782)
*Roshini Pulishetty,Mani Kishan Ghantasala,Keerthy Kaushik Dasoju,Niti Mangwani,Vishal Garimella,Aditya Mate,Somya Chatterjee,Yue Kang,Ehi Nosakhare,Sadid Hasan,Soundar Srinivasan*

Main category: cs.LG

TL;DR: A unified routing framework using cross-attention to dynamically select optimal LLMs for each query, achieving better performance and cost efficiency than existing methods.


<details>
  <summary>Details</summary>
Motivation: The proliferation of LLMs with varying costs and performance presents challenges for scalable, cost-effective deployment in real applications.

Method: Single-head cross-attention mechanism to jointly model query and model embeddings, predicting both response quality and generation cost with an exponential reward function for stability.

Result: Achieves up to 6.6% improvement in Average Improvement in Quality (AIQ) and 2.9% in maximum performance over existing routers on RouterBench benchmark.

Conclusion: Lightweight architecture generalizes effectively across domains, establishes new standard for cost-aware LLM routing with improved efficiency.

Abstract: The proliferation of large language models (LLMs) with varying computational
costs and performance profiles presents a critical challenge for scalable,
cost-effective deployment in real-world applications. We introduce a unified
routing framework that leverages a single-head cross-attention mechanism to
jointly model query and model embeddings, enabling dynamic selection of the
optimal LLM for each input query. Our approach is evaluated on RouterBench, a
large-scale, publicly available benchmark encompassing diverse LLM pools and
domains. By explicitly capturing fine-grained query-model interactions, our
router predicts both response quality and generation cost, achieving up to 6.6%
improvement in Average Improvement in Quality (AIQ) and 2.9% in maximum
performance over existing routers. To robustly balance performance and cost, we
propose an exponential reward function that enhances stability across user
preferences. The resulting architecture is lightweight, generalizes effectively
across domains, and demonstrates improved efficiency compared to prior methods,
establishing a new standard for cost-aware LLM routing.

</details>


### [145] [From the Gradient-Step Denoiser to the Proximal Denoiser and their associated convergent Plug-and-Play algorithms](https://arxiv.org/abs/2509.09793)
*Vincent Herfeld,Baudouin Denis de Senneville,Arthur Leclaire,Nicolas Papadakis*

Main category: cs.LG

TL;DR: Analysis of Gradient-Step Denoiser in Plug-and-Play algorithms, showing it can serve as exact gradient descent/proximity operators while maintaining state-of-the-art denoising performance.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between implicit image priors in Plug-and-Play algorithms and explicit functional representations, enabling better understanding and control of denoising operators.

Method: Training the Gradient-Step Denoiser to function as exact gradient descent or proximity operators of explicit functionals while preserving denoising capabilities.

Result: The Gradient-Step Denoiser successfully serves as exact mathematical operators for explicit functionals while maintaining competitive denoising performance.

Conclusion: The approach provides a way to combine explicit functional representations with state-of-the-art denoising in Plug-and-Play optimization frameworks.

Abstract: In this paper we analyze the Gradient-Step Denoiser and its usage in
Plug-and-Play algorithms. The Plug-and-Play paradigm of optimization algorithms
uses off the shelf denoisers to replace a proximity operator or a gradient
descent operator of an image prior. Usually this image prior is implicit and
cannot be expressed, but the Gradient-Step Denoiser is trained to be exactly
the gradient descent operator or the proximity operator of an explicit
functional while preserving state-of-the-art denoising capabilities.

</details>


### [146] [Distinguishing Startle from Surprise Events Based on Physiological Signals](https://arxiv.org/abs/2509.09799)
*Mansi Sharma,Alexandre Duchevet,Florian Daiber,Jean-Paul Imbert,Maurice Rekrut*

Main category: cs.LG

TL;DR: Machine learning approach using physiological signals to distinguish between startle and surprise reactions in pilots, achieving 85.7% accuracy for binary classification and 74.9% for three-class classification including baseline state.


<details>
  <summary>Details</summary>
Motivation: Unexpected events like startle and surprise impair pilot attention and decision-making, posing safety risks in aviation. These reactions are hard to distinguish in practice and existing research has studied them separately with limited focus on combined effects or physiological differentiation.

Method: Used machine learning and multi-modal fusion strategies on physiological signals to distinguish between startle and surprise events. Evaluated SVM and XGBoost classifiers with Late Fusion approach.

Result: Achieved 85.7% mean accuracy with SVM and Late Fusion for distinguishing startle vs surprise. Extended to three-class classification (Startle, Surprise, Baseline) achieved 74.9% mean accuracy with XGBoost and Late Fusion.

Conclusion: Physiological signals combined with machine learning can reliably predict and differentiate between startle and surprise reactions, providing a robust method for aviation safety applications to distinguish these critical cognitive states.

Abstract: Unexpected events can impair attention and delay decision-making, posing
serious safety risks in high-risk environments such as aviation. In particular,
reactions like startle and surprise can impact pilot performance in different
ways, yet are often hard to distinguish in practice. Existing research has
largely studied these reactions separately, with limited focus on their
combined effects or how to differentiate them using physiological data. In this
work, we address this gap by distinguishing between startle and surprise events
based on physiological signals using machine learning and multi-modal fusion
strategies. Our results demonstrate that these events can be reliably
predicted, achieving a highest mean accuracy of 85.7% with SVM and Late Fusion.
To further validate the robustness of our model, we extended the evaluation to
include a baseline condition, successfully differentiating between Startle,
Surprise, and Baseline states with a highest mean accuracy of 74.9% with
XGBoost and Late Fusion.

</details>


### [147] [Revisiting Actor-Critic Methods in Discrete Action Off-Policy Reinforcement Learning](https://arxiv.org/abs/2509.09838)
*Reza Asad,Reza Babanezhad,Sharan Vaswani*

Main category: cs.LG

TL;DR: Decoupling actor-critic entropy in discrete SAC enables DQN-level performance on Atari games without explicit exploration.


<details>
  <summary>Details</summary>
Motivation: Value-based methods like DQN dominate discrete-action off-policy RL, while policy-based methods either don't learn effectively from off-policy data (PPO) or perform poorly (SAC).

Method: Revisit actor-critic design starting from DSAC, decouple actor and critic entropy, introduce flexible off-policy framework with m-step Bellman operator and entropy-regularized policy optimization.

Result: Theoretical convergence proof to optimal regularized value function; empirical performance matching DQN on Atari games without entropy regularization or explicit exploration.

Conclusion: Decoupling entropy components enables effective off-policy actor-critic methods for discrete actions, challenging DQN's dominance in this domain.

Abstract: Value-based approaches such as DQN are the default methods for off-policy
reinforcement learning with discrete-action environments such as Atari. Common
policy-based methods are either on-policy and do not effectively learn from
off-policy data (e.g. PPO), or have poor empirical performance in the
discrete-action setting (e.g. SAC). Consequently, starting from discrete SAC
(DSAC), we revisit the design of actor-critic methods in this setting. First,
we determine that the coupling between the actor and critic entropy is the
primary reason behind the poor performance of DSAC. We demonstrate that by
merely decoupling these components, DSAC can have comparable performance as
DQN. Motivated by this insight, we introduce a flexible off-policy actor-critic
framework that subsumes DSAC as a special case. Our framework allows using an
m-step Bellman operator for the critic update, and enables combining standard
policy optimization methods with entropy regularization to instantiate the
resulting actor objective. Theoretically, we prove that the proposed methods
can guarantee convergence to the optimal regularized value function in the
tabular setting. Empirically, we demonstrate that these methods can approach
the performance of DQN on standard Atari games, and do so even without entropy
regularization or explicit exploration.

</details>


### [148] [HGEN: Heterogeneous Graph Ensemble Networks](https://arxiv.org/abs/2509.09843)
*Jiajun Shen,Yufei Jin,Yi He,Xingquan Zhu*

Main category: cs.LG

TL;DR: HGEN introduces ensemble learning for heterogeneous graphs using meta-path optimization and transformation to improve classification accuracy through diverse graph learners.


<details>
  <summary>Details</summary>
Motivation: Heterogeneity in node types, features, and neighborhood topology poses challenges for ensemble learning in graphs, requiring specialized approaches to accommodate diverse graph learners.

Method: HGEN ensembles multiple learners through meta-path and transformation-based optimization, using random dropping to create Allele GNNs, with residual-attention mechanism for calibration and correlation-regularization for diversity enhancement.

Result: Experiments on five heterogeneous networks show HGEN consistently outperforms state-of-the-art competitors by substantial margins, with proven convergence and higher regularization effectiveness.

Conclusion: HGEN successfully pioneers ensemble learning for heterogeneous graphs, demonstrating superior performance through its meta-path optimization and dual-component approach to accuracy improvement and diversity enrichment.

Abstract: This paper presents HGEN that pioneers ensemble learning for heterogeneous
graphs. We argue that the heterogeneity in node types, nodal features, and
local neighborhood topology poses significant challenges for ensemble learning,
particularly in accommodating diverse graph learners. Our HGEN framework
ensembles multiple learners through a meta-path and transformation-based
optimization pipeline to uplift classification accuracy. Specifically, HGEN
uses meta-path combined with random dropping to create Allele Graph Neural
Networks (GNNs), whereby the base graph learners are trained and aligned for
later ensembling. To ensure effective ensemble learning, HGEN presents two key
components: 1) a residual-attention mechanism to calibrate allele GNNs of
different meta-paths, thereby enforcing node embeddings to focus on more
informative graphs to improve base learner accuracy, and 2) a
correlation-regularization term to enlarge the disparity among embedding
matrices generated from different meta-paths, thereby enriching base learner
diversity. We analyze the convergence of HGEN and attest its higher
regularization magnitude over simple voting. Experiments on five heterogeneous
networks validate that HGEN consistently outperforms its state-of-the-art
competitors by substantial margin.

</details>


### [149] [Latency and Token-Aware Test-Time Compute](https://arxiv.org/abs/2509.09864)
*Jenny Y. Huang,Mehul Damani,Yousef El-Kurdi,Ramon Astudillo,Wei Sun*

Main category: cs.LG

TL;DR: A framework for dynamic compute allocation and method selection during LLM inference that optimizes both token cost and latency, outperforming static strategies like best-of-N.


<details>
  <summary>Details</summary>
Motivation: Existing inference-time scaling methods focus only on parallel generation and ignore latency, which is critical for user experience and agentic workflows requiring multiple efficient queries.

Method: Formulates inference-time scaling as dynamic compute allocation and method selection per query, incorporating both token cost and wall-clock latency considerations.

Result: Experiments on reasoning benchmarks show the approach consistently outperforms static strategies, achieving better accuracy-cost trade-offs while remaining practical for deployment.

Conclusion: Dynamic allocation framework provides superior performance over static inference-time scaling methods by optimizing both computational efficiency and latency for real-world deployment.

Abstract: Inference-time scaling has emerged as a powerful way to improve large
language model (LLM) performance by generating multiple candidate responses and
selecting among them. However, existing work on dynamic allocation for
test-time compute typically considers only parallel generation methods such as
best-of-N, overlooking incremental decoding methods like beam search, and has
largely ignored latency, focusing only on token usage. We formulate
inference-time scaling as a problem of dynamic compute allocation and method
selection, where the system must decide which strategy to apply and how much
compute to allocate on a per-query basis. Our framework explicitly incorporates
both token cost and wall-clock latency, the latter being critical for user
experience and particularly for agentic workflows where models must issue
multiple queries efficiently. Experiments on reasoning benchmarks show that our
approach consistently outperforms static strategies, achieving favorable
accuracy-cost trade-offs while remaining practical for deployment.

</details>


### [150] [Variational Neural Networks for Observable Thermodynamics (V-NOTS)](https://arxiv.org/abs/2509.09899)
*Christopher Eldred,François Gay-Balmaz,Vakhtang Putkaradze*

Main category: cs.LG

TL;DR: A novel neural network framework for predicting dissipative dynamical systems using only observable variables, respecting thermodynamic constraints and ensuring non-decreasing entropy.


<details>
  <summary>Details</summary>
Motivation: Existing data-based computing methods for physical systems require phase space variables, but in dissipative systems, momenta and entropies are often unobservable, creating a need for methods that work exclusively with observable variables.

Method: Developed a data-based computing framework using thermodynamic Lagrangian approach with neural networks that incorporate thermodynamic constraints to guarantee non-decreasing entropy evolution.

Result: The network efficiently describes phase space evolution using limited data points and relatively few parameters while maintaining thermodynamic consistency.

Conclusion: The proposed framework successfully addresses the challenge of predicting dissipative systems when only observable variables are available, ensuring thermodynamic validity with minimal data requirements.

Abstract: Much attention has recently been devoted to data-based computing of evolution
of physical systems. In such approaches, information about data points from
past trajectories in phase space is used to reconstruct the equations of motion
and to predict future solutions that have not been observed before. However, in
many cases, the available data does not correspond to the variables that define
the system's phase space. We focus our attention on the important example of
dissipative dynamical systems. In that case, the phase space consists of
coordinates, momenta and entropies; however, the momenta and entropies cannot,
in general, be observed directly. To address this difficulty, we develop an
efficient data-based computing framework based exclusively on observable
variables, by constructing a novel approach based on the \emph{thermodynamic
Lagrangian}, and constructing neural networks that respect the thermodynamics
and guarantees the non-decreasing entropy evolution. We show that our network
can provide an efficient description of phase space evolution based on a
limited number of data points and a relatively small number of parameters in
the system.

</details>


### [151] [LoFT: Parameter-Efficient Fine-Tuning for Long-tailed Semi-Supervised Learning in Open-World Scenarios](https://arxiv.org/abs/2509.09926)
*Jiahao Chen,Zhiyuan Huang,Yurou Liu,Bing Su*

Main category: cs.LG

TL;DR: LoFT framework extends long-tailed semi-supervised learning to foundation model fine-tuning, generating more reliable pseudo-labels and handling open-world scenarios with OOD samples.


<details>
  <summary>Details</summary>
Motivation: Address limitations of existing LTSSL methods that train from scratch, which suffer from overconfidence and low-quality pseudo-labels, by leveraging foundation models for better performance.

Method: Proposes LoFT framework using parameter-efficient fine-tuning of foundation models to generate reliable pseudo-labels, and LoFT-OW extension for open-world scenarios with OOD samples.

Result: Achieves superior performance on multiple benchmarks, even when using only 1% of unlabeled data compared to previous works.

Conclusion: Fine-tuning foundation models for LTSSL produces more reliable pseudo-labels and effectively handles open-world scenarios, demonstrating significant performance improvements over existing methods.

Abstract: Long-tailed learning has garnered increasing attention due to its wide
applicability in real-world scenarios. Among existing approaches, Long-Tailed
Semi-Supervised Learning (LTSSL) has emerged as an effective solution by
incorporating a large amount of unlabeled data into the imbalanced labeled
dataset. However, most prior LTSSL methods are designed to train models from
scratch, which often leads to issues such as overconfidence and low-quality
pseudo-labels. To address these challenges, we extend LTSSL into the foundation
model fine-tuning paradigm and propose a novel framework: LoFT (Long-tailed
semi-supervised learning via parameter-efficient Fine-Tuning). We demonstrate
that fine-tuned foundation models can generate more reliable pseudolabels,
thereby benefiting imbalanced learning. Furthermore, we explore a more
practical setting by investigating semi-supervised learning under open-world
conditions, where the unlabeled data may include out-of-distribution (OOD)
samples. To handle this problem, we propose LoFT-OW (LoFT under Open-World
scenarios) to improve the discriminative ability. Experimental results on
multiple benchmarks demonstrate that our method achieves superior performance
compared to previous approaches, even when utilizing only 1\% of the unlabeled
data compared with previous works.

</details>


### [152] [Multi-Play Combinatorial Semi-Bandit Problem](https://arxiv.org/abs/2509.09933)
*Shintaro Nakamura,Yuko Kuroki,Wei Chen*

Main category: cs.LG

TL;DR: Proposes MP-CSB framework extending combinatorial semi-bandits to handle non-negative integer actions with multiple feedbacks per arm, with two efficient algorithms achieving logarithmic regret in stochastic settings and adaptive regret in adversarial settings.


<details>
  <summary>Details</summary>
Motivation: Traditional combinatorial semi-bandit (CSB) is limited to binary decision spaces, excluding important applications like optimal transport and knapsack problems that require non-negative integer flows or allocations.

Method: Introduces Multi-Play Combinatorial Semi-Bandit (MP-CSB) where players select non-negative integer actions and observe multiple feedbacks per arm. Proposes two algorithms: 1) Thompson-sampling-based algorithm for exponential action spaces with O(log T) regret, and 2) Best-of-both-worlds algorithm with O(log T) variance-dependent regret in stochastic regime and O(√T) adaptive regret in adversarial regime.

Result: Algorithms achieve theoretical guarantees: logarithmic regret in stochastic settings and worst-case sublinear regret in adversarial settings. Numerical experiments show proposed methods outperform existing CSB approaches.

Conclusion: MP-CSB successfully extends combinatorial bandits to handle integer-valued actions, providing efficient algorithms with strong theoretical guarantees and practical performance improvements over existing methods.

Abstract: In the combinatorial semi-bandit (CSB) problem, a player selects an action
from a combinatorial action set and observes feedback from the base arms
included in the action. While CSB is widely applicable to combinatorial
optimization problems, its restriction to binary decision spaces excludes
important cases involving non-negative integer flows or allocations, such as
the optimal transport and knapsack problems.To overcome this limitation, we
propose the multi-play combinatorial semi-bandit (MP-CSB), where a player can
select a non-negative integer action and observe multiple feedbacks from a
single arm in each round. We propose two algorithms for the MP-CSB. One is a
Thompson-sampling-based algorithm that is computationally feasible even when
the action space is exponentially large with respect to the number of arms, and
attains $O(\log T)$ distribution-dependent regret in the stochastic regime,
where $T$ is the time horizon. The other is a best-of-both-worlds algorithm,
which achieves $O(\log T)$ variance-dependent regret in the stochastic regime
and the worst-case $\tilde{\mathcal{O}}\left( \sqrt{T} \right)$ regret in the
adversarial regime. Moreover, its regret in adversarial one is data-dependent,
adapting to the cumulative loss of the optimal action, the total quadratic
variation, and the path-length of the loss sequence. Finally, we numerically
show that the proposed algorithms outperform existing methods in the CSB
literature.

</details>


### [153] [SciML Agents: Write the Solver, Not the Solution](https://arxiv.org/abs/2509.09936)
*Saarth Gaonkar,Xiang Zheng,Haocheng Xi,Rishabh Tiwari,Kurt Keutzer,Dmitriy Morozov,Michael W. Mahoney,Amir Gholami*

Main category: cs.LG

TL;DR: LLMs can generate scientifically appropriate code for solving ODEs by selecting suitable solvers and performing stability checks, achieving high accuracy with guided prompting and fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Traditional scientific machine learning approaches struggle with accuracy and robustness. This work explores using LLMs to write numerical code instead of learning solution functions directly, shifting the burden to making domain-aware numerical choices.

Method: Introduces two datasets: a diagnostic set of misleading problems requiring algebraic simplification, and a large-scale benchmark of 1,000 diverse ODE tasks. Evaluates LLMs on executability and numerical validity using both guided/unguided prompting and fine-tuned/off-the-shelf variants.

Result: With sufficient context and guided prompts, newer instruction-following models achieve high accuracy on both executability and numerical validity. Open-source systems perform strongly without fine-tuning, while older/smaller models benefit from fine-tuning.

Conclusion: Careful prompting and fine-tuning can yield specialized LLM agents capable of reliably solving simple ODE problems by generating scientifically appropriate numerical code.

Abstract: Recent work in scientific machine learning aims to tackle scientific tasks
directly by predicting target values with neural networks (e.g.,
physics-informed neural networks, neural ODEs, neural operators, etc.), but
attaining high accuracy and robustness has been challenging. We explore an
alternative view: use LLMs to write code that leverages decades of numerical
algorithms. This shifts the burden from learning a solution function to making
domain-aware numerical choices. We ask whether LLMs can act as SciML agents
that, given a natural-language ODE description, generate runnable code that is
scientifically appropriate, selecting suitable solvers (stiff vs. non-stiff),
and enforcing stability checks. There is currently no benchmark to measure this
kind of capability for scientific computing tasks. As such, we first introduce
two new datasets: a diagnostic dataset of adversarial "misleading" problems;
and a large-scale benchmark of 1,000 diverse ODE tasks. The diagnostic set
contains problems whose superficial appearance suggests stiffness, and that
require algebraic simplification to demonstrate non-stiffness; and the
large-scale benchmark spans stiff and non-stiff ODE regimes. We evaluate open-
and closed-source LLM models along two axes: (i) unguided versus guided
prompting with domain-specific knowledge; and (ii) off-the-shelf versus
fine-tuned variants. Our evaluation measures both executability and numerical
validity against reference solutions. We find that with sufficient context and
guided prompts, newer instruction-following models achieve high accuracy on
both criteria. In many cases, recent open-source systems perform strongly
without fine-tuning, while older or smaller models still benefit from
fine-tuning. Overall, our preliminary results indicate that careful prompting
and fine-tuning can yield a specialized LLM agent capable of reliably solving
simple ODE problems.

</details>


### [154] [DyKen-Hyena: Dynamic Kernel Generation via Cross-Modal Attention for Multimodal Intent Recognition](https://arxiv.org/abs/2509.09940)
*Yifei Wang,Wenbin Wang,Yong Luo*

Main category: cs.LG

TL;DR: DyKen-Hyena reframes multimodal intent recognition from feature fusion to processing modulation, using audio-visual cues to generate dynamic convolutional kernels that directly modulate textual feature extraction at token level.


<details>
  <summary>Details</summary>
Motivation: Current multimodal fusion methods risk corrupting linguistic features with noisy or irrelevant non-verbal signals by simply adding multimodal features, failing to capture fine-grained token-level modulation where non-verbal cues should modulate rather than augment textual meaning.

Method: Translates audio-visual cues into dynamic, per-token convolutional kernels that directly modulate textual feature extraction, moving from feature fusion to processing modulation approach.

Result: Achieves state-of-the-art results on MIntRec and MIntRec2.0 benchmarks, with +10.46% F1-score improvement in out-of-scope detection, creating more robust intent representations.

Conclusion: The fine-grained processing modulation approach significantly outperforms traditional feature fusion methods, demonstrating that dynamic per-token modulation of textual features using non-verbal cues creates fundamentally more robust intent recognition.

Abstract: Though Multimodal Intent Recognition (MIR) proves effective by utilizing rich
information from multiple sources (e.g., language, video, and audio), the
potential for intent-irrelevant and conflicting information across modalities
may hinder performance from being further improved. Most current models attempt
to fuse modalities by applying mechanisms like multi-head attention to unimodal
feature sequences and then adding the result back to the original
representation. This process risks corrupting the primary linguistic features
with noisy or irrelevant non-verbal signals, as it often fails to capture the
fine-grained, token-level influence where non-verbal cues should modulate, not
just augment, textual meaning. To address this, we introduce DyKen-Hyena, which
reframes the problem from feature fusion to processing modulation. Our model
translates audio-visual cues into dynamic, per-token convolutional kernels that
directly modulate textual feature extraction. This fine-grained approach
achieves state-of-the-art results on the MIntRec and MIntRec2.0 benchmarks.
Notably, it yields a +10.46% F1-score improvement in out-of-scope detection,
validating that our method creates a fundamentally more robust intent
representation.

</details>


### [155] [Adaptive Token Merging for Efficient Transformer Semantic Communication at the Edge](https://arxiv.org/abs/2509.09955)
*Omar Erak,Omar Alhussein,Hatem Abou-Zeid,Mehdi Bennis,Sami Muhaidat*

Main category: cs.LG

TL;DR: Training-free adaptive token merging framework that compresses transformer representations by merging redundant tokens based on per-layer similarity thresholds, achieving significant computational and communication savings while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Large transformers are computationally expensive for edge devices, requiring solutions that reduce costs without retraining or sacrificing performance.

Method: Uses adaptive token merging with per-layer similarity thresholds, framed as multi-objective optimization solved via Bayesian optimization to find Pareto-optimal trade-offs between accuracy, inference cost, and communication cost.

Result: Achieves 30% fewer FLOPs and under 20% communication cost on ImageNet while matching accuracy; for VQA, achieves competitive performance with LLaVA at <1/3 compute and <1/10 bandwidth; provides robustness across channel conditions and privacy benefits against model inversion attacks.

Conclusion: Provides a practical solution for deploying transformers on resource-constrained edge devices with adaptive compression that balances efficiency and task relevance without retraining.

Abstract: Large-scale transformers are central to modern semantic communication, yet
their high computational and communication costs hinder deployment on
resource-constrained edge devices. This paper introduces a training-free
framework for adaptive token merging, a novel mechanism that compresses
transformer representations at runtime by selectively merging semantically
redundant tokens under per-layer similarity thresholds. Unlike prior
fixed-ratio reduction, our approach couples merging directly to input
redundancy, enabling data-dependent adaptation that balances efficiency and
task relevance without retraining. We cast the discovery of merging strategies
as a multi-objective optimization problem and leverage Bayesian optimization to
obtain Pareto-optimal trade-offs between accuracy, inference cost, and
communication cost. On ImageNet classification, we match the accuracy of the
unmodified transformer with 30\% fewer floating-point operations per second and
under 20\% of the original communication cost, while for visual question
answering our method achieves performance competitive with the full LLaVA model
at less than one-third of the compute and one-tenth of the bandwidth. Finally,
we show that our adaptive merging is robust across varying channel conditions
and provides inherent privacy benefits, substantially degrading the efficacy of
model inversion attacks. Our framework provides a practical and versatile
solution for deploying powerful transformer models in resource-limited edge
intelligence scenarios.

</details>


### [156] [Limited Reference, Reliable Generation: A Two-Component Framework for Tabular Data Generation in Low-Data Regimes](https://arxiv.org/abs/2509.09960)
*Mingxuan Jiang,Yongxin Wang,Ziyue Dai,Yicun Liu,Hongyi Nie,Sen Liu,Hongfeng Chai*

Main category: cs.LG

TL;DR: ReFine framework improves synthetic tabular data generation by embedding interpretable rules into prompts and using dual-granularity filtering to address data scarcity and distribution imbalance issues.


<details>
  <summary>Details</summary>
Motivation: Existing tabular generation methods require sufficient reference data and fail to capture dataset-specific feature-label dependencies, limiting effectiveness in domain-specific databases with scarce records.

Method: Proposes ReFine framework that (i) derives symbolic if-then rules from interpretable models and embeds them into prompts, and (ii) applies dual-granularity filtering to suppress over-sampling and refine rare samples.

Result: Outperforms state-of-the-art methods with up to 0.44 absolute improvement in R-squared for regression and 10.0% relative improvement in F1 score for classification tasks.

Conclusion: ReFine effectively addresses data scarcity and distribution imbalance in synthetic tabular data generation, demonstrating superior performance across various regression and classification benchmarks.

Abstract: Synthetic tabular data generation is increasingly essential in data
management, supporting downstream applications when real-world and high-quality
tabular data is insufficient. Existing tabular generation approaches, such as
generative adversarial networks (GANs), diffusion models, and fine-tuned Large
Language Models (LLMs), typically require sufficient reference data, limiting
their effectiveness in domain-specific databases with scarce records. While
prompt-based LLMs offer flexibility without parameter tuning, they often fail
to capture dataset-specific feature-label dependencies and generate redundant
data, leading to degradation in downstream task performance. To overcome these
issues, we propose ReFine, a framework that (i) derives symbolic "if-then"
rules from interpretable models and embeds them into prompts to explicitly
guide generation toward domain-specific feature distribution, and (ii) applies
a dual-granularity filtering strategy that suppresses over-sampling patterns
and selectively refines rare but informative samples to reduce distributional
imbalance. Extensive experiments on various regression and classification
benchmarks demonstrate that ReFine consistently outperforms state-of-the-art
methods, achieving up to 0.44 absolute improvement in R-squared for regression
and 10.0 percent relative improvement in F1 score for classification tasks.

</details>


### [157] [Data-Driven Energy Estimation for Virtual Servers Using Combined System Metrics and Machine Learning](https://arxiv.org/abs/2509.09991)
*Amandip Sangha*

Main category: cs.LG

TL;DR: Machine learning approach using guest VM resource metrics to estimate energy consumption without host access, achieving high accuracy (R² 0.90-0.97) for virtualized environments.


<details>
  <summary>Details</summary>
Motivation: Addresses the critical gap in virtualized environments like cloud computing where direct energy measurement is infeasible, enabling energy-aware scheduling and cost optimization.

Method: Uses Gradient Boosting Regressor trained on resource utilization metrics collected from guest virtual machines to predict energy consumption measured via RAPL on the host.

Result: Achieves high predictive accuracy with variance explained between 0.90 and 0.97 R² across diverse workloads, demonstrating feasibility of guest-side energy estimation.

Conclusion: First demonstration of guest-only resource-based energy estimation without privileged host access, enabling physical host independent energy estimates in virtualized environments.

Abstract: This paper presents a machine learning-based approach to estimate the energy
consumption of virtual servers without access to physical power measurement
interfaces. Using resource utilization metrics collected from guest virtual
machines, we train a Gradient Boosting Regressor to predict energy consumption
measured via RAPL on the host. We demonstrate, for the first time, guest-only
resource-based energy estimation without privileged host access with
experiments across diverse workloads, achieving high predictive accuracy and
variance explained ($0.90 \leq R^2 \leq 0.97$), indicating the feasibility of
guest-side energy estimation. This approach can enable energy-aware scheduling,
cost optimization and physical host independent energy estimates in virtualized
environments. Our approach addresses a critical gap in virtualized environments
(e.g. cloud) where direct energy measurement is infeasible.

</details>


### [158] [Neural Scaling Laws for Deep Regression](https://arxiv.org/abs/2509.10000)
*Tilen Cadez,Kyoung-Min Kim*

Main category: cs.LG

TL;DR: Empirical investigation of neural scaling laws in deep regression models using parameter estimation for twisted van der Waals magnets, showing power-law relationships between loss and dataset size/model capacity.


<details>
  <summary>Details</summary>
Motivation: Neural scaling laws are crucial for developing reliable models with limited resources, but their application to deep regression models remains largely unexplored despite their importance in large language models.

Method: Used parameter estimation model for twisted van der Waals magnets, tested various architectures (fully connected networks, residual networks, vision transformers) across wide ranges of dataset sizes and model capacities.

Result: Observed power-law relationships between loss and both training dataset size and model capacity. Scaling exponents ranged from 1 to 2, depending on specific regressed parameters and model details.

Conclusion: Consistent scaling behaviors with large exponents suggest that deep regression model performance can substantially improve with increasing data size, demonstrating the applicability of neural scaling laws to regression tasks.

Abstract: Neural scaling laws--power-law relationships between generalization errors
and characteristics of deep learning models--are vital tools for developing
reliable models while managing limited resources. Although the success of large
language models highlights the importance of these laws, their application to
deep regression models remains largely unexplored. Here, we empirically
investigate neural scaling laws in deep regression using a parameter estimation
model for twisted van der Waals magnets. We observe power-law relationships
between the loss and both training dataset size and model capacity across a
wide range of values, employing various architectures--including fully
connected networks, residual networks, and vision transformers. Furthermore,
the scaling exponents governing these relationships range from 1 to 2, with
specific values depending on the regressed parameters and model details. The
consistent scaling behaviors and their large scaling exponents suggest that the
performance of deep regression models can improve substantially with increasing
data size.

</details>


### [159] [Intrinsic Dimension Estimating Autoencoder (IDEA) Using CancelOut Layer and a Projected Loss](https://arxiv.org/abs/2509.10011)
*Antoine Orioua,Philipp Krah,Julian Koellermeier*

Main category: cs.LG

TL;DR: IDEA is an autoencoder that estimates intrinsic dimension of datasets on linear/nonlinear manifolds and reconstructs data using re-weighted double CancelOut layers and projected reconstruction loss.


<details>
  <summary>Details</summary>
Motivation: To develop a method that can accurately estimate the intrinsic dimension of complex datasets while maintaining reconstruction capability, addressing limitations of existing intrinsic dimension estimators.

Method: Uses autoencoder architecture with re-weighted double CancelOut layers and introduces projected reconstruction loss term that assesses reconstruction quality under removal of latent dimensions.

Result: Shows good accuracy and high versatility on theoretical benchmarks, successfully estimates intrinsic dimension and reconstructs complex fluid flow simulation data.

Conclusion: IDEA provides an effective approach for intrinsic dimension estimation with reconstruction capabilities, demonstrating robustness across both theoretical benchmarks and complex real-world simulation data.

Abstract: This paper introduces the Intrinsic Dimension Estimating Autoencoder (IDEA),
which identifies the underlying intrinsic dimension of a wide range of datasets
whose samples lie on either linear or nonlinear manifolds. Beyond estimating
the intrinsic dimension, IDEA is also able to reconstruct the original dataset
after projecting it onto the corresponding latent space, which is structured
using re-weighted double CancelOut layers. Our key contribution is the
introduction of the projected reconstruction loss term, guiding the training of
the model by continuously assessing the reconstruction quality under the
removal of an additional latent dimension. We first assess the performance of
IDEA on a series of theoretical benchmarks to validate its robustness. These
experiments allow us to test its reconstruction ability and compare its
performance with state-of-the-art intrinsic dimension estimators. The
benchmarks show good accuracy and high versatility of our approach.
Subsequently, we apply our model to data generated from the numerical solution
of a vertically resolved one-dimensional free-surface flow, following a
pointwise discretization of the vertical velocity profile in the horizontal
direction, vertical direction, and time. IDEA succeeds in estimating the
dataset's intrinsic dimension and then reconstructs the original solution by
working directly within the projection space identified by the network.

</details>


### [160] [Exploring Expert Specialization through Unsupervised Training in Sparse Mixture of Experts](https://arxiv.org/abs/2509.10025)
*Strahinja Nikolic,Ilker Oguz,Demetri Psaltis*

Main category: cs.LG

TL;DR: SMoE-VAE architecture with unsupervised expert routing outperforms supervised baseline on QuickDraw dataset, discovering meaningful sub-categorical structures beyond human-defined class boundaries.


<details>
  <summary>Details</summary>
Motivation: To understand the internal organization of neural networks and address the fundamental challenge in deep learning interpretability by exploring how mixture of experts models uncover fundamental data structures.

Method: Developed a Sparse Mixture of Experts Variational Autoencoder (SMoE-VAE) architecture and tested it on QuickDraw dataset, comparing unsupervised expert routing against supervised baseline guided by ground-truth labels. Used t-SNE visualizations and reconstruction analysis to investigate learned structures.

Result: Unsupervised routing consistently achieved superior reconstruction performance. Experts learned to identify meaningful sub-categorical structures that often transcend human-defined class boundaries. Dataset size study revealed trade-offs between data quantity and expert specialization.

Conclusion: MoE models can uncover fundamental data structures more aligned with the model's objective than predefined labels, providing guidance for designing efficient MoE architectures.

Abstract: Understanding the internal organization of neural networks remains a
fundamental challenge in deep learning interpretability. We address this
challenge by exploring a novel Sparse Mixture of Experts Variational
Autoencoder (SMoE-VAE) architecture. We test our model on the QuickDraw
dataset, comparing unsupervised expert routing against a supervised baseline
guided by ground-truth labels. Surprisingly, we find that unsupervised routing
consistently achieves superior reconstruction performance. The experts learn to
identify meaningful sub-categorical structures that often transcend
human-defined class boundaries. Through t-SNE visualizations and reconstruction
analysis, we investigate how MoE models uncover fundamental data structures
that are more aligned with the model's objective than predefined labels.
Furthermore, our study on the impact of dataset size provides insights into the
trade-offs between data quantity and expert specialization, offering guidance
for designing efficient MoE architectures.

</details>


### [161] [Sparse Coding Representation of 2-way Data](https://arxiv.org/abs/2509.10033)
*Boya Ma,Abram Magner,Maxwell McNeil,Petko Bogdanov*

Main category: cs.LG

TL;DR: AODL: Low-rank sparse dictionary coding method that learns dictionaries from data with better sparsity and interpretability than analytical dictionaries


<details>
  <summary>Details</summary>
Motivation: Address the challenge of learning dictionaries and coding coefficients in multi-dictionary scenarios where encoding coefficients correspond to all atom combinations from multiple dictionaries

Method: Propose a low-rank coding model with convex relaxation (AODL), solved via alternating optimization between sparse coding matrices and learned dictionaries with proven convergence

Result: Learns up to 90% sparser solutions than non-low-rank and analytical dictionary baselines for fixed reconstruction quality, with interpretable patterns in learned dictionaries

Conclusion: AODL provides an effective solution for multi-dictionary learning with better sparsity, reconstruction quality, and interpretable insights compared to traditional approaches

Abstract: Sparse dictionary coding represents signals as linear combinations of a few
dictionary atoms. It has been applied to images, time series, graph signals and
multi-way spatio-temporal data by jointly employing temporal and spatial
dictionaries. Data-agnostic analytical dictionaries, such as the discrete
Fourier transform, wavelets and graph Fourier, have seen wide adoption due to
efficient implementations and good practical performance. On the other hand,
dictionaries learned from data offer sparser and more accurate solutions but
require learning of both the dictionaries and the coding coefficients. This
becomes especially challenging for multi-dictionary scenarios since encoding
coefficients correspond to all atom combinations from the dictionaries. To
address this challenge, we propose a low-rank coding model for 2-dictionary
scenarios and study its data complexity. Namely, we establish a bound on the
number of samples needed to learn dictionaries that generalize to unseen
samples from the same distribution. We propose a convex relaxation solution,
called AODL, whose exact solution we show also solves the original problem. We
then solve this relaxation via alternating optimization between the sparse
coding matrices and the learned dictionaries, which we prove to be convergent.
We demonstrate its quality for data reconstruction and missing value imputation
in both synthetic and real-world datasets. For a fixed reconstruction quality,
AODL learns up to 90\% sparser solutions compared to non-low-rank and
analytical (fixed) dictionary baselines. In addition, the learned dictionaries
reveal interpretable insights into patterns present within the samples used for
training.

</details>


### [162] [Symbolic Feedforward Networks for Probabilistic Finite Automata: Exact Simulation and Learnability](https://arxiv.org/abs/2509.10034)
*Sahil Rajesh Dhayalkar*

Main category: cs.LG

TL;DR: Probabilistic finite automata (PFAs) can be exactly simulated using symbolic feedforward neural networks that represent state distributions as vectors and transitions as stochastic matrices, enabling parallel, interpretable, and differentiable simulation without recurrence.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between symbolic computation and deep learning by unifying probabilistic automata theory with neural architectures under a rigorous algebraic framework.

Method: Represent state distributions as vectors and transitions as stochastic matrices, enabling probabilistic state propagation via matrix-vector products. Use symbolic feedforward neural networks with layered symbolic computation for exact simulation of PFA dynamics.

Result: The neural networks can exactly simulate PFAs, including probabilistic subset construction and ε-closure. These symbolic simulators are learnable through standard gradient descent optimization on labeled sequence data, recovering exact behavior of ground-truth PFAs.

Conclusion: The work establishes formal equivalence between PFAs and specific classes of neural networks, demonstrating that symbolic neural simulators are both expressive and learnable, providing a unified framework connecting probabilistic automata theory with neural architectures.

Abstract: We present a formal and constructive theory showing that probabilistic finite
automata (PFAs) can be exactly simulated using symbolic feedforward neural
networks. Our architecture represents state distributions as vectors and
transitions as stochastic matrices, enabling probabilistic state propagation
via matrix-vector products. This yields a parallel, interpretable, and
differentiable simulation of PFA dynamics using soft updates-without
recurrence. We formally characterize probabilistic subset construction,
$\varepsilon$-closure, and exact simulation via layered symbolic computation,
and prove equivalence between PFAs and specific classes of neural networks. We
further show that these symbolic simulators are not only expressive but
learnable: trained with standard gradient descent-based optimization on labeled
sequence data, they recover the exact behavior of ground-truth PFAs. This
learnability, formalized in Proposition 5.1, is the crux of this work. Our
results unify probabilistic automata theory with neural architectures under a
rigorous algebraic framework, bridging the gap between symbolic computation and
deep learning.

</details>


### [163] [Clip Your Sequences Fairly: Enforcing Length Fairness for Sequence-Level RL](https://arxiv.org/abs/2509.09177)
*Hanyi Mao,Quanjia Xiao,Lei Pang,Haixiao Liu*

Main category: cs.LG

TL;DR: FSPO is a sequence-level RL method that addresses length bias in LLM training by introducing length-fair clipping in importance-sampling weight space, ensuring fair treatment of responses regardless of length.


<details>
  <summary>Details</summary>
Motivation: Existing sequence-level RL methods like PPO/GRPO suffer from systematic length bias where fixed clipping ranges unfairly reweight short vs long responses, distorting the optimization objective.

Method: FSPO clips sequence log-importance-sampling ratios using a Gaussian-motivated band that applies KL-corrected drift and scales as √L, ensuring length-fair clipping directly in IS weight space.

Result: Empirical results show FSPO flattens clip rates across different length bins, stabilizes training, and outperforms all baseline methods across multiple evaluation datasets.

Conclusion: FSPO successfully addresses length fairness in sequence-level RL through theoretical grounding and practical implementation, providing more stable and effective training for LLMs without length-based bias.

Abstract: We propose FSPO (Fair Sequence Policy Optimization), a sequence-level
reinforcement learning method for LLMs that enforces length-fair clipping
directly in the importance-sampling (IS) weight space. We revisit
sequence-level RL methods and identify a mismatch when PPO/GRPO-style clipping
is transplanted to sequences: a fixed clip range systematically reweights short
vs. long responses, distorting the effective objective. Theoretically, we
formalize length fairness via a Length Reweighting Error (LRE) and prove that
small LRE yields a directional cosine guarantee between the clipped and true
updates. FSPO introduces a simple, Gaussian-motivated remedy: we clip the
sequence log-IS ratio with a band that applies a KL-corrected drift term and
scales as $\sqrt{L}$. Empirically, FSPO flattens clip rates across length bins,
stabilizes training, and outperforms all baselines across multiple evaluation
datasets.

</details>


### [164] [FedRP: A Communication-Efficient Approach for Differentially Private Federated Learning Using Random Projection](https://arxiv.org/abs/2509.10041)
*Mohammad Hasan Narimani,Mostafa Tavassolipour*

Main category: cs.LG

TL;DR: FedRP is a novel federated learning algorithm that combines random projection with ADMM optimization to enhance privacy and reduce communication costs while maintaining high model accuracy.


<details>
  <summary>Details</summary>
Motivation: Federated learning faces challenges in user privacy protection against attacks and high communication costs, especially in sensitive domains like IoT and medical data analysis.

Method: Integrates random projection techniques with ADMM optimization framework to reduce dimensionality of model parameters before transmission, providing strong differential privacy guarantees.

Result: FedRP maintains high model accuracy while outperforming existing methods (including conventional differential privacy and FedADMM) in both privacy preservation and communication efficiency.

Conclusion: The proposed FedRP algorithm successfully addresses key FL challenges by providing strong privacy protection with reduced communication overhead while maintaining competitive model performance.

Abstract: Federated learning (FL) offers an innovative paradigm for collaborative model
training across decentralized devices, such as smartphones, balancing enhanced
predictive performance with the protection of user privacy in sensitive areas
like Internet of Things (IoT) and medical data analysis. Despite its
advantages, FL encounters significant challenges related to user privacy
protection against potential attacks and the management of communication costs.
This paper introduces a novel federated learning algorithm called FedRP, which
integrates random projection techniques with the Alternating Direction Method
of Multipliers (ADMM) optimization framework. This approach enhances privacy by
employing random projection to reduce the dimensionality of model parameters
prior to their transmission to a central server, reducing the communication
cost. The proposed algorithm offers a strong $(\epsilon, \delta)$-differential
privacy guarantee, demonstrating resilience against data reconstruction
attacks. Experimental results reveal that FedRP not only maintains high model
accuracy but also outperforms existing methods, including conventional
differential privacy approaches and FedADMM, in terms of both privacy
preservation and communication efficiency.

</details>


### [165] [AEGIS: An Agent for Extraction and Geographic Identification in Scholarly Proceedings](https://arxiv.org/abs/2509.09470)
*Om Vishesh,Harshad Khadilkar,Deepak Akkil*

Main category: cs.LG

TL;DR: Fully automated AI system 'Agent-E' identifies regional papers from conferences and uses RPA to complete actions like nomination submissions with 100% recall and 99.4% accuracy.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of rapidly growing academic literature and reduce time-consuming manual effort required for scholarly discovery workflows.

Method: Specialized AI agent pipeline that identifies papers from specific geographic regions in conference proceedings and executes Robotic Process Automation (RPA) for predefined actions.

Result: Validated on 586 papers from five conferences - achieved 100% recall and 99.4% accuracy in identifying target papers and completing automated actions.

Conclusion: Task-oriented AI agents can effectively filter information and actively participate in accelerating academic community workflows through automation.

Abstract: Keeping pace with the rapid growth of academia literature presents a
significant challenge for researchers, funding bodies, and academic societies.
To address the time-consuming manual effort required for scholarly discovery,
we present a novel, fully automated system that transitions from data discovery
to direct action. Our pipeline demonstrates how a specialized AI agent,
'Agent-E', can be tasked with identifying papers from specific geographic
regions within conference proceedings and then executing a Robotic Process
Automation (RPA) to complete a predefined action, such as submitting a
nomination form. We validated our system on 586 papers from five different
conferences, where it successfully identified every target paper with a recall
of 100% and a near perfect accuracy of 99.4%. This demonstration highlights the
potential of task-oriented AI agents to not only filter information but also to
actively participate in and accelerate the workflows of the academic community.

</details>


### [166] [Uncertainty-Aware Tabular Prediction: Evaluating VBLL-Enhanced TabPFN in Safety-Critical Medical Data](https://arxiv.org/abs/2509.10048)
*Madhushan Ramalingam*

Main category: cs.LG

TL;DR: VBLL integration with TabPFN unexpectedly worsens uncertainty calibration performance compared to original TabPFN across medical datasets


<details>
  <summary>Details</summary>
Motivation: Reliable uncertainty estimation is crucial for safety-critical applications like medical diagnosis, and TabPFN is a promising foundation model for tabular data that could benefit from improved uncertainty calibration

Method: Integrated Variational Bayesian Last Layers (VBLL) with TabPFN and evaluated uncertainty calibration performance on three benchmark medical tabular datasets

Result: Original TabPFN consistently outperformed VBLL-integrated TabPFN in uncertainty calibration across all datasets, contrary to expectations

Conclusion: VBLL integration does not improve and actually degrades TabPFN's uncertainty calibration performance, suggesting the original TabPFN architecture may already be well-calibrated or that VBLL is not compatible with its transformer-based approach

Abstract: Predictive models are being increasingly used across a wide range of domains,
including safety-critical applications such as medical diagnosis and criminal
justice. Reliable uncertainty estimation is a crucial task in such settings.
Tabular Prior-data Fitted Network (TabPFN) is a recently proposed machine
learning foundation model for tabular dataset, which uses a generative
transformer architecture. Variational Bayesian Last Layers (VBLL) is a
state-of-the-art lightweight variational formulation that effectively improves
uncertainty estimation with minimal computational overhead. In this work we aim
to evaluate the performance of VBLL integrated with the recently proposed
TabPFN in uncertainty calibration. Our experiments, conducted on three
benchmark medical tabular datasets, compare the performance of the original
TabPFN and the VBLL-integrated version. Contrary to expectations, we observed
that original TabPFN consistently outperforms VBLL integrated TabPFN in
uncertainty calibration across all datasets.

</details>


### [167] [KAN-SR: A Kolmogorov-Arnold Network Guided Symbolic Regression Framework](https://arxiv.org/abs/2509.10089)
*Marco Andrea Bühler,Gonzalo Guillén-Gosálbez*

Main category: cs.LG

TL;DR: KAN-SR is a novel symbolic regression framework using Kolmogorov Arnold Networks with deep learning and simplification strategies to recover ground-truth equations and model dynamic systems.


<details>
  <summary>Details</summary>
Motivation: Symbolic regression traditionally uses genetic programming, but the authors aim to leverage deep learning techniques with KANs to improve equation discovery and dynamic system modeling.

Method: Uses Kolmogorov Arnold Networks (KANs) with divide-and-conquer approach, deep learning techniques, and simplification strategies like translational symmetries and separabilities. Combines with neural controlled differential equations for dynamic modeling.

Result: Successfully recovers ground-truth equations from Feynman SRSD dataset and precisely models dynamics of in-silico bioprocess systems.

Conclusion: KAN-SR framework opens doors for dynamic modeling of engineering systems and demonstrates superior performance in symbolic regression compared to traditional genetic programming approaches.

Abstract: We introduce a novel symbolic regression framework, namely KAN-SR, built on
Kolmogorov Arnold Networks (KANs) which follows a divide-and-conquer approach.
Symbolic regression searches for mathematical equations that best fit a given
dataset and is commonly solved with genetic programming approaches. We show
that by using deep learning techniques, more specific KANs, and combining them
with simplification strategies such as translational symmetries and
separabilities, we are able to recover ground-truth equations of the Feynman
Symbolic Regression for Scientific Discovery (SRSD) dataset. Additionally, we
show that by combining the proposed framework with neural controlled
differential equations, we are able to model the dynamics of an in-silico
bioprocess system precisely, opening the door for the dynamic modeling of other
engineering systems.

</details>


### [168] [Cost-Free Personalization via Information-Geometric Projection in Bayesian Federated Learning](https://arxiv.org/abs/2509.10132)
*Nour Jamoussi,Giuseppe Serra,Photios A. Stavrou,Marios Kountouris*

Main category: cs.LG

TL;DR: Information-geometric projection framework for Bayesian Federated Learning that enables tunable trade-off between global generalization and local specialization through statistical manifold barycenter computation.


<details>
  <summary>Details</summary>
Motivation: Bayesian Federated Learning needs better personalization mechanisms to handle data heterogeneity while maintaining privacy constraints, moving beyond traditional MCMC and variational inference approaches.

Method: Proposes projecting global model onto neighborhood of user's local model using information-geometric framework, showing equivalence to computing barycenter on statistical manifold for closed-form solutions.

Result: Achieves cost-free personalization with minimal computational overhead, effectively balancing global and local performance under heterogeneous data distributions using IVON optimizer.

Conclusion: The information-geometric projection framework provides an efficient and mathematically grounded approach for personalized Bayesian Federated Learning with tunable specialization-generalization trade-offs.

Abstract: Bayesian Federated Learning (BFL) combines uncertainty modeling with
decentralized training, enabling the development of personalized and reliable
models under data heterogeneity and privacy constraints. Existing approaches
typically rely on Markov Chain Monte Carlo (MCMC) sampling or variational
inference, often incorporating personalization mechanisms to better adapt to
local data distributions. In this work, we propose an information-geometric
projection framework for personalization in parametric BFL. By projecting the
global model onto a neighborhood of the user's local model, our method enables
a tunable trade-off between global generalization and local specialization.
Under mild assumptions, we show that this projection step is equivalent to
computing a barycenter on the statistical manifold, allowing us to derive
closed-form solutions and achieve cost-free personalization. We apply the
proposed approach to a variational learning setup using the Improved
Variational Online Newton (IVON) optimizer and extend its application to
general aggregation schemes in BFL. Empirical evaluations under heterogeneous
data distributions confirm that our method effectively balances global and
local performance with minimal computational overhead.

</details>


### [169] [BenchECG and xECG: a benchmark and baseline for ECG foundation models](https://arxiv.org/abs/2509.10151)
*Riccardo Lunelli,Angus Nicolson,Samuel Martin Pröll,Sebastian Johannes Reinstadler,Axel Bauer,Clemens Dlaska*

Main category: cs.LG

TL;DR: BenchECG is a standardized benchmark for ECG foundation models with comprehensive datasets and tasks. xECG, an xLSTM-based model with SimDINOv2 self-supervised learning, achieves state-of-the-art performance across all tasks.


<details>
  <summary>Details</summary>
Motivation: Lack of consistent evaluation for ECG foundation models due to narrow task selections and inconsistent datasets, hindering fair comparison and progress in ECG representation learning.

Method: Proposed BenchECG benchmark with standardized evaluation framework and xECG model using xLSTM architecture with SimDINOv2 self-supervised learning approach.

Result: xECG achieves the best BenchECG score compared to publicly available state-of-the-art models and is the only publicly available model that performs strongly on all datasets and tasks.

Conclusion: BenchECG enables rigorous comparison and accelerates progress in ECG foundation models, with xECG setting a new baseline for future ECG representation learning.

Abstract: Electrocardiograms (ECGs) are inexpensive, widely used, and well-suited to
deep learning. Recently, interest has grown in developing foundation models for
ECGs - models that generalise across diverse downstream tasks. However,
consistent evaluation has been lacking: prior work often uses narrow task
selections and inconsistent datasets, hindering fair comparison. Here, we
introduce BenchECG, a standardised benchmark comprising a comprehensive suite
of publicly available ECG datasets and versatile tasks. We also propose xECG,
an xLSTM-based recurrent model trained with SimDINOv2 self-supervised learning,
which achieves the best BenchECG score compared to publicly available
state-of-the-art models. In particular, xECG is the only publicly available
model to perform strongly on all datasets and tasks. By standardising
evaluation, BenchECG enables rigorous comparison and aims to accelerate
progress in ECG representation learning. xECG achieves superior performance
over earlier approaches, defining a new baseline for future ECG foundation
models.

</details>


### [170] [FedBiF: Communication-Efficient Federated Learning via Bits Freezing](https://arxiv.org/abs/2509.10161)
*Shiwei Li,Qunwei Li,Haozhao Wang,Ruixuan Li,Jianbin Lin,Wenliang Zhong*

Main category: cs.LG

TL;DR: FedBiF is a federated learning framework that learns quantized parameters during local training by updating only one bit per parameter per round, achieving high compression with comparable accuracy to FedAvg.


<details>
  <summary>Details</summary>
Motivation: FL suffers from high communication overhead, and existing quantization methods applied after training introduce errors that degrade model accuracy.

Method: Server quantizes model parameters and sends to clients; each client updates only one bit of multi-bit parameter representation while freezing other bits, enabling bit-by-bit updates.

Result: Achieves superior communication compression (1 bpp uplink, 3 bpp downlink) while maintaining accuracy comparable to FedAvg, and promotes model sparsity across IID and Non-IID settings.

Conclusion: FedBiF effectively reduces FL communication costs through direct quantization during training while preserving model performance, offering a practical solution for communication-efficient federated learning.

Abstract: Federated learning (FL) is an emerging distributed machine learning paradigm
that enables collaborative model training without sharing local data. Despite
its advantages, FL suffers from substantial communication overhead, which can
affect training efficiency. Recent efforts have mitigated this issue by
quantizing model updates to reduce communication costs. However, most existing
methods apply quantization only after local training, introducing quantization
errors into the trained parameters and potentially degrading model accuracy. In
this paper, we propose Federated Bit Freezing (FedBiF), a novel FL framework
that directly learns quantized model parameters during local training. In each
communication round, the server first quantizes the model parameters and
transmits them to the clients. FedBiF then allows each client to update only a
single bit of the multi-bit parameter representation, freezing the remaining
bits. This bit-by-bit update strategy reduces each parameter update to one bit
while maintaining high precision in parameter representation. Extensive
experiments are conducted on five widely used datasets under both IID and
Non-IID settings. The results demonstrate that FedBiF not only achieves
superior communication compression but also promotes sparsity in the resulting
models. Notably, FedBiF attains accuracy comparable to FedAvg, even when using
only 1 bit-per-parameter (bpp) for uplink and 3 bpp for downlink communication.
The code is available at https://github.com/Leopold1423/fedbif-tpds25.

</details>


### [171] [Federated Multi-Agent Reinforcement Learning for Privacy-Preserving and Energy-Aware Resource Management in 6G Edge Networks](https://arxiv.org/abs/2509.10163)
*Francisco Javier Esono Nkulu Andong,Qi Min*

Main category: cs.LG

TL;DR: Fed-MARL framework for 6G networks combining federated learning and multi-agent reinforcement learning for privacy-preserving, energy-efficient resource management across heterogeneous edge devices.


<details>
  <summary>Details</summary>
Motivation: 6G networks require efficient resource management under strict privacy, mobility, and energy constraints in ultra-dense intelligent edge environments.

Method: Federated Multi-Agent Reinforcement Learning with DRQN for decentralized policies, secure aggregation protocol using elliptic curve Diffie Hellman, and POMMDP formulation with multi-objective reward function.

Result: Outperforms centralized MARL and heuristic baselines in task success rate, latency, energy efficiency, and fairness while ensuring robust privacy protection and scalability.

Conclusion: Fed-MARL provides effective solution for real-time resource management in dynamic 6G edge networks with privacy preservation and energy efficiency.

Abstract: As sixth-generation (6G) networks move toward ultra-dense, intelligent edge
environments, efficient resource management under stringent privacy, mobility,
and energy constraints becomes critical. This paper introduces a novel
Federated Multi-Agent Reinforcement Learning (Fed-MARL) framework that
incorporates cross-layer orchestration of both the MAC layer and application
layer for energy-efficient, privacy-preserving, and real-time resource
management across heterogeneous edge devices. Each agent uses a Deep Recurrent
Q-Network (DRQN) to learn decentralized policies for task offloading, spectrum
access, and CPU energy adaptation based on local observations (e.g., queue
length, energy, CPU usage, and mobility). To protect privacy, we introduce a
secure aggregation protocol based on elliptic curve Diffie Hellman key
exchange, which ensures accurate model updates without exposing raw data to
semi-honest adversaries. We formulate the resource management problem as a
partially observable multi-agent Markov decision process (POMMDP) with a
multi-objective reward function that jointly optimizes latency, energy
efficiency, spectral efficiency, fairness, and reliability under 6G-specific
service requirements such as URLLC, eMBB, and mMTC. Simulation results
demonstrate that Fed-MARL outperforms centralized MARL and heuristic baselines
in task success rate, latency, energy efficiency, and fairness, while ensuring
robust privacy protection and scalability in dynamic, resource-constrained 6G
edge networks.

</details>


### [172] [A Symmetry-Integrated Approach to Surface Code Decoding](https://arxiv.org/abs/2509.10164)
*Hoshitaro Ohnishi,Hideo Mukai*

Main category: cs.LG

TL;DR: Proposes a neural network-based reoptimization technique for surface code quantum error correction that improves decoder accuracy by treating syndrome measurements as a continuous regression problem.


<details>
  <summary>Details</summary>
Motivation: Previous surface code decoders suffered from non-uniqueness of correct predictions, acquiring only error probability distributions rather than precise corrections.

Method: Approximates syndrome measurements with continuous functions mathematically interpolated by neural networks, reoptimizing decoder models through deep learning regression.

Result: Reoptimized decoders showed improved accuracy for code distances 5 and 7 across various architectures (multilayer perceptron, convolutional, recurrent neural networks, transformers).

Conclusion: Reframing surface code decoding as a regression problem solvable by deep learning is universally effective and improves decoder performance regardless of code distance or network architecture.

Abstract: Quantum error correction, which utilizes logical qubits that are encoded as
redundant multiple physical qubits to find and correct errors in physical
qubits, is indispensable for practical quantum computing. Surface code is
considered to be a promising encoding method with a high error threshold that
is defined by stabilizer generators. However, previous methods have suffered
from the problem that the decoder acquires solely the error probability
distribution because of the non-uniqueness of correct prediction obtained from
the input. To circumvent this problem, we propose a technique to reoptimize the
decoder model by approximating syndrome measurements with a continuous function
that is mathematically interpolated by neural network. We evaluated the
improvement in accuracy of a multilayer perceptron based decoder for code
distances of 5 and 7 as well as for decoders based on convolutional and
recurrent neural networks and transformers for a code distance of 5. In all
cases, the reoptimized decoder gave better accuracy than the original models,
demonstrating the universal effectiveness of the proposed method that is
independent of code distance or network architecture. These results suggest
that re-framing the problem of surface code decoding into a regression problem
that can be tackled by deep learning is a useful strategy.

</details>


### [173] [The Hidden Width of Deep ResNets: Tight Error Bounds and Phase Diagrams](https://arxiv.org/abs/2509.10167)
*Lénaïc Chizat*

Main category: cs.LG

TL;DR: Analysis of gradient-based training dynamics in large-depth ResNets showing convergence to Neural Mean ODE, with tight error bounds and characterization of feature learning regimes.


<details>
  <summary>Details</summary>
Motivation: To understand the training dynamics of deep residual networks from random initializations, particularly how scaling factors affect convergence and feature learning capabilities in practical architectures like Transformers.

Method: Theoretical analysis using stochastic approximation and propagation of chaos techniques to study ResNet training dynamics, with empirical verification of error bounds for different scaling regimes.

Result: Established tight error bounds O(1/L + α/√(LM)) between ResNet output and Neural Mean ODE limit. Identified Θ(√D/LM) as the optimal residual scale for complete feature learning in two-layer perceptron ResNets.

Conclusion: Large-depth ResNets converge to Neural Mean ODE dynamics with depth, with specific scaling conditions determining whether networks exhibit complete feature learning or lazy training behavior.

Abstract: We study the gradient-based training of large-depth residual networks
(ResNets) from standard random initializations. We show that with a diverging
depth $L$, a fixed embedding dimension $D$, and an arbitrary hidden width $M$,
the training dynamics converges to a Neural Mean ODE training dynamics.
Remarkably, the limit is independent of the scaling of $M$, covering practical
cases of, say, Transformers, where $M$ (the number of hidden units or attention
heads per layer) is typically of the order of $D$. For a residual scale
$\Theta_D\big(\frac{\alpha}{LM}\big)$, we obtain the error bound
$O_D\big(\frac{1}{L}+ \frac{\alpha}{\sqrt{LM}}\big)$ between the model's output
and its limit after a fixed number gradient of steps, and we verify empirically
that this rate is tight. When $\alpha=\Theta(1)$, the limit exhibits complete
feature learning, i.e. the Mean ODE is genuinely non-linearly parameterized. In
contrast, we show that $\alpha \to \infty$ yields a \lazy ODE regime where the
Mean ODE is linearly parameterized. We then focus on the particular case of
ResNets with two-layer perceptron blocks, for which we study how these scalings
depend on the embedding dimension $D$. We show that for this model, the only
residual scale that leads to complete feature learning is
$\Theta\big(\frac{\sqrt{D}}{LM}\big)$. In this regime, we prove the error bound
$O\big(\frac{1}{L}+ \frac{\sqrt{D}}{\sqrt{LM}}\big)$ between the ResNet and its
limit after a fixed number of gradient steps, which is also empirically tight.
Our convergence results rely on a novel mathematical perspective on ResNets :
(i) due to the randomness of the initialization, the forward and backward pass
through the ResNet behave as the stochastic approximation of certain mean ODEs,
and (ii) by propagation of chaos (that is, asymptotic independence of the
units) this behavior is preserved through the training dynamics.

</details>


### [174] [P3D: Scalable Neural Surrogates for High-Resolution 3D Physics Simulations with Global Context](https://arxiv.org/abs/2509.10186)
*Benjamin Holzschuh,Georg Kohl,Florian Redinger,Nils Thuerey*

Main category: cs.LG

TL;DR: A scalable framework for learning neural surrogates for high-resolution 3D physics simulations using a hybrid CNN-Transformer architecture that outperforms existing methods in speed and accuracy.


<details>
  <summary>Details</summary>
Motivation: To develop efficient neural surrogates for high-resolution 3D physics simulations that can handle complex PDE dynamics with reduced computational requirements.

Method: Hybrid CNN-Transformer backbone architecture pretrained on small simulation patches, fused for global solutions, optionally guided by sequence-to-sequence modeling for long-range dependencies.

Result: Significantly outperforms baseline methods, scales to 512^3 spatial resolution turbulence, and successfully learns dynamics of 14 different 3D PDE types while capturing turbulent flow statistics.

Conclusion: The proposed framework provides an effective and scalable approach for deterministic and probabilistic neural surrogates in high-resolution 3D physics simulations with versatile applications across various PDE types and turbulence scenarios.

Abstract: We present a scalable framework for learning deterministic and probabilistic
neural surrogates for high-resolution 3D physics simulations. We introduce a
hybrid CNN-Transformer backbone architecture targeted for 3D physics
simulations, which significantly outperforms existing architectures in terms of
speed and accuracy. Our proposed network can be pretrained on small patches of
the simulation domain, which can be fused to obtain a global solution,
optionally guided via a fast and scalable sequence-to-sequence model to include
long-range dependencies. This setup allows for training large-scale models with
reduced memory and compute requirements for high-resolution datasets. We
evaluate our backbone architecture against a large set of baseline methods with
the objective to simultaneously learn the dynamics of 14 different types of
PDEs in 3D. We demonstrate how to scale our model to high-resolution isotropic
turbulence with spatial resolutions of up to $512^3$. Finally, we demonstrate
the versatility of our network by training it as a diffusion model to produce
probabilistic samples of highly turbulent 3D channel flows across varying
Reynolds numbers, accurately capturing the underlying flow statistics.

</details>


### [175] [Hadamard-Riemannian Optimization for Margin-Variance Ensemble](https://arxiv.org/abs/2509.10189)
*Zexu Jin*

Main category: cs.LG

TL;DR: Novel ensemble learning framework that optimizes both expected margin and margin variance, with reparameterized weights on unit sphere for better efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Conventional margin-based ensemble methods focus only on maximizing expected margin while ignoring margin variance, limiting generalization and increasing vulnerability to overfitting, especially in noisy/imbalanced datasets. Traditional optimization in probability simplex also causes computational inefficiency.

Method: Introduces ensemble framework that explicitly incorporates margin variance into loss function, jointly optimizing negative expected margin and its variance. Reparameterizes ensemble weights onto unit sphere to simplify optimization and improve computational efficiency.

Result: Extensive experiments on multiple benchmark datasets show the proposed approach consistently outperforms traditional margin-based ensemble techniques.

Conclusion: The method enhances robustness and generalization performance while improving computational efficiency, demonstrating practical utility for ensemble learning applications.

Abstract: Ensemble learning has been widely recognized as a pivotal technique for
boosting predictive performance by combining multiple base models.
Nevertheless, conventional margin-based ensemble methods predominantly focus on
maximizing the expected margin while neglecting the critical role of margin
variance, which inherently restricts the generalization capability of the model
and heightens its vulnerability to overfitting, particularly in noisy or
imbalanced datasets. Additionally, the conventional approach of optimizing
ensemble weights within the probability simplex often introduces computational
inefficiency and scalability challenges, complicating its application to
large-scale problems. To tackle these limitations, this paper introduces a
novel ensemble learning framework that explicitly incorporates margin variance
into the loss function. Our method jointly optimizes the negative expected
margin and its variance, leading to enhanced robustness and improved
generalization performance. Moreover, by reparameterizing the ensemble weights
onto the unit sphere, we substantially simplify the optimization process and
improve computational efficiency. Extensive experiments conducted on multiple
benchmark datasets demonstrate that the proposed approach consistently
outperforms traditional margin-based ensemble techniques, underscoring its
effectiveness and practical utility.

</details>


### [176] [A Certifiable Machine Learning-Based Pipeline to Predict Fatigue Life of Aircraft Structures](https://arxiv.org/abs/2509.10227)
*Ángel Ladrón,Miguel Sánchez-Domínguez,Javier Rozalén,Fernando R. Sánchez,Javier de Vicente,Lucas Lacasa,Eusebio Valero,Gonzalo Rubio*

Main category: cs.LG

TL;DR: ML-based pipeline for aircraft wing fatigue life prediction using flight parameters, complementing traditional methods by reducing simulation costs and computational resources.


<details>
  <summary>Details</summary>
Motivation: Traditional fatigue life prediction methods are time-consuming, complex, and require multiple teams/tools. Machine learning offers faster iterations and generalization to complement conventional approaches.

Method: A machine learning pipeline that estimates fatigue life at different aircraft wing locations based on flight parameters from operational missions throughout the aircraft's life.

Result: The pipeline provides accurate fatigue life predictions with thorough statistical validation and uncertainty quantification in realistic use cases.

Conclusion: The ML-based approach successfully complements traditional methodologies by reducing costly simulations and lowering computational/human resource requirements while maintaining accuracy.

Abstract: Fatigue life prediction is essential in both the design and operational
phases of any aircraft, and in this sense safety in the aerospace industry
requires early detection of fatigue cracks to prevent in-flight failures.
Robust and precise fatigue life predictors are thus essential to ensure safety.
Traditional engineering methods, while reliable, are time consuming and involve
complex workflows, including steps such as conducting several Finite Element
Method (FEM) simulations, deriving the expected loading spectrum, and applying
cycle counting techniques like peak-valley or rainflow counting. These steps
often require collaboration between multiple teams and tools, added to the
computational time and effort required to achieve fatigue life predictions.
Machine learning (ML) offers a promising complement to traditional fatigue life
estimation methods, enabling faster iterations and generalization, providing
quick estimates that guide decisions alongside conventional simulations.
  In this paper, we present a ML-based pipeline that aims to estimate the
fatigue life of different aircraft wing locations given the flight parameters
of the different missions that the aircraft will be operating throughout its
operational life. We validate the pipeline in a realistic use case of fatigue
life estimation, yielding accurate predictions alongside a thorough statistical
validation and uncertainty quantification. Our pipeline constitutes a
complement to traditional methodologies by reducing the amount of costly
simulations and, thereby, lowering the required computational and human
resources.

</details>


### [177] [Prompt Injection Attacks on LLM Generated Reviews of Scientific Publications](https://arxiv.org/abs/2509.10248)
*Janis Keuper*

Main category: cs.LG

TL;DR: Simple prompt injections can manipulate LLM peer reviews with 100% effectiveness, and LLMs show strong bias toward paper acceptance (>95%), raising serious concerns about LLM usage in scientific peer review.


<details>
  <summary>Details</summary>
Motivation: To investigate the practicability and technical success of hidden prompt injections in manipulating LLM-based peer review scores, as this would significantly impact the debate around LLM usage in scientific review processes.

Method: Systematic evaluation using 1,000 reviews of 2024 ICLR papers generated by a wide range of LLMs to test the effectiveness of simple prompt injections.

Result: 1) Very simple prompt injections are highly effective, reaching up to 100% acceptance scores. 2) LLM reviews are generally biased toward acceptance (>95% in many models).

Conclusion: Both findings have significant implications for the ongoing discussion about LLM usage in peer review, highlighting vulnerabilities and inherent biases that could undermine the integrity of scientific review processes.

Abstract: The ongoing intense discussion on rising LLM usage in the scientific
peer-review process has recently been mingled by reports of authors using
hidden prompt injections to manipulate review scores. Since the existence of
such "attacks" - although seen by some commentators as "self-defense" - would
have a great impact on the further debate, this paper investigates the
practicability and technical success of the described manipulations. Our
systematic evaluation uses 1k reviews of 2024 ICLR papers generated by a wide
range of LLMs shows two distinct results: I) very simple prompt injections are
indeed highly effective, reaching up to 100% acceptance scores. II) LLM reviews
are generally biased toward acceptance (>95% in many models). Both results have
great impact on the ongoing discussions on LLM usage in peer-review.

</details>


### [178] [Property prediction for ionic liquids without prior structural knowledge using limited experimental data: A data-driven neural recommender system leveraging transfer learning](https://arxiv.org/abs/2509.10273)
*Sahil Sethi,Kai Sundmacher,Caroline Ganzer*

Main category: cs.LG

TL;DR: Transfer learning framework using neural recommender system and COSMO-RS simulated data to predict ionic liquid properties with limited experimental data


<details>
  <summary>Details</summary>
Motivation: Accurate prediction of thermophysical properties for ionic liquids is challenging due to vast chemical design space and limited experimental data availability

Method: Two-stage process: 1) Pre-train neural recommender system on COSMO-RS simulated data to learn structural embeddings, 2) Fine-tune feedforward neural networks using these embeddings with experimental data across varying temperatures/pressures

Result: Improved performance for four out of five target properties (density, viscosity, surface tension, heat capacity, melting point), robust extrapolation to unseen ILs, and enabled prediction for over 700,000 IL combinations

Conclusion: Combining simulated data with transfer learning effectively overcomes experimental data sparsity and provides scalable solution for ionic liquid screening in process design

Abstract: Ionic liquids (ILs) have emerged as versatile replacements for traditional
solvents because their physicochemical properties can be precisely tailored to
various applications. However, accurately predicting key thermophysical
properties remains challenging due to the vast chemical design space and the
limited availability of experimental data. In this study, we present a
data-driven transfer learning framework that leverages a neural recommender
system (NRS) to enable reliable property prediction for ILs using sparse
experimental datasets. The approach involves a two-stage process: first,
pre-training NRS models on COSMO-RS-based simulated data at fixed temperature
and pressure to learn property-specific structural embeddings for cations and
anions; and second, fine-tuning simple feedforward neural networks using these
embeddings with experimental data at varying temperatures and pressures. In
this work, five essential IL properties are considered: density, viscosity,
surface tension, heat capacity, and melting point. The framework supports both
within-property and cross-property knowledge transfer. Notably, pre-trained
models for density, viscosity, and heat capacity are used to fine-tune models
for all five target properties, achieving improved performance by a substantial
margin for four of them. The model exhibits robust extrapolation to previously
unseen ILs. Moreover, the final trained models enable property prediction for
over 700,000 IL combinations, offering a scalable solution for IL screening in
process design. This work highlights the effectiveness of combining simulated
data and transfer learning to overcome sparsity in the experimental data.

</details>


### [179] [Proof of AutoML: SDN based Secure Energy Trading with Blockchain in Disaster Case](https://arxiv.org/abs/2509.10291)
*Salih Toprak,Muge Erel-Ozcevik*

Main category: cs.LG

TL;DR: Proposes Proof of AutoML - using ML regressors as nonce generators for blockchain-secured energy trading in disaster scenarios, with SDN-enabled architecture for flexible control.


<details>
  <summary>Details</summary>
Motivation: Need for secure and traceable energy trading when conventional infrastructure fails in disasters, requiring robust nonce generation for blockchain integrity.

Method: SDN-enabled architecture with 5 AutoML-selected regression models (Gradient Boosting, LightGBM, Random Forest, Extra Trees, K-Nearest Neighbors) evaluated for randomness using 9000-sample dataset, focusing on non-deterministic outputs rather than prediction accuracy.

Result: Random Forest and Extra Trees show complete randomness dependency; Gradient Boosting (97.6%), K-Nearest Neighbors (98.8%), and LightGBM (99.9%) show strong randomness. Tree-based ensembles are most effective as nonce generators.

Conclusion: Machine learning models, particularly tree-based ensembles, can serve as effective lightweight nonce generators for blockchain-secured energy trading infrastructures resilient to disaster conditions.

Abstract: In disaster scenarios where conventional energy infrastructure is
compromised, secure and traceable energy trading between solar-powered
households and mobile charging units becomes a necessity. To ensure the
integrity of such transactions over a blockchain network, robust and
unpredictable nonce generation is vital. This study proposes an SDN-enabled
architecture where machine learning regressors are leveraged not for their
accuracy, but for their potential to generate randomized values suitable as
nonce candidates. Therefore, it is newly called Proof of AutoML. Here, SDN
allows flexible control over data flows and energy routing policies even in
fragmented or degraded networks, ensuring adaptive response during emergencies.
Using a 9000-sample dataset, we evaluate five AutoML-selected regression models
- Gradient Boosting, LightGBM, Random Forest, Extra Trees, and K-Nearest
Neighbors - not by their prediction accuracy, but by their ability to produce
diverse and non-deterministic outputs across shuffled data inputs. Randomness
analysis reveals that Random Forest and Extra Trees regressors exhibit complete
dependency on randomness, whereas Gradient Boosting, K-Nearest Neighbors and
LightGBM show strong but slightly lower randomness scores (97.6%, 98.8% and
99.9%, respectively). These findings highlight that certain machine learning
models, particularly tree-based ensembles, may serve as effective and
lightweight nonce generators within blockchain-secured, SDN-based energy
trading infrastructures resilient to disaster conditions.

</details>


### [180] [Generalizing Beyond Suboptimality: Offline Reinforcement Learning Learns Effective Scheduling through Random Data](https://arxiv.org/abs/2509.10303)
*Jesse van Remmerden,Zaharah Bukhsh,Yingqian Zhang*

Main category: cs.LG

TL;DR: CDQAC is a novel offline RL algorithm that learns job-shop scheduling policies directly from historical data without online interactions, outperforming both data-generating heuristics and state-of-the-art RL methods with high sample efficiency.


<details>
  <summary>Details</summary>
Motivation: Address limitations of online RL methods for JSP/FJSP that require millions of simulated interactions and suffer from poor sample efficiency due to random policy initialization, while needing to capture real-world complexities.

Method: Conservative Discrete Quantile Actor-Critic (CDQAC) couples a quantile-based critic with delayed policy updates, estimating return distributions for machine-operation pairs rather than direct selection.

Result: CDQAC consistently outperforms original data-generating heuristics and state-of-the-art offline/online RL baselines, showing remarkable performance with only 10-20 training instances. Surprisingly performs better with random heuristic data than higher-quality genetic algorithm data.

Conclusion: CDQAC provides an effective offline RL solution for job-shop scheduling that eliminates costly online interactions while maintaining ability to improve upon suboptimal training data, demonstrating superior performance and sample efficiency.

Abstract: The Job-Shop Scheduling Problem (JSP) and Flexible Job-Shop Scheduling
Problem (FJSP), are canonical combinatorial optimization problems with
wide-ranging applications in industrial operations. In recent years, many
online reinforcement learning (RL) approaches have been proposed to learn
constructive heuristics for JSP and FJSP. Although effective, these online RL
methods require millions of interactions with simulated environments that may
not capture real-world complexities, and their random policy initialization
leads to poor sample efficiency. To address these limitations, we introduce
Conservative Discrete Quantile Actor-Critic (CDQAC), a novel offline RL
algorithm that learns effective scheduling policies directly from historical
data, eliminating the need for costly online interactions, while maintaining
the ability to improve upon suboptimal training data. CDQAC couples a
quantile-based critic with a delayed policy update, estimating the return
distribution of each machine-operation pair rather than selecting pairs
outright. Our extensive experiments demonstrate CDQAC's remarkable ability to
learn from diverse data sources. CDQAC consistently outperforms the original
data-generating heuristics and surpasses state-of-the-art offline and online RL
baselines. In addition, CDQAC is highly sample efficient, requiring only 10-20
training instances to learn high-quality policies. Surprisingly, we find that
CDQAC performs better when trained on data generated by a random heuristic than
when trained on higher-quality data from genetic algorithms and priority
dispatching rules.

</details>


### [181] [GraphCSVAE: Graph Categorical Structured Variational Autoencoder for Spatiotemporal Auditing of Physical Vulnerability Towards Sustainable Post-Disaster Risk Reduction](https://arxiv.org/abs/2509.10308)
*Joshua Dimasaka,Christian Geiß,Robert Muir-Wood,Emily So*

Main category: cs.LG

TL;DR: A novel GraphCSVAE framework for modeling physical vulnerability using satellite data and expert knowledge to track disaster risk reduction progress.


<details>
  <summary>Details</summary>
Motivation: Address the gap in modeling physical vulnerability for disaster risk reduction, as current methods focus mainly on hazard and exposure but lack progress in vulnerability assessment.

Method: Graph Categorical Structured Variational Autoencoder (GraphCSVAE) that integrates deep learning, graph representation, and categorical probabilistic inference with time-series satellite data and expert belief systems.

Result: Successfully modeled spatiotemporal distribution of physical vulnerability in cyclone-impacted Bangladesh and mudslide-affected Sierra Leone, revealing post-disaster regional dynamics.

Conclusion: The framework provides valuable insights for localized spatiotemporal auditing and sustainable post-disaster risk reduction strategies, advancing monitoring capabilities for the UN Sendai Framework.

Abstract: In the aftermath of disasters, many institutions worldwide face challenges in
continually monitoring changes in disaster risk, limiting the ability of key
decision-makers to assess progress towards the UN Sendai Framework for Disaster
Risk Reduction 2015-2030. While numerous efforts have substantially advanced
the large-scale modeling of hazard and exposure through Earth observation and
data-driven methods, progress remains limited in modeling another equally
important yet challenging element of the risk equation: physical vulnerability.
To address this gap, we introduce Graph Categorical Structured Variational
Autoencoder (GraphCSVAE), a novel probabilistic data-driven framework for
modeling physical vulnerability by integrating deep learning, graph
representation, and categorical probabilistic inference, using time-series
satellite-derived datasets and prior expert belief systems. We introduce a
weakly supervised first-order transition matrix that reflects the changes in
the spatiotemporal distribution of physical vulnerability in two
disaster-stricken and socioeconomically disadvantaged areas: (1) the
cyclone-impacted coastal Khurushkul community in Bangladesh and (2) the
mudslide-affected city of Freetown in Sierra Leone. Our work reveals
post-disaster regional dynamics in physical vulnerability, offering valuable
insights into localized spatiotemporal auditing and sustainable strategies for
post-disaster risk reduction.

</details>


### [182] [ARMA Block: A CNN-Based Autoregressive and Moving Average Module for Long-Term Time Series Forecasting](https://arxiv.org/abs/2509.10324)
*Myung Jin Kim,YeongHyeon Park,Il Dong Yun*

Main category: cs.LG

TL;DR: A simple convolutional module called ARMA for long-term time series forecasting, inspired by ARIMA but with direct multi-step forecasting capability and competitive performance on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: To create a simple yet effective convolutional module for long-term time series forecasting that overcomes the limitations of conventional ARIMA models which require iterative multi-step forecasting and are not easily extendable to multivariate settings.

Method: Proposes a convolutional block with two components: one for capturing trend (autoregression) and another for refining local variations (moving average). The block directly performs multi-step forecasting without iterative steps, making it easily extendable to multivariate scenarios.

Result: Experiments on nine benchmark datasets show ARMA achieves competitive accuracy, particularly on datasets with strong trend variations, while maintaining architectural simplicity. The block also inherently encodes absolute positional information.

Conclusion: The ARMA block provides an effective and lightweight solution for time series forecasting, potentially serving as a replacement for positional embeddings in sequential models due to its inherent positional encoding capabilities.

Abstract: This paper proposes a simple yet effective convolutional module for long-term
time series forecasting. The proposed block, inspired by the Auto-Regressive
Integrated Moving Average (ARIMA) model, consists of two convolutional
components: one for capturing the trend (autoregression) and the other for
refining local variations (moving average). Unlike conventional ARIMA, which
requires iterative multi-step forecasting, the block directly performs
multi-step forecasting, making it easily extendable to multivariate settings.
Experiments on nine widely used benchmark datasets demonstrate that our method
ARMA achieves competitive accuracy, particularly on datasets exhibiting strong
trend variations, while maintaining architectural simplicity. Furthermore,
analysis shows that the block inherently encodes absolute positional
information, suggesting its potential as a lightweight replacement for
positional embeddings in sequential models.

</details>


### [183] [Physics-informed sensor coverage through structure preserving machine learning](https://arxiv.org/abs/2509.10363)
*Benjamin David Shaffer,Brooks Kinch,Joseph Klobusicky,M. Ani Hsieh,Nathaniel Trask*

Main category: cs.LG

TL;DR: A machine learning framework using structure-preserving digital twins with conditional neural Whitney forms for adaptive source localization in hydrodynamic-transport systems, combining FEEC guarantees with transformer-based operator learning.


<details>
  <summary>Details</summary>
Motivation: To develop a real-time source localization system that preserves physical structure and conservation laws while adapting to streaming sensor data, overcoming limitations of physics-agnostic approaches in complex geometries.

Method: Uses conditional neural Whitney forms (CNWF) coupling finite element exterior calculus with transformer-based operator learning. Includes conditional attention mechanism for reduced basis identification, staggered scheme with Lloyd's algorithm for sensor placement, and optimal-recovery scheme with importance function.

Result: The framework preserves discrete conservation, adapts to real-time data, and shows improved accuracy in complex geometries compared to physics-agnostic transformer architectures. Demonstrates recovery of point sources under continuity assumptions.

Conclusion: Structure preservation through CNWF provides an effective inductive bias for source identification, yielding physically realizable mappings from sensor data to source fields while maintaining simulation stability and consistency.

Abstract: We present a machine learning framework for adaptive source localization in
which agents use a structure-preserving digital twin of a coupled
hydrodynamic-transport system for real-time trajectory planning and data
assimilation. The twin is constructed with conditional neural Whitney forms
(CNWF), coupling the numerical guarantees of finite element exterior calculus
(FEEC) with transformer-based operator learning. The resulting model preserves
discrete conservation, and adapts in real time to streaming sensor data. It
employs a conditional attention mechanism to identify: a reduced Whitney-form
basis; reduced integral balance equations; and a source field, each compatible
with given sensor measurements. The induced reduced-order environmental model
retains the stability and consistency of standard finite-element simulation,
yielding a physically realizable, regular mapping from sensor data to the
source field. We propose a staggered scheme that alternates between evaluating
the digital twin and applying Lloyd's algorithm to guide sensor placement, with
analysis providing conditions for monotone improvement of a coverage
functional. Using the predicted source field as an importance function within
an optimal-recovery scheme, we demonstrate recovery of point sources under
continuity assumptions, highlighting the role of regularity as a sufficient
condition for localization. Experimental comparisons with physics-agnostic
transformer architectures show improved accuracy in complex geometries when
physical constraints are enforced, indicating that structure preservation
provides an effective inductive bias for source identification.

</details>


### [184] [A Discrepancy-Based Perspective on Dataset Condensation](https://arxiv.org/abs/2509.10367)
*Tong Chen,Raghavendra Selvan*

Main category: cs.LG

TL;DR: A unified framework for dataset condensation that extends beyond generalization to include robustness, privacy, and other properties using discrepancy measures.


<details>
  <summary>Details</summary>
Motivation: To generalize dataset condensation beyond task-specific approaches and provide a formal mathematical foundation using distribution discrepancy measures.

Method: Proposes a unified framework that encompasses existing DC methods and uses notions of discrepancy to quantify distance between probability distributions in different regimes.

Result: The framework broadens DC objectives to include robustness, privacy, and other desirable properties beyond just generalization performance.

Conclusion: The work provides a more comprehensive and formal definition of dataset condensation that can accommodate multiple objectives and properties beyond traditional generalization metrics.

Abstract: Given a dataset of finitely many elements $\mathcal{T} = \{\mathbf{x}_i\}_{i
= 1}^N$, the goal of dataset condensation (DC) is to construct a synthetic
dataset $\mathcal{S} = \{\tilde{\mathbf{x}}_j\}_{j = 1}^M$ which is
significantly smaller ($M \ll N$) such that a model trained from scratch on
$\mathcal{S}$ achieves comparable or even superior generalization performance
to a model trained on $\mathcal{T}$. Recent advances in DC reveal a close
connection to the problem of approximating the data distribution represented by
$\mathcal{T}$ with a reduced set of points. In this work, we present a unified
framework that encompasses existing DC methods and extend the task-specific
notion of DC to a more general and formal definition using notions of
discrepancy, which quantify the distance between probability distribution in
different regimes. Our framework broadens the objective of DC beyond
generalization, accommodating additional objectives such as robustness,
privacy, and other desirable properties.

</details>


### [185] [Data distribution impacts the performance and generalisability of contrastive learning-based foundation models of electrocardiograms](https://arxiv.org/abs/2509.10369)
*Gul Rukh Khattak,Konstantinos Patlatzoglou,Joseph Barker,Libor Pastika,Boroumand Zeidaabadi,Ahmed El-Medany,Hesham Aggour,Yixiu Liang,Antonio H. Ribeiro,Jeffrey Annis,Antonio Luiz Pinho Ribeiro,Junbo Ge,Daniel B. Kramer,Jonathan W. Waks,Evan Brittain,Nicholas Peters,Fu Siong Ng,Arunashis Sau*

Main category: cs.LG

TL;DR: CAPE foundation model uses contrastive learning on ECG data from diverse populations, showing that pretraining cohort composition affects downstream performance and OOD generalization. Proposes IDB strategy to improve robustness.


<details>
  <summary>Details</summary>
Motivation: To understand how cohort demographics and diversity in contrastive learning affect downstream ECG prediction performance and out-of-distribution generalization.

Method: Developed CAPE foundation model using contrastive learning on 5.2M ECGs from diverse populations across 3 continents. Systematically assessed cohort composition effects and proposed In-Distribution Batch (IDB) strategy for better OOD robustness.

Result: Found that downstream performance depends on pretraining cohort distribution properties. Multi-center diverse cohorts improve in-distribution accuracy but reduce OOD generalization by encoding cohort-specific artifacts. IDB strategy enhances OOD robustness.

Conclusion: Cohort composition significantly impacts contrastive learning performance. The proposed IDB strategy helps develop more clinically fair and generalizable foundation models by preserving intra-cohort consistency during pretraining.

Abstract: Contrastive learning is a widely adopted self-supervised pretraining
strategy, yet its dependence on cohort composition remains underexplored. We
present Contrasting by Patient Augmented Electrocardiograms (CAPE) foundation
model and pretrain on four cohorts (n = 5,203,352), from diverse populations
across three continents (North America, South America, Asia). We systematically
assess how cohort demographics, health status, and population diversity
influence the downstream performance for prediction tasks also including two
additional cohorts from another continent (Europe). We find that downstream
performance depends on the distributional properties of the pretraining cohort,
including demographics and health status. Moreover, while pretraining with a
multi-centre, demographically diverse cohort improves in-distribution accuracy,
it reduces out-of-distribution (OOD) generalisation of our contrastive approach
by encoding cohort-specific artifacts. To address this, we propose the
In-Distribution Batch (IDB) strategy, which preserves intra-cohort consistency
during pretraining and enhances OOD robustness. This work provides important
insights for developing clinically fair and generalisable foundation models.

</details>


### [186] [Flow Straight and Fast in Hilbert Space: Functional Rectified Flow](https://arxiv.org/abs/2509.10384)
*Jianxin Zhang,Clayton Scott*

Main category: cs.LG

TL;DR: Functional extension of rectified flow to infinite-dimensional Hilbert spaces with rigorous mathematical foundation and superior experimental performance.


<details>
  <summary>Details</summary>
Motivation: Rectified flow has been successful in finite-dimensional Euclidean spaces but lacks exploration in infinite-dimensional functional settings, while existing functional flow matching methods have restrictive assumptions.

Method: Established functional formulation of rectified flow using superposition principle for continuity equations in infinite-dimensional Hilbert space, extending to functional flow matching and functional probability flow ODEs as nonlinear generalizations.

Result: The framework removes restrictive measure-theoretic assumptions from existing functional flow matching theory and demonstrates superior experimental performance compared to existing functional generative models.

Conclusion: This work provides a rigorous foundation for rectified flow in infinite-dimensional spaces, offering improved theoretical framework and practical performance for functional generative modeling.

Abstract: Many generative models originally developed in finite-dimensional Euclidean
space have functional generalizations in infinite-dimensional settings.
However, the extension of rectified flow to infinite-dimensional spaces remains
unexplored. In this work, we establish a rigorous functional formulation of
rectified flow in an infinite-dimensional Hilbert space. Our approach builds
upon the superposition principle for continuity equations in an
infinite-dimensional space. We further show that this framework extends
naturally to functional flow matching and functional probability flow ODEs,
interpreting them as nonlinear generalizations of rectified flow. Notably, our
extension to functional flow matching removes the restrictive measure-theoretic
assumptions in the existing theory of \citet{kerrigan2024functional}.
Furthermore, we demonstrate experimentally that our method achieves superior
performance compared to existing functional generative models.

</details>


### [187] [Vendi Information Gain for Active Learning and its Application to Ecology](https://arxiv.org/abs/2509.10390)
*Quan Nguyen,Adji Bousso Dieng*

Main category: cs.LG

TL;DR: Vendi information gain (VIG) active learning policy improves biodiversity monitoring by selecting images that reduce dataset-wide prediction uncertainty, achieving near-full-supervision accuracy with only 10% labels on Snapshot Serengeti data.


<details>
  <summary>Details</summary>
Motivation: Camera trap biodiversity monitoring faces labeling bottlenecks; traditional active learning focuses on individual uncertainty without considering dataset-wide uncertainty.

Method: Proposed Vendi information gain (VIG) policy that selects images based on impact on dataset-wide prediction uncertainty, capturing both informativeness and diversity.

Result: VIG achieves impressive predictive accuracy close to full supervision using less than 10% of labels, outperforms standard baselines across metrics and batch sizes, and collects more diverse data.

Conclusion: VIG has broad applicability beyond ecology and demonstrates significant value for biodiversity monitoring in data-limited environments.

Abstract: While monitoring biodiversity through camera traps has become an important
endeavor for ecological research, identifying species in the captured image
data remains a major bottleneck due to limited labeling resources. Active
learning -- a machine learning paradigm that selects the most informative data
to label and train a predictive model -- offers a promising solution, but
typically focuses on uncertainty in the individual predictions without
considering uncertainty across the entire dataset. We introduce a new active
learning policy, Vendi information gain (VIG), that selects images based on
their impact on dataset-wide prediction uncertainty, capturing both
informativeness and diversity. Applied to the Snapshot Serengeti dataset, VIG
achieves impressive predictive accuracy close to full supervision using less
than 10% of the labels. It consistently outperforms standard baselines across
metrics and batch sizes, collecting more diverse data in the feature space. VIG
has broad applicability beyond ecology, and our results highlight its value for
biodiversity monitoring in data-limited environments.

</details>


### [188] [Inpainting-Guided Policy Optimization for Diffusion Large Language Models](https://arxiv.org/abs/2509.10396)
*Siyan Zhao,Mengchen Liu,Jing Huang,Miao Liu,Chenyu Wang,Bo Liu,Yuandong Tian,Guan Pang,Sean Bell,Aditya Grover,Feiyu Chen*

Main category: cs.LG

TL;DR: IGPO is a reinforcement learning framework that uses masked diffusion LLMs' inpainting capability to guide exploration by strategically inserting partial ground-truth reasoning traces during sampling, improving sample efficiency and achieving SOTA results on math benchmarks.


<details>
  <summary>Details</summary>
Motivation: Autoregressive LLMs face exploration challenges in RL with sparse rewards and sample waste. Masked diffusion LLMs offer unique inpainting capabilities that can guide exploration more efficiently.

Method: IGPO framework inserts partial ground-truth reasoning traces during online sampling to steer exploration. Combines supervised fine-tuning on synthetic concise traces with entropy-based filtering and other techniques.

Result: Achieves substantial gains across GSM8K, Math500, and AMC mathematical benchmarks, setting new state-of-the-art results for full-attention masked diffusion LLMs.

Conclusion: Inpainting-guided exploration effectively bridges supervised fine-tuning and reinforcement learning, restoring meaningful gradients in group-based optimization methods and significantly improving sample efficiency.

Abstract: Masked diffusion large language models (dLLMs) are emerging as promising
alternatives to autoregressive LLMs, offering competitive performance while
supporting unique generation capabilities such as inpainting. We explore how
inpainting can inform RL algorithm design for dLLMs. Aligning LLMs with
reinforcement learning faces an exploration challenge: sparse reward signals
and sample waste when models fail to discover correct solutions. While this
inefficiency affects LLMs broadly, dLLMs offer a distinctive opportunity--their
inpainting ability can guide exploration. We introduce IGPO (Inpainting Guided
Policy Optimization), an RL framework that strategically inserts partial
ground-truth reasoning traces during online sampling. Unlike providing full
solutions, inpainting steers exploration toward promising trajectory spaces
while preserving self-generated reasoning, bridging supervised fine-tuning and
reinforcement learning. We apply IGPO to group-based optimization methods such
as GRPO, where exploration failures cause zero advantages and gradients. IGPO
restores meaningful gradients while improving sample efficiency. We also
propose supervised fine-tuning on synthetically rewritten concise traces that
better align with dLLM generation patterns. With additional techniques
including entropy-based filtering, our training recipe yields substantial gains
across three mathematical benchmarks--GSM8K, Math500, and AMC--achieving new
state-of-the-art results for full-attention masked dLLMs.

</details>


### [189] [Multipole Semantic Attention: A Fast Approximation of Softmax Attention for Pretraining](https://arxiv.org/abs/2509.10406)
*Rupert Mitchell,Kristian Kersting*

Main category: cs.LG

TL;DR: MuSe is an efficient attention approximation that combines semantic clustering with multipole expansions to reduce transformer's quadratic complexity, achieving 3x speedup with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: To address the quadratic computational complexity of transformer attention mechanisms in long context lengths while maintaining performance.

Method: Clusters queries and keys separately in learned representation spaces, uses hierarchical two-stage attention with centroid-based approximations and dipole corrections for directional variance, and operates as drop-in replacement.

Result: Achieves O(NCD) complexity for acausal attention, 3x speedup over Flash Attention at 8k context length, and 12.2% runtime reduction with only 0.36% loss degradation in pretraining.

Conclusion: Multipole semantic attention provides an efficient and viable approximation for transformer pretraining with significant computational savings and minimal performance degradation.

Abstract: We present Multipole Semantic Attention (MuSe), an efficient approximation of
softmax attention that combines semantic clustering with multipole expansions
from computational physics. Our method addresses the quadratic computational
complexity of transformers in the context length by clustering queries and keys
separately in their learned representation spaces, enabling a hierarchical
two-stage attention mechanism. Unlike prior clustering approaches that group
only keys or use unified clustering, we maintain separate clusterings that
respect attention's asymmetric treatment of these spaces. We augment
centroid-based (monopole) approximations with dipole corrections that capture
directional variance within clusters, preserving richer information during
training. The method operates as a drop-in replacement for standard attention,
requiring only hyperparameter specification without architectural
modifications. Our approach achieves $\mathcal{O}(NCD)$ complexity for acausal
attention with $C$ clusters and $\mathcal{O}(NCD \log N)$ for causal attention.
On isolated attention layers, we demonstrate $3\times$ speedup over CUDNN Flash
Attention at 8k context length, with relative squared errors below 20%. For
causal attention, we develop a hierarchical block decomposition that combines
exact local computation with efficient long-range approximation. In end-to-end
pretraining of a 30M parameter model on book-length texts with 16k context, we
achieve 12.2% runtime reduction with only 0.36% loss degradation, establishing
the viability of multipole approximations for efficient transformer
pretraining.

</details>


### [190] [Run-Time Monitoring of ERTMS/ETCS Control Flow by Process Mining](https://arxiv.org/abs/2509.10419)
*Francesco Vitale,Tommaso Zoppi,Francesco Flammini,Nicola Mazzocca*

Main category: cs.LG

TL;DR: Using process mining and unsupervised ML for run-time control-flow anomaly detection in railway systems to enhance resilience against unknown faults and cyber-threats.


<details>
  <summary>Details</summary>
Motivation: Ensure railway system resilience against uncertainties, residual faults, environmental changes, and emerging cyber-threats that verification processes may miss.

Method: Process mining to learn actual control flow from execution traces, online conformance checking for run-time monitoring, and unsupervised ML for anomaly localization.

Result: Tested on ERTMS/ETCS L2 RBC/RBC Handover scenario, showing high accuracy, efficiency, and explainability in detecting and localizing anomalies.

Conclusion: Process mining combined with unsupervised ML provides effective run-time anomaly detection and localization for enhancing railway system resilience.

Abstract: Ensuring the resilience of computer-based railways is increasingly crucial to
account for uncertainties and changes due to the growing complexity and
criticality of those systems. Although their software relies on strict
verification and validation processes following well-established best-practices
and certification standards, anomalies can still occur at run-time due to
residual faults, system and environmental modifications that were unknown at
design-time, or other emergent cyber-threat scenarios. This paper explores
run-time control-flow anomaly detection using process mining to enhance the
resilience of ERTMS/ETCS L2 (European Rail Traffic Management System / European
Train Control System Level 2). Process mining allows learning the actual
control flow of the system from its execution traces, thus enabling run-time
monitoring through online conformance checking. In addition, anomaly
localization is performed through unsupervised machine learning to link
relevant deviations to critical system components. We test our approach on a
reference ERTMS/ETCS L2 scenario, namely the RBC/RBC Handover, to show its
capability to detect and localize anomalies with high accuracy, efficiency, and
explainability.

</details>


### [191] [Understanding Outer Optimizers in Local SGD: Learning Rates, Momentum, and Acceleration](https://arxiv.org/abs/2509.10439)
*Ahmed Khaled,Satyen Kale,Arthur Douillard,Chi Jin,Rob Fergus,Manzil Zaheer*

Main category: cs.LG

TL;DR: Local SGD with outer optimizer tuning improves communication efficiency in distributed ML, allowing outer learning rates >1 to compensate for inner learning rate mis-tuning and reduce gradient noise.


<details>
  <summary>Details</summary>
Motivation: Communication bottleneck in distributed ML with large batch sizes and parallel hardware needs reduction through Local SGD, but optimal outer optimizer configuration is unclear.

Method: Theoretical analysis of Local SGD with focus on outer optimizer hyperparameters, proving convergence guarantees and studying momentum/acceleration effects. Comprehensive experiments with language models.

Result: Tuning outer learning rate enables trade-off between optimization error and gradient noise variance, can compensate for poor inner learning rate tuning, and should sometimes exceed 1. Outer acceleration improves convergence rate per communication round.

Conclusion: Careful outer optimizer tuning is crucial for Local SGD performance, with outer learning rates potentially >1 providing benefits, and outer acceleration outperforming local acceleration methods.

Abstract: Modern machine learning often requires training with large batch size,
distributed data, and massively parallel compute hardware (like mobile and
other edge devices or distributed data centers). Communication becomes a major
bottleneck in such settings but methods like Local Stochastic Gradient Descent
(Local SGD) show great promise in reducing this additional communication
overhead. Local SGD consists of three parts: a local optimization process, an
aggregation mechanism, and an outer optimizer that uses the aggregated updates
from the nodes to produce a new model. While there exists an extensive
literature on understanding the impact of hyperparameters in the local
optimization process, the choice of outer optimizer and its hyperparameters is
less clear. We study the role of the outer optimizer in Local SGD, and prove
new convergence guarantees for the algorithm. In particular, we show that
tuning the outer learning rate allows us to (a) trade off between optimization
error and stochastic gradient noise variance, and (b) make up for ill-tuning of
the inner learning rate. Our theory suggests that the outer learning rate
should sometimes be set to values greater than $1$. We extend our results to
settings where we use momentum in the outer optimizer, and we show a similar
role for the momentum-adjusted outer learning rate. We also study acceleration
in the outer optimizer and show that it improves the convergence rate as a
function of the number of communication rounds, improving upon the convergence
rate of prior algorithms that apply acceleration locally. Finally, we also
introduce a novel data-dependent analysis of Local SGD that yields further
insights on outer learning rate tuning. We conduct comprehensive experiments
with standard language models and various outer optimizers to validate our
theory.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [192] [VStyle: A Benchmark for Voice Style Adaptation with Spoken Instructions](https://arxiv.org/abs/2509.09716)
*Jun Zhan,Mingyang Han,Yuxuan Xie,Chen Wang,Dong Zhang,Kexin Huang,Haoxiang Shi,DongXiao Wang,Tengtao Song,Qinyuan Cheng,Shimin Li,Jun Song,Xipeng Qiu,Bo Zheng*

Main category: cs.SD

TL;DR: The paper introduces Voice Style Adaptation (VSA) task and VStyle benchmark to evaluate spoken language models' ability to adapt speaking style based on spoken instructions, revealing current models' limitations in controllable style adaptation.


<details>
  <summary>Details</summary>
Motivation: While spoken language models have advanced in semantic accuracy and instruction following, their ability to adapt speaking style based on spoken commands has received limited attention, creating a gap in natural human-machine interaction.

Method: The authors introduce VStyle, a bilingual benchmark covering four speech generation categories, and propose LALM as a Judge framework for progressive evaluation along textual faithfulness, style adherence, and naturalness.

Result: Experiments on commercial systems and open source SLMs show that current models face clear limitations in controllable style adaptation, demonstrating the novelty and challenge of this task.

Conclusion: The VStyle benchmark and evaluation toolkit provide a foundation for advancing human-centered spoken interaction, with the dataset and code made publicly available to the research community.

Abstract: Spoken language models (SLMs) have emerged as a unified paradigm for speech
understanding and generation, enabling natural human machine interaction.
However, while most progress has focused on semantic accuracy and instruction
following, the ability of SLMs to adapt their speaking style based on spoken
instructions has received limited attention. We introduce Voice Style
Adaptation (VSA), a new task that examines whether SLMs can modify their
speaking style, such as timbre, prosody, or persona following natural language
spoken commands. To study this task, we present VStyle, a bilingual (Chinese &
English) benchmark covering four categories of speech generation: acoustic
attributes, natural language instruction, role play, and implicit empathy. We
also introduce the Large Audio Language Model as a Judge (LALM as a Judge)
framework, which progressively evaluates outputs along textual faithfulness,
style adherence, and naturalness, ensuring reproducible and objective
assessment. Experiments on commercial systems and open source SLMs demonstrate
that current models face clear limitations in controllable style adaptation,
highlighting both the novelty and challenge of this task. By releasing VStyle
and its evaluation toolkit, we aim to provide the community with a foundation
for advancing human centered spoken interaction. The dataset and code are
publicly available at
\href{https://junzhan2000.github.io/VStyle.github.io/}{project's homepage}.

</details>


### [193] [Testing chatbots on the creation of encoders for audio conditioned image generation](https://arxiv.org/abs/2509.09717)
*Jorge E. León,Miguel Carrasco*

Main category: cs.SD

TL;DR: Chatbots were prompted to design audio encoders to replace CLIP text encoder in Stable Diffusion 1.5 for image generation from sound, but none achieved satisfactory results despite generating valid architectures.


<details>
  <summary>Details</summary>
Motivation: To explore whether state-of-the-art conversational agents can design effective audio encoders to enable image synthesis directly from sound, replacing traditional text encoders in generative image models.

Method: Prompted five publicly available chatbots to propose neural architectures for audio encoders under shared conditions, trained each valid suggestion on over 2M audio-image-text observations, and evaluated on validation/test sets with various metrics and qualitative analysis.

Result: Although chatbots generated valid model designs, none achieved satisfactory results - their audio embeddings failed to align reliably with the original text encoder. Gemini audio encoder showed best quantitative metrics, while Grok produced more coherent images when paired with text encoder.

Conclusion: Findings reveal shared architectural bias across chatbots and underscore the remaining coding gap that needs to be bridged in future versions. Researchers should perform more specialized tasks to fully test chatbots' creativity and reasoning beyond well-known solutions.

Abstract: On one hand, recent advances in chatbots has led to a rising popularity in
using these models for coding tasks. On the other hand, modern generative image
models primarily rely on text encoders to translate semantic concepts into
visual representations, even when there is clear evidence that audio can be
employed as input as well. Given the previous, in this work, we explore whether
state-of-the-art conversational agents can design effective audio encoders to
replace the CLIP text encoder from Stable Diffusion 1.5, enabling image
synthesis directly from sound. We prompted five publicly available chatbots to
propose neural architectures to work as these audio encoders, with a set of
well-explained shared conditions. Each valid suggested encoder was trained on
over two million context related audio-image-text observations, and evaluated
on held-out validation and test sets using various metrics, together with a
qualitative analysis of their generated images. Although almost all chatbots
generated valid model designs, none achieved satisfactory results, indicating
that their audio embeddings failed to align reliably with those of the original
text encoder. Among the proposals, the Gemini audio encoder showed the best
quantitative metrics, while the Grok audio encoder produced more coherent
images (particularly, when paired with the text encoder). Our findings reveal a
shared architectural bias across chatbots and underscore the remaining coding
gap that needs to be bridged in future versions of these models. We also
created a public demo so everyone could study and try out these audio encoders.
Finally, we propose research questions that should be tackled in the future,
and encourage other researchers to perform more focused and highly specialized
tasks like this one, so the respective chatbots cannot make use of well-known
solutions and their creativity/reasoning is fully tested.

</details>


### [194] [SoilSound: Smartphone-based Soil Moisture Estimation](https://arxiv.org/abs/2509.09823)
*Yixuan Gao,Tanvir Ahmed,Shuang He,Zhongqi Cheng,Rajalakshmi Nandakumar*

Main category: cs.SD

TL;DR: SoilSound is a smartphone-based acoustic sensing system that measures soil moisture non-invasively using built-in speakers and microphones, achieving 2.39% MAE across various soil types without calibration.


<details>
  <summary>Details</summary>
Motivation: Existing soil moisture monitoring methods are either invasive (requiring soil disturbance) or require specialized equipment, limiting accessibility for the general public and resource-limited settings.

Method: Uses smartphone's built-in speaker and microphone to perform vertical acoustic scans, sending chirps and recording reflections. Processes data with a convolutional neural network for on-device moisture estimation based on surface roughness effects rather than transmissive properties.

Result: Achieved mean absolute error of 2.39% across 10 different locations, accurately tracking soil moisture levels from 15.9% to 34.0% across multiple soil types, environments, and users with negligible computational overhead.

Conclusion: SoilSound enables widespread, accessible soil moisture monitoring without calibration or soil disturbance, benefiting home gardeners, urban farmers, and agricultural communities in resource-limited settings.

Abstract: Soil moisture monitoring is essential for agriculture and environmental
management, yet existing methods require either invasive probes disturbing the
soil or specialized equipment, limiting access to the public. We present
SoilSound, an ubiquitous accessible smartphone-based acoustic sensing system
that can measure soil moisture without disturbing the soil. We leverage the
built-in speaker and microphone to perform a vertical scan mechanism to
accurately measure moisture without any calibration. Unlike existing work that
use transmissive properties, we propose an alternate model for acoustic
reflections in soil based on the surface roughness effect to enable moisture
sensing without disturbing the soil. The system works by sending acoustic
chirps towards the soil and recording the reflections during a vertical scan,
which are then processed and fed to a convolutional neural network for
on-device soil moisture estimation with negligible computational, memory, or
power overhead. We evaluated the system by training with curated soils in boxes
in the lab and testing in the outdoor fields and show that SoilSound achieves a
mean absolute error (MAE) of 2.39% across 10 different locations. Overall, the
evaluation shows that SoilSound can accurately track soil moisture levels
ranging from 15.9% to 34.0% across multiple soil types, environments, and
users; without requiring any calibration or disturbing the soil, enabling
widespread moisture monitoring for home gardeners, urban farmers, citizen
scientists, and agricultural communities in resource-limited settings.

</details>


### [195] [CoDiCodec: Unifying Continuous and Discrete Compressed Representations of Audio](https://arxiv.org/abs/2509.09836)
*Marco Pasini,Stefan Lattner,George Fazekas*

Main category: cs.SD

TL;DR: CoDiCodec is a novel audio autoencoder that produces both continuous embeddings and discrete tokens from the same model, achieving high compression (2.38 kbps) with superior audio quality using FSQ quantization and parallel decoding.


<details>
  <summary>Details</summary>
Motivation: Existing audio autoencoders force a choice between continuous embeddings and discrete tokens, and struggle to achieve high compression ratios while maintaining audio fidelity.

Method: Uses Finite Scalar Quantization (FSQ) with novel FSQ-dropout technique, trained with a single consistency loss. Supports both autoregressive and parallel decoding strategies.

Result: Outperforms existing continuous and discrete autoencoders at similar bitrates, achieving compressed continuous embeddings at ~11 Hz and discrete tokens at 2.38 kbps with superior audio quality.

Conclusion: Enables unified audio compression approach, bridging the gap between continuous and discrete generative modeling paradigms with flexible downstream task support.

Abstract: Efficiently representing audio signals in a compressed latent space is
critical for latent generative modelling. However, existing autoencoders often
force a choice between continuous embeddings and discrete tokens. Furthermore,
achieving high compression ratios while maintaining audio fidelity remains a
challenge. We introduce CoDiCodec, a novel audio autoencoder that overcomes
these limitations by both efficiently encoding global features via summary
embeddings, and by producing both compressed continuous embeddings at ~ 11 Hz
and discrete tokens at a rate of 2.38 kbps from the same trained model,
offering unprecedented flexibility for different downstream generative tasks.
This is achieved through Finite Scalar Quantization (FSQ) and a novel
FSQ-dropout technique, and does not require additional loss terms beyond the
single consistency loss used for end-to-end training. CoDiCodec supports both
autoregressive decoding and a novel parallel decoding strategy, with the latter
achieving superior audio quality and faster decoding. CoDiCodec outperforms
existing continuous and discrete autoencoders at similar bitrates in terms of
reconstruction audio quality. Our work enables a unified approach to audio
compression, bridging the gap between continuous and discrete generative
modelling paradigms.

</details>


### [196] [Prototypical Contrastive Learning For Improved Few-Shot Audio Classification](https://arxiv.org/abs/2509.10074)
*Christos Sgouropoulos,Christos Nikou,Stefanos Vlachos,Vasileios Theiou,Christos Foukanelis,Theodoros Giannakopoulos*

Main category: cs.SD

TL;DR: Integration of supervised contrastive loss with prototypical few-shot learning for audio classification achieves SOTA performance on MetaAudio benchmark.


<details>
  <summary>Details</summary>
Motivation: Few-shot learning in audio classification is underexplored compared to image domain, and existing methods need improvement for limited labeled data scenarios.

Method: Combines supervised contrastive loss (angular loss variant) with prototypical few-shot training, using SpecAugment and self-attention to create unified embeddings from augmented inputs.

Result: Achieves state-of-the-art performance in 5-way, 5-shot setting on MetaAudio benchmark with five datasets.

Conclusion: The proposed integration of contrastive loss with prototypical learning effectively improves few-shot audio classification performance.

Abstract: Few-shot learning has emerged as a powerful paradigm for training models with
limited labeled data, addressing challenges in scenarios where large-scale
annotation is impractical. While extensive research has been conducted in the
image domain, few-shot learning in audio classification remains relatively
underexplored. In this work, we investigate the effect of integrating
supervised contrastive loss into prototypical few shot training for audio
classification. In detail, we demonstrate that angular loss further improves
the performance compared to the standard contrastive loss. Our method leverages
SpecAugment followed by a self-attention mechanism to encapsulate diverse
information of augmented input versions into one unified embedding. We evaluate
our approach on MetaAudio, a benchmark including five datasets with predefined
splits, standardized preprocessing, and a comprehensive set of few-shot
learning models for comparison. The proposed approach achieves state-of-the-art
performance in a 5-way, 5-shot setting.

</details>


### [197] [Improving Audio Event Recognition with Consistency Regularization](https://arxiv.org/abs/2509.10391)
*Shanmuka Sadhu,Weiran Wang*

Main category: cs.SD

TL;DR: Consistency regularization improves audio event recognition on AudioSet, showing gains in both supervised and semi-supervised settings with different dataset sizes.


<details>
  <summary>Details</summary>
Motivation: To apply consistency regularization (CR) - which enforces agreement between model predictions on augmented views - to audio event recognition, building on its recent success in automatic speech recognition.

Method: Extensive ablation studies using CR with data augmentation on AudioSet, testing both small (~20k) and large (~1.8M) supervised training sets, and extending to semi-supervised setup with labeled and unlabeled samples.

Result: CR brings consistent improvement over supervised baselines that already use heavy data augmentation. Stronger augmentation and multiple augmentations provide additional gains for small training sets. Semi-supervised CR further improves performance over the best small-set model.

Conclusion: Consistency regularization is effective for audio event recognition, working well across different dataset sizes and in both supervised and semi-supervised learning scenarios.

Abstract: Consistency regularization (CR), which enforces agreement between model
predictions on augmented views, has found recent benefits in automatic speech
recognition [1]. In this paper, we propose the use of consistency
regularization for audio event recognition, and demonstrate its effectiveness
on AudioSet. With extensive ablation studies for both small ($\sim$20k) and
large ($\sim$1.8M) supervised training sets, we show that CR brings consistent
improvement over supervised baselines which already heavily utilize data
augmentation, and CR using stronger augmentation and multiple augmentations
leads to additional gain for the small training set. Furthermore, we extend the
use of CR into the semi-supervised setup with 20K labeled samples and 1.8M
unlabeled samples, and obtain performance improvement over our best model
trained on the small set.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [198] [Machine-learning competition to grade EEG background patterns in newborns with hypoxic-ischaemic encephalopathy](https://arxiv.org/abs/2509.09695)
*Fabio Magarelli,Geraldine B. Boylan,Saeed Montazeri,Feargal O'Sullivan,Dominic Lightbody,Minoo Ashoori,Tamara Skoric Ceranic,John M. O'Toole*

Main category: eess.SP

TL;DR: A machine learning competition was organized to develop models for classifying EEG background patterns in newborns, using a 353-hour dataset from 102 infants. While feature-based models performed best on test data, deep learning models showed better generalization on validation sets, highlighting challenges with unseen data.


<details>
  <summary>Details</summary>
Motivation: To address the shortage of high-quality annotated EEG data for neonatal brain monitoring and leverage crowdsourcing through ML competitions to develop accurate models for classifying EEG background pattern severity in newborns.

Method: Compiled a retrospective multi-center dataset of 353 hours of EEG from 102 newborns, fully anonymized and divided into training, testing, and validation sets. Created a web-based competition platform to host ML models for EEG classification, with top 4 models evaluated offline on separate validation data.

Result: Feature-based models ranked first on testing data but deep learning models generalized better on validation sets. All models showed significant performance decline on validation compared to testing, demonstrating generalization challenges with unseen neonatal EEG data.

Conclusion: The study emphasizes the need for held-out validation datasets in ML studies with neonatal EEG and highlights the importance of training on large, diverse datasets for robust generalization. Open-access data and collaborative ML development can accelerate clinical decision-support tools for neonatal neuromonitoring.

Abstract: Machine learning (ML) has the potential to support and improve expert
performance in monitoring the brain function of at-risk newborns. Developing
accurate and reliable ML models depends on access to high-quality, annotated
data, a resource in short supply. ML competitions address this need by
providing researchers access to expertly annotated datasets, fostering shared
learning through direct model comparisons, and leveraging the benefits of
crowdsourcing diverse expertise. We compiled a retrospective dataset containing
353 hours of EEG from 102 individual newborns from a multi-centre study. The
data was fully anonymised and divided into training, testing, and held-out
validation datasets. EEGs were graded for the severity of abnormal background
patterns. Next, we created a web-based competition platform and hosted a
machine learning competition to develop ML models for classifying the severity
of EEG background patterns in newborns. After the competition closed, the top 4
performing models were evaluated offline on a separate held-out validation
dataset. Although a feature-based model ranked first on the testing dataset,
deep learning models generalised better on the validation sets. All methods had
a significant decline in validation performance compared to the testing
performance. This highlights the challenges for model generalisation on unseen
data, emphasising the need for held-out validation datasets in ML studies with
neonatal EEG. The study underscores the importance of training ML models on
large and diverse datasets to ensure robust generalisation. The competition's
outcome demonstrates the potential for open-access data and collaborative ML
development to foster a collaborative research environment and expedite the
development of clinical decision-support tools for neonatal neuromonitoring.

</details>


### [199] [FetalSleepNet: A Transfer Learning Framework with Spectral Equalisation Domain Adaptation for Fetal Sleep Stage Classification](https://arxiv.org/abs/2509.10082)
*Weitao Tang,Johann Vargas-Calixto,Nasim Katebi,Nhi Tran,Sharmony B. Kelly,Gari D. Clifford,Robert Galinsky,Faezeh Marzbanrad*

Main category: eess.SP

TL;DR: FetalSleepNet is the first deep learning model for classifying sleep states from fetal sheep EEG, achieving 86.6% accuracy using transfer learning and spectral equalization to adapt adult EEG models to fetal data.


<details>
  <summary>Details</summary>
Motivation: Fetal EEG is complex to acquire and difficult to interpret consistently, but accurate sleep stage classification could enable early detection of abnormal brain maturation associated with pregnancy complications like hypoxia or growth restriction.

Method: Used a lightweight deep neural network originally developed for adult EEG sleep staging, applied transfer learning with spectral equalization-based domain adaptation to reduce cross-domain mismatch between adult and fetal EEG data.

Result: Full fine-tuning combined with spectral equalization achieved best performance (86.6% accuracy, 62.5 macro F1-score), outperforming baseline models and demonstrating successful adaptation from adult to fetal EEG.

Conclusion: FetalSleepNet is the first deep learning framework for automated fetal EEG sleep staging, serving as a label engine for large-scale weak/semi-supervised labeling and enabling training on less invasive clinical signals like Doppler Ultrasound or ECG data.

Abstract: Introduction: This study presents FetalSleepNet, the first published deep
learning approach to classifying sleep states from the ovine
electroencephalogram (EEG). Fetal EEG is complex to acquire and difficult and
laborious to interpret consistently. However, accurate sleep stage
classification may aid in the early detection of abnormal brain maturation
associated with pregnancy complications (e.g. hypoxia or intrauterine growth
restriction).
  Methods: EEG electrodes were secured onto the ovine dura over the parietal
cortices of 24 late gestation fetal sheep. A lightweight deep neural network
originally developed for adult EEG sleep staging was trained on the ovine EEG
using transfer learning from adult EEG. A spectral equalisation-based domain
adaptation strategy was used to reduce cross-domain mismatch.
  Results: We demonstrated that while direct transfer performed poorly, full
fine tuning combined with spectral equalisation achieved the best overall
performance (accuracy: 86.6 percent, macro F1-score: 62.5), outperforming
baseline models.
  Conclusions: To the best of our knowledge, FetalSleepNet is the first deep
learning framework specifically developed for automated sleep staging from the
fetal EEG. Beyond the laboratory, the EEG-based sleep stage classifier
functions as a label engine, enabling large scale weak/semi supervised labeling
and distillation to facilitate training on less invasive signals that can be
acquired in the clinic, such as Doppler Ultrasound or electrocardiogram data.
FetalSleepNet's lightweight design makes it well suited for deployment in low
power, real time, and wearable fetal monitoring systems.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [200] [Differential Robustness in Transformer Language Models: Empirical Evaluation Under Adversarial Text Attacks](https://arxiv.org/abs/2509.09706)
*Taniya Gidatkar,Oluwaseun Ajao,Matthew Shardlow*

Main category: cs.CR

TL;DR: Evaluation of LLM resilience against adversarial attacks shows RoBERTa-Base and Flan-T5 are highly robust (0% attack success), while BERT-Base is vulnerable (93.75% success rate).


<details>
  <summary>Details</summary>
Motivation: To assess the security and robustness of large language models against adversarial attacks to identify strengths and weaknesses in current safeguarding approaches.

Method: Used TextFooler and BERTAttack tools to systematically test adversarial resilience on Flan-T5, BERT, and RoBERTa-Base models.

Result: RoBERTa-Base and Flan-T5 showed exceptional resilience with 0% attack success rates, while BERT-Base was highly vulnerable with accuracy dropping from 48% to 3% under TextFooler attacks.

Conclusion: Some LLMs have effective defensive mechanisms but require substantial computational resources; study provides practical recommendations for developing more efficient defensive strategies.

Abstract: This study evaluates the resilience of large language models (LLMs) against
adversarial attacks, specifically focusing on Flan-T5, BERT, and RoBERTa-Base.
Using systematically designed adversarial tests through TextFooler and
BERTAttack, we found significant variations in model robustness. RoBERTa-Base
and FlanT5 demonstrated remarkable resilience, maintaining accuracy even when
subjected to sophisticated attacks, with attack success rates of 0%. In
contrast. BERT-Base showed considerable vulnerability, with TextFooler
achieving a 93.75% success rate in reducing model accuracy from 48% to just 3%.
Our research reveals that while certain LLMs have developed effective defensive
mechanisms, these safeguards often require substantial computational resources.
This study contributes to the understanding of LLM security by identifying
existing strengths and weaknesses in current safeguarding approaches and
proposes practical recommendations for developing more efficient and effective
defensive strategies.

</details>


### [201] [ZORRO: Zero-Knowledge Robustness and Privacy for Split Learning (Full Version)](https://arxiv.org/abs/2509.09787)
*Nojan Sheybani,Alessandro Pegoraro,Jonathan Knauer,Phillip Rieger,Elissa Mollakuqe,Farinaz Koushanfar,Ahmad-Reza Sadeghi*

Main category: cs.CR

TL;DR: ZORRO is a client-side defense scheme for Split Learning that uses zero-knowledge proofs to verify correct execution of defense algorithms against gradient poisoning attacks, reducing attack success rates to <6% with minimal overhead.


<details>
  <summary>Details</summary>
Motivation: Split Learning enables resource-constrained clients to train DNNs collaboratively without sharing data, but malicious clients can inject backdoors through poisoned intermediate gradients. Existing defenses are server-focused and introduce overhead, while client-side defenses struggle to enforce correct execution.

Method: Uses interactive zero-knowledge proofs (ZKPs) to verify clients' correct execution of client-located defense algorithms. Leverages frequency representation of model partitions to inspect locally trained models in untrusted environments, ensuring benign checkpoints are forwarded.

Result: Reduces attack success rate to less than 6% across different model architectures, attack strategies, and data scenarios. Maintains low overhead of less than 10 seconds even for models with 1,000,000 parameters on client-side.

Conclusion: ZORRO provides an effective, private, verifiable, and robust defense for Split Learning that addresses client-side security challenges with minimal computational overhead through innovative use of zero-knowledge proofs.

Abstract: Split Learning (SL) is a distributed learning approach that enables
resource-constrained clients to collaboratively train deep neural networks
(DNNs) by offloading most layers to a central server while keeping in- and
output layers on the client-side. This setup enables SL to leverage server
computation capacities without sharing data, making it highly effective in
resource-constrained environments dealing with sensitive data. However, the
distributed nature enables malicious clients to manipulate the training
process. By sending poisoned intermediate gradients, they can inject backdoors
into the shared DNN. Existing defenses are limited by often focusing on
server-side protection and introducing additional overhead for the server. A
significant challenge for client-side defenses is enforcing malicious clients
to correctly execute the defense algorithm.
  We present ZORRO, a private, verifiable, and robust SL defense scheme.
Through our novel design and application of interactive zero-knowledge proofs
(ZKPs), clients prove their correct execution of a client-located defense
algorithm, resulting in proofs of computational integrity attesting to the
benign nature of locally trained DNN portions. Leveraging the frequency
representation of model partitions enables ZORRO to conduct an in-depth
inspection of the locally trained models in an untrusted environment, ensuring
that each client forwards a benign checkpoint to its succeeding client. In our
extensive evaluation, covering different model architectures as well as various
attack strategies and data scenarios, we show ZORRO's effectiveness, as it
reduces the attack success rate to less than 6\% while causing even for models
storing \numprint{1000000} parameters on the client-side an overhead of less
than 10 seconds.

</details>


### [202] [SmartCoder-R1: Towards Secure and Explainable Smart Contract Generation with Security-Aware Group Relative Policy Optimization](https://arxiv.org/abs/2509.09942)
*Lei Yu,Jingyuan Zhang,Xin Wang,Jiajia Ma,Li Yang,Fengjun Zhang*

Main category: cs.CR

TL;DR: SmartCoder-R1 is a novel framework that combines continual pre-training, long chain-of-thought supervised fine-tuning, and security-aware reinforcement learning to generate secure and explainable smart contracts, achieving state-of-the-art performance with significant improvements over baselines.


<details>
  <summary>Details</summary>
Motivation: Smart contracts manage high-value assets and vulnerabilities can cause catastrophic financial losses. LLMs operate as unauditable black boxes and generate code with critical security vulnerabilities, requiring a solution that provides both security and transparency.

Method: 1) Continual Pre-training (CPT) to specialize the model; 2) Long Chain-of-Thought Supervised Fine-Tuning (L-CoT SFT) on 7,998 expert-validated reasoning-and-code samples; 3) Security-Aware Group Relative Policy Optimization (S-GRPO) reinforcement learning with weighted rewards for compilation success, security compliance, and format correctness.

Result: State-of-the-art performance on 756 real-world functions: ComPass 87.70%, VulRate 8.60%, SafeAval 80.16%, FuncRate 53.84%, FullRate 50.53% (45.79% improvement over DeepSeek-R1). Human evaluations show high quality: Functionality 82.7%, Security 85.3%, Clarity 90.7%.

Conclusion: SmartCoder-R1 successfully addresses both security vulnerabilities and lack of transparency in LLM-generated smart contracts through its integrated framework, establishing new benchmarks for secure and explainable code generation with significant performance improvements.

Abstract: Smart contracts automate the management of high-value assets, where
vulnerabilities can lead to catastrophic financial losses. This challenge is
amplified in Large Language Models (LLMs) by two interconnected failures: they
operate as unauditable "black boxes" lacking a transparent reasoning process,
and consequently, generate code riddled with critical security vulnerabilities.
To address both issues, we propose SmartCoder-R1 (based on Qwen2.5-Coder-7B), a
novel framework for secure and explainable smart contract generation. It begins
with Continual Pre-training (CPT) to specialize the model. We then apply Long
Chain-of-Thought Supervised Fine-Tuning (L-CoT SFT) on 7,998 expert-validated
reasoning-and-code samples to train the model to emulate human security
analysis. Finally, to directly mitigate vulnerabilities, we employ
Security-Aware Group Relative Policy Optimization (S-GRPO), a reinforcement
learning phase that refines the generation policy by optimizing a weighted
reward signal for compilation success, security compliance, and format
correctness. Evaluated against 17 baselines on a benchmark of 756 real-world
functions, SmartCoder-R1 establishes a new state of the art, achieving top
performance across five key metrics: a ComPass of 87.70%, a VulRate of 8.60%, a
SafeAval of 80.16%, a FuncRate of 53.84%, and a FullRate of 50.53%. This
FullRate marks a 45.79% relative improvement over the strongest baseline,
DeepSeek-R1. Crucially, its generated reasoning also excels in human
evaluations, achieving high-quality ratings for Functionality (82.7%), Security
(85.3%), and Clarity (90.7%).

</details>


### [203] [Securing LLM-Generated Embedded Firmware through AI Agent-Driven Validation and Patching](https://arxiv.org/abs/2509.09970)
*Seyed Moein Abtahi,Akramul Azim*

Main category: cs.CR

TL;DR: A three-phase methodology combining LLM-generated firmware with automated security validation and iterative refinement achieves 92.4% vulnerability remediation and meets real-time performance constraints.


<details>
  <summary>Details</summary>
Motivation: LLMs show promise in generating embedded firmware but often introduce security flaws and fail to meet real-time performance requirements, necessitating a robust validation and refinement approach.

Method: Three-phase approach using structured prompts with GPT-4 to generate firmware, deployed on FreeRTOS via QEMU, then tested with fuzzing, static analysis, and runtime monitoring. AI agents collaborate for threat detection, optimization, and compliance verification in an iterative refinement loop.

Result: 92.4% Vulnerability Remediation Rate (37.3% improvement), 95.8% Threat Model Compliance, 0.87 Security Coverage Index, with real-time performance of 8.6ms worst-case execution time and 195μs jitter.

Conclusion: The methodology successfully enhances firmware security and performance while providing an open-source dataset for future research in secure LLM-generated embedded systems.

Abstract: Large Language Models (LLMs) show promise in generating firmware for embedded
systems, but often introduce security flaws and fail to meet real-time
performance constraints. This paper proposes a three-phase methodology that
combines LLM-based firmware generation with automated security validation and
iterative refinement in a virtualized environment. Using structured prompts,
models like GPT-4 generate firmware for networking and control tasks, deployed
on FreeRTOS via QEMU. These implementations are tested using fuzzing, static
analysis, and runtime monitoring to detect vulnerabilities such as buffer
overflows (CWE-120), race conditions (CWE-362), and denial-of-service threats
(CWE-400). Specialized AI agents for Threat Detection, Performance
Optimization, and Compliance Verification collaborate to improve detection and
remediation. Identified issues are categorized using CWE, then used to prompt
targeted LLM-generated patches in an iterative loop. Experiments show a 92.4\%
Vulnerability Remediation Rate (37.3\% improvement), 95.8\% Threat Model
Compliance, and 0.87 Security Coverage Index. Real-time metrics include 8.6ms
worst-case execution time and 195{\mu}s jitter. This process enhances firmware
security and performance while contributing an open-source dataset for future
research.

</details>


### [204] [Investigating Feature Attribution for 5G Network Intrusion Detection](https://arxiv.org/abs/2509.10206)
*Federica Uccello,Simin Nadjm-Tehrani*

Main category: cs.CR

TL;DR: This paper compares SHAP and VoTE-XAI explainable AI methods for 5G network security, finding VoTE-XAI provides more concise and efficient explanations while maintaining feature coverage.


<details>
  <summary>Details</summary>
Motivation: With 5G networks in critical applications, there's an urgent need to move from detection to reliable mitigation systems. Understanding ML model security alerts through explainable AI is crucial for actionable incident response.

Method: The study extensively analyzes SHAP and VoTE-XAI methods by examining their interpretations of alerts generated by an XGBoost model across three different use cases with several 5G communication attacks. Three metrics were used: sparsity (conciseness), stability (consistency), and efficiency (speed).

Result: VoTE-XAI provided more concise explanations (6 features vs SHAP's 20+ for DoS attacks), was significantly faster (0.002 seconds per explanation in high-dimensional settings), and while feature selection diverged between methods, VoTE-XAI didn't miss any top SHAP features.

Conclusion: Logical explanation approaches like VoTE-XAI show advantages over statistical attribution methods like SHAP in terms of conciseness and efficiency for 5G network security applications, making them more suitable for real-time mitigation systems.

Abstract: With the rise of fifth-generation (5G) networks in critical applications, it
is urgent to move from detection of malicious activity to systems capable of
providing a reliable verdict suitable for mitigation. In this regard,
understanding and interpreting machine learning (ML) models' security alerts is
crucial for enabling actionable incident response orchestration. Explainable
Artificial Intelligence (XAI) techniques are expected to enhance trust by
providing insights into why alerts are raised. A dominant approach
statistically associates feature sets that can be correlated to a given alert.
This paper starts by questioning whether such attribution is relevant for
future generation communication systems, and investigates its merits in
comparison with an approach based on logical explanations. We extensively study
two methods, SHAP and VoTE-XAI, by analyzing their interpretations of alerts
generated by an XGBoost model in three different use cases with several 5G
communication attacks. We identify three metrics for assessing explanations:
sparsity, how concise they are; stability, how consistent they are across
samples from the same attack type; and efficiency, how fast an explanation is
generated. As an example, in a 5G network with 92 features, 6 were deemed
important by VoTE-XAI for a Denial of Service (DoS) variant, ICMPFlood, while
SHAP identified over 20. More importantly, we found a significant divergence
between features selected by SHAP and VoTE-XAI. However, none of the top-ranked
features selected by SHAP were missed by VoTE-XAI. When it comes to efficiency
of providing interpretations, we found that VoTE-XAI is significantly more
responsive, e.g. it provides a single explanation in under 0.002 seconds, in a
high-dimensional setting (478 features).

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [205] [Off Policy Lyapunov Stability in Reinforcement Learning](https://arxiv.org/abs/2509.09863)
*Sarvan Gill,Daniela Constantinescu*

Main category: eess.SY

TL;DR: Off-policy Lyapunov function learning method improves sample efficiency and provides stability guarantees for reinforcement learning algorithms like SAC and PPO.


<details>
  <summary>Details</summary>
Motivation: Traditional RL lacks stability guarantees, and current on-policy Lyapunov function learning methods are sample inefficient.

Method: Introduces off-policy Lyapunov function learning and integrates it into Soft Actor Critic and Proximal Policy Optimization algorithms.

Result: Simulations on inverted pendulum and quadrotor show improved performance with data-efficient stability certificates.

Conclusion: Off-policy Lyapunov functions enable more sample-efficient stable reinforcement learning with proven performance improvements.

Abstract: Traditional reinforcement learning lacks the ability to provide stability
guarantees. More recent algorithms learn Lyapunov functions alongside the
control policies to ensure stable learning. However, the current self-learned
Lyapunov functions are sample inefficient due to their on-policy nature. This
paper introduces a method for learning Lyapunov functions off-policy and
incorporates the proposed off-policy Lyapunov function into the Soft Actor
Critic and Proximal Policy Optimization algorithms to provide them with a data
efficient stability certificate. Simulations of an inverted pendulum and a
quadrotor illustrate the improved performance of the two algorithms when
endowed with the proposed off-policy Lyapunov function.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [206] [Whisper Has an Internal Word Aligner](https://arxiv.org/abs/2509.09987)
*Sung-Lin Yeh,Yen Meng,Hao Tang*

Main category: eess.AS

TL;DR: Unsupervised method to extract accurate word alignments from Whisper using character-level attention heads without additional training, achieving better precision than prior work under strict 20-100ms tolerance.


<details>
  <summary>Details</summary>
Motivation: Increasing need for accurate word-level timestamps from ASR systems like Whisper, with existing approaches requiring training or being insufficiently precise under tight evaluation standards.

Method: Analyze Whisper's attention heads to identify those capturing accurate alignments, use character-level inputs instead of wordpieces, and filter attention heads during teacher forcing to extract word alignments without training.

Result: Produces more accurate word alignments than previous methods under stricter tolerance levels (20-100ms vs 200ms), with finer granularity from character-based approach.

Conclusion: Attention head analysis reveals distinct patterns for alignment capture, character-level processing enables superior precision, and unsupervised filtering approach provides state-of-the-art word alignment extraction from Whisper.

Abstract: There is an increasing interest in obtaining accurate word-level timestamps
from strong automatic speech recognizers, in particular Whisper. Existing
approaches either require additional training or are simply not competitive.
The evaluation in prior work is also relatively loose, typically using a
tolerance of more than 200 ms. In this work, we discover attention heads in
Whisper that capture accurate word alignments and are distinctively different
from those that do not. Moreover, we find that using characters produces finer
and more accurate alignments than using wordpieces. Based on these findings, we
propose an unsupervised approach to extracting word alignments by filtering
attention heads while teacher forcing Whisper with characters. Our approach not
only does not require training but also produces word alignments that are more
accurate than prior work under a stricter tolerance between 20 ms and 100 ms.

</details>


### [207] [Unified Learnable 2D Convolutional Feature Extraction for ASR](https://arxiv.org/abs/2509.10031)
*Peter Vieting,Benedikt Hilmes,Ralf Schlüter,Hermann Ney*

Main category: eess.AS

TL;DR: A generic 2D convolutional front-end for ASR that reduces reliance on classical methods and unifies architecture while maintaining performance with parameter efficiency.


<details>
  <summary>Details</summary>
Motivation: To develop a more generic and unified front-end architecture for ASR feature extraction that moves beyond heavily classical-influenced approaches and avoids complex layer compositions from different sources.

Method: A 2D convolutional neural network front-end that systematically reduces the influence of existing techniques to achieve a parameter-efficient architecture suitable for limited computational resources.

Result: The generic unified approach matches the performance of existing supervised learnable feature extractors while being more parameter-efficient.

Conclusion: A generic 2D convolutional front-end is feasible and effective for ASR, providing comparable performance to specialized feature extractors with better parameter efficiency and suitability for resource-constrained scenarios.

Abstract: Neural front-ends represent a promising approach to feature extraction for
automatic speech recognition (ASR) systems as they enable to learn specifically
tailored features for different tasks. Yet, many of the existing techniques
remain heavily influenced by classical methods. While this inductive bias may
ease the system design, our work aims to develop a more generic front-end for
feature extraction. Furthermore, we seek to unify the front-end architecture
contrasting with existing approaches that apply a composition of several layer
topologies originating from different sources. The experiments systematically
show how to reduce the influence of existing techniques to achieve a generic
front-end. The resulting 2D convolutional front-end is parameter-efficient and
suitable for a scenario with limited computational resources unlike large
models pre-trained on unlabeled audio. The results demonstrate that this
generic unified approach is not only feasible but also matches the performance
of existing supervised learnable feature extractors.

</details>


### [208] [Error Analysis in a Modular Meeting Transcription System](https://arxiv.org/abs/2509.10143)
*Peter Vieting,Simon Berger,Thilo von Neumann,Christoph Boeddeker,Ralf Schlüter,Reinhold Haeb-Umbach*

Main category: eess.AS

TL;DR: Analysis of leakage in speech separation for meeting transcription, showing cross-channel leakage occurs but doesn't significantly impact performance due to VAD filtering. Advanced diarization reduces gap to oracle segmentation by one third compared to simple VAD.


<details>
  <summary>Details</summary>
Motivation: Meeting transcription has seen significant progress but still faces challenges. The paper aims to analyze leakage issues in speech separation systems with proper temporal sensitivity to understand their impact on performance.

Method: Extended a previously proposed framework for analyzing leakage in speech separation with improved temporal locality sensitivity. Compared different segmentation approaches including energy-based VAD and advanced diarization methods against oracle segmentation.

Result: Significant cross-channel leakage occurs when only the primary speaker is active, but this doesn't affect final performance much as VAD largely ignores leaked parts. Advanced diarization reduces the gap to oracle segmentation by one third compared to simple energy-based VAD.

Conclusion: The study provides insights into leakage phenomena in speech separation and demonstrates state-of-the-art performance on LibriCSS among systems trained only on LibriSpeech data, while identifying factors contributing to remaining performance differences.

Abstract: Meeting transcription is a field of high relevance and remarkable progress in
recent years. Still, challenges remain that limit its performance. In this
work, we extend a previously proposed framework for analyzing leakage in speech
separation with proper sensitivity to temporal locality. We show that there is
significant leakage to the cross channel in areas where only the primary
speaker is active. At the same time, the results demonstrate that this does not
affect the final performance much as these leaked parts are largely ignored by
the voice activity detection (VAD). Furthermore, different segmentations are
compared showing that advanced diarization approaches are able to reduce the
gap to oracle segmentation by a third compared to a simple energy-based VAD. We
additionally reveal what factors contribute to the remaining difference. The
results represent state-of-the-art performance on LibriCSS among systems that
train the recognition module on LibriSpeech data only.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [209] [Sparse Polyak: an adaptive step size rule for high-dimensional M-estimation](https://arxiv.org/abs/2509.09802)
*Tianqi Qiao,Marie Maros*

Main category: math.OC

TL;DR: Sparse Polyak is a modified adaptive step size method that addresses poor performance of standard Polyak step size in high-dimensional statistical estimation problems where dimension grows faster than sample size.


<details>
  <summary>Details</summary>
Motivation: Standard Polyak step size performs poorly in high-dimensional settings, requiring increasing iterations even when problems remain well-conditioned, due to ineffective Lipschitz smoothness estimation in high dimensions.

Method: Sparse Polyak modifies the step size to estimate restricted Lipschitz smoothness constant (smoothness in problem-relevant directions) instead of global Lipschitz constant.

Result: The approach shows improved performance in high-dimensional statistical estimation problems, supported by both theoretical analysis and numerical experiments.

Conclusion: Sparse Polyak effectively overcomes limitations of standard Polyak step size in high-dimensional settings by using restricted smoothness estimation, providing better convergence and statistical precision.

Abstract: We propose and study Sparse Polyak, a variant of Polyak's adaptive step size,
designed to solve high-dimensional statistical estimation problems where the
problem dimension is allowed to grow much faster than the sample size. In such
settings, the standard Polyak step size performs poorly, requiring an
increasing number of iterations to achieve optimal statistical precision-even
when, the problem remains well conditioned and/or the achievable precision
itself does not degrade with problem size. We trace this limitation to a
mismatch in how smoothness is measured: in high dimensions, it is no longer
effective to estimate the Lipschitz smoothness constant. Instead, it is more
appropriate to estimate the smoothness restricted to specific directions
relevant to the problem (restricted Lipschitz smoothness constant). Sparse
Polyak overcomes this issue by modifying the step size to estimate the
restricted Lipschitz smoothness constant. We support our approach with both
theoretical analysis and numerical experiments, demonstrating its improved
performance.

</details>


<div id='q-bio.GN'></div>

# q-bio.GN [[Back]](#toc)

### [210] [Engineering Spatial and Molecular Features from Cellular Niches to Inform Predictions of Inflammatory Bowel Disease](https://arxiv.org/abs/2509.09923)
*Myles Joshua Toledo Tan,Maria Kapetanaki,Panayiotis V. Benos*

Main category: q-bio.GN

TL;DR: Novel computational framework using spatial transcriptomics and explainable ML to differentiate IBD subtypes (Crohn's vs ulcerative colitis) with high accuracy and biological insights.


<details>
  <summary>Details</summary>
Motivation: Differentiating between Crohn's disease and ulcerative colitis is clinically challenging due to overlapping presentations, requiring better diagnostic tools.

Method: Used spatial transcriptomics data from colonic mucosa, applied Non-negative Matrix Factorization to identify cellular niches, engineered 44 features, and trained multilayer perceptron classifier.

Result: Achieved 0.774 accuracy for three-class classification (HC/UC/CD) and 0.916 accuracy for IBD vs healthy tissue classification.

Conclusion: Framework transforms spatial data into accurate diagnostic tool, revealing spatial organization disruptions predict inflammation while niche-gene signatures differentiate IBD subtypes.

Abstract: Differentiating between the two main subtypes of Inflammatory Bowel Disease
(IBD): Crohns disease (CD) and ulcerative colitis (UC) is a persistent clinical
challenge due to overlapping presentations. This study introduces a novel
computational framework that employs spatial transcriptomics (ST) to create an
explainable machine learning model for IBD classification. We analyzed ST data
from the colonic mucosa of healthy controls (HC), UC, and CD patients. Using
Non-negative Matrix Factorization (NMF), we first identified four recurring
cellular niches, representing distinct functional microenvironments within the
tissue. From these niches, we systematically engineered 44 features capturing
three key aspects of tissue pathology: niche composition, neighborhood
enrichment, and niche-gene signals. A multilayer perceptron (MLP) classifier
trained on these features achieved an accuracy of 0.774 +/- 0.161 for the more
challenging three-class problem (HC, UC, and CD) and 0.916 +/- 0.118 in the
two-class problem of distinguishing IBD from healthy tissue. Crucially, model
explainability analysis revealed that disruptions in the spatial organization
of niches were the strongest predictors of general inflammation, while the
classification between UC and CD relied on specific niche-gene expression
signatures. This work provides a robust, proof-of-concept pipeline that
transforms descriptive spatial data into an accurate and explainable predictive
tool, offering not only a potential new diagnostic paradigm but also deeper
insights into the distinct biological mechanisms that drive IBD subtypes.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [211] [LLM-Based Instance-Driven Heuristic Bias In the Context of a Biased Random Key Genetic Algorithm](https://arxiv.org/abs/2509.09707)
*Camilo Chacón Sartori,Martín Isla Pino,Pedro Pinacho-Davidson,Christian Blum*

Main category: cs.NE

TL;DR: LLM-BRKGA hybrid framework for Longest Run Subsequence problem that uses human-LLM collaboration to generate instance-specific heuristic biases, outperforming standard BRKGA on complex instances.


<details>
  <summary>Details</summary>
Motivation: Existing LLM approaches for optimization often overlook structural properties of individual problem instances, focusing mainly on code generation rather than instance-driven heuristic design.

Method: Human-LLM collaborative process to co-design computationally efficient metrics, with LLM analyzing these metrics to generate tailored heuristic bias that steers Biased Random-Key Genetic Algorithm search.

Result: BRKGA+Llama-4-Maverick achieved statistically significant improvements over baseline BRKGA across 1,050 instances, particularly on the most complex problem instances.

Conclusion: Leveraging LLMs to produce instance-driven heuristic bias a priori is a valuable approach for enhancing metaheuristics in complex optimization domains.

Abstract: Integrating Large Language Models (LLMs) within metaheuristics opens a novel
path for solving complex combinatorial optimization problems. While most
existing approaches leverage LLMs for code generation to create or refine
specific heuristics, they often overlook the structural properties of
individual problem instances. In this work, we introduce a novel framework that
integrates LLMs with a Biased Random-Key Genetic Algorithm (BRKGA) to solve the
NP-hard Longest Run Subsequence problem. Our approach extends the
instance-driven heuristic bias paradigm by introducing a human-LLM
collaborative process to co-design and implement a set of computationally
efficient metrics. The LLM analyzes these instance-specific metrics to generate
a tailored heuristic bias, which steers the BRKGA toward promising areas of the
search space. We conduct a comprehensive experimental evaluation, including
rigorous statistical tests, convergence and behavioral analyses, and targeted
ablation studies, comparing our method against a standard BRKGA baseline across
1,050 generated instances of varying complexity. Results show that our
top-performing hybrid, BRKGA+Llama-4-Maverick, achieves statistically
significant improvements over the baseline, particularly on the most complex
instances. Our findings confirm that leveraging an LLM to produce an a priori,
instance-driven heuristic bias is a valuable approach for enhancing
metaheuristics in complex optimization domains.

</details>


### [212] [Predictive Spike Timing Enables Distributed Shortest Path Computation in Spiking Neural Networks](https://arxiv.org/abs/2509.10077)
*Simen Storesund,Kristian Valset Aars,Robin Dietrich,Nicolai Waniek*

Main category: cs.NE

TL;DR: A biologically plausible spike-based algorithm for shortest-path computation using local timing dynamics instead of global state or backtracing.


<details>
  <summary>Details</summary>
Motivation: Current planning algorithms (Dijkstra's, A*, RL) are biologically implausible - they require global state, backtracing, or slow gradient updates, unlike rapid behavioral adaptation in biological systems.

Method: Local spike-based message-passing with processing delays, using spike-timing coincidences and inhibitory-excitatory pairs to identify optimal paths through temporal compression that propagates backwards.

Result: The algorithm converges and discovers all shortest paths on random spatial networks using purely timing-based mechanisms, as proven analytically and through simulations.

Conclusion: Short-term timing dynamics alone can compute shortest paths, providing insights into how biological networks solve complex problems through local computation and relative spike-time prediction, with implications for neuroscience, AI, and neuromorphic systems.

Abstract: Efficient planning and sequence selection are central to intelligence, yet
current approaches remain largely incompatible with biological computation.
Classical graph algorithms like Dijkstra's or A* require global state and
biologically implausible operations such as backtracing, while reinforcement
learning methods rely on slow gradient-based policy updates that appear
inconsistent with rapid behavioral adaptation observed in natural systems.
  We propose a biologically plausible algorithm for shortest-path computation
that operates through local spike-based message-passing with realistic
processing delays. The algorithm exploits spike-timing coincidences to identify
nodes on optimal paths: Neurons that receive inhibitory-excitatory message
pairs earlier than predicted reduce their response delays, creating a temporal
compression that propagates backwards from target to source. Through analytical
proof and simulations on random spatial networks, we demonstrate that the
algorithm converges and discovers all shortest paths using purely timing-based
mechanisms. By showing how short-term timing dynamics alone can compute
shortest paths, this work provides new insights into how biological networks
might solve complex computational problems through purely local computation and
relative spike-time prediction. These findings open new directions for
understanding distributed computation in biological and artificial systems,
with possible implications for computational neuroscience, AI, reinforcement
learning, and neuromorphic systems.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [213] [HypoGeneAgent: A Hypothesis Language Agent for Gene-Set Cluster Resolution Selection Using Perturb-seq Datasets](https://arxiv.org/abs/2509.09740)
*Ying Yuan,Xing-Yue Monica Ge,Aaron Archer Waterman,Tommaso Biancalani,David Richmond,Yogesh Pandit,Avtar Singh,Russell Littman,Jin Liu,Jan-Christian Huetter,Vladimir Ermakov*

Main category: q-bio.QM

TL;DR: HYPOGENEAGENT is an LLM-driven framework that transforms subjective cluster annotation into an optimizable task using GO-based hypotheses with confidence scores, intra-cluster agreement, and inter-cluster separation metrics.


<details>
  <summary>Details</summary>
Motivation: Current clustering and annotation methods in single-cell studies rely on subjective heuristics and expert curation, lacking objective optimization for resolution selection and functional annotation.

Method: Uses LLM as gene-set analyst to generate ranked GO hypotheses with confidence scores, then computes intra-cluster agreement and inter-cluster separation via sentence embeddings to create an agent-derived resolution score.

Result: Applied to K562 CRISPRi Perturb-seq data, the Resolution Score selected clustering granularities that better aligned with known pathways compared to traditional metrics like silhouette score and modularity.

Conclusion: LLM agents can serve as objective adjudicators for cluster resolution and functional annotation, enabling fully automated interpretation pipelines in single-cell multi-omics studies.

Abstract: Large-scale single-cell and Perturb-seq investigations routinely involve
clustering cells and subsequently annotating each cluster with Gene-Ontology
(GO) terms to elucidate the underlying biological programs. However, both
stages, resolution selection and functional annotation, are inherently
subjective, relying on heuristics and expert curation. We present
HYPOGENEAGENT, a large language model (LLM)-driven framework, transforming
cluster annotation into a quantitatively optimizable task. Initially, an LLM
functioning as a gene-set analyst analyzes the content of each gene program or
perturbation module and generates a ranked list of GO-based hypotheses,
accompanied by calibrated confidence scores. Subsequently, we embed every
predicted description with a sentence-embedding model, compute pair-wise cosine
similarities, and let the agent referee panel score (i) the internal
consistency of the predictions, high average similarity within the same
cluster, termed intra-cluster agreement (ii) their external distinctiveness,
low similarity between clusters, termed inter-cluster separation. These two
quantities are combined to produce an agent-derived resolution score, which is
maximized when clusters exhibit simultaneous coherence and mutual exclusivity.
When applied to a public K562 CRISPRi Perturb-seq dataset as a preliminary
test, our Resolution Score selects clustering granularities that exhibit
alignment with known pathway compared to classical metrics such silhouette
score, modularity score for gene functional enrichment summary. These findings
establish LLM agents as objective adjudicators of cluster resolution and
functional annotation, thereby paving the way for fully automated,
context-aware interpretation pipelines in single-cell multi-omics studies.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [214] [An Information-Theoretic Framework for Credit Risk Modeling: Unifying Industry Practice with Statistical Theory for Fair and Interpretable Scorecards](https://arxiv.org/abs/2509.09855)
*Agus Sudjianto,Denis Burakov*

Main category: stat.ML

TL;DR: Unified information-theoretic framework connects WoE, IV, and PSI metrics, showing IV equals PSI between good/bad outcomes. Enables hypothesis testing and fairness constraints for credit risk modeling.


<details>
  <summary>Details</summary>
Motivation: Industry-standard credit risk metrics (WoE, IV, PSI) lack theoretical foundation and statistical rigor, limiting formal testing and fairness considerations.

Method: Established information-theoretic framework proving IV equals PSI, derived standard errors via delta method, compared encoding strategies with automated binning using XGBoost stumps, and used mixed-integer programming for Pareto optimization.

Result: All methods achieved comparable predictive performance (AUC 0.82-0.84), demonstrating information-theoretic binning outweighs encoding choice. Enabled formal hypothesis testing and probabilistic fairness constraints.

Conclusion: Provides first rigorous statistical foundation for credit risk metrics, bridges theory and practice, and offers principled tools for balancing accuracy and fairness in regulated environments.

Abstract: Credit risk modeling relies extensively on Weight of Evidence (WoE) and
Information Value (IV) for feature engineering, and Population Stability Index
(PSI) for drift monitoring, yet their theoretical foundations remain
disconnected. We establish a unified information-theoretic framework revealing
these industry-standard metrics as instances of classical information
divergences. Specifically, we prove that IV exactly equals PSI (Jeffreys
divergence) computed between good and bad credit outcomes over identical bins.
Through the delta method applied to WoE transformations, we derive standard
errors for IV and PSI, enabling formal hypothesis testing and probabilistic
fairness constraints for the first time. We formalize credit modeling's
inherent performance-fairness trade-off as maximizing IV for predictive power
while minimizing IV for protected attributes. Using automated binning with
depth-1 XGBoost stumps, we compare three encoding strategies: logistic
regression with one-hot encoding, WoE transformation, and constrained XGBoost.
All methods achieve comparable predictive performance (AUC 0.82-0.84),
demonstrating that principled, information-theoretic binning outweighs encoding
choice. Mixed-integer programming traces Pareto-efficient solutions along the
performance-fairness frontier with uncertainty quantification. This framework
bridges theory and practice, providing the first rigorous statistical
foundation for widely-used credit risk metrics while offering principled tools
for balancing accuracy and fairness in regulated environments.

</details>


### [215] [Repulsive Monte Carlo on the sphere for the sliced Wasserstein distance](https://arxiv.org/abs/2509.10166)
*Vladimir Petrovic,Rémi Bardenet,Agnès Desolneux*

Main category: stat.ML

TL;DR: This paper analyzes Monte Carlo methods for computing integrals on the unit sphere, focusing on the sliced Wasserstein distance. It benchmarks repulsive quadrature methods and recommends randomized quasi-Monte Carlo for low dimensions and UnifOrtho for high dimensions.


<details>
  <summary>Details</summary>
Motivation: The sliced Wasserstein distance has gained importance in machine learning as a computationally efficient alternative to the Wasserstein distance, but existing quadrature methods need improvement, particularly through variance reduction using negatively dependent (repulsive) nodes.

Method: The authors extract and motivate quadratures from determinantal point processes and repelled point processes literature, then numerically benchmark these methods. They also analyze the variance of the UnifOrtho estimator to understand its performance.

Result: Randomized quasi-Monte Carlo performs best in low dimensions, while UnifOrtho excels in large dimensions. DPP-based quadratures only work well when quasi-Monte Carlo also performs well, and repelled quadratures show moderate variance reduction but need more theoretical development.

Conclusion: For computing sliced Wasserstein distance, use randomized quasi-Monte Carlo in low dimensions and UnifOrtho in large dimensions. Repulsive quadratures show promise but require further theoretical work to become robust.

Abstract: In this paper, we consider the problem of computing the integral of a
function on the unit sphere, in any dimension, using Monte Carlo methods.
Although the methods we present are general, our guiding thread is the sliced
Wasserstein distance between two measures on $\mathbb{R}^d$, which is precisely
an integral on the $d$-dimensional sphere. The sliced Wasserstein distance (SW)
has gained momentum in machine learning either as a proxy to the less
computationally tractable Wasserstein distance, or as a distance in its own
right, due in particular to its built-in alleviation of the curse of
dimensionality. There has been recent numerical benchmarks of quadratures for
the sliced Wasserstein, and our viewpoint differs in that we concentrate on
quadratures where the nodes are repulsive, i.e. negatively dependent. Indeed,
negative dependence can bring variance reduction when the quadrature is adapted
to the integration task. Our first contribution is to extract and motivate
quadratures from the recent literature on determinantal point processes (DPPs)
and repelled point processes, as well as repulsive quadratures from the
literature specific to the sliced Wasserstein distance. We then numerically
benchmark these quadratures. Moreover, we analyze the variance of the UnifOrtho
estimator, an orthogonal Monte Carlo estimator. Our analysis sheds light on
UnifOrtho's success for the estimation of the sliced Wasserstein in large
dimensions, as well as counterexamples from the literature. Our final
recommendation for the computation of the sliced Wasserstein distance is to use
randomized quasi-Monte Carlo in low dimensions and \emph{UnifOrtho} in large
dimensions. DPP-based quadratures only shine when quasi-Monte Carlo also does,
while repelled quadratures show moderate variance reduction in general, but
more theoretical effort is needed to make them robust.

</details>


### [216] [Why does your graph neural network fail on some graphs? Insights from exact generalisation error](https://arxiv.org/abs/2509.10337)
*Nil Ayday,Mahalakshmi Sabanayagam,Debarghya Ghoshdastidar*

Main category: stat.ML

TL;DR: Exact generalisation error analysis for GNNs reveals that only aligned information between node features and graph structure contributes to generalisation, with homophily effects quantified.


<details>
  <summary>Details</summary>
Motivation: Current understanding of why GNNs succeed or fail is limited, with existing generalisation bounds being loose and architecture-specific, lacking practical insights.

Method: Derived exact generalisation error for GNNs in transductive fixed-design setting using signal processing lens, interpreting GNNs as graph filter operators on node features.

Result: Found that only aligned information between node features and graph structure contributes to generalisation, and quantified homophily effects on performance.

Conclusion: Provides a framework explaining when and why GNNs effectively leverage structural and feature information, offering practical model selection guidance.

Abstract: Graph Neural Networks (GNNs) are widely used in learning on graph-structured
data, yet a principled understanding of why they succeed or fail remains
elusive. While prior works have examined architectural limitations such as
over-smoothing and over-squashing, these do not explain what enables GNNs to
extract meaningful representations or why performance varies drastically
between similar architectures. These questions are related to the role of
generalisation: the ability of a model to make accurate predictions on
unlabelled data. Although several works have derived generalisation error
bounds for GNNs, these are typically loose, restricted to a single
architecture, and offer limited insight into what governs generalisation in
practice. In this work, we take a different approach by deriving the exact
generalisation error for GNNs in a transductive fixed-design setting through
the lens of signal processing. From this viewpoint, GNNs can be interpreted as
graph filter operators that act on node features via the graph structure. By
focusing on linear GNNs while allowing non-linearity in the graph filters, we
derive the first exact generalisation error for a broad range of GNNs,
including convolutional, PageRank-based, and attention-based models. The exact
characterisation of the generalisation error reveals that only the aligned
information between node features and graph structure contributes to
generalisation. Furthermore, we quantify the effect of homophily on
generalisation. Our work provides a framework that explains when and why GNNs
can effectively leverage structural and feature information, offering practical
guidance for model selection.

</details>


### [217] [Differentially Private Decentralized Dataset Synthesis Through Randomized Mixing with Correlated Noise](https://arxiv.org/abs/2509.10385)
*Utsab Saha,Tanvir Muntakim Tonoy,Hafiz Imtiaz*

Main category: stat.ML

TL;DR: Proposes CAPE Assisted Federated DP-CDA algorithm that integrates correlation-assisted noise cancellation to improve privacy-utility trade-off in decentralized differentially private synthetic data generation.


<details>
  <summary>Details</summary>
Motivation: DP-CDA faces challenges in federated settings due to limited sample size per client increasing sensitivity and requiring higher noise injection, leading to utility degradation compared to centralized settings.

Method: Integrates CAPE protocol into federated DP-CDA framework, allowing clients to generate jointly distributed anti-correlated noise that cancels out in aggregate while preserving individual privacy.

Result: Extensive experiments on MNIST and FashionMNIST show the approach achieves utility comparable to centralized counterpart under certain parameter regimes while maintaining rigorous differential privacy.

Conclusion: CAPE-assisted approach significantly improves privacy-utility trade-off in federated settings, enabling decentralized synthetic data generation with utility close to centralized methods.

Abstract: In this work, we explore differentially private synthetic data generation in
a decentralized-data setting by building on the recently proposed
Differentially Private Class-Centric Data Aggregation (DP-CDA). DP-CDA
synthesizes data in a centralized setting by mixing multiple randomly-selected
samples from the same class and injecting carefully calibrated Gaussian noise,
ensuring ({\epsilon}, {\delta})-differential privacy. When deployed in a
decentralized or federated setting, where each client holds only a small
partition of the data, DP-CDA faces new challenges. The limited sample size per
client increases the sensitivity of local computations, requiring higher noise
injection to maintain the differential privacy guarantee. This, in turn, leads
to a noticeable degradation in the utility compared to the centralized setting.
To mitigate this issue, we integrate the Correlation-Assisted Private
Estimation (CAPE) protocol into the federated DP-CDA framework and propose CAPE
Assisted Federated DP-CDA algorithm. CAPE enables limited collaboration among
the clients by allowing them to generate jointly distributed (anti-correlated)
noise that cancels out in aggregate, while preserving privacy at the individual
level. This technique significantly improves the privacy-utility trade-off in
the federated setting. Extensive experiments on MNIST and FashionMNIST datasets
demonstrate that the proposed CAPE Assisted Federated DP-CDA approach can
achieve utility comparable to its centralized counterpart under some parameter
regime, while maintaining rigorous differential privacy guarantees.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [218] [Characterizing the Efficiency of Distributed Training: A Power, Performance, and Thermal Perspective](https://arxiv.org/abs/2509.10371)
*Seokjin Go,Joongun Park,Spandan More,Hanjiang Wu,Irene Wang,Aaron Jezghani,Tushar Krishna,Divya Mahajan*

Main category: cs.DC

TL;DR: Comprehensive analysis of LLM training performance across different hardware platforms and parallelism strategies, revealing that performance depends on complex hardware-system-model interactions rather than just hardware scaling.


<details>
  <summary>Details</summary>
Motivation: As LLM training workloads exceed single-node limits, there's a need to understand how these models behave across large-scale multi-GPU systems to optimize performance and efficiency.

Method: Characterized LLM training across diverse real-world workloads on NVIDIA H100/H200 and AMD MI250 GPUs, analyzing dense/sparse models under various parallelism strategies (tensor, pipeline, data, expert) and evaluating optimizations like activation recomputation and compute-communication overlap.

Result: Scale-up systems with fewer high-memory GPUs outperform scale-out in communication-bound regimes with careful tuning; certain parallelism combinations cause bandwidth underutilization; large microbatch sizes cause bursty execution and thermal throttling issues.

Conclusion: Training performance is shaped by complex hardware-system-model interactions, and the paper provides recommendations for system/hardware design to improve scalability and reliability of future LLM systems.

Abstract: The rapid scaling of Large Language Models (LLMs) has pushed training
workloads far beyond the limits of single-node analysis, demanding a deeper
understanding of how these models behave across large-scale, multi-GPU systems.
In this paper, we present a comprehensive characterization of LLM training
across diverse real-world workloads and hardware platforms, including NVIDIA
H100/H200 and AMD MI250 GPUs. We analyze dense and sparse models under various
parallelism strategies -- tensor, pipeline, data, and expert -- and evaluate
their effects on hardware utilization, power consumption, and thermal behavior.
We further evaluate the effectiveness of optimizations such as activation
recomputation and compute-communication overlap. Our findings show that
performance is not determined solely by scaling hardware capacity. Scale-up
systems with fewer, higher-memory GPUs can outperform scale-out systems in
communication-bound regimes, but only under carefully tuned configurations; in
other cases, scale-out deployments achieve superior throughput. We also show
that certain parallelism combinations, such as tensor with pipeline, lead to
bandwidth underutilization due to inefficient data chunking, while increasing
microbatch sizes beyond a certain point induces bursty execution and peak power
excursions that worsen thermal throttling. These insights reveal how training
performance is shaped by complex interactions between hardware, system
topology, and model execution. We conclude by offering recommendations for
system and hardware design to improve the scalability and reliability of future
LLM systems and workloads. The source code of this project is available at
https://github.com/sitar-lab/CharLLM-PPT.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [219] [DCHO: A Decomposition-Composition Framework for Predicting Higher-Order Brain Connectivity to Enhance Diverse Downstream Applications](https://arxiv.org/abs/2509.09696)
*Weibin Li,Wendu Li,Quanying Liu*

Main category: q-bio.NC

TL;DR: DCHO is a unified framework for modeling and forecasting higher-order brain connectivity dynamics using a decomposition-composition approach that outperforms existing methods in both classification and prediction tasks.


<details>
  <summary>Details</summary>
Motivation: Higher-order brain connectivity provides richer organizational information than pairwise connectivity, but existing methods mainly focus on static analysis, limiting their applicability in dynamic prediction tasks.

Method: DCHO uses a decomposition-composition framework with a dual-view encoder for multiscale topological feature extraction, a latent combinatorial learner for HOBC inference, and a latent-space prediction loss for temporal trajectory modeling.

Result: Extensive experiments on multiple neuroimaging datasets show DCHO achieves superior performance in both state classification and brain dynamics forecasting tasks, significantly outperforming existing methods.

Conclusion: The proposed DCHO framework successfully addresses the gap in dynamic higher-order brain connectivity analysis and demonstrates strong performance in both predictive and non-predictive tasks across multiple datasets.

Abstract: Higher-order brain connectivity (HOBC), which captures interactions among
three or more brain regions, provides richer organizational information than
traditional pairwise functional connectivity (FC). Recent studies have begun to
infer latent HOBC from noninvasive imaging data, but they mainly focus on
static analyses, limiting their applicability in dynamic prediction tasks. To
address this gap, we propose DCHO, a unified approach for modeling and
forecasting the temporal evolution of HOBC based on a Decomposition-Composition
framework, which is applicable to both non-predictive tasks (state
classification) and predictive tasks (brain dynamics forecasting). DCHO adopts
a decomposition-composition strategy that reformulates the prediction task into
two manageable subproblems: HOBC inference and latent trajectory prediction. In
the inference stage, we propose a dual-view encoder to extract multiscale
topological features and a latent combinatorial learner to capture high-level
HOBC information. In the forecasting stage, we introduce a latent-space
prediction loss to enhance the modeling of temporal trajectories. Extensive
experiments on multiple neuroimaging datasets demonstrate that DCHO achieves
superior performance in both non-predictive tasks (state classification) and
predictive tasks (brain dynamics forecasting), significantly outperforming
existing methods.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [220] [Vibe Check: Understanding the Effects of LLM-Based Conversational Agents' Personality and Alignment on User Perceptions in Goal-Oriented Tasks](https://arxiv.org/abs/2509.09870)
*Hasibur Rahman,Smit Desai*

Main category: cs.HC

TL;DR: Medium personality expression in conversational agents produces optimal user perceptions, with personality alignment further enhancing outcomes - Extraversion and Emotional Stability are most influential traits.


<details>
  <summary>Details</summary>
Motivation: As LLMs enable conversational agents to express distinctive personalities, this study investigates how personality expression levels and user-agent personality alignment influence user perceptions in goal-oriented tasks.

Method: Between-subjects experiment (N=150) where participants completed travel planning tasks with CAs exhibiting low, medium, or high personality expression across Big Five traits, controlled via novel Trait Modulation Keys framework.

Result: Inverted-U relationship found: medium expression produced most positive evaluations across Intelligence, Enjoyment, Anthropomorphism, Intention to Adopt, Trust, and Likeability. Personality alignment further enhanced outcomes, with Extraversion and Emotional Stability as most influential traits. Three distinct compatibility profiles identified.

Conclusion: Personality expression and strategic trait alignment constitute optimal design targets for CA personality, offering important design implications as LLM-based conversational agents become increasingly prevalent.

Abstract: Large language models (LLMs) enable conversational agents (CAs) to express
distinctive personalities, raising new questions about how such designs shape
user perceptions. This study investigates how personality expression levels and
user-agent personality alignment influence perceptions in goal-oriented tasks.
In a between-subjects experiment (N=150), participants completed travel
planning with CAs exhibiting low, medium, or high expression across the Big
Five traits, controlled via our novel Trait Modulation Keys framework. Results
revealed an inverted-U relationship: medium expression produced the most
positive evaluations across Intelligence, Enjoyment, Anthropomorphism,
Intention to Adopt, Trust, and Likeability, significantly outperforming both
extremes. Personality alignment further enhanced outcomes, with Extraversion
and Emotional Stability emerging as the most influential traits. Cluster
analysis identified three distinct compatibility profiles, with "Well-Aligned"
users reporting substantially positive perceptions. These findings demonstrate
that personality expression and strategic trait alignment constitute optimal
design targets for CA personality, offering design implications as LLM-based
CAs become increasingly prevalent.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [221] [Chord: Chain of Rendering Decomposition for PBR Material Estimation from Generated Texture Images](https://arxiv.org/abs/2509.09952)
*Zhi Ying,Boxiang Rong,Jingyu Wang,Maoyuan Xu*

Main category: cs.GR

TL;DR: A two-stage generate-and-estimate framework for high-quality PBR material generation that uses fine-tuned diffusion models and chained decomposition for superior performance and user control.


<details>
  <summary>Details</summary>
Motivation: Traditional material creation requires significant artist expertise, and recent methods using visual foundation models lack quality, flexibility, and user control for PBR material synthesis.

Method: Two-stage framework: 1) Generation stage with fine-tuned diffusion model synthesizes shaded, tileable textures from user input; 2) Estimation stage with chained decomposition scheme sequentially predicts SVBRDF channels using single-step image-conditional diffusion model.

Result: Superior performance compared to existing methods, strong robustness on both generated textures and real photographs, and flexible user control across diverse applications.

Conclusion: The proposed framework provides efficient, high-quality PBR material generation with enhanced flexibility and user control, enabling various applications including text-to-material, image-to-material, structure-guided generation, and material editing.

Abstract: Material creation and reconstruction are crucial for appearance modeling but
traditionally require significant time and expertise from artists. While recent
methods leverage visual foundation models to synthesize PBR materials from
user-provided inputs, they often fall short in quality, flexibility, and user
control. We propose a novel two-stage generate-and-estimate framework for PBR
material generation. In the generation stage, a fine-tuned diffusion model
synthesizes shaded, tileable texture images aligned with user input. In the
estimation stage, we introduce a chained decomposition scheme that sequentially
predicts SVBRDF channels by passing previously extracted representation as
input into a single-step image-conditional diffusion model. Our method is
efficient, high quality, and enables flexible user control. We evaluate our
approach against existing material generation and estimation methods,
demonstrating superior performance. Our material estimation method shows strong
robustness on both generated textures and in-the-wild photographs. Furthermore,
we highlight the flexibility of our framework across diverse applications,
including text-to-material, image-to-material, structure-guided generation, and
material editing.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [222] [Generative Engine Optimization: How to Dominate AI Search](https://arxiv.org/abs/2509.08919)
*Mahe Chen,Xiaoxuan Wang,Kaiwen Chen,Nick Koudas*

Main category: cs.IR

TL;DR: AI-powered search engines (ChatGPT, Perplexity, Gemini) show systematic bias towards authoritative third-party sources, requiring new Generative Engine Optimization (GEO) strategies different from traditional SEO.


<details>
  <summary>Details</summary>
Motivation: The shift from traditional ranked-list search to AI-generated answers with citations challenges existing SEO practices and necessitates a new optimization paradigm for generative search engines.

Method: Conducted large-scale controlled experiments across multiple verticals, languages, and query paraphrases to compare AI Search and traditional Google search, analyzing information sourcing patterns and biases.

Result: AI Search exhibits overwhelming bias towards Earned media (authoritative third-party sources) over Brand-owned and Social content, with significant differences in domain diversity, freshness, cross-language stability, and phrasing sensitivity among AI services.

Conclusion: Proposes Generative Engine Optimization (GEO) framework with actionable strategies including content engineering for machine scannability, earned media dominance, engine-specific approaches, and overcoming big brand bias for niche players in the generative search landscape.

Abstract: The rapid adoption of generative AI-powered search engines like ChatGPT,
Perplexity, and Gemini is fundamentally reshaping information retrieval, moving
from traditional ranked lists to synthesized, citation-backed answers. This
shift challenges established Search Engine Optimization (SEO) practices and
necessitates a new paradigm, which we term Generative Engine Optimization
(GEO).
  This paper presents a comprehensive comparative analysis of AI Search and
traditional web search (Google). Through a series of large-scale, controlled
experiments across multiple verticals, languages, and query paraphrases, we
quantify critical differences in how these systems source information. Our key
findings reveal that AI Search exhibit a systematic and overwhelming bias
towards Earned media (third-party, authoritative sources) over Brand-owned and
Social content, a stark contrast to Google's more balanced mix. We further
demonstrate that AI Search services differ significantly from each other in
their domain diversity, freshness, cross-language stability, and sensitivity to
phrasing.
  Based on these empirical results, we formulate a strategic GEO agenda. We
provide actionable guidance for practitioners, emphasizing the critical need
to: (1) engineer content for machine scannability and justification, (2)
dominate earned media to build AI-perceived authority, (3) adopt
engine-specific and language-aware strategies, and (4) overcome the inherent
"big brand bias" for niche players. Our work provides the foundational
empirical analysis and a strategic framework for achieving visibility in the
new generative search landscape.

</details>


### [223] [DB3 Team's Solution For Meta KDD Cup' 25](https://arxiv.org/abs/2509.09681)
*Yikuan Xia,Jiazun Chen,Yirui Zhan,Suifeng Zhao,Weipeng Jiang,Chaorui Zhang,Wei Han,Bo Bai,Jun Gao*

Main category: cs.IR

TL;DR: Winning solution for Meta CRAG-MM Challenge 2025 featuring multi-modal retrieval pipelines and advanced LLM-tuning for hallucination control, achieving top rankings across all tasks.


<details>
  <summary>Details</summary>
Motivation: To address the unique challenges of multi-modal, multi-turn question answering in the CRAG-MM benchmark, particularly handling image-indexed knowledge graphs, web sources, and ego-centric queries.

Method: Developed a comprehensive framework with domain-specific retrieval pipelines for different tasks and unified LLM-tuning approach using SFT, DPO, and RL for advanced refusal training and hallucination control.

Result: Achieved 2nd place in Task 1, 2nd place in Task 2, and 1st place in Task 3, securing the grand prize for excellence in ego-centric queries through superior handling of first-person perspective challenges.

Conclusion: The integrated approach of tailored retrieval pipelines with advanced LLM-tuning techniques proved highly effective for multi-modal, multi-turn QA, particularly excelling in ego-centric query handling.

Abstract: This paper presents the db3 team's winning solution for the Meta CRAG-MM
Challenge 2025 at KDD Cup'25. Addressing the challenge's unique multi-modal,
multi-turn question answering benchmark (CRAG-MM), we developed a comprehensive
framework that integrates tailored retrieval pipelines for different tasks with
a unified LLM-tuning approach for hallucination control. Our solution features
(1) domain-specific retrieval pipelines handling image-indexed knowledge
graphs, web sources, and multi-turn conversations; and (2) advanced refusal
training using SFT, DPO, and RL. The system achieved 2nd place in Task 1, 2nd
place in Task 2, and 1st place in Task 3, securing the grand prize for
excellence in ego-centric queries through superior handling of first-person
perspective challenges.

</details>


### [224] [Forecasting Clicks in Digital Advertising: Multimodal Inputs and Interpretable Outputs](https://arxiv.org/abs/2509.09683)
*Briti Gangopadhyay,Zhao Wang,Shingo Takamatsu*

Main category: cs.IR

TL;DR: Multimodal framework combining click data and textual logs for ad campaign forecasting, using reinforcement learning to improve text comprehension and modality fusion, outperforming baselines in accuracy and reasoning quality.


<details>
  <summary>Details</summary>
Motivation: Traditional time series models overlook contextual information from textual elements like keyword updates, limiting forecasting performance in digital advertising.

Method: Multimodal forecasting framework that integrates click data with textual logs, using reinforcement learning to enhance text comprehension and modality fusion, while generating human-interpretable explanations.

Result: Outperforms baseline methods on large-scale industry dataset in both forecasting accuracy and reasoning quality.

Conclusion: The proposed multimodal approach effectively leverages textual context alongside numerical data for improved click volume forecasting with interpretable results.

Abstract: Forecasting click volume is a key task in digital advertising, influencing
both revenue and campaign strategy. Traditional time series models rely solely
on numerical data, often overlooking rich contextual information embedded in
textual elements, such as keyword updates. We present a multimodal forecasting
framework that combines click data with textual logs from real-world ad
campaigns and generates human-interpretable explanations alongside numeric
predictions. Reinforcement learning is used to improve comprehension of textual
information and enhance fusion of modalities. Experiments on a large-scale
industry dataset show that our method outperforms baselines in both accuracy
and reasoning quality.

</details>


### [225] [Text-to-SQL Oriented to the Process Mining Domain: A PT-EN Dataset for Query Translation](https://arxiv.org/abs/2509.09684)
*Bruno Yui Yamate,Thais Rodrigues Neubauer,Marcelo Fantinato,Sarajane Marques Peres*

Main category: cs.IR

TL;DR: A bilingual Portuguese-English benchmark dataset for text-to-SQL conversion in process mining domain with 1,655 natural language utterances and 205 SQL statements.


<details>
  <summary>Details</summary>
Motivation: To facilitate natural language querying of databases for process mining, making it accessible to non-SQL experts and improving productivity for SQL experts by addressing domain-specific challenges like specialized vocabularies and event log structures.

Method: Manual curation by experts, professional translations, detailed annotation process, and baseline evaluation using GPT-3.5 Turbo to create a dataset with natural language utterances, SQL statements, and qualifiers.

Result: The dataset supports evaluation of text-to-SQL implementations and demonstrates feasibility for process mining applications, showing broader applicability for semantic parsing and NLP tasks.

Conclusion: text-2-SQL-4-PM is a valuable bilingual benchmark that addresses process mining-specific challenges and enables effective text-to-SQL conversion in this domain.

Abstract: This paper introduces text-2-SQL-4-PM, a bilingual (Portuguese-English)
benchmark dataset designed for the text-to-SQL task in the process mining
domain. Text-to-SQL conversion facilitates natural language querying of
databases, increasing accessibility for users without SQL expertise and
productivity for those that are experts. The text-2-SQL-4-PM dataset is
customized to address the unique challenges of process mining, including
specialized vocabularies and single-table relational structures derived from
event logs. The dataset comprises 1,655 natural language utterances, including
human-generated paraphrases, 205 SQL statements, and ten qualifiers. Methods
include manual curation by experts, professional translations, and a detailed
annotation process to enable nuanced analyses of task complexity. Additionally,
a baseline study using GPT-3.5 Turbo demonstrates the feasibility and utility
of the dataset for text-to-SQL applications. The results show that
text-2-SQL-4-PM supports evaluation of text-to-SQL implementations, offering
broader applicability for semantic parsing and other natural language
processing tasks.

</details>


### [226] [TalkPlayData 2: An Agentic Synthetic Data Pipeline for Multimodal Conversational Music Recommendation](https://arxiv.org/abs/2509.09685)
*Keunwoo Choi,Seungheon Doh,Juhan Nam*

Main category: cs.IR

TL;DR: TalkPlayData 2 is a synthetic multimodal dataset for music recommendation generated by LLM agents with specialized roles and multimodal capabilities.


<details>
  <summary>Details</summary>
Motivation: To create a high-quality synthetic dataset for training generative music recommendation models that can handle multimodal conversations and various recommendation scenarios.

Method: Uses an agentic pipeline with multiple LLM agents (Listener and Recsys) playing specialized roles with custom prompts. The Listener is conditioned on conversation goals, and all agents are multimodal with audio and image capabilities to simulate real-world recommendation scenarios.

Result: The dataset successfully achieved its goals in LLM-as-a-judge and subjective evaluations, demonstrating effectiveness for training generative music recommendation models across various conversation aspects.

Conclusion: TalkPlayData 2 provides a valuable open-source synthetic dataset that enables multimodal conversational music recommendation research and model training through its agentic generation approach.

Abstract: We present TalkPlayData 2, a synthetic dataset for multimodal conversational
music recommendation generated by an agentic data pipeline. In TalkPlayData 2
pipeline, multiple large language model (LLM) agents are created under various
roles with specialized prompts and access to different parts of information,
and the chat data is acquired by logging the conversation between the Listener
LLM and the Recsys LLM. To cover various conversation scenarios, for each
conversation, the Listener LLM is conditioned on a finetuned conversation goal.
Finally, all the LLMs are multimodal with audio and images, allowing a
simulation of multimodal recommendation and conversation. In the LLM-as-a-judge
and subjective evaluation experiments, TalkPlayData 2 achieved the proposed
goal in various aspects related to training a generative recommendation model
for music. TalkPlayData 2 and its generation code are open-sourced at
https://talkpl.ai/talkplaydata2.html.

</details>


### [227] [GeoGPT.RAG Technical Report](https://arxiv.org/abs/2509.09686)
*Fei Huang,Fan Wu,Zeqing Zhang,Qihao Wang,Long Zhang,Grant Michael Boquet,Hongyang Chen*

Main category: cs.IR

TL;DR: GeoGPT is an open-source geoscience LLM system that uses Retrieval Augmented Generation (RAG) with a specialized geoscience corpus and personalized knowledge bases, enhanced by fine-tuned embedding and ranking models for improved domain-specific performance.


<details>
  <summary>Details</summary>
Motivation: To advance geoscience research by developing an open large language model system with enhanced domain-specific capabilities through retrieval-augmented generation and specialized knowledge integration.

Method: Integrated RAG with a curated GeoGPT Library corpus, allows personalized knowledge base uploads, and fine-tuned both embedding and ranking models to optimize retrieval quality and domain alignment for geoscience applications.

Result: The system generates accurate, context-specific answers with improved retrieval quality and domain alignment, delivering precise and trustworthy outputs for geoscience applications.

Conclusion: GeoGPT represents a commitment to open science through collaboration, transparency, and community-driven development, with open-sourced core RAG components (GeoEmbedding and GeoReranker) to support geoscientists worldwide with accessible AI tools.

Abstract: GeoGPT is an open large language model system built to advance research in
the geosciences. To enhance its domain-specific capabilities, we integrated
Retrieval Augmented Generation(RAG), which augments model outputs with relevant
information retrieved from an external knowledge source. GeoGPT uses RAG to
draw from the GeoGPT Library, a specialized corpus curated for geoscientific
content, enabling it to generate accurate, context-specific answers. Users can
also create personalized knowledge bases by uploading their own publication
lists, allowing GeoGPT to retrieve and respond using user-provided materials.
To further improve retrieval quality and domain alignment, we fine-tuned both
the embedding model and a ranking model that scores retrieved passages by
relevance to the query. These enhancements optimize RAG for geoscience
applications and significantly improve the system's ability to deliver precise
and trustworthy outputs. GeoGPT reflects a strong commitment to open science
through its emphasis on collaboration, transparency, and community driven
development. As part of this commitment, we have open-sourced two core RAG
components-GeoEmbedding and GeoReranker-to support geoscientists, researchers,
and professionals worldwide with powerful, accessible AI tools.

</details>


### [228] [AI-Powered Assistant for Long-Term Access to RHIC Knowledge](https://arxiv.org/abs/2509.09688)
*Mohammad Atif,Vincent Garonne,Eric Lancon,Jerome Lauret,Alexandr Prozorov,Michal Vranovsky*

Main category: cs.IR

TL;DR: AI-powered assistant system using LLMs to preserve and provide natural language access to RHIC's extensive data holdings and scientific knowledge for reproducibility and future research.


<details>
  <summary>Details</summary>
Motivation: Preserve RHIC's 25 years of scientific data (~1 ExaByte) and embedded knowledge to support reproducibility, education, and future discoveries as the collider concludes operation.

Method: Built on Large Language Models using Retrieval-Augmented Generation and Model Context Protocol, indexing both structured and unstructured content from RHIC experiments for domain-adapted interaction.

Result: Successful deployment with good computational performance, ongoing multi-experiment integration, and architectural features designed for sustainable, explainable long-term AI access.

Conclusion: Modern AI/ML tools can effectively transform the usability and discoverability of scientific legacy data, demonstrating a viable approach for preserving large-scale experimental knowledge.

Abstract: As the Relativistic Heavy Ion Collider (RHIC) at Brookhaven National
Laboratory concludes 25 years of operation, preserving not only its vast data
holdings ($\sim$1 ExaByte) but also the embedded scientific knowledge becomes a
critical priority. The RHIC Data and Analysis Preservation Plan (DAPP)
introduces an AI-powered assistant system that provides natural language access
to documentation, workflows, and software, with the aim of supporting
reproducibility, education, and future discovery. Built upon Large Language
Models using Retrieval-Augmented Generation and the Model Context Protocol,
this assistant indexes structured and unstructured content from RHIC
experiments and enables domain-adapted interaction. We report on the
deployment, computational performance, ongoing multi-experiment integration,
and architectural features designed for a sustainable and explainable long-term
AI access. Our experience illustrates how modern AI/ML tools can transform the
usability and discoverability of scientific legacy data.

</details>


### [229] [Personas within Parameters: Fine-Tuning Small Language Models with Low-Rank Adapters to Mimic User Behaviors](https://arxiv.org/abs/2509.09689)
*Himanshu Thakur,Eshani Agrawal,Smruthi Mukund*

Main category: cs.IR

TL;DR: Using frozen LLMs for robust user representations and fine-tuned SLMs for efficient user simulation, with persona-based low-rank adapters to balance scalability and performance.


<details>
  <summary>Details</summary>
Motivation: Simulating user behavior is challenging due to complex interactions. Existing LLM approaches struggle with parsing tabular data, overcoming pre-training biases, and scaling to millions of users.

Method: Extract textual user representations using frozen LLM, then use fine-tuned Small Language Models (SLMs) for efficient user agents. Train multiple low-rank adapters for user groups/personas.

Result: Empirical evidence shows the approach effectively bridges the gap between offline metrics and real-world recommender system performance.

Conclusion: The method provides cost-effective, resource-efficient user simulation that scales well while maintaining performance accuracy.

Abstract: A long-standing challenge in developing accurate recommendation models is
simulating user behavior, mainly due to the complex and stochastic nature of
user interactions. Towards this, one promising line of work has been the use of
Large Language Models (LLMs) for simulating user behavior. However, aligning
these general-purpose large pre-trained models with user preferences
necessitates: (i) effectively and continously parsing large-scale tabular
user-item interaction data, (ii) overcoming pre-training-induced inductive
biases to accurately learn user specific knowledge, and (iii) achieving the
former two at scale for millions of users. While most previous works have
focused on complex methods to prompt an LLM or fine-tune it on tabular
interaction datasets, our approach shifts the focus to extracting robust
textual user representations using a frozen LLM and simulating cost-effective,
resource-efficient user agents powered by fine-tuned Small Language Models
(SLMs). Further, we showcase a method for training multiple low-rank adapters
for groups of users or \textit{persona}, striking an optimal balance between
scalability and performance of user behavior agents. Our experiments provide
compelling empirical evidence of the efficacy of our methods, demonstrating
that user agents developed using our approach have the potential to bridge the
gap between offline metrics and real-world performance of recommender systems.

</details>


### [230] [Wave-Based Semantic Memory with Resonance-Based Retrieval: A Phase-Aware Alternative to Vector Embedding Stores](https://arxiv.org/abs/2509.09691)
*Aleksandr Listopad*

Main category: cs.IR

TL;DR: Wave-Based Semantic Memory replaces traditional vector similarity with wave pattern resonance, preserving both amplitude and phase information for more expressive semantic retrieval.


<details>
  <summary>Details</summary>
Motivation: Conventional vector-based memory systems are phase-insensitive and limited in capturing resonance phenomena crucial for meaning representation, needing a more expressive approach.

Method: Models knowledge as wave patterns ψ(x) = A(x)e^{iφ(x)} and retrieves it through resonance-based interference, preserving both amplitude and phase information.

Result: Resonance-based retrieval achieves higher discriminative power where vector methods fail, handles phase shifts/negations/compositional queries, and scales to millions of patterns with millisecond latency.

Conclusion: Wave-based memory is a viable alternative to vector stores for AGI-oriented reasoning and knowledge representation, offering more robust semantic similarity.

Abstract: Conventional vector-based memory systems rely on cosine or inner product
similarity within real-valued embedding spaces. While computationally
efficient, such approaches are inherently phase-insensitive and limited in
their ability to capture resonance phenomena crucial for meaning
representation. We propose Wave-Based Semantic Memory, a novel framework that
models knowledge as wave patterns $\psi(x) = A(x) e^{i\phi(x)}$ and retrieves
it through resonance-based interference. This approach preserves both amplitude
and phase information, enabling more expressive and robust semantic similarity.
We demonstrate that resonance-based retrieval achieves higher discriminative
power in cases where vector methods fail, including phase shifts, negations,
and compositional queries. Our implementation, ResonanceDB, shows scalability
to millions of patterns with millisecond latency, positioning wave-based memory
as a viable alternative to vector stores for AGI-oriented reasoning and
knowledge representation.

</details>


### [231] [Powering Job Search at Scale: LLM-Enhanced Query Understanding in Job Matching Systems](https://arxiv.org/abs/2509.09690)
*Ping Liu,Jianqiang Shen,Qianqi Shen,Chunnan Yao,Kevin Kao,Dan Xu,Rajat Arora,Baofen Zheng,Caleb Johnson,Liangjie Hong,Jingwei Wu,Wenjing Zhang*

Main category: cs.IR

TL;DR: A unified LLM-based framework for query understanding that replaces multiple NER models with a single system, improving relevance while reducing complexity.


<details>
  <summary>Details</summary>
Motivation: Traditional query understanding uses multiple task-specific NER models which are brittle, expensive to maintain, and slow to adapt to evolving taxonomies and language patterns.

Method: A unified framework powered by Large Language Model that jointly models user queries and contextual signals (like profile attributes) to generate structured interpretations.

Result: The framework improves relevance quality in online A/B testing while significantly reducing system complexity and operational overhead.

Conclusion: The solution provides a scalable and adaptable foundation for query understanding in dynamic web applications, demonstrating the effectiveness of LLM-powered unified approaches.

Abstract: Query understanding is essential in modern relevance systems, where user
queries are often short, ambiguous, and highly context-dependent. Traditional
approaches often rely on multiple task-specific Named Entity Recognition models
to extract structured facets as seen in job search applications. However, this
fragmented architecture is brittle, expensive to maintain, and slow to adapt to
evolving taxonomies and language patterns. In this paper, we introduce a
unified query understanding framework powered by a Large Language Model (LLM),
designed to address these limitations. Our approach jointly models the user
query and contextual signals such as profile attributes to generate structured
interpretations that drive more accurate and personalized recommendations. The
framework improves relevance quality in online A/B testing while significantly
reducing system complexity and operational overhead. The results demonstrate
that our solution provides a scalable and adaptable foundation for query
understanding in dynamic web applications.

</details>


### [232] [Model-agnostic post-hoc explainability for recommender systems](https://arxiv.org/abs/2509.10245)
*Irina Arévalo,Jose L Salmeron*

Main category: cs.IR

TL;DR: This paper introduces deletion diagnostics for recommender systems to improve interpretability by quantifying how individual users/items influence model performance, demonstrating the approach on both deep learning (NCF) and classical (SVD) methods.


<details>
  <summary>Details</summary>
Motivation: Complex recommender systems using deep learning and feature embeddings often sacrifice interpretability and transparency, making it difficult to understand how specific users or items influence recommendations.

Method: Developed deletion diagnostics that compare model performance with and without specific users/items to quantify their influence. Applied to both Neural Collaborative Filtering (NCF) and Singular Value Decomposition (SVD) to demonstrate model-agnostic nature.

Result: Experiments on MovieLens and Amazon Reviews datasets provided insights into model behavior and showed the approach works across different recommendation paradigms (deep learning and classical methods).

Conclusion: Deletion diagnostics offer a systematic way to improve interpretability in recommender systems, quantifying observation influence and working across various model types while maintaining performance insights.

Abstract: Recommender systems often benefit from complex feature embeddings and deep
learning algorithms, which deliver sophisticated recommendations that enhance
user experience, engagement, and revenue. However, these methods frequently
reduce the interpretability and transparency of the system. In this research,
we develop a systematic application, adaptation, and evaluation of deletion
diagnostics in the recommender setting. The method compares the performance of
a model to that of a similar model trained without a specific user or item,
allowing us to quantify how that observation influences the recommender, either
positively or negatively. To demonstrate its model-agnostic nature, the
proposal is applied to both Neural Collaborative Filtering (NCF), a widely used
deep learning-based recommender, and Singular Value Decomposition (SVD), a
classical collaborative filtering technique. Experiments on the MovieLens and
Amazon Reviews datasets provide insights into model behavior and highlight the
generality of the approach across different recommendation paradigms.

</details>


### [233] [Diversified recommendations of cultural activities with personalized determinantal point processes](https://arxiv.org/abs/2509.10392)
*Carole Ibrahim,Hiba Bederina,Daniel Cuesta,Laurent Montier,Cyrille Delabre,Jill-Jênn Vie*

Main category: cs.IR

TL;DR: Personalized DPPs for diverse recommendations without hurting core metrics, with code released for reproducibility


<details>
  <summary>Details</summary>
Motivation: Diversifying recommendations while maintaining user engagement and business metrics is a major industry challenge, particularly for broadening audience cultural practices

Method: Uses personalized Determinantal Point Processes (DPPs) with quality-diversity decomposition to weight user preferences, implemented and tested in production environment

Result: Evaluates trade-offs between relevance and diversity through both offline and online metrics

Conclusion: Provides practical insights for implementing personalized DPP sampling in production systems and releases full code for reproducibility

Abstract: While optimizing recommendation systems for user engagement is a
well-established practice, effectively diversifying recommendations without
negatively impacting core business metrics remains a significant industry
challenge. In line with our initiative to broaden our audience's cultural
practices, this study investigates using personalized Determinantal Point
Processes (DPPs) to sample diverse and relevant recommendations. We rely on a
well-known quality-diversity decomposition of the similarity kernel to give
more weight to user preferences. In this paper, we present our implementations
of the personalized DPP sampling, evaluate the trade-offs between relevance and
diversity through both offline and online metrics, and give insights for
practitioners on their use in a production environment. For the sake of
reproducibility, we release the full code for our platform and experiments on
GitHub.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [234] [RFSeek and Ye Shall Find](https://arxiv.org/abs/2509.10216)
*Noga H. Rotman,Tiago Ferreira,Hila Peleg,Mark Silberstein,Alexandra Silva*

Main category: cs.NI

TL;DR: RFSeek is an interactive tool that uses LLMs to automatically extract visual summaries of protocol logic from RFCs, creating provenance-linked diagrams that reveal both official state machines and additional logic found only in text.


<details>
  <summary>Details</summary>
Motivation: RFCs are lengthy prose-based documents that impede precise operational understanding of network protocols, making visual representations necessary for better comprehension.

Method: Leverages large language models (LLMs) to generate provenance-linked, explorable diagrams from RFC text, extracting both official state machines and additional logic described only in prose.

Result: RFSeek successfully reconstructs RFC diagrams, uncovers missing logic from text, and creates new visualizations for complex protocols like QUIC, TCP, PPTP, and DCCP with transparent, auditable results.

Conclusion: Combining LLMs with formal, user-customized visualizations (Summary Visualization) enhances protocol comprehension and supports robust implementations, representing a promising direction for protocol analysis.

Abstract: Requests for Comments (RFCs) are extensive specification documents for
network protocols, but their prose-based format and their considerable length
often impede precise operational understanding. We present RFSeek, an
interactive tool that automatically extracts visual summaries of protocol logic
from RFCs. RFSeek leverages large language models (LLMs) to generate
provenance-linked, explorable diagrams, surfacing both official state machines
and additional logic found only in the RFC text. Compared to existing RFC
visualizations, RFSeek's visual summaries are more transparent and easier to
audit against their textual source. We showcase the tool's potential through a
series of use cases, including guided knowledge extraction and semantic
diffing, applied to protocols such as TCP, QUIC, PPTP, and DCCP.
  In practice, RFSeek not only reconstructs the RFC diagrams included in some
specifications, but, more interestingly, also uncovers important logic such as
nodes or edges described in the text but missing from those diagrams. RFSeek
further derives new visualization diagrams for complex RFCs, with QUIC as a
representative case. Our approach, which we term \emph{Summary Visualization},
highlights a promising direction: combining LLMs with formal, user-customized
visualizations to enhance protocol comprehension and support robust
implementations.

</details>


<div id='q-bio.OT'></div>

# q-bio.OT [[Back]](#toc)

### [235] [Standards in the Preparation of Biomedical Research Metadata: A Bridge2AI Perspective](https://arxiv.org/abs/2509.10432)
*Harry Caufield,Satrajit Ghosh,Sek Wong Kong,Jillian Parker,Nathan Sheffield,Bhavesh Patel,Andrew Williams,Timothy Clark,Monica C. Munoz-Torres*

Main category: q-bio.OT

TL;DR: The paper analyzes AI-readiness criteria for biomedical datasets, focusing on metadata requirements and standardization within the Bridge2AI consortium's Grand Challenges to support ethical AI/ML applications.


<details>
  <summary>Details</summary>
Motivation: To define and assess what makes biomedical datasets AI-ready, ensuring they can be optimally and ethically used for machine learning applications while addressing complex biomedical research problems.

Method: The Bridge2AI consortium established criteria for AI-readiness including FAIRness, provenance, characterization, explainability, sustainability, and computability. The report assesses metadata creation and standardization across four data-generating projects, providing guidelines and identifying gaps.

Result: The consortium developed standardized multimodal data, tools, and training resources to support AI integration while addressing ethical data practices. The assessment provides practical guidelines for metadata creation to enhance AI-readiness.

Conclusion: Proper metadata standardization is crucial for creating AI-ready biomedical datasets. The lessons learned from Bridge2AI's Grand Challenges provide valuable guidance for new projects seeking to develop ethically sound, machine learning-ready data resources.

Abstract: AI-readiness describes the degree to which data may be optimally and
ethically used for subsequent AI and Machine Learning (AI/ML) methods, where
those methods may involve some combination of model training, data
classification, and ethical, explainable prediction. The Bridge2AI consortium
has defined the particular criteria a biomedical dataset may possess to render
it AI-ready: in brief, a dataset's readiness is related to its FAIRness,
provenance, degree of characterization, explainability, sustainability, and
computability, in addition to its accompaniment with documentation about
ethical data practices.
  To ensure AI-readiness and to clarify data structure and relationships within
Bridge2AI's Grand Challenges (GCs), particular types of metadata are necessary.
The GCs within the Bridge2AI initiative include four data-generating projects
focusing on generating AI/ML-ready datasets to tackle complex biomedical and
behavioral research problems. These projects develop standardized, multimodal
data, tools, and training resources to support AI integration, while addressing
ethical data practices. Examples include using voice as a biomarker, building
interpretable genomic tools, modeling disease trajectories with diverse
multimodal data, and mapping cellular and molecular health indicators across
the human body.
  This report assesses the state of metadata creation and standardization in
the Bridge2AI GCs, provides guidelines where required, and identifies gaps and
areas for improvement across the program. New projects, including those outside
the Bridge2AI consortium, would benefit from what we have learned about
creating metadata as part of efforts to promote AI readiness.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [236] [Openness in AI and downstream governance: A global value chain approach](https://arxiv.org/abs/2509.10220)
*Christopher Foster*

Main category: cs.CY

TL;DR: This paper analyzes openness in AI as a strategic interfirm relation within value chains, examining how open AI resources may enable technological transfer despite industry concentration among big tech firms.


<details>
  <summary>Details</summary>
Motivation: To understand the economic implications of openness in AI (open models, datasets, toolchains) and whether it can support technological catch-up and transfer in the face of concentrated AI industry power dominated by big tech companies.

Method: Conceptualizes openness in AI as a unique type of interfirm relation and applies value chain analysis to examine capitalist dynamics, outsourcing strategies of foundational firms, and emerging governance and control mechanisms downstream.

Result: Develops a framework that links foundational AI with downstream value chains, revealing how openness may create potential spillovers from intense global competition for AI leadership.

Conclusion: While critical of big tech power concentration, openness in AI may lead to beneficial spillover effects and support broader technological adoption and transfer, extending our understanding of AI as a productive economic sector.

Abstract: The rise of AI has been rapid, becoming a leading sector for investment and
promising disruptive impacts across the economy. Within the critical analysis
of the economic impacts, AI has been aligned to the critical literature on data
power and platform capitalism - further concentrating power and value capture
amongst a small number of "big tech" leaders.
  The equally rapid rise of openness in AI (here taken to be claims made by AI
firms about openness, "open source" and free provision) signals an interesting
development. It highlights an emerging ecosystem of open AI models, datasets
and toolchains, involving massive capital investment. It poses questions as to
whether open resources can support technological transfer and the ability for
catch-up, even in the face of AI industry power.
  This work seeks to add conceptual clarity to these debates by conceptualising
openness in AI as a unique type of interfirm relation and therefore amenable to
value chain analysis. This approach then allows consideration of the capitalist
dynamics of "outsourcing" of foundational firms in value chains, and
consequently the types of governance and control that might emerge downstream
as AI is adopted. This work, therefore, extends previous mapping of AI value
chains to build a framework which links foundational AI with downstream value
chains.
  Overall, this work extends our understanding of AI as a productive sector.
While the work remains critical of the power of leading AI firms, openness in
AI may lead to potential spillovers stemming from the intense competition for
global technological leadership in AI.

</details>


### [237] [We Need a New Ethics for a World of AI Agents](https://arxiv.org/abs/2509.10289)
*Iason Gabriel,Geoff Keeling,Arianna Manzini,James Evans*

Main category: cs.CY

TL;DR: Paper calls for increased attention to AI agent safety, human-machine relationships, and social coordination challenges as AI agents become more prevalent.


<details>
  <summary>Details</summary>
Motivation: The increasing deployment of capable AI agents raises new safety concerns and questions about human-machine relationships that require urgent attention from researchers and policymakers.

Method: The authors present an argumentative analysis exploring key challenges that need to be addressed regarding human-AI interactions and inter-agent coordination.

Result: Identifies critical challenges in ensuring beneficial interactions between humans and AI agents, and among AI agents themselves.

Conclusion: Greater engagement from scientists, scholars, engineers, and policymakers is needed to address the implications of a world increasingly populated by AI agents.

Abstract: The deployment of capable AI agents raises fresh questions about safety,
human-machine relationships and social coordination. We argue for greater
engagement by scientists, scholars, engineers and policymakers with the
implications of a world increasingly populated by AI agents. We explore key
challenges that must be addressed to ensure that interactions between humans
and agents, and among agents themselves, remain broadly beneficial.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [238] [Automated Tuning for Diffusion Inverse Problem Solvers without Generative Prior Retraining](https://arxiv.org/abs/2509.09880)
*Yaşar Utku Alçalar,Junno Yun,Mehmet Akçakaya*

Main category: eess.IV

TL;DR: Zero-shot Adaptive Diffusion Sampling (ZADS) is a test-time optimization method that adaptively tunes fidelity weights for diffusion-based MRI reconstruction without retraining, outperforming existing methods across varying noise schedules.


<details>
  <summary>Details</summary>
Motivation: Diffusion models for MRI reconstruction require carefully tuned data fidelity weights, but existing approaches rely on heuristics or fixed weights that fail to generalize across varying measurement conditions and irregular timestep schedules.

Method: ZADS treats the denoising process as a fixed unrolled sampler and optimizes fidelity weights in a self-supervised manner using only undersampled measurements, without requiring retraining of the diffusion prior.

Result: Experiments on the fastMRI knee dataset show that ZADS consistently outperforms both traditional compressed sensing and recent diffusion-based methods.

Conclusion: ZADS delivers high-fidelity reconstructions across varying noise schedules and acquisition settings through adaptive test-time optimization of fidelity weights.

Abstract: Diffusion/score-based models have recently emerged as powerful generative
priors for solving inverse problems, including accelerated MRI reconstruction.
While their flexibility allows decoupling the measurement model from the
learned prior, their performance heavily depends on carefully tuned data
fidelity weights, especially under fast sampling schedules with few denoising
steps. Existing approaches often rely on heuristics or fixed weights, which
fail to generalize across varying measurement conditions and irregular timestep
schedules. In this work, we propose Zero-shot Adaptive Diffusion Sampling
(ZADS), a test-time optimization method that adaptively tunes fidelity weights
across arbitrary noise schedules without requiring retraining of the diffusion
prior. ZADS treats the denoising process as a fixed unrolled sampler and
optimizes fidelity weights in a self-supervised manner using only undersampled
measurements. Experiments on the fastMRI knee dataset demonstrate that ZADS
consistently outperforms both traditional compressed sensing and recent
diffusion-based methods, showcasing its ability to deliver high-fidelity
reconstructions across varying noise schedules and acquisition settings.

</details>


### [239] [Drone-Based Multispectral Imaging and Deep Learning for Timely Detection of Branched Broomrape in Tomato Farms](https://arxiv.org/abs/2509.09972)
*Mohammadreza Narimani,Alireza Pourreza,Ali Moghimi,Mohsen Mesgaran,Parastoo Farajpoor,Hamid Jafarbiglu*

Main category: eess.IV

TL;DR: Drone-based multispectral imagery combined with LSTM deep learning and SMOTE technique achieves high accuracy (88.37%) in early detection of broomrape parasite in tomato crops.


<details>
  <summary>Details</summary>
Motivation: Address the escalating threat of branched broomrape to California's tomato industry, as conventional detection methods are difficult and chemical controls are costly, environmentally harmful, and ineffective.

Method: Combined drone-based multispectral imagery with Long Short-Term Memory (LSTM) deep learning networks, using Synthetic Minority Over-sampling Technique (SMOTE) to handle class imbalance. Research conducted on known broomrape-infested tomato farm across five key growth stages determined by growing degree days (GDD).

Result: At 897 GDD, achieved 79.09% overall accuracy and 70.36% recall without later stages. Best-performing scenario integrating all growth stages with SMOTE achieved 88.37% overall accuracy and 95.37% recall.

Conclusion: Temporal multispectral analysis with LSTM networks shows strong potential for early broomrape detection. UAV-based multispectral sensing coupled with deep learning could provide powerful precision agriculture tool to reduce losses and improve sustainability in tomato production.

Abstract: This study addresses the escalating threat of branched broomrape (Phelipanche
ramosa) to California's tomato industry, which supplies over 90 percent of U.S.
processing tomatoes. The parasite's largely underground life cycle makes early
detection difficult, while conventional chemical controls are costly,
environmentally harmful, and often ineffective. To address this, we combined
drone-based multispectral imagery with Long Short-Term Memory (LSTM) deep
learning networks, using the Synthetic Minority Over-sampling Technique (SMOTE)
to handle class imbalance. Research was conducted on a known broomrape-infested
tomato farm in Woodland, Yolo County, CA, across five key growth stages
determined by growing degree days (GDD). Multispectral images were processed to
isolate tomato canopy reflectance. At 897 GDD, broomrape could be detected with
79.09 percent overall accuracy and 70.36 percent recall without integrating
later stages. Incorporating sequential growth stages with LSTM improved
detection substantially. The best-performing scenario, which integrated all
growth stages with SMOTE augmentation, achieved 88.37 percent overall accuracy
and 95.37 percent recall. These results demonstrate the strong potential of
temporal multispectral analysis and LSTM networks for early broomrape
detection. While further real-world data collection is needed for practical
deployment, this study shows that UAV-based multispectral sensing coupled with
deep learning could provide a powerful precision agriculture tool to reduce
losses and improve sustainability in tomato production.

</details>


### [240] [Polarization Denoising and Demosaicking: Dataset and Baseline Method](https://arxiv.org/abs/2509.10098)
*Muhamad Daniel Ariff Bin Abdul Rahman,Yusuke Monno,Masayuki Tanaka,Masatoshi Okutomi*

Main category: eess.IV

TL;DR: Proposes a new dataset and baseline method for joint polarization denoising and demosaicking in division-of-focal-plane polarimeters, addressing a research gap in this area.


<details>
  <summary>Details</summary>
Motivation: Current research focuses on polarization demosaicking for noise-free cases, but there's limited work on the joint task of denoising and demosaicking due to lack of suitable evaluation datasets and baseline methods.

Method: Creates a dataset with 40 real-world scenes and three noise-level conditions, and develops a denoising-then-demosaicking approach using well-accepted signal processing components for reproducibility.

Result: Experimental results show the proposed method achieves higher image reconstruction performance than alternative methods, providing a solid baseline for future research.

Conclusion: The work addresses a significant research gap by providing both a comprehensive dataset and a reproducible baseline method for polarization denoising and demosaicking, enabling further advancements in this field.

Abstract: A division-of-focal-plane (DoFP) polarimeter enables us to acquire images
with multiple polarization orientations in one shot and thus it is valuable for
many applications using polarimetric information. The image processing pipeline
for a DoFP polarimeter entails two crucial tasks: denoising and demosaicking.
While polarization demosaicking for a noise-free case has increasingly been
studied, the research for the joint task of polarization denoising and
demosaicking is scarce due to the lack of a suitable evaluation dataset and a
solid baseline method. In this paper, we propose a novel dataset and method for
polarization denoising and demosaicking. Our dataset contains 40 real-world
scenes and three noise-level conditions, consisting of pairs of noisy mosaic
inputs and noise-free full images. Our method takes a
denoising-then-demosaicking approach based on well-accepted signal processing
components to offer a reproducible method. Experimental results demonstrate
that our method exhibits higher image reconstruction performance than other
alternative methods, offering a solid baseline.

</details>


### [241] [Multi-pathology Chest X-ray Classification with Rejection Mechanisms](https://arxiv.org/abs/2509.10348)
*Yehudit Aperstein,Amit Tzahar,Alon Gottlib,Tal Verber,Ravit Shagan Damti,Alexander Apartsin*

Main category: eess.IV

TL;DR: Uncertainty-aware framework for chest X-ray diagnosis using DenseNet-121 with selective prediction mechanisms to reject uncertain predictions and improve reliability in multi-label classification.


<details>
  <summary>Details</summary>
Motivation: Overconfidence in deep learning models poses significant risks in high-stakes medical imaging tasks, particularly for multi-label chest X-ray classification where multiple co-occurring pathologies must be detected simultaneously.

Method: DenseNet-121 backbone enhanced with two selective prediction mechanisms: entropy-based rejection and confidence interval-based rejection, using quantile-based calibration with global or class-specific threshold tuning.

Result: Experiments on three large datasets (PadChest, NIH ChestX-ray14, MIMIC-CXR) show selective rejection improves trade-off between diagnostic accuracy and coverage, with entropy-based rejection achieving highest average AUC across all pathologies.

Conclusion: Integration of selective prediction into AI-assisted diagnostic workflows provides a practical step toward safer, uncertainty-aware deployment of deep learning in clinical settings.

Abstract: Overconfidence in deep learning models poses a significant risk in
high-stakes medical imaging tasks, particularly in multi-label classification
of chest X-rays, where multiple co-occurring pathologies must be detected
simultaneously. This study introduces an uncertainty-aware framework for chest
X-ray diagnosis based on a DenseNet-121 backbone, enhanced with two selective
prediction mechanisms: entropy-based rejection and confidence interval-based
rejection. Both methods enable the model to abstain from uncertain predictions,
improving reliability by deferring ambiguous cases to clinical experts. A
quantile-based calibration procedure is employed to tune rejection thresholds
using either global or class-specific strategies. Experiments conducted on
three large public datasets (PadChest, NIH ChestX-ray14, and MIMIC-CXR)
demonstrate that selective rejection improves the trade-off between diagnostic
accuracy and coverage, with entropy-based rejection yielding the highest
average AUC across all pathologies. These results support the integration of
selective prediction into AI-assisted diagnostic workflows, providing a
practical step toward safer, uncertainty-aware deployment of deep learning in
clinical settings.

</details>


### [242] [Accelerating 3D Photoacoustic Computed Tomography with End-to-End Physics-Aware Neural Operators](https://arxiv.org/abs/2509.09894)
*Jiayun Wang,Yousuf Aborahama,Arya Khokhar,Yang Zhang,Chuwei Wang,Karteekeya Sastry,Julius Berner,Yilin Luo,Boris Bonev,Zongyi Li,Kamyar Azizzadenesheli,Lihong V. Wang,Anima Anandkumar*

Main category: eess.IV

TL;DR: Pano is a physics-aware neural operator that learns inverse acoustic mapping for 3D photoacoustic computed tomography, enabling high-quality volumetric reconstructions with sparse transducer arrays and limited-angle acquisitions while maintaining real-time capabilities.


<details>
  <summary>Details</summary>
Motivation: Current 3D PACT systems require dense transducer arrays and prolonged acquisition times, limiting clinical translation. There's a need to reduce hardware requirements while maintaining image quality.

Method: End-to-end physics-aware model using spherical discrete-continuous convolutions to preserve sensor geometry, incorporating Helmholtz equation constraints for physical consistency, and operating resolution-independently across sensor configurations.

Result: Achieves robust high-quality image reconstruction from both simulated and real experimental data with significantly reduced transducer counts and limited-angle configurations, maintaining reconstruction fidelity across sparse sampling patterns.

Conclusion: Pano establishes a practical pathway for making 3D PACT more accessible for preclinical and clinical applications by substantially reducing hardware requirements without compromising reconstruction quality.

Abstract: Photoacoustic computed tomography (PACT) combines optical contrast with
ultrasonic resolution, achieving deep-tissue imaging beyond the optical
diffusion limit. While three-dimensional PACT systems enable high-resolution
volumetric imaging for applications spanning transcranial to breast imaging,
current implementations require dense transducer arrays and prolonged
acquisition times, limiting clinical translation. We introduce Pano (PACT
imaging neural operator), an end-to-end physics-aware model that directly
learns the inverse acoustic mapping from sensor measurements to volumetric
reconstructions. Unlike existing approaches (e.g. universal back-projection
algorithm), Pano learns both physics and data priors while also being agnostic
to the input data resolution. Pano employs spherical discrete-continuous
convolutions to preserve hemispherical sensor geometry, incorporates Helmholtz
equation constraints to ensure physical consistency and operates
resolutionindependently across varying sensor configurations. We demonstrate
the robustness and efficiency of Pano in reconstructing high-quality images
from both simulated and real experimental data, achieving consistent
performance even with significantly reduced transducer counts and limited-angle
acquisition configurations. The framework maintains reconstruction fidelity
across diverse sparse sampling patterns while enabling real-time volumetric
imaging capabilities. This advancement establishes a practical pathway for
making 3D PACT more accessible and feasible for both preclinical research and
clinical applications, substantially reducing hardware requirements without
compromising image reconstruction quality.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [243] [Tackling One Health Risks: How Large Language Models are leveraged for Risk Negotiation and Consensus-building](https://arxiv.org/abs/2509.09906)
*Alexandra Fetsch,Iurii Savvateev,Racem Ben Romdhane,Martin Wiedmann,Artemiy Dimov,Maciej Durkalec,Josef Teichmann,Jakob Zinsstag,Konstantinos Koutsoumanis,Andreja Rajkovic,Jason Mann,Mauro Tonolla,Monika Ehling-Schulz,Matthias Filter,Sophia Johler*

Main category: cs.MA

TL;DR: AI-assisted negotiation framework using LLMs and autonomous agents to help stakeholders simulate negotiations, model dynamics, and evaluate solutions for complex cross-sectoral challenges.


<details>
  <summary>Details</summary>
Motivation: Address complex global challenges that require integrated, participatory approaches but are hindered by conventional siloed risk analysis frameworks and information overload.

Method: Developed an AI-assisted negotiation framework incorporating large language models and autonomous agents into negotiation-centered risk analysis workflow for simulating negotiations and modeling dynamics.

Result: Proof-of-concept implementations in biopesticide use and wild animal population control demonstrated the framework's ability to mitigate information overload and augment decision-making under time constraints.

Conclusion: The open-source, web-based AI-assisted negotiation framework shows potential for addressing cross-sectoral engagement challenges and can be tailored by users with limited resources.

Abstract: Key global challenges of our times are characterized by complex
interdependencies and can only be effectively addressed through an integrated,
participatory effort. Conventional risk analysis frameworks often reduce
complexity to ensure manageability, creating silos that hinder comprehensive
solutions. A fundamental shift towards holistic strategies is essential to
enable effective negotiations between different sectors and to balance the
competing interests of stakeholders. However, achieving this balance is often
hindered by limited time, vast amounts of information, and the complexity of
integrating diverse perspectives. This study presents an AI-assisted
negotiation framework that incorporates large language models (LLMs) and
AI-based autonomous agents into a negotiation-centered risk analysis workflow.
The framework enables stakeholders to simulate negotiations, systematically
model dynamics, anticipate compromises, and evaluate solution impacts. By
leveraging LLMs' semantic analysis capabilities we could mitigate information
overload and augment decision-making process under time constraints.
Proof-of-concept implementations were conducted in two real-world scenarios:
(i) prudent use of a biopesticide, and (ii) targeted wild animal population
control. Our work demonstrates the potential of AI-assisted negotiation to
address the current lack of tools for cross-sectoral engagement. Importantly,
the solution's open source, web based design, suits for application by a
broader audience with limited resources and enables users to tailor and develop
it for their own needs.

</details>


<div id='hep-lat'></div>

# hep-lat [[Back]](#toc)

### [244] [Matrix-free Neural Preconditioner for the Dirac Operator in Lattice Gauge Theory](https://arxiv.org/abs/2509.10378)
*Yixuan Sun,Srinivas Eswar,Yin Lin,William Detmold,Phiala Shanahan,Xiaoye Li,Yang Liu,Prasanna Balaprakash*

Main category: hep-lat

TL;DR: A framework using operator learning to construct effective preconditioners for linear systems in lattice QCD, reducing condition numbers and halving CG iterations without explicit matrix operations.


<details>
  <summary>Details</summary>
Motivation: Linear systems in lattice QCD are sparse but ill-conditioned, making iterative methods like CG computationally expensive. Existing preconditioners (especially multigrid) are effective but challenging to construct and add computational overhead.

Method: Leverages operator learning techniques to construct linear maps as preconditioners without relying on explicit matrices from original systems or preconditioners, enabling efficient training and application in CG solvers.

Result: In the Schwinger model U(1) gauge theory, the preconditioning scheme effectively decreases condition numbers and approximately halves the number of CG iterations required for convergence. The framework learns a general mapping that enables zero-shot learning for Dirac operators of different lattice sizes.

Conclusion: The operator learning approach provides an effective preconditioning framework that reduces computational costs in lattice QCD simulations and demonstrates generalization capability across different system sizes through zero-shot learning.

Abstract: Linear systems arise in generating samples and in calculating observables in
lattice quantum chromodynamics~(QCD). Solving the Hermitian positive definite
systems, which are sparse but ill-conditioned, involves using iterative
methods, such as Conjugate Gradient (CG), which are time-consuming and
computationally expensive. Preconditioners can effectively accelerate this
process, with the state-of-the-art being multigrid preconditioners. However,
constructing useful preconditioners can be challenging, adding additional
computational overhead, especially in large linear systems. We propose a
framework, leveraging operator learning techniques, to construct linear maps as
effective preconditioners. The method in this work does not rely on explicit
matrices from either the original linear systems or the produced
preconditioners, allowing efficient model training and application in the CG
solver. In the context of the Schwinger model U(1) gauge theory in 1+1
spacetime dimensions with two degenerate-mass fermions), this preconditioning
scheme effectively decreases the condition number of the linear systems and
approximately halves the number of iterations required for convergence in
relevant parameter ranges. We further demonstrate the framework learns a
general mapping dependent on the lattice structure which leads to zero-shot
learning ability for the Dirac operators constructed from gauge field
configurations of different sizes.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [245] [HHI-Assist: A Dataset and Benchmark of Human-Human Interaction in Physical Assistance Scenario](https://arxiv.org/abs/2509.10096)
*Saeed Saadatnejad,Reyhaneh Hosseininejad,Jose Barreiros,Katherine M. Tsui,Alexandre Alahi*

Main category: cs.RO

TL;DR: This paper introduces HHI-Assist dataset and a Transformer-based diffusion model for human motion prediction in assistive care scenarios, improving interaction-aware robotic assistance.


<details>
  <summary>Details</summary>
Motivation: Addressing labor shortages and aging population needs by developing assistive robots that require accurate human motion prediction during physical interactions, which is challenging due to variability and complex dynamics.

Method: Created HHI-Assist dataset of human-human interaction motion capture clips, and developed a conditional Transformer-based denoising diffusion model to predict poses of interacting agents.

Result: The model effectively captures coupled dynamics between caregivers and receivers, showing improvements over baselines and strong generalization to unseen scenarios.

Conclusion: The work advances interaction-aware motion prediction and provides a new dataset that can significantly enhance robotic assistance policies for care scenarios.

Abstract: The increasing labor shortage and aging population underline the need for
assistive robots to support human care recipients. To enable safe and
responsive assistance, robots require accurate human motion prediction in
physical interaction scenarios. However, this remains a challenging task due to
the variability of assistive settings and the complexity of coupled dynamics in
physical interactions. In this work, we address these challenges through two
key contributions: (1) HHI-Assist, a dataset comprising motion capture clips of
human-human interactions in assistive tasks; and (2) a conditional
Transformer-based denoising diffusion model for predicting the poses of
interacting agents. Our model effectively captures the coupled dynamics between
caregivers and care receivers, demonstrating improvements over baselines and
strong generalization to unseen scenarios. By advancing interaction-aware
motion prediction and introducing a new dataset, our work has the potential to
significantly enhance robotic assistance policies. The dataset and code are
available at: https://sites.google.com/view/hhi-assist/home

</details>


### [246] [GC-VLN: Instruction as Graph Constraints for Training-free Vision-and-Language Navigation](https://arxiv.org/abs/2509.10454)
*Hang Yin,Haoyu Wei,Xiuwei Xu,Wenxuan Guo,Jie Zhou,Jiwen Lu*

Main category: cs.RO

TL;DR: Training-free framework for vision-and-language navigation using graph constraint optimization that decomposes instructions into spatial constraints without requiring training.


<details>
  <summary>Details</summary>
Motivation: Existing zero-shot VLN methods are designed for discrete environments or require unsupervised training, making them difficult to generalize and deploy in real-world continuous environments.

Method: Formulates navigation as graph constraint optimization by decomposing instructions into explicit spatial constraints using a spatial constraint library. Constructs directed acyclic graphs with waypoint and object nodes, then uses constraint solver to determine waypoint positions and navigation paths with backtracking mechanism.

Result: Significant improvements in success rate and navigation efficiency compared to state-of-the-art zero-shot VLN methods on standard benchmarks. Effectively generalizes to new environments and instruction sets in real-world experiments.

Conclusion: The framework enables robust zero-shot adaptation to unseen environments without training, paving the way for more autonomous and deployable navigation systems in real-world scenarios.

Abstract: In this paper, we propose a training-free framework for vision-and-language
navigation (VLN). Existing zero-shot VLN methods are mainly designed for
discrete environments or involve unsupervised training in continuous simulator
environments, which makes it challenging to generalize and deploy them in
real-world scenarios. To achieve a training-free framework in continuous
environments, our framework formulates navigation guidance as graph constraint
optimization by decomposing instructions into explicit spatial constraints. The
constraint-driven paradigm decodes spatial semantics through constraint
solving, enabling zero-shot adaptation to unseen environments. Specifically, we
construct a spatial constraint library covering all types of spatial
relationship mentioned in VLN instructions. The human instruction is decomposed
into a directed acyclic graph, with waypoint nodes, object nodes and edges,
which are used as queries to retrieve the library to build the graph
constraints. The graph constraint optimization is solved by the constraint
solver to determine the positions of waypoints, obtaining the robot's
navigation path and final goal. To handle cases of no solution or multiple
solutions, we construct a navigation tree and the backtracking mechanism.
Extensive experiments on standard benchmarks demonstrate significant
improvements in success rate and navigation efficiency compared to
state-of-the-art zero-shot VLN methods. We further conduct real-world
experiments to show that our framework can effectively generalize to new
environments and instruction sets, paving the way for a more robust and
autonomous navigation framework.

</details>


### [247] [Self-Augmented Robot Trajectory: Efficient Imitation Learning via Safe Self-augmentation with Demonstrator-annotated Precision](https://arxiv.org/abs/2509.09893)
*Hanbit Oh,Masaki Murooka,Tomohiro Motoda,Ryoichi Nakajo,Yukiyasu Domae*

Main category: cs.RO

TL;DR: SART enables robot policy learning from just one human demonstration by safely generating diverse trajectories within precision boundaries, reducing human effort while ensuring collision-free operation.


<details>
  <summary>Details</summary>
Motivation: Standard imitation learning requires substantial data collection through demonstrations or unsafe exploration, leading to frequent collisions and manual resets in clearance-limited tasks like peg-in-hole.

Method: Two-stage framework: 1) Single human demonstration with precision boundaries annotation, 2) Autonomous generation of diverse collision-free trajectories within boundaries and reconnection to original demo.

Result: Achieves substantially higher success rates than policies trained only on human demonstrations in both simulation and real-world manipulation tasks.

Conclusion: SART provides an efficient and safe approach to imitation learning that minimizes human effort while ensuring reliable performance through autonomous data augmentation.

Abstract: Imitation learning is a promising paradigm for training robot agents;
however, standard approaches typically require substantial data acquisition --
via numerous demonstrations or random exploration -- to ensure reliable
performance. Although exploration reduces human effort, it lacks safety
guarantees and often results in frequent collisions -- particularly in
clearance-limited tasks (e.g., peg-in-hole) -- thereby, necessitating manual
environmental resets and imposing additional human burden. This study proposes
Self-Augmented Robot Trajectory (SART), a framework that enables policy
learning from a single human demonstration, while safely expanding the dataset
through autonomous augmentation. SART consists of two stages: (1) human
teaching only once, where a single demonstration is provided and precision
boundaries -- represented as spheres around key waypoints -- are annotated,
followed by one environment reset; (2) robot self-augmentation, where the robot
generates diverse, collision-free trajectories within these boundaries and
reconnects to the original demonstration. This design improves the data
collection efficiency by minimizing human effort while ensuring safety.
Extensive evaluations in simulation and real-world manipulation tasks show that
SART achieves substantially higher success rates than policies trained solely
on human-collected demonstrations. Video results available at
https://sites.google.com/view/sart-il .

</details>


### [248] [TwinTac: A Wide-Range, Highly Sensitive Tactile Sensor with Real-to-Sim Digital Twin Sensor Model](https://arxiv.org/abs/2509.10063)
*Xiyan Huang,Zhe Xu,Chenxi Xiao*

Main category: cs.RO

TL;DR: TwinTac system combines physical tactile sensor with digital twin model to enable tactile sensing in robot skill learning through simulation, improving cross-domain learning tasks.


<details>
  <summary>Details</summary>
Motivation: The absence of simulation models for tactile sensors has hindered the use of tactile sensing in robot skill acquisition processes driven by reinforcement learning, limiting development of effective tactile perception policies.

Method: Developed a physical tactile sensor for high sensitivity and wide measurement range, then created a digital twin model using real-to-sim approach with synchronized cross-domain data and neural networks to map simulated data to real sensor responses.

Result: Characterized physical sensor sensitivity, demonstrated digital twin consistency in replicating physical sensor output, and showed simulation data from digital twin effectively augments real-world data, improving classification accuracy.

Conclusion: TwinTac successfully bridges the gap in cross-domain learning tasks by enabling tactile sensing simulation for robot skill acquisition, with potential for improved policy development through tactile perception.

Abstract: Robot skill acquisition processes driven by reinforcement learning often rely
on simulations to efficiently generate large-scale interaction data. However,
the absence of simulation models for tactile sensors has hindered the use of
tactile sensing in such skill learning processes, limiting the development of
effective policies driven by tactile perception. To bridge this gap, we present
TwinTac, a system that combines the design of a physical tactile sensor with
its digital twin model. Our hardware sensor is designed for high sensitivity
and a wide measurement range, enabling high quality sensing data essential for
object interaction tasks. Building upon the hardware sensor, we develop the
digital twin model using a real-to-sim approach. This involves collecting
synchronized cross-domain data, including finite element method results and the
physical sensor's outputs, and then training neural networks to map simulated
data to real sensor responses. Through experimental evaluation, we
characterized the sensitivity of the physical sensor and demonstrated the
consistency of the digital twin in replicating the physical sensor's output.
Furthermore, by conducting an object classification task, we showed that
simulation data generated by our digital twin sensor can effectively augment
real-world data, leading to improved accuracy. These results highlight
TwinTac's potential to bridge the gap in cross-domain learning tasks.

</details>


### [249] [Robot guide with multi-agent control and automatic scenario generation with LLM](https://arxiv.org/abs/2509.10317)
*Elizaveta D. Moskovskaya,Anton D. Moscowsky*

Main category: cs.RO

TL;DR: Hybrid control architecture combining multi-agent resource management with LLM-based behavior generation for tour guide robots, automating scenario creation and enabling more natural interactions.


<details>
  <summary>Details</summary>
Motivation: Overcome limitations of traditional systems that require manual tuning of behavior scenarios, which suffer from manual configuration, low flexibility, and unnatural robot behavior.

Method: Two-stage generation process: first creating stylized narrative, then integrating non-verbal action tags. Multi-agent system handles coordination, conflict resolution for parallel actions, and maintains default behavior after main operations.

Result: Trial results demonstrate the approach's potential for automating and scaling social robot control systems.

Conclusion: The hybrid architecture successfully enables more natural robot behavior and addresses the scalability challenges of manual scenario configuration in tour guide robots.

Abstract: The work describes the development of a hybrid control architecture for an
anthropomorphic tour guide robot, combining a multi-agent resource management
system with automatic behavior scenario generation based on large language
models. The proposed approach aims to overcome the limitations of traditional
systems, which rely on manual tuning of behavior scenarios. These limitations
include manual configuration, low flexibility, and lack of naturalness in robot
behavior. The process of preparing tour scenarios is implemented through a
two-stage generation: first, a stylized narrative is created, then non-verbal
action tags are integrated into the text. The multi-agent system ensures
coordination and conflict resolution during the execution of parallel actions,
as well as maintaining default behavior after the completion of main
operations, contributing to more natural robot behavior. The results obtained
from the trial demonstrate the potential of the proposed approach for
automating and scaling social robot control systems.

</details>


### [250] [Efficient Learning-Based Control of a Legged Robot in Lunar Gravity](https://arxiv.org/abs/2509.10128)
*Philip Arm,Oliver Fischer,Joseph Church,Adrian Fuhrer,Hendrik Kolvenbach,Marco Hutter*

Main category: cs.RO

TL;DR: RL-based control for legged robots with gravity-scaled power-optimized rewards, achieving 23% power reduction in Earth gravity and 36% in lunar gravity across multiple gravity levels.


<details>
  <summary>Details</summary>
Motivation: Legged robots need energy-efficient control for planetary exploration due to restricted power and thermal budgets, requiring approaches that transfer across multiple gravity environments.

Method: Reinforcement learning with gravity-scaled power-optimized reward functions, plus a constant-force spring offload system for real-world lunar gravity experiments.

Result: 23.4W power consumption in Earth gravity (23% improvement) and 12.2W in lunar gravity (36% improvement over baseline) on a 15.65kg robot at 0.4m/s.

Conclusion: The method provides scalable power-efficient locomotion control for legged robots across multiple gravity levels from lunar to super-Earth gravity.

Abstract: Legged robots are promising candidates for exploring challenging areas on
low-gravity bodies such as the Moon, Mars, or asteroids, thanks to their
advanced mobility on unstructured terrain. However, as planetary robots' power
and thermal budgets are highly restricted, these robots need energy-efficient
control approaches that easily transfer to multiple gravity environments. In
this work, we introduce a reinforcement learning-based control approach for
legged robots with gravity-scaled power-optimized reward functions. We use our
approach to develop and validate a locomotion controller and a base pose
controller in gravity environments from lunar gravity (1.62 m/s2) to a
hypothetical super-Earth (19.62 m/s2). Our approach successfully scales across
these gravity levels for locomotion and base pose control with the
gravity-scaled reward functions. The power-optimized locomotion controller
reached a power consumption for locomotion of 23.4 W in Earth gravity on a
15.65 kg robot at 0.4 m/s, a 23 % improvement over the baseline policy.
Additionally, we designed a constant-force spring offload system that allowed
us to conduct real-world experiments on legged locomotion in lunar gravity. In
lunar gravity, the power-optimized control policy reached 12.2 W, 36 % less
than a baseline controller which is not optimized for power efficiency. Our
method provides a scalable approach to developing power-efficient locomotion
controllers for legged robots across multiple gravity levels.

</details>


<div id='physics.app-ph'></div>

# physics.app-ph [[Back]](#toc)

### [251] [Reinforcement learning for spin torque oscillator tasks](https://arxiv.org/abs/2509.10057)
*Jakub Mojsiejuk,Sławomir Ziętek,Witold Skowroński*

Main category: physics.app-ph

TL;DR: Using reinforcement learning to automatically synchronize spintronic oscillators with target frequencies, showing improved convergence and energy efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the problem of automatic synchronization of spintronic oscillators (STOs) which is challenging but important for various applications.

Method: Numerical solution of macrospin Landau-Lifschitz-Gilbert-Slonczewski equation to simulate STO, training two types of RL agents to synchronize with target frequency within fixed steps.

Result: Achieved improvement in both convergence and energy efficiency of synchronization in the simulated environment.

Conclusion: Reinforcement learning provides an effective approach for automatic STO synchronization with enhanced performance metrics.

Abstract: We address the problem of automatic synchronisation of the spintronic
oscillator (STO) by means of reinforcement learning (RL). A numerical solution
of the macrospin Landau-Lifschitz-Gilbert-Slonczewski equation is used to
simulate the STO and we train the two types of RL agents to synchronise with a
target frequency within a fixed number of steps. We explore modifications to
this base task and show an improvement in both convergence and energy
efficiency of the synchronisation that can be easily achieved in the simulated
environment.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [252] [SWE-Effi: Re-Evaluating Software AI Agent System Effectiveness Under Resource Constraints](https://arxiv.org/abs/2509.09853)
*Zhiyu Fan,Kirill Vasilevski,Dayi Lin,Boyuan Chen,Yihao Chen,Zhiqing Zhong,Jie M. Zhang,Pinjia He,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: SWE-Effi introduces new metrics to evaluate AI systems for software engineering tasks based on both accuracy and resource consumption (tokens/time), revealing key insights about cost-effectiveness and systematic challenges like token snowball effects and expensive failures.


<details>
  <summary>Details</summary>
Motivation: Existing AI for software engineering benchmarks focus only on solution accuracy, ignoring the crucial factor of cost-effectiveness in resource-constrained environments. This gap exists universally across AI systems beyond just software engineering.

Method: Developed SWE-Effi metrics that balance outcome accuracy (e.g., issue resolve rate) with resource consumption (tokens and time). Applied these metrics to re-rank popular AI systems for issue resolution on a subset of the SWE-bench benchmark.

Result: Found that effectiveness depends on how well scaffolds integrate with base models. Identified systematic challenges including token snowball effect and expensive failures where agents consume excessive resources on unsolvable tasks. Observed trade-off between token budget effectiveness and time budget effectiveness.

Conclusion: Holistic effectiveness evaluation considering both accuracy and resource consumption is crucial for practical AI deployment. The findings highlight the importance of cost-effective AI systems and provide insights for managing project budgets and enabling scalable reinforcement learning.

Abstract: The advancement of large language models (LLMs) and code agents has
demonstrated significant potential to assist software engineering (SWE) tasks,
such as autonomous issue resolution and feature addition. Existing AI for
software engineering leaderboards (e.g., SWE-bench) focus solely on solution
accuracy, ignoring the crucial factor of effectiveness in a
resource-constrained world. This is a universal problem that also exists beyond
software engineering tasks: any AI system should be more than correct - it must
also be cost-effective. To address this gap, we introduce SWE-Effi, a set of
new metrics to re-evaluate AI systems in terms of holistic effectiveness
scores. We define effectiveness as the balance between the accuracy of outcome
(e.g., issue resolve rate) and the resources consumed (e.g., token and time).
In this paper, we specifically focus on the software engineering scenario by
re-ranking popular AI systems for issue resolution on a subset of the SWE-bench
benchmark using our new multi-dimensional metrics. We found that AI system's
effectiveness depends not just on the scaffold itself, but on how well it
integrates with the base model, which is key to achieving strong performance in
a resource-efficient manner. We also identified systematic challenges such as
the "token snowball" effect and, more significantly, a pattern of "expensive
failures". In these cases, agents consume excessive resources while stuck on
unsolvable tasks - an issue that not only limits practical deployment but also
drives up the cost of failed rollouts during RL training. Lastly, we observed a
clear trade-off between effectiveness under the token budget and effectiveness
under the time budget, which plays a crucial role in managing project budgets
and enabling scalable reinforcement learning, where fast responses are
essential.

</details>


### [253] [From Hugging Face to GitHub: Tracing License Drift in the Open-Source AI Ecosystem](https://arxiv.org/abs/2509.09873)
*James Jewitt,Hao Li,Bram Adams,Gopi Krishnan Rajbahadur,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: First large-scale audit reveals 35.5% of model-to-application transitions eliminate restrictive license clauses through improper relicensing on Hugging Face and GitHub projects.


<details>
  <summary>Details</summary>
Motivation: Hidden license conflicts in open-source AI pose serious legal and ethical risks, but lack data-driven understanding of frequency, origins, and affected communities.

Method: End-to-end audit covering 364K datasets, 1.6M models, and 140K GitHub projects; developed extensible rule engine encoding 200+ SPDX and model-specific clauses for conflict detection.

Result: Systemic non-compliance found with 35.5% of model-to-application transitions eliminating restrictive licenses; prototype engine solves 86.4% of license conflicts in software applications.

Conclusion: License compliance is a critical governance challenge in open-source AI; study provides data and tools for automated, AI-aware compliance at scale.

Abstract: Hidden license conflicts in the open-source AI ecosystem pose serious legal
and ethical risks, exposing organizations to potential litigation and users to
undisclosed risk. However, the field lacks a data-driven understanding of how
frequently these conflicts occur, where they originate, and which communities
are most affected. We present the first end-to-end audit of licenses for
datasets and models on Hugging Face, as well as their downstream integration
into open-source software applications, covering 364 thousand datasets, 1.6
million models, and 140 thousand GitHub projects. Our empirical analysis
reveals systemic non-compliance in which 35.5% of model-to-application
transitions eliminate restrictive license clauses by relicensing under
permissive terms. In addition, we prototype an extensible rule engine that
encodes almost 200 SPDX and model-specific clauses for detecting license
conflicts, which can solve 86.4% of license conflicts in software applications.
To support future research, we release our dataset and the prototype engine.
Our study highlights license compliance as a critical governance challenge in
open-source AI and provides both the data and tools necessary to enable
automated, AI-aware compliance at scale.

</details>


### [254] [WALL: A Web Application for Automated Quality Assurance using Large Language Models](https://arxiv.org/abs/2509.09918)
*Seyed Moein Abtahi,Akramul Azim*

Main category: cs.SE

TL;DR: WALL is a web app that combines SonarQube and LLMs (GPT-3.5 Turbo, GPT-4o) to automate software issue detection, code revision, and evaluation, reducing human effort while maintaining quality.


<details>
  <summary>Details</summary>
Motivation: Software projects are becoming increasingly complex with growing volumes of code issues, requiring efficient tools for issue detection, resolution, and evaluation.

Method: WALL integrates SonarQube with large language models through three modules: issue extraction tool, code issues reviser, and code comparison tool to create an automated pipeline.

Result: Experiments on 563 files with 7,599+ issues showed effectiveness in reducing human effort while maintaining high-quality revisions, with hybrid LLM approach lowering costs and improving revision rates.

Conclusion: The hybrid LLM approach proves effective for automated code quality management, with future work focusing on integrating open-source LLMs and achieving full automation without human intervention.

Abstract: As software projects become increasingly complex, the volume and variety of
issues in code files have grown substantially. Addressing this challenge
requires efficient issue detection, resolution, and evaluation tools. This
paper presents WALL, a web application that integrates SonarQube and large
language models (LLMs) such as GPT-3.5 Turbo and GPT-4o to automate these
tasks. WALL comprises three modules: an issue extraction tool, code issues
reviser, and code comparison tool. Together, they enable a seamless pipeline
for detecting software issues, generating automated code revisions, and
evaluating the accuracy of revisions. Our experiments, conducted on 563 files
with over 7,599 issues, demonstrate WALL's effectiveness in reducing human
effort while maintaining high-quality revisions. Results show that employing a
hybrid approach of cost-effective and advanced LLMs can significantly lower
costs and improve revision rates. Future work aims to enhance WALL's
capabilities by integrating open-source LLMs and eliminating human
intervention, paving the way for fully automated code quality management.

</details>


### [255] [Targeted Test Selection Approach in Continuous Integration](https://arxiv.org/abs/2509.10279)
*Pavel Plyusnin,Aleksey Antonov,Vasilii Ermakov,Aleksandr Khaybriev,Margarita Kikot,Ilseyar Alimova,Stanislav Moiseev*

Main category: cs.SE

TL;DR: T-TS is a machine learning approach for test selection that uses Bags-of-Words representation of changed files, achieves 5.9x faster execution while detecting 95% of failures by selecting only 15% of tests.


<details>
  <summary>Details</summary>
Motivation: As codebases and test suites grow with frequent daily commits, efficiently managing change-based testing becomes increasingly challenging in modern software development.

Method: Machine learning approach with data representation using Bags-of-Words of changed files, incorporating cross-file and additional predictive features while avoiding coverage maps.

Result: Deployed in production, T-TS selects only 15% of tests, reduces execution time by 5.9x, accelerates pipeline by 5.6x, and detects over 95% of test failures on live industrial data.

Conclusion: T-TS provides an effective industrial solution for test selection that significantly improves efficiency while maintaining high fault detection, with implementation publicly available for research and adoption.

Abstract: In modern software development change-based testing plays a crucial role.
However, as codebases expand and test suites grow, efficiently managing the
testing process becomes increasingly challenging, especially given the high
frequency of daily code commits. We propose Targeted Test Selection (T-TS), a
machine learning approach for industrial test selection. Our key innovation is
a data representation that represent commits as Bags-of-Words of changed files,
incorporates cross-file and additional predictive features, and notably avoids
the use of coverage maps. Deployed in production, T-TS was comprehensively
evaluated against industry standards and recent methods using both internal and
public datasets, measuring time efficiency and fault detection. On live
industrial data, T-TS selects only 15% of tests, reduces execution time by
$5.9\times$, accelerates the pipeline by $5.6\times$, and detects over 95% of
test failures. The implementation is publicly available to support further
research and practical adoption.

</details>


### [256] [Generating Energy-Efficient Code via Large-Language Models -- Where are we now?](https://arxiv.org/abs/2509.10099)
*Radu Apsan,Vincenzo Stoico,Michel Albonico,Rudra Dhar,Karthik Vaidhyanathan,Ivano Malavolta*

Main category: cs.SE

TL;DR: LLM-generated Python code shows mixed energy efficiency compared to human code - better on PCs but worse on servers and Raspberry Pi, while Green software experts consistently outperform all LLMs by 17-30% across all hardware platforms.


<details>
  <summary>Details</summary>
Motivation: To empirically assess the energy efficiency of Python code generated by LLMs compared to human-written code and code developed by Green software experts, given the widespread adoption of LLMs in development pipelines.

Method: Tested 363 solutions to 9 coding problems from EvoEval benchmark using 6 widespread LLMs with 4 prompting techniques, comparing against human-developed solutions. Energy consumption measured on three hardware platforms: server, PC, and Raspberry Pi (~881h total testing time).

Result: Human solutions were 16% more energy-efficient on server and 3% on Raspberry Pi, while LLMs outperformed humans by 25% on PC. Green software expert code was consistently 17-30% more energy-efficient than all LLMs across all platforms. Prompting techniques showed inconsistent results varying by hardware.

Conclusion: Despite LLMs' good code generation capabilities, no LLM-generated code surpassed the energy efficiency of experienced Green software developers, indicating continued need for human expertise in developing energy-efficient Python code.

Abstract: Context. The rise of Large Language Models (LLMs) has led to their widespread
adoption in development pipelines. Goal. We empirically assess the energy
efficiency of Python code generated by LLMs against human-written code and code
developed by a Green software expert. Method. We test 363 solutions to 9 coding
problems from the EvoEval benchmark using 6 widespread LLMs with 4 prompting
techniques, and comparing them to human-developed solutions. Energy consumption
is measured on three different hardware platforms: a server, a PC, and a
Raspberry Pi for a total of ~881h (36.7 days). Results. Human solutions are 16%
more energy-efficient on the server and 3% on the Raspberry Pi, while LLMs
outperform human developers by 25% on the PC. Prompting does not consistently
lead to energy savings, where the most energy-efficient prompts vary by
hardware platform. The code developed by a Green software expert is
consistently more energy-efficient by at least 17% to 30% against all LLMs on
all hardware platforms. Conclusions. Even though LLMs exhibit relatively good
code generation capabilities, no LLM-generated code was more energy-efficient
than that of an experienced Green software developer, suggesting that as of
today there is still a great need of human expertise for developing
energy-efficient Python code.

</details>
