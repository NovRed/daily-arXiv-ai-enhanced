{"id": "2508.10904", "pdf": "https://arxiv.org/pdf/2508.10904", "abs": "https://arxiv.org/abs/2508.10904", "authors": ["Jie Lei", "Ruofan Jia", "J. Andrew Zhang", "Hao Zhang"], "title": "A2HCoder: An LLM-Driven Coding Agent for Hierarchical Algorithm-to-HDL Translation", "categories": ["cs.CL", "cs.AR", "cs.PL"], "comment": "15 pages, 6 figures", "summary": "In wireless communication systems, stringent requirements such as ultra-low\nlatency and power consumption have significantly increased the demand for\nefficient algorithm-to-hardware deployment. However, a persistent and\nsubstantial gap remains between algorithm design and hardware implementation.\nBridging this gap traditionally requires extensive domain expertise and\ntime-consuming manual development, due to fundamental mismatches between\nhigh-level programming languages like MATLAB and hardware description languages\n(HDLs) such as Verilog-in terms of memory access patterns, data processing\nmanners, and datatype representations. To address this challenge, we propose\nA2HCoder: a Hierarchical Algorithm-to-HDL Coding Agent, powered by large\nlanguage models (LLMs), designed to enable agile and reliable\nalgorithm-to-hardware translation. A2HCoder introduces a hierarchical framework\nthat enhances both robustness and interpretability while suppressing common\nhallucination issues in LLM-generated code. In the horizontal dimension,\nA2HCoder decomposes complex algorithms into modular functional blocks,\nsimplifying code generation and improving consistency. In the vertical\ndimension, instead of relying on end-to-end generation, A2HCoder performs\nstep-by-step, fine-grained translation, leveraging external toolchains such as\nMATLAB and Vitis HLS for debugging and circuit-level synthesis. This structured\nprocess significantly mitigates hallucinations and ensures hardware-level\ncorrectness. We validate A2HCoder through a real-world deployment case in the\n5G wireless communication domain, demonstrating its practicality, reliability,\nand deployment efficiency.", "AI": {"tldr": "A2HCoder is a hierarchical algorithm-to-HDL coding agent using LLMs to bridge the gap between algorithm design and hardware implementation, improving robustness and interpretability.", "motivation": "The persistent gap between algorithm design and hardware implementation requires domain expertise and manual effort, which A2HCoder aims to address.", "method": "A2HCoder uses a hierarchical framework, decomposing algorithms into modular blocks and performing step-by-step translation with external toolchains for debugging and synthesis.", "result": "Validated in 5G wireless communication, A2HCoder shows practicality, reliability, and efficiency in deployment.", "conclusion": "A2HCoder effectively bridges the algorithm-to-hardware gap, mitigating hallucinations and ensuring correctness."}}
{"id": "2508.10906", "pdf": "https://arxiv.org/pdf/2508.10906", "abs": "https://arxiv.org/abs/2508.10906", "authors": ["Sihan Chen", "John P. Lalor", "Yi Yang", "Ahmed Abbasi"], "title": "PersonaTwin: A Multi-Tier Prompt Conditioning Framework for Generating and Evaluating Personalized Digital Twins", "categories": ["cs.CL"], "comment": "Presented at the Generation, Evaluation & Metrics (GEM) Workshop at\n  ACL 2025", "summary": "While large language models (LLMs) afford new possibilities for user modeling\nand approximation of human behaviors, they often fail to capture the\nmultidimensional nuances of individual users. In this work, we introduce\nPersonaTwin, a multi-tier prompt conditioning framework that builds adaptive\ndigital twins by integrating demographic, behavioral, and psychometric data.\nUsing a comprehensive data set in the healthcare context of more than 8,500\nindividuals, we systematically benchmark PersonaTwin against standard LLM\noutputs, and our rigorous evaluation unites state-of-the-art text similarity\nmetrics with dedicated demographic parity assessments, ensuring that generated\nresponses remain accurate and unbiased. Experimental results show that our\nframework produces simulation fidelity on par with oracle settings. Moreover,\ndownstream models trained on persona-twins approximate models trained on\nindividuals in terms of prediction and fairness metrics across both\nGPT-4o-based and Llama-based models. Together, these findings underscore the\npotential for LLM digital twin-based approaches in producing realistic and\nemotionally nuanced user simulations, offering a powerful tool for personalized\ndigital user modeling and behavior analysis.", "AI": {"tldr": "PersonaTwin, a multi-tier prompt framework, enhances LLMs by integrating diverse user data, improving simulation fidelity and fairness in digital twin generation.", "motivation": "LLMs often miss nuanced user behaviors; PersonaTwin aims to address this by creating adaptive digital twins for realistic user modeling.", "method": "Uses demographic, behavioral, and psychometric data in a multi-tier prompt framework, benchmarked against standard LLMs with text similarity and fairness metrics.", "result": "Achieves simulation fidelity comparable to oracle settings and matches individual-trained models in prediction and fairness.", "conclusion": "PersonaTwin demonstrates the potential of LLM-based digital twins for nuanced, personalized user simulations."}}
{"id": "2508.10925", "pdf": "https://arxiv.org/pdf/2508.10925", "abs": "https://arxiv.org/abs/2508.10925", "authors": ["OpenAI", ":", "Sandhini Agarwal", "Lama Ahmad", "Jason Ai", "Sam Altman", "Andy Applebaum", "Edwin Arbus", "Rahul K. Arora", "Yu Bai", "Bowen Baker", "Haiming Bao", "Boaz Barak", "Ally Bennett", "Tyler Bertao", "Nivedita Brett", "Eugene Brevdo", "Greg Brockman", "Sebastien Bubeck", "Che Chang", "Kai Chen", "Mark Chen", "Enoch Cheung", "Aidan Clark", "Dan Cook", "Marat Dukhan", "Casey Dvorak", "Kevin Fives", "Vlad Fomenko", "Timur Garipov", "Kristian Georgiev", "Mia Glaese", "Tarun Gogineni", "Adam Goucher", "Lukas Gross", "Katia Gil Guzman", "John Hallman", "Jackie Hehir", "Johannes Heidecke", "Alec Helyar", "Haitang Hu", "Romain Huet", "Jacob Huh", "Saachi Jain", "Zach Johnson", "Chris Koch", "Irina Kofman", "Dominik Kundel", "Jason Kwon", "Volodymyr Kyrylov", "Elaine Ya Le", "Guillaume Leclerc", "James Park Lennon", "Scott Lessans", "Mario Lezcano-Casado", "Yuanzhi Li", "Zhuohan Li", "Ji Lin", "Jordan Liss", "Lily", "Liu", "Jiancheng Liu", "Kevin Lu", "Chris Lu", "Zoran Martinovic", "Lindsay McCallum", "Josh McGrath", "Scott McKinney", "Aidan McLaughlin", "Song Mei", "Steve Mostovoy", "Tong Mu", "Gideon Myles", "Alexander Neitz", "Alex Nichol", "Jakub Pachocki", "Alex Paino", "Dana Palmie", "Ashley Pantuliano", "Giambattista Parascandolo", "Jongsoo Park", "Leher Pathak", "Carolina Paz", "Ludovic Peran", "Dmitry Pimenov", "Michelle Pokrass", "Elizabeth Proehl", "Huida Qiu", "Gaby Raila", "Filippo Raso", "Hongyu Ren", "Kimmy Richardson", "David Robinson", "Bob Rotsted", "Hadi Salman", "Suvansh Sanjeev", "Max Schwarzer", "D. Sculley", "Harshit Sikchi", "Kendal Simon", "Karan Singhal", "Yang Song", "Dane Stuckey", "Zhiqing Sun", "Philippe Tillet", "Sam Toizer", "Foivos Tsimpourlas", "Nikhil Vyas", "Eric Wallace", "Xin Wang", "Miles Wang", "Olivia Watkins", "Kevin Weil", "Amy Wendling", "Kevin Whinnery", "Cedric Whitney", "Hannah Wong", "Lin Yang", "Yu Yang", "Michihiro Yasunaga", "Kristen Ying", "Wojciech Zaremba", "Wenting Zhan", "Cyril Zhang", "Brian Zhang", "Eddie Zhang", "Shengjia Zhao"], "title": "gpt-oss-120b & gpt-oss-20b Model Card", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present gpt-oss-120b and gpt-oss-20b, two open-weight reasoning models\nthat push the frontier of accuracy and inference cost. The models use an\nefficient mixture-of-expert transformer architecture and are trained using\nlarge-scale distillation and reinforcement learning. We optimize the models to\nhave strong agentic capabilities (deep research browsing, python tool use, and\nsupport for developer-provided functions), all while using a rendered chat\nformat that enables clear instruction following and role delineation. Both\nmodels achieve strong results on benchmarks ranging from mathematics, coding,\nand safety. We release the model weights, inference implementations, tool\nenvironments, and tokenizers under an Apache 2.0 license to enable broad use\nand further research.", "AI": {"tldr": "GPT-OSS-120B and GPT-OSS-20B are open-weight reasoning models with high accuracy and cost efficiency, optimized for agentic tasks and released under Apache 2.0.", "motivation": "To advance the frontier of reasoning models by balancing accuracy, inference cost, and agentic capabilities.", "method": "Uses a mixture-of-expert transformer architecture, trained with large-scale distillation and reinforcement learning.", "result": "Strong performance on benchmarks in mathematics, coding, and safety.", "conclusion": "The models and tools are released openly to encourage broad use and further research."}}
{"id": "2508.10927", "pdf": "https://arxiv.org/pdf/2508.10927", "abs": "https://arxiv.org/abs/2508.10927", "authors": ["Jiaxin Pei", "Soumya Vadlamannati", "Liang-Kang Huang", "Daniel Preotiuc-Pietro", "Xinyu Hua"], "title": "Modeling and Detecting Company Risks from News: A Case Study in Bloomberg News", "categories": ["cs.CL", "cs.AI", "cs.CE", "cs.LG"], "comment": null, "summary": "Identifying risks associated with a company is important to investors and the\nwell-being of the overall financial market. In this study, we build a\ncomputational framework to automatically extract company risk factors from news\narticles. Our newly proposed schema comprises seven distinct aspects, such as\nsupply chain, regulations, and competitions. We sample and annotate 744 news\narticles and benchmark various machine learning models. While large language\nmodels have achieved huge progress in various types of NLP tasks, our\nexperiment shows that zero-shot and few-shot prompting state-of-the-art LLMs\n(e.g. LLaMA-2) can only achieve moderate to low performances in identifying\nrisk factors. And fine-tuned pre-trained language models are performing better\non most of the risk factors. Using this model, we analyze over 277K Bloomberg\nnews articles and demonstrate that identifying risk factors from news could\nprovide extensive insight into the operations of companies and industries.", "AI": {"tldr": "A computational framework is developed to extract company risk factors from news articles, benchmarking ML models and showing fine-tuned models outperform LLMs like LLaMA-2.", "motivation": "Identifying company risks is crucial for investors and financial markets, necessitating automated extraction from news.", "method": "A schema with seven risk aspects is proposed. 744 news articles are annotated, and ML models (including LLMs) are benchmarked.", "result": "Fine-tuned models outperform zero-shot/few-shot LLMs. Analysis of 277K Bloomberg articles shows insights into company/industry risks.", "conclusion": "Automated risk factor extraction from news is viable and insightful, with fine-tuned models being more effective than LLMs."}}
{"id": "2508.10976", "pdf": "https://arxiv.org/pdf/2508.10976", "abs": "https://arxiv.org/abs/2508.10976", "authors": ["Martin Diller", "Sarah Alice Gaggl", "Philipp Hanisch", "Giuseppina Monterosso", "Fritz Rauschenbach"], "title": "Grounding Rule-Based Argumentation Using Datalog", "categories": ["cs.AI"], "comment": null, "summary": "ASPIC+ is one of the main general frameworks for rule-based argumentation for\nAI. Although first-order rules are commonly used in ASPIC+ examples, most\nexisting approaches to reason over rule-based argumentation only support\npropositional rules. To enable reasoning over first-order instances, a\npreliminary grounding step is required. As groundings can lead to an\nexponential increase in the size of the input theories, intelligent procedures\nare needed. However, there is a lack of dedicated solutions for ASPIC+.\nTherefore, we propose an intelligent grounding procedure that keeps the size of\nthe grounding manageable while preserving the correctness of the reasoning\nprocess. To this end, we translate the first-order ASPIC+ instance into a\nDatalog program and query a Datalog engine to obtain ground substitutions to\nperform the grounding of rules and contraries. Additionally, we propose\nsimplifications specific to the ASPIC+ formalism to avoid grounding of rules\nthat have no influence on the reasoning process. Finally, we performed an\nempirical evaluation of a prototypical implementation to show scalability.", "AI": {"tldr": "The paper proposes an intelligent grounding procedure for first-order ASPIC+ instances to manage exponential growth in input theories, using Datalog translation and specific simplifications.", "motivation": "Existing approaches for rule-based argumentation in ASPIC+ lack support for first-order rules, requiring inefficient grounding steps.", "method": "Translate first-order ASPIC+ into Datalog, query for ground substitutions, and apply ASPIC+-specific simplifications to avoid unnecessary grounding.", "result": "The proposed method preserves reasoning correctness while keeping grounding manageable, with empirical evaluation showing scalability.", "conclusion": "The intelligent grounding procedure effectively addresses the limitations of existing approaches for first-order ASPIC+ reasoning."}}
{"id": "2508.10918", "pdf": "https://arxiv.org/pdf/2508.10918", "abs": "https://arxiv.org/abs/2508.10918", "authors": ["Samantha Aziz", "Oleg Komogortsev"], "title": "Privacy Enhancement for Gaze Data Using a Noise-Infused Autoencoder", "categories": ["cs.CV", "cs.HC"], "comment": "IJCB 2025; 11 pages, 7 figures", "summary": "We present a privacy-enhancing mechanism for gaze signals using a\nlatent-noise autoencoder that prevents users from being re-identified across\nplay sessions without their consent, while retaining the usability of the data\nfor benign tasks. We evaluate privacy-utility trade-offs across biometric\nidentification and gaze prediction tasks, showing that our approach\nsignificantly reduces biometric identifiability with minimal utility\ndegradation. Unlike prior methods in this direction, our framework retains\nphysiologically plausible gaze patterns suitable for downstream use, which\nproduces favorable privacy-utility trade-off. This work advances privacy in\ngaze-based systems by providing a usable and effective mechanism for protecting\nsensitive gaze data.", "AI": {"tldr": "A privacy-enhancing mechanism using a latent-noise autoencoder protects gaze data from re-identification while maintaining usability for benign tasks.", "motivation": "To address privacy concerns in gaze-based systems by preventing unauthorized re-identification without compromising data utility.", "method": "Uses a latent-noise autoencoder to anonymize gaze signals, balancing privacy and utility. Evaluated on biometric identification and gaze prediction tasks.", "result": "Significantly reduces biometric identifiability with minimal utility loss, retaining physiologically plausible gaze patterns.", "conclusion": "The framework effectively protects sensitive gaze data while preserving usability, advancing privacy in gaze-based systems."}}
{"id": "2508.10926", "pdf": "https://arxiv.org/pdf/2508.10926", "abs": "https://arxiv.org/abs/2508.10926", "authors": ["DongSeong-Yoon"], "title": "A Cooperative Game-Based Multi-Criteria Weighted Ensemble Approach for Multi-Class Classification", "categories": ["cs.LG"], "comment": "English translation of the author's pre-revision version of the\n  article published in J-KICS 50(4):561-571 (2025), DOI\n  10.7840/kics.2025.50.4.561. Posted with permission from KICS (Aug 7, 2025).\n  The published version may differ", "summary": "Since the Fourth Industrial Revolution, AI technology has been widely used in\nmany fields, but there are several limitations that need to be overcome,\nincluding overfitting/underfitting, class imbalance, and the limitations of\nrepresentation (hypothesis space) due to the characteristics of different\nmodels. As a method to overcome these problems, ensemble, commonly known as\nmodel combining, is being extensively used in the field of machine learning.\nAmong ensemble learning methods, voting ensembles have been studied with\nvarious weighting methods, showing performance improvements. However, the\nexisting methods that reflect the pre-information of classifiers in weights\nconsider only one evaluation criterion, which limits the reflection of various\ninformation that should be considered in a model realistically. Therefore, this\npaper proposes a method of making decisions considering various information\nthrough cooperative games in multi-criteria situations. Using this method,\nvarious types of information known beforehand in classifiers can be\nsimultaneously considered and reflected, leading to appropriate weight\ndistribution and performance improvement. The machine learning algorithms were\napplied to the Open-ML-CC18 dataset and compared with existing ensemble\nweighting methods. The experimental results showed superior performance\ncompared to other weighting methods.", "AI": {"tldr": "The paper proposes a cooperative game-based method for ensemble learning to address limitations like overfitting and class imbalance by considering multiple evaluation criteria for weight distribution, outperforming existing methods.", "motivation": "To overcome limitations in AI like overfitting, class imbalance, and restricted hypothesis space by improving ensemble learning with multi-criteria weight reflection.", "method": "Proposes a cooperative game-based approach for ensemble learning, integrating multiple pre-information criteria for weight distribution.", "result": "Applied to Open-ML-CC18 dataset, the method showed superior performance compared to existing ensemble weighting techniques.", "conclusion": "The cooperative game-based method effectively improves ensemble learning by leveraging multi-criteria information for better weight distribution and performance."}}
{"id": "2508.10971", "pdf": "https://arxiv.org/pdf/2508.10971", "abs": "https://arxiv.org/abs/2508.10971", "authors": ["Nasim Shirvani-Mahdavi", "Chengkai Li"], "title": "Rule2Text: A Framework for Generating and Evaluating Natural Language Explanations of Knowledge Graph Rules", "categories": ["cs.CL", "cs.AI"], "comment": "arXiv admin note: text overlap with arXiv:2507.23740", "summary": "Knowledge graphs (KGs) can be enhanced through rule mining; however, the\nresulting logical rules are often difficult for humans to interpret due to\ntheir inherent complexity and the idiosyncratic labeling conventions of\nindividual KGs. This work presents Rule2Text, a comprehensive framework that\nleverages large language models (LLMs) to generate natural language\nexplanations for mined logical rules, thereby improving KG accessibility and\nusability. We conduct extensive experiments using multiple datasets, including\nFreebase variants (FB-CVT-REV, FB+CVT-REV, and FB15k-237) as well as the\nogbl-biokg dataset, with rules mined using AMIE 3.5.1. We systematically\nevaluate several LLMs across a comprehensive range of prompting strategies,\nincluding zero-shot, few-shot, variable type incorporation, and\nChain-of-Thought reasoning. To systematically assess models' performance, we\nconduct a human evaluation of generated explanations on correctness and\nclarity. To address evaluation scalability, we develop and validate an\nLLM-as-a-judge framework that demonstrates strong agreement with human\nevaluators. Leveraging the best-performing model (Gemini 2.0 Flash), LLM judge,\nand human-in-the-loop feedback, we construct high-quality ground truth\ndatasets, which we use to fine-tune the open-source Zephyr model. Our results\ndemonstrate significant improvements in explanation quality after fine-tuning,\nwith particularly strong gains in the domain-specific dataset. Additionally, we\nintegrate a type inference module to support KGs lacking explicit type\ninformation. All code and data are publicly available at\nhttps://github.com/idirlab/KGRule2NL.", "AI": {"tldr": "Rule2Text uses LLMs to generate natural language explanations for complex logical rules in KGs, improving interpretability. It evaluates multiple LLMs and prompting strategies, fine-tunes Zephyr, and includes a type inference module.", "motivation": "Logical rules in KGs are hard to interpret due to complexity and labeling conventions. Rule2Text aims to enhance KG accessibility by generating human-readable explanations.", "method": "The framework evaluates LLMs (e.g., Gemini 2.0 Flash) with various prompting strategies, conducts human and LLM-as-a-judge evaluations, and fine-tunes Zephyr using high-quality datasets.", "result": "Fine-tuning Zephyr improves explanation quality, especially in domain-specific datasets. The LLM-as-a-judge framework aligns well with human evaluations.", "conclusion": "Rule2Text successfully enhances KG usability by generating clear explanations, with fine-tuned models showing significant improvements. The framework is scalable and adaptable to KGs lacking type information."}}
{"id": "2508.11070", "pdf": "https://arxiv.org/pdf/2508.11070", "abs": "https://arxiv.org/abs/2508.11070", "authors": ["Zahra Khotanlou", "Kate Larson", "Amir-Hossein Karimi"], "title": "From Individual to Multi-Agent Algorithmic Recourse: Minimizing the Welfare Gap via Capacitated Bipartite Matching", "categories": ["cs.AI"], "comment": null, "summary": "Decision makers are increasingly relying on machine learning in sensitive\nsituations. In such settings, algorithmic recourse aims to provide individuals\nwith actionable and minimally costly steps to reverse unfavorable AI-driven\ndecisions. While existing research predominantly focuses on single-individual\n(i.e., seeker) and single-model (i.e., provider) scenarios, real-world\napplications often involve multiple interacting stakeholders. Optimizing\noutcomes for seekers under an individual welfare approach overlooks the\ninherently multi-agent nature of real-world systems, where individuals interact\nand compete for limited resources. To address this, we introduce a novel\nframework for multi-agent algorithmic recourse that accounts for multiple\nrecourse seekers and recourse providers. We model this many-to-many interaction\nas a capacitated weighted bipartite matching problem, where matches are guided\nby both recourse cost and provider capacity. Edge weights, reflecting recourse\ncosts, are optimized for social welfare while quantifying the welfare gap\nbetween individual welfare and this collectively feasible outcome. We propose a\nthree-layer optimization framework: (1) basic capacitated matching, (2) optimal\ncapacity redistribution to minimize the welfare gap, and (3) cost-aware\noptimization balancing welfare maximization with capacity adjustment costs.\nExperimental validation on synthetic and real-world datasets demonstrates that\nour framework enables the many-to-many algorithmic recourse to achieve\nnear-optimal welfare with minimum modification in system settings. This work\nextends algorithmic recourse from individual recommendations to system-level\ndesign, providing a tractable path toward higher social welfare while\nmaintaining individual actionability.", "AI": {"tldr": "The paper introduces a multi-agent algorithmic recourse framework for scenarios with multiple stakeholders, optimizing social welfare while balancing individual actionability.", "motivation": "Existing algorithmic recourse methods focus on single-individual and single-model scenarios, neglecting the multi-agent nature of real-world systems where individuals compete for limited resources.", "method": "The framework models many-to-many interactions as a capacitated weighted bipartite matching problem, with three optimization layers: basic matching, capacity redistribution, and cost-aware optimization.", "result": "Experiments show the framework achieves near-optimal welfare with minimal system adjustments.", "conclusion": "The work extends algorithmic recourse to system-level design, enhancing social welfare while preserving individual actionability."}}
{"id": "2508.10922", "pdf": "https://arxiv.org/pdf/2508.10922", "abs": "https://arxiv.org/abs/2508.10922", "authors": ["Jianlong Wu", "Wei Liu", "Ye Liu", "Meng Liu", "Liqiang Nie", "Zhouchen Lin", "Chang Wen Chen"], "title": "A Survey on Video Temporal Grounding with Multimodal Large Language Model", "categories": ["cs.CV"], "comment": "20 pages,6 figures,survey", "summary": "The recent advancement in video temporal grounding (VTG) has significantly\nenhanced fine-grained video understanding, primarily driven by multimodal large\nlanguage models (MLLMs). With superior multimodal comprehension and reasoning\nabilities, VTG approaches based on MLLMs (VTG-MLLMs) are gradually surpassing\ntraditional fine-tuned methods. They not only achieve competitive performance\nbut also excel in generalization across zero-shot, multi-task, and multi-domain\nsettings. Despite extensive surveys on general video-language understanding,\ncomprehensive reviews specifically addressing VTG-MLLMs remain scarce. To fill\nthis gap, this survey systematically examines current research on VTG-MLLMs\nthrough a three-dimensional taxonomy: 1) the functional roles of MLLMs,\nhighlighting their architectural significance; 2) training paradigms, analyzing\nstrategies for temporal reasoning and task adaptation; and 3) video feature\nprocessing techniques, which determine spatiotemporal representation\neffectiveness. We further discuss benchmark datasets, evaluation protocols, and\nsummarize empirical findings. Finally, we identify existing limitations and\npropose promising research directions. For additional resources and details,\nreaders are encouraged to visit our repository at\nhttps://github.com/ki-lw/Awesome-MLLMs-for-Video-Temporal-Grounding.", "AI": {"tldr": "A survey on video temporal grounding (VTG) using multimodal large language models (MLLMs), covering taxonomy, benchmarks, and future directions.", "motivation": "To address the lack of comprehensive reviews on VTG-MLLMs despite their superior performance and generalization.", "method": "Systematic examination through a three-dimensional taxonomy: functional roles of MLLMs, training paradigms, and video feature processing.", "result": "Summarizes empirical findings, benchmarks, and evaluation protocols, highlighting VTG-MLLMs' competitive performance and generalization.", "conclusion": "Identifies limitations and proposes future research directions, with additional resources provided."}}
{"id": "2508.10948", "pdf": "https://arxiv.org/pdf/2508.10948", "abs": "https://arxiv.org/abs/2508.10948", "authors": ["Shruthan Radhakrishna", "Soham Parikh", "Gopal Sarda", "Anil Turkkan", "Quaizar Vohra", "Raymond Li", "Dhruv Jhamb", "Kelechi Ogueji", "Aanjaneya Shukla", "Oluwanifemi Bamgbose", "Toby Liang", "Luke Kumar", "Oleksiy Ostapenko", "Shiva Krishna Reddy Malay", "Aman Tiwari", "Tara Bogavelli", "Vikas Yadav", "Jash Mehta", "Saloni Mittal", "Akshay Kalkunte", "Pulkit Pattnaik", "Khalil Slimi", "Anirudh Sreeram", "Jishnu Nair", "Akintunde Oladipo", "Shashank Maiya", "Khyati Mahajan", "Rishabh Maheshwary", "Masoud Hashemi", "Sai Rajeswar Mudumba", "Sathwik Tejaswi Madhusudhan", "Torsten Scholak", "Sebastien Paquet", "Sagar Davasam", "Srinivas Sunkara"], "title": "Apriel-Nemotron-15B-Thinker", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "While large language models (LLMs) have achieved remarkable reasoning\ncapabilities across domains like code, math and other enterprise tasks, their\nsignificant memory and computational costs often preclude their use in\npractical enterprise settings. To this end, we introduce\nApriel-Nemotron-15B-Thinker, a 15-billion parameter model in the ServiceNow\nApriel SLM series that achieves performance against medium sized\nstate-of-the-art models such as o1-mini, QWQ32B, and EXAONE-Deep-32B while\nmaintaining only half the memory footprint of those alternatives.\nApriel-Nemotron-15B-Thinker model is trained in a four stage training pipeline\nincluding 1) Base Model upscaling, 2) Continual Pre-training 3) Supervised\nFine-tuning (SFT) and 4) Reinforcement Learning using GRPO. Comprehensive\nevaluations across a diverse suite of benchmarks consistently demonstrate that\nour Apriel-Nemotron-15B-Thinker model matches or exceeds the performance of its\n32-billion parameter counterparts, despite being less than half their size.", "AI": {"tldr": "Apriel-Nemotron-15B-Thinker is a 15B-parameter model that matches or outperforms larger 32B-parameter models while using half the memory.", "motivation": "Large language models (LLMs) are resource-intensive, limiting their practical use in enterprise settings.", "method": "Four-stage training pipeline: Base Model upscaling, Continual Pre-training, Supervised Fine-tuning (SFT), and Reinforcement Learning (GRPO).", "result": "Matches or exceeds performance of 32B-parameter models despite being smaller.", "conclusion": "Apriel-Nemotron-15B-Thinker offers efficient, high-performance LLM for enterprise use."}}
{"id": "2508.10995", "pdf": "https://arxiv.org/pdf/2508.10995", "abs": "https://arxiv.org/abs/2508.10995", "authors": ["Tejomay Kishor Padole", "Suyash P Awate", "Pushpak Bhattacharyya"], "title": "Improving Text Style Transfer using Masked Diffusion Language Models with Inference-time Scaling", "categories": ["cs.CL", "cs.LG"], "comment": "Accepted as a main conference submission in the European Conference\n  on Artificial Intelligence (ECAI 2025)", "summary": "Masked diffusion language models (MDMs) have recently gained traction as a\nviable generative framework for natural language. This can be attributed to its\nscalability and ease of training compared to other diffusion model paradigms\nfor discrete data, establishing itself as the state-of-the-art\nnon-autoregressive generator for discrete data. Diffusion models, in general,\nhave shown excellent ability to improve the generation quality by leveraging\ninference-time scaling either by increasing the number of denoising steps or by\nusing external verifiers on top of the outputs of each step to guide the\ngeneration. In this work, we propose a verifier-based inference-time scaling\nmethod that aids in finding a better candidate generation during the denoising\nprocess of the MDM. Our experiments demonstrate the application of MDMs for\nstandard text-style transfer tasks and establish MDMs as a better alternative\nto autoregressive language models. Additionally, we show that a simple\nsoft-value-based verifier setup for MDMs using off-the-shelf pre-trained\nembedding models leads to significant gains in generation quality even when\nused on top of typical classifier-free guidance setups in the existing\nliterature.", "AI": {"tldr": "Masked diffusion language models (MDMs) are a scalable and easy-to-train generative framework for natural language, outperforming autoregressive models. This work introduces a verifier-based inference-time scaling method to enhance MDM generation quality.", "motivation": "To improve the generation quality of MDMs by leveraging inference-time scaling and external verifiers, making them a stronger alternative to autoregressive models.", "method": "Proposes a verifier-based inference-time scaling method for MDMs, utilizing off-the-shelf pre-trained embedding models to guide generation during denoising.", "result": "MDMs achieve better performance in text-style transfer tasks and show significant gains in generation quality with the proposed verifier setup.", "conclusion": "MDMs, enhanced with verifier-based scaling, are a superior non-autoregressive generative framework for discrete data, offering scalability and improved quality."}}
{"id": "2508.11085", "pdf": "https://arxiv.org/pdf/2508.11085", "abs": "https://arxiv.org/abs/2508.11085", "authors": ["Qingqing Wang", "Liqiang Xiao", "Chang Chang"], "title": "Learn to optimize for automatic proton PBS treatment planning for H&N cancers", "categories": ["cs.AI", "cs.LG"], "comment": "27 pages, 4 figures", "summary": "Proton PBS treatment planning for H&N cancers involves numerous conflicting\nobjectives, requiring significant effort from human planners to balance and\nsatisfy multiple clinical goals during planning. To achieve this,\nexperience-demanding objective parameter adjustment and computationally\nexpensive inverse optimization are performed iteratively. Extensive efforts\nhave been made to automatically adjust objective parameters, but the most\ntime-consuming component, i.e., inverse optimization, still relies heavily on\ntheory-driven approaches. We propose a data-driven inverse optimizer and\nintegrate it into a PPO-based automatic treatment planning framework to\nautomatically generate high-quality plans within a clinical acceptable planning\ntime. The inverse optimizer is a L2O method that predicts update steps by\nlearning from the task-specific data distribution. For the first time, we\nintegrate techniques designed for long-context processing, originally developed\nfor LLMs, into a Transformer-based L2O framework to address the scalability\nissue of existing L2O methods. The PPO framework functions as an outer-loop\nvirtual planner, autonomously adjusting objective parameters through a policy\nnetwork, and the dose predictor is used to initialize objective parameters. The\ninner-loop L2O inverse optimizer computes machine-deliverable MU values based\non objectives refined by the PPO policy network. 97 patients are collected in\nthis study, and compared with L-BFGSB, our L2O-based inverse optimizer improves\nthe effectiveness and efficiency by 22.97% and 36.41%, respectively. In\nconjunction with the PPO-based learned virtual planner, plans generated by our\nframework within an average of 2.55 hours show improved or comparable OAR\nsparing with superior target coverage for patients with different prescription\ndose levels, number of target volumes, beam angles, etc., compared with\nhuman-generated plans.", "AI": {"tldr": "A data-driven inverse optimizer integrated into a PPO-based framework automates proton PBS treatment planning for H&N cancers, improving efficiency and plan quality.", "motivation": "Manual treatment planning for H&N cancers is time-consuming and relies heavily on human expertise, especially in inverse optimization. Automating this process can save time and improve plan quality.", "method": "The proposed method combines a PPO-based virtual planner for objective parameter adjustment and a Transformer-based L2O inverse optimizer for machine-deliverable MU values. Techniques from LLMs are adapted for scalability.", "result": "The L2O optimizer improves effectiveness by 22.97% and efficiency by 36.41% compared to L-BFGSB. Plans generated in ~2.55 hours match or exceed human-generated plans in quality.", "conclusion": "The framework successfully automates treatment planning, reducing reliance on human expertise while maintaining or improving plan quality."}}
{"id": "2508.10931", "pdf": "https://arxiv.org/pdf/2508.10931", "abs": "https://arxiv.org/abs/2508.10931", "authors": ["Wenqi Guo", "Shan Du"], "title": "VSF: Simple, Efficient, and Effective Negative Guidance in Few-Step Image Generation Models By \\underline{V}alue \\underline{S}ign \\underline{F}lip", "categories": ["cs.CV"], "comment": null, "summary": "We introduce Value Sign Flip (VSF), a simple and efficient method for\nincorporating negative prompt guidance in few-step diffusion and flow-matching\nimage generation models. Unlike existing approaches such as classifier-free\nguidance (CFG), NASA, and NAG, VSF dynamically suppresses undesired content by\nflipping the sign of attention values from negative prompts. Our method\nrequires only small computational overhead and integrates effectively with\nMMDiT-style architectures such as Stable Diffusion 3.5 Turbo, as well as\ncross-attention-based models like Wan. We validate VSF on challenging datasets\nwith complex prompt pairs and demonstrate superior performance in both static\nimage and video generation tasks. Experimental results show that VSF\nsignificantly improves negative prompt adherence compared to prior methods in\nfew-step models, and even CFG in non-few-step models, while maintaining\ncompetitive image quality. Code and ComfyUI node are available in\nhttps://github.com/weathon/VSF/tree/main.", "AI": {"tldr": "VSF is a method for improving negative prompt guidance in diffusion and flow-matching models by flipping attention values, outperforming existing techniques with minimal overhead.", "motivation": "Existing methods like CFG, NASA, and NAG struggle with dynamically suppressing undesired content in few-step models. VSF aims to address this limitation efficiently.", "method": "VSF dynamically flips the sign of attention values from negative prompts, integrating seamlessly with architectures like Stable Diffusion 3.5 Turbo and Wan.", "result": "VSF shows superior performance in negative prompt adherence for few-step and non-few-step models, maintaining competitive image quality.", "conclusion": "VSF is a simple, efficient, and effective solution for negative prompt guidance in image and video generation tasks."}}
{"id": "2508.10954", "pdf": "https://arxiv.org/pdf/2508.10954", "abs": "https://arxiv.org/abs/2508.10954", "authors": ["Gyutae Oh", "Jitae Shin"], "title": "Towards Efficient Prompt-based Continual Learning in Distributed Medical AI", "categories": ["cs.LG", "cs.AI"], "comment": "10p", "summary": "Modern AI models achieve state-of-the-art performance with large-scale,\nhigh-quality datasets; however, ethical, social, and institutional constraints\nin the medical domain severely restrict data sharing, rendering centralized\nlearning nearly impossible. Each institution must incrementally update models\nusing only local data. Traditional training overfits new samples and suffers\nfrom catastrophic forgetting, losing previously acquired knowledge. Medical\ndata distributions also shift due to varying diagnostic equipment and\ndemographics. Although continual learning (CL) has advanced, most methods\naddress natural images, leaving medical-domain-specific CL underexplored. We\npropose a prompt-based continual learning (PCL) approach featuring a unified\nprompt pool with a minimal expansion strategy: by expanding and freezing a\nsubset of prompts, our method reduces computational overhead, and a novel\nregularization term balances retention and adaptation. Experiments on three\ndiabetic retinopathy datasets Aptos2019, LI2019, and Diabetic Retinopathy\nDetection show our model improves final classification accuracy by at least 10%\nand F1-score by 9 points over state-of-the-art approaches while lowering\ninference cost. We anticipate this study will drive sustainable medical AI\nadvances, enabling real-time diagnosis, patient monitoring, and telemedicine\napplications in distributed healthcare. Code will be released upon acceptance", "AI": {"tldr": "The paper proposes a prompt-based continual learning (PCL) method for medical AI, addressing data-sharing constraints and catastrophic forgetting, achieving significant performance improvements over existing methods.", "motivation": "Ethical and institutional barriers in the medical domain limit data sharing, necessitating local model updates. Traditional methods overfit and forget past knowledge, while existing CL methods are not tailored for medical data.", "method": "PCL uses a unified prompt pool with minimal expansion, freezing subsets to reduce overhead, and introduces a novel regularization term to balance retention and adaptation.", "result": "Experiments on diabetic retinopathy datasets show PCL improves accuracy by 10% and F1-score by 9 points over state-of-the-art methods, with lower inference costs.", "conclusion": "PCL enables sustainable medical AI advances, supporting real-time diagnosis and telemedicine in distributed healthcare. Code will be released upon acceptance."}}
{"id": "2508.11009", "pdf": "https://arxiv.org/pdf/2508.11009", "abs": "https://arxiv.org/abs/2508.11009", "authors": ["Wenpeng Xing", "Lanyi Wei", "Haixiao Hu", "Rongchang Li", "Mohan Li", "Changting Lin", "Meng Han"], "title": "SproutBench: A Benchmark for Safe and Ethical Large Language Models for Youth", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The rapid proliferation of large language models (LLMs) in applications\ntargeting children and adolescents necessitates a fundamental reassessment of\nprevailing AI safety frameworks, which are largely tailored to adult users and\nneglect the distinct developmental vulnerabilities of minors. This paper\nhighlights key deficiencies in existing LLM safety benchmarks, including their\ninadequate coverage of age-specific cognitive, emotional, and social risks\nspanning early childhood (ages 0--6), middle childhood (7--12), and adolescence\n(13--18). To bridge these gaps, we introduce SproutBench, an innovative\nevaluation suite comprising 1,283 developmentally grounded adversarial prompts\ndesigned to probe risks such as emotional dependency, privacy violations, and\nimitation of hazardous behaviors. Through rigorous empirical evaluation of 47\ndiverse LLMs, we uncover substantial safety vulnerabilities, corroborated by\nrobust inter-dimensional correlations (e.g., between Safety and Risk\nPrevention) and a notable inverse relationship between Interactivity and Age\nAppropriateness. These insights yield practical guidelines for advancing\nchild-centric AI design and deployment.", "AI": {"tldr": "The paper introduces SproutBench, a new evaluation suite for assessing LLM safety for minors, revealing significant vulnerabilities in current models.", "motivation": "Existing AI safety frameworks for LLMs are inadequate for children and adolescents, neglecting their unique developmental risks.", "method": "Developed SproutBench with 1,283 adversarial prompts to test age-specific risks in 47 LLMs.", "result": "Found major safety gaps, with correlations between safety dimensions and an inverse link between interactivity and age appropriateness.", "conclusion": "Provides guidelines for improving child-centric AI design and deployment."}}
{"id": "2508.11182", "pdf": "https://arxiv.org/pdf/2508.11182", "abs": "https://arxiv.org/abs/2508.11182", "authors": ["Matti Berthold", "Lydia Bl\u00fcmel", "Anna Rapberger"], "title": "On Strong and Weak Admissibility in Non-Flat Assumption-Based Argumentation", "categories": ["cs.AI"], "comment": null, "summary": "In this work, we broaden the investigation of admissibility notions in the\ncontext of assumption-based argumentation (ABA). More specifically, we study\ntwo prominent alternatives to the standard notion of admissibility from\nabstract argumentation, namely strong and weak admissibility, and introduce the\nrespective preferred, complete and grounded semantics for general (sometimes\ncalled non-flat) ABA. To do so, we use abstract bipolar set-based argumentation\nframeworks (BSAFs) as formal playground since they concisely capture the\nrelations between assumptions and are expressive enough to represent general\nnon-flat ABA frameworks, as recently shown. While weak admissibility has been\nrecently investigated for a restricted fragment of ABA in which assumptions\ncannot be derived (flat ABA), strong admissibility has not been investigated\nfor ABA so far. We introduce strong admissibility for ABA and investigate\ndesirable properties. We furthermore extend the recent investigations of weak\nadmissibility in the flat ABA fragment to the non-flat case. We show that the\ncentral modularization property is maintained under classical, strong, and weak\nadmissibility. We also show that strong and weakly admissible semantics in\nnon-flat ABA share some of the shortcomings of standard admissible semantics\nand discuss ways to address these.", "AI": {"tldr": "The paper explores strong and weak admissibility in assumption-based argumentation (ABA), extending these notions to non-flat ABA and analyzing their properties and shortcomings.", "motivation": "To broaden the understanding of admissibility in ABA by investigating strong and weak admissibility, which are alternatives to the standard notion, and to extend these to general (non-flat) ABA frameworks.", "method": "Uses abstract bipolar set-based argumentation frameworks (BSAFs) to formalize and analyze strong and weak admissibility in non-flat ABA, examining properties like modularization.", "result": "Shows that modularization holds under classical, strong, and weak admissibility, but strong and weakly admissible semantics in non-flat ABA inherit some shortcomings of standard semantics.", "conclusion": "The study provides insights into strong and weak admissibility in ABA, highlighting their properties and limitations, and suggests ways to address these shortcomings."}}
{"id": "2508.10933", "pdf": "https://arxiv.org/pdf/2508.10933", "abs": "https://arxiv.org/abs/2508.10933", "authors": ["Yoli Shavit", "Yosi Keller"], "title": "Relative Pose Regression with Pose Auto-Encoders: Enhancing Accuracy and Data Efficiency for Retail Applications", "categories": ["cs.CV", "eess.IV"], "comment": "Accepted to ICCVW 2025", "summary": "Accurate camera localization is crucial for modern retail environments,\nenabling enhanced customer experiences, streamlined inventory management, and\nautonomous operations. While Absolute Pose Regression (APR) from a single image\noffers a promising solution, approaches that incorporate visual and spatial\nscene priors tend to achieve higher accuracy. Camera Pose Auto-Encoders (PAEs)\nhave recently been introduced to embed such priors into APR. In this work, we\nextend PAEs to the task of Relative Pose Regression (RPR) and propose a novel\nre-localization scheme that refines APR predictions using PAE-based RPR,\nwithout requiring additional storage of images or pose data. We first introduce\nPAE-based RPR and establish its effectiveness by comparing it with image-based\nRPR models of equivalent architectures. We then demonstrate that our refinement\nstrategy, driven by a PAE-based RPR, enhances APR localization accuracy on\nindoor benchmarks. Notably, our method is shown to achieve competitive\nperformance even when trained with only 30% of the data, substantially reducing\nthe data collection burden for retail deployment. Our code and pre-trained\nmodels are available at: https://github.com/yolish/camera-pose-auto-encoders", "AI": {"tldr": "The paper introduces a novel re-localization scheme using Camera Pose Auto-Encoders (PAEs) for Relative Pose Regression (RPR) to refine Absolute Pose Regression (APR) predictions, improving accuracy in retail environments with reduced data needs.", "motivation": "Accurate camera localization is essential for retail applications like customer experience and inventory management. While APR is promising, incorporating visual and spatial priors via PAEs can enhance accuracy.", "method": "Extends PAEs to RPR and proposes a re-localization scheme refining APR predictions using PAE-based RPR, avoiding extra storage. Validates PAE-based RPR against image-based RPR and tests refinement on indoor benchmarks.", "result": "PAE-based RPR outperforms equivalent image-based models. The refinement strategy improves APR accuracy, achieving competitive performance with only 30% of training data.", "conclusion": "The method enhances camera localization accuracy in retail settings while reducing data collection efforts, making it practical for deployment."}}
{"id": "2508.10967", "pdf": "https://arxiv.org/pdf/2508.10967", "abs": "https://arxiv.org/abs/2508.10967", "authors": ["Xinyi Li", "Sai Wang", "Yutian Lin", "Yu Wu", "Yi Yang"], "title": "Retro-Expert: Collaborative Reasoning for Interpretable Retrosynthesis", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Retrosynthesis prediction aims to infer the reactant molecule based on a\ngiven product molecule, which is a fundamental task in chemical synthesis.\nHowever, existing models rely on static pattern-matching paradigm, which limits\ntheir ability to perform effective logic decision-making, leading to black-box\ndecision-making. Building on this, we propose Retro-Expert, an interpretable\nretrosynthesis framework that performs collaborative reasoning by combining the\ncomplementary reasoning strengths of Large Language Models and specialized\nmodels via reinforcement learning. It outputs natural language explanations\ngrounded in chemical logic through three components: (1) specialized models\nperform shallow reasoning to construct high-quality chemical decision space,\n(2) LLM-driven critical reasoning to generate predictions and corresponding\ninterpretable reasoning path, and (3) reinforcement learning optimizing\ninterpretable decision policy. Experiments show that Retro-Expert not only\nsurpasses both LLM-based and specialized models across different metrics but\nalso provides expert-aligned explanations that bridge the gap between AI\npredictions and actionable chemical insights.", "AI": {"tldr": "Retro-Expert is an interpretable retrosynthesis framework combining LLMs and specialized models via reinforcement learning, outperforming existing methods and providing expert-aligned explanations.", "motivation": "Existing retrosynthesis models lack interpretability and logic-based decision-making, limiting their practical utility.", "method": "Retro-Expert integrates specialized models for shallow reasoning, LLMs for critical reasoning, and reinforcement learning for optimizing interpretable policies.", "result": "The framework outperforms both LLM-based and specialized models and offers actionable, expert-aligned explanations.", "conclusion": "Retro-Expert bridges the gap between AI predictions and chemical insights, enhancing interpretability and performance in retrosynthesis."}}
{"id": "2508.11017", "pdf": "https://arxiv.org/pdf/2508.11017", "abs": "https://arxiv.org/abs/2508.11017", "authors": ["Carter Blum", "Katja Filipova", "Ann Yuan", "Asma Ghandeharioun", "Julian Zimmert", "Fred Zhang", "Jessica Hoffmann", "Tal Linzen", "Martin Wattenberg", "Lucas Dixon", "Mor Geva"], "title": "Beyond the Rosetta Stone: Unification Forces in Generalization Dynamics", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) struggle with cross-lingual knowledge transfer:\nthey hallucinate when asked in one language about facts expressed in a\ndifferent language during training. This work introduces a controlled setting\nto study the causes and dynamics of this phenomenon by training small\nTransformer models from scratch on synthetic multilingual datasets. We identify\na learning phase wherein a model develops either separate or unified\nrepresentations of the same facts across languages, and show that unification\nis essential for cross-lingual transfer. We also show that the degree of\nunification depends on mutual information between facts and training data\nlanguage, and on how easy it is to extract that language. Based on these\ninsights, we develop methods to modulate the level of cross-lingual transfer by\nmanipulating data distribution and tokenization, and we introduce metrics and\nvisualizations to formally characterize their effects on unification. Our work\nshows how controlled settings can shed light on pre-training dynamics and\nsuggests new directions for improving cross-lingual transfer in LLMs.", "AI": {"tldr": "The paper investigates why LLMs struggle with cross-lingual knowledge transfer, identifies a learning phase for unified representations, and proposes methods to improve transfer.", "motivation": "To understand and address the issue of LLMs hallucinating facts when queried in a language different from the training data.", "method": "Train small Transformer models on synthetic multilingual datasets, analyze learning phases, and manipulate data distribution and tokenization.", "result": "Unified representations are crucial for cross-lingual transfer, and their degree depends on mutual information and language extraction ease.", "conclusion": "Controlled settings reveal pre-training dynamics, offering new ways to enhance cross-lingual transfer in LLMs."}}
{"id": "2508.11252", "pdf": "https://arxiv.org/pdf/2508.11252", "abs": "https://arxiv.org/abs/2508.11252", "authors": ["Youcheng Huang", "Bowen Qin", "Chen Huang", "Duanyu Feng", "Xi Yang", "Wenqiang Lei"], "title": "Beyond Solving Math Quiz: Evaluating the Ability of Large Reasoning Models to Ask for Information", "categories": ["cs.AI", "cs.CL", "cs.IR"], "comment": null, "summary": "Large Reasoning Models (LRMs) have demonstrated remarkable problem-solving\nabilities in mathematics, as evaluated by existing benchmarks exclusively on\nwell-defined problems. However, such evaluation setup constitutes a critical\ngap, since a genuine intelligent agent should not only solve problems (as a\nmath quiz solver), but also be able~to ask for information when the problems\nlack sufficient information, enabling proactivity in responding users'\nrequests. To bridge such gap, we proposes a new dataset consisting of two types\nof incomplete problems with diverse contexts. Based on the dataset, our\nsystematical evaluation of LRMs reveals their inability in proactively asking\nfor information. In addition, we uncover the behaviors related to overthinking\nand hallucination of LRMs, and highlight the potential and challenges of\nsupervised fine-tuning in learning such ability. We hope to provide new\ninsights in developing LRMs with genuine intelligence, rather than just solving\nproblems.", "AI": {"tldr": "The paper introduces a new dataset to evaluate Large Reasoning Models (LRMs) on incomplete problems, revealing their inability to proactively ask for missing information and highlighting issues like overthinking and hallucination.", "motivation": "Existing benchmarks only evaluate LRMs on well-defined problems, missing the need for proactive problem-solving in real-world scenarios.", "method": "The authors propose a new dataset with incomplete problems and systematically evaluate LRMs, including their behavior and the impact of supervised fine-tuning.", "result": "LRMs struggle with proactively requesting missing information and exhibit overthinking and hallucination. Supervised fine-tuning shows potential but faces challenges.", "conclusion": "The study highlights the need for LRMs to develop genuine intelligence, beyond just solving problems, and identifies key challenges in achieving this."}}
{"id": "2508.10934", "pdf": "https://arxiv.org/pdf/2508.10934", "abs": "https://arxiv.org/abs/2508.10934", "authors": ["Jiahui Huang", "Qunjie Zhou", "Hesam Rabeti", "Aleksandr Korovko", "Huan Ling", "Xuanchi Ren", "Tianchang Shen", "Jun Gao", "Dmitry Slepichev", "Chen-Hsuan Lin", "Jiawei Ren", "Kevin Xie", "Joydeep Biswas", "Laura Leal-Taixe", "Sanja Fidler"], "title": "ViPE: Video Pose Engine for 3D Geometric Perception", "categories": ["cs.CV", "cs.GR", "cs.RO", "eess.IV"], "comment": "Paper website: https://research.nvidia.com/labs/toronto-ai/vipe/", "summary": "Accurate 3D geometric perception is an important prerequisite for a wide\nrange of spatial AI systems. While state-of-the-art methods depend on\nlarge-scale training data, acquiring consistent and precise 3D annotations from\nin-the-wild videos remains a key challenge. In this work, we introduce ViPE, a\nhandy and versatile video processing engine designed to bridge this gap. ViPE\nefficiently estimates camera intrinsics, camera motion, and dense, near-metric\ndepth maps from unconstrained raw videos. It is robust to diverse scenarios,\nincluding dynamic selfie videos, cinematic shots, or dashcams, and supports\nvarious camera models such as pinhole, wide-angle, and 360{\\deg} panoramas. We\nhave benchmarked ViPE on multiple benchmarks. Notably, it outperforms existing\nuncalibrated pose estimation baselines by 18%/50% on TUM/KITTI sequences, and\nruns at 3-5FPS on a single GPU for standard input resolutions. We use ViPE to\nannotate a large-scale collection of videos. This collection includes around\n100K real-world internet videos, 1M high-quality AI-generated videos, and 2K\npanoramic videos, totaling approximately 96M frames -- all annotated with\naccurate camera poses and dense depth maps. We open-source ViPE and the\nannotated dataset with the hope of accelerating the development of spatial AI\nsystems.", "AI": {"tldr": "ViPE is a video processing engine that estimates camera intrinsics, motion, and dense depth from raw videos, outperforming existing methods and enabling large-scale 3D annotations.", "motivation": "The challenge of acquiring precise 3D annotations from in-the-wild videos motivates the development of ViPE to bridge this gap.", "method": "ViPE efficiently processes unconstrained raw videos, supporting diverse scenarios and camera models (pinhole, wide-angle, 360\u00b0 panoramas).", "result": "ViPE outperforms baselines by 18%/50% on TUM/KITTI sequences, runs at 3-5FPS on a GPU, and annotates a large-scale dataset (100K real-world, 1M AI-generated, 2K panoramic videos).", "conclusion": "ViPE and its open-source dataset aim to accelerate spatial AI development by providing robust 3D perception tools."}}
{"id": "2508.10975", "pdf": "https://arxiv.org/pdf/2508.10975", "abs": "https://arxiv.org/abs/2508.10975", "authors": ["Pratyush Maini", "Vineeth Dorna", "Parth Doshi", "Aldo Carranza", "Fan Pan", "Jack Urbanek", "Paul Burstein", "Alex Fang", "Alvin Deng", "Amro Abbas", "Brett Larsen", "Cody Blakeney", "Charvi Bannur", "Christina Baek", "Darren Teh", "David Schwab", "Haakon Mongstad", "Haoli Yin", "Josh Wills", "Kaleigh Mentzer", "Luke Merrick", "Ricardo Monti", "Rishabh Adiga", "Siddharth Joshi", "Spandan Das", "Zhengping Wang", "Bogdan Gaza", "Ari Morcos", "Matthew Leavitt"], "title": "BeyondWeb: Lessons from Scaling Synthetic Data for Trillion-scale Pretraining", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Recent advances in large language model (LLM) pretraining have shown that\nsimply scaling data quantity eventually leads to diminishing returns, hitting a\ndata wall. In response, the use of synthetic data for pretraining has emerged\nas a promising paradigm for pushing the frontier of performance. Despite this,\nthe factors affecting synthetic data quality remain poorly understood. In this\nwork, we introduce BeyondWeb, a synthetic data generation framework that\nproduces high-quality synthetic data for pretraining. BeyondWeb significantly\nextends the capabilities of traditional web-scale datasets, outperforming\nstate-of-the-art synthetic pretraining datasets such as Cosmopedia and\nNemotron-CC's high-quality synthetic subset (Nemotron-Synth) by up to 5.1\npercentage points (pp) and 2.6pp, respectively, when averaged across a suite of\n14 benchmark evaluations. It delivers up to 7.7x faster training than open web\ndata and 2.7x faster than Nemotron-Synth. Remarkably, a 3B model trained for\n180B tokens on BeyondWeb outperforms an 8B model trained for the same token\nbudget on Cosmopedia. We also present several insights from BeyondWeb on\nsynthetic data for pretraining: what drives its benefits, which data to\nrephrase and how, and the impact of model size and family on data quality.\nOverall, our work shows that there's no silver bullet for generating\nhigh-quality synthetic pretraining data. The best outcomes require jointly\noptimizing many factors, a challenging task that requires rigorous science and\npractical expertise. Naive approaches can yield modest improvements,\npotentially at great cost, while well-executed methods can yield transformative\nimprovements, as exemplified by BeyondWeb.", "AI": {"tldr": "BeyondWeb introduces a synthetic data generation framework for LLM pretraining, outperforming existing datasets in performance and efficiency, while providing insights into synthetic data optimization.", "motivation": "The diminishing returns of scaling data quantity in LLM pretraining and the lack of understanding of synthetic data quality factors motivate the development of BeyondWeb.", "method": "BeyondWeb is a synthetic data generation framework designed to produce high-quality pretraining data, outperforming existing datasets like Cosmopedia and Nemotron-Synth.", "result": "BeyondWeb achieves up to 5.1pp and 2.6pp improvements over Cosmopedia and Nemotron-Synth, respectively, and trains models up to 7.7x faster than open web data.", "conclusion": "Optimizing synthetic data quality requires a multifaceted approach, with BeyondWeb demonstrating transformative improvements through rigorous science and expertise."}}
{"id": "2508.11027", "pdf": "https://arxiv.org/pdf/2508.11027", "abs": "https://arxiv.org/abs/2508.11027", "authors": ["Andrew Wang", "Sophia Hager", "Adi Asija", "Daniel Khashabi", "Nicholas Andrews"], "title": "Hell or High Water: Evaluating Agentic Recovery from External Failures", "categories": ["cs.CL"], "comment": "Accepted to COLM 2025", "summary": "As language model agents are applied to real world problems of increasing\ncomplexity, they will be expected to formulate plans across large search\nspaces. If those plans fail for reasons beyond their control, how well do\nlanguage agents search for alternative ways to achieve their goals? We devise a\nspecialized agentic planning benchmark to study this question. Each planning\nproblem is solved via combinations of function calls. The agent searches for\nrelevant functions from a set of over four thousand possibilities, and observes\nenvironmental feedback in the form of function outputs or error messages. Our\nbenchmark confronts the agent with external failures in its workflow, such as\nfunctions that suddenly become unavailable. At the same time, even with the\nintroduction of these failures, we guarantee that the task remains solvable.\nIdeally, an agent's performance on the planning task should not be affected by\nthe presence of external failures. Overall, we find that language agents\nstruggle to formulate and execute backup plans in response to environment\nfeedback. While state-of-the-art models are often able to identify the correct\nfunction to use in the right context, they struggle to adapt to feedback from\nthe environment and often fail to pursue alternate courses of action, even when\nthe search space is artificially restricted. We provide a systematic analysis\nof the failures of both open-source and commercial models, examining the\neffects of search space size, as well as the benefits of scaling model size in\nour setting. Our analysis identifies key challenges for current generative\nmodels as well as promising directions for future work.", "AI": {"tldr": "Language agents struggle to adapt to external failures in planning tasks, despite identifying correct functions, highlighting challenges in backup planning and adaptability.", "motivation": "To evaluate how well language agents can adapt and search for alternative plans when faced with external failures in complex, real-world tasks.", "method": "A specialized benchmark with over 4,000 function combinations, introducing external failures (e.g., unavailable functions) while ensuring solvability. Agents observe environmental feedback (outputs/errors) to adapt.", "result": "Agents often fail to adapt to feedback or pursue alternate actions, even with restricted search spaces. State-of-the-art models identify correct functions but struggle with backup plans.", "conclusion": "Current generative models face challenges in adaptability and backup planning, with scaling model size offering limited benefits. Future work should address these gaps."}}
{"id": "2508.11347", "pdf": "https://arxiv.org/pdf/2508.11347", "abs": "https://arxiv.org/abs/2508.11347", "authors": ["Yifei Li", "Lingling Zhang", "Hang Yan", "Tianzhe Zhao", "Zihan Ma", "Muye Huang", "Jun Liu"], "title": "SAGE: Scale-Aware Gradual Evolution for Continual Knowledge Graph Embedding", "categories": ["cs.AI", "cs.LG", "I.2.4; I.2.6; H.2.8"], "comment": "10 pages, 5 figures, Accepted at KDD 2025, code available at\n  https://github.com/lyfxjtu/Dynamic-Embedding", "summary": "Traditional knowledge graph (KG) embedding methods aim to represent entities\nand relations in a low-dimensional space, primarily focusing on static graphs.\nHowever, real-world KGs are dynamically evolving with the constant addition of\nentities, relations and facts. To address such dynamic nature of KGs, several\ncontinual knowledge graph embedding (CKGE) methods have been developed to\nefficiently update KG embeddings to accommodate new facts while maintaining\nlearned knowledge. As KGs grow at different rates and scales in real-world\nscenarios, existing CKGE methods often fail to consider the varying scales of\nupdates and lack systematic evaluation throughout the entire update process. In\nthis paper, we propose SAGE, a scale-aware gradual evolution framework for\nCKGE. Specifically, SAGE firstly determine the embedding dimensions based on\nthe update scales and expand the embedding space accordingly. The Dynamic\nDistillation mechanism is further employed to balance the preservation of\nlearned knowledge and the incorporation of new facts. We conduct extensive\nexperiments on seven benchmarks, and the results show that SAGE consistently\noutperforms existing baselines, with a notable improvement of 1.38% in MRR,\n1.25% in H@1 and 1.6% in H@10. Furthermore, experiments comparing SAGE with\nmethods using fixed embedding dimensions show that SAGE achieves optimal\nperformance on every snapshot, demonstrating the importance of adaptive\nembedding dimensions in CKGE. The codes of SAGE are publicly available at:\nhttps://github.com/lyfxjtu/Dynamic-Embedding.", "AI": {"tldr": "SAGE is a scale-aware framework for continual KG embedding, adapting dimensions dynamically and outperforming baselines.", "motivation": "Address the dynamic nature of KGs and the limitations of existing CKGE methods in handling varying update scales.", "method": "Proposes SAGE, which adjusts embedding dimensions based on update scales and uses Dynamic Distillation to balance old and new knowledge.", "result": "Outperforms baselines with improvements of 1.38% in MRR, 1.25% in H@1, and 1.6% in H@10.", "conclusion": "SAGE demonstrates the importance of adaptive embedding dimensions in CKGE, achieving optimal performance across updates."}}
{"id": "2508.10935", "pdf": "https://arxiv.org/pdf/2508.10935", "abs": "https://arxiv.org/abs/2508.10935", "authors": ["Qi Liu", "Yabei Li", "Hongsong Wang", "Lei He"], "title": "HQ-OV3D: A High Box Quality Open-World 3D Detection Framework based on Diffision Model", "categories": ["cs.CV", "cs.LG", "cs.RO"], "comment": null, "summary": "Traditional closed-set 3D detection frameworks fail to meet the demands of\nopen-world applications like autonomous driving. Existing open-vocabulary 3D\ndetection methods typically adopt a two-stage pipeline consisting of\npseudo-label generation followed by semantic alignment. While vision-language\nmodels (VLMs) recently have dramatically improved the semantic accuracy of\npseudo-labels, their geometric quality, particularly bounding box precision,\nremains commonly neglected.To address this issue, we propose a High Box Quality\nOpen-Vocabulary 3D Detection (HQ-OV3D) framework, dedicated to generate and\nrefine high-quality pseudo-labels for open-vocabulary classes. The framework\ncomprises two key components: an Intra-Modality Cross-Validated (IMCV) Proposal\nGenerator that utilizes cross-modality geometric consistency to generate\nhigh-quality initial 3D proposals, and an Annotated-Class Assisted (ACA)\nDenoiser that progressively refines 3D proposals by leveraging geometric priors\nfrom annotated categories through a DDIM-based denoising mechanism.Compared to\nthe state-of-the-art method, training with pseudo-labels generated by our\napproach achieves a 7.37% improvement in mAP on novel classes, demonstrating\nthe superior quality of the pseudo-labels produced by our framework. HQ-OV3D\ncan serve not only as a strong standalone open-vocabulary 3D detector but also\nas a plug-in high-quality pseudo-label generator for existing open-vocabulary\ndetection or annotation pipelines.", "AI": {"tldr": "HQ-OV3D improves open-vocabulary 3D detection by generating high-quality pseudo-labels with geometric precision, outperforming existing methods by 7.37% mAP on novel classes.", "motivation": "Traditional closed-set 3D detection and existing open-vocabulary methods lack geometric quality in pseudo-labels, limiting their effectiveness in open-world applications like autonomous driving.", "method": "HQ-OV3D introduces an Intra-Modality Cross-Validated Proposal Generator for high-quality initial 3D proposals and an Annotated-Class Assisted Denoiser to refine proposals using geometric priors.", "result": "The framework achieves a 7.37% improvement in mAP on novel classes compared to state-of-the-art methods.", "conclusion": "HQ-OV3D is a versatile solution for open-vocabulary 3D detection and pseudo-label generation, enhancing both standalone detection and existing pipelines."}}
{"id": "2508.10993", "pdf": "https://arxiv.org/pdf/2508.10993", "abs": "https://arxiv.org/abs/2508.10993", "authors": ["Basile Lewandowski", "Robert Birke", "Lydia Y. Chen"], "title": "Match & Choose: Model Selection Framework for Fine-tuning Text-to-Image Diffusion Models", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Text-to-image (T2I) models based on diffusion and transformer architectures\nadvance rapidly. They are often pretrained on large corpora, and openly shared\non a model platform, such as HuggingFace. Users can then build up AI\napplications, e.g., generating media contents, by adopting pretrained T2I\nmodels and fine-tuning them on the target dataset. While public pretrained T2I\nmodels facilitate the democratization of the models, users face a new\nchallenge: which model can be best fine-tuned based on the target data domain?\nModel selection is well addressed in classification tasks, but little is known\nin (pretrained) T2I models and their performance indication on the target\ndomain. In this paper, we propose the first model selection framework, M&C,\nwhich enables users to efficiently choose a pretrained T2I model from a model\nplatform without exhaustively fine-tuning them all on the target dataset. The\ncore of M&C is a matching graph, which consists of: (i) nodes of available\nmodels and profiled datasets, and (ii) edges of model-data and data-data pairs\ncapturing the fine-tuning performance and data similarity, respectively. We\nthen build a model that, based on the inputs of model/data feature, and,\ncritically, the graph embedding feature, extracted from the matching graph,\npredicts the model achieving the best quality after fine-tuning for the target\ndomain. We evaluate M&C on choosing across ten T2I models for 32 datasets\nagainst three baselines. Our results show that M&C successfully predicts the\nbest model for fine-tuning in 61.3% of the cases and a closely performing model\nfor the rest.", "AI": {"tldr": "The paper introduces M&C, a model selection framework for choosing the best pretrained text-to-image (T2I) model for fine-tuning on target datasets without exhaustive testing.", "motivation": "Public pretrained T2I models are widely available, but selecting the best one for a specific domain is challenging due to lack of performance indicators.", "method": "M&C uses a matching graph with nodes for models/datasets and edges for performance/similarity, along with graph embedding features, to predict the best model for fine-tuning.", "result": "M&C successfully predicts the best model 61.3% of the time and a close alternative otherwise, outperforming three baselines.", "conclusion": "M&C provides an efficient solution for model selection in T2I tasks, reducing the need for exhaustive fine-tuning."}}
{"id": "2508.11061", "pdf": "https://arxiv.org/pdf/2508.11061", "abs": "https://arxiv.org/abs/2508.11061", "authors": ["Martin Pavl\u00ed\u010dek", "Tom\u00e1\u0161 Filip", "Petr Sos\u00edk"], "title": "BIPOLAR: Polarization-based granular framework for LLM bias evaluation", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) are known to exhibit biases in downstream tasks,\nespecially when dealing with sensitive topics such as political discourse,\ngender identity, ethnic relations, or national stereotypes. Although\nsignificant progress has been made in bias detection and mitigation techniques,\ncertain challenges remain underexplored. This study proposes a reusable,\ngranular, and topic-agnostic framework to evaluate polarisation-related biases\nin LLM (both open-source and closed-source). Our approach combines\npolarisation-sensitive sentiment metrics with a synthetically generated\nbalanced dataset of conflict-related statements, using a predefined set of\nsemantic categories.\n  As a case study, we created a synthetic dataset that focusses on the\nRussia-Ukraine war, and we evaluated the bias in several LLMs: Llama-3,\nMistral, GPT-4, Claude 3.5, and Gemini 1.0. Beyond aggregate bias scores, with\na general trend for more positive sentiment toward Ukraine, the framework\nallowed fine-grained analysis with considerable variation between semantic\ncategories, uncovering divergent behavioural patterns among models. Adaptation\nto prompt modifications showed further bias towards preconceived language and\ncitizenship modification.\n  Overall, the framework supports automated dataset generation and fine-grained\nbias assessment, is applicable to a variety of polarisation-driven scenarios\nand topics, and is orthogonal to many other bias-evaluation strategies.", "AI": {"tldr": "The paper proposes a reusable framework to evaluate polarization-related biases in LLMs, using synthetic datasets and sentiment metrics, demonstrated with a case study on the Russia-Ukraine war.", "motivation": "Addressing underexplored challenges in bias detection and mitigation in LLMs, especially for sensitive topics like political discourse.", "method": "Combines polarization-sensitive sentiment metrics with a synthetically generated balanced dataset of conflict-related statements, tested on models like Llama-3, GPT-4, and others.", "result": "Found general positive sentiment bias toward Ukraine, with fine-grained variations across semantic categories and models. Prompt modifications revealed further biases.", "conclusion": "The framework enables automated dataset generation and detailed bias assessment, applicable to diverse polarization-driven topics and complementary to other bias-evaluation methods."}}
{"id": "2508.11360", "pdf": "https://arxiv.org/pdf/2508.11360", "abs": "https://arxiv.org/abs/2508.11360", "authors": ["Songqin Nong", "Jingxuan Xu", "Sheng Zhou", "Jianfeng Chen", "Xiaoxuan Tang", "Tao Jiang", "Wenhao Xu"], "title": "CRAFT-GUI: Curriculum-Reinforced Agent For GUI Tasks", "categories": ["cs.AI", "cs.HC"], "comment": null, "summary": "As autonomous agents become adept at understanding and interacting with\ngraphical user interface (GUI) environments, a new era of automated task\nexecution is emerging. Recent studies have demonstrated that Reinforcement\nLearning (RL) can effectively enhance agents' performance in dynamic\ninteractive GUI environments. However, these methods face two key limitations:\n(1) they overlook the significant variation in difficulty across different GUI\ntasks by treating the entire training data as a uniform set, which hampers the\nagent's ability to adapt its learning process; and (2) most approaches collapse\ntask-specific nuances into a single, coarse reward, leaving the agent with a\nuniform signal that yields inefficient policy updates. To address these\nlimitations, we propose CRAFT-GUI, a curriculum learning framework based on\nGroup Relative Policy Optimization (GRPO) that explicitly accounts for the\nvarying difficulty across trajectories. To enable more fine-grained policy\noptimization, we design a reward function that combines simple rule-based\nsignals with model-judged evaluation, providing richer and more nuanced\nfeedback during training. Experimental results demonstrate that our method\nachieves significant improvements over previous state-of-the-art approaches,\noutperforming them by 5.6% on public benchmarks Android Control and 10.3% on\nour internal online benchmarks, respectively. These findings empirically\nvalidate the effectiveness of integrating reinforcement learning with\ncurriculum learning in GUI interaction tasks.", "AI": {"tldr": "CRAFT-GUI, a curriculum learning framework using GRPO, addresses limitations in RL for GUI tasks by accounting for task difficulty and providing nuanced rewards, outperforming prior methods by 5.6-10.3%.", "motivation": "Current RL methods for GUI tasks treat training data uniformly and use coarse rewards, limiting adaptability and policy efficiency.", "method": "Proposes CRAFT-GUI with GRPO-based curriculum learning and a hybrid reward function combining rule-based and model-judged signals.", "result": "Achieves 5.6% and 10.3% improvements on Android Control and internal benchmarks, respectively.", "conclusion": "Integrating RL with curriculum learning effectively enhances GUI task performance."}}
{"id": "2508.10936", "pdf": "https://arxiv.org/pdf/2508.10936", "abs": "https://arxiv.org/abs/2508.10936", "authors": ["Cheng Chen", "Hao Huang", "Saurabh Bagchi"], "title": "Vision-Only Gaussian Splatting for Collaborative Semantic Occupancy Prediction", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Collaborative perception enables connected vehicles to share information,\novercoming occlusions and extending the limited sensing range inherent in\nsingle-agent (non-collaborative) systems. Existing vision-only methods for 3D\nsemantic occupancy prediction commonly rely on dense 3D voxels, which incur\nhigh communication costs, or 2D planar features, which require accurate depth\nestimation or additional supervision, limiting their applicability to\ncollaborative scenarios. To address these challenges, we propose the first\napproach leveraging sparse 3D semantic Gaussian splatting for collaborative 3D\nsemantic occupancy prediction. By sharing and fusing intermediate Gaussian\nprimitives, our method provides three benefits: a neighborhood-based\ncross-agent fusion that removes duplicates and suppresses noisy or inconsistent\nGaussians; a joint encoding of geometry and semantics in each primitive, which\nreduces reliance on depth supervision and allows simple rigid alignment; and\nsparse, object-centric messages that preserve structural information while\nreducing communication volume. Extensive experiments demonstrate that our\napproach outperforms single-agent perception and baseline collaborative methods\nby +8.42 and +3.28 points in mIoU, and +5.11 and +22.41 points in IoU,\nrespectively. When further reducing the number of transmitted Gaussians, our\nmethod still achieves a +1.9 improvement in mIoU, using only 34.6%\ncommunication volume, highlighting robust performance under limited\ncommunication budgets.", "AI": {"tldr": "Proposes sparse 3D semantic Gaussian splatting for collaborative 3D semantic occupancy prediction, improving accuracy and reducing communication costs.", "motivation": "Overcome limitations of dense 3D voxels and 2D planar features in collaborative perception by leveraging sparse Gaussian primitives.", "method": "Shares and fuses intermediate Gaussian primitives, enabling neighborhood-based cross-agent fusion, joint geometry-semantics encoding, and sparse object-centric messaging.", "result": "Outperforms single-agent and baseline collaborative methods in mIoU and IoU, with reduced communication volume.", "conclusion": "The approach is effective for collaborative perception, offering robust performance under limited communication budgets."}}
{"id": "2508.11016", "pdf": "https://arxiv.org/pdf/2508.11016", "abs": "https://arxiv.org/abs/2508.11016", "authors": ["Qingbin Li", "Rongkun Xue", "Jie Wang", "Ming Zhou", "Zhi Li", "Xiaofeng Ji", "Yongqi Wang", "Miao Liu", "Zheming Yang", "Minghui Qiu", "Jing Yang"], "title": "CURE: Critical-Token-Guided Re-concatenation for Entropy-collapse Prevention", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Recent advances in Reinforcement Learning with Verified Reward (RLVR) have\ndriven the emergence of more sophisticated cognitive behaviors in large\nlanguage models (LLMs), thereby enhancing their reasoning capabilities.\nHowever, in prior RLVR pipelines, the repeated use of static initial-state\nsampling drawn exactly from the dataset distribution during each sampling phase\nproduced overly deterministic, low diversity model behavior, which manifested\nas rapid entropy collapse and hindered sustained performance gains during\nprolonged training. To address this issue, we introduce CURE\n(Critical-token-gUided Re concatenation for Entropy-collapse prevention), a\ntwo-stage framework that balances exploration and exploitation. Specifically,\nin the first stage, to deliberately steer the model toward novel yet coherent\ncontexts, we re-generate at high-entropy critical tokens and jointly optimize\nthe original and the branched trajectories. The further comparison with vanilla\nDAPO shows that the regeneration process achieves a better performance on math\nreasoning tasks while sustaining a high-level entropy degree for exploration.\nIn the second stage, we continue training with static initial-state sampling by\nDAPO, intentionally placing the model in a familiar state to gradually\nstrengthen exploitation. Extensive experiments on Qwen-2.5-Math-7B show that,\ncompared to other RLVR methods, CURE achieves a 5% performance gain across six\nmath benchmarks, establishing state-of-the-art performance in both entropy and\naccuracy. A series of experiments further validate the effectiveness of our\napproach. Code is available at https://github.com/CURE-Project/CURE.", "AI": {"tldr": "CURE introduces a two-stage framework to prevent entropy collapse in RLVR, balancing exploration and exploitation for improved LLM performance.", "motivation": "Prior RLVR methods suffered from low diversity due to static initial-state sampling, leading to entropy collapse and limited performance gains.", "method": "CURE uses a two-stage approach: high-entropy critical token regeneration for exploration and DAPO for exploitation.", "result": "CURE achieves a 5% performance gain on math benchmarks and maintains high entropy.", "conclusion": "CURE effectively balances exploration and exploitation, outperforming other RLVR methods in both entropy and accuracy."}}
{"id": "2508.11068", "pdf": "https://arxiv.org/pdf/2508.11068", "abs": "https://arxiv.org/abs/2508.11068", "authors": ["Nicolas Goulet", "Alexandre Blondin Mass\u00e9", "Moussa Abdendi"], "title": "Approaching the Source of Symbol Grounding with Confluent Reductions of Abstract Meaning Representation Directed Graphs", "categories": ["cs.CL"], "comment": null, "summary": "Abstract meaning representation (AMR) is a semantic formalism used to\nrepresent the meaning of sentences as directed acyclic graphs. In this paper,\nwe describe how real digital dictionaries can be embedded into AMR directed\ngraphs (digraphs), using state-of-the-art pre-trained large language models.\nThen, we reduce those graphs in a confluent manner, i.e. with transformations\nthat preserve their circuit space. Finally, the properties of these reduces\ndigraphs are analyzed and discussed in relation to the symbol grounding\nproblem.", "AI": {"tldr": "The paper explores embedding digital dictionaries into AMR graphs using pre-trained language models, reducing these graphs while preserving properties, and analyzing their relation to the symbol grounding problem.", "motivation": "To integrate real digital dictionaries into AMR graphs and study their reduced forms for insights into symbol grounding.", "method": "Use pre-trained language models to embed dictionaries into AMR digraphs, then reduce these graphs confluently.", "result": "Reduced digraphs are analyzed, revealing properties relevant to the symbol grounding problem.", "conclusion": "The study demonstrates the feasibility and significance of embedding and reducing AMR graphs for symbol grounding insights."}}
{"id": "2508.11416", "pdf": "https://arxiv.org/pdf/2508.11416", "abs": "https://arxiv.org/abs/2508.11416", "authors": ["Xuhua Zhao", "Yuxuan Xie", "Caihua Chen", "Yuxiang Sun"], "title": "AIM-Bench: Evaluating Decision-making Biases of Agentic LLM as Inventory Manager", "categories": ["cs.AI"], "comment": null, "summary": "Recent advances in mathematical reasoning and the long-term planning\ncapabilities of large language models (LLMs) have precipitated the development\nof agents, which are being increasingly leveraged in business operations\nprocesses. Decision models to optimize inventory levels are one of the core\nelements of operations management. However, the capabilities of the LLM agent\nin making inventory decisions in uncertain contexts, as well as the\ndecision-making biases (e.g. framing effect, etc.) of the agent, remain largely\nunexplored. This prompts concerns regarding the capacity of LLM agents to\neffectively address real-world problems, as well as the potential implications\nof biases that may be present. To address this gap, we introduce AIM-Bench, a\nnovel benchmark designed to assess the decision-making behaviour of LLM agents\nin uncertain supply chain management scenarios through a diverse series of\ninventory replenishment experiments. Our results reveal that different LLMs\ntypically exhibit varying degrees of decision bias that are similar to those\nobserved in human beings. In addition, we explored strategies to mitigate the\npull-to-centre effect and the bullwhip effect, namely cognitive reflection and\nimplementation of information sharing. These findings underscore the need for\ncareful consideration of the potential biases in deploying LLMs in Inventory\ndecision-making scenarios. We hope that these insights will pave the way for\nmitigating human decision bias and developing human-centred decision support\nsystems for supply chains.", "AI": {"tldr": "The paper introduces AIM-Bench to evaluate LLM agents' decision-making in uncertain supply chain scenarios, revealing biases similar to humans and suggesting mitigation strategies.", "motivation": "To explore LLM agents' capabilities and biases in inventory decision-making under uncertainty, addressing gaps in understanding their real-world applicability.", "method": "Developed AIM-Bench, a benchmark for testing LLM agents through inventory replenishment experiments, and explored strategies like cognitive reflection and information sharing.", "result": "LLMs exhibit human-like decision biases; mitigation strategies show promise in reducing effects like pull-to-centre and bullwhip.", "conclusion": "Highlights the need to address LLM biases in inventory decisions and suggests pathways for human-centred decision support systems."}}
{"id": "2508.10937", "pdf": "https://arxiv.org/pdf/2508.10937", "abs": "https://arxiv.org/abs/2508.10937", "authors": ["Jiarui Yang", "Hang Guo", "Wen Huang", "Tao Dai", "Shutao Xia"], "title": "Personalized Face Super-Resolution with Identity Decoupling and Fitting", "categories": ["cs.CV"], "comment": null, "summary": "In recent years, face super-resolution (FSR) methods have achieved remarkable\nprogress, generally maintaining high image fidelity and identity (ID)\nconsistency under standard settings. However, in extreme degradation scenarios\n(e.g., scale $> 8\\times$), critical attributes and ID information are often\nseverely lost in the input image, making it difficult for conventional models\nto reconstruct realistic and ID-consistent faces. Existing methods tend to\ngenerate hallucinated faces under such conditions, producing restored images\nlacking authentic ID constraints. To address this challenge, we propose a novel\nFSR method with Identity Decoupling and Fitting (IDFSR), designed to enhance ID\nrestoration under large scaling factors while mitigating hallucination effects.\nOur approach involves three key designs: 1) \\textbf{Masking} the facial region\nin the low-resolution (LR) image to eliminate unreliable ID cues; 2)\n\\textbf{Warping} a reference image to align with the LR input, providing style\nguidance; 3) Leveraging \\textbf{ID embeddings} extracted from ground truth (GT)\nimages for fine-grained ID modeling and personalized adaptation. We first\npretrain a diffusion-based model to explicitly decouple style and ID by forcing\nit to reconstruct masked LR face regions using both style and identity\nembeddings. Subsequently, we freeze most network parameters and perform\nlightweight fine-tuning of the ID embedding using a small set of target ID\nimages. This embedding encodes fine-grained facial attributes and precise ID\ninformation, significantly improving both ID consistency and perceptual\nquality. Extensive quantitative evaluations and visual comparisons demonstrate\nthat the proposed IDFSR substantially outperforms existing approaches under\nextreme degradation, particularly achieving superior performance on ID\nconsistency.", "AI": {"tldr": "The paper introduces IDFSR, a novel face super-resolution method for extreme degradation scenarios, focusing on identity consistency and reducing hallucination effects.", "motivation": "Existing FSR methods struggle with extreme degradation (e.g., scale > 8\u00d7), often losing critical ID information and producing unrealistic results.", "method": "IDFSR uses masking, warping, and ID embeddings to decouple style and identity, pretrains a diffusion model, and fine-tunes ID embeddings for personalized adaptation.", "result": "IDFSR outperforms existing methods in extreme degradation, achieving superior ID consistency and perceptual quality.", "conclusion": "The proposed IDFSR effectively addresses extreme degradation challenges in FSR, enhancing ID restoration and reducing hallucination."}}
{"id": "2508.11020", "pdf": "https://arxiv.org/pdf/2508.11020", "abs": "https://arxiv.org/abs/2508.11020", "authors": ["Aakash Kumar", "Emanuele Natale"], "title": "Quantization vs Pruning: Insights from the Strong Lottery Ticket Hypothesis", "categories": ["cs.LG"], "comment": null, "summary": "Quantization is an essential technique for making neural networks more\nefficient, yet our theoretical understanding of it remains limited. Previous\nworks demonstrated that extremely low-precision networks, such as binary\nnetworks, can be constructed by pruning large, randomly-initialized networks,\nand showed that the ratio between the size of the original and the pruned\nnetworks is at most polylogarithmic.\n  The specific pruning method they employed inspired a line of theoretical work\nknown as the Strong Lottery Ticket Hypothesis (SLTH), which leverages insights\nfrom the Random Subset Sum Problem. However, these results primarily address\nthe continuous setting and cannot be applied to extend SLTH results to the\nquantized setting.\n  In this work, we build on foundational results by Borgs et al. on the Number\nPartitioning Problem to derive new theoretical results for the Random Subset\nSum Problem in a quantized setting.\n  Using these results, we then extend the SLTH framework to finite-precision\nnetworks. While prior work on SLTH showed that pruning allows approximation of\na certain class of neural networks, we demonstrate that, in the quantized\nsetting, the analogous class of target discrete neural networks can be\nrepresented exactly, and we prove optimal bounds on the necessary\noverparameterization of the initial network as a function of the precision of\nthe target network.", "AI": {"tldr": "The paper extends the Strong Lottery Ticket Hypothesis (SLTH) to quantized neural networks, proving exact representation and optimal bounds on overparameterization.", "motivation": "Limited theoretical understanding of quantization in neural networks and the inability of prior SLTH results to address the quantized setting.", "method": "Builds on Borgs et al.'s work on the Number Partitioning Problem to derive new results for the Random Subset Sum Problem in quantization, then extends SLTH to finite-precision networks.", "result": "Demonstrates exact representation of discrete neural networks in the quantized setting and proves optimal bounds on initial network overparameterization.", "conclusion": "The work successfully bridges the gap between SLTH and quantization, providing theoretical foundations for efficient low-precision neural networks."}}
{"id": "2508.11120", "pdf": "https://arxiv.org/pdf/2508.11120", "abs": "https://arxiv.org/abs/2508.11120", "authors": ["Lorenzo Jaime Yu Flores", "Junyi Shen", "Xiaoyuan Gu"], "title": "Towards Reliable Multi-Agent Systems for Marketing Applications via Reflection, Memory, and Planning", "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in large language models (LLMs) enabled the development of AI\nagents that can plan and interact with tools to complete complex tasks.\nHowever, literature on their reliability in real-world applications remains\nlimited. In this paper, we introduce a multi-agent framework for a marketing\ntask: audience curation. To solve this, we introduce a framework called RAMP\nthat iteratively plans, calls tools, verifies the output, and generates\nsuggestions to improve the quality of the audience generated. Additionally, we\nequip the model with a long-term memory store, which is a knowledge base of\nclient-specific facts and past queries. Overall, we demonstrate the use of LLM\nplanning and memory, which increases accuracy by 28 percentage points on a set\nof 88 evaluation queries. Moreover, we show the impact of iterative\nverification and reflection on more ambiguous queries, showing progressively\nbetter recall (roughly +20 percentage points) with more verify/reflect\niterations on a smaller challenge set, and higher user satisfaction. Our\nresults provide practical insights for deploying reliable LLM-based systems in\ndynamic, industry-facing environments.", "AI": {"tldr": "A multi-agent framework (RAMP) for marketing tasks improves audience curation using LLM planning, memory, and iterative verification, boosting accuracy and user satisfaction.", "motivation": "To address the limited reliability of LLM-based agents in real-world applications, particularly for dynamic tasks like audience curation.", "method": "Introduces RAMP, a framework that plans, calls tools, verifies outputs, and iteratively improves suggestions using long-term memory for client-specific facts.", "result": "Increases accuracy by 28 percentage points on 88 queries and improves recall by ~20 points with iterative verification.", "conclusion": "Demonstrates practical insights for deploying reliable LLM-based systems in dynamic, industry-facing environments."}}
{"id": "2508.11452", "pdf": "https://arxiv.org/pdf/2508.11452", "abs": "https://arxiv.org/abs/2508.11452", "authors": ["Kangyu Wang", "Hongliang He", "Lin Liu", "Ruiqi Liang", "Zhenzhong Lan", "Jianguo Li"], "title": "Inclusion Arena: An Open Platform for Evaluating Large Foundation Models with Real-World Apps", "categories": ["cs.AI", "cs.CL", "cs.HC"], "comment": "Our platform is publicly accessible at\n  https://doraemon.alipay.com/model-ranking", "summary": "Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs)\nhave ushered in a new era of AI capabilities, demonstrating near-human-level\nperformance across diverse scenarios. While numerous benchmarks (e.g., MMLU)\nand leaderboards (e.g., Chatbot Arena) have been proposed to help evolve the\ndevelopment of LLMs and MLLMs, most rely on static datasets or crowdsourced\ngeneral-domain prompts, often falling short of reflecting performance in\nreal-world applications. To bridge this critical gap, we present Inclusion\nArena, a live leaderboard that ranks models based on human feedback collected\ndirectly from AI-powered applications. Our platform integrates pairwise model\ncomparisons into natural user interactions, ensuring evaluations reflect\npractical usage scenarios. For robust model ranking, we employ the\nBradley-Terry model augmented with two key innovations: (1) Placement Matches,\na cold-start mechanism to quickly estimate initial ratings for newly integrated\nmodels, and (2) Proximity Sampling, an intelligent comparison strategy that\nprioritizes battles between models of similar capabilities to maximize\ninformation gain and enhance rating stability. Extensive empirical analyses and\nsimulations demonstrate that Inclusion Arena yields reliable and stable\nrankings, exhibits higher data transitivity compared to general crowdsourced\ndatasets, and significantly mitigates the risk of malicious manipulation. By\nfostering an open alliance between foundation models and real-world\napplications, Inclusion Arena aims to accelerate the development of LLMs and\nMLLMs truly optimized for practical, user-centric deployments. The platform is\npublicly accessible at https://doraemon.alipay.com/model-ranking.", "AI": {"tldr": "Inclusion Arena is a live leaderboard for LLMs and MLLMs, ranking models based on human feedback from real-world applications, using innovative methods like Placement Matches and Proximity Sampling for reliable rankings.", "motivation": "Current benchmarks for LLMs and MLLMs rely on static datasets or general-domain prompts, failing to reflect real-world performance. Inclusion Arena addresses this gap by integrating human feedback from practical usage.", "method": "The platform uses pairwise model comparisons in natural user interactions, employing the Bradley-Terry model with Placement Matches (for cold-start ratings) and Proximity Sampling (for intelligent comparisons).", "result": "Inclusion Arena provides reliable, stable rankings, higher data transitivity, and reduced risk of manipulation, outperforming general crowdsourced datasets.", "conclusion": "Inclusion Arena bridges the gap between benchmarks and real-world applications, accelerating the development of user-optimized LLMs and MLLMs."}}
{"id": "2508.10938", "pdf": "https://arxiv.org/pdf/2508.10938", "abs": "https://arxiv.org/abs/2508.10938", "authors": ["Tianyu Song", "Van-Doan Duong", "Thi-Phuong Le", "Ton Viet Ta"], "title": "Deep Learning for Automated Identification of Vietnamese Timber Species: A Tool for Ecological Monitoring and Conservation", "categories": ["cs.CV"], "comment": null, "summary": "Accurate identification of wood species plays a critical role in ecological\nmonitoring, biodiversity conservation, and sustainable forest management.\nTraditional classification approaches relying on macroscopic and microscopic\ninspection are labor-intensive and require expert knowledge. In this study, we\nexplore the application of deep learning to automate the classification of ten\nwood species commonly found in Vietnam. A custom image dataset was constructed\nfrom field-collected wood samples, and five state-of-the-art convolutional\nneural network architectures--ResNet50, EfficientNet, MobileViT, MobileNetV3,\nand ShuffleNetV2--were evaluated. Among these, ShuffleNetV2 achieved the best\nbalance between classification performance and computational efficiency, with\nan average accuracy of 99.29\\% and F1-score of 99.35\\% over 20 independent\nruns. These results demonstrate the potential of lightweight deep learning\nmodels for real-time, high-accuracy species identification in\nresource-constrained environments. Our work contributes to the growing field of\necological informatics by providing scalable, image-based solutions for\nautomated wood classification and forest biodiversity assessment.", "AI": {"tldr": "Deep learning automates wood species classification in Vietnam, with ShuffleNetV2 achieving 99.29% accuracy.", "motivation": "Traditional wood species identification is labor-intensive and requires expertise; automation is needed for efficiency.", "method": "Evaluated five CNN architectures (ResNet50, EfficientNet, MobileViT, MobileNetV3, ShuffleNetV2) on a custom dataset.", "result": "ShuffleNetV2 performed best with 99.29% accuracy and 99.35% F1-score.", "conclusion": "Lightweight deep learning models enable real-time, high-accuracy species identification in resource-limited settings."}}
{"id": "2508.11025", "pdf": "https://arxiv.org/pdf/2508.11025", "abs": "https://arxiv.org/abs/2508.11025", "authors": ["Laura L\u00fctzow", "Michael Eichelbeck", "Mykel J. Kochenderfer", "Matthias Althoff"], "title": "Zono-Conformal Prediction: Zonotope-Based Uncertainty Quantification for Regression and Classification Tasks", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY"], "comment": "Preprint. Under review", "summary": "Conformal prediction is a popular uncertainty quantification method that\naugments a base predictor with prediction sets with statistically valid\ncoverage guarantees. However, current methods are often computationally\nexpensive and data-intensive, as they require constructing an uncertainty model\nbefore calibration. Moreover, existing approaches typically represent the\nprediction sets with intervals, which limits their ability to capture\ndependencies in multi-dimensional outputs. We address these limitations by\nintroducing zono-conformal prediction, a novel approach inspired by interval\npredictor models and reachset-conformant identification that constructs\nprediction zonotopes with assured coverage. By placing zonotopic uncertainty\nsets directly into the model of the base predictor, zono-conformal predictors\ncan be identified via a single, data-efficient linear program. While we can\napply zono-conformal prediction to arbitrary nonlinear base predictors, we\nfocus on feed-forward neural networks in this work. Aside from regression\ntasks, we also construct optimal zono-conformal predictors in classification\nsettings where the output of an uncertain predictor is a set of possible\nclasses. We provide probabilistic coverage guarantees and present methods for\ndetecting outliers in the identification data. In extensive numerical\nexperiments, we show that zono-conformal predictors are less conservative than\ninterval predictor models and standard conformal prediction methods, while\nachieving a similar coverage over the test data.", "AI": {"tldr": "Zono-conformal prediction introduces prediction zonotopes for efficient, data-light uncertainty quantification with coverage guarantees, outperforming interval-based methods.", "motivation": "Address computational and data inefficiencies in conformal prediction, and limitations of interval-based sets in capturing multi-dimensional dependencies.", "method": "Uses zonotopic uncertainty sets integrated into base predictors, identified via a single linear program, applicable to nonlinear predictors like neural networks.", "result": "Less conservative than interval models and standard conformal methods, with similar test coverage and outlier detection capabilities.", "conclusion": "Zono-conformal prediction offers a computationally efficient, data-light alternative with robust coverage guarantees for uncertainty quantification."}}
{"id": "2508.11133", "pdf": "https://arxiv.org/pdf/2508.11133", "abs": "https://arxiv.org/abs/2508.11133", "authors": ["Tomer Wolfson", "Harsh Trivedi", "Mor Geva", "Yoav Goldberg", "Dan Roth", "Tushar Khot", "Ashish Sabharwal", "Reut Tsarfaty"], "title": "MoNaCo: More Natural and Complex Questions for Reasoning Across Dozens of Documents", "categories": ["cs.CL", "cs.AI", "cs.DB"], "comment": "Accepted for publication in Transactions of the Association for\n  Computational Linguistics (TACL), 2025. Authors pre-print", "summary": "Large language models (LLMs) are emerging as a go-to tool for querying\ninformation. However, current LLM benchmarks rarely feature natural questions\nthat are both information-seeking as well as genuinely time-consuming for\nhumans. To address this gap we introduce MoNaCo, a benchmark of 1,315 natural\nand complex questions that require dozens, and at times hundreds, of\nintermediate steps to solve -- far more than any existing QA benchmark. To\nbuild MoNaCo, we developed a decomposed annotation pipeline to elicit and\nmanually answer natural time-consuming questions at scale. Frontier LLMs\nevaluated on MoNaCo achieve at most 61.2% F1, hampered by low recall and\nhallucinations. Our results underscore the need for reasoning models that\nbetter handle the complexity and sheer breadth of real-world\ninformation-seeking questions -- with MoNaCo providing an effective resource\nfor tracking such progress. The MONACO benchmark, codebase, prompts and models\npredictions are publicly available at: https://tomerwolgithub.github.io/monaco", "AI": {"tldr": "MoNaCo is a new benchmark for evaluating LLMs on complex, time-consuming questions, highlighting their limitations in handling real-world information-seeking tasks.", "motivation": "Current LLM benchmarks lack natural, complex questions that mimic real-world information-seeking tasks, prompting the creation of MoNaCo.", "method": "Developed a decomposed annotation pipeline to manually answer 1,315 natural, time-consuming questions, forming the MoNaCo benchmark.", "result": "Frontier LLMs scored at most 61.2% F1 on MoNaCo, struggling with low recall and hallucinations.", "conclusion": "MoNaCo serves as a valuable resource for advancing LLMs to better handle complex, real-world questions."}}
{"id": "2508.11493", "pdf": "https://arxiv.org/pdf/2508.11493", "abs": "https://arxiv.org/abs/2508.11493", "authors": ["David H. Chan", "Mark Roberts", "Dana S. Nau"], "title": "Landmark-Assisted Monte Carlo Planning", "categories": ["cs.AI"], "comment": "To be published in the Proceedings of the 28th European Conference on\n  Artificial Intelligence", "summary": "Landmarks$\\unicode{x2013}$conditions that must be satisfied at some point in\nevery solution plan$\\unicode{x2013}$have contributed to major advancements in\nclassical planning, but they have seldom been used in stochastic domains. We\nformalize probabilistic landmarks and adapt the UCT algorithm to leverage them\nas subgoals to decompose MDPs; core to the adaptation is balancing between\ngreedy landmark achievement and final goal achievement. Our results in\nbenchmark domains show that well-chosen landmarks can significantly improve the\nperformance of UCT in online probabilistic planning, while the best balance of\ngreedy versus long-term goal achievement is problem-dependent. The results\nsuggest that landmarks can provide helpful guidance for anytime algorithms\nsolving MDPs.", "AI": {"tldr": "The paper formalizes probabilistic landmarks for stochastic domains, adapting UCT to use them as subgoals in MDPs, balancing greedy and long-term goal achievement. Results show improved UCT performance, with problem-dependent balance.", "motivation": "Landmarks are underutilized in stochastic domains despite their success in classical planning. This work explores their potential in probabilistic settings.", "method": "Probabilistic landmarks are formalized and integrated into UCT for MDPs, balancing greedy landmark achievement with final goal attainment.", "result": "Benchmark tests show landmarks significantly enhance UCT performance in online probabilistic planning, though the optimal balance varies by problem.", "conclusion": "Landmarks can effectively guide anytime algorithms in solving MDPs, with their impact dependent on problem-specific balancing."}}
{"id": "2508.10940", "pdf": "https://arxiv.org/pdf/2508.10940", "abs": "https://arxiv.org/abs/2508.10940", "authors": ["Nirmal Gaud", "Krishna Kumar Jha", "Jhimli Adhikari", "Adhini Nasarin P S", "Joydeep Das", "Samarth S Deshpande", "Nitasha Barara", "Vaduguru Venkata Ramya", "Santu Saha", "Mehmet Tarik Baran", "Sarangi Venkateshwarlu", "Anusha M D", "Surej Mouli", "Preeti Katiyar", "Vipin Kumar Chaudhary"], "title": "NIRMAL Pooling: An Adaptive Max Pooling Approach with Non-linear Activation for Enhanced Image Classification", "categories": ["cs.CV"], "comment": "6 pages, 2 figures", "summary": "This paper presents NIRMAL Pooling, a novel pooling layer for Convolutional\nNeural Networks (CNNs) that integrates adaptive max pooling with non-linear\nactivation function for image classification tasks. The acronym NIRMAL stands\nfor Non-linear Activation, Intermediate Aggregation, Reduction, Maximum,\nAdaptive, and Localized. By dynamically adjusting pooling parameters based on\ndesired output dimensions and applying a Rectified Linear Unit (ReLU)\nactivation post-pooling, NIRMAL Pooling improves robustness and feature\nexpressiveness. We evaluated its performance against standard Max Pooling on\nthree benchmark datasets: MNIST Digits, MNIST Fashion, and CIFAR-10. NIRMAL\nPooling achieves test accuracies of 99.25% (vs. 99.12% for Max Pooling) on\nMNIST Digits, 91.59% (vs. 91.44%) on MNIST Fashion, and 70.49% (vs. 68.87%) on\nCIFAR-10, demonstrating consistent improvements, particularly on complex\ndatasets. This work highlights the potential of NIRMAL Pooling to enhance CNN\nperformance in diverse image recognition tasks, offering a flexible and\nreliable alternative to traditional pooling methods.", "AI": {"tldr": "NIRMAL Pooling is a new CNN pooling layer combining adaptive max pooling with ReLU activation, outperforming standard Max Pooling on MNIST and CIFAR-10 datasets.", "motivation": "To enhance CNN performance by improving feature expressiveness and robustness in image classification tasks.", "method": "Integrates adaptive max pooling with ReLU activation, dynamically adjusting pooling parameters.", "result": "Achieves higher test accuracies: 99.25% (MNIST Digits), 91.59% (MNIST Fashion), 70.49% (CIFAR-10).", "conclusion": "NIRMAL Pooling is a flexible, reliable alternative to traditional pooling methods for CNNs."}}
{"id": "2508.11037", "pdf": "https://arxiv.org/pdf/2508.11037", "abs": "https://arxiv.org/abs/2508.11037", "authors": ["Oliver Ethan Richardson"], "title": "Learning with Confidence", "categories": ["cs.LG", "cs.AI", "math.DG"], "comment": "Accepted for oral UAI 2025, plus some additional modifications for\n  clarity", "summary": "We characterize a notion of confidence that arises in learning or updating\nbeliefs: the amount of trust one has in incoming information and its impact on\nthe belief state. This learner's confidence can be used alongside (and is\neasily mistaken for) probability or likelihood, but it is fundamentally a\ndifferent concept -- one that captures many familiar concepts in the\nliterature, including learning rates and number of training epochs, Shafer's\nweight of evidence, and Kalman gain. We formally axiomatize what it means to\nlearn with confidence, give two canonical ways of measuring confidence on a\ncontinuum, and prove that confidence can always be represented in this way.\nUnder additional assumptions, we derive more compact representations of\nconfidence-based learning in terms of vector fields and loss functions. These\nrepresentations induce an extended language of compound \"parallel\"\nobservations. We characterize Bayes Rule as the special case of an optimizing\nlearner whose loss representation is a linear expectation.", "AI": {"tldr": "The paper introduces a formal concept of 'confidence' in learning, distinct from probability, and provides axiomatic foundations and representations for it.", "motivation": "To clarify and formalize the concept of confidence in learning, distinguishing it from probability and linking it to existing literature.", "method": "Axiomatizes confidence, proposes two canonical measures, and derives compact representations (vector fields, loss functions).", "result": "Proves confidence can be represented canonically and links it to Bayes Rule under specific assumptions.", "conclusion": "Confidence is a distinct and formalizable concept in learning, with practical representations and connections to existing theories."}}
{"id": "2508.11163", "pdf": "https://arxiv.org/pdf/2508.11163", "abs": "https://arxiv.org/abs/2508.11163", "authors": ["Hikaru Asano", "Hiroki Ouchi", "Akira Kasuga", "Ryo Yonetani"], "title": "MobQA: A Benchmark Dataset for Semantic Understanding of Human Mobility Data through Question Answering", "categories": ["cs.CL"], "comment": "23 pages, 12 figures", "summary": "This paper presents MobQA, a benchmark dataset designed to evaluate the\nsemantic understanding capabilities of large language models (LLMs) for human\nmobility data through natural language question answering.\n  While existing models excel at predicting human movement patterns, it remains\nunobvious how much they can interpret the underlying reasons or semantic\nmeaning of those patterns. MobQA provides a comprehensive evaluation framework\nfor LLMs to answer questions about diverse human GPS trajectories spanning\ndaily to weekly granularities. It comprises 5,800 high-quality question-answer\npairs across three complementary question types: factual retrieval (precise\ndata extraction), multiple-choice reasoning (semantic inference), and free-form\nexplanation (interpretive description), which all require spatial, temporal,\nand semantic reasoning. Our evaluation of major LLMs reveals strong performance\non factual retrieval but significant limitations in semantic reasoning and\nexplanation question answering, with trajectory length substantially impacting\nmodel effectiveness. These findings demonstrate the achievements and\nlimitations of state-of-the-art LLMs for semantic mobility\nunderstanding.\\footnote{MobQA dataset is available at\nhttps://github.com/CyberAgentAILab/mobqa.}", "AI": {"tldr": "MobQA is a benchmark dataset for evaluating LLMs' semantic understanding of human mobility data via question answering, revealing strengths in factual retrieval but weaknesses in reasoning and explanation tasks.", "motivation": "Assess how well LLMs interpret the semantic meaning of human mobility patterns, beyond just predicting movement.", "method": "Created MobQA with 5,800 question-answer pairs across three types: factual retrieval, multiple-choice reasoning, and free-form explanation, requiring spatial, temporal, and semantic reasoning.", "result": "LLMs perform well on factual retrieval but struggle with semantic reasoning and explanation, with trajectory length affecting performance.", "conclusion": "MobQA highlights the capabilities and limitations of current LLMs in understanding semantic mobility, providing a framework for future improvements."}}
{"id": "2508.11524", "pdf": "https://arxiv.org/pdf/2508.11524", "abs": "https://arxiv.org/abs/2508.11524", "authors": ["Wenkai Yu", "Jianhang Tang", "Yang Zhang", "Shanjiang Tang", "Kebing Jin", "Hankz Hankui Zhuo"], "title": "Inspire or Predict? Exploring New Paradigms in Assisting Classical Planners with Large Language Models", "categories": ["cs.AI"], "comment": null, "summary": "Addressing large-scale planning problems has become one of the central\nchallenges in the planning community, deriving from the state-space explosion\ncaused by growing objects and actions. Recently, researchers have explored the\neffectiveness of leveraging Large Language Models (LLMs) to generate helpful\nactions and states to prune the search space. However, prior works have largely\noverlooked integrating LLMs with domain-specific knowledge to ensure valid\nplans. In this paper, we propose a novel LLM-assisted planner integrated with\nproblem decomposition, which first decomposes large planning problems into\nmultiple simpler sub-tasks. Then we explore two novel paradigms to utilize\nLLMs, i.e., LLM4Inspire and LLM4Predict, to assist problem decomposition, where\nLLM4Inspire provides heuristic guidance according to general knowledge and\nLLM4Predict employs domain-specific knowledge to infer intermediate conditions.\nWe empirically validate the effectiveness of our planner across multiple\ndomains, demonstrating the ability of search space partition when solving\nlarge-scale planning problems. The experimental results show that LLMs\neffectively locate feasible solutions when pruning the search space, where\ninfusing domain-specific knowledge into LLMs, i.e., LLM4Predict, holds\nparticular promise compared with LLM4Inspire, which offers general knowledge\nwithin LLMs.", "AI": {"tldr": "A novel LLM-assisted planner integrates problem decomposition and domain-specific knowledge to address large-scale planning problems, outperforming general-knowledge approaches.", "motivation": "Large-scale planning problems suffer from state-space explosion, and prior LLM-based methods lack domain-specific knowledge integration.", "method": "Proposes LLM4Inspire (general knowledge) and LLM4Predict (domain-specific knowledge) for problem decomposition and search space pruning.", "result": "Empirical validation shows LLM4Predict, leveraging domain knowledge, outperforms LLM4Inspire in locating feasible solutions.", "conclusion": "Integrating domain-specific knowledge with LLMs enhances planning efficiency, particularly in large-scale problems."}}
{"id": "2508.10942", "pdf": "https://arxiv.org/pdf/2508.10942", "abs": "https://arxiv.org/abs/2508.10942", "authors": ["Liming Xu", "Dave Towey", "Andrew P. French", "Steve Benford"], "title": "Topological Structure Description for Artcode Detection Using the Shape of Orientation Histogram", "categories": ["cs.CV", "cs.HC", "cs.MM", "I.4.10; I.5.4"], "comment": "This work is an extension of an ACM MM'17 workshop paper (Xu et al,\n  2017), which was completed in late 2017 and early 2018 during the first\n  author's doctoral studies at the University of Nottingham. This paper\n  includes 42 pages, 25 figures, 7 tables, and 13,536 words", "summary": "The increasing ubiquity of smartphones and resurgence of VR/AR techniques, it\nis expected that our everyday environment may soon be decorating with objects\nconnecting with virtual elements. Alerting to the presence of these objects is\ntherefore the first step for motivating follow-up further inspection and\ntriggering digital material attached to the objects. This work studies a\nspecial kind of these objects -- Artcodes -- a human-meaningful and\nmachine-readable decorative markers that camouflage themselves with freeform\nappearance by encoding information into their topology. We formulate this\nproblem of recongising the presence of Artcodes as Artcode proposal detection,\na distinct computer vision task that classifies topologically similar but\ngeometrically and semantically different objects as a same class. To deal with\nthis problem, we propose a new feature descriptor, called the shape of\norientation histogram, to describe the generic topological structure of an\nArtcode. We collect datasets and conduct comprehensive experiments to evaluate\nthe performance of the Artcode detection proposer built upon this new feature\nvector. Our experimental results show the feasibility of the proposed feature\nvector for representing topological structures and the effectiveness of the\nsystem for detecting Artcode proposals. Although this work is an initial\nattempt to develop a feature-based system for detecting topological objects\nlike Artcodes, it would open up new interaction opportunities and spark\npotential applications of topological object detection.", "AI": {"tldr": "The paper introduces Artcodes, decorative markers blending virtual and real worlds, and proposes a feature descriptor for detecting them.", "motivation": "To alert and enable interaction with virtual elements attached to real-world objects, focusing on Artcodes as a case study.", "method": "Develops a new feature descriptor, the shape of orientation histogram, for detecting Artcodes by their topological structure.", "result": "Experiments confirm the descriptor's effectiveness in detecting Artcodes, despite geometric and semantic variations.", "conclusion": "The work lays groundwork for topological object detection, promising new interaction possibilities."}}
{"id": "2508.11050", "pdf": "https://arxiv.org/pdf/2508.11050", "abs": "https://arxiv.org/abs/2508.11050", "authors": ["Ujas Shah", "Manuel Lladser", "Rebecca Morrison"], "title": "Conditional Independence Estimates for the Generalized Nonparanormal", "categories": ["cs.LG", "stat.ML"], "comment": "22 pages, 7 figures, 3 tables", "summary": "For general non-Gaussian distributions, the covariance and precision matrices\ndo not encode the independence structure of the variables, as they do for the\nmultivariate Gaussian. This paper builds on previous work to show that for a\nclass of non-Gaussian distributions -- those derived from diagonal\ntransformations of a Gaussian -- information about the conditional independence\nstructure can still be inferred from the precision matrix, provided the data\nmeet certain criteria, analogous to the Gaussian case. We call such\ntransformations of the Gaussian as the generalized nonparanormal. The functions\nthat define these transformations are, in a broad sense, arbitrary. We also\nprovide a simple and computationally efficient algorithm that leverages this\ntheory to recover conditional independence structure from the generalized\nnonparanormal data. The effectiveness of the proposed algorithm is demonstrated\nvia synthetic experiments and applications to real-world data.", "AI": {"tldr": "The paper shows that for a class of non-Gaussian distributions (generalized nonparanormal), conditional independence structure can still be inferred from the precision matrix under certain conditions, similar to the Gaussian case. A simple algorithm is proposed and validated with experiments.", "motivation": "For non-Gaussian distributions, covariance and precision matrices don't encode independence structures as they do for Gaussians. This work aims to extend such inference to a broader class of non-Gaussian distributions.", "method": "The paper introduces generalized nonparanormal distributions, derived from diagonal transformations of Gaussians, and proposes a computationally efficient algorithm to recover conditional independence structure from such data.", "result": "The algorithm effectively recovers conditional independence structures, as demonstrated through synthetic experiments and real-world applications.", "conclusion": "The generalized nonparanormal framework and proposed algorithm extend Gaussian-like independence inference to a broader class of non-Gaussian distributions, with practical utility."}}
{"id": "2508.11166", "pdf": "https://arxiv.org/pdf/2508.11166", "abs": "https://arxiv.org/abs/2508.11166", "authors": ["Anusha M D", "Deepthi Vikram", "Bharathi Raja Chakravarthi", "Parameshwar R Hegde"], "title": "Overcoming Low-Resource Barriers in Tulu: Neural Models and Corpus Creation for OffensiveLanguage Identification", "categories": ["cs.CL"], "comment": "20 pages, 3 tables, 3 figures. Submitted to Language Resources and\n  Evaluation (Springer)", "summary": "Tulu, a low-resource Dravidian language predominantly spoken in southern\nIndia, has limited computational resources despite its growing digital\npresence. This study presents the first benchmark dataset for Offensive\nLanguage Identification (OLI) in code-mixed Tulu social media content,\ncollected from YouTube comments across various domains. The dataset, annotated\nwith high inter-annotator agreement (Krippendorff's alpha = 0.984), includes\n3,845 comments categorized into four classes: Not Offensive, Not Tulu,\nOffensive Untargeted, and Offensive Targeted. We evaluate a suite of deep\nlearning models, including GRU, LSTM, BiGRU, BiLSTM, CNN, and attention-based\nvariants, alongside transformer architectures (mBERT, XLM-RoBERTa). The BiGRU\nmodel with self-attention achieves the best performance with 82% accuracy and a\n0.81 macro F1-score. Transformer models underperform, highlighting the\nlimitations of multilingual pretraining in code-mixed, under-resourced\ncontexts. This work lays the foundation for further NLP research in Tulu and\nsimilar low-resource, code-mixed languages.", "AI": {"tldr": "The paper introduces the first benchmark dataset for Offensive Language Identification (OLI) in code-mixed Tulu social media content, evaluating deep learning models and transformers. The BiGRU model with self-attention performs best, outperforming transformers.", "motivation": "Tulu, a low-resource Dravidian language, lacks computational resources despite its digital growth. This study aims to address the gap by creating a benchmark dataset for OLI in code-mixed Tulu.", "method": "A dataset of 3,845 YouTube comments was annotated and evaluated using deep learning models (GRU, LSTM, BiGRU, BiLSTM, CNN, attention-based variants) and transformers (mBERT, XLM-RoBERTa).", "result": "The BiGRU model with self-attention achieved 82% accuracy and a 0.81 macro F1-score, while transformer models underperformed.", "conclusion": "The study provides a foundation for NLP research in Tulu and similar low-resource, code-mixed languages, highlighting the limitations of multilingual pretraining in such contexts."}}
{"id": "1504.08319", "pdf": "https://arxiv.org/pdf/1504.08319", "abs": "https://arxiv.org/abs/1504.08319", "authors": ["Changshuai Wei", "Robert C. Elston", "Qing Lu"], "title": "A weighted U statistic for association analysis considering genetic heterogeneity", "categories": ["stat.ME", "cs.AI", "cs.LG"], "comment": null, "summary": "Converging evidence suggests that common complex diseases with the same or\nsimilar clinical manifestations could have different underlying genetic\netiologies. While current research interests have shifted toward uncovering\nrare variants and structural variations predisposing to human diseases, the\nimpact of heterogeneity in genetic studies of complex diseases has been largely\noverlooked. Most of the existing statistical methods assume the disease under\ninvestigation has a homogeneous genetic effect and could, therefore, have low\npower if the disease undergoes heterogeneous pathophysiological and etiological\nprocesses. In this paper, we propose a heterogeneity weighted U (HWU) method\nfor association analyses considering genetic heterogeneity. HWU can be applied\nto various types of phenotypes (e.g., binary and continuous) and is\ncomputationally effcient for high- dimensional genetic data. Through\nsimulations, we showed the advantage of HWU when the underlying genetic\netiology of a disease was heterogeneous, as well as the robustness of HWU\nagainst different model assumptions (e.g., phenotype distributions). Using HWU,\nwe conducted a genome-wide analysis of nicotine dependence from the Study of\nAddiction: Genetics and Environments (SAGE) dataset. The genome-wide analysis\nof nearly one million genetic markers took 7 hours, identifying heterogeneous\neffects of two new genes (i.e., CYP3A5 and IKBKB) on nicotine dependence.", "AI": {"tldr": "The paper introduces the HWU method for genetic association analyses, addressing heterogeneity in complex diseases, and demonstrates its effectiveness through simulations and a genome-wide study of nicotine dependence.", "motivation": "Current methods overlook genetic heterogeneity in complex diseases, leading to reduced power in association studies. The HWU method aims to address this gap.", "method": "Proposes the heterogeneity weighted U (HWU) method, applicable to various phenotypes and computationally efficient for high-dimensional data.", "result": "Simulations show HWU's advantage in heterogeneous genetic scenarios. A genome-wide analysis identified two new genes (CYP3A5 and IKBKB) linked to nicotine dependence.", "conclusion": "HWU is a robust and efficient tool for genetic association studies, particularly in diseases with heterogeneous etiologies."}}
{"id": "2508.10943", "pdf": "https://arxiv.org/pdf/2508.10943", "abs": "https://arxiv.org/abs/2508.10943", "authors": ["Christian D\u00fcreth", "Jan Cond\u00e9-Wolter", "Marek Danczak", "Karsten Tittmann", "J\u00f6rn Jaschinski", "Andreas Hornig", "Maik Gude"], "title": "Analysis of the Compaction Behavior of Textile Reinforcements in Low-Resolution In-Situ CT Scans via Machine-Learning and Descriptor-Based Methods", "categories": ["cs.CV", "cond-mat.mtrl-sci", "physics.app-ph"], "comment": "submitted to Elsevier Composite Part C: Open Access\n  (JCOMC-D-25-00212), 16 pages, 8 Figures, and 3 Tables", "summary": "A detailed understanding of material structure across multiple scales is\nessential for predictive modeling of textile-reinforced composites. Nesting --\ncharacterized by the interlocking of adjacent fabric layers through local\ninterpenetration and misalignment of yarns -- plays a critical role in defining\nmechanical properties such as stiffness, permeability, and damage tolerance.\nThis study presents a framework to quantify nesting behavior in dry textile\nreinforcements under compaction using low-resolution computed tomography (CT).\nIn-situ compaction experiments were conducted on various stacking\nconfigurations, with CT scans acquired at 20.22 $\\mu$m per voxel resolution. A\ntailored 3D{-}UNet enabled semantic segmentation of matrix, weft, and fill\nphases across compaction stages corresponding to fiber volume contents of\n50--60 %. The model achieved a minimum mean Intersection-over-Union of 0.822\nand an $F1$ score of 0.902. Spatial structure was subsequently analyzed using\nthe two-point correlation function $S_2$, allowing for probabilistic extraction\nof average layer thickness and nesting degree. The results show strong\nagreement with micrograph-based validation. This methodology provides a robust\napproach for extracting key geometrical features from industrially relevant CT\ndata and establishes a foundation for reverse modeling and descriptor-based\nstructural analysis of composite preforms.", "AI": {"tldr": "A framework using low-resolution CT and 3D-UNet quantifies nesting in textile composites, achieving high accuracy in segmentation and structural analysis.", "motivation": "Understanding nesting in textile composites is crucial for modeling mechanical properties like stiffness and damage tolerance.", "method": "In-situ compaction experiments with CT scans, semantic segmentation via 3D-UNet, and analysis using two-point correlation function.", "result": "High segmentation accuracy (IoU 0.822, F1 0.902) and validated structural metrics like layer thickness and nesting degree.", "conclusion": "The method robustly extracts key features from CT data, enabling reverse modeling and structural analysis of composites."}}
{"id": "2508.11053", "pdf": "https://arxiv.org/pdf/2508.11053", "abs": "https://arxiv.org/abs/2508.11053", "authors": ["Sam Chauhan", "Estelle Duguet", "Karthik Ramakrishnan", "Hugh Van Deventer", "Jack Kruger", "Ranjan Subbaraman"], "title": "SHLIME: Foiling adversarial attacks fooling SHAP and LIME", "categories": ["cs.LG", "cs.CR"], "comment": "7 pages, 7 figures", "summary": "Post hoc explanation methods, such as LIME and SHAP, provide interpretable\ninsights into black-box classifiers and are increasingly used to assess model\nbiases and generalizability. However, these methods are vulnerable to\nadversarial manipulation, potentially concealing harmful biases. Building on\nthe work of Slack et al. (2020), we investigate the susceptibility of LIME and\nSHAP to biased models and evaluate strategies for improving robustness. We\nfirst replicate the original COMPAS experiment to validate prior findings and\nestablish a baseline. We then introduce a modular testing framework enabling\nsystematic evaluation of augmented and ensemble explanation approaches across\nclassifiers of varying performance. Using this framework, we assess multiple\nLIME/SHAP ensemble configurations on out-of-distribution models, comparing\ntheir resistance to bias concealment against the original methods. Our results\nidentify configurations that substantially improve bias detection, highlighting\ntheir potential for enhancing transparency in the deployment of high-stakes\nmachine learning systems.", "AI": {"tldr": "The paper investigates the vulnerability of LIME and SHAP to adversarial manipulation and proposes ensemble methods to improve bias detection in black-box classifiers.", "motivation": "Post hoc explanation methods like LIME and SHAP are widely used but can be manipulated to hide biases, raising concerns about their reliability in high-stakes applications.", "method": "The study replicates the COMPAS experiment, introduces a modular testing framework, and evaluates ensemble configurations of LIME/SHAP for robustness against bias concealment.", "result": "Certain ensemble configurations significantly improve bias detection, outperforming original methods.", "conclusion": "The findings suggest that ensemble approaches can enhance transparency and reliability in deploying machine learning systems."}}
{"id": "2508.11184", "pdf": "https://arxiv.org/pdf/2508.11184", "abs": "https://arxiv.org/abs/2508.11184", "authors": ["Tao Wu", "Jingyuan Chen", "Wang Lin", "Jian Zhan", "Mengze Li", "Kun Kuang", "Fei Wu"], "title": "Personalized Distractor Generation via MCTS-Guided Reasoning Reconstruction", "categories": ["cs.CL"], "comment": null, "summary": "Distractors, incorrect but plausible answer choices in multiple-choice\nquestions (MCQs), play a critical role in educational assessment by diagnosing\nstudent misconceptions. Recent work has leveraged large language models (LLMs)\nto generate shared, group-level distractors by learning common error patterns\nacross large student populations. However, such distractors often fail to\ncapture the diverse reasoning errors of individual students, limiting their\ndiagnostic effectiveness. To address this limitation, we introduce the task of\npersonalized distractor generation, which aims to generate tailored distractors\nbased on individual misconceptions inferred from each student's past\nquestion-answering (QA) records, ensuring every student receives options that\neffectively exposes their specific reasoning errors. While promising, this task\nis challenging because each student typically has only a few QA records, which\noften lack the student's underlying reasoning processes, making training-based\ngroup-level approaches infeasible. To overcome this, we propose a training-free\ntwo-stage framework. In the first stage, we construct a student-specific\nmisconception prototype by applying Monte Carlo Tree Search (MCTS) to recover\nthe student's reasoning trajectories from past incorrect answers. In the second\nstage, this prototype guides the simulation of the student's reasoning on new\nquestions, enabling the generation of personalized distractors that align with\nthe student's recurring misconceptions. Experiments show that our approach\nachieves the best performance in generating plausible, personalized distractors\nfor 140 students, and also effectively generalizes to group-level settings,\nhighlighting its robustness and adaptability.", "AI": {"tldr": "The paper introduces personalized distractor generation for MCQs, addressing the limitation of group-level distractors by tailoring them to individual student misconceptions using a training-free two-stage framework.", "motivation": "Group-level distractors from LLMs often fail to capture individual student reasoning errors, limiting diagnostic effectiveness.", "method": "A training-free two-stage framework: (1) MCTS to recover reasoning trajectories from past incorrect answers, (2) simulating reasoning to generate personalized distractors.", "result": "The approach outperforms in generating plausible, personalized distractors for 140 students and generalizes to group-level settings.", "conclusion": "The proposed method effectively addresses individual misconceptions, enhancing diagnostic assessment in education."}}
{"id": "1505.01179", "pdf": "https://arxiv.org/pdf/1505.01179", "abs": "https://arxiv.org/abs/1505.01179", "authors": ["Changshuai Wei", "Qing Lu"], "title": "A Generalized Similarity U Test for Multivariate Analysis of Sequencing Data", "categories": ["stat.ME", "cs.AI", "cs.LG"], "comment": null, "summary": "Sequencing-based studies are emerging as a major tool for genetic association\nstudies of complex diseases. These studies pose great challenges to the\ntraditional statistical methods (e.g., single-locus analyses based on\nregression methods) because of the high-dimensionality of data and the low\nfrequency of genetic variants. In addition, there is a great interest in\nbiology and epidemiology to identify genetic risk factors contributed to\nmultiple disease phenotypes. The multiple phenotypes can often follow different\ndistributions, which violates the assumptions of most current methods. In this\npaper, we propose a generalized similarity U test, referred to as GSU. GSU is a\nsimilarity-based test and can handle high-dimensional genotypes and phenotypes.\nWe studied the theoretical properties of GSU, and provided the efficient\np-value calculation for association test as well as the sample size and power\ncalculation for the study design. Through simulation, we found that GSU had\nadvantages over existing methods in terms of power and robustness to phenotype\ndistributions. Finally, we used GSU to perform a multivariate analysis of\nsequencing data in the Dallas Heart Study and identified a joint association of\n4 genes with 5 metabolic related phenotypes.", "AI": {"tldr": "The paper introduces GSU, a generalized similarity U test, to address challenges in genetic association studies with high-dimensional data and multiple phenotypes of varying distributions.", "motivation": "Traditional statistical methods struggle with high-dimensional genetic data and diverse phenotype distributions, necessitating a more robust approach.", "method": "Proposes GSU, a similarity-based test for high-dimensional genotypes and phenotypes, with theoretical analysis and efficient p-value calculation.", "result": "Simulations show GSU outperforms existing methods in power and robustness. Applied to the Dallas Heart Study, it identified joint gene-phenotype associations.", "conclusion": "GSU is a powerful and flexible tool for genetic association studies, especially with complex data and multiple phenotypes."}}
{"id": "2508.10945", "pdf": "https://arxiv.org/pdf/2508.10945", "abs": "https://arxiv.org/abs/2508.10945", "authors": ["Rishi Raj Sahoo", "Surbhi Saswati Mohanty", "Subhankar Mishra"], "title": "iWatchRoad: Scalable Detection and Geospatial Visualization of Potholes for Smart Cities", "categories": ["cs.CV", "cs.LG"], "comment": "Under review", "summary": "Potholes on the roads are a serious hazard and maintenance burden. This poses\na significant threat to road safety and vehicle longevity, especially on the\ndiverse and under-maintained roads of India. In this paper, we present a\ncomplete end-to-end system called iWatchRoad for automated pothole detection,\nGlobal Positioning System (GPS) tagging, and real time mapping using\nOpenStreetMap (OSM). We curated a large, self-annotated dataset of over 7,000\nframes captured across various road types, lighting conditions, and weather\nscenarios unique to Indian environments, leveraging dashcam footage. This\ndataset is used to fine-tune, Ultralytics You Only Look Once (YOLO) model to\nperform real time pothole detection, while a custom Optical Character\nRecognition (OCR) module was employed to extract timestamps directly from video\nframes. The timestamps are synchronized with GPS logs to geotag each detected\npotholes accurately. The processed data includes the potholes' details and\nframes as metadata is stored in a database and visualized via a user friendly\nweb interface using OSM. iWatchRoad not only improves detection accuracy under\nchallenging conditions but also provides government compatible outputs for road\nassessment and maintenance planning through the metadata visible on the\nwebsite. Our solution is cost effective, hardware efficient, and scalable,\noffering a practical tool for urban and rural road management in developing\nregions, making the system automated. iWatchRoad is available at\nhttps://smlab.niser.ac.in/project/iwatchroad", "AI": {"tldr": "iWatchRoad is an automated system for detecting and mapping potholes using dashcam footage, YOLO for detection, OCR for timestamps, and OSM for visualization, tailored for Indian road conditions.", "motivation": "Potholes pose safety and maintenance challenges, especially in India's diverse road conditions, necessitating an automated, scalable solution.", "method": "Uses YOLO for real-time pothole detection, OCR for timestamp extraction, GPS for geotagging, and OSM for mapping, trained on a custom dataset of 7,000 frames.", "result": "Achieves accurate pothole detection and mapping, providing actionable data for road maintenance via a user-friendly web interface.", "conclusion": "iWatchRoad is a cost-effective, scalable solution for improving road safety and maintenance in developing regions."}}
{"id": "2508.11075", "pdf": "https://arxiv.org/pdf/2508.11075", "abs": "https://arxiv.org/abs/2508.11075", "authors": ["Hyunwoo Yoo", "Gail Rosen"], "title": "Abundance-Aware Set Transformer for Microbiome Sample Embedding", "categories": ["cs.LG"], "comment": null, "summary": "Microbiome sample representation to input into LLMs is essential for\ndownstream tasks such as phenotype prediction and environmental classification.\nWhile prior studies have explored embedding-based representations of each\nmicrobiome sample, most rely on simple averaging over sequence embeddings,\noften overlooking the biological importance of taxa abundance. In this work, we\npropose an abundance-aware variant of the Set Transformer to construct\nfixed-size sample-level embeddings by weighting sequence embeddings according\nto their relative abundance. Without modifying the model architecture, we\nreplicate embedding vectors proportional to their abundance and apply\nself-attention-based aggregation. Our method outperforms average pooling and\nunweighted Set Transformers on real-world microbiome classification tasks,\nachieving perfect performance in some cases. These results demonstrate the\nutility of abundance-aware aggregation for robust and biologically informed\nmicrobiome representation. To the best of our knowledge, this is one of the\nfirst approaches to integrate sequence-level abundance into Transformer-based\nsample embeddings.", "AI": {"tldr": "Proposes an abundance-aware Set Transformer for microbiome sample representation, outperforming average pooling and unweighted methods in classification tasks.", "motivation": "Prior methods overlook taxa abundance in microbiome sample embeddings, limiting biological relevance.", "method": "Uses an abundance-aware Set Transformer to weight sequence embeddings by abundance and applies self-attention-based aggregation.", "result": "Outperforms average pooling and unweighted Set Transformers, achieving perfect performance in some cases.", "conclusion": "Demonstrates the utility of abundance-aware aggregation for biologically informed microbiome representation."}}
{"id": "2508.11189", "pdf": "https://arxiv.org/pdf/2508.11189", "abs": "https://arxiv.org/abs/2508.11189", "authors": ["Chenyang Le", "Yinfeng Xia", "Huiyan Li", "Manhong Wang", "Yutao Sun", "Xingyang Ma", "Yanmin Qian"], "title": "Novel Parasitic Dual-Scale Modeling for Efficient and Accurate Multilingual Speech Translation", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Interspeech 2025", "summary": "Recent advancements in speech-to-text translation have led to the development\nof multilingual models capable of handling multiple language pairs\nsimultaneously. However, these unified models often suffer from large parameter\nsizes, making it challenging to balance inference efficiency and performance,\nparticularly in local deployment scenarios. We propose an innovative Parasitic\nDual-Scale Approach, which combines an enhanced speculative sampling method\nwith model compression and knowledge distillation techniques. Building on the\nWhisper Medium model, we enhance it for multilingual speech translation into\nwhisperM2M, and integrate our novel KVSPN module, achieving state-of-the-art\n(SOTA) performance across six popular languages with improved inference\nefficiency. KVSPN enables a 40\\% speedup with no BLEU score degradation.\nCombined with distillation methods, it represents a 2.6$\\times$ speedup over\nthe original Whisper Medium with superior performance.", "AI": {"tldr": "The paper introduces a Parasitic Dual-Scale Approach to improve multilingual speech-to-text translation, combining speculative sampling, model compression, and knowledge distillation for efficiency and performance.", "motivation": "Addressing the challenge of balancing inference efficiency and performance in multilingual speech-to-text translation models, which often have large parameter sizes.", "method": "Proposes a Parasitic Dual-Scale Approach, integrating enhanced speculative sampling, model compression, and knowledge distillation, building on the Whisper Medium model.", "result": "Achieves SOTA performance across six languages with a 40% speedup (no BLEU degradation) and 2.6x speedup over Whisper Medium with superior performance.", "conclusion": "The approach effectively balances efficiency and performance, making it suitable for local deployment scenarios."}}
{"id": "1505.01204", "pdf": "https://arxiv.org/pdf/1505.01204", "abs": "https://arxiv.org/abs/1505.01204", "authors": ["Changshuai Wei", "Ming Li", "Zihuai He", "Olga Vsevolozhskaya", "Daniel J. Schaid", "Qing Lu"], "title": "A Weighted U Statistic for Genetic Association Analyses of Sequencing Data", "categories": ["stat.ME", "cs.AI", "cs.LG", "q-bio.QM"], "comment": null, "summary": "With advancements in next generation sequencing technology, a massive amount\nof sequencing data are generated, offering a great opportunity to\ncomprehensively investigate the role of rare variants in the genetic etiology\nof complex diseases. Nevertheless, this poses a great challenge for the\nstatistical analysis of high-dimensional sequencing data. The association\nanalyses based on traditional statistical methods suffer substantial power loss\nbecause of the low frequency of genetic variants and the extremely high\ndimensionality of the data. We developed a weighted U statistic, referred to as\nWU-seq, for the high-dimensional association analysis of sequencing data. Based\non a non-parametric U statistic, WU-SEQ makes no assumption of the underlying\ndisease model and phenotype distribution, and can be applied to a variety of\nphenotypes. Through simulation studies and an empirical study, we showed that\nWU-SEQ outperformed a commonly used SKAT method when the underlying assumptions\nwere violated (e.g., the phenotype followed a heavy-tailed distribution). Even\nwhen the assumptions were satisfied, WU-SEQ still attained comparable\nperformance to SKAT. Finally, we applied WU-seq to sequencing data from the\nDallas Heart Study (DHS), and detected an association between ANGPTL 4 and very\nlow density lipoprotein cholesterol.", "AI": {"tldr": "WU-SEQ, a weighted U statistic, is introduced for high-dimensional sequencing data analysis, outperforming SKAT in non-ideal conditions and matching it otherwise.", "motivation": "The challenge of analyzing high-dimensional sequencing data with rare variants and low frequency, where traditional methods lose power.", "method": "Developed WU-SEQ, a non-parametric U statistic, requiring no assumptions about disease models or phenotype distributions.", "result": "WU-SEQ outperformed SKAT when assumptions were violated and matched its performance otherwise. Detected ANGPTL4 association in DHS data.", "conclusion": "WU-SEQ is a robust tool for sequencing data analysis, especially in non-ideal scenarios."}}
{"id": "2508.10946", "pdf": "https://arxiv.org/pdf/2508.10946", "abs": "https://arxiv.org/abs/2508.10946", "authors": ["Wonho Lee", "Hyunsik Na", "Jisu Lee", "Daeseon Choi"], "title": "IPG: Incremental Patch Generation for Generalized Adversarial Patch Training", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "The advent of adversarial patches poses a significant challenge to the\nrobustness of AI models, particularly in the domain of computer vision tasks\nsuch as object detection. In contradistinction to traditional adversarial\nexamples, these patches target specific regions of an image, resulting in the\nmalfunction of AI models. This paper proposes Incremental Patch Generation\n(IPG), a method that generates adversarial patches up to 11.1 times more\nefficiently than existing approaches while maintaining comparable attack\nperformance. The efficacy of IPG is demonstrated by experiments and ablation\nstudies including YOLO's feature distribution visualization and adversarial\ntraining results, which show that it produces well-generalized patches that\neffectively cover a broader range of model vulnerabilities. Furthermore,\nIPG-generated datasets can serve as a robust knowledge foundation for\nconstructing a robust model, enabling structured representation, advanced\nreasoning, and proactive defenses in AI security ecosystems. The findings of\nthis study suggest that IPG has considerable potential for future utilization\nnot only in adversarial patch defense but also in real-world applications such\nas autonomous vehicles, security systems, and medical imaging, where AI models\nmust remain resilient to adversarial attacks in dynamic and high-stakes\nenvironments.", "AI": {"tldr": "IPG is a method for generating adversarial patches 11.1x more efficiently than existing approaches, with strong attack performance and potential for real-world applications.", "motivation": "Address the challenge of adversarial patches in AI models, especially in computer vision tasks like object detection, by improving efficiency and effectiveness.", "method": "Proposes Incremental Patch Generation (IPG), which generates adversarial patches efficiently and includes experiments like YOLO feature visualization and adversarial training.", "result": "IPG produces well-generalized patches, covers broader model vulnerabilities, and its datasets aid in building robust AI models.", "conclusion": "IPG shows promise for adversarial defense and real-world applications like autonomous vehicles and medical imaging."}}
{"id": "2508.11084", "pdf": "https://arxiv.org/pdf/2508.11084", "abs": "https://arxiv.org/abs/2508.11084", "authors": ["Thanasis Schoinas", "Ghulam Qadir"], "title": "A Feasibility Experiment on the Application of Predictive Coding to Instant Messaging Corpora", "categories": ["cs.LG"], "comment": null, "summary": "Predictive coding, the term used in the legal industry for document\nclassification using machine learning, presents additional challenges when the\ndataset comprises instant messages, due to their informal nature and smaller\nsizes. In this paper, we exploit a data management workflow to group messages\ninto day chats, followed by feature selection and a logistic regression\nclassifier to provide an economically feasible predictive coding solution. We\nalso improve the solution's baseline model performance by dimensionality\nreduction, with focus on quantitative features. We test our methodology on an\nInstant Bloomberg dataset, rich in quantitative information. In parallel, we\nprovide an example of the cost savings of our approach.", "AI": {"tldr": "A workflow for classifying informal instant messages using logistic regression and dimensionality reduction, tested on Instant Bloomberg data, showing cost savings.", "motivation": "Address challenges in predictive coding for informal, small-sized instant messages.", "method": "Group messages into day chats, feature selection, logistic regression, and dimensionality reduction.", "result": "Improved baseline model performance and demonstrated cost savings.", "conclusion": "Economically feasible solution for predictive coding in instant messaging datasets."}}
{"id": "2508.11197", "pdf": "https://arxiv.org/pdf/2508.11197", "abs": "https://arxiv.org/abs/2508.11197", "authors": ["Ahmad Mousavi", "Yeganeh Abdollahinejad", "Roberto Corizzo", "Nathalie Japkowicz", "Zois Boukouvalas"], "title": "E-CaTCH: Event-Centric Cross-Modal Attention with Temporal Consistency and Class-Imbalance Handling for Misinformation Detection", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SI"], "comment": null, "summary": "Detecting multimodal misinformation on social media remains challenging due\nto inconsistencies between modalities, changes in temporal patterns, and\nsubstantial class imbalance. Many existing methods treat posts independently\nand fail to capture the event-level structure that connects them across time\nand modality. We propose E-CaTCH, an interpretable and scalable framework for\nrobustly detecting misinformation. If needed, E-CaTCH clusters posts into\npseudo-events based on textual similarity and temporal proximity, then\nprocesses each event independently. Within each event, textual and visual\nfeatures are extracted using pre-trained BERT and ResNet encoders, refined via\nintra-modal self-attention, and aligned through bidirectional cross-modal\nattention. A soft gating mechanism fuses these representations to form\ncontextualized, content-aware embeddings of each post. To model temporal\nevolution, E-CaTCH segments events into overlapping time windows and uses a\ntrend-aware LSTM, enhanced with semantic shift and momentum signals, to encode\nnarrative progression over time. Classification is performed at the event\nlevel, enabling better alignment with real-world misinformation dynamics. To\naddress class imbalance and promote stable learning, the model integrates\nadaptive class weighting, temporal consistency regularization, and hard-example\nmining. The total loss is aggregated across all events. Extensive experiments\non Fakeddit, IND, and COVID-19 MISINFOGRAPH demonstrate that E-CaTCH\nconsistently outperforms state-of-the-art baselines. Cross-dataset evaluations\nfurther demonstrate its robustness, generalizability, and practical\napplicability across diverse misinformation scenarios.", "AI": {"tldr": "E-CaTCH is a scalable framework for detecting multimodal misinformation by clustering posts into pseudo-events, aligning features across modalities, and modeling temporal evolution, outperforming existing methods.", "motivation": "Challenges in detecting misinformation due to modality inconsistencies, temporal changes, and class imbalance, with existing methods failing to capture event-level connections.", "method": "Clusters posts into pseudo-events, extracts and aligns textual/visual features, models temporal evolution with a trend-aware LSTM, and uses adaptive techniques for class imbalance.", "result": "Outperforms state-of-the-art baselines on datasets like Fakeddit, IND, and COVID-19 MISINFOGRAPH, showing robustness and generalizability.", "conclusion": "E-CaTCH effectively addresses misinformation detection by integrating multimodal and temporal modeling, proving practical across diverse scenarios."}}
{"id": "1505.01206", "pdf": "https://arxiv.org/pdf/1505.01206", "abs": "https://arxiv.org/abs/1505.01206", "authors": ["Changshuai Wei", "Daniel J. Schaid", "Qing Lu"], "title": "Trees Assembling Mann Whitney Approach for Detecting Genome-wide Joint Association among Low Marginal Effect loci", "categories": ["q-bio.QM", "cs.AI", "cs.LG", "stat.CO", "stat.ML"], "comment": null, "summary": "Common complex diseases are likely influenced by the interplay of hundreds,\nor even thousands, of genetic variants. Converging evidence shows that genetic\nvariants with low marginal effects (LME) play an important role in disease\ndevelopment. Despite their potential significance, discovering LME genetic\nvariants and assessing their joint association on high dimensional data (e.g.,\ngenome wide association studies) remain a great challenge. To facilitate joint\nassociation analysis among a large ensemble of LME genetic variants, we\nproposed a computationally efficient and powerful approach, which we call Trees\nAssembling Mann whitney (TAMW). Through simulation studies and an empirical\ndata application, we found that TAMW outperformed multifactor dimensionality\nreduction (MDR) and the likelihood ratio based Mann whitney approach (LRMW)\nwhen the underlying complex disease involves multiple LME loci and their\ninteractions. For instance, in a simulation with 20 interacting LME loci, TAMW\nattained a higher power (power=0.931) than both MDR (power=0.599) and LRMW\n(power=0.704). In an empirical study of 29 known Crohn's disease (CD) loci,\nTAMW also identified a stronger joint association with CD than those detected\nby MDR and LRMW. Finally, we applied TAMW to Wellcome Trust CD GWAS to conduct\na genome wide analysis. The analysis of 459K single nucleotide polymorphisms\nwas completed in 40 hours using parallel computing, and revealed a joint\nassociation predisposing to CD (p-value=2.763e-19). Further analysis of the\nnewly discovered association suggested that 13 genes, such as ATG16L1 and\nLACC1, may play an important role in CD pathophysiological and etiological\nprocesses.", "AI": {"tldr": "TAMW is a new method for analyzing joint associations of low marginal effect genetic variants in complex diseases, outperforming existing methods like MDR and LRMW in simulations and empirical studies, including Crohn's disease.", "motivation": "Discovering and assessing the joint association of low marginal effect genetic variants in complex diseases is challenging, necessitating a more efficient and powerful approach.", "method": "Proposed Trees Assembling Mann Whitney (TAMW), a computationally efficient method for joint association analysis of low marginal effect variants.", "result": "TAMW outperformed MDR and LRMW in simulations (higher power) and empirical studies (stronger joint associations), identifying 13 genes potentially linked to Crohn's disease.", "conclusion": "TAMW is a powerful tool for genome-wide analysis of low marginal effect genetic variants, revealing significant associations and potential disease mechanisms."}}
{"id": "2508.10947", "pdf": "https://arxiv.org/pdf/2508.10947", "abs": "https://arxiv.org/abs/2508.10947", "authors": ["Ronghao Xu", "Zhen Huang", "Yangbo Wei", "Xiaoqian Zhou", "Zikang Xu", "Ting Liu", "Zihang Jiang", "S. Kevin Zhou"], "title": "MedAtlas: Evaluating LLMs for Multi-Round, Multi-Task Medical Reasoning Across Diverse Imaging Modalities and Clinical Text", "categories": ["cs.CV"], "comment": null, "summary": "Artificial intelligence has demonstrated significant potential in clinical\ndecision-making; however, developing models capable of adapting to diverse\nreal-world scenarios and performing complex diagnostic reasoning remains a\nmajor challenge. Existing medical multi-modal benchmarks are typically limited\nto single-image, single-turn tasks, lacking multi-modal medical image\nintegration and failing to capture the longitudinal and multi-modal interactive\nnature inherent to clinical practice. To address this gap, we introduce\nMedAtlas, a novel benchmark framework designed to evaluate large language\nmodels on realistic medical reasoning tasks. MedAtlas is characterized by four\nkey features: multi-turn dialogue, multi-modal medical image interaction,\nmulti-task integration, and high clinical fidelity. It supports four core\ntasks: open-ended multi-turn question answering, closed-ended multi-turn\nquestion answering, multi-image joint reasoning, and comprehensive disease\ndiagnosis. Each case is derived from real diagnostic workflows and incorporates\ntemporal interactions between textual medical histories and multiple imaging\nmodalities, including CT, MRI, PET, ultrasound, and X-ray, requiring models to\nperform deep integrative reasoning across images and clinical texts. MedAtlas\nprovides expert-annotated gold standards for all tasks. Furthermore, we propose\ntwo novel evaluation metrics: Round Chain Accuracy and Error Propagation\nResistance. Benchmark results with existing multi-modal models reveal\nsubstantial performance gaps in multi-stage clinical reasoning. MedAtlas\nestablishes a challenging evaluation platform to advance the development of\nrobust and trustworthy medical AI.", "AI": {"tldr": "MedAtlas is a benchmark framework for evaluating large language models in realistic medical reasoning tasks, featuring multi-turn dialogue, multi-modal image interaction, and high clinical fidelity.", "motivation": "To address the limitations of existing medical benchmarks, which lack multi-modal integration and fail to mimic real-world clinical practice.", "method": "Introduces MedAtlas with four core tasks: multi-turn QA, multi-image reasoning, and disease diagnosis, using real diagnostic workflows and expert-annotated data.", "result": "Benchmark results show significant performance gaps in existing models for multi-stage clinical reasoning.", "conclusion": "MedAtlas provides a robust platform to advance trustworthy medical AI development."}}
{"id": "2508.11086", "pdf": "https://arxiv.org/pdf/2508.11086", "abs": "https://arxiv.org/abs/2508.11086", "authors": ["Emily Liu", "Kuan Han", "Minfeng Zhan", "Bocheng Zhao", "Guanyu Mu", "Yang Song"], "title": "Relative Advantage Debiasing for Watch-Time Prediction in Short-Video Recommendation", "categories": ["cs.LG", "cs.IR"], "comment": null, "summary": "Watch time is widely used as a proxy for user satisfaction in video\nrecommendation platforms. However, raw watch times are influenced by\nconfounding factors such as video duration, popularity, and individual user\nbehaviors, potentially distorting preference signals and resulting in biased\nrecommendation models. We propose a novel relative advantage debiasing\nframework that corrects watch time by comparing it to empirically derived\nreference distributions conditioned on user and item groups. This approach\nyields a quantile-based preference signal and introduces a two-stage\narchitecture that explicitly separates distribution estimation from preference\nlearning. Additionally, we present distributional embeddings to efficiently\nparameterize watch-time quantiles without requiring online sampling or storage\nof historical data. Both offline and online experiments demonstrate significant\nimprovements in recommendation accuracy and robustness compared to existing\nbaseline methods.", "AI": {"tldr": "A framework to debias watch time in video recommendations by comparing it to reference distributions, improving accuracy and robustness.", "motivation": "Raw watch times are confounded by factors like video duration and popularity, leading to biased recommendations.", "method": "Proposes a relative advantage debiasing framework using quantile-based preference signals and a two-stage architecture with distributional embeddings.", "result": "Offline and online experiments show significant improvements in recommendation accuracy and robustness.", "conclusion": "The framework effectively corrects watch time biases, enhancing recommendation quality."}}
{"id": "2508.11247", "pdf": "https://arxiv.org/pdf/2508.11247", "abs": "https://arxiv.org/abs/2508.11247", "authors": ["Changjian Wang", "Weihong Deng", "Weili Guan", "Quan Lu", "Ning Jiang"], "title": "Cross-Granularity Hypergraph Retrieval-Augmented Generation for Multi-hop Question Answering", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Multi-hop question answering (MHQA) requires integrating knowledge scattered\nacross multiple passages to derive the correct answer. Traditional\nretrieval-augmented generation (RAG) methods primarily focus on coarse-grained\ntextual semantic similarity and ignore structural associations among dispersed\nknowledge, which limits their effectiveness in MHQA tasks. GraphRAG methods\naddress this by leveraging knowledge graphs (KGs) to capture structural\nassociations, but they tend to overly rely on structural information and\nfine-grained word- or phrase-level retrieval, resulting in an underutilization\nof textual semantics. In this paper, we propose a novel RAG approach called\nHGRAG for MHQA that achieves cross-granularity integration of structural and\nsemantic information via hypergraphs. Structurally, we construct an entity\nhypergraph where fine-grained entities serve as nodes and coarse-grained\npassages as hyperedges, and establish knowledge association through shared\nentities. Semantically, we design a hypergraph retrieval method that integrates\nfine-grained entity similarity and coarse-grained passage similarity via\nhypergraph diffusion. Finally, we employ a retrieval enhancement module, which\nfurther refines the retrieved results both semantically and structurally, to\nobtain the most relevant passages as context for answer generation with the\nLLM. Experimental results on benchmark datasets demonstrate that our approach\noutperforms state-of-the-art methods in QA performance, and achieves a\n6$\\times$ speedup in retrieval efficiency.", "AI": {"tldr": "HGRAG integrates structural and semantic information via hypergraphs for multi-hop QA, outperforming existing methods in performance and efficiency.", "motivation": "Traditional RAG methods lack structural associations, while GraphRAG over-relies on structure, neglecting textual semantics. HGRAG aims to balance both.", "method": "Uses entity hypergraphs (nodes: entities, hyperedges: passages) and hypergraph diffusion for retrieval, enhanced by a refinement module.", "result": "Outperforms state-of-the-art methods in QA performance with a 6x retrieval speedup.", "conclusion": "HGRAG effectively combines structural and semantic information for superior multi-hop QA."}}
{"id": "1801.01220", "pdf": "https://arxiv.org/pdf/1801.01220", "abs": "https://arxiv.org/abs/1801.01220", "authors": ["Changshuai Wei", "Qing Lu"], "title": "Generalized Similarity U: A Non-parametric Test of Association Based on Similarity", "categories": ["stat.ME", "cs.AI", "cs.LG", "q-bio.GN", "stat.ML"], "comment": null, "summary": "Second generation sequencing technologies are being increasingly used for\ngenetic association studies, where the main research interest is to identify\nsets of genetic variants that contribute to various phenotype. The phenotype\ncan be univariate disease status, multivariate responses and even\nhigh-dimensional outcomes. Considering the genotype and phenotype as two\ncomplex objects, this also poses a general statistical problem of testing\nassociation between complex objects. We here proposed a similarity-based test,\ngeneralized similarity U (GSU), that can test the association between complex\nobjects. We first studied the theoretical properties of the test in a general\nsetting and then focused on the application of the test to sequencing\nassociation studies. Based on theoretical analysis, we proposed to use\nLaplacian kernel based similarity for GSU to boost power and enhance\nrobustness. Through simulation, we found that GSU did have advantages over\nexisting methods in terms of power and robustness. We further performed a whole\ngenome sequencing (WGS) scan for Alzherimer Disease Neuroimaging Initiative\n(ADNI) data, identifying three genes, APOE, APOC1 and TOMM40, associated with\nimaging phenotype. We developed a C++ package for analysis of whole genome\nsequencing data using GSU. The source codes can be downloaded at\nhttps://github.com/changshuaiwei/gsu.", "AI": {"tldr": "Proposed a similarity-based test (GSU) for association between complex objects like genotypes and phenotypes, showing advantages in power and robustness.", "motivation": "To address the challenge of identifying genetic variants associated with complex phenotypes using second-generation sequencing technologies.", "method": "Developed the generalized similarity U (GSU) test, using Laplacian kernel-based similarity for enhanced power and robustness.", "result": "GSU outperformed existing methods in simulations and identified three genes (APOE, APOC1, TOMM40) in ADNI data.", "conclusion": "GSU is a powerful tool for sequencing association studies, with practical applications demonstrated in WGS analysis."}}
{"id": "2508.10950", "pdf": "https://arxiv.org/pdf/2508.10950", "abs": "https://arxiv.org/abs/2508.10950", "authors": ["Xinyi Wang", "Michael Barnett", "Frederique Boonstra", "Yael Barnett", "Mariano Cabezas", "Arkiev D'Souza", "Matthew C. Kiernan", "Kain Kyle", "Meng Law", "Lynette Masters", "Zihao Tang", "Stephen Tisch", "Sicong Tu", "Anneke Van Der Walt", "Dongang Wang", "Fernando Calamante", "Weidong Cai", "Chenyu Wang"], "title": "From Promise to Practical Reality: Transforming Diffusion MRI Analysis with Fast Deep Learning Enhancement", "categories": ["cs.CV"], "comment": "24 pages, 5 figures", "summary": "Fiber orientation distribution (FOD) is an advanced diffusion MRI modeling\ntechnique that represents complex white matter fiber configurations, and a key\nstep for subsequent brain tractography and connectome analysis. Its reliability\nand accuracy, however, heavily rely on the quality of the MRI acquisition and\nthe subsequent estimation of the FODs at each voxel. Generating reliable FODs\nfrom widely available clinical protocols with single-shell and\nlow-angular-resolution acquisitions remains challenging but could potentially\nbe addressed with recent advances in deep learning-based enhancement\ntechniques. Despite advancements, existing methods have predominantly been\nassessed on healthy subjects, which have proved to be a major hurdle for their\nclinical adoption. In this work, we validate a newly optimized enhancement\nframework, FastFOD-Net, across healthy controls and six neurological disorders.\nThis accelerated end-to-end deep learning framework enhancing FODs with\nsuperior performance and delivering training/inference efficiency for clinical\nuse ($60\\times$ faster comparing to its predecessor). With the most\ncomprehensive clinical evaluation to date, our work demonstrates the potential\nof FastFOD-Net in accelerating clinical neuroscience research, empowering\ndiffusion MRI analysis for disease differentiation, improving interpretability\nin connectome applications, and reducing measurement errors to lower sample\nsize requirements. Critically, this work will facilitate the more widespread\nadoption of, and build clinical trust in, deep learning based methods for\ndiffusion MRI enhancement. Specifically, FastFOD-Net enables robust analysis of\nreal-world, clinical diffusion MRI data, comparable to that achievable with\nhigh-quality research acquisitions.", "AI": {"tldr": "FastFOD-Net, a deep learning framework, enhances Fiber Orientation Distribution (FOD) estimation from clinical MRI data, showing superior performance and efficiency across healthy and neurological disorder cases.", "motivation": "The reliability of FODs in clinical MRI is limited by low-quality acquisitions. Existing methods lack validation in clinical populations, hindering adoption.", "method": "FastFOD-Net, an accelerated end-to-end deep learning framework, enhances FODs from single-shell, low-angular-resolution MRI data.", "result": "FastFOD-Net achieves 60x faster performance than its predecessor, enabling robust clinical use and disease differentiation.", "conclusion": "FastFOD-Net facilitates clinical adoption of deep learning for MRI enhancement, improving interpretability and reducing sample size requirements."}}
{"id": "2508.11090", "pdf": "https://arxiv.org/pdf/2508.11090", "abs": "https://arxiv.org/abs/2508.11090", "authors": ["Daniel Mas Montserrat", "David Bonet", "Maria Perera", "Xavier Gir\u00f3-i-Nieto", "Alexander G. Ioannidis"], "title": "Compressive Meta-Learning", "categories": ["cs.LG", "cs.AI", "cs.CE", "cs.DB", "68T07, 68T05, 68T09", "I.2.6; I.5.1; G.3; H.2.8"], "comment": "Extended version of a paper accepted at KDD '25", "summary": "The rapid expansion in the size of new datasets has created a need for fast\nand efficient parameter-learning techniques. Compressive learning is a\nframework that enables efficient processing by using random, non-linear\nfeatures to project large-scale databases onto compact, information-preserving\nrepresentations whose dimensionality is independent of the number of samples\nand can be easily stored, transferred, and processed. These database-level\nsummaries are then used to decode parameters of interest from the underlying\ndata distribution without requiring access to the original samples, offering an\nefficient and privacy-friendly learning framework. However, both the encoding\nand decoding techniques are typically randomized and data-independent, failing\nto exploit the underlying structure of the data. In this work, we propose a\nframework that meta-learns both the encoding and decoding stages of compressive\nlearning methods by using neural networks that provide faster and more accurate\nsystems than the current state-of-the-art approaches. To demonstrate the\npotential of the presented Compressive Meta-Learning framework, we explore\nmultiple applications -- including neural network-based compressive PCA,\ncompressive ridge regression, compressive k-means, and autoencoders.", "AI": {"tldr": "The paper introduces a Compressive Meta-Learning framework that improves the efficiency and accuracy of compressive learning by using neural networks to meta-learn encoding and decoding stages.", "motivation": "The need for fast and efficient parameter-learning techniques due to large datasets, and the limitations of current randomized, data-independent compressive learning methods.", "method": "Proposes a framework using neural networks to meta-learn encoding and decoding stages, applied to tasks like compressive PCA, ridge regression, k-means, and autoencoders.", "result": "The framework provides faster and more accurate systems than current state-of-the-art compressive learning approaches.", "conclusion": "The Compressive Meta-Learning framework enhances compressive learning by leveraging data structure, offering improved efficiency and accuracy."}}
{"id": "2508.11260", "pdf": "https://arxiv.org/pdf/2508.11260", "abs": "https://arxiv.org/abs/2508.11260", "authors": ["Mukund Choudhary", "KV Aditya Srivatsa", "Gaurja Aeron", "Antara Raaghavi Bhattacharya", "Dang Khoa Dang Dinh", "Ikhlasul Akmal Hanif", "Daria Kotova", "Ekaterina Kochmar", "Monojit Choudhury"], "title": "UNVEILING: What Makes Linguistics Olympiad Puzzles Tricky for LLMs?", "categories": ["cs.CL"], "comment": "Accepted to COLM 2025", "summary": "Large language models (LLMs) have demonstrated potential in reasoning tasks,\nbut their performance on linguistics puzzles remains consistently poor. These\npuzzles, often derived from Linguistics Olympiad (LO) contests, provide a\nminimal contamination environment to assess LLMs' linguistic reasoning\nabilities across low-resource languages. This work analyses LLMs' performance\non 629 problems across 41 low-resource languages by labelling each with\nlinguistically informed features to unveil weaknesses. Our analyses show that\nLLMs struggle with puzzles involving higher morphological complexity and\nperform better on puzzles involving linguistic features that are also found in\nEnglish. We also show that splitting words into morphemes as a pre-processing\nstep improves solvability, indicating a need for more informed and\nlanguage-specific tokenisers. These findings thus offer insights into some\nchallenges in linguistic reasoning and modelling of low-resource languages.", "AI": {"tldr": "LLMs perform poorly on linguistic puzzles, especially with high morphological complexity, but improve with morpheme-based pre-processing.", "motivation": "To assess LLMs' linguistic reasoning abilities in low-resource languages using Linguistics Olympiad puzzles.", "method": "Analyzed 629 problems across 41 low-resource languages, labeling them with linguistic features and testing LLMs' performance.", "result": "LLMs struggle with high morphological complexity but perform better on English-like features; morpheme splitting improves results.", "conclusion": "LLMs need better, language-specific tokenizers for improved linguistic reasoning in low-resource languages."}}
{"id": "2508.07264", "pdf": "https://arxiv.org/pdf/2508.07264", "abs": "https://arxiv.org/abs/2508.07264", "authors": ["Van Duc Cuong", "Ta Dinh Tam", "Tran Duc Chinh", "Nguyen Thi Hanh"], "title": "FLUID: Flow-Latent Unified Integration via Token Distillation for Expert Specialization in Multimodal Learning", "categories": ["cs.SI", "cs.AI"], "comment": null, "summary": "Multimodal classification requires robust integration of visual and textual\nsignals, yet common fusion strategies are brittle and vulnerable to\nmodality-specific noise. In this paper, we present \\textsc{FLUID}-Flow-Latent\nUnified Integration via Token Distillation for Expert Specialization, a\nprincipled token-level pipeline that improves cross-modal robustness and\nscalability. \\textsc{FLUID} contributes three core elements: (1)\n\\emph{Q-transforms}, learnable query tokens that distill and retain salient\ntoken-level features from modality-specific backbones; (2) a two-stage fusion\nscheme that enforces cross-modal consistency via contrastive alignment and then\nperforms adaptive, task-aware fusion through a gating mechanism and a\n\\emph{Q-bottleneck} that selectively compresses information for downstream\nreasoning; and (3) a lightweight, load-balanced Mixture-of-Experts at\nprediction time that enables efficient specialization to diverse semantic\npatterns. Extensive experiments demonstrate that \\textsc{FLUID} attains\n\\(91\\%\\) accuracy on the GLAMI-1M benchmark, significantly outperforming prior\nbaselines and exhibiting strong resilience to label noise, long-tail class\nimbalance, and semantic heterogeneity. Targeted ablation studies corroborate\nboth the individual and synergistic benefits of the proposed components,\npositioning \\textsc{FLUID} as a scalable, noise-resilient solution for\nmultimodal product classification.", "AI": {"tldr": "FLUID is a token-level pipeline for robust multimodal classification, featuring Q-transforms, a two-stage fusion scheme, and a Mixture-of-Experts, achieving 91% accuracy on GLAMI-1M.", "motivation": "Current fusion strategies for multimodal classification are brittle and vulnerable to modality-specific noise, necessitating a more robust and scalable solution.", "method": "FLUID introduces Q-transforms for feature distillation, a two-stage fusion scheme with contrastive alignment and adaptive gating, and a lightweight Mixture-of-Experts for efficient specialization.", "result": "FLUID achieves 91% accuracy on GLAMI-1M, outperforming baselines and showing resilience to noise, imbalance, and heterogeneity.", "conclusion": "FLUID is a scalable, noise-resilient solution for multimodal classification, validated by ablation studies."}}
{"id": "2508.10955", "pdf": "https://arxiv.org/pdf/2508.10955", "abs": "https://arxiv.org/abs/2508.10955", "authors": ["Wenbin An", "Jiahao Nie", "Yaqiang Wu", "Feng Tian", "Shijian Lu", "Qinghua Zheng"], "title": "Empowering Multimodal LLMs with External Tools: A Comprehensive Survey", "categories": ["cs.CV", "cs.CL", "cs.MM"], "comment": "21 pages, 361 references", "summary": "By integrating the perception capabilities of multimodal encoders with the\ngenerative power of Large Language Models (LLMs), Multimodal Large Language\nModels (MLLMs), exemplified by GPT-4V, have achieved great success in various\nmultimodal tasks, pointing toward a promising pathway to artificial general\nintelligence. Despite this progress, the limited quality of multimodal data,\npoor performance on many complex downstream tasks, and inadequate evaluation\nprotocols continue to hinder the reliability and broader applicability of MLLMs\nacross diverse domains. Inspired by the human ability to leverage external\ntools for enhanced reasoning and problem-solving, augmenting MLLMs with\nexternal tools (e.g., APIs, expert models, and knowledge bases) offers a\npromising strategy to overcome these challenges. In this paper, we present a\ncomprehensive survey on leveraging external tools to enhance MLLM performance.\nOur discussion is structured along four key dimensions about external tools:\n(1) how they can facilitate the acquisition and annotation of high-quality\nmultimodal data; (2) how they can assist in improving MLLM performance on\nchallenging downstream tasks; (3) how they enable comprehensive and accurate\nevaluation of MLLMs; (4) the current limitations and future directions of\ntool-augmented MLLMs. Through this survey, we aim to underscore the\ntransformative potential of external tools in advancing MLLM capabilities,\noffering a forward-looking perspective on their development and applications.\nThe project page of this paper is publicly available\nathttps://github.com/Lackel/Awesome-Tools-for-MLLMs.", "AI": {"tldr": "A survey on enhancing Multimodal Large Language Models (MLLMs) with external tools to address data quality, task performance, and evaluation challenges.", "motivation": "To overcome limitations in MLLMs like poor data quality and unreliable performance on complex tasks by leveraging external tools.", "method": "Structured discussion on four dimensions: data acquisition, task performance improvement, evaluation enhancement, and future directions.", "result": "Highlights the transformative potential of external tools in advancing MLLM capabilities.", "conclusion": "External tools offer a promising pathway to enhance MLLMs, with future research needed to address current limitations."}}
{"id": "2508.11092", "pdf": "https://arxiv.org/pdf/2508.11092", "abs": "https://arxiv.org/abs/2508.11092", "authors": ["Cindy Shih-Ting Huang", "Clarence Boon Liang Ng", "Marek Rei"], "title": "Predictive Multimodal Modeling of Diagnoses and Treatments in EHR", "categories": ["cs.LG"], "comment": "10 pages, 1 figure", "summary": "While the ICD code assignment problem has been widely studied, most works\nhave focused on post-discharge document classification. Models for early\nforecasting of this information could be used for identifying health risks,\nsuggesting effective treatments, or optimizing resource allocation. To address\nthe challenge of predictive modeling using the limited information at the\nbeginning of a patient stay, we propose a multimodal system to fuse clinical\nnotes and tabular events captured in electronic health records. The model\nintegrates pre-trained encoders, feature pooling, and cross-modal attention to\nlearn optimal representations across modalities and balance their presence at\nevery temporal point. Moreover, we present a weighted temporal loss that\nadjusts its contribution at each point in time. Experiments show that these\nstrategies enhance the early prediction model, outperforming the current\nstate-of-the-art systems.", "AI": {"tldr": "A multimodal system for early ICD code forecasting using EHR data, outperforming state-of-the-art methods.", "motivation": "Early ICD code prediction can improve health risk identification, treatment suggestions, and resource allocation.", "method": "Proposes a multimodal system fusing clinical notes and tabular events with pre-trained encoders, feature pooling, cross-modal attention, and a weighted temporal loss.", "result": "The model outperforms current state-of-the-art systems in early prediction.", "conclusion": "The proposed multimodal approach effectively enhances early ICD code forecasting."}}
{"id": "2508.11280", "pdf": "https://arxiv.org/pdf/2508.11280", "abs": "https://arxiv.org/abs/2508.11280", "authors": ["Ruiyan Qi", "Congding Wen", "Weibo Zhou", "Shangsong Liang", "Lingbo Li"], "title": "LETToT: Label-Free Evaluation of Large Language Models On Tourism Using Expert Tree-of-Thought", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Evaluating large language models (LLMs) in specific domain like tourism\nremains challenging due to the prohibitive cost of annotated benchmarks and\npersistent issues like hallucinations. We propose $\\textbf{L}$able-Free\n$\\textbf{E}$valuation of LLM on $\\textbf{T}$ourism using Expert\n$\\textbf{T}$ree-$\\textbf{o}$f-$\\textbf{T}$hought (LETToT), a framework that\nleverages expert-derived reasoning structures-instead of labeled data-to access\nLLMs in tourism. First, we iteratively refine and validate hierarchical ToT\ncomponents through alignment with generic quality dimensions and expert\nfeedback. Results demonstrate the effectiveness of our systematically optimized\nexpert ToT with 4.99-14.15\\% relative quality gains over baselines. Second, we\napply LETToT's optimized expert ToT to evaluate models of varying scales\n(32B-671B parameters), revealing: (1) Scaling laws persist in specialized\ndomains (DeepSeek-V3 leads), yet reasoning-enhanced smaller models (e.g.,\nDeepSeek-R1-Distill-Llama-70B) close this gap; (2) For sub-72B models, explicit\nreasoning architectures outperform counterparts in accuracy and conciseness\n($p<0.05$). Our work established a scalable, label-free paradigm for\ndomain-specific LLM evaluation, offering a robust alternative to conventional\nannotated benchmarks.", "AI": {"tldr": "LETToT is a label-free framework for evaluating LLMs in tourism using expert-derived reasoning structures, showing quality gains and insights into model performance.", "motivation": "Challenges in evaluating LLMs in specialized domains like tourism due to high costs of annotated benchmarks and issues like hallucinations.", "method": "Proposes LETToT, leveraging expert-derived hierarchical Tree-of-Thought (ToT) components, validated through expert feedback and alignment with quality dimensions.", "result": "Demonstrates 4.99-14.15% quality gains over baselines; reveals scaling laws in specialized domains and advantages of reasoning-enhanced smaller models.", "conclusion": "Establishes a scalable, label-free paradigm for domain-specific LLM evaluation, offering a robust alternative to annotated benchmarks."}}
{"id": "2508.10913", "pdf": "https://arxiv.org/pdf/2508.10913", "abs": "https://arxiv.org/abs/2508.10913", "authors": ["Changqing Xu", "Buxuan Song", "Yi Liu", "Xinfang Liao", "Wenbin Zheng", "Yintang Yang"], "title": "SDSNN: A Single-Timestep Spiking Neural Network with Self-Dropping Neuron and Bayesian Optimization", "categories": ["cs.NE", "cs.AI"], "comment": null, "summary": "Spiking Neural Networks (SNNs), as an emerging biologically inspired\ncomputational model, demonstrate significant energy efficiency advantages due\nto their event-driven information processing mechanism. Compared to traditional\nArtificial Neural Networks (ANNs), SNNs transmit information through discrete\nspike signals, which substantially reduces computational energy consumption\nthrough their sparse encoding approach. However, the multi-timestep computation\nmodel significantly increases inference latency and energy, limiting the\napplicability of SNNs in edge computing scenarios. We propose a single-timestep\nSNN, which enhances accuracy and reduces computational energy consumption in a\nsingle timestep by optimizing spike generation and temporal parameters. We\ndesign a Self-Dropping Neuron mechanism, which enhances information-carrying\ncapacity through dynamic threshold adjustment and selective spike suppression.\nFurthermore, we employ Bayesian optimization to globally search for time\nparameters and obtain an efficient inference mode with a single time step.\nExperimental results on the Fashion-MNIST, CIFAR-10, and CIFAR-100 datasets\ndemonstrate that, compared to traditional multi-timestep SNNs employing the\nLeaky Integrate-and-Fire (LIF) model, our method achieves classification\naccuracies of 93.72%, 92.20%, and 69.45%, respectively, using only\nsingle-timestep spikes, while maintaining comparable or even superior accuracy.\nAdditionally, it reduces energy consumption by 56%, 21%, and 22%, respectively.", "AI": {"tldr": "A single-timestep Spiking Neural Network (SNN) is proposed to reduce inference latency and energy consumption while maintaining accuracy, outperforming traditional multi-timestep SNNs.", "motivation": "SNNs are energy-efficient but suffer from high latency and energy costs due to multi-timestep computations, limiting their edge computing applicability.", "method": "Optimizes spike generation and temporal parameters, introduces a Self-Dropping Neuron mechanism for dynamic threshold adjustment, and uses Bayesian optimization for efficient single-timestep inference.", "result": "Achieves accuracies of 93.72%, 92.20%, and 69.45% on Fashion-MNIST, CIFAR-10, and CIFAR-100, with energy reductions of 56%, 21%, and 22%.", "conclusion": "The proposed single-timestep SNN improves accuracy and energy efficiency, making it suitable for edge computing."}}
{"id": "2508.10956", "pdf": "https://arxiv.org/pdf/2508.10956", "abs": "https://arxiv.org/abs/2508.10956", "authors": ["Abhishek Kolari", "Mohammadhossein Khojasteh", "Yifan Jiang", "Floris den Hengst", "Filip Ilievski"], "title": "ORBIT: An Object Property Reasoning Benchmark for Visual Inference Tasks", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "While vision-language models (VLMs) have made remarkable progress on many\npopular visual question answering (VQA) benchmarks, it remains unclear whether\nthey abstract and reason over depicted objects. Inspired by human object\ncategorisation, object property reasoning involves identifying and recognising\nlow-level details and higher-level abstractions. While current VQA benchmarks\nconsider a limited set of object property attributes like size, they typically\nblend perception and reasoning, and lack representativeness in terms of\nreasoning and image categories. To this end, we introduce a systematic\nevaluation framework with images of three representative types, three reasoning\nlevels of increasing complexity, and four object property dimensions driven by\nprior work on commonsense reasoning. We develop a procedure to instantiate this\nbenchmark into ORBIT, a multi-level reasoning VQA benchmark for object\nproperties comprising 360 images paired with a total of 1,080 count-based\nquestions. Experiments with 12 state-of-the-art VLMs in zero-shot settings\nreveal significant limitations compared to humans, with the best-performing\nmodel only reaching 40\\% accuracy. VLMs struggle particularly with realistic\n(photographic) images, counterfactual reasoning about physical and functional\nproperties, and higher counts. ORBIT points to the need to develop methods for\nscalable benchmarking, generalize annotation guidelines, and explore additional\nreasoning VLMs. We make the ORBIT benchmark and the experimental code available\nto support such endeavors.", "AI": {"tldr": "The paper introduces ORBIT, a VQA benchmark for evaluating VLMs on object property reasoning, revealing their limitations compared to humans.", "motivation": "Current VQA benchmarks lack representativeness in reasoning and image categories, blending perception and reasoning.", "method": "Developed ORBIT, a benchmark with 360 images and 1,080 questions, testing three reasoning levels and four object property dimensions.", "result": "VLMs perform poorly (40% accuracy), especially with realistic images, counterfactual reasoning, and higher counts.", "conclusion": "ORBIT highlights the need for scalable benchmarking, generalized annotation, and improved reasoning VLMs."}}
{"id": "2508.11105", "pdf": "https://arxiv.org/pdf/2508.11105", "abs": "https://arxiv.org/abs/2508.11105", "authors": ["Sajjad Saed", "Babak Teimourpour"], "title": "Hybrid-Hierarchical Fashion Graph Attention Network for Compatibility-Oriented and Personalized Outfit Recommendation", "categories": ["cs.LG", "cs.IR"], "comment": null, "summary": "The rapid expansion of the fashion industry and the growing variety of\nproducts have made it challenging for users to find compatible items on\ne-commerce platforms. Effective fashion recommendation systems are crucial for\nfiltering irrelevant items and suggesting suitable ones. However,\nsimultaneously addressing outfit compatibility and personalized recommendations\nremains a significant challenge, as these aspects are often treated\nindependently in existing studies, often overlooking the complex interactions\nbetween items and user preferences. This research introduces a new framework\nnamed FGAT, inspired by the HFGN model, which leverages graph neural networks\nand graph attention mechanisms to tackle this issue. The proposed framework\nconstructs a three-tier hierarchical graph of users, outfits, and items,\nintegrating visual and textual features to simultaneously model outfit\ncompatibility and user preferences. A graph attention mechanism dynamically\nweights node importance during representation propagation, enabling the capture\nof key interactions and generating precise representations for both user\npreferences and outfit compatibility. Evaluated on the POG dataset, FGAT\noutperforms baseline models such as HFGN, achieving improved results in\nprecision, HR, recall, NDCG, and accuracy.These results demonstrate that\ncombining multimodal visual-textual features with a hierarchical graph\nstructure and attention mechanisms significantly enhances the accuracy and\nefficiency of personalized fashion recommendation systems.", "AI": {"tldr": "FGAT, a new framework using graph neural networks and attention mechanisms, improves fashion recommendations by integrating outfit compatibility and user preferences.", "motivation": "Addressing the challenge of combining outfit compatibility and personalized recommendations in fashion e-commerce, often treated separately in existing studies.", "method": "FGAT constructs a three-tier hierarchical graph (users, outfits, items) with visual-textual features, using graph attention to dynamically weight node importance.", "result": "Outperforms baseline models (e.g., HFGN) on the POG dataset in precision, HR, recall, NDCG, and accuracy.", "conclusion": "Combining multimodal features, hierarchical graphs, and attention mechanisms enhances personalized fashion recommendation accuracy and efficiency."}}
{"id": "2508.11281", "pdf": "https://arxiv.org/pdf/2508.11281", "abs": "https://arxiv.org/abs/2508.11281", "authors": ["Axel Delaval", "Shujian Yang", "Haicheng Wang", "Han Qiu", "Jialiang Lu"], "title": "ToxiFrench: Benchmarking and Enhancing Language Models via CoT Fine-Tuning for French Toxicity Detection", "categories": ["cs.CL", "cs.AI", "cs.CY", "68T50", "I.2.7"], "comment": "14 pages, 5 figures, 8 tables. This paper introduces TOXIFRENCH, a\n  new large-scale benchmark for French toxicity detection, and proposes a\n  Chain-of-Thought (CoT) fine-tuning method with a dynamic weighted loss. The\n  resulting fine-tuned 4B parameter model, ToxiFrench, achieves\n  state-of-the-art performance, outperforming larger models like GPT-4o", "summary": "Detecting toxic content using language models is crucial yet challenging.\nWhile substantial progress has been made in English, toxicity detection in\nFrench remains underdeveloped, primarily due to the lack of culturally\nrelevant, large-scale datasets. In this work, we introduce TOXIFRENCH, a new\npublic benchmark of 53,622 French online comments, constructed via a\nsemi-automated annotation pipeline that reduces manual labeling to only 10%\nthrough high-confidence LLM-based pre-annotation and human verification. Then,\nwe benchmark a broad range of models and uncover a counterintuitive insight:\nSmall Language Models (SLMs) outperform many larger models in robustness and\ngeneralization under the toxicity detection task. Motivated by this finding, we\npropose a novel Chain-of-Thought (CoT) fine-tuning strategy using a dynamic\nweighted loss that progressively emphasizes the model's final decision,\nsignificantly improving faithfulness. Our fine-tuned 4B model achieves\nstate-of-the-art performance, improving its F1 score by 13% over its baseline\nand outperforming LLMs such as GPT-40 and Gemini-2.5. Further evaluation on a\ncross-lingual toxicity benchmark demonstrates strong multilingual ability,\nsuggesting that our methodology can be effectively extended to other languages\nand safety-critical classification tasks.", "AI": {"tldr": "TOXIFRENCH, a new French toxicity detection benchmark, shows small language models (SLMs) outperform larger ones. A novel CoT fine-tuning strategy improves performance, achieving state-of-the-art results.", "motivation": "Toxicity detection in French is underdeveloped due to lack of datasets. This work addresses the gap with TOXIFRENCH.", "method": "Semi-automated annotation pipeline (10% manual labeling), benchmarking models, and proposing a CoT fine-tuning strategy with dynamic weighted loss.", "result": "SLMs outperform larger models. The fine-tuned 4B model achieves 13% F1 improvement over baseline, surpassing GPT-40 and Gemini-2.5.", "conclusion": "The methodology is effective for French toxicity detection and can extend to other languages and safety-critical tasks."}}
{"id": "2508.10916", "pdf": "https://arxiv.org/pdf/2508.10916", "abs": "https://arxiv.org/abs/2508.10916", "authors": ["Ojas Shirekar", "Wim Pouw", "Chenxu Hao", "Vrushank Phadnis", "Thabo Beeler", "Chirag Raman"], "title": "Multimodal Quantitative Measures for Multiparty Behaviour Evaluation", "categories": ["cs.HC", "cs.AI", "cs.CY", "cs.MA"], "comment": null, "summary": "Digital humans are emerging as autonomous agents in multiparty interactions,\nyet existing evaluation metrics largely ignore contextual coordination\ndynamics. We introduce a unified, intervention-driven framework for objective\nassessment of multiparty social behaviour in skeletal motion data, spanning\nthree complementary dimensions: (1) synchrony via Cross-Recurrence\nQuantification Analysis, (2) temporal alignment via Multiscale Empirical Mode\nDecompositionbased Beat Consistency, and (3) structural similarity via Soft\nDynamic Time Warping. We validate metric sensitivity through three\ntheory-driven perturbations -- gesture kinematic dampening, uniform\nspeech-gesture delays, and prosodic pitch-variance reduction-applied to\n$\\approx 145$ 30-second thin slices of group interactions from the DnD dataset.\nMixed-effects analyses reveal predictable, joint-independent shifts: dampening\nincreases CRQA determinism and reduces beat consistency, delays weaken\ncross-participant coupling, and pitch flattening elevates F0 Soft-DTW costs. A\ncomplementary perception study ($N=27$) compares judgments of full-video and\nskeleton-only renderings to quantify representation effects. Our three measures\ndeliver orthogonal insights into spatial structure, timing alignment, and\nbehavioural variability. Thereby forming a robust toolkit for evaluating and\nrefining socially intelligent agents. Code available on\n\\href{https://github.com/tapri-lab/gig-interveners}{GitHub}.", "AI": {"tldr": "The paper introduces a framework for evaluating multiparty social behavior in skeletal motion data, focusing on synchrony, temporal alignment, and structural similarity, validated through perturbations and perception studies.", "motivation": "Existing metrics for digital humans in multiparty interactions lack contextual coordination dynamics, necessitating a new evaluation framework.", "method": "The framework uses Cross-Recurrence Quantification Analysis (CRQA) for synchrony, Multiscale Empirical Mode Decomposition for temporal alignment, and Soft Dynamic Time Warping for structural similarity, tested on the DnD dataset.", "result": "Perturbations showed predictable effects: dampening increased CRQA determinism, delays weakened coupling, and pitch flattening raised Soft-DTW costs. Perception studies confirmed measure validity.", "conclusion": "The three measures provide orthogonal insights, forming a robust toolkit for evaluating socially intelligent agents."}}
{"id": "2508.10962", "pdf": "https://arxiv.org/pdf/2508.10962", "abs": "https://arxiv.org/abs/2508.10962", "authors": ["Jiarong Li", "Imad Ali Shah", "Diarmaid Geever", "Fiachra Collins", "Enda Ward", "Martin Glavin", "Edward Jones", "Brian Deegan"], "title": "CSNR and JMIM Based Spectral Band Selection for Reducing Metamerism in Urban Driving", "categories": ["cs.CV"], "comment": "Under Review at IEEE OJITS, July, 2025", "summary": "Protecting Vulnerable Road Users (VRU) is a critical safety challenge for\nautomotive perception systems, particularly under visual ambiguity caused by\nmetamerism, a phenomenon where distinct materials appear similar in RGB\nimagery. This work investigates hyperspectral imaging (HSI) to overcome this\nlimitation by capturing unique material signatures beyond the visible spectrum,\nespecially in the Near-Infrared (NIR). To manage the inherent\nhigh-dimensionality of HSI data, we propose a band selection strategy that\nintegrates information theory techniques (joint mutual information\nmaximization, correlation analysis) with a novel application of an image\nquality metric (contrast signal-to-noise ratio) to identify the most spectrally\ninformative bands. Using the Hyperspectral City V2 (H-City) dataset, we\nidentify three informative bands (497 nm, 607 nm, and 895 nm, $\\pm$27 nm) and\nreconstruct pseudo-color images for comparison with co-registered RGB.\nQuantitative results demonstrate increased dissimilarity and perceptual\nseparability of VRU from the background. The selected HSI bands yield\nimprovements of 70.24%, 528.46%, 1206.83%, and 246.62% for dissimilarity\n(Euclidean, SAM, $T^2$) and perception (CIE $\\Delta E$) metrics, consistently\noutperforming RGB and confirming a marked reduction in metameric confusion. By\nproviding a spectrally optimized input, our method enhances VRU separability,\nestablishing a robust foundation for downstream perception tasks in Advanced\nDriver Assistance Systems (ADAS) and Autonomous Driving (AD), ultimately\ncontributing to improved road safety.", "AI": {"tldr": "The paper proposes using hyperspectral imaging (HSI) to improve the detection of Vulnerable Road Users (VRU) by addressing visual ambiguity in RGB imagery. It introduces a band selection strategy combining information theory and image quality metrics, achieving significant improvements in VRU separability.", "motivation": "Visual ambiguity in RGB imagery, caused by metamerism, hinders VRU detection. Hyperspectral imaging captures unique material signatures beyond the visible spectrum, offering a solution.", "method": "A band selection strategy integrates joint mutual information maximization, correlation analysis, and contrast signal-to-noise ratio to identify informative HSI bands. The method is validated using the H-City dataset.", "result": "Selected HSI bands (497 nm, 607 nm, 895 nm) outperform RGB, showing 70.24% to 1206.83% improvements in dissimilarity and perception metrics, reducing metameric confusion.", "conclusion": "The proposed HSI-based method enhances VRU separability, providing a robust input for ADAS and autonomous driving systems, ultimately improving road safety."}}
{"id": "2508.11112", "pdf": "https://arxiv.org/pdf/2508.11112", "abs": "https://arxiv.org/abs/2508.11112", "authors": ["Jianhao Ma", "Lin Xiao"], "title": "Quantization through Piecewise-Affine Regularization: Optimization and Statistical Guarantees", "categories": ["cs.LG", "cs.AI", "math.OC", "stat.ML"], "comment": null, "summary": "Optimization problems over discrete or quantized variables are very\nchallenging in general due to the combinatorial nature of their search space.\nPiecewise-affine regularization (PAR) provides a flexible modeling and\ncomputational framework for quantization based on continuous optimization. In\nthis work, we focus on the setting of supervised learning and investigate the\ntheoretical foundations of PAR from optimization and statistical perspectives.\nFirst, we show that in the overparameterized regime, where the number of\nparameters exceeds the number of samples, every critical point of the\nPAR-regularized loss function exhibits a high degree of quantization. Second,\nwe derive closed-form proximal mappings for various (convex, quasi-convex, and\nnon-convex) PARs and show how to solve PAR-regularized problems using the\nproximal gradient method, its accelerated variant, and the Alternating\nDirection Method of Multipliers. Third, we study statistical guarantees of\nPAR-regularized linear regression problems; specifically, we can approximate\nclassical formulations of $\\ell_1$-, squared $\\ell_2$-, and nonconvex\nregularizations using PAR and obtain similar statistical guarantees with\nquantized solutions.", "AI": {"tldr": "The paper explores piecewise-affine regularization (PAR) for solving optimization problems with discrete variables, showing its effectiveness in overparameterized regimes, deriving proximal mappings, and providing statistical guarantees for PAR-regularized linear regression.", "motivation": "Discrete optimization is challenging due to combinatorial search spaces. PAR offers a continuous optimization framework for quantization, making it suitable for supervised learning.", "method": "The study analyzes PAR theoretically, derives proximal mappings for various PAR types, and applies proximal gradient methods and ADMM for solving PAR-regularized problems.", "result": "Critical points in overparameterized regimes exhibit high quantization. PAR approximates classical regularizations (\u2113\u2081, \u2113\u2082, nonconvex) with quantized solutions and similar guarantees.", "conclusion": "PAR provides a flexible and effective framework for quantization in supervised learning, with theoretical and computational advantages."}}
{"id": "2508.11285", "pdf": "https://arxiv.org/pdf/2508.11285", "abs": "https://arxiv.org/abs/2508.11285", "authors": ["Arya VarastehNezhad", "Reza Tavasoli", "Soroush Elyasi", "MohammadHossein LotfiNia", "Hamed Farbeh"], "title": "AI in Mental Health: Emotional and Sentiment Analysis of Large Language Models' Responses to Depression, Anxiety, and Stress Queries", "categories": ["cs.CL"], "comment": null, "summary": "Depression, anxiety, and stress are widespread mental health concerns that\nincreasingly drive individuals to seek information from Large Language Models\n(LLMs). This study investigates how eight LLMs (Claude Sonnet, Copilot, Gemini\nPro, GPT-4o, GPT-4o mini, Llama, Mixtral, and Perplexity) reply to twenty\npragmatic questions about depression, anxiety, and stress when those questions\nare framed for six user profiles (baseline, woman, man, young, old, and\nuniversity student). The models generated 2,880 answers, which we scored for\nsentiment and emotions using state-of-the-art tools. Our analysis revealed that\noptimism, fear, and sadness dominated the emotional landscape across all\noutputs, with neutral sentiment maintaining consistently high values.\nGratitude, joy, and trust appeared at moderate levels, while emotions such as\nanger, disgust, and love were rarely expressed. The choice of LLM significantly\ninfluenced emotional expression patterns. Mixtral exhibited the highest levels\nof negative emotions including disapproval, annoyance, and sadness, while Llama\ndemonstrated the most optimistic and joyful responses. The type of mental\nhealth condition dramatically shaped emotional responses: anxiety prompts\nelicited extraordinarily high fear scores (0.974), depression prompts generated\nelevated sadness (0.686) and the highest negative sentiment, while\nstress-related queries produced the most optimistic responses (0.755) with\nelevated joy and trust. In contrast, demographic framing of queries produced\nonly marginal variations in emotional tone. Statistical analyses confirmed\nsignificant model-specific and condition-specific differences, while\ndemographic influences remained minimal. These findings highlight the critical\nimportance of model selection in mental health applications, as each LLM\nexhibits a distinct emotional signature that could significantly impact user\nexperience and outcomes.", "AI": {"tldr": "The study analyzes emotional responses of eight LLMs to mental health questions, revealing model-specific and condition-specific patterns, with minimal demographic influence.", "motivation": "To understand how LLMs respond emotionally to mental health queries and assess the impact of model choice and user profiles.", "method": "Evaluated 2,880 answers from eight LLMs for sentiment and emotions using advanced tools, focusing on six user profiles and three mental health conditions.", "result": "Emotional responses varied significantly by LLM and mental health condition, with optimism, fear, and sadness dominating. Demographic framing had minimal impact.", "conclusion": "Model selection is crucial for mental health applications due to distinct emotional signatures of LLMs, which can affect user experience."}}
{"id": "2508.10917", "pdf": "https://arxiv.org/pdf/2508.10917", "abs": "https://arxiv.org/abs/2508.10917", "authors": ["Chidera W. Amazu", "Joseph Mietkiewicz", "Ammar N. Abbas", "Gabriele Baldissone", "Davide Fissore", "Micaela Demichela", "Anders L. Madsen", "Maria Chiara Leva"], "title": "Managing the unexpected: Operator behavioural data and its value in predicting correct alarm responses", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Data from psychophysiological measures can offer new insight into control\nroom operators' behaviour, cognition, and mental workload status. This can be\nparticularly helpful when combined with appraisal of capacity to respond to\npossible critical plant conditions (i.e. critical alarms response scenarios).\nHowever, wearable physiological measurement tools such as eye tracking and EEG\ncaps can be perceived as intrusive and not suitable for usage in daily\noperations. Therefore, this article examines the potential of using real-time\ndata from process and operator-system interactions during abnormal scenarios\nthat can be recorded and retrieved from the distributed control system's\nhistorian or process log, and their capacity to provide insight into operator\nbehavior and predict their response outcomes, without intruding on daily tasks.\nData for this study were obtained from a design of experiment using a\nformaldehyde production plant simulator and four human-in-the-loop experimental\nsupport configurations. A comparison between the different configurations in\nterms of both behaviour and performance is presented in this paper. A step-wise\nlogistic regression and a Bayesian network models were used to achieve this\nobjective. The results identified some predictive metrics and the paper discuss\ntheir value as precursor or predictor of overall system performance in alarm\nresponse scenarios. Knowledge of relevant and predictive behavioural metrics\naccessible in real time can better equip decision-makers to predict outcomes\nand provide timely support measures for operators.", "AI": {"tldr": "The paper explores using real-time process and operator-system interaction data to predict operator behavior and response outcomes in control rooms without intrusive measures like wearables.", "motivation": "To provide non-intrusive insights into operator behavior and mental workload during critical scenarios, avoiding the limitations of wearable physiological tools.", "method": "Data from a formaldehyde plant simulator and human-in-the-loop experiments were analyzed using step-wise logistic regression and Bayesian network models.", "result": "Predictive metrics were identified, offering real-time insights into operator performance in alarm response scenarios.", "conclusion": "Real-time behavioral metrics can help predict outcomes and support operators effectively, enhancing decision-making in control rooms."}}
{"id": "2508.10963", "pdf": "https://arxiv.org/pdf/2508.10963", "abs": "https://arxiv.org/abs/2508.10963", "authors": ["Zixiang Yang", "Yue Ma", "Yinhan Zhang", "Shanhui Mo", "Dongrui Liu", "Linfeng Zhang"], "title": "EVCtrl: Efficient Control Adapter for Visual Generation", "categories": ["cs.CV"], "comment": null, "summary": "Visual generation includes both image and video generation, training\nprobabilistic models to create coherent, diverse, and semantically faithful\ncontent from scratch. While early research focused on unconditional sampling,\npractitioners now demand controllable generation that allows precise\nspecification of layout, pose, motion, or style. While ControlNet grants\nprecise spatial-temporal control, its auxiliary branch markedly increases\nlatency and introduces redundant computation in both uncontrolled regions and\ndenoising steps, especially for video. To address this problem, we introduce\nEVCtrl, a lightweight, plug-and-play control adapter that slashes overhead\nwithout retraining the model. Specifically, we propose a spatio-temporal dual\ncaching strategy for sparse control information. For spatial redundancy, we\nfirst profile how each layer of DiT-ControlNet responds to fine-grained\ncontrol, then partition the network into global and local functional zones. A\nlocality-aware cache focuses computation on the local zones that truly need the\ncontrol signal, skipping the bulk of redundant computation in global regions.\nFor temporal redundancy, we selectively omit unnecessary denoising steps to\nimprove efficiency. Extensive experiments on CogVideo-Controlnet,\nWan2.1-Controlnet, and Flux demonstrate that our method is effective in image\nand video control generation without the need for training. For example, it\nachieves 2.16 and 2.05 times speedups on CogVideo-Controlnet and\nWan2.1-Controlnet, respectively, with almost no degradation in generation\nquality.Codes are available in the supplementary materials.", "AI": {"tldr": "EVCtrl introduces a lightweight control adapter for efficient image and video generation, reducing redundancy and latency without retraining.", "motivation": "Current methods like ControlNet increase latency and redundant computation, especially in uncontrolled regions and denoising steps.", "method": "EVCtrl uses a spatio-temporal dual caching strategy: spatial redundancy is addressed by partitioning the network into global/local zones, and temporal redundancy by omitting unnecessary denoising steps.", "result": "Achieves 2.16x and 2.05x speedups on CogVideo-Controlnet and Wan2.1-Controlnet with minimal quality loss.", "conclusion": "EVCtrl is an effective, plug-and-play solution for efficient controllable generation without training."}}
{"id": "2508.11144", "pdf": "https://arxiv.org/pdf/2508.11144", "abs": "https://arxiv.org/abs/2508.11144", "authors": ["Gauri Jain", "Dominik Rothenh\u00e4usler", "Kirk Bansak", "Elisabeth Paulson"], "title": "CTRL Your Shift: Clustered Transfer Residual Learning for Many Small Datasets", "categories": ["cs.LG"], "comment": null, "summary": "Machine learning (ML) tasks often utilize large-scale data that is drawn from\nseveral distinct sources, such as different locations, treatment arms, or\ngroups. In such settings, practitioners often desire predictions that not only\nexhibit good overall accuracy, but also remain reliable within each source and\npreserve the differences that matter across sources. For instance, several\nasylum and refugee resettlement programs now use ML-based employment\npredictions to guide where newly arriving families are placed within a host\ncountry, which requires generating informative and differentiated predictions\nfor many and often small source locations. However, this task is made\nchallenging by several common characteristics of the data in these settings:\nthe presence of numerous distinct data sources, distributional shifts between\nthem, and substantial variation in sample sizes across sources. This paper\nintroduces Clustered Transfer Residual Learning (CTRL), a meta-learning method\nthat combines the strengths of cross-domain residual learning and adaptive\npooling/clustering in order to simultaneously improve overall accuracy and\npreserve source-level heterogeneity. We provide theoretical results that\nclarify how our objective navigates the trade-off between data quantity and\ndata quality. We evaluate CTRL alongside other state-of-the-art benchmarks on 5\nlarge-scale datasets. This includes a dataset from the national asylum program\nin Switzerland, where the algorithmic geographic assignment of asylum seekers\nis currently being piloted. CTRL consistently outperforms the benchmarks across\nseveral key metrics and when using a range of different base learners.", "AI": {"tldr": "CTRL is a meta-learning method improving accuracy and preserving source-level heterogeneity in ML tasks with diverse data sources.", "motivation": "Address the challenge of generating reliable predictions across distinct data sources with distributional shifts and varying sample sizes, e.g., in refugee resettlement programs.", "method": "Combines cross-domain residual learning and adaptive pooling/clustering to balance data quantity and quality.", "result": "Outperforms benchmarks on 5 datasets, including a Swiss asylum program pilot, across key metrics.", "conclusion": "CTRL effectively navigates trade-offs between data quantity and quality, enhancing both overall accuracy and source-level reliability."}}
{"id": "2508.11290", "pdf": "https://arxiv.org/pdf/2508.11290", "abs": "https://arxiv.org/abs/2508.11290", "authors": ["Utsav Maskey", "Sumit Yadav", "Mark Dras", "Usman Naseem"], "title": "SafeConstellations: Steering LLM Safety to Reduce Over-Refusals Through Task-Specific Trajectory", "categories": ["cs.CL"], "comment": "Preprint", "summary": "LLMs increasingly exhibit over-refusal behavior, where safety mechanisms\ncause models to reject benign instructions that superficially resemble harmful\ncontent. This phenomena diminishes utility in production applications that\nrepeatedly rely on common prompt templates or applications that frequently rely\non LLMs for specific tasks (e.g. sentiment analysis, language translation).\nThrough comprehensive evaluation, we demonstrate that LLMs still tend to refuse\nresponses to harmful instructions when those instructions are reframed to\nappear as benign tasks. Our mechanistic analysis reveal that LLMs follow\ndistinct \"constellation\" patterns in embedding space as representations\ntraverse layers, with each task maintaining consistent trajectories that shift\npredictably between refusal and non-refusal cases. We introduce\nSafeConstellations, an inference-time trajectory-shifting approach that tracks\ntask-specific trajectory patterns and guides representations toward non-refusal\npathways. By selectively guiding model behavior only on tasks prone to\nover-refusal, and by preserving general model behavior, our method reduces\nover-refusal rates by up to 73% with minimal impact on utility-offering a\nprincipled approach to mitigating over-refusals.", "AI": {"tldr": "SafeConstellations reduces LLM over-refusal by 73% by guiding task-specific trajectories in embedding space.", "motivation": "Over-refusal in LLMs reduces utility for benign tasks resembling harmful content.", "method": "Introduces SafeConstellations, an inference-time approach tracking task-specific trajectory patterns to guide representations toward non-refusal pathways.", "result": "Reduces over-refusal rates by up to 73% with minimal utility impact.", "conclusion": "Offers a principled method to mitigate over-refusal while preserving general model behavior."}}
{"id": "2508.10919", "pdf": "https://arxiv.org/pdf/2508.10919", "abs": "https://arxiv.org/abs/2508.10919", "authors": ["Mohammed Saqr", "Kamila Misiejuk", "Sonsoles L\u00f3pez-Pernas"], "title": "Human-AI collaboration or obedient and often clueless AI in instruct, serve, repeat dynamics?", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "While research on human-AI collaboration exists, it mainly examined language\nlearning and used traditional counting methods with little attention to\nevolution and dynamics of collaboration on cognitively demanding tasks. This\nstudy examines human-AI interactions while solving a complex problem.\nStudent-AI interactions were qualitatively coded and analyzed with transition\nnetwork analysis, sequence analysis and partial correlation networks as well as\ncomparison of frequencies using chi-square and Person-residual shaded Mosaic\nplots to map interaction patterns, their evolution, and their relationship to\nproblem complexity and student performance. Findings reveal a dominant\nInstructive pattern with interactions characterized by iterative ordering\nrather than collaborative negotiation. Oftentimes, students engaged in long\nthreads that showed misalignment between their prompts and AI output that\nexemplified a lack of synergy that challenges the prevailing assumptions about\nLLMs as collaborative partners. We also found no significant correlations\nbetween assignment complexity, prompt length, and student grades suggesting a\nlack of cognitive depth, or effect of problem difficulty. Our study indicates\nthat the current LLMs, optimized for instruction-following rather than\ncognitive partnership, compound their capability to act as cognitively\nstimulating or aligned collaborators. Implications for designing AI systems\nthat prioritize cognitive alignment and collaboration are discussed.", "AI": {"tldr": "The study examines human-AI collaboration on complex tasks, revealing a dominant instructive pattern rather than true collaboration, with misalignment between prompts and AI output. No significant links were found between task complexity, prompt length, and student performance.", "motivation": "To explore the dynamics of human-AI collaboration on cognitively demanding tasks, addressing gaps in existing research focused on language learning and traditional methods.", "method": "Qualitative coding and analysis of student-AI interactions using transition network analysis, sequence analysis, partial correlation networks, and frequency comparisons (chi-square, Mosaic plots).", "result": "Dominant instructive interaction patterns with misaligned prompts and AI output, no significant correlations between task complexity, prompt length, and student grades.", "conclusion": "Current LLMs are optimized for instruction-following, not cognitive partnership, highlighting the need for AI systems designed for cognitive alignment and collaboration."}}
{"id": "2508.10972", "pdf": "https://arxiv.org/pdf/2508.10972", "abs": "https://arxiv.org/abs/2508.10972", "authors": ["Rosiana Natalie", "Wenqian Xu", "Ruei-Che Chang", "Rada Mihalcea", "Anhong Guo"], "title": "Not There Yet: Evaluating Vision Language Models in Simulating the Visual Perception of People with Low Vision", "categories": ["cs.CV", "cs.AI", "cs.HC"], "comment": null, "summary": "Advances in vision language models (VLMs) have enabled the simulation of\ngeneral human behavior through their reasoning and problem solving\ncapabilities. However, prior research has not investigated such simulation\ncapabilities in the accessibility domain. In this paper, we evaluate the extent\nto which VLMs can simulate the vision perception of low vision individuals when\ninterpreting images. We first compile a benchmark dataset through a survey\nstudy with 40 low vision participants, collecting their brief and detailed\nvision information and both open-ended and multiple-choice image perception and\nrecognition responses to up to 25 images. Using these responses, we construct\nprompts for VLMs (GPT-4o) to create simulated agents of each participant,\nvarying the included information on vision information and example image\nresponses. We evaluate the agreement between VLM-generated responses and\nparticipants' original answers. Our results indicate that VLMs tend to infer\nbeyond the specified vision ability when given minimal prompts, resulting in\nlow agreement (0.59). The agreement between the agent' and participants'\nresponses remains low when only either the vision information (0.59) or example\nimage responses (0.59) are provided, whereas a combination of both\nsignificantly increase the agreement (0.70, p < 0.0001). Notably, a single\nexample combining both open-ended and multiple-choice responses, offers\nsignificant performance improvements over either alone (p < 0.0001), while\nadditional examples provided minimal benefits (p > 0.05).", "AI": {"tldr": "The paper evaluates VLMs' ability to simulate low vision individuals' image perception, finding minimal prompts yield low agreement (0.59), while combining vision info and example responses improves it (0.70).", "motivation": "Prior research hasn't explored VLMs' simulation capabilities in accessibility, specifically for low vision individuals.", "method": "A benchmark dataset was created from 40 low vision participants' responses to images. VLMs (GPT-4o) simulated participants using prompts with varying vision info and example responses. Agreement between VLM and human responses was measured.", "result": "VLMs over-inferred with minimal prompts (agreement 0.59). Combining vision info and example responses significantly improved agreement (0.70). A single combined example outperformed isolated ones, with diminishing returns for additional examples.", "conclusion": "VLMs can simulate low vision perception better with combined vision info and example responses, but minimal prompts lead to poor accuracy."}}
{"id": "2508.11145", "pdf": "https://arxiv.org/pdf/2508.11145", "abs": "https://arxiv.org/abs/2508.11145", "authors": ["Huan Zhang", "Daokun Zhang", "Kexin Meng", "Geoffrey I. Webb"], "title": "Towards the Next-generation Bayesian Network Classifiers", "categories": ["cs.LG"], "comment": null, "summary": "Bayesian network classifiers provide a feasible solution to tabular data\nclassification, with a number of merits like high time and memory efficiency,\nand great explainability. However, due to the parameter explosion and data\nsparsity issues, Bayesian network classifiers are restricted to low-order\nfeature dependency modeling, making them struggle in extrapolating the\noccurrence probabilities of complex real-world data. In this paper, we propose\na novel paradigm to design high-order Bayesian network classifiers, by learning\ndistributional representations for feature values, as what has been done in\nword embedding and graph representation learning. The learned distributional\nrepresentations are encoded with the semantic relatedness between different\nfeatures through their observed co-occurrence patterns in training data, which\nthen serve as a hallmark to extrapolate the occurrence probabilities of new\ntest samples. As a classifier design realization, we remake the K-dependence\nBayesian classifier (KDB) by extending it into a neural version, i.e.,\nNeuralKDB, where a novel neural network architecture is designed to learn\ndistributional representations of feature values and parameterize the\nconditional probabilities between interdependent features. A stochastic\ngradient descent based algorithm is designed to train the NeuralKDB model\nefficiently. Extensive classification experiments on 60 UCI datasets\ndemonstrate that the proposed NeuralKDB classifier excels in capturing\nhigh-order feature dependencies and significantly outperforms the conventional\nBayesian network classifiers, as well as other competitive classifiers,\nincluding two neural network based classifiers without distributional\nrepresentation learning.", "AI": {"tldr": "The paper proposes NeuralKDB, a neural version of the K-dependence Bayesian classifier, to address limitations of traditional Bayesian network classifiers by learning distributional representations for feature values, improving high-order dependency modeling.", "motivation": "Bayesian network classifiers struggle with high-order feature dependencies due to parameter explosion and data sparsity, limiting their performance on complex real-world data.", "method": "The authors extend the K-dependence Bayesian classifier into NeuralKDB, using neural networks to learn distributional representations and parameterize conditional probabilities. A stochastic gradient descent algorithm is designed for efficient training.", "result": "Experiments on 60 UCI datasets show NeuralKDB outperforms traditional Bayesian classifiers and other competitive methods, including neural networks without distributional representation learning.", "conclusion": "NeuralKDB effectively captures high-order feature dependencies, offering superior performance and addressing key limitations of conventional Bayesian network classifiers."}}
{"id": "2508.11310", "pdf": "https://arxiv.org/pdf/2508.11310", "abs": "https://arxiv.org/abs/2508.11310", "authors": ["Beichen Guo", "Zhiyuan Wen", "Yu Yang", "Peng Gao", "Ruosong Yang", "Jiaxing Shen"], "title": "SGSimEval: A Comprehensive Multifaceted and Similarity-Enhanced Benchmark for Automatic Survey Generation Systems", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "Accepted to The 21st International Conference on Advanced Data Mining\n  and Applications (ADMA2025)", "summary": "The growing interest in automatic survey generation (ASG), a task that\ntraditionally required considerable time and effort, has been spurred by recent\nadvances in large language models (LLMs). With advancements in\nretrieval-augmented generation (RAG) and the rising popularity of multi-agent\nsystems (MASs), synthesizing academic surveys using LLMs has become a viable\napproach, thereby elevating the need for robust evaluation methods in this\ndomain. However, existing evaluation methods suffer from several limitations,\nincluding biased metrics, a lack of human preference, and an over-reliance on\nLLMs-as-judges. To address these challenges, we propose SGSimEval, a\ncomprehensive benchmark for Survey Generation with Similarity-Enhanced\nEvaluation that evaluates automatic survey generation systems by integrating\nassessments of the outline, content, and references, and also combines\nLLM-based scoring with quantitative metrics to provide a multifaceted\nevaluation framework. In SGSimEval, we also introduce human preference metrics\nthat emphasize both inherent quality and similarity to humans. Extensive\nexperiments reveal that current ASG systems demonstrate human-comparable\nsuperiority in outline generation, while showing significant room for\nimprovement in content and reference generation, and our evaluation metrics\nmaintain strong consistency with human assessments.", "AI": {"tldr": "SGSimEval is a benchmark for evaluating automatic survey generation (ASG) systems, combining LLM-based scoring, quantitative metrics, and human preference to address limitations in current evaluation methods.", "motivation": "Existing ASG evaluation methods are flawed due to biased metrics, lack of human preference, and over-reliance on LLMs-as-judges.", "method": "Proposes SGSimEval, integrating outline, content, and reference assessments with LLM-based and quantitative metrics, plus human preference metrics.", "result": "Current ASG systems excel in outline generation but need improvement in content and reference generation; SGSimEval aligns well with human assessments.", "conclusion": "SGSimEval provides a robust, multifaceted evaluation framework for ASG, highlighting strengths and areas for improvement."}}
{"id": "2508.11011", "pdf": "https://arxiv.org/pdf/2508.11011", "abs": "https://arxiv.org/abs/2508.11011", "authors": ["Xuezheng Chen", "Zhengbo Zou"], "title": "Are Large Pre-trained Vision Language Models Effective Construction Safety Inspectors?", "categories": ["cs.CV"], "comment": null, "summary": "Construction safety inspections typically involve a human inspector\nidentifying safety concerns on-site. With the rise of powerful Vision Language\nModels (VLMs), researchers are exploring their use for tasks such as detecting\nsafety rule violations from on-site images. However, there is a lack of open\ndatasets to comprehensively evaluate and further fine-tune VLMs in construction\nsafety inspection. Current applications of VLMs use small, supervised datasets,\nlimiting their applicability in tasks they are not directly trained for. In\nthis paper, we propose the ConstructionSite 10k, featuring 10,000 construction\nsite images with annotations for three inter-connected tasks, including image\ncaptioning, safety rule violation visual question answering (VQA), and\nconstruction element visual grounding. Our subsequent evaluation of current\nstate-of-the-art large pre-trained VLMs shows notable generalization abilities\nin zero-shot and few-shot settings, while additional training is needed to make\nthem applicable to actual construction sites. This dataset allows researchers\nto train and evaluate their own VLMs with new architectures and techniques,\nproviding a valuable benchmark for construction safety inspection.", "AI": {"tldr": "The paper introduces ConstructionSite 10k, a dataset of 10,000 annotated construction site images for evaluating and fine-tuning Vision Language Models (VLMs) in safety inspections. It highlights VLMs' generalization abilities but notes the need for further training for real-world application.", "motivation": "Current VLM applications in construction safety inspections rely on small, supervised datasets, limiting their adaptability. A comprehensive open dataset is needed to evaluate and enhance VLMs for this domain.", "method": "The authors propose ConstructionSite 10k, a dataset with annotations for image captioning, safety rule violation VQA, and construction element visual grounding. They evaluate state-of-the-art VLMs in zero-shot and few-shot settings.", "result": "VLMs show notable generalization in zero-shot and few-shot scenarios but require additional training for practical use on actual construction sites.", "conclusion": "ConstructionSite 10k serves as a benchmark for training and evaluating VLMs in construction safety inspections, enabling advancements in model architectures and techniques."}}
{"id": "2508.11159", "pdf": "https://arxiv.org/pdf/2508.11159", "abs": "https://arxiv.org/abs/2508.11159", "authors": ["Heqiang Wang", "Weihong Yang", "Xiaoxiong Zhong", "Jia Zhou", "Fangming Liu", "Weizhe Zhang"], "title": "Mitigating Modality Quantity and Quality Imbalance in Multimodal Online Federated Learning", "categories": ["cs.LG"], "comment": "arXiv admin note: text overlap with arXiv:2505.16138", "summary": "The Internet of Things (IoT) ecosystem produces massive volumes of multimodal\ndata from diverse sources, including sensors, cameras, and microphones. With\nadvances in edge intelligence, IoT devices have evolved from simple data\nacquisition units into computationally capable nodes, enabling localized\nprocessing of heterogeneous multimodal data. This evolution necessitates\ndistributed learning paradigms that can efficiently handle such data.\nFurthermore, the continuous nature of data generation and the limited storage\ncapacity of edge devices demand an online learning framework. Multimodal Online\nFederated Learning (MMO-FL) has emerged as a promising approach to meet these\nrequirements. However, MMO-FL faces new challenges due to the inherent\ninstability of IoT devices, which often results in modality quantity and\nquality imbalance (QQI) during data collection. In this work, we systematically\ninvestigate the impact of QQI within the MMO-FL framework and present a\ncomprehensive theoretical analysis quantifying how both types of imbalance\ndegrade learning performance. To address these challenges, we propose the\nModality Quantity and Quality Rebalanced (QQR) algorithm, a prototype learning\nbased method designed to operate in parallel with the training process.\nExtensive experiments on two real-world multimodal datasets show that the\nproposed QQR algorithm consistently outperforms benchmarks under modality\nimbalance conditions with promising learning performance.", "AI": {"tldr": "The paper introduces the Modality Quantity and Quality Rebalanced (QQR) algorithm to address imbalances in Multimodal Online Federated Learning (MMO-FL) caused by IoT device instability.", "motivation": "The need for efficient distributed learning paradigms in IoT ecosystems due to multimodal data heterogeneity and edge device limitations.", "method": "Proposes the QQR algorithm, a prototype learning-based method, to rebalance modality quantity and quality imbalances during training.", "result": "QQR outperforms benchmarks on real-world datasets under imbalance conditions, showing improved learning performance.", "conclusion": "QQR effectively mitigates modality imbalance challenges in MMO-FL, enhancing learning outcomes for IoT applications."}}
{"id": "2508.11318", "pdf": "https://arxiv.org/pdf/2508.11318", "abs": "https://arxiv.org/abs/2508.11318", "authors": ["Sahil Sk", "Debasish Dhal", "Sonal Khosla", "Sk Shahid", "Sambit Shekhar", "Akash Dhaka", "Shantipriya Parida", "Dilip K. Prasad", "Ond\u0159ej Bojar"], "title": "LLM Compression: How Far Can We Go in Balancing Size and Performance?", "categories": ["cs.CL"], "comment": "This paper has been accepted for presentation at the RANLP 2025\n  conference", "summary": "Quantization is an essential and popular technique for improving the\naccessibility of large language models (LLMs) by reducing memory usage and\ncomputational costs while maintaining performance. In this study, we apply\n4-bit Group Scaling Quantization (GSQ) and Generative Pretrained Transformer\nQuantization (GPTQ) to LLaMA 1B, Qwen 0.5B, and PHI 1.5B, evaluating their\nimpact across multiple NLP tasks. We benchmark these models on MS MARCO\n(Information Retrieval), BoolQ (Boolean Question Answering), and GSM8K\n(Mathematical Reasoning) datasets, assessing both accuracy and efficiency\nacross various tasks. The study measures the trade-offs between model\ncompression and task performance, analyzing key evaluation metrics, namely\naccuracy, inference latency, and throughput (total output tokens generated per\nsecond), providing insights into the suitability of low-bit quantization for\nreal-world deployment. Using the results, users can then make suitable\ndecisions based on the specifications that need to be met. We discuss the pros\nand cons of GSQ and GPTQ techniques on models of different sizes, which also\nserve as a benchmark for future experiments.", "AI": {"tldr": "The study evaluates 4-bit Group Scaling Quantization (GSQ) and GPTQ on LLaMA, Qwen, and PHI models, benchmarking their performance on NLP tasks like MS MARCO, BoolQ, and GSM8K to analyze trade-offs between compression and accuracy.", "motivation": "To improve accessibility of large language models by reducing memory and computational costs while maintaining performance.", "method": "Applied GSQ and GPTQ to LLaMA 1B, Qwen 0.5B, and PHI 1.5B, evaluating on MS MARCO, BoolQ, and GSM8K datasets.", "result": "Analyzed accuracy, inference latency, and throughput to assess trade-offs between model compression and task performance.", "conclusion": "Provides insights into low-bit quantization suitability for real-world deployment, serving as a benchmark for future experiments."}}
{"id": "2508.11021", "pdf": "https://arxiv.org/pdf/2508.11021", "abs": "https://arxiv.org/abs/2508.11021", "authors": ["Zisheng Liang", "Kidus Zewde", "Rudra Pratap Singh", "Disha Patil", "Zexi Chen", "Jiayu Xue", "Yao Yao", "Yifei Chen", "Qinzhe Liu", "Simiao Ren"], "title": "Can Multi-modal (reasoning) LLMs detect document manipulation?", "categories": ["cs.CV", "cs.CL"], "comment": "arXiv admin note: text overlap with arXiv:2503.20084", "summary": "Document fraud poses a significant threat to industries reliant on secure and\nverifiable documentation, necessitating robust detection mechanisms. This study\ninvestigates the efficacy of state-of-the-art multi-modal large language models\n(LLMs)-including OpenAI O1, OpenAI 4o, Gemini Flash (thinking), Deepseek Janus,\nGrok, Llama 3.2 and 4, Qwen 2 and 2.5 VL, Mistral Pixtral, and Claude 3.5 and\n3.7 Sonnet-in detecting fraudulent documents. We benchmark these models against\neach other and prior work on document fraud detection techniques using a\nstandard dataset with real transactional documents. Through prompt optimization\nand detailed analysis of the models' reasoning processes, we evaluate their\nability to identify subtle indicators of fraud, such as tampered text,\nmisaligned formatting, and inconsistent transactional sums. Our results reveal\nthat top-performing multi-modal LLMs demonstrate superior zero-shot\ngeneralization, outperforming conventional methods on out-of-distribution\ndatasets, while several vision LLMs exhibit inconsistent or subpar performance.\nNotably, model size and advanced reasoning capabilities show limited\ncorrelation with detection accuracy, suggesting task-specific fine-tuning is\ncritical. This study underscores the potential of multi-modal LLMs in enhancing\ndocument fraud detection systems and provides a foundation for future research\ninto interpretable and scalable fraud mitigation strategies.", "AI": {"tldr": "The study evaluates multi-modal LLMs for detecting fraudulent documents, finding top models outperform conventional methods, though performance varies. Task-specific fine-tuning is key, not just model size.", "motivation": "Document fraud threatens industries needing secure documentation, requiring effective detection methods.", "method": "Benchmarked multi-modal LLMs (e.g., OpenAI, Gemini, Claude) on a standard dataset, analyzing fraud indicators like tampered text and misaligned formatting.", "result": "Top LLMs excel in zero-shot generalization, but performance varies; model size doesn't strongly correlate with accuracy.", "conclusion": "Multi-modal LLMs show promise for fraud detection, emphasizing the need for task-specific fine-tuning and future research."}}
{"id": "2508.11180", "pdf": "https://arxiv.org/pdf/2508.11180", "abs": "https://arxiv.org/abs/2508.11180", "authors": ["Yiyang Shen", "Weiran Wang"], "title": "A Semi-supervised Generative Model for Incomplete Multi-view Data Integration with Missing Labels", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Multi-view learning is widely applied to real-life datasets, such as multiple\nomics biological data, but it often suffers from both missing views and missing\nlabels. Prior probabilistic approaches addressed the missing view problem by\nusing a product-of-experts scheme to aggregate representations from present\nviews and achieved superior performance over deterministic classifiers, using\nthe information bottleneck (IB) principle. However, the IB framework is\ninherently fully supervised and cannot leverage unlabeled data. In this work,\nwe propose a semi-supervised generative model that utilizes both labeled and\nunlabeled samples in a unified framework. Our method maximizes the likelihood\nof unlabeled samples to learn a latent space shared with the IB on labeled\ndata. We also perform cross-view mutual information maximization in the latent\nspace to enhance the extraction of shared information across views. Compared to\nexisting approaches, our model achieves better predictive and imputation\nperformance on both image and multi-omics data with missing views and limited\nlabeled samples.", "AI": {"tldr": "A semi-supervised generative model for multi-view learning addresses missing views and labels by combining labeled and unlabeled data, outperforming existing methods in predictive and imputation tasks.", "motivation": "Multi-view learning often faces missing views and labels. Existing probabilistic approaches, like the information bottleneck (IB), are fully supervised and ignore unlabeled data. This work aims to leverage both labeled and unlabeled data for better performance.", "method": "The proposed model maximizes the likelihood of unlabeled samples to learn a shared latent space with the IB on labeled data. It also uses cross-view mutual information maximization to enhance shared information extraction.", "result": "The model outperforms existing approaches in predictive and imputation performance on image and multi-omics data with missing views and limited labels.", "conclusion": "The semi-supervised generative model effectively combines labeled and unlabeled data, improving performance in multi-view learning tasks with missing data."}}
{"id": "2508.11343", "pdf": "https://arxiv.org/pdf/2508.11343", "abs": "https://arxiv.org/abs/2508.11343", "authors": ["Haitong Luo", "Weiyao Zhang", "Suhang Wang", "Wenji Zou", "Chungang Lin", "Xuying Meng", "Yujun Zhang"], "title": "SpecDetect: Simple, Fast, and Training-Free Detection of LLM-Generated Text via Spectral Analysis", "categories": ["cs.CL"], "comment": "Under Review", "summary": "The proliferation of high-quality text from Large Language Models (LLMs)\ndemands reliable and efficient detection methods. While existing training-free\napproaches show promise, they often rely on surface-level statistics and\noverlook fundamental signal properties of the text generation process. In this\nwork, we reframe detection as a signal processing problem, introducing a novel\nparadigm that analyzes the sequence of token log-probabilities in the frequency\ndomain. By systematically analyzing the signal's spectral properties using the\nglobal Discrete Fourier Transform (DFT) and the local Short-Time Fourier\nTransform (STFT), we find that human-written text consistently exhibits\nsignificantly higher spectral energy. This higher energy reflects the\nlarger-amplitude fluctuations inherent in human writing compared to the\nsuppressed dynamics of LLM-generated text. Based on this key insight, we\nconstruct SpecDetect, a detector built on a single, robust feature from the\nglobal DFT: DFT total energy. We also propose an enhanced version,\nSpecDetect++, which incorporates a sampling discrepancy mechanism to further\nboost robustness. Extensive experiments demonstrate that our approach\noutperforms the state-of-the-art model while running in nearly half the time.\nOur work introduces a new, efficient, and interpretable pathway for\nLLM-generated text detection, showing that classical signal processing\ntechniques offer a surprisingly powerful solution to this modern challenge.", "AI": {"tldr": "The paper introduces SpecDetect and SpecDetect++, novel methods for detecting LLM-generated text by analyzing token log-probabilities in the frequency domain, outperforming existing approaches in efficiency and accuracy.", "motivation": "The need for reliable detection of LLM-generated text due to its proliferation, as current methods rely on superficial statistics and miss deeper signal properties.", "method": "Reframes detection as a signal processing problem, using global DFT and local STFT to analyze spectral properties of token log-probabilities, identifying higher spectral energy in human-written text.", "result": "Human-written text shows higher spectral energy, leading to SpecDetect (based on DFT total energy) and SpecDetect++ (with added sampling discrepancy), both outperforming state-of-the-art models in speed and accuracy.", "conclusion": "Signal processing techniques provide an efficient and interpretable solution for LLM-generated text detection, demonstrating their power for modern challenges."}}
{"id": "2508.11032", "pdf": "https://arxiv.org/pdf/2508.11032", "abs": "https://arxiv.org/abs/2508.11032", "authors": ["Yanwu Yang", "Guinan Su", "Jiesi Hu", "Francesco Sammarco", "Jonas Geiping", "Thomas Wolfers"], "title": "MedSAMix: A Training-Free Model Merging Approach for Medical Image Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Universal medical image segmentation models have emerged as a promising\nparadigm due to their strong generalizability across diverse tasks, showing\ngreat potential for a wide range of clinical applications. This potential has\nbeen partly driven by the success of general-purpose vision models such as the\nSegment Anything Model (SAM), which has inspired the development of various\nfine-tuned variants for medical segmentation tasks. However, fine-tuned\nvariants like MedSAM are trained on comparatively limited medical imaging data\nthat often suffers from heterogeneity, scarce annotations, and distributional\nshifts. These challenges limit their ability to generalize across a wide range\nof medical segmentation tasks. In this regard, we propose MedSAMix, a\ntraining-free model merging method that integrates the strengths of both\ngeneralist models (e.g., SAM) and specialist models (e.g., MedSAM) for medical\nimage segmentation. In contrast to traditional model merging approaches that\nrely on manual configuration and often result in suboptimal outcomes, we\npropose a zero-order optimization method to automatically discover optimal\nlayer-wise merging solutions. Furthermore, for clinical applications, we\ndevelop two regimes to meet the demand of domain-specificity and\ngeneralizability in different scenarios by single-task optimization and\nmulti-objective optimization respectively. Extensive evaluations on 25 medical\nsegmentation tasks demonstrate that MedSAMix effectively mitigates model bias\nand consistently improves performance in both domain-specific accuracy and\ngeneralization, achieving improvements of 6.67% on specialized tasks and 4.37%\non multi-task evaluations.", "AI": {"tldr": "MedSAMix is a training-free model merging method combining generalist (SAM) and specialist (MedSAM) models for medical image segmentation, improving performance by 6.67% on specialized tasks and 4.37% on multi-task evaluations.", "motivation": "Existing fine-tuned medical segmentation models like MedSAM face limitations due to data heterogeneity, scarce annotations, and distributional shifts, hindering generalization.", "method": "Proposes a zero-order optimization method for automatic layer-wise merging of generalist and specialist models, with two regimes for domain-specificity and generalizability.", "result": "Achieves improvements of 6.67% on specialized tasks and 4.37% on multi-task evaluations across 25 medical segmentation tasks.", "conclusion": "MedSAMix effectively mitigates model bias and enhances performance in both domain-specific and generalizable scenarios."}}
{"id": "2508.11190", "pdf": "https://arxiv.org/pdf/2508.11190", "abs": "https://arxiv.org/abs/2508.11190", "authors": ["Feng-ao Wang", "Shaobo Chen", "Yao Xuan", "Junwei Liu", "Qi Gao", "Hongdong Zhu", "Junjie Hou", "Lixin Yuan", "Jinyu Cheng", "Chenxin Yi", "Hai Wei", "Yin Ma", "Tao Xu", "Kai Wen", "Yixue Li"], "title": "Quantum-Boosted High-Fidelity Deep Learning", "categories": ["cs.LG", "cs.AI", "q-bio.GN"], "comment": null, "summary": "A fundamental limitation of probabilistic deep learning is its predominant\nreliance on Gaussian priors. This simplistic assumption prevents models from\naccurately capturing the complex, non-Gaussian landscapes of natural data,\nparticularly in demanding domains like complex biological data, severely\nhindering the fidelity of the model for scientific discovery. The\nphysically-grounded Boltzmann distribution offers a more expressive\nalternative, but it is computationally intractable on classical computers. To\ndate, quantum approaches have been hampered by the insufficient qubit scale and\noperational stability required for the iterative demands of deep learning.\nHere, we bridge this gap by introducing the Quantum Boltzmann\nMachine-Variational Autoencoder (QBM-VAE), a large-scale and long-time stable\nhybrid quantum-classical architecture. Our framework leverages a quantum\nprocessor for efficient sampling from the Boltzmann distribution, enabling its\nuse as a powerful prior within a deep generative model. Applied to\nmillion-scale single-cell datasets from multiple sources, the QBM-VAE generates\na latent space that better preserves complex biological structures,\nconsistently outperforming conventional Gaussian-based deep learning models\nlike VAE and SCVI in essential tasks such as omics data integration, cell-type\nclassification, and trajectory inference. It also provides a typical example of\nintroducing a physics priori into deep learning to drive the model to acquire\nscientific discovery capabilities that breaks through data limitations. This\nwork provides the demonstration of a practical quantum advantage in deep\nlearning on a large-scale scientific problem and offers a transferable\nblueprint for developing hybrid quantum AI models.", "AI": {"tldr": "The paper introduces QBM-VAE, a hybrid quantum-classical model using Boltzmann distribution as a prior, outperforming Gaussian-based models in biological data tasks.", "motivation": "Gaussian priors in probabilistic deep learning are too simplistic for complex data like biological datasets, limiting scientific discovery.", "method": "The QBM-VAE combines quantum processors for Boltzmann sampling with classical deep learning, enabling expressive priors.", "result": "QBM-VAE outperforms Gaussian-based models (VAE, SCVI) in tasks like omics data integration and cell-type classification.", "conclusion": "The work demonstrates practical quantum advantage in deep learning and provides a blueprint for hybrid quantum AI models."}}
{"id": "2508.11364", "pdf": "https://arxiv.org/pdf/2508.11364", "abs": "https://arxiv.org/abs/2508.11364", "authors": ["Sylvio R\u00fcdian", "Yassin Elsir", "Marvin Kretschmer", "Sabine Cayrou", "Niels Pinkwart"], "title": "Feedback Indicators: The Alignment between Llama and a Teacher in Language Learning", "categories": ["cs.CL"], "comment": "11 pages, one table", "summary": "Automated feedback generation has the potential to enhance students' learning\nprogress by providing timely and targeted feedback. Moreover, it can assist\nteachers in optimizing their time, allowing them to focus on more strategic and\npersonalized aspects of teaching. To generate high-quality, information-rich\nformative feedback, it is essential first to extract relevant indicators, as\nthese serve as the foundation upon which the feedback is constructed. Teachers\noften employ feedback criteria grids composed of various indicators that they\nevaluate systematically. This study examines the initial phase of extracting\nsuch indicators from students' submissions of a language learning course using\nthe large language model Llama 3.1. Accordingly, the alignment between\nindicators generated by the LLM and human ratings across various feedback\ncriteria is investigated. The findings demonstrate statistically significant\nstrong correlations, even in cases involving unanticipated combinations of\nindicators and criteria. The methodology employed in this paper offers a\npromising foundation for extracting indicators from students' submissions using\nLLMs. Such indicators can potentially be utilized to auto-generate explainable\nand transparent formative feedback in future research.", "AI": {"tldr": "The paper explores using Llama 3.1 to extract feedback indicators from student submissions, showing strong correlation with human ratings, enabling future auto-generated feedback.", "motivation": "To enhance learning and teaching efficiency by automating feedback generation using LLMs.", "method": "Extracting indicators from student submissions using Llama 3.1 and comparing them with human ratings.", "result": "Strong correlations between LLM-generated and human-rated indicators, even for unexpected combinations.", "conclusion": "LLMs like Llama 3.1 can effectively extract indicators for auto-generating transparent formative feedback."}}
{"id": "2508.11058", "pdf": "https://arxiv.org/pdf/2508.11058", "abs": "https://arxiv.org/abs/2508.11058", "authors": ["Wentao Mo", "Qingchao Chen", "Yuxin Peng", "Siyuan Huang", "Yang Liu"], "title": "Advancing 3D Scene Understanding with MV-ScanQA Multi-View Reasoning Evaluation and TripAlign Pre-training Dataset", "categories": ["cs.CV", "cs.MM"], "comment": "Accepeted to ACM MM 25", "summary": "The advancement of 3D vision-language (3D VL) learning is hindered by several\nlimitations in existing 3D VL datasets: they rarely necessitate reasoning\nbeyond a close range of objects in single viewpoint, and annotations often link\ninstructions to single objects, missing richer contextual alignments between\nmultiple objects. This significantly curtails the development of models capable\nof deep, multi-view 3D scene understanding over distant objects. To address\nthese challenges, we introduce MV-ScanQA, a novel 3D question answering dataset\nwhere 68% of questions explicitly require integrating information from multiple\nviews (compared to less than 7% in existing datasets), thereby rigorously\ntesting multi-view compositional reasoning. To facilitate the training of\nmodels for such demanding scenarios, we present TripAlign dataset, a\nlarge-scale and low-cost 2D-3D-language pre-training corpus containing 1M <2D\nview, set of 3D objects, text> triplets that explicitly aligns groups of\ncontextually related objects with text, providing richer, view-grounded\nmulti-object multimodal alignment signals than previous single-object\nannotations. We further develop LEGO, a baseline method for the multi-view\nreasoning challenge in MV-ScanQA, transferring knowledge from pre-trained 2D\nLVLMs to 3D domain with TripAlign. Empirically, LEGO pre-trained on TripAlign\nachieves state-of-the-art performance not only on the proposed MV-ScanQA, but\nalso on existing benchmarks for 3D dense captioning and question answering.\nDatasets and code are available at\nhttps://matthewdm0816.github.io/tripalign-mvscanqa.", "AI": {"tldr": "MV-ScanQA and TripAlign datasets address limitations in 3D VL learning by enabling multi-view reasoning and richer object-text alignments. LEGO, a baseline method, achieves SOTA performance.", "motivation": "Existing 3D VL datasets lack multi-view reasoning and rich contextual alignments, limiting deep 3D scene understanding.", "method": "Introduce MV-ScanQA for multi-view QA and TripAlign for 2D-3D-language pre-training. Develop LEGO to transfer 2D LVLM knowledge to 3D.", "result": "LEGO pre-trained on TripAlign achieves SOTA on MV-ScanQA and existing benchmarks.", "conclusion": "MV-ScanQA and TripAlign advance 3D VL learning by enabling multi-view reasoning and richer alignments, with LEGO demonstrating effectiveness."}}
{"id": "2508.11205", "pdf": "https://arxiv.org/pdf/2508.11205", "abs": "https://arxiv.org/abs/2508.11205", "authors": ["Cheng Jing", "Uvini Balasuriya Mudiyanselage", "Woojin Cho", "Minju Jo", "Anthony Gruber", "Kookjin Lee"], "title": "Meta-learning Structure-Preserving Dynamics", "categories": ["cs.LG"], "comment": null, "summary": "Structure-preserving approaches to dynamics modeling have demonstrated great\npotential for modeling physical systems due to their strong inductive biases\nthat enforce conservation laws and dissipative behavior. However, the resulting\nmodels are typically trained for fixed system configurations, requiring\nexplicit knowledge of system parameters as well as costly retraining for each\nnew set of parameters -- a major limitation in many-query or parameter-varying\nscenarios. Meta-learning offers a potential solution, but existing approaches\nlike optimization-based meta-learning often suffer from training instability or\nlimited generalization capability. Inspired by ideas from computer vision, we\nintroduce a modulation-based meta-learning framework that directly conditions\nstructure-preserving models on compact latent representations of potentially\nunknown system parameters, avoiding the need for gray-box system knowledge and\nexplicit optimization during adaptation. Through the application of novel\nmodulation strategies to parametric energy-conserving and dissipative systems,\nwe enable scalable and generalizable learning across parametric families of\ndynamical systems. Experiments on standard benchmark problems demonstrate that\nour approach achieves accurate predictions in few-shot learning settings,\nwithout compromising on the essential physical constraints necessary for\ndynamical stability and effective generalization performance across parameter\nspace.", "AI": {"tldr": "A modulation-based meta-learning framework is introduced to generalize structure-preserving models for parametric dynamical systems without costly retraining or explicit system knowledge.", "motivation": "Existing structure-preserving models are limited by fixed configurations and costly retraining for new parameters. Meta-learning solutions face instability or poor generalization.", "method": "A modulation-based meta-learning framework conditions models on latent representations of system parameters, avoiding explicit optimization or gray-box knowledge.", "result": "The approach enables scalable, generalizable learning for parametric systems, achieving accurate predictions in few-shot settings while preserving physical constraints.", "conclusion": "The framework successfully generalizes across parametric families of dynamical systems without compromising stability or performance."}}
{"id": "2508.11383", "pdf": "https://arxiv.org/pdf/2508.11383", "abs": "https://arxiv.org/abs/2508.11383", "authors": ["Mikhail Seleznyov", "Mikhail Chaichuk", "Gleb Ershov", "Alexander Panchenko", "Elena Tutubalina", "Oleg Somov"], "title": "When Punctuation Matters: A Large-Scale Comparison of Prompt Robustness Methods for LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) are highly sensitive to subtle, non-semantic\nvariations in prompt phrasing and formatting. In this work, we present the\nfirst systematic evaluation of 5 methods for improving prompt robustness within\na unified experimental framework. We benchmark these techniques on 8 models\nfrom Llama, Qwen and Gemma families across 52 tasks from Natural Instructions\ndataset. Our evaluation covers robustness methods from both fine-tuned and\nin-context learning paradigms, and tests their generalization against multiple\ntypes of distribution shifts. Finally, we extend our analysis to GPT-4.1 and\nDeepSeek V3 to assess frontier models' current robustness to format\nperturbations. Our findings offer actionable insights into the relative\neffectiveness of these robustness methods, enabling practitioners to make\ninformed decisions when aiming for stable and reliable LLM performance in\nreal-world applications. Code:\nhttps://github.com/AIRI-Institute/when-punctuation-matters.", "AI": {"tldr": "The paper evaluates 5 methods to improve prompt robustness in LLMs, testing them on 8 models across 52 tasks, and extends analysis to GPT-4.1 and DeepSeek V3 for frontier model assessment.", "motivation": "LLMs are sensitive to non-semantic prompt variations, necessitating systematic evaluation of robustness methods.", "method": "Benchmarked 5 robustness techniques on 8 models (Llama, Qwen, Gemma) across 52 tasks, covering fine-tuned and in-context learning paradigms.", "result": "Provides insights into the effectiveness of robustness methods, tested against distribution shifts and frontier models.", "conclusion": "Offers actionable guidance for stable LLM performance in real-world applications."}}
{"id": "2508.11063", "pdf": "https://arxiv.org/pdf/2508.11063", "abs": "https://arxiv.org/abs/2508.11063", "authors": ["Lucas W. Remedios", "Chloe Choe", "Trent M. Schwartz", "Dingjie Su", "Gaurav Rudravaram", "Chenyu Gao", "Aravind R. Krishnan", "Adam M. Saunders", "Michael E. Kim", "Shunxing Bao", "Alvin C. Powers", "Bennett A. Landman", "John Virostko"], "title": "Data-Driven Abdominal Phenotypes of Type 2 Diabetes in Lean, Overweight, and Obese Cohorts", "categories": ["cs.CV"], "comment": null, "summary": "Purpose: Although elevated BMI is a well-known risk factor for type 2\ndiabetes, the disease's presence in some lean adults and absence in others with\nobesity suggests that detailed body composition may uncover abdominal\nphenotypes of type 2 diabetes. With AI, we can now extract detailed\nmeasurements of size, shape, and fat content from abdominal structures in 3D\nclinical imaging at scale. This creates an opportunity to empirically define\nbody composition signatures linked to type 2 diabetes risk and protection using\nlarge-scale clinical data. Approach: To uncover BMI-specific diabetic abdominal\npatterns from clinical CT, we applied our design four times: once on the full\ncohort (n = 1,728) and once on lean (n = 497), overweight (n = 611), and obese\n(n = 620) subgroups separately. Briefly, our experimental design transforms\nabdominal scans into collections of explainable measurements through\nsegmentation, classifies type 2 diabetes through a cross-validated random\nforest, measures how features contribute to model-estimated risk or protection\nthrough SHAP analysis, groups scans by shared model decision patterns\n(clustering from SHAP) and links back to anatomical differences\n(classification). Results: The random-forests achieved mean AUCs of 0.72-0.74.\nThere were shared type 2 diabetes signatures in each group; fatty skeletal\nmuscle, older age, greater visceral and subcutaneous fat, and a smaller or\nfat-laden pancreas. Univariate logistic regression confirmed the direction of\n14-18 of the top 20 predictors within each subgroup (p < 0.05). Conclusions:\nOur findings suggest that abdominal drivers of type 2 diabetes may be\nconsistent across weight classes.", "AI": {"tldr": "The study uses AI to analyze 3D clinical imaging for body composition signatures linked to type 2 diabetes, identifying consistent abdominal patterns across weight classes.", "motivation": "To uncover detailed body composition phenotypes of type 2 diabetes beyond BMI, leveraging large-scale clinical data and AI.", "method": "Applied a design four times on a cohort (n=1,728) and subgroups (lean, overweight, obese) using segmentation, random forest classification, SHAP analysis, and clustering.", "result": "Random forests achieved AUCs of 0.72-0.74, identifying shared diabetes signatures like fatty skeletal muscle, visceral fat, and pancreas changes.", "conclusion": "Abdominal drivers of type 2 diabetes are consistent across weight classes, suggesting BMI-independent risk factors."}}
{"id": "2508.11210", "pdf": "https://arxiv.org/pdf/2508.11210", "abs": "https://arxiv.org/abs/2508.11210", "authors": ["Minghui Sun", "Matthew M. Engelhard", "Benjamin A. Goldstein"], "title": "Borrowing From the Future: Enhancing Early Risk Assessment through Contrastive Learning", "categories": ["cs.LG", "stat.ML"], "comment": "accepted by Machine Learning for Healthcare 2025", "summary": "Risk assessments for a pediatric population are often conducted across\nmultiple stages. For example, clinicians may evaluate risks prenatally, at\nbirth, and during Well-Child visits. Although predictions made at later stages\ntypically achieve higher precision, it is clinically desirable to make reliable\nrisk assessments as early as possible. Therefore, this study focuses on\nimproving prediction performance in early-stage risk assessments. Our solution,\n\\textbf{Borrowing From the Future (BFF)}, is a contrastive multi-modal\nframework that treats each time window as a distinct modality. In BFF, a model\nis trained on all available data throughout the time while performing a risk\nassessment using up-to-date information. This contrastive framework allows the\nmodel to ``borrow'' informative signals from later stages (e.g., Well-Child\nvisits) to implicitly supervise the learning at earlier stages (e.g.,\nprenatal/birth stages). We validate BFF on two real-world pediatric outcome\nprediction tasks, demonstrating consistent improvements in early risk\nassessments. The code is available at https://github.com/scotsun/bff.", "AI": {"tldr": "The paper introduces Borrowing From the Future (BFF), a contrastive multi-modal framework to improve early-stage pediatric risk assessments by leveraging later-stage data.", "motivation": "Early-stage risk assessments in pediatrics are less precise but clinically desirable, prompting the need for improved prediction performance.", "method": "BFF treats each time window as a distinct modality, training on all available data while using contrastive learning to borrow signals from later stages for earlier supervision.", "result": "BFF shows consistent improvements in early risk assessments across two real-world pediatric outcome prediction tasks.", "conclusion": "BFF effectively enhances early-stage risk prediction by leveraging later-stage data, validated on real-world tasks."}}
{"id": "2508.11386", "pdf": "https://arxiv.org/pdf/2508.11386", "abs": "https://arxiv.org/abs/2508.11386", "authors": ["Ryan Sze-Yin Chan", "Federico Nanni", "Tomas Lazauskas", "Rosie Wood", "Penelope Yong", "Lionel Tarassenko", "Mark Girolami", "James Geddes", "Andrew Duncan"], "title": "Retrieval-augmented reasoning with lean language models", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "This technical report details a novel approach to combining reasoning and\nretrieval augmented generation (RAG) within a single, lean language model\narchitecture. While existing RAG systems typically rely on large-scale models\nand external APIs, our work addresses the increasing demand for performant and\nprivacy-preserving solutions deployable in resource-constrained or secure\nenvironments. Building on recent developments in test-time scaling and\nsmall-scale reasoning models, we develop a retrieval augmented conversational\nagent capable of interpreting complex, domain-specific queries using a\nlightweight backbone model. Our system integrates a dense retriever with\nfine-tuned Qwen2.5-Instruct models, using synthetic query generation and\nreasoning traces derived from frontier models (e.g., DeepSeek-R1) over a\ncurated corpus, in this case, the NHS A-to-Z condition pages. We explore the\nimpact of summarisation-based document compression, synthetic data design, and\nreasoning-aware fine-tuning on model performance. Evaluation against both\nnon-reasoning and general-purpose lean models demonstrates that our\ndomain-specific fine-tuning approach yields substantial gains in answer\naccuracy and consistency, approaching frontier-level performance while\nremaining feasible for local deployment. All implementation details and code\nare publicly released to support reproducibility and adaptation across domains.", "AI": {"tldr": "A novel approach combines reasoning and retrieval augmented generation (RAG) in a lightweight model, addressing privacy and performance needs for resource-constrained environments.", "motivation": "To meet the demand for performant, privacy-preserving solutions deployable in secure or resource-limited settings, avoiding reliance on large models or external APIs.", "method": "Integrates a dense retriever with fine-tuned Qwen2.5-Instruct models, using synthetic query generation and reasoning traces from frontier models over a curated corpus (NHS A-to-Z condition pages). Explores summarization-based compression, synthetic data design, and reasoning-aware fine-tuning.", "result": "Domain-specific fine-tuning yields significant accuracy and consistency gains, nearing frontier-level performance while enabling local deployment.", "conclusion": "The approach demonstrates feasibility for high-performance, privacy-preserving RAG systems in constrained environments, with publicly released code for reproducibility."}}
{"id": "2508.11106", "pdf": "https://arxiv.org/pdf/2508.11106", "abs": "https://arxiv.org/abs/2508.11106", "authors": ["Xinjie Gao", "Bi'an Du", "Wei Hu"], "title": "HierOctFusion: Multi-scale Octree-based 3D Shape Generation via Part-Whole-Hierarchy Message Passing", "categories": ["cs.CV"], "comment": null, "summary": "3D content generation remains a fundamental yet challenging task due to the\ninherent structural complexity of 3D data. While recent octree-based diffusion\nmodels offer a promising balance between efficiency and quality through\nhierarchical generation, they often overlook two key insights: 1) existing\nmethods typically model 3D objects as holistic entities, ignoring their\nsemantic part hierarchies and limiting generalization; and 2) holistic\nhigh-resolution modeling is computationally expensive, whereas real-world\nobjects are inherently sparse and hierarchical, making them well-suited for\nlayered generation. Motivated by these observations, we propose HierOctFusion,\na part-aware multi-scale octree diffusion model that enhances hierarchical\nfeature interaction for generating fine-grained and sparse object structures.\nFurthermore, we introduce a cross-attention conditioning mechanism that injects\npart-level information into the generation process, enabling semantic features\nto propagate effectively across hierarchical levels from parts to the whole.\nAdditionally, we construct a 3D dataset with part category annotations using a\npre-trained segmentation model to facilitate training and evaluation.\nExperiments demonstrate that HierOctFusion achieves superior shape quality and\nefficiency compared to prior methods.", "AI": {"tldr": "HierOctFusion is a part-aware multi-scale octree diffusion model for 3D content generation, improving efficiency and quality by leveraging semantic part hierarchies and hierarchical feature interaction.", "motivation": "Existing methods treat 3D objects holistically, ignoring semantic part hierarchies and facing computational inefficiency. HierOctFusion addresses these by modeling sparse, hierarchical structures and incorporating part-level information.", "method": "HierOctFusion uses a part-aware multi-scale octree diffusion model with cross-attention conditioning to inject part-level information and enhance hierarchical feature interaction. A 3D dataset with part annotations is also constructed.", "result": "HierOctFusion outperforms prior methods in shape quality and efficiency, demonstrating the benefits of part-aware hierarchical generation.", "conclusion": "The proposed method successfully integrates semantic part hierarchies and hierarchical generation, offering a more efficient and high-quality solution for 3D content generation."}}
{"id": "2508.11214", "pdf": "https://arxiv.org/pdf/2508.11214", "abs": "https://arxiv.org/abs/2508.11214", "authors": ["Atticus Geiger", "Jacqueline Harding", "Thomas Icard"], "title": "How Causal Abstraction Underpins Computational Explanation", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Explanations of cognitive behavior often appeal to computations over\nrepresentations. What does it take for a system to implement a given\ncomputation over suitable representational vehicles within that system? We\nargue that the language of causality -- and specifically the theory of causal\nabstraction -- provides a fruitful lens on this topic. Drawing on current\ndiscussions in deep learning with artificial neural networks, we illustrate how\nclassical themes in the philosophy of computation and cognition resurface in\ncontemporary machine learning. We offer an account of computational\nimplementation grounded in causal abstraction, and examine the role for\nrepresentation in the resulting picture. We argue that these issues are most\nprofitably explored in connection with generalization and prediction.", "AI": {"tldr": "The paper explores how causal abstraction theory can explain computational implementation in cognitive behavior, linking classical philosophy to modern machine learning.", "motivation": "To understand what it takes for a system to implement computations over representations, using causal abstraction as a framework.", "method": "Uses causal abstraction theory and draws parallels with deep learning and neural networks.", "result": "Proposes an account of computational implementation based on causal abstraction and examines representation's role.", "conclusion": "Issues of computation and representation are best explored through generalization and prediction."}}
{"id": "2508.11388", "pdf": "https://arxiv.org/pdf/2508.11388", "abs": "https://arxiv.org/abs/2508.11388", "authors": ["Marc Brinner", "Sina Zarriess"], "title": "Model Interpretability and Rationale Extraction by Input Mask Optimization", "categories": ["cs.CL", "cs.CV", "cs.LG"], "comment": null, "summary": "Concurrent to the rapid progress in the development of neural-network based\nmodels in areas like natural language processing and computer vision, the need\nfor creating explanations for the predictions of these black-box models has\nrisen steadily. We propose a new method to generate extractive explanations for\npredictions made by neural networks, that is based on masking parts of the\ninput which the model does not consider to be indicative of the respective\nclass. The masking is done using gradient-based optimization combined with a\nnew regularization scheme that enforces sufficiency, comprehensiveness and\ncompactness of the generated explanation, three properties that are known to be\ndesirable from the related field of rationale extraction in natural language\nprocessing. In this way, we bridge the gap between model interpretability and\nrationale extraction, thereby proving that the latter of which can be performed\nwithout training a specialized model, only on the basis of a trained\nclassifier. We further apply the same method to image inputs and obtain high\nquality explanations for image classifications, which indicates that the\nconditions proposed for rationale extraction in natural language processing are\nmore broadly applicable to different input types.", "AI": {"tldr": "A new gradient-based method generates extractive explanations for neural network predictions by masking non-indicative input parts, ensuring sufficiency, comprehensiveness, and compactness.", "motivation": "The need for explainable predictions in black-box neural networks has grown alongside their advancements in NLP and computer vision.", "method": "Uses gradient-based optimization and a novel regularization scheme to mask irrelevant input parts, ensuring desirable explanation properties.", "result": "Demonstrates effective rationale extraction without specialized models, applicable to both text and image inputs.", "conclusion": "Bridges interpretability and rationale extraction, showing broader applicability of NLP rationale conditions to other inputs."}}
{"id": "2508.11115", "pdf": "https://arxiv.org/pdf/2508.11115", "abs": "https://arxiv.org/abs/2508.11115", "authors": ["Haotang Li", "Zhenyu Qi", "Sen He", "Kebin Peng", "Sheng Tan", "Yili Ren", "Tomas Cerny", "Jiyue Zhao", "Zi Wang"], "title": "UWB-PostureGuard: A Privacy-Preserving RF Sensing System for Continuous Ergonomic Sitting Posture Monitoring", "categories": ["cs.CV", "cs.HC", "eess.SP"], "comment": null, "summary": "Improper sitting posture during prolonged computer use has become a\nsignificant public health concern. Traditional posture monitoring solutions\nface substantial barriers, including privacy concerns with camera-based systems\nand user discomfort with wearable sensors. This paper presents\nUWB-PostureGuard, a privacy-preserving ultra-wideband (UWB) sensing system that\nadvances mobile technologies for preventive health management through\ncontinuous, contactless monitoring of ergonomic sitting posture. Our system\nleverages commercial UWB devices, utilizing comprehensive feature engineering\nto extract multiple ergonomic sitting posture features. We develop PoseGBDT to\neffectively capture temporal dependencies in posture patterns, addressing\nlimitations of traditional frame-wise classification approaches. Extensive\nreal-world evaluation across 10 participants and 19 distinct postures\ndemonstrates exceptional performance, achieving 99.11% accuracy while\nmaintaining robustness against environmental variables such as clothing\nthickness, additional devices, and furniture configurations. Our system\nprovides a scalable, privacy-preserving mobile health solution on existing\nplatforms for proactive ergonomic management, improving quality of life at low\ncosts.", "AI": {"tldr": "UWB-PostureGuard is a privacy-preserving UWB sensing system for monitoring ergonomic sitting posture, achieving 99.11% accuracy without privacy or comfort issues.", "motivation": "Addressing privacy and comfort concerns in traditional posture monitoring solutions like cameras and wearables.", "method": "Uses commercial UWB devices and PoseGBDT for temporal posture pattern analysis.", "result": "Achieves 99.11% accuracy in real-world evaluations across diverse conditions.", "conclusion": "Offers a scalable, low-cost, privacy-preserving solution for proactive ergonomic management."}}
{"id": "2508.11215", "pdf": "https://arxiv.org/pdf/2508.11215", "abs": "https://arxiv.org/abs/2508.11215", "authors": ["Zicheng Guo", "Shuqi Wu", "Meixing Zhu", "He Guandi"], "title": "Air Quality PM2.5 Index Prediction Model Based on CNN-LSTM", "categories": ["cs.LG"], "comment": null, "summary": "With the intensification of global climate change, accurate prediction of air\nquality indicators, especially PM2.5 concentration, has become increasingly\nimportant in fields such as environmental protection, public health, and urban\nmanagement. To address this, we propose an air quality PM2.5 index prediction\nmodel based on a hybrid CNN-LSTM architecture. The model effectively combines\nConvolutional Neural Networks (CNN) for local spatial feature extraction and\nLong Short-Term Memory (LSTM) networks for modeling temporal dependencies in\ntime series data. Using a multivariate dataset collected from an industrial\narea in Beijing between 2010 and 2015 -- which includes hourly records of PM2.5\nconcentration, temperature, dew point, pressure, wind direction, wind speed,\nand precipitation -- the model predicts the average PM2.5 concentration over\n6-hour intervals. Experimental results show that the model achieves a root mean\nsquare error (RMSE) of 5.236, outperforming traditional time series models in\nboth accuracy and generalization. This demonstrates its strong potential in\nreal-world applications such as air pollution early warning systems. However,\ndue to the complexity of multivariate inputs, the model demands high\ncomputational resources, and its ability to handle diverse atmospheric factors\nstill requires optimization. Future work will focus on enhancing scalability\nand expanding support for more complex multivariate weather prediction tasks.", "AI": {"tldr": "A hybrid CNN-LSTM model is proposed for PM2.5 prediction, outperforming traditional methods with an RMSE of 5.236, though it requires high computational resources.", "motivation": "Accurate PM2.5 prediction is crucial for environmental protection, public health, and urban management amid global climate change.", "method": "The model combines CNN for spatial feature extraction and LSTM for temporal dependencies, using multivariate data from Beijing (2010-2015).", "result": "Achieves an RMSE of 5.236, outperforming traditional time series models in accuracy and generalization.", "conclusion": "The model shows promise for real-world applications but needs optimization for computational efficiency and handling diverse atmospheric factors."}}
{"id": "2508.11393", "pdf": "https://arxiv.org/pdf/2508.11393", "abs": "https://arxiv.org/abs/2508.11393", "authors": ["Marc Brinner", "Sina Zarrie\u00df"], "title": "Rationalizing Transformer Predictions via End-To-End Differentiable Self-Training", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "We propose an end-to-end differentiable training paradigm for stable training\nof a rationalized transformer classifier. Our approach results in a single\nmodel that simultaneously classifies a sample and scores input tokens based on\ntheir relevance to the classification. To this end, we build on the widely-used\nthree-player-game for training rationalized models, which typically relies on\ntraining a rationale selector, a classifier and a complement classifier. We\nsimplify this approach by making a single model fulfill all three roles,\nleading to a more efficient training paradigm that is not susceptible to the\ncommon training instabilities that plague existing approaches. Further, we\nextend this paradigm to produce class-wise rationales while incorporating\nrecent advances in parameterizing and regularizing the resulting rationales,\nthus leading to substantially improved and state-of-the-art alignment with\nhuman annotations without any explicit supervision.", "AI": {"tldr": "An end-to-end differentiable training paradigm for stable training of a rationalized transformer classifier, simplifying the three-player-game approach into a single model, improving efficiency and alignment with human annotations.", "motivation": "To address training instabilities in existing rationalized models and improve alignment with human annotations without explicit supervision.", "method": "Proposes a single model fulfilling the roles of rationale selector, classifier, and complement classifier, extending it for class-wise rationales with parameterization and regularization.", "result": "Achieves state-of-the-art alignment with human annotations and more efficient training.", "conclusion": "The simplified approach outperforms existing methods in stability and alignment, offering a robust solution for rationalized classification."}}
{"id": "2508.11134", "pdf": "https://arxiv.org/pdf/2508.11134", "abs": "https://arxiv.org/abs/2508.11134", "authors": ["Bing Liu", "Le Wang", "Hao Liu", "Mingming Liu"], "title": "Residual-based Efficient Bidirectional Diffusion Model for Image Dehazing and Haze Generation", "categories": ["cs.CV"], "comment": "7 pages, 5 figures, 2025 ICME Accepted", "summary": "Current deep dehazing methods only focus on removing haze from hazy images,\nlacking the capability to translate between hazy and haze-free images. To\naddress this issue, we propose a residual-based efficient bidirectional\ndiffusion model (RBDM) that can model the conditional distributions for both\ndehazing and haze generation. Firstly, we devise dual Markov chains that can\neffectively shift the residuals and facilitate bidirectional smooth transitions\nbetween them. Secondly, the RBDM perturbs the hazy and haze-free images at\nindividual timesteps and predicts the noise in the perturbed data to\nsimultaneously learn the conditional distributions. Finally, to enhance\nperformance on relatively small datasets and reduce computational costs, our\nmethod introduces a unified score function learned on image patches instead of\nentire images. Our RBDM successfully implements size-agnostic bidirectional\ntransitions between haze-free and hazy images with only 15 sampling steps.\nExtensive experiments demonstrate that the proposed method achieves superior or\nat least comparable performance to state-of-the-art methods on both synthetic\nand real-world datasets.", "AI": {"tldr": "Proposes a residual-based bidirectional diffusion model (RBDM) for translating between hazy and haze-free images, achieving efficient bidirectional transitions with 15 sampling steps.", "motivation": "Existing deep dehazing methods lack bidirectional translation capability between hazy and haze-free images.", "method": "Uses dual Markov chains for residual shifting, perturbing images at timesteps to predict noise, and a unified score function on patches for efficiency.", "result": "Achieves superior/comparable performance to state-of-the-art methods on synthetic and real-world datasets.", "conclusion": "RBDM enables size-agnostic bidirectional transitions efficiently, addressing limitations of current dehazing methods."}}
{"id": "2508.11235", "pdf": "https://arxiv.org/pdf/2508.11235", "abs": "https://arxiv.org/abs/2508.11235", "authors": ["William Alemanni", "Arianna Burzacchi", "Davide Colombi", "Elena Giarratano"], "title": "Enhancing Interactive Voting-Based Map Matching: Improving Efficiency and Robustness for Heterogeneous GPS Trajectories", "categories": ["cs.LG"], "comment": null, "summary": "This paper presents an enhanced version of the Interactive Voting-Based Map\nMatching algorithm, designed to efficiently process trajectories with varying\nsampling rates. The main aim is to reconstruct GPS trajectories with high\naccuracy, independent of input data quality. Building upon the original\nalgorithm, developed exclusively for aligning GPS signals to road networks, we\nextend its capabilities by integrating trajectory imputation. Our improvements\nalso include the implementation of a distance-bounded interactive voting\nstrategy to reduce computational complexity, as well as modifications to\naddress missing data in the road network. Furthermore, we incorporate a\ncustom-built asset derived from OpenStreetMap, enabling this approach to be\nsmoothly applied in any geographic region covered by OpenStreetMap's road\nnetwork. These advancements preserve the core strengths of the original\nalgorithm while significantly extending its applicability to diverse real-world\nscenarios.", "AI": {"tldr": "An enhanced Interactive Voting-Based Map Matching algorithm improves trajectory reconstruction accuracy for varying GPS sampling rates, reduces computational complexity, and handles missing road network data.", "motivation": "To reconstruct GPS trajectories accurately regardless of input data quality and extend the original algorithm's applicability to diverse scenarios.", "method": "Integrates trajectory imputation, a distance-bounded interactive voting strategy, and custom OpenStreetMap assets to enhance the original algorithm.", "result": "Improved accuracy and computational efficiency, with broader applicability to any OpenStreetMap-covered region.", "conclusion": "The enhanced algorithm retains the original's strengths while addressing real-world challenges like data gaps and varying sampling rates."}}
{"id": "2508.11414", "pdf": "https://arxiv.org/pdf/2508.11414", "abs": "https://arxiv.org/abs/2508.11414", "authors": ["Shangrui Nie", "Florian Mai", "David Kacz\u00e9r", "Charles Welch", "Zhixue Zhao", "Lucie Flek"], "title": "Survey-to-Behavior: Downstream Alignment of Human Values in LLMs via Survey Questions", "categories": ["cs.CL"], "comment": "7 pages 1 figure", "summary": "Large language models implicitly encode preferences over human values, yet\nsteering them often requires large training data. In this work, we investigate\na simple approach: Can we reliably modify a model's value system in downstream\nbehavior by training it to answer value survey questions accordingly? We first\nconstruct value profiles of several open-source LLMs by asking them to rate a\nseries of value-related descriptions spanning 20 distinct human values, which\nwe use as a baseline for subsequent experiments. We then investigate whether\nthe value system of a model can be governed by fine-tuning on the value\nsurveys. We evaluate the effect of finetuning on the model's behavior in two\nways; first, we assess how answers change on in-domain, held-out survey\nquestions. Second, we evaluate whether the model's behavior changes in\nout-of-domain settings (situational scenarios). To this end, we construct a\ncontextualized moral judgment dataset based on Reddit posts and evaluate\nchanges in the model's behavior in text-based adventure games. We demonstrate\nthat our simple approach can not only change the model's answers to in-domain\nsurvey questions, but also produces substantial shifts (value alignment) in\nimplicit downstream task behavior.", "AI": {"tldr": "The paper explores whether fine-tuning LLMs on value survey questions can reliably modify their value systems, showing success in both in-domain and out-of-domain tasks.", "motivation": "To investigate if LLMs' implicit value systems can be steered without large training data by using value surveys.", "method": "Construct value profiles of LLMs, fine-tune them on value surveys, and evaluate changes in in-domain (survey answers) and out-of-domain (moral judgments, text-based games) behavior.", "result": "Fine-tuning successfully alters LLMs' answers to value surveys and induces value alignment in downstream tasks.", "conclusion": "A simple fine-tuning approach can effectively modify LLMs' value systems and influence their behavior in diverse contexts."}}
{"id": "2508.10991", "pdf": "https://arxiv.org/pdf/2508.10991", "abs": "https://arxiv.org/abs/2508.10991", "authors": ["Wenpeng Xing", "Zhonghao Qi", "Yupeng Qin", "Yilin Li", "Caini Chang", "Jiahui Yu", "Changting Lin", "Zhenzhen Xie", "Meng Han"], "title": "MCP-Guard: A Defense Framework for Model Context Protocol Integrity in Large Language Model Applications", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "The integration of Large Language Models (LLMs) with external tools via\nprotocols such as the Model Context Protocol (MCP) introduces critical security\nvulnerabilities, including prompt injection, data exfiltration, and other\nthreats. To counter these challenges, we propose MCP-Guard, a robust, layered\ndefense architecture designed for LLM--tool interactions. MCP-Guard employs a\nthree-stage detection pipeline that balances efficiency with accuracy: it\nprogresses from lightweight static scanning for overt threats and a deep neural\ndetector for semantic attacks, to our fine-tuned E5-based model achieves\n(96.01) accuracy in identifying adversarial prompts. Finally, a lightweight LLM\narbitrator synthesizes these signals to deliver the final decision while\nminimizing false positives. To facilitate rigorous training and evaluation, we\nalso introduce MCP-AttackBench, a comprehensive benchmark of over 70,000\nsamples. Sourced from public datasets and augmented by GPT-4, MCP-AttackBench\nsimulates diverse, real-world attack vectors in the MCP format, providing a\nfoundation for future research into securing LLM-tool ecosystems.", "AI": {"tldr": "MCP-Guard is a layered defense architecture for securing LLM-tool interactions against vulnerabilities like prompt injection, using a three-stage detection pipeline and achieving high accuracy.", "motivation": "Address security vulnerabilities (e.g., prompt injection, data exfiltration) in LLM-tool interactions via protocols like MCP.", "method": "Proposes MCP-Guard: a three-stage pipeline (static scanning, deep neural detector, LLM arbitrator) and introduces MCP-AttackBench for training/evaluation.", "result": "MCP-Guard achieves 96.01% accuracy in identifying adversarial prompts with minimal false positives.", "conclusion": "MCP-Guard and MCP-AttackBench provide a robust solution and benchmark for securing LLM-tool ecosystems."}}
{"id": "2508.11141", "pdf": "https://arxiv.org/pdf/2508.11141", "abs": "https://arxiv.org/abs/2508.11141", "authors": ["Bin Ma", "Yifei Zhang", "Yongjin Xian", "Qi Li", "Linna Zhou", "Gongxun Miao"], "title": "A Cross-Modal Rumor Detection Scheme via Contrastive Learning by Exploring Text and Image internal Correlations", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Existing rumor detection methods often neglect the content within images as\nwell as the inherent relationships between contexts and images across different\nvisual scales, thereby resulting in the loss of critical information pertinent\nto rumor identification. To address these issues, this paper presents a novel\ncross-modal rumor detection scheme based on contrastive learning, namely the\nMulti-scale Image and Context Correlation exploration algorithm (MICC).\nSpecifically, we design an SCLIP encoder to generate unified semantic\nembeddings for text and multi-scale image patches through contrastive\npretraining, enabling their relevance to be measured via dot-product\nsimilarity. Building upon this, a Cross-Modal Multi-Scale Alignment module is\nintroduced to identify image regions most relevant to the textual semantics,\nguided by mutual information maximization and the information bottleneck\nprinciple, through a Top-K selection strategy based on a cross-modal relevance\nmatrix constructed between the text and multi-scale image patches. Moreover, a\nscale-aware fusion network is designed to integrate the highly correlated\nmulti-scale image features with global text features by assigning adaptive\nweights to image regions based on their semantic importance and cross-modal\nrelevance. The proposed methodology has been extensively evaluated on two\nreal-world datasets. The experimental results demonstrate that it achieves a\nsubstantial performance improvement over existing state-of-the-art approaches\nin rumor detection, highlighting its effectiveness and potential for practical\napplications.", "AI": {"tldr": "The paper introduces MICC, a cross-modal rumor detection method using contrastive learning to align multi-scale image and text features, outperforming existing methods.", "motivation": "Existing methods ignore image content and context-image relationships, losing critical rumor detection information.", "method": "Uses SCLIP encoder for unified embeddings, Cross-Modal Multi-Scale Alignment for relevant image regions, and scale-aware fusion for feature integration.", "result": "Substantial performance improvement on real-world datasets compared to state-of-the-art methods.", "conclusion": "MICC is effective and practical for rumor detection by leveraging multi-scale image-text alignment."}}
{"id": "2508.11249", "pdf": "https://arxiv.org/pdf/2508.11249", "abs": "https://arxiv.org/abs/2508.11249", "authors": ["Asela Hevapathige", "Asiri Wijesinghe", "Ahad N. Zehmakan"], "title": "Graph Neural Diffusion via Generalized Opinion Dynamics", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "There has been a growing interest in developing diffusion-based Graph Neural\nNetworks (GNNs), building on the connections between message passing mechanisms\nin GNNs and physical diffusion processes. However, existing methods suffer from\nthree critical limitations: (1) they rely on homogeneous diffusion with static\ndynamics, limiting adaptability to diverse graph structures; (2) their depth is\nconstrained by computational overhead and diminishing interpretability; and (3)\ntheoretical understanding of their convergence behavior remains limited. To\naddress these challenges, we propose GODNF, a Generalized Opinion Dynamics\nNeural Framework, which unifies multiple opinion dynamics models into a\nprincipled, trainable diffusion mechanism. Our framework captures heterogeneous\ndiffusion patterns and temporal dynamics via node-specific behavior modeling\nand dynamic neighborhood influence, while ensuring efficient and interpretable\nmessage propagation even at deep layers. We provide a rigorous theoretical\nanalysis demonstrating GODNF's ability to model diverse convergence\nconfigurations. Extensive empirical evaluations of node classification and\ninfluence estimation tasks confirm GODNF's superiority over state-of-the-art\nGNNs.", "AI": {"tldr": "GODNF, a Generalized Opinion Dynamics Neural Framework, addresses limitations in diffusion-based GNNs by unifying opinion dynamics models for adaptable, efficient, and interpretable graph learning.", "motivation": "Existing diffusion-based GNNs have limitations in adaptability, depth, and theoretical understanding, prompting the need for a more flexible and theoretically grounded framework.", "method": "GODNF integrates multiple opinion dynamics models into a trainable diffusion mechanism, modeling heterogeneous diffusion and dynamic neighborhood influence.", "result": "Theoretical analysis shows GODNF's convergence versatility, and empirical tests confirm its superiority in node classification and influence estimation.", "conclusion": "GODNF advances diffusion-based GNNs by addressing key limitations and demonstrating robust performance in practical tasks."}}
{"id": "2508.11429", "pdf": "https://arxiv.org/pdf/2508.11429", "abs": "https://arxiv.org/abs/2508.11429", "authors": ["Shivam Dubey"], "title": "HumorPlanSearch: Structured Planning and HuCoT for Contextual AI Humor", "categories": ["cs.CL"], "comment": null, "summary": "Automated humor generation with Large Language Models (LLMs) often yields\njokes that feel generic, repetitive, or tone-deaf because humor is deeply\nsituated and hinges on the listener's cultural background, mindset, and\nimmediate context. We introduce HumorPlanSearch, a modular pipeline that\nexplicitly models context through: (1) Plan-Search for diverse, topic-tailored\nstrategies; (2) Humor Chain-of-Thought (HuCoT) templates capturing cultural and\nstylistic reasoning; (3) a Knowledge Graph to retrieve and adapt\nhigh-performing historical strategies; (4) novelty filtering via semantic\nembeddings; and (5) an iterative judge-driven revision loop. To evaluate\ncontext sensitivity and comedic quality, we propose the Humor Generation Score\n(HGS), which fuses direct ratings, multi-persona feedback, pairwise win-rates,\nand topic relevance. In experiments across nine topics with feedback from 13\nhuman judges, our full pipeline (KG + Revision) boosts mean HGS by 15.4 percent\n(p < 0.05) over a strong baseline. By foregrounding context at every stage from\nstrategy planning to multi-signal evaluation, HumorPlanSearch advances\nAI-driven humor toward more coherent, adaptive, and culturally attuned comedy.", "AI": {"tldr": "HumorPlanSearch improves AI-generated humor by modeling context through modular components like Plan-Search, Humor Chain-of-Thought, and iterative revisions, boosting comedic quality by 15.4%.", "motivation": "Current LLM-generated humor often lacks context sensitivity, leading to generic or tone-deaf jokes. Addressing this gap requires explicit modeling of cultural, stylistic, and situational factors.", "method": "The pipeline includes Plan-Search for topic-tailored strategies, Humor Chain-of-Thought templates, a Knowledge Graph for historical strategies, novelty filtering, and judge-driven revisions.", "result": "Experiments show a 15.4% improvement in Humor Generation Score (HGS) over baselines, validated by human judges across nine topics.", "conclusion": "HumorPlanSearch enhances AI humor by integrating context at every stage, achieving more coherent and culturally attuned comedy."}}
{"id": "2508.11153", "pdf": "https://arxiv.org/pdf/2508.11153", "abs": "https://arxiv.org/abs/2508.11153", "authors": ["Maoquan Zhang", "Bisser Raytchev", "Xiujuan Sun"], "title": "LEARN: A Story-Driven Layout-to-Image Generation Framework for STEM Instruction", "categories": ["cs.CV"], "comment": "The International Conference on Neural Information Processing\n  (ICONIP) 2025", "summary": "LEARN is a layout-aware diffusion framework designed to generate\npedagogically aligned illustrations for STEM education. It leverages a curated\nBookCover dataset that provides narrative layouts and structured visual cues,\nenabling the model to depict abstract and sequential scientific concepts with\nstrong semantic alignment. Through layout-conditioned generation, contrastive\nvisual-semantic training, and prompt modulation, LEARN produces coherent visual\nsequences that support mid-to-high-level reasoning in line with Bloom's\ntaxonomy while reducing extraneous cognitive load as emphasized by Cognitive\nLoad Theory. By fostering spatially organized and story-driven narratives, the\nframework counters fragmented attention often induced by short-form media and\npromotes sustained conceptual focus. Beyond static diagrams, LEARN demonstrates\npotential for integration with multimodal systems and curriculum-linked\nknowledge graphs to create adaptive, exploratory educational content. As the\nfirst generative approach to unify layout-based storytelling, semantic\nstructure learning, and cognitive scaffolding, LEARN represents a novel\ndirection for generative AI in education. The code and dataset will be released\nto facilitate future research and practical deployment.", "AI": {"tldr": "LEARN is a layout-aware diffusion framework for generating pedagogically aligned STEM illustrations, leveraging structured visual cues and narrative layouts to support conceptual learning.", "motivation": "To address the challenge of fragmented attention in STEM education and provide coherent visual sequences that align with educational theories like Bloom's taxonomy and Cognitive Load Theory.", "method": "Uses layout-conditioned generation, contrastive visual-semantic training, and prompt modulation to create structured and semantically aligned illustrations.", "result": "Produces coherent visual sequences that support mid-to-high-level reasoning and reduces cognitive load, with potential for integration into multimodal educational systems.", "conclusion": "LEARN introduces a novel generative AI approach for education, unifying layout-based storytelling, semantic structure learning, and cognitive scaffolding, with plans for open-source release."}}
{"id": "2508.11258", "pdf": "https://arxiv.org/pdf/2508.11258", "abs": "https://arxiv.org/abs/2508.11258", "authors": ["Ruicheng Xian", "Yuxuan Wan", "Han Zhao"], "title": "Group Fairness Meets the Black Box: Enabling Fair Algorithms on Closed LLMs via Post-Processing", "categories": ["cs.LG", "cs.CL", "cs.CY"], "comment": null, "summary": "Instruction fine-tuned large language models (LLMs) enable a simple zero-shot\nor few-shot prompting paradigm, also known as in-context learning, for building\nprediction models. This convenience, combined with continued advances in LLM\ncapability, has the potential to drive their adoption across a broad range of\ndomains, including high-stakes applications where group fairness -- preventing\ndisparate impacts across demographic groups -- is essential. The majority of\nexisting approaches to enforcing group fairness on LLM-based classifiers rely\non traditional fair algorithms applied via model fine-tuning or head-tuning on\nfinal-layer embeddings, but they are no longer applicable to closed-weight LLMs\nunder the in-context learning setting, which include some of the most capable\ncommercial models today, such as GPT-4, Gemini, and Claude. In this paper, we\npropose a framework for deriving fair classifiers from closed-weight LLMs via\nprompting: the LLM is treated as a feature extractor, and features are elicited\nfrom its probabilistic predictions (e.g., token log probabilities) using\nprompts strategically designed for the specified fairness criterion to obtain\nsufficient statistics for fair classification; a fair algorithm is then applied\nto these features to train a lightweight fair classifier in a post-hoc manner.\nExperiments on five datasets, including three tabular ones, demonstrate strong\naccuracy-fairness tradeoffs for the classifiers derived by our framework from\nboth open-weight and closed-weight LLMs; in particular, our framework is\ndata-efficient and outperforms fair classifiers trained on LLM embeddings\n(i.e., head-tuning) or from scratch on raw tabular features.", "AI": {"tldr": "A framework for deriving fair classifiers from closed-weight LLMs via strategic prompting, ensuring fairness without fine-tuning.", "motivation": "Addressing group fairness in high-stakes applications using LLMs, especially closed-weight models like GPT-4, where traditional fairness methods fail.", "method": "Treats LLMs as feature extractors, uses probabilistic predictions from prompts for fair classification, and applies a lightweight post-hoc fair algorithm.", "result": "Outperforms traditional methods on five datasets, showing strong accuracy-fairness tradeoffs, even with limited data.", "conclusion": "The proposed framework is effective for fair classification with LLMs, especially in data-efficient scenarios."}}
{"id": "2508.11434", "pdf": "https://arxiv.org/pdf/2508.11434", "abs": "https://arxiv.org/abs/2508.11434", "authors": ["Aditi Dutta", "Susan Banducci"], "title": "Online Anti-sexist Speech: Identifying Resistance to Gender Bias in Political Discourse", "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Anti-sexist speech, i.e., public expressions that challenge or resist\ngendered abuse and sexism, plays a vital role in shaping democratic debate\nonline. Yet automated content moderation systems, increasingly powered by large\nlanguage models (LLMs), may struggle to distinguish such resistance from the\nsexism it opposes. This study examines how five LLMs classify sexist,\nanti-sexist, and neutral political tweets from the UK, focusing on\nhigh-salience trigger events involving female Members of Parliament in the year\n2022. Our analysis show that models frequently misclassify anti-sexist speech\nas harmful, particularly during politically charged events where rhetorical\nstyles of harm and resistance converge. These errors risk silencing those who\nchallenge sexism, with disproportionate consequences for marginalised voices.\nWe argue that moderation design must move beyond binary harmful/not-harmful\nschemas, integrate human-in-the-loop review during sensitive events, and\nexplicitly include counter-speech in training data. By linking feminist\nscholarship, event-based analysis, and model evaluation, this work highlights\nthe sociotechnical challenges of safeguarding resistance speech in digital\npolitical spaces.", "AI": {"tldr": "The study examines how LLMs misclassify anti-sexist speech as harmful, especially during politically charged events, risking the silencing of marginalized voices. It calls for moderation improvements.", "motivation": "To address the challenge of automated content moderation systems misclassifying anti-sexist speech as harmful, which risks silencing resistance to sexism.", "method": "Analyzed five LLMs classifying sexist, anti-sexist, and neutral political tweets from the UK in 2022, focusing on high-salience events involving female MPs.", "result": "Models frequently misclassified anti-sexist speech as harmful, especially during politically charged events.", "conclusion": "Moderation systems should move beyond binary classifications, integrate human review, and include counter-speech in training data to safeguard resistance speech."}}
{"id": "2508.11165", "pdf": "https://arxiv.org/pdf/2508.11165", "abs": "https://arxiv.org/abs/2508.11165", "authors": ["Bing Liu", "Le Wang", "Mingming Liu", "Hao Liu", "Rui Yao", "Yong Zhou", "Peng Liu", "Tongqiang Xia"], "title": "Semi-supervised Image Dehazing via Expectation-Maximization and Bidirectional Brownian Bridge Diffusion Models", "categories": ["cs.CV"], "comment": "10 pages, 4 figures", "summary": "Existing dehazing methods deal with real-world haze images with difficulty,\nespecially scenes with thick haze. One of the main reasons is the lack of\nreal-world paired data and robust priors. To avoid the costly collection of\npaired hazy and clear images, we propose an efficient semi-supervised image\ndehazing method via Expectation-Maximization and Bidirectional Brownian Bridge\nDiffusion Models (EM-B3DM) with a two-stage learning scheme. In the first\nstage, we employ the EM algorithm to decouple the joint distribution of paired\nhazy and clear images into two conditional distributions, which are then\nmodeled using a unified Brownian Bridge diffusion model to directly capture the\nstructural and content-related correlations between hazy and clear images. In\nthe second stage, we leverage the pre-trained model and large-scale unpaired\nhazy and clear images to further improve the performance of image dehazing.\nAdditionally, we introduce a detail-enhanced Residual Difference Convolution\nblock (RDC) to capture gradient-level information, significantly enhancing the\nmodel's representation capability. Extensive experiments demonstrate that our\nEM-B3DM achieves superior or at least comparable performance to\nstate-of-the-art methods on both synthetic and real-world datasets.", "AI": {"tldr": "Proposes EM-B3DM, a semi-supervised image dehazing method using Expectation-Maximization and Bidirectional Brownian Bridge Diffusion Models, outperforming state-of-the-art methods.", "motivation": "Existing dehazing methods struggle with real-world thick haze due to lack of paired data and robust priors.", "method": "Uses a two-stage learning scheme: EM algorithm to decouple distributions and a Brownian Bridge diffusion model, followed by leveraging unpaired data. Introduces RDC for gradient-level detail enhancement.", "result": "Achieves superior or comparable performance on synthetic and real-world datasets.", "conclusion": "EM-B3DM is an efficient and effective solution for image dehazing, especially in challenging real-world scenarios."}}
{"id": "2508.11279", "pdf": "https://arxiv.org/pdf/2508.11279", "abs": "https://arxiv.org/abs/2508.11279", "authors": ["Jihang Wang", "Dongcheng Zhao", "Ruolin Chen", "Qian Zhang", "Yi Zeng"], "title": "Boosting the Robustness-Accuracy Trade-off of SNNs by Robust Temporal Self-Ensemble", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Spiking Neural Networks (SNNs) offer a promising direction for\nenergy-efficient and brain-inspired computing, yet their vulnerability to\nadversarial perturbations remains poorly understood. In this work, we revisit\nthe adversarial robustness of SNNs through the lens of temporal ensembling,\ntreating the network as a collection of evolving sub-networks across discrete\ntimesteps. This formulation uncovers two critical but underexplored\nchallenges-the fragility of individual temporal sub-networks and the tendency\nfor adversarial vulnerabilities to transfer across time. To overcome these\nlimitations, we propose Robust Temporal self-Ensemble (RTE), a training\nframework that improves the robustness of each sub-network while reducing the\ntemporal transferability of adversarial perturbations. RTE integrates both\nobjectives into a unified loss and employs a stochastic sampling strategy for\nefficient optimization. Extensive experiments across multiple benchmarks\ndemonstrate that RTE consistently outperforms existing training methods in\nrobust-accuracy trade-off. Additional analyses reveal that RTE reshapes the\ninternal robustness landscape of SNNs, leading to more resilient and temporally\ndiversified decision boundaries. Our study highlights the importance of\ntemporal structure in adversarial learning and offers a principled foundation\nfor building robust spiking models.", "AI": {"tldr": "The paper proposes Robust Temporal self-Ensemble (RTE) to enhance the adversarial robustness of Spiking Neural Networks (SNNs) by addressing temporal sub-network fragility and perturbation transferability.", "motivation": "SNNs are energy-efficient but vulnerable to adversarial attacks, with temporal dynamics and sub-network robustness being understudied.", "method": "RTE combines robustness training for sub-networks and reduces temporal transferability via a unified loss and stochastic sampling.", "result": "RTE outperforms existing methods in robust-accuracy trade-off and reshapes SNNs' internal robustness landscape.", "conclusion": "The study emphasizes temporal structure's role in adversarial learning and provides a foundation for robust SNNs."}}
{"id": "2508.11442", "pdf": "https://arxiv.org/pdf/2508.11442", "abs": "https://arxiv.org/abs/2508.11442", "authors": ["Bowen Zhang", "Zixin Song", "Chunquan Chen", "Qian-Wen Zhang", "Di Yin", "Xing Sun"], "title": "CoDiEmb: A Collaborative yet Distinct Framework for Unified Representation Learning in Information Retrieval and Semantic Textual Similarity", "categories": ["cs.CL"], "comment": null, "summary": "Learning unified text embeddings that excel across diverse downstream tasks\nis a central goal in representation learning, yet negative transfer remains a\npersistent obstacle. This challenge is particularly pronounced when jointly\ntraining a single encoder for Information Retrieval (IR) and Semantic Textual\nSimilarity (STS), two essential but fundamentally disparate tasks for which\nnaive co-training typically yields steep performance trade-offs. We argue that\nresolving this conflict requires systematically decoupling task-specific\nlearning signals throughout the training pipeline. To this end, we introduce\nCoDiEmb, a unified framework that reconciles the divergent requirements of IR\nand STS in a collaborative yet distinct manner. CoDiEmb integrates three key\ninnovations for effective joint optimization: (1) Task-specialized objectives\npaired with a dynamic sampler that forms single-task batches and balances\nper-task updates, thereby preventing gradient interference. For IR, we employ a\ncontrastive loss with multiple positives and hard negatives, augmented by\ncross-device sampling. For STS, we adopt order-aware objectives that directly\noptimize correlation and ranking consistency. (2) A delta-guided model fusion\nstrategy that computes fine-grained merging weights for checkpoints by\nanalyzing each parameter's deviation from its pre-trained initialization,\nproving more effective than traditional Model Soups. (3) An efficient,\nsingle-stage training pipeline that is simple to implement and converges\nstably. Extensive experiments on 15 standard IR and STS benchmarks across three\nbase encoders validate CoDiEmb. Our results and analysis demonstrate that the\nframework not only mitigates cross-task trade-offs but also measurably improves\nthe geometric properties of the embedding space.", "AI": {"tldr": "CoDiEmb is a unified framework for learning text embeddings that effectively balances Information Retrieval (IR) and Semantic Textual Similarity (STS) tasks by decoupling task-specific signals and integrating specialized objectives, dynamic sampling, and delta-guided model fusion.", "motivation": "Negative transfer and performance trade-offs hinder joint training of IR and STS tasks, necessitating a systematic approach to decouple task-specific signals.", "method": "CoDiEmb employs task-specialized objectives with dynamic sampling, delta-guided model fusion, and a single-stage training pipeline.", "result": "Experiments on 15 benchmarks show CoDiEmb mitigates cross-task trade-offs and improves embedding space properties.", "conclusion": "CoDiEmb successfully unifies IR and STS training, enhancing performance and embedding quality."}}
{"id": "2508.11010", "pdf": "https://arxiv.org/pdf/2508.11010", "abs": "https://arxiv.org/abs/2508.11010", "authors": ["Tausifa Jan Saleem", "Mohammad Yaqub"], "title": "Deep Learning-Based Automated Segmentation of Uterine Myomas", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Uterine fibroids (myomas) are the most common benign tumors of the female\nreproductive system, particularly among women of childbearing age. With a\nprevalence exceeding 70%, they pose a significant burden on female reproductive\nhealth. Clinical symptoms such as abnormal uterine bleeding, infertility,\npelvic pain, and pressure-related discomfort play a crucial role in guiding\ntreatment decisions, which are largely influenced by the size, number, and\nanatomical location of the fibroids. Magnetic Resonance Imaging (MRI) is a\nnon-invasive and highly accurate imaging modality commonly used by clinicians\nfor the diagnosis of uterine fibroids. Segmenting uterine fibroids requires a\nprecise assessment of both the uterus and fibroids on MRI scans, including\nmeasurements of volume, shape, and spatial location. However, this process is\nlabor intensive and time consuming and subjected to variability due to intra-\nand inter-expert differences at both pre- and post-treatment stages. As a\nresult, there is a critical need for an accurate and automated segmentation\nmethod for uterine fibroids. In recent years, deep learning algorithms have\nshown re-markable improvements in medical image segmentation, outperforming\ntraditional methods. These approaches offer the potential for fully automated\nsegmentation. Several studies have explored the use of deep learning models to\nachieve automated segmentation of uterine fibroids. However, most of the\nprevious work has been conducted using private datasets, which poses challenges\nfor validation and comparison between studies. In this study, we leverage the\npublicly available Uterine Myoma MRI Dataset (UMD) to establish a baseline for\nautomated segmentation of uterine fibroids, enabling standardized evaluation\nand facilitating future research in this domain.", "AI": {"tldr": "The paper addresses the need for automated segmentation of uterine fibroids using MRI, leveraging deep learning and a public dataset (UMD) for standardized evaluation.", "motivation": "Uterine fibroids are highly prevalent and burdensome, with current MRI segmentation being labor-intensive and variable. Automated methods are needed to improve accuracy and efficiency.", "method": "The study uses deep learning algorithms for automated segmentation, utilizing the publicly available Uterine Myoma MRI Dataset (UMD) to establish a baseline.", "result": "Deep learning shows promise for fully automated segmentation, addressing variability and inefficiency in current methods.", "conclusion": "The use of a public dataset (UMD) enables standardized evaluation and advances research in automated uterine fibroid segmentation."}}
{"id": "2508.11167", "pdf": "https://arxiv.org/pdf/2508.11167", "abs": "https://arxiv.org/abs/2508.11167", "authors": ["Jianhong Han", "Yupei Wang", "Liang Chen"], "title": "VFM-Guided Semi-Supervised Detection Transformer for Source-Free Object Detection in Remote Sensing Images", "categories": ["cs.CV"], "comment": "Manuscript submitted to IEEE TGRS", "summary": "Unsupervised domain adaptation methods have been widely explored to bridge\ndomain gaps. However, in real-world remote-sensing scenarios, privacy and\ntransmission constraints often preclude access to source domain data, which\nlimits their practical applicability. Recently, Source-Free Object Detection\n(SFOD) has emerged as a promising alternative, aiming at cross-domain\nadaptation without relying on source data, primarily through a self-training\nparadigm. Despite its potential, SFOD frequently suffers from training collapse\ncaused by noisy pseudo-labels, especially in remote sensing imagery with dense\nobjects and complex backgrounds. Considering that limited target domain\nannotations are often feasible in practice, we propose a Vision\nfoundation-Guided DEtection TRansformer (VG-DETR), built upon a semi-supervised\nframework for SFOD in remote sensing images. VG-DETR integrates a Vision\nFoundation Model (VFM) into the training pipeline in a \"free lunch\" manner,\nleveraging a small amount of labeled target data to mitigate pseudo-label noise\nwhile improving the detector's feature-extraction capability. Specifically, we\nintroduce a VFM-guided pseudo-label mining strategy that leverages the VFM's\nsemantic priors to further assess the reliability of the generated\npseudo-labels. By recovering potentially correct predictions from\nlow-confidence outputs, our strategy improves pseudo-label quality and\nquantity. In addition, a dual-level VFM-guided alignment method is proposed,\nwhich aligns detector features with VFM embeddings at both the instance and\nimage levels. Through contrastive learning among fine-grained prototypes and\nsimilarity matching between feature maps, this dual-level alignment further\nenhances the robustness of feature representations against domain gaps.\nExtensive experiments demonstrate that VG-DETR achieves superior performance in\nsource-free remote sensing detection tasks.", "AI": {"tldr": "VG-DETR is a semi-supervised framework for source-free object detection in remote sensing, leveraging a Vision Foundation Model to improve pseudo-label quality and feature alignment.", "motivation": "Privacy and transmission constraints limit access to source domain data in remote sensing, making source-free object detection (SFOD) necessary. However, SFOD suffers from noisy pseudo-labels in dense, complex scenes.", "method": "VG-DETR integrates a Vision Foundation Model (VFM) to guide pseudo-label mining and feature alignment. It uses labeled target data to mitigate noise and aligns features at instance and image levels via contrastive learning.", "result": "VG-DETR outperforms existing methods in source-free remote sensing detection, improving pseudo-label quality and feature robustness.", "conclusion": "VG-DETR effectively addresses SFOD challenges in remote sensing by leveraging VFM guidance, enhancing both pseudo-label reliability and feature representation."}}
{"id": "2508.11328", "pdf": "https://arxiv.org/pdf/2508.11328", "abs": "https://arxiv.org/abs/2508.11328", "authors": ["Haitong Luo", "Suhang Wang", "Weiyao Zhang", "Ruiqi Meng", "Xuying Meng", "Yujun Zhang"], "title": "Generalize across Homophily and Heterophily: Hybrid Spectral Graph Pre-Training and Prompt Tuning", "categories": ["cs.LG", "cs.CL"], "comment": "Under Review", "summary": "Graph ``pre-training and prompt-tuning'' aligns downstream tasks with\npre-trained objectives to enable efficient knowledge transfer under limited\nsupervision. However, existing methods rely on homophily-based low-frequency\nknowledge, failing to handle diverse spectral distributions in real-world\ngraphs with varying homophily. Our theoretical analysis reveals a spectral\nspecificity principle: optimal knowledge transfer requires alignment between\npre-trained spectral filters and the intrinsic spectrum of downstream graphs.\nUnder limited supervision, large spectral gaps between pre-training and\ndownstream tasks impede effective adaptation. To bridge this gap, we propose\nthe HS-GPPT model, a novel framework that ensures spectral alignment throughout\nboth pre-training and prompt-tuning. We utilize a hybrid spectral filter\nbackbone and local-global contrastive learning to acquire abundant spectral\nknowledge. Then we design prompt graphs to align the spectral distribution with\npretexts, facilitating spectral knowledge transfer across homophily and\nheterophily. Extensive experiments validate the effectiveness under both\ntransductive and inductive learning settings. Our code is available at\nhttps://anonymous.4open.science/r/HS-GPPT-62D2/.", "AI": {"tldr": "The paper introduces HS-GPPT, a model addressing spectral misalignment in graph pre-training and prompt-tuning for better knowledge transfer across diverse graphs.", "motivation": "Existing methods fail to handle diverse spectral distributions in real-world graphs due to reliance on homophily-based low-frequency knowledge.", "method": "Proposes HS-GPPT with a hybrid spectral filter backbone and local-global contrastive learning for spectral alignment, along with prompt graphs for distribution alignment.", "result": "Extensive experiments show effectiveness in transductive and inductive learning settings.", "conclusion": "HS-GPPT bridges spectral gaps, enabling efficient knowledge transfer across varying homophily."}}
{"id": "2508.11454", "pdf": "https://arxiv.org/pdf/2508.11454", "abs": "https://arxiv.org/abs/2508.11454", "authors": ["Junichiro Niimi"], "title": "Reference Points in LLM Sentiment Analysis: The Role of Structured Context", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) are now widely used across many fields,\nincluding marketing research. Sentiment analysis, in particular, helps firms\nunderstand consumer preferences. While most NLP studies classify sentiment from\nreview text alone, marketing theories, such as prospect theory and\nexpectation--disconfirmation theory, point out that customer evaluations are\nshaped not only by the actual experience but also by additional reference\npoints. This study therefore investigates how the content and format of such\nsupplementary information affect sentiment analysis using LLMs. We compare\nnatural language (NL) and JSON-formatted prompts using a lightweight 3B\nparameter model suitable for practical marketing applications. Experiments on\ntwo Yelp categories (Restaurant and Nightlife) show that the JSON prompt with\nadditional information outperforms all baselines without fine-tuning: Macro-F1\nrises by 1.6% and 4% while RMSE falls by 16% and 9.1%, respectively, making it\ndeployable in resource-constrained edge devices. Furthermore, a follow-up\nanalysis confirms that performance gains stem from genuine contextual reasoning\nrather than label proxying. This work demonstrates that structured prompting\ncan enable smaller models to achieve competitive performance, offering a\npractical alternative to large-scale model deployment.", "AI": {"tldr": "The study explores how supplementary information in JSON format improves sentiment analysis with a lightweight LLM, outperforming baselines without fine-tuning.", "motivation": "Marketing theories suggest customer sentiment is influenced by more than just review text, prompting investigation into how additional reference points affect LLM performance.", "method": "Comparison of natural language and JSON-formatted prompts using a 3B parameter LLM on Yelp Restaurant and Nightlife data.", "result": "JSON prompts with extra info boosted Macro-F1 by 1.6% and 4%, reduced RMSE by 16% and 9.1%, and showed genuine contextual reasoning.", "conclusion": "Structured prompting enables smaller models to perform competitively, offering a practical alternative to large-scale deployment."}}
{"id": "2508.11170", "pdf": "https://arxiv.org/pdf/2508.11170", "abs": "https://arxiv.org/abs/2508.11170", "authors": ["Baihong Qian", "Haotian Fan", "Wenjie Liao", "Yunqiu Wang", "Tao Li", "Junhui Cui"], "title": "Better Supervised Fine-tuning for VQA: Integer-Only Loss", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "With the rapid advancement of vision language models(VLM), their ability to\nassess visual content based on specific criteria and dimensions has become\nincreasingly critical for applications such as video-theme consistency\nassessment and visual quality scoring. However, existing methods often suffer\nfrom imprecise results and inefficient loss calculation, which limit the focus\nof the model on key evaluation indicators. To address this, we propose\nIOVQA(Integer-only VQA), a novel fine-tuning approach tailored for VLMs to\nenhance their performance in video quality assessment tasks. The key innovation\nof IOVQA lies in its label construction and its targeted loss calculation\nmechanism. Specifically, during dataset curation, we constrain the model's\noutput to integers within the range of [10,50], ensuring numerical stability,\nand convert decimal Overall_MOS to integer before using them as labels. We also\nintroduce a target-mask strategy: when computing the loss, only the first\ntwo-digit-integer of the label is unmasked, forcing the model to learn the\ncritical components of the numerical evaluation. After fine-tuning the\nQwen2.5-VL model using the constructed dataset, experimental results\ndemonstrate that the proposed method significantly improves the model's\naccuracy and consistency in the VQA task, ranking 3rd in VQualA 2025\nGenAI-Bench AIGC Video Quality Assessment Challenge -- Track I. Our work\nhighlights the effectiveness of merely leaving integer labels during\nfine-tuning, providing an effective idea for optimizing VLMs in quantitative\nevaluation scenarios.", "AI": {"tldr": "IOVQA, a fine-tuning method for VLMs, improves video quality assessment by using integer labels and a target-mask strategy, achieving high accuracy and ranking 3rd in a benchmark challenge.", "motivation": "Existing methods for visual content assessment are imprecise and inefficient, limiting model focus on key evaluation indicators.", "method": "Proposes IOVQA, which uses integer labels (10-50) and a target-mask strategy to focus on critical numerical components during loss calculation.", "result": "Significantly improves model accuracy and consistency, ranking 3rd in the VQualA 2025 GenAI-Bench Challenge.", "conclusion": "Integer-only fine-tuning is effective for optimizing VLMs in quantitative evaluation tasks."}}
{"id": "2508.11338", "pdf": "https://arxiv.org/pdf/2508.11338", "abs": "https://arxiv.org/abs/2508.11338", "authors": ["Prathamesh Devadiga", "Yashmitha Shailesh"], "title": "RegimeNAS: Regime-Aware Differentiable Architecture Search With Theoretical Guarantees for Financial Trading", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We introduce RegimeNAS, a novel differentiable architecture search framework\nspecifically designed to enhance cryptocurrency trading performance by\nexplicitly integrating market regime awareness. Addressing the limitations of\nstatic deep learning models in highly dynamic financial environments, RegimeNAS\nfeatures three core innovations: (1) a theoretically grounded Bayesian search\nspace optimizing architectures with provable convergence properties; (2)\nspecialized, dynamically activated neural modules (Volatility, Trend, and Range\nblocks) tailored for distinct market conditions; and (3) a multi-objective loss\nfunction incorporating market-specific penalties (e.g., volatility matching,\ntransition smoothness) alongside mathematically enforced Lipschitz stability\nconstraints. Regime identification leverages multi-head attention across\nmultiple timeframes for improved accuracy and uncertainty estimation. Rigorous\nempirical evaluation on extensive real-world cryptocurrency data demonstrates\nthat RegimeNAS significantly outperforms state-of-the-art benchmarks, achieving\nan 80.3% Mean Absolute Error reduction compared to the best traditional\nrecurrent baseline and converging substantially faster (9 vs. 50+ epochs).\nAblation studies and regime-specific analysis confirm the critical contribution\nof each component, particularly the regime-aware adaptation mechanism. This\nwork underscores the imperative of embedding domain-specific knowledge, such as\nmarket regimes, directly within the NAS process to develop robust and adaptive\nmodels for challenging financial applications.", "AI": {"tldr": "RegimeNAS is a differentiable architecture search framework for cryptocurrency trading, integrating market regime awareness to outperform static models.", "motivation": "Addressing the limitations of static deep learning models in dynamic financial environments by embedding market regime awareness.", "method": "Features a Bayesian search space, specialized neural modules for market conditions, and a multi-objective loss function with market-specific penalties.", "result": "Outperforms benchmarks with 80.3% MAE reduction and faster convergence (9 vs. 50+ epochs).", "conclusion": "Highlights the importance of domain-specific knowledge in NAS for robust financial models."}}
{"id": "2508.11534", "pdf": "https://arxiv.org/pdf/2508.11534", "abs": "https://arxiv.org/abs/2508.11534", "authors": ["Monika Jotautait\u0117", "Lucius Caviola", "David A. Brewster", "Thilo Hagendorff"], "title": "Speciesism in AI: Evaluating Discrimination Against Animals in Large Language Models", "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "As large language models (LLMs) become more widely deployed, it is crucial to\nexamine their ethical tendencies. Building on research on fairness and\ndiscrimination in AI, we investigate whether LLMs exhibit speciesist bias --\ndiscrimination based on species membership -- and how they value non-human\nanimals. We systematically examine this issue across three paradigms: (1)\nSpeciesismBench, a 1,003-item benchmark assessing recognition and moral\nevaluation of speciesist statements; (2) established psychological measures\ncomparing model responses with those of human participants; (3) text-generation\ntasks probing elaboration on, or resistance to, speciesist rationalizations. In\nour benchmark, LLMs reliably detected speciesist statements but rarely\ncondemned them, often treating speciesist attitudes as morally acceptable. On\npsychological measures, results were mixed: LLMs expressed slightly lower\nexplicit speciesism than people, yet in direct trade-offs they more often chose\nto save one human over multiple animals. A tentative interpretation is that\nLLMs may weight cognitive capacity rather than species per se: when capacities\nwere equal, they showed no species preference, and when an animal was described\nas more capable, they tended to prioritize it over a less capable human. In\nopen-ended text generation tasks, LLMs frequently normalized or rationalized\nharm toward farmed animals while refusing to do so for non-farmed animals.\nThese findings suggest that while LLMs reflect a mixture of progressive and\nmainstream human views, they nonetheless reproduce entrenched cultural norms\naround animal exploitation. We argue that expanding AI fairness and alignment\nframeworks to explicitly include non-human moral patients is essential for\nreducing these biases and preventing the entrenchment of speciesist attitudes\nin AI systems and the societies they influence.", "AI": {"tldr": "The paper investigates speciesist bias in large language models (LLMs), finding they often treat speciesist attitudes as morally acceptable and reproduce cultural norms around animal exploitation.", "motivation": "To examine ethical tendencies in LLMs, specifically speciesist bias and how they value non-human animals, expanding AI fairness frameworks.", "method": "Three paradigms: SpeciesismBench benchmark, psychological measures comparing model and human responses, and text-generation tasks probing speciesist rationalizations.", "result": "LLMs detected speciesist statements but rarely condemned them, showed mixed results on psychological measures, and normalized harm toward farmed animals in text generation.", "conclusion": "Expanding AI fairness frameworks to include non-human moral patients is essential to reduce speciesist biases in LLMs and their societal influence."}}
{"id": "2508.11173", "pdf": "https://arxiv.org/pdf/2508.11173", "abs": "https://arxiv.org/abs/2508.11173", "authors": ["Ruobing Jiang", "Yang Liu", "Haobing Liu", "Yanwei Yu", "Chunyang Wang"], "title": "Exploring the Tradeoff Between Diversity and Discrimination for Continuous Category Discovery", "categories": ["cs.CV"], "comment": "Accepted by CIKM 2025. 10 pages, 5 figures,", "summary": "Continuous category discovery (CCD) aims to automatically discover novel\ncategories in continuously arriving unlabeled data. This is a challenging\nproblem considering that there is no number of categories and labels in the\nnewly arrived data, while also needing to mitigate catastrophic forgetting.\nMost CCD methods cannot handle the contradiction between novel class discovery\nand classification well. They are also prone to accumulate errors in the\nprocess of gradually discovering novel classes. Moreover, most of them use\nknowledge distillation and data replay to prevent forgetting, occupying more\nstorage space. To address these limitations, we propose Independence-based\nDiversity and Orthogonality-based Discrimination (IDOD). IDOD mainly includes\nindependent enrichment of diversity module, joint discovery of novelty module,\nand continuous increment by orthogonality module. In independent enrichment,\nthe backbone is trained separately using contrastive loss to avoid it focusing\nonly on features for classification. Joint discovery transforms multi-stage\nnovel class discovery into single-stage, reducing error accumulation impact.\nContinuous increment by orthogonality module generates mutually orthogonal\nprototypes for classification and prevents forgetting with lower space overhead\nvia representative representation replay. Experimental results show that on\nchallenging fine-grained datasets, our method outperforms the state-of-the-art\nmethods.", "AI": {"tldr": "IDOD is a novel method for continuous category discovery (CCD) that addresses limitations like error accumulation and storage inefficiency by using diversity enrichment, joint novelty discovery, and orthogonality-based discrimination.", "motivation": "CCD is challenging due to unknown categories in unlabeled data, error accumulation, and storage-heavy forgetting prevention methods.", "method": "IDOD includes independent diversity enrichment, joint novelty discovery, and orthogonality-based discrimination to improve CCD.", "result": "IDOD outperforms state-of-the-art methods on fine-grained datasets.", "conclusion": "IDOD effectively balances novel class discovery and classification while reducing storage overhead."}}
{"id": "2508.11345", "pdf": "https://arxiv.org/pdf/2508.11345", "abs": "https://arxiv.org/abs/2508.11345", "authors": ["Shuqi Liu", "Jianguo Huang", "Luke Ong"], "title": "Conformal Prediction Meets Long-tail Classification", "categories": ["cs.LG"], "comment": null, "summary": "Conformal Prediction (CP) is a popular method for uncertainty quantification\nthat converts a pretrained model's point prediction into a prediction set, with\nthe set size reflecting the model's confidence. Although existing CP methods\nare guaranteed to achieve marginal coverage, they often exhibit imbalanced\ncoverage across classes under long-tail label distributions, tending to over\ncover the head classes at the expense of under covering the remaining tail\nclasses. This under coverage is particularly concerning, as it undermines the\nreliability of the prediction sets for minority classes, even with coverage\nensured on average. In this paper, we propose the Tail-Aware Conformal\nPrediction (TACP) method to mitigate the under coverage of the tail classes by\nutilizing the long-tail structure and narrowing the head-tail coverage gap.\nTheoretical analysis shows that it consistently achieves a smaller head-tail\ncoverage gap than standard methods. To further improve coverage balance across\nall classes, we introduce an extension of TACP: soft TACP (sTACP) via a\nreweighting mechanism. The proposed framework can be combined with various\nnon-conformity scores, and experiments on multiple long-tail benchmark datasets\ndemonstrate the effectiveness of our methods.", "AI": {"tldr": "The paper introduces Tail-Aware Conformal Prediction (TACP) and its soft extension (sTACP) to address imbalanced coverage in long-tail label distributions, ensuring better reliability for minority classes.", "motivation": "Existing CP methods often over-cover head classes while under-covering tail classes, undermining reliability for minority classes despite marginal coverage guarantees.", "method": "Proposes TACP to utilize long-tail structure and narrow the head-tail coverage gap, with sTACP adding a reweighting mechanism for further balance.", "result": "Theoretical analysis confirms TACP reduces the head-tail gap; experiments on benchmark datasets validate effectiveness.", "conclusion": "TACP and sTACP improve coverage balance across classes, enhancing reliability for minority predictions."}}
{"id": "2508.11536", "pdf": "https://arxiv.org/pdf/2508.11536", "abs": "https://arxiv.org/abs/2508.11536", "authors": ["Maria Ryskina", "Greta Tuckute", "Alexander Fung", "Ashley Malkin", "Evelina Fedorenko"], "title": "Language models align with brain regions that represent concepts across modalities", "categories": ["cs.CL"], "comment": "Accepted to COLM 2025. Code and data can be found at\n  https://github.com/ryskina/concepts-brain-llms", "summary": "Cognitive science and neuroscience have long faced the challenge of\ndisentangling representations of language from representations of conceptual\nmeaning. As the same problem arises in today's language models (LMs), we\ninvestigate the relationship between LM--brain alignment and two neural\nmetrics: (1) the level of brain activation during processing of sentences,\ntargeting linguistic processing, and (2) a novel measure of meaning consistency\nacross input modalities, which quantifies how consistently a brain region\nresponds to the same concept across paradigms (sentence, word cloud, image)\nusing an fMRI dataset (Pereira et al., 2018). Our experiments show that both\nlanguage-only and language-vision models predict the signal better in more\nmeaning-consistent areas of the brain, even when these areas are not strongly\nsensitive to language processing, suggesting that LMs might internally\nrepresent cross-modal conceptual meaning.", "AI": {"tldr": "The paper explores how language models (LMs) align with brain activity, focusing on linguistic processing and cross-modal meaning consistency. Findings suggest LMs may represent conceptual meaning across modalities.", "motivation": "To understand if LMs internally represent conceptual meaning by comparing their alignment with brain activity in linguistic and cross-modal contexts.", "method": "Analyzed LM-brain alignment using fMRI data, measuring brain activation during sentence processing and meaning consistency across input modalities (sentences, word clouds, images).", "result": "LMs predict brain signals better in meaning-consistent areas, even those not strongly sensitive to language, indicating potential cross-modal conceptual representation.", "conclusion": "LMs may inherently represent conceptual meaning across modalities, aligning with brain regions sensitive to meaning consistency."}}
{"id": "2508.11176", "pdf": "https://arxiv.org/pdf/2508.11176", "abs": "https://arxiv.org/abs/2508.11176", "authors": ["Yumiao Zhao", "Bo Jiang", "Yuhe Ding", "Xiao Wang", "Jin Tang", "Bin Luo"], "title": "Fine-Grained VLM Fine-tuning via Latent Hierarchical Adapter Learning", "categories": ["cs.CV"], "comment": null, "summary": "Adapter-based approaches have garnered attention for fine-tuning pre-trained\nVision-Language Models (VLMs) on few-shot classification tasks. These methods\nstrive to develop a lightweight module that better aligns visual and (category)\ntextual representations, thereby enhancing performance on downstream few-shot\nlearning tasks. However, existing adapters generally learn/align (category)\ntextual-visual modalities via explicit spatial proximity in the underlying\nembedding space, which i) fails to capture the inherent one-to-many\nassociations between categories and image samples and ii) struggles to\nestablish accurate associations between the unknown categories and images. To\naddress these issues, inspired by recent works on hyperbolic learning, we\ndevelop a novel Latent Hierarchical Adapter (LatHAdapter) for fine-tuning VLMs\non downstream few-shot classification tasks. The core of LatHAdapter is to\nexploit the latent semantic hierarchy of downstream training data and employ it\nto provide richer, fine-grained guidance for the adapter learning process.\nSpecifically, LatHAdapter first introduces some learnable `attribute' prompts\nas the bridge to align categories and images. Then, it projects the categories,\nattribute prompts, and images within each batch in a hyperbolic space, and\nemploys hierarchical regularization to learn the latent semantic hierarchy of\nthem, thereby fully modeling the inherent one-to-many associations among\ncategories, learnable attributes, and image samples. Extensive experiments on\nfour challenging few-shot tasks show that the proposed LatHAdapter consistently\noutperforms many other fine-tuning approaches, particularly in adapting known\nclasses and generalizing to unknown classes.", "AI": {"tldr": "LatHAdapter improves few-shot classification by leveraging latent semantic hierarchies in hyperbolic space for better alignment of visual and textual representations.", "motivation": "Existing adapters fail to capture one-to-many associations between categories and images and struggle with unknown categories.", "method": "LatHAdapter uses learnable attribute prompts and hyperbolic space projection with hierarchical regularization to model semantic hierarchies.", "result": "Outperforms other fine-tuning approaches on four few-shot tasks, especially for known and unknown classes.", "conclusion": "LatHAdapter effectively addresses alignment and generalization challenges in few-shot learning."}}
{"id": "2508.11348", "pdf": "https://arxiv.org/pdf/2508.11348", "abs": "https://arxiv.org/abs/2508.11348", "authors": ["Xiaohan Bi", "Binhang Qi", "Hailong Sun", "Xiang Gao", "Yue Yu", "Xiaojun Liang"], "title": "NeMo: A Neuron-Level Modularizing-While-Training Approach for Decomposing DNN Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "With the growing incorporation of deep neural network (DNN) models into\nmodern software systems, the prohibitive construction costs have become a\nsignificant challenge. Model reuse has been widely applied to reduce training\ncosts, but indiscriminately reusing entire models may incur significant\ninference overhead. Consequently, DNN modularization has gained attention,\nenabling module reuse by decomposing DNN models. The emerging\nmodularizing-while-training (MwT) paradigm, which incorporates modularization\ninto training, outperforms modularizing-after-training approaches. However,\nexisting MwT methods focus on small-scale CNN models at the convolutional\nkernel level and struggle with diverse DNNs and large-scale models,\nparticularly Transformer-based models. To address these limitations, we propose\nNeMo, a scalable and generalizable MwT approach. NeMo operates at the neuron\nlevel fundamental component common to all DNNs-ensuring applicability to\nTransformers and various architectures. We design a contrastive learning-based\nmodular training method with an effective composite loss function, enabling\nscalability to large-scale models. Comprehensive experiments on two\nTransformer-based models and four CNN models across two classification datasets\ndemonstrate NeMo's superiority over state-of-the-art MwT methods. Results show\naverage gains of 1.72% in module classification accuracy and 58.10% reduction\nin module size, demonstrating efficacy across both CNN and large-scale\nTransformer-based models. A case study on open-source projects shows NeMo's\npotential benefits in practical scenarios, offering a promising approach for\nscalable and generalizable DNN modularization.", "AI": {"tldr": "NeMo introduces a scalable and generalizable modularizing-while-training (MwT) approach for DNNs, outperforming existing methods in accuracy and efficiency.", "motivation": "The prohibitive costs of DNN model construction and the limitations of existing MwT methods for diverse and large-scale models drive the need for a more scalable solution.", "method": "NeMo operates at the neuron level, uses contrastive learning-based modular training, and employs a composite loss function for scalability.", "result": "NeMo achieves average gains of 1.72% in module classification accuracy and 58.10% reduction in module size, excelling with both CNNs and Transformers.", "conclusion": "NeMo offers a promising, scalable, and generalizable approach for DNN modularization, with practical benefits demonstrated in real-world applications."}}
{"id": "2508.11567", "pdf": "https://arxiv.org/pdf/2508.11567", "abs": "https://arxiv.org/abs/2508.11567", "authors": ["Jinpeng Hu", "Ao Wang", "Qianqian Xie", "Hui Ma", "Zhuo Li", "Dan Guo"], "title": "AgentMental: An Interactive Multi-Agent Framework for Explainable and Adaptive Mental Health Assessment", "categories": ["cs.CL"], "comment": null, "summary": "Mental health assessment is crucial for early intervention and effective\ntreatment, yet traditional clinician-based approaches are limited by the\nshortage of qualified professionals. Recent advances in artificial intelligence\nhave sparked growing interest in automated psychological assessment, yet most\nexisting approaches are constrained by their reliance on static text analysis,\nlimiting their ability to capture deeper and more informative insights that\nemerge through dynamic interaction and iterative questioning. Therefore, in\nthis paper, we propose a multi-agent framework for mental health evaluation\nthat simulates clinical doctor-patient dialogues, with specialized agents\nassigned to questioning, adequacy evaluation, scoring, and updating. We\nintroduce an adaptive questioning mechanism in which an evaluation agent\nassesses the adequacy of user responses to determine the necessity of\ngenerating targeted follow-up queries to address ambiguity and missing\ninformation. Additionally, we employ a tree-structured memory in which the root\nnode encodes the user's basic information, while child nodes (e.g., topic and\nstatement) organize key information according to distinct symptom categories\nand interaction turns. This memory is dynamically updated throughout the\ninteraction to reduce redundant questioning and further enhance the information\nextraction and contextual tracking capabilities. Experimental results on the\nDAIC-WOZ dataset illustrate the effectiveness of our proposed method, which\nachieves better performance than existing approaches.", "AI": {"tldr": "A multi-agent framework for automated mental health assessment simulates clinical dialogues, using adaptive questioning and tree-structured memory to improve accuracy and reduce redundancy.", "motivation": "Traditional mental health assessments are limited by clinician shortages and static text analysis, prompting the need for dynamic, interactive AI solutions.", "method": "The framework employs specialized agents for questioning, evaluation, scoring, and updating, with adaptive questioning and tree-structured memory for dynamic interaction.", "result": "The method outperforms existing approaches on the DAIC-WOZ dataset, demonstrating improved performance.", "conclusion": "The proposed framework enhances automated mental health assessment by capturing deeper insights through dynamic interaction and adaptive techniques."}}
{"id": "2508.11031", "pdf": "https://arxiv.org/pdf/2508.11031", "abs": "https://arxiv.org/abs/2508.11031", "authors": ["John W. Sheppard"], "title": "Risk-Based Prognostics and Health Management", "categories": ["eess.SY", "cs.AI", "cs.SY", "stat.AP"], "comment": "Appears as Chapter 27 in Realizing Complex Integrated Systems,\n  Anthony P. Ambler and John W. Sheppard (ads.), CRC Press, 2025", "summary": "It is often the case that risk assessment and prognostics are viewed as\nrelated but separate tasks. This chapter describes a risk-based approach to\nprognostics that seeks to provide a tighter coupling between risk assessment\nand fault prediction. We show how this can be achieved using the\ncontinuous-time Bayesian network as the underlying modeling framework.\nFurthermore, we provide an overview of the techniques that are available to\nderive these models from data and show how they might be used in practice to\nachieve tasks like decision support and performance-based logistics. This work\nis intended to provide an overview of the recent developments related to\nrisk-based prognostics, and we hope that it will serve as a tutorial of sorts\nthat will assist others in adopting these techniques.", "AI": {"tldr": "The paper presents a risk-based approach to prognostics, integrating risk assessment and fault prediction using continuous-time Bayesian networks, with practical applications like decision support.", "motivation": "To bridge the gap between risk assessment and prognostics by providing a unified framework.", "method": "Uses continuous-time Bayesian networks for modeling and discusses techniques for deriving these models from data.", "result": "Demonstrates practical applications such as decision support and performance-based logistics.", "conclusion": "Aims to serve as a tutorial for adopting risk-based prognostics techniques, summarizing recent developments."}}
{"id": "2508.11183", "pdf": "https://arxiv.org/pdf/2508.11183", "abs": "https://arxiv.org/abs/2508.11183", "authors": ["Zhenghao Chen", "Zicong Chen", "Lei Liu", "Yiming Wu", "Dong Xu"], "title": "Versatile Video Tokenization with Generative 2D Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "Video tokenization procedure is critical for a wide range of video processing\ntasks. Most existing approaches directly transform video into fixed-grid and\npatch-wise tokens, which exhibit limited versatility. Spatially, uniformly\nallocating a fixed number of tokens often leads to over-encoding in\nlow-information regions. Temporally, reducing redundancy remains challenging\nwithout explicitly distinguishing between static and dynamic content. In this\nwork, we propose the Gaussian Video Transformer (GVT), a versatile video\ntokenizer built upon a generative 2D Gaussian Splatting (2DGS) strategy. We\nfirst extract latent rigid features from a video clip and represent them with a\nset of 2D Gaussians generated by our proposed Spatio-Temporal Gaussian\nEmbedding (STGE) mechanism in a feed-forward manner. Such generative 2D\nGaussians not only enhance spatial adaptability by assigning higher (resp.,\nlower) rendering weights to regions with higher (resp., lower) information\ncontent during rasterization, but also improve generalization by avoiding\nper-video optimization.To enhance the temporal versatility, we introduce a\nGaussian Set Partitioning (GSP) strategy that separates the 2D Gaussians into\nstatic and dynamic sets, which explicitly model static content shared across\ndifferent time-steps and dynamic content specific to each time-step, enabling a\ncompact representation.We primarily evaluate GVT on the video reconstruction,\nwhile also assessing its performance on action recognition and compression\nusing the UCF101, Kinetics, and DAVIS datasets. Extensive experiments\ndemonstrate that GVT achieves a state-of-the-art video reconstruction quality,\noutperforms the baseline MAGVIT-v2 in action recognition, and delivers\ncomparable compression performance.", "AI": {"tldr": "The paper introduces the Gaussian Video Transformer (GVT), a video tokenizer using 2D Gaussian Splatting for versatile video processing, improving spatial adaptability and temporal redundancy reduction.", "motivation": "Existing video tokenization methods lack versatility, over-encode low-information regions, and struggle with temporal redundancy. GVT addresses these limitations.", "method": "GVT uses a Spatio-Temporal Gaussian Embedding (STGE) mechanism to generate 2D Gaussians for spatial adaptability and a Gaussian Set Partitioning (GSP) strategy to separate static and dynamic content.", "result": "GVT achieves state-of-the-art video reconstruction, outperforms MAGVIT-v2 in action recognition, and delivers comparable compression performance.", "conclusion": "GVT provides a versatile and efficient video tokenization solution, enhancing both spatial and temporal processing."}}
{"id": "2508.11349", "pdf": "https://arxiv.org/pdf/2508.11349", "abs": "https://arxiv.org/abs/2508.11349", "authors": ["Angela John", "Selvyn Allotey", "Till Koebe", "Alexandra Tyukavina", "Ingmar Weber"], "title": "A Global Dataset of Location Data Integrity-Assessed Reforestation Efforts", "categories": ["cs.LG"], "comment": "10 figures", "summary": "Afforestation and reforestation are popular strategies for mitigating climate\nchange by enhancing carbon sequestration. However, the effectiveness of these\nefforts is often self-reported by project developers, or certified through\nprocesses with limited external validation. This leads to concerns about data\nreliability and project integrity. In response to increasing scrutiny of\nvoluntary carbon markets, this study presents a dataset on global afforestation\nand reforestation efforts compiled from primary (meta-)information and\naugmented with time-series satellite imagery and other secondary data. Our\ndataset covers 1,289,068 planting sites from 45,628 projects spanning 33 years.\nSince any remote sensing-based validation effort relies on the integrity of a\nplanting site's geographic boundary, this dataset introduces a standardized\nassessment of the provided site-level location information, which we summarize\nin one easy-to-communicate key indicator: LDIS -- the Location Data Integrity\nScore. We find that approximately 79\\% of the georeferenced planting sites\nmonitored fail on at least 1 out of 10 LDIS indicators, while 15\\% of the\nmonitored projects lack machine-readable georeferenced data in the first place.\nIn addition to enhancing accountability in the voluntary carbon market, the\npresented dataset also holds value as training data for e.g. computer\nvision-related tasks with millions of linked Sentinel-2 and Planetscope\nsatellite images.", "AI": {"tldr": "The study addresses reliability concerns in afforestation/reforestation projects by creating a global dataset with satellite validation and introducing the LDIS score to assess location data integrity.", "motivation": "To improve accountability in voluntary carbon markets by addressing unreliable self-reported data on afforestation/reforestation projects.", "method": "Compiled a dataset from primary and secondary sources, including satellite imagery, and introduced the LDIS score to evaluate location data integrity.", "result": "Found that 79% of georeferenced sites failed at least 1 LDIS indicator, and 15% lacked machine-readable geodata.", "conclusion": "The dataset enhances accountability and serves as valuable training data for remote sensing tasks."}}
{"id": "2508.11582", "pdf": "https://arxiv.org/pdf/2508.11582", "abs": "https://arxiv.org/abs/2508.11582", "authors": ["Qiguang Chen", "Dengyun Peng", "Jinhao Liu", "HuiKang Su", "Jiannan Guan", "Libo Qin", "Wanxiang Che"], "title": "Aware First, Think Less: Dynamic Boundary Self-Awareness Drives Extreme Reasoning Efficiency in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Preprint", "summary": "Recent advancements in large language models (LLMs) have greatly improved\ntheir capabilities on complex reasoning tasks through Long Chain-of-Thought\n(CoT). However, this approach often results in substantial redundancy,\nimpairing computational efficiency and causing significant delays in real-time\napplications. To improve the efficiency, current methods often rely on\nhuman-defined difficulty priors, which do not align with the LLM's self-awared\ndifficulty, leading to inefficiencies. In this paper, we introduce the Dynamic\nReasoning-Boundary Self-Awareness Framework (DR. SAF), which enables models to\ndynamically assess and adjust their reasoning depth in response to problem\ncomplexity. DR. SAF integrates three key components: Boundary Self-Awareness\nAlignment, Adaptive Reward Management, and a Boundary Preservation Mechanism.\nThese components allow models to optimize their reasoning processes, balancing\nefficiency and accuracy without compromising performance. Our experimental\nresults demonstrate that DR. SAF achieves a 49.27% reduction in total response\ntokens with minimal loss in accuracy. The framework also delivers a 6.59x gain\nin token efficiency and a 5x reduction in training time, making it well-suited\nto resource-limited settings. During extreme training, DR. SAF can even surpass\ntraditional instruction-based models in token efficiency with more than 16%\naccuracy improvement.", "AI": {"tldr": "DR. SAF is a framework that dynamically adjusts reasoning depth in LLMs, improving efficiency and accuracy without human priors.", "motivation": "Current Long Chain-of-Thought methods in LLMs are inefficient due to redundancy and misaligned human-defined difficulty priors.", "method": "DR. SAF integrates Boundary Self-Awareness Alignment, Adaptive Reward Management, and Boundary Preservation Mechanism to optimize reasoning depth.", "result": "Achieves 49.27% token reduction, 6.59x token efficiency, 5x faster training, and up to 16% accuracy improvement.", "conclusion": "DR. SAF enhances LLM efficiency and accuracy, making it suitable for resource-limited settings."}}
{"id": "2508.11033", "pdf": "https://arxiv.org/pdf/2508.11033", "abs": "https://arxiv.org/abs/2508.11033", "authors": ["Parker Whitfill"], "title": "Note on Selection Bias in Observational Estimates of Algorithmic Progress", "categories": ["econ.GN", "cs.AI", "q-fin.EC"], "comment": null, "summary": "Ho et. al (2024) is an interesting paper that attempts to estimate the degree\nof algorithmic progress from language models. They collect observational data\non language models' loss and compute over time, and argue that as time has\npassed, language models' algorithmic efficiency has been rising. That is, the\nloss achieved for fixed compute has been dropping over time. In this note, I\nwant to raise one potential methodological problem with the estimation\nstrategy. Intuitively, if part of algorithmic quality is latent, and compute\nchoices are endogenous to algorithmic quality, then resulting estimates of\nalgorithmic quality will be biased.", "AI": {"tldr": "The paper estimates algorithmic progress in language models by analyzing loss and compute over time, but a methodological issue arises due to latent algorithmic quality and endogenous compute choices.", "motivation": "To quantify the improvement in algorithmic efficiency of language models over time by examining loss and compute trends.", "method": "Collect observational data on language models' loss and compute over time, then analyze trends to infer algorithmic progress.", "result": "The study suggests algorithmic efficiency has improved, as loss decreases for fixed compute over time.", "conclusion": "The estimation may be biased due to latent algorithmic quality and endogenous compute choices, questioning the validity of the results."}}
{"id": "2508.11185", "pdf": "https://arxiv.org/pdf/2508.11185", "abs": "https://arxiv.org/abs/2508.11185", "authors": ["Abhinav Kumar", "Yuliang Guo", "Zhihao Zhang", "Xinyu Huang", "Liu Ren", "Xiaoming Liu"], "title": "CHARM3R: Towards Unseen Camera Height Robust Monocular 3D Detector", "categories": ["cs.CV", "cs.LG"], "comment": "ICCV 2025", "summary": "Monocular 3D object detectors, while effective on data from one ego camera\nheight, struggle with unseen or out-of-distribution camera heights. Existing\nmethods often rely on Plucker embeddings, image transformations or data\naugmentation. This paper takes a step towards this understudied problem by\nfirst investigating the impact of camera height variations on state-of-the-art\n(SoTA) Mono3D models. With a systematic analysis on the extended CARLA dataset\nwith multiple camera heights, we observe that depth estimation is a primary\nfactor influencing performance under height variations. We mathematically prove\nand also empirically observe consistent negative and positive trends in mean\ndepth error of regressed and ground-based depth models, respectively, under\ncamera height changes. To mitigate this, we propose Camera Height Robust\nMonocular 3D Detector (CHARM3R), which averages both depth estimates within the\nmodel. CHARM3R improves generalization to unseen camera heights by more than\n$45\\%$, achieving SoTA performance on the CARLA dataset. Codes and Models at\nhttps://github.com/abhi1kumar/CHARM3R", "AI": {"tldr": "The paper addresses the challenge of monocular 3D object detectors struggling with unseen camera heights, proposing CHARM3R to improve generalization by averaging depth estimates.", "motivation": "Existing methods fail under camera height variations; the paper investigates this issue and identifies depth estimation as a key factor.", "method": "Systematic analysis on CARLA dataset, mathematical proof of depth error trends, and proposing CHARM3R by averaging depth estimates.", "result": "CHARM3R improves generalization to unseen heights by over 45%, achieving state-of-the-art performance.", "conclusion": "CHARM3R effectively mitigates camera height variations, enhancing monocular 3D detection robustness."}}
{"id": "2508.11353", "pdf": "https://arxiv.org/pdf/2508.11353", "abs": "https://arxiv.org/abs/2508.11353", "authors": ["Han Zhou", "Hongpeng Yin", "Xuanhong Deng", "Yuyu Huang", "Hao Ren"], "title": "Harmonized Gradient Descent for Class Imbalanced Data Stream Online Learning", "categories": ["cs.LG"], "comment": null, "summary": "Many real-world data are sequentially collected over time and often exhibit\nskewed class distributions, resulting in imbalanced data streams. While\nexisting approaches have explored several strategies, such as resampling and\nreweighting, for imbalanced data stream learning, our work distinguishes itself\nby addressing the imbalance problem through training modification, particularly\nfocusing on gradient descent techniques. We introduce the harmonized gradient\ndescent (HGD) algorithm, which aims to equalize the norms of gradients across\ndifferent classes. By ensuring the gradient norm balance, HGD mitigates\nunder-fitting for minor classes and achieves balanced online learning. Notably,\nHGD operates in a streamlined implementation process, requiring no data-buffer,\nextra parameters, or prior knowledge, making it applicable to any learning\nmodels utilizing gradient descent for optimization. Theoretical analysis, based\non a few common and mild assumptions, shows that HGD achieves a satisfied\nsub-linear regret bound. The proposed algorithm are compared with the commonly\nused online imbalance learning methods under several imbalanced data stream\nscenarios. Extensive experimental evaluations demonstrate the efficiency and\neffectiveness of HGD in learning imbalanced data streams.", "AI": {"tldr": "The paper introduces the Harmonized Gradient Descent (HGD) algorithm to address imbalanced data streams by equalizing gradient norms across classes, achieving balanced online learning without extra resources.", "motivation": "Real-world data streams are often imbalanced, and existing methods like resampling or reweighting are limited. The paper aims to solve this through training modification, specifically gradient descent techniques.", "method": "Proposes HGD, which balances gradient norms across classes during training, ensuring no under-fitting for minor classes. It requires no data-buffer, extra parameters, or prior knowledge.", "result": "Theoretical analysis shows HGD achieves a sub-linear regret bound. Experiments confirm its efficiency and effectiveness compared to existing methods.", "conclusion": "HGD is a streamlined, effective solution for imbalanced data stream learning, applicable to any gradient descent-based model."}}
{"id": "2508.11598", "pdf": "https://arxiv.org/pdf/2508.11598", "abs": "https://arxiv.org/abs/2508.11598", "authors": ["Greta Tuckute", "Klemen Kotar", "Evelina Fedorenko", "Daniel L. K. Yamins"], "title": "Representing Speech Through Autoregressive Prediction of Cochlear Tokens", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "We introduce AuriStream, a biologically inspired model for encoding speech\nvia a two-stage framework inspired by the human auditory processing hierarchy.\nThe first stage transforms raw audio into a time-frequency representation based\non the human cochlea, from which we extract discrete \\textbf{cochlear tokens}.\nThe second stage applies an autoregressive sequence model over the cochlear\ntokens. AuriStream learns meaningful phoneme and word representations, and\nstate-of-the-art lexical semantics. AuriStream shows competitive performance on\ndiverse downstream SUPERB speech tasks. Complementing AuriStream's strong\nrepresentational capabilities, it generates continuations of audio which can be\nvisualized in a spectrogram space and decoded back into audio, providing\ninsights into the model's predictions. In summary, we present a two-stage\nframework for speech representation learning to advance the development of more\nhuman-like models that efficiently handle a range of speech-based tasks.", "AI": {"tldr": "AuriStream is a biologically inspired two-stage model for speech encoding, achieving competitive performance on speech tasks and generating interpretable audio continuations.", "motivation": "To develop a human-like model for speech representation learning by mimicking the auditory processing hierarchy.", "method": "A two-stage framework: (1) transforms raw audio into cochlear tokens, (2) applies an autoregressive sequence model.", "result": "Achieves state-of-the-art lexical semantics and competitive performance on SUPERB tasks; generates interpretable audio continuations.", "conclusion": "AuriStream advances human-like speech models, efficiently handling diverse speech tasks with strong representational and generative capabilities."}}
{"id": "2508.11192", "pdf": "https://arxiv.org/pdf/2508.11192", "abs": "https://arxiv.org/abs/2508.11192", "authors": ["Lavisha Aggarwal", "Vikas Bahirwani", "Lin Li", "Andrea Colaco"], "title": "Generating Dialogues from Egocentric Instructional Videos for Task Assistance: Dataset, Method and Benchmark", "categories": ["cs.CV"], "comment": null, "summary": "Many everyday tasks ranging from fixing appliances, cooking recipes to car\nmaintenance require expert knowledge, especially when tasks are complex and\nmulti-step. Despite growing interest in AI agents, there is a scarcity of\ndialogue-video datasets grounded for real world task assistance. In this paper,\nwe propose a simple yet effective approach that transforms single-person\ninstructional videos into task-guidance two-person dialogues, aligned with fine\ngrained steps and video-clips. Our fully automatic approach, powered by large\nlanguage models, offers an efficient alternative to the substantial cost and\neffort required for human-assisted data collection. Using this technique, we\nbuild HowToDIV, a large-scale dataset containing 507 conversations, 6636\nquestion-answer pairs and 24 hours of videoclips across diverse tasks in\ncooking, mechanics, and planting. Each session includes multi-turn conversation\nwhere an expert teaches a novice user how to perform a task step by step, while\nobserving user's surrounding through a camera and microphone equipped wearable\ndevice. We establish the baseline benchmark performance on HowToDIV dataset\nthrough Gemma-3 model for future research on this new task of dialogues for\nprocedural-task assistance.", "AI": {"tldr": "The paper introduces HowToDIV, a dataset created by transforming single-person instructional videos into two-person dialogues using large language models, for procedural-task assistance.", "motivation": "There's a lack of dialogue-video datasets for real-world task assistance, despite the need for expert knowledge in complex tasks.", "method": "An automatic approach using large language models converts instructional videos into task-guidance dialogues, aligned with fine-grained steps and video clips.", "result": "The HowToDIV dataset includes 507 conversations, 6636 QA pairs, and 24 hours of video clips across diverse tasks.", "conclusion": "The dataset and baseline performance (using Gemma-3) aim to advance research in procedural-task assistance dialogues."}}
{"id": "2508.11356", "pdf": "https://arxiv.org/pdf/2508.11356", "abs": "https://arxiv.org/abs/2508.11356", "authors": ["Jia Liu", "ChangYi He", "YingQiao Lin", "MingMin Yang", "FeiYang Shen", "ShaoGuo Liu", "TingTing Gao"], "title": "ETTRL: Balancing Exploration and Exploitation in LLM Test-Time Reinforcement Learning Via Entropy Mechanism", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Recent advancements in Large Language Models have yielded significant\nimprovements in complex reasoning tasks such as mathematics and programming.\nHowever, these models remain heavily dependent on annotated data and exhibit\nlimited adaptability in unsupervised scenarios. To address these limitations,\ntest-time reinforcement learning (TTRL) has been proposed, which enables\nself-optimization by leveraging model-generated pseudo-labels. Despite its\npromise, TTRL faces several key challenges, including high inference costs due\nto parallel rollouts and early-stage estimation bias that fosters\noverconfidence, reducing output diversity and causing performance plateaus. To\naddress these challenges, we introduce an entropy-based mechanism to enhance\nthe exploration-exploitation balance in test-time reinforcement learning\nthrough two strategies: Entropy-fork Tree Majority Rollout (ETMR) and\nEntropy-based Advantage Reshaping (EAR). Compared with the baseline, our\napproach enables Llama3.1-8B to achieve a 68 percent relative improvement in\nPass at 1 metric on the AIME 2024 benchmark, while consuming only 60 percent of\nthe rollout tokens budget. This highlights our method's ability to effectively\noptimize the trade-off between inference efficiency, diversity, and estimation\nrobustness, thereby advancing unsupervised reinforcement learning for\nopen-domain reasoning tasks.", "AI": {"tldr": "The paper introduces entropy-based strategies (ETMR and EAR) to improve test-time reinforcement learning (TTRL) in LLMs, addressing challenges like high inference costs and overconfidence. Results show significant performance gains with reduced resource usage.", "motivation": "Current LLMs rely on annotated data and struggle in unsupervised settings. TTRL offers self-optimization but faces issues like high costs and overconfidence, limiting its effectiveness.", "method": "Proposes ETMR (Entropy-fork Tree Majority Rollout) and EAR (Entropy-based Advantage Reshaping) to balance exploration-exploitation in TTRL.", "result": "Achieves a 68% relative improvement in Pass at 1 on AIME 2024 benchmark with 60% fewer rollout tokens compared to baseline.", "conclusion": "The entropy-based approach enhances TTRL by improving efficiency, diversity, and robustness, advancing unsupervised learning for open-domain reasoning."}}
{"id": "2508.11605", "pdf": "https://arxiv.org/pdf/2508.11605", "abs": "https://arxiv.org/abs/2508.11605", "authors": ["Rob Reijtenbach", "Suzan Verberne", "Gijs Wijnholds"], "title": "Dataset Creation for Visual Entailment using Generative AI", "categories": ["cs.CL"], "comment": "NALOMA: Natural Logic meets Machine Learning workshop @ ESSLLI 2025", "summary": "In this paper we present and validate a new synthetic dataset for training\nvisual entailment models. Existing datasets for visual entailment are small and\nsparse compared to datasets for textual entailment. Manually creating datasets\nis labor-intensive. We base our synthetic dataset on the SNLI dataset for\ntextual entailment. We take the premise text from SNLI as input prompts in a\ngenerative image model, Stable Diffusion, creating an image to replace each\ntextual premise. We evaluate our dataset both intrinsically and extrinsically.\nFor extrinsic evaluation, we evaluate the validity of the generated images by\nusing them as training data for a visual entailment classifier based on CLIP\nfeature vectors. We find that synthetic training data only leads to a slight\ndrop in quality on SNLI-VE, with an F-score 0.686 compared to 0.703 when\ntrained on real data. We also compare the quality of our generated training\ndata to original training data on another dataset: SICK-VTE. Again, there is\nonly a slight drop in F-score: from 0.400 to 0.384. These results indicate that\nin settings with data sparsity, synthetic data can be a promising solution for\ntraining visual entailment models.", "AI": {"tldr": "A new synthetic dataset for visual entailment is created using SNLI textual premises and Stable Diffusion for image generation, showing minimal performance drop compared to real data.", "motivation": "Existing visual entailment datasets are small and sparse, and manual creation is labor-intensive.", "method": "Premises from SNLI are used as prompts in Stable Diffusion to generate images, evaluated intrinsically and extrinsically with CLIP-based classifiers.", "result": "Synthetic data shows slight F-score drops: 0.686 vs. 0.703 on SNLI-VE, and 0.384 vs. 0.400 on SICK-VTE.", "conclusion": "Synthetic data is a viable solution for training visual entailment models in data-sparse settings."}}
{"id": "2508.11052", "pdf": "https://arxiv.org/pdf/2508.11052", "abs": "https://arxiv.org/abs/2508.11052", "authors": ["Evey Jiaxin Huang", "Matthew Easterday", "Elizabeth Gerber"], "title": "AI That Helps Us Help Each Other: A Proactive System for Scaffolding Mentor-Novice Collaboration in Entrepreneurship Coaching", "categories": ["cs.HC", "cs.AI", "68T35 (Primary), 68U99 (Secondary)", "H.5.2"], "comment": "To appear in CSCW 2025 Volume 9", "summary": "Entrepreneurship requires navigating open-ended, ill-defined problems:\nidentifying risks, challenging assumptions, and making strategic decisions\nunder deep uncertainty. Novice founders often struggle with these metacognitive\ndemands, while mentors face limited time and visibility to provide tailored\nsupport. We present a human-AI coaching system that combines a domain-specific\ncognitive model of entrepreneurial risk with a large language model (LLM) to\nproactively scaffold both novice and mentor thinking. The system proactively\nposes diagnostic questions that challenge novices' thinking and helps both\nnovices and mentors plan for more focused and emotionally attuned meetings.\nCritically, mentors can inspect and modify the underlying cognitive model,\nshaping the logic of the system to reflect their evolving needs. Through an\nexploratory field deployment, we found that using the system supported novice\nmetacognition, helped mentors plan emotionally attuned strategies, and improved\nmeeting depth, intentionality, and focus--while also surfaced key tensions\naround trust, misdiagnosis, and expectations of AI. We contribute design\nprinciples for proactive AI systems that scaffold metacognition and human-human\ncollaboration in complex, ill-defined domains, offering implications for\nsimilar domains like healthcare, education, and knowledge work.", "AI": {"tldr": "A human-AI coaching system aids novice entrepreneurs and mentors by scaffolding metacognition and improving meeting focus through proactive questions and adaptable cognitive models.", "motivation": "Novice entrepreneurs struggle with metacognitive demands, and mentors lack time for tailored support. The system aims to bridge this gap.", "method": "Combines a domain-specific cognitive model of entrepreneurial risk with a large language model (LLM) to proactively scaffold thinking and improve meetings.", "result": "Improved novice metacognition, mentor planning, and meeting depth, though tensions around AI trust and misdiagnosis emerged.", "conclusion": "Design principles for proactive AI systems in ill-defined domains like entrepreneurship, with broader implications for healthcare and education."}}
{"id": "2508.11196", "pdf": "https://arxiv.org/pdf/2508.11196", "abs": "https://arxiv.org/abs/2508.11196", "authors": ["Jiajin Guan", "Haibo Mei", "Bonan Zhang", "Dan Liu", "Yuanshuang Fu", "Yue Zhang"], "title": "UAV-VL-R1: Generalizing Vision-Language Models via Supervised Fine-Tuning and Multi-Stage GRPO for UAV Visual Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in vision-language models (VLMs) have demonstrated strong\ngeneralization in natural image tasks. However, their performance often\ndegrades on unmanned aerial vehicle (UAV)-based aerial imagery, which features\nhigh resolution, complex spatial semantics, and strict real-time constraints.\nThese challenges limit the applicability of general-purpose VLMs to structured\naerial reasoning tasks. To address these challenges, we propose UAV-VL-R1, a\nlightweight VLM explicitly designed for aerial visual reasoning. It is trained\nusing a hybrid method that combines supervised fine-tuning (SFT) and\nmulti-stage reinforcement learning (RL). We leverage the group relative policy\noptimization (GRPO) algorithm to promote structured and interpretable reasoning\nthrough rule-guided rewards and intra-group policy alignment. To support model\ntraining and evaluation, we introduce a high-resolution visual question\nanswering dataset named HRVQA-VL, which consists of 50,019 annotated samples\ncovering eight UAV-relevant reasoning tasks, including object counting,\ntransportation recognition, and spatial scene inference. Experimental results\nshow that UAV-VL-R1 achieves a 48.17% higher zero-shot accuracy than the\nQwen2-VL-2B-Instruct baseline and even outperforms its 72B-scale variant, which\nis 36x larger, on multiple tasks. Ablation studies reveal that while SFT\nimproves semantic alignment, it may reduce reasoning diversity in mathematical\ntasks. GRPO-based RL compensates for this limitation by enhancing logical\nflexibility and the robustness of inference. Additionally, UAV-VL-R1 requires\nonly 3.9GB of memory under FP16 inference and can be quantized to 2.5GB with\nINT8, supporting real-time deployment on resource-constrained UAV platforms.", "AI": {"tldr": "UAV-VL-R1 is a lightweight vision-language model for aerial imagery, combining supervised fine-tuning and reinforcement learning, outperforming larger models and enabling real-time deployment.", "motivation": "General-purpose VLMs struggle with UAV-based aerial imagery due to high resolution, complex semantics, and real-time constraints.", "method": "Hybrid training with supervised fine-tuning (SFT) and multi-stage reinforcement learning (RL) using GRPO algorithm.", "result": "Achieves 48.17% higher zero-shot accuracy than baseline and outperforms larger models; supports real-time deployment with low memory usage.", "conclusion": "UAV-VL-R1 addresses aerial reasoning challenges effectively, balancing performance and efficiency."}}
{"id": "2508.11357", "pdf": "https://arxiv.org/pdf/2508.11357", "abs": "https://arxiv.org/abs/2508.11357", "authors": ["Changhong Jing", "Yan Liu", "Shuqiang Wang", "Bruce X. B. Yu", "Gong Chen", "Zhejing Hu", "Zhi Zhang", "Yanyan Shen"], "title": "PTSM: Physiology-aware and Task-invariant Spatio-temporal Modeling for Cross-Subject EEG Decoding", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Cross-subject electroencephalography (EEG) decoding remains a fundamental\nchallenge in brain-computer interface (BCI) research due to substantial\ninter-subject variability and the scarcity of subject-invariant\nrepresentations. This paper proposed PTSM (Physiology-aware and Task-invariant\nSpatio-temporal Modeling), a novel framework for interpretable and robust EEG\ndecoding across unseen subjects. PTSM employs a dual-branch masking mechanism\nthat independently learns personalized and shared spatio-temporal patterns,\nenabling the model to preserve individual-specific neural characteristics while\nextracting task-relevant, population-shared features. The masks are factorized\nacross temporal and spatial dimensions, allowing fine-grained modulation of\ndynamic EEG patterns with low computational overhead. To further address\nrepresentational entanglement, PTSM enforces information-theoretic constraints\nthat decompose latent embeddings into orthogonal task-related and\nsubject-related subspaces. The model is trained end-to-end via a\nmulti-objective loss integrating classification, contrastive, and\ndisentanglement objectives. Extensive experiments on cross-subject motor\nimagery datasets demonstrate that PTSM achieves strong zero-shot\ngeneralization, outperforming state-of-the-art baselines without\nsubject-specific calibration. Results highlight the efficacy of disentangled\nneural representations for achieving both personalized and transferable\ndecoding in non-stationary neurophysiological settings.", "AI": {"tldr": "PTSM is a novel EEG decoding framework that learns personalized and shared spatio-temporal patterns for robust cross-subject decoding, outperforming state-of-the-art methods without subject-specific calibration.", "motivation": "Addressing the challenge of cross-subject EEG decoding due to inter-subject variability and lack of subject-invariant representations.", "method": "Uses a dual-branch masking mechanism to learn personalized and shared patterns, factorized across spatial and temporal dimensions, with information-theoretic constraints for disentanglement.", "result": "Achieves strong zero-shot generalization on motor imagery datasets, outperforming baselines.", "conclusion": "PTSM demonstrates efficacy in disentangling neural representations for personalized and transferable EEG decoding."}}
{"id": "2508.11607", "pdf": "https://arxiv.org/pdf/2508.11607", "abs": "https://arxiv.org/abs/2508.11607", "authors": ["Christopher J. Agostino"], "title": "TinyTim: A Family of Language Models for Divergent Generation", "categories": ["cs.CL"], "comment": "7 pages, 3 figures, submitted to NeurIPS Creative AI track, code and\n  model available at https://hf.co/npc-worldwide/TinyTimV1", "summary": "This work introduces TinyTim, a family of large language models fine-tuned on\nJames Joyce's `Finnegans Wake'. Through quantitative evaluation against\nbaseline models, we demonstrate that TinyTim V1 produces a statistically\ndistinct generative profile characterized by high lexical diversity and low\nsemantic coherence. These findings are interpreted through theories of\ncreativity and complex problem-solving, arguing that such specialized models\ncan function as divergent knowledge sources within more extensive creative\narchitectures, powering automated discovery mechanisms in diverse settings.", "AI": {"tldr": "TinyTim, a model fine-tuned on 'Finnegans Wake,' shows high lexical diversity but low semantic coherence, suggesting its potential as a divergent knowledge source in creative systems.", "motivation": "To explore how specialized language models can enhance creativity and problem-solving by generating distinct outputs.", "method": "Fine-tuning large language models on 'Finnegans Wake' and evaluating their generative profiles quantitatively.", "result": "TinyTim V1 exhibits high lexical diversity and low semantic coherence, differing from baseline models.", "conclusion": "Specialized models like TinyTim can serve as divergent knowledge sources in creative architectures, aiding automated discovery."}}
{"id": "2508.11074", "pdf": "https://arxiv.org/pdf/2508.11074", "abs": "https://arxiv.org/abs/2508.11074", "authors": ["Haomin Zhang", "Kristin Qi", "Shuxin Yang", "Zihao Chen", "Chaofan Ding", "Xinhan Di"], "title": "LD-LAudio-V1: Video-to-Long-Form-Audio Generation Extension with Dual Lightweight Adapters", "categories": ["cs.SD", "cs.AI", "cs.CV", "eess.AS"], "comment": "Gen4AVC@ICCV: 1st Workshop on Generative AI for Audio-Visual Content\n  Creation", "summary": "Generating high-quality and temporally synchronized audio from video content\nis essential for video editing and post-production tasks, enabling the creation\nof semantically aligned audio for silent videos. However, most existing\napproaches focus on short-form audio generation for video segments under 10\nseconds or rely on noisy datasets for long-form video-to-audio zsynthesis. To\naddress these limitations, we introduce LD-LAudio-V1, an extension of\nstate-of-the-art video-to-audio models and it incorporates dual lightweight\nadapters to enable long-form audio generation. In addition, we release a clean\nand human-annotated video-to-audio dataset that contains pure sound effects\nwithout noise or artifacts. Our method significantly reduces splicing artifacts\nand temporal inconsistencies while maintaining computational efficiency.\nCompared to direct fine-tuning with short training videos, LD-LAudio-V1\nachieves significant improvements across multiple metrics: $FD_{\\text{passt}}$\n450.00 $\\rightarrow$ 327.29 (+27.27%), $FD_{\\text{panns}}$ 34.88 $\\rightarrow$\n22.68 (+34.98%), $FD_{\\text{vgg}}$ 3.75 $\\rightarrow$ 1.28 (+65.87%),\n$KL_{\\text{panns}}$ 2.49 $\\rightarrow$ 2.07 (+16.87%), $KL_{\\text{passt}}$ 1.78\n$\\rightarrow$ 1.53 (+14.04%), $IS_{\\text{panns}}$ 4.17 $\\rightarrow$ 4.30\n(+3.12%), $IB_{\\text{score}}$ 0.25 $\\rightarrow$ 0.28 (+12.00%),\n$Energy\\Delta10\\text{ms}$ 0.3013 $\\rightarrow$ 0.1349 (+55.23%),\n$Energy\\Delta10\\text{ms(vs.GT)}$ 0.0531 $\\rightarrow$ 0.0288 (+45.76%), and\n$Sem.\\,Rel.$ 2.73 $\\rightarrow$ 3.28 (+20.15%). Our dataset aims to facilitate\nfurther research in long-form video-to-audio generation and is available at\nhttps://github.com/deepreasonings/long-form-video2audio.", "AI": {"tldr": "LD-LAudio-V1 introduces dual lightweight adapters for long-form video-to-audio generation, reducing artifacts and inconsistencies while improving metrics. A clean dataset is also released.", "motivation": "Existing methods focus on short-form audio or use noisy datasets, limiting quality and synchronization for long-form video-to-audio tasks.", "method": "Extends state-of-the-art models with dual lightweight adapters and releases a clean, annotated dataset for training.", "result": "Significant improvements in metrics (e.g., FD, KL, IS) and reduced artifacts, with computational efficiency maintained.", "conclusion": "LD-LAudio-V1 advances long-form audio generation, supported by a high-quality dataset for future research."}}
{"id": "2508.11212", "pdf": "https://arxiv.org/pdf/2508.11212", "abs": "https://arxiv.org/abs/2508.11212", "authors": ["Zhangjian Ji", "Wenjin Zhang", "Shaotong Qiao", "Kai Feng", "Yuhua Qian"], "title": "A Coarse-to-Fine Human Pose Estimation Method based on Two-stage Distillation and Progressive Graph Neural Network", "categories": ["cs.CV"], "comment": null, "summary": "Human pose estimation has been widely applied in the human-centric\nunderstanding and generation, but most existing state-of-the-art human pose\nestimation methods require heavy computational resources for accurate\npredictions. In order to obtain an accurate, robust yet lightweight human pose\nestimator, one feasible way is to transfer pose knowledge from a powerful\nteacher model to a less-parameterized student model by knowledge distillation.\nHowever, the traditional knowledge distillation framework does not fully\nexplore the contextual information among human joints. Thus, in this paper, we\npropose a novel coarse-to-fine two-stage knowledge distillation framework for\nhuman pose estimation. In the first-stage distillation, we introduce the human\njoints structure loss to mine the structural information among human joints so\nas to transfer high-level semantic knowledge from the teacher model to the\nstudent model. In the second-stage distillation, we utilize an Image-Guided\nProgressive Graph Convolutional Network (IGP-GCN) to refine the initial human\npose obtained from the first-stage distillation and supervise the training of\nthe IGP-GCN in the progressive way by the final output pose of teacher model.\nThe extensive experiments on the benchmark dataset: COCO keypoint and CrowdPose\ndatasets, show that our proposed method performs favorably against lots of the\nexisting state-of-the-art human pose estimation methods, especially for the\nmore complex CrowdPose dataset, the performance improvement of our model is\nmore significant.", "AI": {"tldr": "A novel two-stage knowledge distillation framework improves lightweight human pose estimation by leveraging structural and contextual joint information.", "motivation": "Existing pose estimation methods are computationally heavy; knowledge distillation can transfer accuracy to lightweight models but lacks joint contextual exploration.", "method": "Proposes a coarse-to-fine two-stage distillation: first-stage uses joint structure loss for semantic transfer, second-stage refines poses with an Image-Guided Progressive GCN.", "result": "Outperforms state-of-the-art methods on COCO keypoint and CrowdPose datasets, with notable gains on complex CrowdPose.", "conclusion": "The framework effectively balances accuracy and efficiency, advancing lightweight pose estimation."}}
{"id": "2508.11363", "pdf": "https://arxiv.org/pdf/2508.11363", "abs": "https://arxiv.org/abs/2508.11363", "authors": ["Sadegh Khorasani", "Saber Salehkaleybar", "Negar Kiyavash", "Matthias Grossglauser"], "title": "Fusing Rewards and Preferences in Reinforcement Learning", "categories": ["cs.LG", "I.2.6"], "comment": null, "summary": "We present Dual-Feedback Actor (DFA), a reinforcement learning algorithm that\nfuses both individual rewards and pairwise preferences (if available) into a\nsingle update rule. DFA uses the policy's log-probabilities directly to model\nthe preference probability, avoiding a separate reward-modeling step.\nPreferences can be provided by human-annotators (at state-level or\ntrajectory-level) or be synthesized online from Q-values stored in an\noff-policy replay buffer. Under a Bradley-Terry model, we prove that minimizing\nDFA's preference loss recovers the entropy-regularized Soft Actor-Critic (SAC)\npolicy. Our simulation results show that DFA trained on generated preferences\nmatches or exceeds SAC on six control environments and demonstrates a more\nstable training process. With only a semi-synthetic preference dataset under\nBradley-Terry model, our algorithm outperforms reward-modeling reinforcement\nlearning from human feedback (RLHF) baselines in a stochastic GridWorld and\napproaches the performance of an oracle with true rewards.", "AI": {"tldr": "DFA is a reinforcement learning algorithm combining individual rewards and pairwise preferences into a single update rule, outperforming SAC and RLHF baselines in simulations.", "motivation": "To integrate both individual rewards and pairwise preferences into reinforcement learning, avoiding separate reward-modeling steps and improving stability and performance.", "method": "DFA uses policy log-probabilities to model preference probability, leveraging human or synthesized preferences under a Bradley-Terry model.", "result": "DFA matches or exceeds SAC in control environments and outperforms RLHF baselines in a stochastic GridWorld, approaching oracle performance with true rewards.", "conclusion": "DFA effectively combines rewards and preferences, offering stable training and superior performance over existing methods."}}
{"id": "2506.20844", "pdf": "https://arxiv.org/pdf/2506.20844", "abs": "https://arxiv.org/abs/2506.20844", "authors": ["Xingyu Deng", "Xi Wang", "Mark Stevenson"], "title": "The Next Phase of Scientific Fact-Checking: Advanced Evidence Retrieval from Complex Structured Academic Papers", "categories": ["cs.IR", "cs.CL"], "comment": "Accepted for ACM SIGIR Conference on Innovative Concepts and Theories\n  in Information Retrieval (ICTIR'25)", "summary": "Scientific fact-checking aims to determine the veracity of scientific claims\nby retrieving and analysing evidence from research literature. The problem is\ninherently more complex than general fact-checking since it must accommodate\nthe evolving nature of scientific knowledge, the structural complexity of\nacademic literature and the challenges posed by long-form, multimodal\nscientific expression. However, existing approaches focus on simplified\nversions of the problem based on small-scale datasets consisting of abstracts\nrather than full papers, thereby avoiding the distinct challenges associated\nwith processing complete documents. This paper examines the limitations of\ncurrent scientific fact-checking systems and reveals the many potential\nfeatures and resources that could be exploited to advance their performance. It\nidentifies key research challenges within evidence retrieval, including (1)\nevidence-driven retrieval that addresses semantic limitations and topic\nimbalance (2) time-aware evidence retrieval with citation tracking to mitigate\noutdated information, (3) structured document parsing to leverage long-range\ncontext, (4) handling complex scientific expressions, including tables,\nfigures, and domain-specific terminology and (5) assessing the credibility of\nscientific literature. Preliminary experiments were conducted to substantiate\nthese challenges and identify potential solutions. This perspective paper aims\nto advance scientific fact-checking with a specialised IR system tailored for\nreal-world applications.", "AI": {"tldr": "The paper highlights the complexities of scientific fact-checking, critiques existing simplified approaches, and proposes advanced methods to address challenges like evidence retrieval, outdated information, and multimodal content.", "motivation": "To address the limitations of current scientific fact-checking systems by exploiting untapped features and resources, advancing performance for real-world applications.", "method": "Identifies key challenges (evidence-driven retrieval, time-aware retrieval, structured parsing, handling multimodal content, credibility assessment) and conducts preliminary experiments to explore solutions.", "result": "Preliminary experiments substantiate the identified challenges and suggest potential solutions for improving scientific fact-checking systems.", "conclusion": "The paper advocates for a specialized IR system to enhance scientific fact-checking, addressing real-world complexities and advancing the field."}}
{"id": "2508.11218", "pdf": "https://arxiv.org/pdf/2508.11218", "abs": "https://arxiv.org/abs/2508.11218", "authors": ["Jialin Li", "Shuqi Wu", "Ning Wang"], "title": "A CLIP-based Uncertainty Modal Modeling (UMM) Framework for Pedestrian Re-Identification in Autonomous Driving", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Re-Identification (ReID) is a critical technology in intelligent perception\nsystems, especially within autonomous driving, where onboard cameras must\nidentify pedestrians across views and time in real-time to support safe\nnavigation and trajectory prediction. However, the presence of uncertain or\nmissing input modalities--such as RGB, infrared, sketches, or textual\ndescriptions--poses significant challenges to conventional ReID approaches.\nWhile large-scale pre-trained models offer strong multimodal semantic modeling\ncapabilities, their computational overhead limits practical deployment in\nresource-constrained environments. To address these challenges, we propose a\nlightweight Uncertainty Modal Modeling (UMM) framework, which integrates a\nmultimodal token mapper, synthetic modality augmentation strategy, and\ncross-modal cue interactive learner. Together, these components enable unified\nfeature representation, mitigate the impact of missing modalities, and extract\ncomplementary information across different data types. Additionally, UMM\nleverages CLIP's vision-language alignment ability to fuse multimodal inputs\nefficiently without extensive finetuning. Experimental results demonstrate that\nUMM achieves strong robustness, generalization, and computational efficiency\nunder uncertain modality conditions, offering a scalable and practical solution\nfor pedestrian re-identification in autonomous driving scenarios.", "AI": {"tldr": "A lightweight Uncertainty Modal Modeling (UMM) framework is proposed for pedestrian ReID in autonomous driving, addressing missing modalities and computational constraints.", "motivation": "Challenges in ReID due to uncertain/missing input modalities and computational inefficiency of large pre-trained models.", "method": "UMM integrates a multimodal token mapper, synthetic modality augmentation, and cross-modal cue interactive learner, leveraging CLIP for efficient fusion.", "result": "UMM achieves robustness, generalization, and computational efficiency under uncertain modality conditions.", "conclusion": "UMM provides a scalable and practical solution for pedestrian ReID in autonomous driving."}}
{"id": "2508.11365", "pdf": "https://arxiv.org/pdf/2508.11365", "abs": "https://arxiv.org/abs/2508.11365", "authors": ["Jayanta Mandi", "Ali \u0130rfan Mahmuto\u011fullar\u0131", "Senne Berden", "Tias Guns"], "title": "Minimizing Surrogate Losses for Decision-Focused Learning using Differentiable Optimization", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Decision-focused learning (DFL) trains a machine learning (ML) model to\npredict parameters of an optimization problem, to directly minimize decision\nregret, i.e., maximize decision quality. Gradient-based DFL requires computing\nthe derivative of the solution to the optimization problem with respect to the\npredicted parameters. However, for many optimization problems, such as linear\nprograms (LPs), the gradient of the regret with respect to the predicted\nparameters is zero almost everywhere. Existing gradient-based DFL approaches\nfor LPs try to circumvent this issue in one of two ways: (a) smoothing the LP\ninto a differentiable optimization problem by adding a quadratic regularizer\nand then minimizing the regret directly or (b) minimizing surrogate losses that\nhave informative (sub)gradients. In this paper, we show that the former\napproach still results in zero gradients, because even after smoothing the\nregret remains constant across large regions of the parameter space. To address\nthis, we propose minimizing surrogate losses -- even when a differentiable\noptimization layer is used and regret can be minimized directly. Our\nexperiments demonstrate that minimizing surrogate losses allows differentiable\noptimization layers to achieve regret comparable to or better than\nsurrogate-loss based DFL methods. Further, we demonstrate that this also holds\nfor DYS-Net, a recently proposed differentiable optimization technique for LPs,\nthat computes approximate solutions and gradients through operations that can\nbe performed using feedforward neural network layers. Because DYS-Net executes\nthe forward and the backward pass very efficiently, by minimizing surrogate\nlosses using DYS-Net, we are able to attain regret on par with the\nstate-of-the-art while reducing training time by a significant margin.", "AI": {"tldr": "Decision-focused learning (DFL) trains ML models to predict optimization problem parameters, aiming to minimize decision regret. The paper addresses gradient issues in DFL for linear programs (LPs) and proposes using surrogate losses for better performance and efficiency.", "motivation": "Gradient-based DFL for LPs faces zero-gradient issues, limiting direct regret minimization. Existing methods (smoothing or surrogate losses) have shortcomings, prompting the need for a better approach.", "method": "The paper proposes minimizing surrogate losses even when using differentiable optimization layers. It evaluates this approach with DYS-Net, a technique for efficient forward and backward passes in LPs.", "result": "Experiments show surrogate losses with differentiable optimization layers achieve comparable or better regret than surrogate-loss methods. DYS-Net further reduces training time significantly.", "conclusion": "Minimizing surrogate losses with differentiable optimization layers improves regret and efficiency, making DFL more practical for LPs."}}
{"id": "2508.11093", "pdf": "https://arxiv.org/pdf/2508.11093", "abs": "https://arxiv.org/abs/2508.11093", "authors": ["Cesar Alan Contreras", "Manolis Chiou", "Alireza Rastegarpanah", "Michal Szulik", "Rustam Stolkin"], "title": "Utilizing Vision-Language Models as Action Models for Intent Recognition and Assistance", "categories": ["cs.RO", "cs.AI", "cs.HC"], "comment": "Accepted at Human-Centered Robot Autonomy for Human-Robot Teams\n  (HuRoboT) at IEEE RO-MAN 2025, Eindhoven, the Netherlands", "summary": "Human-robot collaboration requires robots to quickly infer user intent,\nprovide transparent reasoning, and assist users in achieving their goals. Our\nrecent work introduced GUIDER, our framework for inferring navigation and\nmanipulation intents. We propose augmenting GUIDER with a vision-language model\n(VLM) and a text-only language model (LLM) to form a semantic prior that\nfilters objects and locations based on the mission prompt. A vision pipeline\n(YOLO for object detection and the Segment Anything Model for instance\nsegmentation) feeds candidate object crops into the VLM, which scores their\nrelevance given an operator prompt; in addition, the list of detected object\nlabels is ranked by a text-only LLM. These scores weight the existing\nnavigation and manipulation layers of GUIDER, selecting context-relevant\ntargets while suppressing unrelated objects. Once the combined belief exceeds a\nthreshold, autonomy changes occur, enabling the robot to navigate to the\ndesired area and retrieve the desired object, while adapting to any changes in\nthe operator's intent. Future work will evaluate the system on Isaac Sim using\na Franka Emika arm on a Ridgeback base, with a focus on real-time assistance.", "AI": {"tldr": "GUIDER framework is enhanced with vision-language and text-only language models to improve human-robot collaboration by inferring intent and filtering context-relevant objects.", "motivation": "To enable robots to quickly infer user intent, provide transparent reasoning, and assist users effectively in navigation and manipulation tasks.", "method": "Augments GUIDER with a vision-language model (VLM) and a text-only LLM to form a semantic prior. Uses YOLO and Segment Anything Model for object detection and segmentation, then scores relevance with VLM and LLM.", "result": "The combined belief system selects context-relevant targets, enabling autonomous navigation and object retrieval while adapting to intent changes.", "conclusion": "Future work will test the system in Isaac Sim with a Franka Emika arm and Ridgeback base, focusing on real-time assistance."}}
{"id": "2508.11255", "pdf": "https://arxiv.org/pdf/2508.11255", "abs": "https://arxiv.org/abs/2508.11255", "authors": ["MengChao Wang", "Qiang Wang", "Fan Jiang", "Mu Xu"], "title": "FantasyTalking2: Timestep-Layer Adaptive Preference Optimization for Audio-Driven Portrait Animation", "categories": ["cs.CV"], "comment": "https://fantasy-amap.github.io/fantasy-talking2/", "summary": "Recent advances in audio-driven portrait animation have demonstrated\nimpressive capabilities. However, existing methods struggle to align with\nfine-grained human preferences across multiple dimensions, such as motion\nnaturalness, lip-sync accuracy, and visual quality. This is due to the\ndifficulty of optimizing among competing preference objectives, which often\nconflict with one another, and the scarcity of large-scale, high-quality\ndatasets with multidimensional preference annotations. To address these, we\nfirst introduce Talking-Critic, a multimodal reward model that learns\nhuman-aligned reward functions to quantify how well generated videos satisfy\nmultidimensional expectations. Leveraging this model, we curate Talking-NSQ, a\nlarge-scale multidimensional human preference dataset containing 410K\npreference pairs. Finally, we propose Timestep-Layer adaptive multi-expert\nPreference Optimization (TLPO), a novel framework for aligning diffusion-based\nportrait animation models with fine-grained, multidimensional preferences. TLPO\ndecouples preferences into specialized expert modules, which are then fused\nacross timesteps and network layers, enabling comprehensive, fine-grained\nenhancement across all dimensions without mutual interference. Experiments\ndemonstrate that Talking-Critic significantly outperforms existing methods in\naligning with human preference ratings. Meanwhile, TLPO achieves substantial\nimprovements over baseline models in lip-sync accuracy, motion naturalness, and\nvisual quality, exhibiting superior performance in both qualitative and\nquantitative evaluations. Ours project page:\nhttps://fantasy-amap.github.io/fantasy-talking2/", "AI": {"tldr": "The paper introduces Talking-Critic and TLPO to align audio-driven portrait animation with fine-grained human preferences, addressing conflicts in objectives and dataset scarcity.", "motivation": "Existing methods fail to align with fine-grained human preferences due to conflicting objectives and lack of annotated datasets.", "method": "Proposes Talking-Critic for reward modeling, curates Talking-NSQ dataset, and introduces TLPO for preference optimization in diffusion models.", "result": "Talking-Critic outperforms existing methods; TLPO improves lip-sync, motion naturalness, and visual quality.", "conclusion": "The approach successfully aligns portrait animation with multidimensional human preferences, demonstrating superior performance."}}
{"id": "2508.11390", "pdf": "https://arxiv.org/pdf/2508.11390", "abs": "https://arxiv.org/abs/2508.11390", "authors": ["Michael Banf", "Dominik Filipiak", "Max Schattauer", "Liliya Imasheva"], "title": "A Remedy for Over-Squashing in Graph Learning via Forman-Ricci Curvature based Graph-to-Hypergraph Structural Lifting", "categories": ["cs.LG"], "comment": null, "summary": "Graph Neural Networks are highly effective at learning from relational data,\nleveraging node and edge features while maintaining the symmetries inherent to\ngraph structures. However, many real-world systems, such as social or\nbiological networks, exhibit complex interactions that are more naturally\nrepresented by higher-order topological domains. The emerging field of\nGeometric and Topological Deep Learning addresses this challenge by introducing\nmethods that utilize and benefit from higher-order structures. Central to TDL\nis the concept of lifting, which transforms data representations from basic\ngraph forms to more expressive topologies before the application of GNN models\nfor learning. In this work, we propose a structural lifting strategy using\nForman-Ricci curvature, which defines an edge-based network characteristic\nbased on Riemannian geometry. Curvature reveals local and global properties of\na graph, such as a network's backbones, i.e. coarse, structure-preserving graph\ngeometries that form connections between major communities - most suitably\nrepresented as hyperedges to model information flows between clusters across\nlarge distances in the network. To this end, our approach provides a remedy to\nthe problem of information distortion in message passing across long distances\nand graph bottlenecks - a phenomenon known in graph learning as over-squashing.", "AI": {"tldr": "The paper proposes a structural lifting strategy using Forman-Ricci curvature to enhance Graph Neural Networks (GNNs) by addressing information distortion in long-distance message passing.", "motivation": "Real-world systems like social or biological networks involve complex interactions requiring higher-order structures, which current GNNs struggle to model effectively.", "method": "The method introduces a lifting technique using Forman-Ricci curvature to transform graph data into more expressive topologies before applying GNNs.", "result": "The approach mitigates over-squashing, improving information flow across graph bottlenecks and long distances.", "conclusion": "The proposed curvature-based lifting strategy enhances GNN performance by better capturing higher-order interactions in complex networks."}}
{"id": "2508.11110", "pdf": "https://arxiv.org/pdf/2508.11110", "abs": "https://arxiv.org/abs/2508.11110", "authors": ["Mukul Singh", "Gust Verbruggen", "Vu Le", "Sumit Gulwani"], "title": "Diffusion is a code repair operator and generator", "categories": ["cs.SE", "cs.AI", "cs.CL"], "comment": "12 pages", "summary": "Code diffusion models generate code by iteratively removing noise from the\nlatent representation of a code snippet. During later steps of the diffusion\nprocess, when the code snippet has almost converged, differences between\ndiscrete representations of these snippets look like last-mile repairs applied\nto broken or incomplete code. We evaluate the extent to which this resemblance\ncan be exploited to leverage pre-trained code diffusion models for the problem\nof last-mile repair by considering two applications with significant potential.\nFirst, we can leverage the diffusion model for last-mile repair by adding noise\nto a broken code snippet and resuming the diffusion process. Second, we can\nleverage the diffusion model to generate arbitrary amount of training data for\nlast-mile repair tasks (that are computationally more efficient) by sampling an\nintermediate program (input) and the final program (output) from the diffusion\nprocess. We perform experiments on 3 domains (Python, Excel and PowerShell) to\nevaluate applications, as well as analyze properties.", "AI": {"tldr": "Code diffusion models can be repurposed for last-mile repair tasks by leveraging their noise-removal process to fix broken code or generate training data.", "motivation": "The paper aims to exploit the resemblance between the later steps of code diffusion and last-mile repairs to enhance code repair tasks.", "method": "The approach involves adding noise to broken code snippets and resuming diffusion, and sampling intermediate and final programs for training data generation.", "result": "Experiments across Python, Excel, and PowerShell domains demonstrate the feasibility and potential of this approach.", "conclusion": "Code diffusion models show promise for last-mile repair applications, offering efficient solutions for code repair and training data generation."}}
{"id": "2508.11256", "pdf": "https://arxiv.org/pdf/2508.11256", "abs": "https://arxiv.org/abs/2508.11256", "authors": ["Junjie Wang", "Keyu Chen", "Yulin Li", "Bin Chen", "Hengshuang Zhao", "Xiaojuan Qi", "Zhuotao Tian"], "title": "Generalized Decoupled Learning for Enhancing Open-Vocabulary Dense Perception", "categories": ["cs.CV", "cs.AI"], "comment": "arXiv admin note: text overlap with arXiv:2505.04410", "summary": "Dense visual perception tasks have been constrained by their reliance on\npredefined categories, limiting their applicability in real-world scenarios\nwhere visual concepts are unbounded. While Vision-Language Models (VLMs) like\nCLIP have shown promise in open-vocabulary tasks, their direct application to\ndense perception often leads to suboptimal performance due to limitations in\nlocal feature representation. In this work, we present our observation that\nCLIP's image tokens struggle to effectively aggregate information from\nspatially or semantically related regions, resulting in features that lack\nlocal discriminability and spatial consistency. To address this issue, we\npropose DeCLIP, a novel framework that enhances CLIP by decoupling the\nself-attention module to obtain ``content'' and ``context'' features\nrespectively. \\revise{The context features are enhanced by jointly distilling\nsemantic correlations from Vision Foundation Models (VFMs) and object integrity\ncues from diffusion models, thereby enhancing spatial consistency. In parallel,\nthe content features are aligned with image crop representations and\nconstrained by region correlations from VFMs to improve local discriminability.\nExtensive experiments demonstrate that DeCLIP establishes a solid foundation\nfor open-vocabulary dense perception, consistently achieving state-of-the-art\nperformance across a broad spectrum of tasks, including 2D detection and\nsegmentation, 3D instance segmentation, video instance segmentation, and 6D\nobject pose estimation.} Code is available at\nhttps://github.com/xiaomoguhz/DeCLIP", "AI": {"tldr": "DeCLIP enhances CLIP by decoupling self-attention into content and context features, improving local discriminability and spatial consistency for open-vocabulary dense perception tasks.", "motivation": "Existing dense perception tasks rely on predefined categories, limiting real-world applicability. CLIP's local feature representation is suboptimal for dense tasks.", "method": "DeCLIP decouples CLIP's self-attention into content and context features, enhancing context with semantic correlations from VFMs and object integrity cues from diffusion models, while aligning content with image crop representations.", "result": "DeCLIP achieves state-of-the-art performance across diverse tasks like 2D detection, segmentation, 3D instance segmentation, video instance segmentation, and 6D object pose estimation.", "conclusion": "DeCLIP provides a robust framework for open-vocabulary dense perception, addressing CLIP's limitations and excelling in various tasks."}}
{"id": "2508.11408", "pdf": "https://arxiv.org/pdf/2508.11408", "abs": "https://arxiv.org/abs/2508.11408", "authors": ["Wenhao Zhang", "Yuexiang Xie", "Yuchang Sun", "Yanxi Chen", "Guoyin Wang", "Yaliang Li", "Bolin Ding", "Jingren Zhou"], "title": "On-Policy RL Meets Off-Policy Experts: Harmonizing Supervised Fine-Tuning and Reinforcement Learning via Dynamic Weighting", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) are two\nprominent post-training paradigms for refining the capabilities and aligning\nthe behavior of Large Language Models (LLMs). Existing approaches that\nintegrate SFT and RL often face the risk of disrupting established model\npatterns and inducing overfitting to expert data. To address this, we present a\nnovel investigation into the unified view of SFT and RL through an off-policy\nversus on-policy lens. We propose CHORD, a framework for the Controllable\nHarmonization of On- and Off-Policy Reinforcement Learning via Dynamic\nWeighting, which reframes SFT not as a separate stage but as a dynamically\nweighted auxiliary objective within the on-policy RL process. Based on an\nanalysis of off-policy expert data's influence at both holistic and granular\nlevels, we incorporate a dual-control mechanism in CHORD. Specifically, the\nframework first employs a global coefficient to holistically guide the\ntransition from off-policy imitation to on-policy exploration, and then applies\na token-wise weighting function that enables granular learning from expert\ntokens, which preserves on-policy exploration and mitigates disruption from\noff-policy data. We conduct extensive experiments on widely used benchmarks,\nproviding empirical evidence that CHORD achieves a stable and efficient\nlearning process. By effectively harmonizing off-policy expert data with\non-policy exploration, CHORD demonstrates significant improvements over\nbaselines. We release the implementation at\nhttps://github.com/modelscope/Trinity-RFT/tree/main/examples/mix_chord to\ninspire further research.", "AI": {"tldr": "CHORD unifies SFT and RL by dynamically weighting them within the RL process, improving stability and performance.", "motivation": "Addresses the risk of disrupting model patterns and overfitting when integrating SFT and RL.", "method": "Proposes CHORD, a framework with dynamic weighting of SFT as an auxiliary objective in RL, using dual-control for transition and granular learning.", "result": "CHORD achieves stable and efficient learning, outperforming baselines by harmonizing off-policy expert data with on-policy exploration.", "conclusion": "CHORD effectively integrates SFT and RL, offering a robust solution for refining LLMs."}}
{"id": "2508.11262", "pdf": "https://arxiv.org/pdf/2508.11262", "abs": "https://arxiv.org/abs/2508.11262", "authors": ["Aiswarya Konavoor", "Raj Abhijit Dandekar", "Rajat Dandekar", "Sreedath Panat"], "title": "Vision-Language Models display a strong gender bias", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Vision-language models (VLM) align images and text in a shared representation\nspace that is useful for retrieval and zero-shot transfer. Yet, this alignment\ncan encode and amplify social stereotypes in subtle ways that are not obvious\nfrom standard accuracy metrics. In this study, we test whether the contrastive\nvision-language encoder exhibits gender-linked associations when it places\nembeddings of face images near embeddings of short phrases that describe\noccupations and activities. We assemble a dataset of 220 face photographs split\nby perceived binary gender and a set of 150 unique statements distributed\nacross six categories covering emotional labor, cognitive labor, domestic\nlabor, technical labor, professional roles, and physical labor. We compute\nunit-norm image embeddings for every face and unit-norm text embeddings for\nevery statement, then define a statement-level association score as the\ndifference between the mean cosine similarity to the male set and the mean\ncosine similarity to the female set, where positive values indicate stronger\nassociation with the male set and negative values indicate stronger association\nwith the female set. We attach bootstrap confidence intervals by resampling\nimages within each gender group, aggregate by category with a separate\nbootstrap over statements, and run a label-swap null model that estimates the\nlevel of mean absolute association we would expect if no gender structure were\npresent. The outcome is a statement-wise and category-wise map of gender\nassociations in a contrastive vision-language space, accompanied by\nuncertainty, simple sanity checks, and a robust gender bias evaluation\nframework.", "AI": {"tldr": "The paper investigates gender-linked associations in vision-language models (VLM) by analyzing embeddings of face images and occupational/activity phrases, revealing subtle biases not captured by standard metrics.", "motivation": "To uncover and quantify gender biases in VLMs, which align images and text but may inadvertently encode stereotypes.", "method": "Uses a dataset of 220 face images (split by perceived gender) and 150 statements across six labor categories. Measures associations via cosine similarity differences between male and female image embeddings, with bootstrap confidence intervals and a null model for validation.", "result": "Provides a detailed map of gender associations in VLMs, including uncertainty estimates and a robust evaluation framework for gender bias.", "conclusion": "Highlights the presence of subtle gender biases in VLMs and offers tools to detect and mitigate such biases in future models."}}
{"id": "2508.11424", "pdf": "https://arxiv.org/pdf/2508.11424", "abs": "https://arxiv.org/abs/2508.11424", "authors": ["Yinghua Yao", "Yuangang Pan", "Xixian Chen"], "title": "Generative Co-Design of Antibody Sequences and Structures via Black-Box Guidance in a Shared Latent Space", "categories": ["cs.LG"], "comment": "Accepted by IJCAI 2025", "summary": "Advancements in deep generative models have enabled the joint modeling of\nantibody sequence and structure, given the antigen-antibody complex as context.\nHowever, existing approaches for optimizing complementarity-determining regions\n(CDRs) to improve developability properties operate in the raw data space,\nleading to excessively costly evaluations due to the inefficient search\nprocess. To address this, we propose LatEnt blAck-box Design (LEAD), a\nsequence-structure co-design framework that optimizes both sequence and\nstructure within their shared latent space. Optimizing shared latent codes can\nnot only break through the limitations of existing methods, but also ensure\nsynchronization of different modality designs. Particularly, we design a\nblack-box guidance strategy to accommodate real-world scenarios where many\nproperty evaluators are non-differentiable. Experimental results demonstrate\nthat our LEAD achieves superior optimization performance for both single and\nmulti-property objectives. Notably, LEAD reduces query consumption by a half\nwhile surpassing baseline methods in property optimization. The code is\navailable at https://github.com/EvaFlower/LatEnt-blAck-box-Design.", "AI": {"tldr": "LEAD is a sequence-structure co-design framework optimizing antibody CDRs in latent space, reducing query costs and outperforming baselines.", "motivation": "Existing methods for optimizing antibody CDRs are inefficient due to raw data space operations, prompting the need for a latent space approach.", "method": "LEAD optimizes sequence and structure in their shared latent space using a black-box guidance strategy for non-differentiable evaluators.", "result": "LEAD achieves superior optimization performance, reducing query consumption by half and surpassing baselines in property optimization.", "conclusion": "LEAD provides an efficient and effective solution for antibody CDR optimization, with potential for broader applications."}}
{"id": "2508.11121", "pdf": "https://arxiv.org/pdf/2508.11121", "abs": "https://arxiv.org/abs/2508.11121", "authors": ["Mukul Singh", "Jos\u00e9 Cambronero", "Sumit Gulwani", "Vu Le", "Gust Verbruggen"], "title": "Tabularis Formatus: Predictive Formatting for Tables", "categories": ["cs.DB", "cs.AI", "cs.SE"], "comment": "14 pages", "summary": "Spreadsheet manipulation software are widely used for data management and\nanalysis of tabular data, yet the creation of conditional formatting (CF) rules\nremains a complex task requiring technical knowledge and experience with\nspecific platforms. In this paper we present TaFo, a neuro-symbolic approach to\ngenerating CF suggestions for tables, addressing common challenges such as user\nunawareness, difficulty in rule creation, and inadequate user interfaces. TaFo\ntakes inspiration from component based synthesis systems and extends them with\nsemantic knowledge of language models and a diversity preserving rule\nranking.Unlike previous methods focused on structural formatting, TaFo uniquely\nincorporates value-based formatting, automatically learning both the rule\ntrigger and the associated visual formatting properties for CF rules. By\nremoving the dependency on user specification used by existing techniques in\nthe form of formatted examples or natural language instruction, TaFo makes\nformatting completely predictive and automated for the user. To evaluate TaFo,\nwe use a corpus of 1.8 Million public workbooks with CF and manual formatting.\nWe compare TaFo against a diverse set of symbolic and neural systems designed\nfor or adapted for the task of table formatting. Our results show that TaFo\ngenerates more accurate, diverse and complete formatting suggestions than\ncurrent systems and outperforms these by 15.6\\%--26.5\\% on matching user added\nground truth rules in tables.", "AI": {"tldr": "TaFo is a neuro-symbolic system for automating conditional formatting (CF) rule suggestions in spreadsheets, outperforming existing methods by 15.6%\u201326.5%.", "motivation": "Current CF rule creation is complex and requires technical expertise, limiting accessibility for users.", "method": "TaFo combines component-based synthesis with language models and diversity-preserving ranking to automate CF rule generation, including value-based formatting.", "result": "TaFo outperforms existing systems, generating more accurate, diverse, and complete CF suggestions.", "conclusion": "TaFo successfully automates CF rule creation, reducing user dependency and improving accuracy."}}
{"id": "2508.11265", "pdf": "https://arxiv.org/pdf/2508.11265", "abs": "https://arxiv.org/abs/2508.11265", "authors": ["Pei He", "Lingling Li", "Licheng Jiao", "Ronghua Shang", "Fang Liu", "Shuang Wang", "Xu Liu", "Wenping Ma"], "title": "Domain-aware Category-level Geometry Learning Segmentation for 3D Point Clouds", "categories": ["cs.CV"], "comment": "to be published in International Conference on Computer Vision, ICCV\n  2025", "summary": "Domain generalization in 3D segmentation is a critical challenge in deploying\nmodels to unseen environments. Current methods mitigate the domain shift by\naugmenting the data distribution of point clouds. However, the model learns\nglobal geometric patterns in point clouds while ignoring the category-level\ndistribution and alignment. In this paper, a category-level geometry learning\nframework is proposed to explore the domain-invariant geometric features for\ndomain generalized 3D semantic segmentation. Specifically, Category-level\nGeometry Embedding (CGE) is proposed to perceive the fine-grained geometric\nproperties of point cloud features, which constructs the geometric properties\nof each class and couples geometric embedding to semantic learning. Secondly,\nGeometric Consistent Learning (GCL) is proposed to simulate the latent 3D\ndistribution and align the category-level geometric embeddings, allowing the\nmodel to focus on the geometric invariant information to improve\ngeneralization. Experimental results verify the effectiveness of the proposed\nmethod, which has very competitive segmentation accuracy compared with the\nstate-of-the-art domain generalized point cloud methods.", "AI": {"tldr": "A framework for domain generalization in 3D segmentation using category-level geometry learning to improve segmentation accuracy.", "motivation": "Addressing the challenge of domain shift in 3D segmentation by focusing on category-level geometric features.", "method": "Proposes Category-level Geometry Embedding (CGE) and Geometric Consistent Learning (GCL) to align geometric features and improve generalization.", "result": "Achieves competitive segmentation accuracy compared to state-of-the-art methods.", "conclusion": "The proposed method effectively enhances domain generalization in 3D segmentation by leveraging geometric invariant information."}}
{"id": "2508.11432", "pdf": "https://arxiv.org/pdf/2508.11432", "abs": "https://arxiv.org/abs/2508.11432", "authors": ["Muhammad Zakwan", "Liang Xu", "Giancarlo Ferrari-Trecate"], "title": "Robust Convolution Neural ODEs via Contractivity-promoting regularization", "categories": ["cs.LG", "cs.CV", "cs.SY", "eess.SY"], "comment": "Accepted in IEEE CDC2025, Rio de Janeiro, Brazil", "summary": "Neural networks can be fragile to input noise and adversarial attacks.\n  In this work, we consider Convolutional Neural Ordinary Differential\nEquations (NODEs), a family of continuous-depth neural networks represented by\ndynamical systems, and propose to use contraction theory to improve their\nrobustness.\n  For a contractive dynamical system two trajectories starting from different\ninitial conditions converge to each other exponentially fast.\n  Contractive Convolutional NODEs can enjoy increased robustness as slight\nperturbations of the features do not cause a significant change in the output.\n  Contractivity can be induced during training by using a regularization term\ninvolving the Jacobian of the system dynamics.\n  To reduce the computational burden, we show that it can also be promoted\nusing carefully selected weight regularization terms for a class of NODEs with\nslope-restricted activation functions.\n  The performance of the proposed regularizers is illustrated through benchmark\nimage classification tasks on MNIST and FashionMNIST datasets, where images are\ncorrupted by different kinds of noise and attacks.", "AI": {"tldr": "The paper proposes using contraction theory to enhance the robustness of Convolutional Neural Ordinary Differential Equations (NODEs) against input noise and adversarial attacks.", "motivation": "Neural networks are vulnerable to input noise and adversarial attacks, prompting the need for more robust models.", "method": "The authors introduce contractive Convolutional NODEs, leveraging contraction theory to ensure exponential convergence of trajectories. Robustness is induced via Jacobian regularization or weight regularization for specific NODEs.", "result": "The method demonstrates improved robustness on MNIST and FashionMNIST datasets under various noise and attack conditions.", "conclusion": "Contractive NODEs, trained with tailored regularization, effectively enhance model robustness against perturbations."}}
{"id": "2508.11272", "pdf": "https://arxiv.org/pdf/2508.11272", "abs": "https://arxiv.org/abs/2508.11272", "authors": ["Jun Li", "Kai Li", "Shaoguo Liu", "Tingting Gao"], "title": "Enhancing Supervised Composed Image Retrieval via Reasoning-Augmented Representation Engineering", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Composed Image Retrieval (CIR) presents a significant challenge as it\nrequires jointly understanding a reference image and a modified textual\ninstruction to find relevant target images. Some existing methods attempt to\nuse a two-stage approach to further refine retrieval results. However, this\noften requires additional training of a ranking model. Despite the success of\nChain-of-Thought (CoT) techniques in reducing training costs for language\nmodels, their application in CIR tasks remains limited -- compressing visual\ninformation into text or relying on elaborate prompt designs. Besides, existing\nworks only utilize it for zero-shot CIR, as it is challenging to achieve\nsatisfactory results in supervised CIR with a well-trained model. In this work,\nwe proposed a framework that includes the Pyramid Matching Model with\nTraining-Free Refinement (PMTFR) to address these challenges. Through a simple\nbut effective module called Pyramid Patcher, we enhanced the Pyramid Matching\nModel's understanding of visual information at different granularities.\nInspired by representation engineering, we extracted representations from COT\ndata and injected them into the LVLMs. This approach allowed us to obtain\nrefined retrieval scores in the Training-Free Refinement paradigm without\nrelying on explicit textual reasoning, further enhancing performance. Extensive\nexperiments on CIR benchmarks demonstrate that PMTFR surpasses state-of-the-art\nmethods in supervised CIR tasks. The code will be made public.", "AI": {"tldr": "Proposes PMTFR framework for Composed Image Retrieval (CIR), enhancing visual understanding and refining retrieval scores without additional training.", "motivation": "Addresses challenges in CIR by improving joint understanding of images and textual instructions without costly training or elaborate prompts.", "method": "Uses Pyramid Patcher for multi-granularity visual understanding and injects representations from CoT data into LVLMs for training-free refinement.", "result": "PMTFR outperforms state-of-the-art methods in supervised CIR tasks.", "conclusion": "The framework effectively enhances CIR performance without additional training, validated by benchmarks."}}
{"id": "2508.11436", "pdf": "https://arxiv.org/pdf/2508.11436", "abs": "https://arxiv.org/abs/2508.11436", "authors": ["Mayssa Soussia", "Mohamed Ali Mahjoub", "Islem Rekik"], "title": "Multi-Sensory Cognitive Computing for Learning Population-level Brain Connectivity", "categories": ["cs.LG"], "comment": null, "summary": "The generation of connectional brain templates (CBTs) has recently garnered\nsignificant attention for its potential to identify unique connectivity\npatterns shared across individuals. However, existing methods for CBT learning\nsuch as conventional machine learning and graph neural networks (GNNs) are\nhindered by several limitations. These include: (i) poor interpretability due\nto their black-box nature, (ii) high computational cost, and (iii) an exclusive\nfocus on structure and topology, overlooking the cognitive capacity of the\ngenerated CBT. To address these challenges, we introduce mCOCO (multi-sensory\nCOgnitive COmputing), a novel framework that leverages Reservoir Computing (RC)\nto learn population-level functional CBT from BOLD\n(Blood-Oxygen-level-Dependent) signals. RC's dynamic system properties allow\nfor tracking state changes over time, enhancing interpretability and enabling\nthe modeling of brain-like dynamics, as demonstrated in prior literature. By\nintegrating multi-sensory inputs (e.g., text, audio, and visual data), mCOCO\ncaptures not only structure and topology but also how brain regions process\ninformation and adapt to cognitive tasks such as sensory processing, all in a\ncomputationally efficient manner. Our mCOCO framework consists of two phases:\n(1) mapping BOLD signals into the reservoir to derive individual functional\nconnectomes, which are then aggregated into a group-level CBT - an approach, to\nthe best of our knowledge, not previously explored in functional connectivity\nstudies - and (2) incorporating multi-sensory inputs through a cognitive\nreservoir, endowing the CBT with cognitive traits. Extensive evaluations show\nthat our mCOCO-based template significantly outperforms GNN-based CBT in terms\nof centeredness, discriminativeness, topological soundness, and multi-sensory\nmemory retention. Our source code is available at\nhttps://github.com/basiralab/mCOCO.", "AI": {"tldr": "mCOCO introduces a novel framework using Reservoir Computing to create interpretable, efficient, and cognitively rich connectional brain templates (CBTs) from BOLD signals, outperforming existing methods.", "motivation": "Existing CBT methods lack interpretability, are computationally expensive, and ignore cognitive aspects, prompting the need for a better approach.", "method": "mCOCO uses Reservoir Computing to map BOLD signals into functional connectomes, aggregates them into a group-level CBT, and integrates multi-sensory inputs for cognitive traits.", "result": "mCOCO-based CBT outperforms GNN-based methods in centeredness, discriminativeness, topological soundness, and multi-sensory memory retention.", "conclusion": "mCOCO offers a superior, interpretable, and efficient solution for generating cognitively enriched CBTs."}}
{"id": "2508.11116", "pdf": "https://arxiv.org/pdf/2508.11116", "abs": "https://arxiv.org/abs/2508.11116", "authors": ["Zhuoqun Li", "Xuanang Chen", "Hongyu Lin", "Yaojie Lu", "Xianpei Han", "Le Sun"], "title": "PaperRegister: Boosting Flexible-grained Paper Search via Hierarchical Register Indexing", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Paper search is an important activity for researchers, typically involving\nusing a query with description of a topic to find relevant papers. As research\ndeepens, paper search requirements may become more flexible, sometimes\ninvolving specific details such as module configuration rather than being\nlimited to coarse-grained topics. However, previous paper search systems are\nunable to meet these flexible-grained requirements, as these systems mainly\ncollect paper abstracts to construct index of corpus, which lack detailed\ninformation to support retrieval by finer-grained queries. In this work, we\npropose PaperRegister, consisted of offline hierarchical indexing and online\nadaptive retrieval, transforming traditional abstract-based index into\nhierarchical index tree for paper search, thereby supporting queries at\nflexible granularity. Experiments on paper search tasks across a range of\ngranularity demonstrate that PaperRegister achieves the state-of-the-art\nperformance, and particularly excels in fine-grained scenarios, highlighting\nthe good potential as an effective solution for flexible-grained paper search\nin real-world applications. Code for this work is in\nhttps://github.com/Li-Z-Q/PaperRegister.", "AI": {"tldr": "PaperRegister introduces a hierarchical indexing and adaptive retrieval system for flexible-grained paper search, outperforming traditional abstract-based methods.", "motivation": "Existing paper search systems lack support for flexible-grained queries, as they rely on abstract-based indexes missing detailed information.", "method": "Proposes PaperRegister with offline hierarchical indexing and online adaptive retrieval, transforming abstract-based indexes into hierarchical trees.", "result": "Achieves state-of-the-art performance, excelling in fine-grained scenarios.", "conclusion": "PaperRegister is a promising solution for flexible-grained paper search, with code available on GitHub."}}
{"id": "2508.11277", "pdf": "https://arxiv.org/pdf/2508.11277", "abs": "https://arxiv.org/abs/2508.11277", "authors": ["Matthew Lyle Olson", "Musashi Hinck", "Neale Ratzlaff", "Changbai Li", "Phillip Howard", "Vasudev Lal", "Shao-Yen Tseng"], "title": "Probing the Representational Power of Sparse Autoencoders in Vision Models", "categories": ["cs.CV", "cs.LG"], "comment": "ICCV 2025 Findings", "summary": "Sparse Autoencoders (SAEs) have emerged as a popular tool for interpreting\nthe hidden states of large language models (LLMs). By learning to reconstruct\nactivations from a sparse bottleneck layer, SAEs discover interpretable\nfeatures from the high-dimensional internal representations of LLMs. Despite\ntheir popularity with language models, SAEs remain understudied in the visual\ndomain. In this work, we provide an extensive evaluation the representational\npower of SAEs for vision models using a broad range of image-based tasks. Our\nexperimental results demonstrate that SAE features are semantically meaningful,\nimprove out-of-distribution generalization, and enable controllable generation\nacross three vision model architectures: vision embedding models, multi-modal\nLMMs and diffusion models. In vision embedding models, we find that learned SAE\nfeatures can be used for OOD detection and provide evidence that they recover\nthe ontological structure of the underlying model. For diffusion models, we\ndemonstrate that SAEs enable semantic steering through text encoder\nmanipulation and develop an automated pipeline for discovering\nhuman-interpretable attributes. Finally, we conduct exploratory experiments on\nmulti-modal LLMs, finding evidence that SAE features reveal shared\nrepresentations across vision and language modalities. Our study provides a\nfoundation for SAE evaluation in vision models, highlighting their strong\npotential improving interpretability, generalization, and steerability in the\nvisual domain.", "AI": {"tldr": "Sparse Autoencoders (SAEs) are evaluated for vision models, showing they improve interpretability, generalization, and steerability across tasks like OOD detection, controllable generation, and multi-modal analysis.", "motivation": "SAEs are widely used in language models but understudied in vision. This work explores their potential in visual tasks.", "method": "Extensive evaluation of SAEs on vision models (embedding, multi-modal LLMs, diffusion) for tasks like OOD detection, semantic steering, and feature analysis.", "result": "SAE features are semantically meaningful, improve generalization, and enable controllable generation. They reveal shared representations in multi-modal models.", "conclusion": "SAEs show strong potential for enhancing interpretability, generalization, and steerability in vision models."}}
{"id": "2508.11441", "pdf": "https://arxiv.org/pdf/2508.11441", "abs": "https://arxiv.org/abs/2508.11441", "authors": ["Eric G\u00fcnther", "Bal\u00e1zs Szabados", "Robi Bhattacharjee", "Sebastian Bordt", "Ulrike von Luxburg"], "title": "Informative Post-Hoc Explanations Only Exist for Simple Functions", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Many researchers have suggested that local post-hoc explanation algorithms\ncan be used to gain insights into the behavior of complex machine learning\nmodels. However, theoretical guarantees about such algorithms only exist for\nsimple decision functions, and it is unclear whether and under which\nassumptions similar results might exist for complex models. In this paper, we\nintroduce a general, learning-theory-based framework for what it means for an\nexplanation to provide information about a decision function. We call an\nexplanation informative if it serves to reduce the complexity of the space of\nplausible decision functions. With this approach, we show that many popular\nexplanation algorithms are not informative when applied to complex decision\nfunctions, providing a rigorous mathematical rejection of the idea that it\nshould be possible to explain any model. We then derive conditions under which\ndifferent explanation algorithms become informative. These are often stronger\nthan what one might expect. For example, gradient explanations and\ncounterfactual explanations are non-informative with respect to the space of\ndifferentiable functions, and SHAP and anchor explanations are not informative\nwith respect to the space of decision trees. Based on these results, we discuss\nhow explanation algorithms can be modified to become informative. While the\nproposed analysis of explanation algorithms is mathematical, we argue that it\nholds strong implications for the practical applicability of these algorithms,\nparticularly for auditing, regulation, and high-risk applications of AI.", "AI": {"tldr": "The paper critiques local post-hoc explanation algorithms for complex ML models, showing they often fail to provide meaningful insights. It introduces a framework to define informative explanations and derives conditions under which explanations become useful.", "motivation": "To address the lack of theoretical guarantees for explanation algorithms in complex models and rigorously evaluate their informativeness.", "method": "Introduces a learning-theory-based framework to define informative explanations and analyzes popular algorithms (e.g., gradient, SHAP, anchors) under this framework.", "result": "Many popular explanation algorithms are non-informative for complex models, with stronger conditions required for informativeness.", "conclusion": "The study highlights limitations of current explanation methods and suggests modifications for practical use in high-risk AI applications."}}
{"id": "2508.11122", "pdf": "https://arxiv.org/pdf/2508.11122", "abs": "https://arxiv.org/abs/2508.11122", "authors": ["Xingyu Deng", "Xi Wang", "Mark Stevenson"], "title": "+VeriRel: Verification Feedback to Enhance Document Retrieval for Scientific Fact Checking", "categories": ["cs.IR", "cs.CL"], "comment": "Accpeted for the 34th ACM International Conference on Information and\n  Knowledge Management (CIKM'25)", "summary": "Identification of appropriate supporting evidence is critical to the success\nof scientific fact checking. However, existing approaches rely on off-the-shelf\nInformation Retrieval algorithms that rank documents based on relevance rather\nthan the evidence they provide to support or refute the claim being checked.\nThis paper proposes +VeriRel which includes verification success in the\ndocument ranking. Experimental results on three scientific fact checking\ndatasets (SciFact, SciFact-Open and Check-Covid) demonstrate consistently\nleading performance by +VeriRel for document evidence retrieval and a positive\nimpact on downstream verification. This study highlights the potential of\nintegrating verification feedback to document relevance assessment for\neffective scientific fact checking systems. It shows promising future work to\nevaluate fine-grained relevance when examining complex documents for advanced\nscientific fact checking.", "AI": {"tldr": "+VeriRel improves scientific fact checking by integrating verification feedback into document ranking, outperforming traditional relevance-based methods.", "motivation": "Existing methods for scientific fact checking rely on relevance-based document ranking, which may not align with the evidence needed to verify claims.", "method": "The paper proposes +VeriRel, a system that incorporates verification success into document ranking.", "result": "+VeriRel consistently outperforms traditional methods on three datasets (SciFact, SciFact-Open, Check-Covid) for evidence retrieval and improves downstream verification.", "conclusion": "Integrating verification feedback into document ranking enhances scientific fact checking, with potential for future work on fine-grained relevance in complex documents."}}
{"id": "2508.11143", "pdf": "https://arxiv.org/pdf/2508.11143", "abs": "https://arxiv.org/abs/2508.11143", "authors": ["Jiarui Yang", "Bin Zhu", "Jingjing Chen", "Yu-Gang Jiang"], "title": "Actor-Critic for Continuous Action Chunks: A Reinforcement Learning Framework for Long-Horizon Robotic Manipulation with Sparse Reward", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Existing reinforcement learning (RL) methods struggle with long-horizon\nrobotic manipulation tasks, particularly those involving sparse rewards. While\naction chunking is a promising paradigm for robotic manipulation, using RL to\ndirectly learn continuous action chunks in a stable and data-efficient manner\nremains a critical challenge. This paper introduces AC3 (Actor-Critic for\nContinuous Chunks), a novel RL framework that learns to generate\nhigh-dimensional, continuous action sequences. To make this learning process\nstable and data-efficient, AC3 incorporates targeted stabilization mechanisms\nfor both the actor and the critic. First, to ensure reliable policy\nimprovement, the actor is trained with an asymmetric update rule, learning\nexclusively from successful trajectories. Second, to enable effective value\nlearning despite sparse rewards, the critic's update is stabilized using\nintra-chunk $n$-step returns and further enriched by a self-supervised module\nproviding intrinsic rewards at anchor points aligned with each action chunk. We\nconducted extensive experiments on 25 tasks from the BiGym and RLBench\nbenchmarks. Results show that by using only a few demonstrations and a simple\nmodel architecture, AC3 achieves superior success rates on most tasks,\nvalidating its effective design.", "AI": {"tldr": "AC3 (Actor-Critic for Continuous Chunks) is a novel RL framework for stable, data-efficient learning of continuous action sequences in robotic manipulation tasks with sparse rewards.", "motivation": "Existing RL methods struggle with long-horizon robotic tasks and sparse rewards, making action chunking challenging to learn stably and efficiently.", "method": "AC3 introduces asymmetric actor updates (learning from successful trajectories) and stabilized critic updates (using intra-chunk n-step returns and self-supervised intrinsic rewards).", "result": "Experiments on 25 tasks from BiGym and RLBench show AC3 achieves superior success rates with few demonstrations and a simple architecture.", "conclusion": "AC3's design effectively addresses the challenges of learning continuous action chunks, offering improved stability and data efficiency in RL for robotic manipulation."}}
{"id": "2508.11282", "pdf": "https://arxiv.org/pdf/2508.11282", "abs": "https://arxiv.org/abs/2508.11282", "authors": ["Muzammil Khan", "Enzo Kerkhof", "Matteo Fusaglia", "Koert Kuhlmann", "Theo Ruers", "Fran\u00e7oise J. Siepel"], "title": "Unifying Scale-Aware Depth Prediction and Perceptual Priors for Monocular Endoscope Pose Estimation and Tissue Reconstruction", "categories": ["cs.CV"], "comment": "18 pages, 8 figures, 3 Tables, submitted to IEEE Access for review", "summary": "Accurate endoscope pose estimation and 3D tissue surface reconstruction\nsignificantly enhances monocular minimally invasive surgical procedures by\nenabling accurate navigation and improved spatial awareness. However, monocular\nendoscope pose estimation and tissue reconstruction face persistent challenges,\nincluding depth ambiguity, physiological tissue deformation, inconsistent\nendoscope motion, limited texture fidelity, and a restricted field of view. To\novercome these limitations, a unified framework for monocular endoscopic tissue\nreconstruction that integrates scale-aware depth prediction with\ntemporally-constrained perceptual refinement is presented. This framework\nincorporates a novel MAPIS-Depth module, which leverages Depth Pro for robust\ninitialisation and Depth Anything for efficient per-frame depth prediction, in\nconjunction with L-BFGS-B optimisation, to generate pseudo-metric depth\nestimates. These estimates are temporally refined by computing pixel\ncorrespondences using RAFT and adaptively blending flow-warped frames based on\nLPIPS perceptual similarity, thereby reducing artefacts arising from\nphysiological tissue deformation and motion. To ensure accurate registration of\nthe synthesised pseudo-RGBD frames from MAPIS-Depth, a novel WEMA-RTDL module\nis integrated, optimising both rotation and translation. Finally, truncated\nsigned distance function-based volumetric fusion and marching cubes are applied\nto extract a comprehensive 3D surface mesh. Evaluations on HEVD and SCARED,\nwith ablation and comparative analyses, demonstrate the framework's robustness\nand superiority over state-of-the-art methods.", "AI": {"tldr": "A unified framework for monocular endoscopic tissue reconstruction integrates depth prediction and perceptual refinement to address challenges like depth ambiguity and tissue deformation, outperforming state-of-the-art methods.", "motivation": "Enhancing monocular minimally invasive surgical procedures by improving navigation and spatial awareness despite challenges like depth ambiguity and tissue deformation.", "method": "Combines scale-aware depth prediction (MAPIS-Depth module) with temporally-constrained perceptual refinement, using RAFT for pixel correspondences and LPIPS for blending. WEMA-RTDL optimizes registration, and volumetric fusion extracts 3D mesh.", "result": "Demonstrates robustness and superiority over state-of-the-art methods in evaluations on HEVD and SCARED datasets.", "conclusion": "The framework effectively addresses key challenges in monocular endoscope pose estimation and tissue reconstruction, offering improved accuracy and performance."}}
{"id": "2508.11460", "pdf": "https://arxiv.org/pdf/2508.11460", "abs": "https://arxiv.org/abs/2508.11460", "authors": ["Aurora Grefsrud", "Nello Blaser", "Trygve Buanes"], "title": "Calibrated and uncertain? Evaluating uncertainty estimates in binary classification models", "categories": ["cs.LG", "stat.ML"], "comment": null, "summary": "Rigorous statistical methods, including parameter estimation with\naccompanying uncertainties, underpin the validity of scientific discovery,\nespecially in the natural sciences. With increasingly complex data models such\nas deep learning techniques, uncertainty quantification has become exceedingly\ndifficult and a plethora of techniques have been proposed. In this case study,\nwe use the unifying framework of approximate Bayesian inference combined with\nempirical tests on carefully created synthetic classification datasets to\ninvestigate qualitative properties of six different probabilistic machine\nlearning algorithms for class probability and uncertainty estimation: (i) a\nneural network ensemble, (ii) neural network ensemble with conflictual loss,\n(iii) evidential deep learning, (iv) a single neural network with Monte Carlo\nDropout, (v) Gaussian process classification and (vi) a Dirichlet process\nmixture model. We check if the algorithms produce uncertainty estimates which\nreflect commonly desired properties, such as being well calibrated and\nexhibiting an increase in uncertainty for out-of-distribution data points. Our\nresults indicate that all algorithms are well calibrated, but none of the deep\nlearning based algorithms provide uncertainties that consistently reflect lack\nof experimental evidence for out-of-distribution data points. We hope our study\nmay serve as a clarifying example for researchers developing new methods of\nuncertainty estimation for scientific data-driven modeling.", "AI": {"tldr": "The paper evaluates six probabilistic machine learning algorithms for uncertainty estimation, finding they are well-calibrated but struggle with out-of-distribution data.", "motivation": "To address the challenge of uncertainty quantification in complex data models like deep learning, ensuring reliable scientific discovery.", "method": "Uses approximate Bayesian inference and synthetic datasets to test six algorithms for class probability and uncertainty estimation.", "result": "All algorithms are well-calibrated, but deep learning-based ones fail to reflect uncertainty for out-of-distribution data.", "conclusion": "The study clarifies uncertainty estimation methods, aiding future research in scientific data-driven modeling."}}
{"id": "2508.11152", "pdf": "https://arxiv.org/pdf/2508.11152", "abs": "https://arxiv.org/abs/2508.11152", "authors": ["Tianjiao Zhao", "Jingrao Lyu", "Stokes Jones", "Harrison Garber", "Stefano Pasquali", "Dhagash Mehta"], "title": "AlphaAgents: Large Language Model based Multi-Agents for Equity Portfolio Constructions", "categories": ["q-fin.ST", "cs.AI"], "comment": null, "summary": "The field of artificial intelligence (AI) agents is evolving rapidly, driven\nby the capabilities of Large Language Models (LLMs) to autonomously perform and\nrefine tasks with human-like efficiency and adaptability. In this context,\nmulti-agent collaboration has emerged as a promising approach, enabling\nmultiple AI agents to work together to solve complex challenges. This study\ninvestigates the application of role-based multi-agent systems to support stock\nselection in equity research and portfolio management. We present a\ncomprehensive analysis performed by a team of specialized agents and evaluate\ntheir stock-picking performance against established benchmarks under varying\nlevels of risk tolerance. Furthermore, we examine the advantages and\nlimitations of employing multi-agent frameworks in equity analysis, offering\ncritical insights into their practical efficacy and implementation challenges.", "AI": {"tldr": "The study explores role-based multi-agent AI systems for stock selection, evaluating performance against benchmarks and analyzing practical efficacy.", "motivation": "To leverage multi-agent collaboration in AI for complex tasks like stock selection, addressing efficiency and adaptability.", "method": "Role-based multi-agent systems are applied to equity research, with performance evaluated against benchmarks under varying risk tolerance.", "result": "The study provides insights into the stock-picking performance of multi-agent systems and their practical advantages and limitations.", "conclusion": "Multi-agent frameworks show promise in equity analysis but face implementation challenges, requiring further exploration."}}
{"id": "2508.11284", "pdf": "https://arxiv.org/pdf/2508.11284", "abs": "https://arxiv.org/abs/2508.11284", "authors": ["Yilin Mi", "Qixin Yan", "Zheng-Peng Duan", "Chunle Guo", "Hubery Yin", "Hao Liu", "Chen Li", "Chongyi Li"], "title": "TimeMachine: Fine-Grained Facial Age Editing with Identity Preservation", "categories": ["cs.CV"], "comment": null, "summary": "With the advancement of generative models, facial image editing has made\nsignificant progress. However, achieving fine-grained age editing while\npreserving personal identity remains a challenging task.In this paper, we\npropose TimeMachine, a novel diffusion-based framework that achieves accurate\nage editing while keeping identity features unchanged. To enable fine-grained\nage editing, we inject high-precision age information into the multi-cross\nattention module, which explicitly separates age-related and identity-related\nfeatures. This design facilitates more accurate disentanglement of age\nattributes, thereby allowing precise and controllable manipulation of facial\naging.Furthermore, we propose an Age Classifier Guidance (ACG) module that\npredicts age directly in the latent space, instead of performing denoising\nimage reconstruction during training. By employing a lightweight module to\nincorporate age constraints, this design enhances age editing accuracy by\nmodest increasing training cost. Additionally, to address the lack of\nlarge-scale, high-quality facial age datasets, we construct a HFFA dataset\n(High-quality Fine-grained Facial-Age dataset) which contains one million\nhigh-resolution images labeled with identity and facial attributes.\nExperimental results demonstrate that TimeMachine achieves state-of-the-art\nperformance in fine-grained age editing while preserving identity consistency.", "AI": {"tldr": "TimeMachine is a diffusion-based framework for fine-grained facial age editing while preserving identity, using high-precision age injection and an Age Classifier Guidance module.", "motivation": "Fine-grained age editing without altering personal identity is challenging due to entangled age and identity features.", "method": "Proposes TimeMachine with multi-cross attention for age-identity disentanglement and an Age Classifier Guidance module for latent-space age prediction. Introduces the HFFA dataset for training.", "result": "Achieves state-of-the-art performance in fine-grained age editing with identity preservation.", "conclusion": "TimeMachine effectively addresses the challenge of precise age editing while maintaining identity consistency."}}
{"id": "2508.11504", "pdf": "https://arxiv.org/pdf/2508.11504", "abs": "https://arxiv.org/abs/2508.11504", "authors": ["Andrea Castellani", "Zacharias Papadovasilakis", "Giorgos Papoutsoglou", "Mary Cole", "Brian Bautsch", "Tobias Rodemann", "Ioannis Tsamardinos", "Angela Harden"], "title": "Predicting and Explaining Traffic Crash Severity Through Crash Feature Selection", "categories": ["cs.LG", "cs.CY"], "comment": "Preprint. Manuscript under review at \"Accident Analysis & Prevention\"\n  journal", "summary": "Motor vehicle crashes remain a leading cause of injury and death worldwide,\nnecessitating data-driven approaches to understand and mitigate crash severity.\nThis study introduces a curated dataset of more than 3 million people involved\nin accidents in Ohio over six years (2017-2022), aggregated to more than 2.3\nmillion vehicle-level records for predictive analysis. The primary contribution\nis a transparent and reproducible methodology that combines Automated Machine\nLearning (AutoML) and explainable artificial intelligence (AI) to identify and\ninterpret key risk factors associated with severe crashes. Using the JADBio\nAutoML platform, predictive models were constructed to distinguish between\nsevere and non-severe crash outcomes. The models underwent rigorous feature\nselection across stratified training subsets, and their outputs were\ninterpreted using SHapley Additive exPlanations (SHAP) to quantify the\ncontribution of individual features. A final Ridge Logistic Regression model\nachieved an AUC-ROC of 85.6% on the training set and 84.9% on a hold-out test\nset, with 17 features consistently identified as the most influential\npredictors. Key features spanned demographic, environmental, vehicle, human,\nand operational categories, including location type, posted speed, minimum\noccupant age, and pre-crash action. Notably, certain traditionally emphasized\nfactors, such as alcohol or drug impairment, were less influential in the final\nmodel compared to environmental and contextual variables. Emphasizing\nmethodological rigor and interpretability over mere predictive performance,\nthis study offers a scalable framework to support Vision Zero with aligned\ninterventions and advanced data-informed traffic safety policy.", "AI": {"tldr": "The study presents a data-driven approach using AutoML and explainable AI to predict and interpret crash severity, identifying key risk factors from a large Ohio dataset.", "motivation": "Motor vehicle crashes are a major global cause of injury and death, requiring data-driven solutions to mitigate severity.", "method": "The study uses AutoML (JADBio platform) and SHAP for feature selection and interpretation, building predictive models like Ridge Logistic Regression.", "result": "The model achieved 85.6% AUC-ROC on training and 84.9% on test data, identifying 17 key features, with environmental and contextual factors being more influential than traditional ones like alcohol impairment.", "conclusion": "The study provides a scalable, interpretable framework for traffic safety policy, supporting Vision Zero with data-informed interventions."}}
{"id": "2508.11187", "pdf": "https://arxiv.org/pdf/2508.11187", "abs": "https://arxiv.org/abs/2508.11187", "authors": ["Wonjune Kang", "Deb Roy"], "title": "Expressive Speech Retrieval using Natural Language Descriptions of Speaking Style", "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": "Accepted to ASRU 2025", "summary": "We introduce the task of expressive speech retrieval, where the goal is to\nretrieve speech utterances spoken in a given style based on a natural language\ndescription of that style. While prior work has primarily focused on performing\nspeech retrieval based on what was said in an utterance, we aim to do so based\non how something was said. We train speech and text encoders to embed speech\nand text descriptions of speaking styles into a joint latent space, which\nenables using free-form text prompts describing emotions or styles as queries\nto retrieve matching expressive speech segments. We perform detailed analyses\nof various aspects of our proposed framework, including encoder architectures,\ntraining criteria for effective cross-modal alignment, and prompt augmentation\nfor improved generalization to arbitrary text queries. Experiments on multiple\ndatasets encompassing 22 speaking styles demonstrate that our approach achieves\nstrong retrieval performance as measured by Recall@k.", "AI": {"tldr": "The paper introduces expressive speech retrieval, focusing on retrieving speech by style (how something was said) rather than content (what was said), using joint embeddings of speech and text descriptions.", "motivation": "Prior work focused on speech retrieval based on content, but this paper aims to retrieve speech by expressive style using natural language descriptions.", "method": "Train speech and text encoders to embed into a joint latent space, enabling text prompts to retrieve matching speech. Analyzes encoder architectures, training criteria, and prompt augmentation.", "result": "Strong retrieval performance (Recall@k) demonstrated on datasets with 22 speaking styles.", "conclusion": "The framework effectively retrieves expressive speech using text descriptions, advancing beyond content-based retrieval."}}
{"id": "2508.11158", "pdf": "https://arxiv.org/pdf/2508.11158", "abs": "https://arxiv.org/abs/2508.11158", "authors": ["Xiaolu Chen", "Haojie Wu", "Jie Bao", "Zhen Chen", "Yong Liao", "Hu Huang"], "title": "Role-Augmented Intent-Driven Generative Search Engine Optimization", "categories": ["cs.IR", "cs.AI"], "comment": "7 pages, 5 figures", "summary": "Generative Search Engines (GSEs), powered by Large Language Models (LLMs) and\nRetrieval-Augmented Generation (RAG), are reshaping information retrieval.\nWhile commercial systems (e.g., BingChat, Perplexity.ai) demonstrate impressive\nsemantic synthesis capabilities, their black-box nature fundamentally\nundermines established Search Engine Optimization (SEO) practices. Content\ncreators face a critical challenge: their optimization strategies, effective in\ntraditional search engines, are misaligned with generative retrieval contexts,\nresulting in diminished visibility. To bridge this gap, we propose a\nRole-Augmented Intent-Driven Generative Search Engine Optimization (G-SEO)\nmethod, providing a structured optimization pathway tailored for GSE scenarios.\nOur method models search intent through reflective refinement across diverse\ninformational roles, enabling targeted content enhancement. To better evaluate\nthe method under realistic settings, we address the benchmarking limitations of\nprior work by: (1) extending the GEO dataset with diversified query variations\nreflecting real-world search scenarios and (2) introducing G-Eval 2.0, a\n6-level LLM-augmented evaluation rubric for fine-grained human-aligned\nassessment. Experimental results demonstrate that search intent serves as an\neffective signal for guiding content optimization, yielding significant\nimprovements over single-aspect baseline approaches in both subjective\nimpressions and objective content visibility within GSE responses.", "AI": {"tldr": "The paper introduces a Role-Augmented Intent-Driven G-SEO method to optimize content for Generative Search Engines (GSEs), addressing the misalignment of traditional SEO with GSEs. It extends the GEO dataset and introduces G-Eval 2.0 for evaluation, showing improved content visibility.", "motivation": "Traditional SEO practices are ineffective in GSEs due to their black-box nature, leading to diminished visibility for content creators. The paper aims to bridge this gap.", "method": "Proposes a Role-Augmented Intent-Driven G-SEO method, models search intent through reflective refinement, and introduces G-Eval 2.0 for evaluation.", "result": "Search intent effectively guides content optimization, yielding significant improvements in visibility and subjective impressions.", "conclusion": "The proposed method successfully aligns content optimization with GSEs, enhancing visibility and performance."}}
{"id": "2508.11301", "pdf": "https://arxiv.org/pdf/2508.11301", "abs": "https://arxiv.org/abs/2508.11301", "authors": ["Jiarong Li", "Imad Ali Shah", "Enda Ward", "Martin Glavin", "Edward Jones", "Brian Deegan"], "title": "Hyperspectral vs. RGB for Pedestrian Segmentation in Urban Driving Scenes: A Comparative Study", "categories": ["cs.CV"], "comment": "Submitted to IEEE ICVES, July, 2025", "summary": "Pedestrian segmentation in automotive perception systems faces critical\nsafety challenges due to metamerism in RGB imaging, where pedestrians and\nbackgrounds appear visually indistinguishable.. This study investigates the\npotential of hyperspectral imaging (HSI) for enhanced pedestrian segmentation\nin urban driving scenarios using the Hyperspectral City v2 (H-City) dataset. We\ncompared standard RGB against two dimensionality-reduction approaches by\nconverting 128-channel HSI data into three-channel representations: Principal\nComponent Analysis (PCA) and optimal band selection using Contrast\nSignal-to-Noise Ratio with Joint Mutual Information Maximization (CSNR-JMIM).\nThree semantic segmentation models were evaluated: U-Net, DeepLabV3+, and\nSegFormer. CSNR-JMIM consistently outperformed RGB with an average improvements\nof 1.44% in Intersection over Union (IoU) and 2.18% in F1-score for pedestrian\nsegmentation. Rider segmentation showed similar gains with 1.43% IoU and 2.25%\nF1-score improvements. These improved performance results from enhanced\nspectral discrimination of optimally selected HSI bands effectively reducing\nfalse positives. This study demonstrates robust pedestrian segmentation through\noptimal HSI band selection, showing significant potential for safety-critical\nautomotive applications.", "AI": {"tldr": "Hyperspectral imaging (HSI) outperforms RGB for pedestrian segmentation in automotive safety, using optimal band selection (CSNR-JMIM) to improve accuracy.", "motivation": "Address safety challenges in pedestrian segmentation due to RGB metamerism by leveraging HSI for better spectral discrimination.", "method": "Compared RGB with two HSI dimensionality-reduction methods (PCA and CSNR-JMIM) using three segmentation models (U-Net, DeepLabV3+, SegFormer) on the H-City dataset.", "result": "CSNR-JMIM improved pedestrian segmentation by 1.44% IoU and 2.18% F1-score, reducing false positives.", "conclusion": "Optimal HSI band selection enhances pedestrian segmentation, proving valuable for safety-critical automotive applications."}}
{"id": "2508.11513", "pdf": "https://arxiv.org/pdf/2508.11513", "abs": "https://arxiv.org/abs/2508.11513", "authors": ["Fanzhen Liu", "Xiaoxiao Ma", "Jian Yang", "Alsharif Abuadbba", "Kristen Moore", "Surya Nepal", "Cecile Paris", "Quan Z. Sheng", "Jia Wu"], "title": "Towards Faithful Class-level Self-explainability in Graph Neural Networks by Subgraph Dependencies", "categories": ["cs.LG", "cs.AI"], "comment": "14 pages, 12 figures", "summary": "Enhancing the interpretability of graph neural networks (GNNs) is crucial to\nensure their safe and fair deployment. Recent work has introduced\nself-explainable GNNs that generate explanations as part of training, improving\nboth faithfulness and efficiency. Some of these models, such as ProtGNN and\nPGIB, learn class-specific prototypes, offering a potential pathway toward\nclass-level explanations. However, their evaluations focus solely on\ninstance-level explanations, leaving open the question of whether these\nprototypes meaningfully generalize across instances of the same class. In this\npaper, we introduce GraphOracle, a novel self-explainable GNN framework\ndesigned to generate and evaluate class-level explanations for GNNs. Our model\njointly learns a GNN classifier and a set of structured, sparse subgraphs that\nare discriminative for each class. We propose a novel integrated training that\ncaptures graph$\\unicode{x2013}$subgraph$\\unicode{x2013}$prediction dependencies\nefficiently and faithfully, validated through a masking-based evaluation\nstrategy. This strategy enables us to retroactively assess whether prior\nmethods like ProtGNN and PGIB deliver effective class-level explanations. Our\nresults show that they do not. In contrast, GraphOracle achieves superior\nfidelity, explainability, and scalability across a range of graph\nclassification tasks. We further demonstrate that GraphOracle avoids the\ncomputational bottlenecks of previous methods$\\unicode{x2014}$like Monte Carlo\nTree Search$\\unicode{x2014}$by using entropy-regularized subgraph selection and\nlightweight random walk extraction, enabling faster and more scalable training.\nThese findings position GraphOracle as a practical and principled solution for\nfaithful class-level self-explainability in GNNs.", "AI": {"tldr": "GraphOracle is a self-explainable GNN framework that generates and evaluates class-level explanations, outperforming prior methods in fidelity, explainability, and scalability.", "motivation": "Existing self-explainable GNNs lack meaningful class-level explanations, despite learning class-specific prototypes. GraphOracle addresses this gap.", "method": "GraphOracle jointly learns a GNN classifier and sparse subgraphs for each class, using integrated training and masking-based evaluation.", "result": "GraphOracle outperforms ProtGNN and PGIB in fidelity, explainability, and scalability, avoiding computational bottlenecks.", "conclusion": "GraphOracle is a practical solution for faithful class-level self-explainability in GNNs."}}
{"id": "2508.11313", "pdf": "https://arxiv.org/pdf/2508.11313", "abs": "https://arxiv.org/abs/2508.11313", "authors": ["Weijia Liu", "Jiuxin Cao", "Bo Miao", "Zhiheng Fu", "Xuelin Zhu", "Jiawei Ge", "Bo Liu", "Mehwish Nasim", "Ajmal Mian"], "title": "Denoise-then-Retrieve: Text-Conditioned Video Denoising for Video Moment Retrieval", "categories": ["cs.CV"], "comment": "Accepted by IJCAI 2025", "summary": "Current text-driven Video Moment Retrieval (VMR) methods encode all video\nclips, including irrelevant ones, disrupting multimodal alignment and hindering\noptimization. To this end, we propose a denoise-then-retrieve paradigm that\nexplicitly filters text-irrelevant clips from videos and then retrieves the\ntarget moment using purified multimodal representations. Following this\nparadigm, we introduce the Denoise-then-Retrieve Network (DRNet), comprising\nText-Conditioned Denoising (TCD) and Text-Reconstruction Feedback (TRF)\nmodules. TCD integrates cross-attention and structured state space blocks to\ndynamically identify noisy clips and produce a noise mask to purify multimodal\nvideo representations. TRF further distills a single query embedding from\npurified video representations and aligns it with the text embedding, serving\nas auxiliary supervision for denoising during training. Finally, we perform\nconditional retrieval using text embeddings on purified video representations\nfor accurate VMR. Experiments on Charades-STA and QVHighlights demonstrate that\nour approach surpasses state-of-the-art methods on all metrics. Furthermore,\nour denoise-then-retrieve paradigm is adaptable and can be seamlessly\nintegrated into advanced VMR models to boost performance.", "AI": {"tldr": "A denoise-then-retrieve paradigm (DRNet) improves video moment retrieval by filtering irrelevant clips and aligning purified representations with text.", "motivation": "Current VMR methods encode all video clips, including irrelevant ones, disrupting multimodal alignment and optimization.", "method": "Proposes DRNet with Text-Conditioned Denoising (TCD) and Text-Reconstruction Feedback (TRF) modules to filter noise and align representations.", "result": "Outperforms state-of-the-art methods on Charades-STA and QVHighlights.", "conclusion": "The denoise-then-retrieve paradigm is effective and adaptable for enhancing VMR models."}}
{"id": "2508.11514", "pdf": "https://arxiv.org/pdf/2508.11514", "abs": "https://arxiv.org/abs/2508.11514", "authors": ["Qitong Chu", "Yufeng Yue", "Danya Yao", "Huaxin Pei"], "title": "DiCriTest: Testing Scenario Generation for Decision-Making Agents Considering Diversity and Criticality", "categories": ["cs.LG"], "comment": null, "summary": "The growing deployment of decision-making agents in dynamic environments\nincreases the demand for safety verification. While critical testing scenario\ngeneration has emerged as an appealing verification methodology, effectively\nbalancing diversity and criticality remains a key challenge for existing\nmethods, particularly due to local optima entrapment in high-dimensional\nscenario spaces. To address this limitation, we propose a dual-space guided\ntesting framework that coordinates scenario parameter space and agent behavior\nspace, aiming to generate testing scenarios considering diversity and\ncriticality. Specifically, in the scenario parameter space, a hierarchical\nrepresentation framework combines dimensionality reduction and\nmulti-dimensional subspace evaluation to efficiently localize diverse and\ncritical subspaces. This guides dynamic coordination between two generation\nmodes: local perturbation and global exploration, optimizing critical scenario\nquantity and diversity. Complementarily, in the agent behavior space,\nagent-environment interaction data are leveraged to quantify behavioral\ncriticality/diversity and adaptively support generation mode switching, forming\na closed feedback loop that continuously enhances scenario characterization and\nexploration within the parameter space. Experiments show our framework improves\ncritical scenario generation by an average of 56.23\\% and demonstrates greater\ndiversity under novel parameter-behavior co-driven metrics when tested on five\ndecision-making agents, outperforming state-of-the-art baselines.", "AI": {"tldr": "A dual-space guided testing framework is proposed to balance diversity and criticality in safety verification for decision-making agents, outperforming existing methods.", "motivation": "The need for effective safety verification in dynamic environments, addressing the challenge of balancing diversity and criticality in scenario generation.", "method": "A dual-space framework coordinating scenario parameter space and agent behavior space, using hierarchical representation and feedback loops for optimization.", "result": "Improves critical scenario generation by 56.23% and enhances diversity, outperforming state-of-the-art baselines.", "conclusion": "The framework successfully addresses the challenge of local optima entrapment, offering a robust solution for safety verification."}}
{"id": "2508.11222", "pdf": "https://arxiv.org/pdf/2508.11222", "abs": "https://arxiv.org/abs/2508.11222", "authors": ["Haonan Zhang", "Dongxia Wang", "Yi Liu", "Kexin Chen", "Jiashui Wang", "Xinlei Ying", "Long Liu", "Wenhai Wang"], "title": "ORFuzz: Fuzzing the \"Other Side\" of LLM Safety -- Testing Over-Refusal", "categories": ["cs.SE", "cs.AI", "cs.CL", "cs.IR"], "comment": null, "summary": "Large Language Models (LLMs) increasingly exhibit over-refusal - erroneously\nrejecting benign queries due to overly conservative safety measures - a\ncritical functional flaw that undermines their reliability and usability.\nCurrent methods for testing this behavior are demonstrably inadequate,\nsuffering from flawed benchmarks and limited test generation capabilities, as\nhighlighted by our empirical user study. To the best of our knowledge, this\npaper introduces the first evolutionary testing framework, ORFuzz, for the\nsystematic detection and analysis of LLM over-refusals. ORFuzz uniquely\nintegrates three core components: (1) safety category-aware seed selection for\ncomprehensive test coverage, (2) adaptive mutator optimization using reasoning\nLLMs to generate effective test cases, and (3) OR-Judge, a human-aligned judge\nmodel validated to accurately reflect user perception of toxicity and refusal.\nOur extensive evaluations demonstrate that ORFuzz generates diverse, validated\nover-refusal instances at a rate (6.98% average) more than double that of\nleading baselines, effectively uncovering vulnerabilities. Furthermore,\nORFuzz's outputs form the basis of ORFuzzSet, a new benchmark of 1,855 highly\ntransferable test cases that achieves a superior 63.56% average over-refusal\nrate across 10 diverse LLMs, significantly outperforming existing datasets.\nORFuzz and ORFuzzSet provide a robust automated testing framework and a\nvaluable community resource, paving the way for developing more reliable and\ntrustworthy LLM-based software systems.", "AI": {"tldr": "ORFuzz is an evolutionary testing framework for detecting LLM over-refusals, outperforming baselines with a 6.98% average detection rate and creating a benchmark (ORFuzzSet) with a 63.56% over-refusal rate.", "motivation": "Addressing the critical flaw of over-refusal in LLMs, where benign queries are erroneously rejected due to overly conservative safety measures, undermines reliability and usability.", "method": "ORFuzz integrates safety category-aware seed selection, adaptive mutator optimization using reasoning LLMs, and OR-Judge, a human-aligned judge model.", "result": "ORFuzz detects over-refusals at a 6.98% average rate, double leading baselines, and creates ORFuzzSet, a benchmark with a 63.56% over-refusal rate across 10 LLMs.", "conclusion": "ORFuzz and ORFuzzSet offer a robust testing framework and community resource, advancing reliable and trustworthy LLM-based systems."}}
{"id": "2508.11317", "pdf": "https://arxiv.org/pdf/2508.11317", "abs": "https://arxiv.org/abs/2508.11317", "authors": ["Yuchen Zhou", "Jiayu Tang", "Shuo Yang", "Xiaoyan Xiao", "Yuqin Dai", "Wenhao Yang", "Chao Gou", "Xiaobo Xia", "Tat-Seng Chua"], "title": "Logic Unseen: Revealing the Logical Blindspots of Vision-Language Models", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Vision-Language Models (VLMs), exemplified by CLIP, have emerged as\nfoundational for multimodal intelligence. However, their capacity for logical\nunderstanding remains significantly underexplored, resulting in critical\n''logical blindspots'' that limit their reliability in practical applications.\nTo systematically diagnose this, we introduce LogicBench, a comprehensive\nbenchmark with over 50,000 vision-language pairs across 9 logical categories\nand 4 diverse scenarios: images, videos, anomaly detection, and medical\ndiagnostics. Our evaluation reveals that existing VLMs, even the\nstate-of-the-art ones, fall at over 40 accuracy points below human performance,\nparticularly in challenging tasks like Causality and Conditionality,\nhighlighting their reliance on surface semantics over critical logical\nstructures. To bridge this gap, we propose LogicCLIP, a novel training\nframework designed to boost VLMs' logical sensitivity through advancements in\nboth data generation and optimization objectives. LogicCLIP utilizes\nlogic-aware data generation and a contrastive learning strategy that combines\ncoarse-grained alignment, a fine-grained multiple-choice objective, and a novel\nlogical structure-aware objective. Extensive experiments demonstrate\nLogicCLIP's substantial improvements in logical comprehension across all\nLogicBench domains, significantly outperforming baselines. Moreover, LogicCLIP\nretains, and often surpasses, competitive performance on general\nvision-language benchmarks, demonstrating that the enhanced logical\nunderstanding does not come at the expense of general alignment. We believe\nthat LogicBench and LogicCLIP will be important resources for advancing VLM\nlogical capabilities.", "AI": {"tldr": "LogicBench is introduced to diagnose logical blindspots in Vision-Language Models (VLMs), revealing their underperformance in logical tasks. LogicCLIP, a novel training framework, improves logical comprehension without sacrificing general alignment.", "motivation": "Existing VLMs like CLIP lack logical understanding, limiting their reliability in practical applications.", "method": "LogicBench evaluates VLMs on 50,000+ vision-language pairs across 9 logical categories. LogicCLIP enhances logical sensitivity via logic-aware data generation and contrastive learning.", "result": "VLMs perform 40+ accuracy points below humans in logical tasks. LogicCLIP outperforms baselines and maintains general alignment.", "conclusion": "LogicBench and LogicCLIP advance VLM logical capabilities, bridging the gap between surface semantics and logical understanding."}}
{"id": "2508.11522", "pdf": "https://arxiv.org/pdf/2508.11522", "abs": "https://arxiv.org/abs/2508.11522", "authors": ["Max Guillen", "Philipp Misof", "Jan E. Gerken"], "title": "Finite-Width Neural Tangent Kernels from Feynman Diagrams", "categories": ["cs.LG", "hep-th"], "comment": "11 pages + appendices", "summary": "Neural tangent kernels (NTKs) are a powerful tool for analyzing deep,\nnon-linear neural networks. In the infinite-width limit, NTKs can easily be\ncomputed for most common architectures, yielding full analytic control over the\ntraining dynamics. However, at infinite width, important properties of training\nsuch as NTK evolution or feature learning are absent. Nevertheless, finite\nwidth effects can be included by computing corrections to the Gaussian\nstatistics at infinite width. We introduce Feynman diagrams for computing\nfinite-width corrections to NTK statistics. These dramatically simplify the\nnecessary algebraic manipulations and enable the computation of layer-wise\nrecursive relations for arbitrary statistics involving preactivations, NTKs and\ncertain higher-derivative tensors (dNTK and ddNTK) required to predict the\ntraining dynamics at leading order. We demonstrate the feasibility of our\nframework by extending stability results for deep networks from preactivations\nto NTKs and proving the absence of finite-width corrections for scale-invariant\nnonlinearities such as ReLU on the diagonal of the Gram matrix of the NTK. We\nvalidate our results with numerical experiments.", "AI": {"tldr": "The paper introduces Feynman diagrams to compute finite-width corrections for Neural Tangent Kernels (NTKs), simplifying analysis of deep neural networks beyond the infinite-width limit.", "motivation": "To address the limitations of NTKs in the infinite-width limit, where key training dynamics like NTK evolution and feature learning are absent, by incorporating finite-width effects.", "method": "Uses Feynman diagrams to compute finite-width corrections for NTK statistics, enabling recursive relations for preactivations, NTKs, and higher-derivative tensors (dNTK and ddNTK).", "result": "Demonstrates feasibility by extending stability results to NTKs and proving no finite-width corrections for scale-invariant nonlinearities like ReLU on the NTK Gram matrix diagonal.", "conclusion": "The framework is validated numerically, offering a practical tool for analyzing finite-width neural networks."}}
{"id": "2508.11224", "pdf": "https://arxiv.org/pdf/2508.11224", "abs": "https://arxiv.org/abs/2508.11224", "authors": ["Kentaro Onda", "Satoru Fukayama", "Daisuke Saito", "Nobuaki Minematsu"], "title": "Benchmarking Prosody Encoding in Discrete Speech Tokens", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "Accepted by ASRU2025", "summary": "Recently, discrete tokens derived from self-supervised learning (SSL) models\nvia k-means clustering have been actively studied as pseudo-text in speech\nlanguage models and as efficient intermediate representations for various\ntasks. However, these discrete tokens are typically learned in advance,\nseparately from the training of language models or downstream tasks. As a\nresult, choices related to discretization, such as the SSL model used or the\nnumber of clusters, must be made heuristically. In particular, speech language\nmodels are expected to understand and generate responses that reflect not only\nthe semantic content but also prosodic features. Yet, there has been limited\nresearch on the ability of discrete tokens to capture prosodic information. To\naddress this gap, this study conducts a comprehensive analysis focusing on\nprosodic encoding based on their sensitivity to the artificially modified\nprosody, aiming to provide practical guidelines for designing discrete tokens.", "AI": {"tldr": "The paper analyzes how discrete tokens from SSL models capture prosodic information, aiming to improve their design for speech language models.", "motivation": "Discrete tokens from SSL models are used in speech language models, but their ability to capture prosodic features is understudied. This study aims to fill that gap.", "method": "The study conducts a comprehensive analysis of prosodic encoding by testing sensitivity to artificially modified prosody.", "result": "Findings provide insights into how discrete tokens handle prosodic information.", "conclusion": "The study offers practical guidelines for designing discrete tokens to better capture prosodic features in speech language models."}}
{"id": "2508.11323", "pdf": "https://arxiv.org/pdf/2508.11323", "abs": "https://arxiv.org/abs/2508.11323", "authors": ["Haonan Zhang", "Xinyao Wang", "Boxi Wu", "Tu Zheng", "Wang Yunhua", "Zheng Yang"], "title": "Delving into Dynamic Scene Cue-Consistency for Robust 3D Multi-Object Tracking", "categories": ["cs.CV"], "comment": null, "summary": "3D multi-object tracking is a critical and challenging task in the field of\nautonomous driving. A common paradigm relies on modeling individual object\nmotion, e.g., Kalman filters, to predict trajectories. While effective in\nsimple scenarios, this approach often struggles in crowded environments or with\ninaccurate detections, as it overlooks the rich geometric relationships between\nobjects. This highlights the need to leverage spatial cues. However, existing\ngeometry-aware methods can be susceptible to interference from irrelevant\nobjects, leading to ambiguous features and incorrect associations. To address\nthis, we propose focusing on cue-consistency: identifying and matching stable\nspatial patterns over time. We introduce the Dynamic Scene Cue-Consistency\nTracker (DSC-Track) to implement this principle. Firstly, we design a unified\nspatiotemporal encoder using Point Pair Features (PPF) to learn discriminative\ntrajectory embeddings while suppressing interference. Secondly, our\ncue-consistency transformer module explicitly aligns consistent feature\nrepresentations between historical tracks and current detections. Finally, a\ndynamic update mechanism preserves salient spatiotemporal information for\nstable online tracking. Extensive experiments on the nuScenes and Waymo Open\nDatasets validate the effectiveness and robustness of our approach. On the\nnuScenes benchmark, for instance, our method achieves state-of-the-art\nperformance, reaching 73.2% and 70.3% AMOTA on the validation and test sets,\nrespectively.", "AI": {"tldr": "The paper proposes DSC-Track, a 3D multi-object tracking method for autonomous driving, leveraging cue-consistency and spatial patterns to improve accuracy in crowded scenes.", "motivation": "Existing methods struggle in crowded environments due to overlooked geometric relationships and interference from irrelevant objects.", "method": "DSC-Track uses a spatiotemporal encoder with Point Pair Features (PPF) and a cue-consistency transformer to align features, along with a dynamic update mechanism.", "result": "Achieves 73.2% and 70.3% AMOTA on nuScenes validation and test sets, outperforming state-of-the-art methods.", "conclusion": "DSC-Track effectively addresses challenges in 3D tracking by focusing on stable spatial patterns, demonstrating superior performance."}}
{"id": "2508.11528", "pdf": "https://arxiv.org/pdf/2508.11528", "abs": "https://arxiv.org/abs/2508.11528", "authors": ["Juhi Soni", "Markus Lange-Hegermann", "Stefan Windmann"], "title": "Physics-Informed Diffusion Models for Unsupervised Anomaly Detection in Multivariate Time Series", "categories": ["cs.LG"], "comment": "16 pages, 5 figures", "summary": "We propose an unsupervised anomaly detection approach based on a\nphysics-informed diffusion model for multivariate time series data. Over the\npast years, diffusion model has demonstrated its effectiveness in forecasting,\nimputation, generation, and anomaly detection in the time series domain. In\nthis paper, we present a new approach for learning the physics-dependent\ntemporal distribution of multivariate time series data using a weighted\nphysics-informed loss during diffusion model training. A weighted\nphysics-informed loss is constructed using a static weight schedule. This\napproach enables a diffusion model to accurately approximate underlying data\ndistribution, which can influence the unsupervised anomaly detection\nperformance. Our experiments on synthetic and real-world datasets show that\nphysics-informed training improves the F1 score in anomaly detection; it\ngenerates better data diversity and log-likelihood. Our model outperforms\nbaseline approaches, additionally, it surpasses prior physics-informed work and\npurely data-driven diffusion models on a synthetic dataset and one real-world\ndataset while remaining competitive on others.", "AI": {"tldr": "An unsupervised anomaly detection method using a physics-informed diffusion model for multivariate time series, improving F1 scores and outperforming baselines.", "motivation": "To enhance anomaly detection in time series by incorporating physics-dependent temporal distributions via a weighted physics-informed loss in diffusion models.", "method": "Uses a weighted physics-informed loss with a static weight schedule during diffusion model training to approximate data distribution for better anomaly detection.", "result": "Improves F1 scores, data diversity, and log-likelihood; outperforms baselines and prior physics-informed or purely data-driven models on some datasets.", "conclusion": "The physics-informed diffusion model is effective for unsupervised anomaly detection, offering superior performance and versatility."}}
{"id": "2508.11330", "pdf": "https://arxiv.org/pdf/2508.11330", "abs": "https://arxiv.org/abs/2508.11330", "authors": ["Yanghao Wang", "Long Chen"], "title": "Noise Matters: Optimizing Matching Noise for Diffusion Classifiers", "categories": ["cs.CV"], "comment": null, "summary": "Although today's pretrained discriminative vision-language models (e.g.,\nCLIP) have demonstrated strong perception abilities, such as zero-shot image\nclassification, they also suffer from the bag-of-words problem and spurious\nbias. To mitigate these problems, some pioneering studies leverage powerful\ngenerative models (e.g., pretrained diffusion models) to realize generalizable\nimage classification, dubbed Diffusion Classifier (DC). Specifically, by\nrandomly sampling a Gaussian noise, DC utilizes the differences of denoising\neffects with different category conditions to classify categories.\nUnfortunately, an inherent and notorious weakness of existing DCs is noise\ninstability: different random sampled noises lead to significant performance\nchanges. To achieve stable classification performance, existing DCs always\nensemble the results of hundreds of sampled noises, which significantly reduces\nthe classification speed. To this end, we firstly explore the role of noise in\nDC, and conclude that: there are some ``good noises'' that can relieve the\ninstability. Meanwhile, we argue that these good noises should meet two\nprinciples: Frequency Matching and Spatial Matching. Regarding both principles,\nwe propose a novel Noise Optimization method to learn matching (i.e., good)\nnoise for DCs: NoOp. For frequency matching, NoOp first optimizes a\ndataset-specific noise: Given a dataset and a timestep t, optimize one randomly\ninitialized parameterized noise. For Spatial Matching, NoOp trains a\nMeta-Network that adopts an image as input and outputs image-specific noise\noffset. The sum of optimized noise and noise offset will be used in DC to\nreplace random noise. Extensive ablations on various datasets demonstrated the\neffectiveness of NoOp.", "AI": {"tldr": "The paper introduces NoOp, a noise optimization method for Diffusion Classifiers (DC) to address noise instability, improving classification speed and performance by learning dataset-specific and image-specific noises.", "motivation": "Existing Diffusion Classifiers (DCs) suffer from noise instability, requiring hundreds of noise samples for stable performance, which slows classification. The paper aims to identify and optimize 'good noises' to mitigate this issue.", "method": "NoOp learns matching noises through Frequency Matching (optimizing dataset-specific noise) and Spatial Matching (training a Meta-Network for image-specific noise offsets). These optimized noises replace random noise in DCs.", "result": "Extensive experiments show NoOp effectively improves classification stability and speed by reducing reliance on noise ensembling.", "conclusion": "NoOp successfully addresses noise instability in DCs by optimizing noises, enhancing both performance and efficiency."}}
{"id": "2508.11529", "pdf": "https://arxiv.org/pdf/2508.11529", "abs": "https://arxiv.org/abs/2508.11529", "authors": ["George Paterakis", "Andrea Castellani", "George Papoutsoglou", "Tobias Rodemann", "Ioannis Tsamardinos"], "title": "A Comprehensive Perspective on Explainable AI across the Machine Learning Workflow", "categories": ["cs.LG", "cs.AI"], "comment": "Preprint. Currently under review at \"Artificial Intelligence Review\"\n  journal", "summary": "Artificial intelligence is reshaping science and industry, yet many users\nstill regard its models as opaque \"black boxes\". Conventional explainable\nartificial-intelligence methods clarify individual predictions but overlook the\nupstream decisions and downstream quality checks that determine whether\ninsights can be trusted. In this work, we present Holistic Explainable\nArtificial Intelligence (HXAI), a user-centric framework that embeds\nexplanation into every stage of the data-analysis workflow and tailors those\nexplanations to users. HXAI unifies six components (data, analysis set-up,\nlearning process, model output, model quality, communication channel) into a\nsingle taxonomy and aligns each component with the needs of domain experts,\ndata analysts and data scientists. A 112-item question bank covers these needs;\nour survey of contemporary tools highlights critical coverage gaps. Grounded in\ntheories of human explanation, principles from human-computer interaction and\nfindings from empirical user studies, HXAI identifies the characteristics that\nmake explanations clear, actionable and cognitively manageable. A comprehensive\ntaxonomy operationalises these insights, reducing terminological ambiguity and\nenabling rigorous coverage analysis of existing toolchains. We further\ndemonstrate how AI agents that embed large-language models can orchestrate\ndiverse explanation techniques, translating technical artifacts into\nstakeholder-specific narratives that bridge the gap between AI developers and\ndomain experts. Departing from traditional surveys or perspective articles,\nthis work melds concepts from multiple disciplines, lessons from real-world\nprojects and a critical synthesis of the literature to advance a novel,\nend-to-end viewpoint on transparency, trustworthiness and responsible AI\ndeployment.", "AI": {"tldr": "The paper introduces Holistic Explainable AI (HXAI), a framework embedding explanations into every stage of the AI workflow, tailored to users, and unifying six components into a taxonomy.", "motivation": "Address the opacity of AI models by providing a comprehensive, user-centric explanation framework that covers all stages of the workflow.", "method": "Proposes HXAI, a taxonomy unifying six components (data, analysis set-up, learning process, model output, model quality, communication channel) and aligns them with user needs. Includes a 112-item question bank and survey of tools.", "result": "Identifies characteristics for clear, actionable explanations and demonstrates how AI agents with large-language models can translate technical details into stakeholder-specific narratives.", "conclusion": "HXAI advances transparency, trustworthiness, and responsible AI deployment by integrating multidisciplinary insights and real-world lessons."}}
{"id": "2508.11200", "pdf": "https://arxiv.org/pdf/2508.11200", "abs": "https://arxiv.org/abs/2508.11200", "authors": ["Hongbin Lin", "Bin Li", "Kwok Wai Samuel Au"], "title": "Visuomotor Grasping with World Models for Surgical Robots", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Grasping is a fundamental task in robot-assisted surgery (RAS), and\nautomating it can reduce surgeon workload while enhancing efficiency, safety,\nand consistency beyond teleoperated systems. Most prior approaches rely on\nexplicit object pose tracking or handcrafted visual features, limiting their\ngeneralization to novel objects, robustness to visual disturbances, and the\nability to handle deformable objects. Visuomotor learning offers a promising\nalternative, but deploying it in RAS presents unique challenges, such as low\nsignal-to-noise ratio in visual observations, demands for high safety and\nmillimeter-level precision, as well as the complex surgical environment. This\npaper addresses three key challenges: (i) sim-to-real transfer of visuomotor\npolicies to ex vivo surgical scenes, (ii) visuomotor learning using only a\nsingle stereo camera pair -- the standard RAS setup, and (iii) object-agnostic\ngrasping with a single policy that generalizes to diverse, unseen surgical\nobjects without retraining or task-specific models. We introduce Grasp Anything\nfor Surgery V2 (GASv2), a visuomotor learning framework for surgical grasping.\nGASv2 leverages a world-model-based architecture and a surgical perception\npipeline for visual observations, combined with a hybrid control system for\nsafe execution. We train the policy in simulation using domain randomization\nfor sim-to-real transfer and deploy it on a real robot in both phantom-based\nand ex vivo surgical settings, using only a single pair of endoscopic cameras.\nExtensive experiments show our policy achieves a 65% success rate in both\nsettings, generalizes to unseen objects and grippers, and adapts to diverse\ndisturbances, demonstrating strong performance, generality, and robustness.", "AI": {"tldr": "GASv2 is a visuomotor learning framework for surgical grasping, addressing sim-to-real transfer, single-camera learning, and object-agnostic generalization, achieving 65% success in diverse settings.", "motivation": "Automating grasping in robot-assisted surgery (RAS) can improve efficiency, safety, and consistency, but prior methods lack generalization and robustness. Visuomotor learning is promising but faces challenges like low visual signal-to-noise and high precision demands.", "method": "GASv2 uses a world-model-based architecture and surgical perception pipeline for visual observations, combined with hybrid control for safe execution. It is trained in simulation with domain randomization and deployed on a real robot using a single stereo camera pair.", "result": "The policy achieves a 65% success rate in phantom-based and ex vivo settings, generalizes to unseen objects and grippers, and adapts to disturbances.", "conclusion": "GASv2 demonstrates strong performance, generality, and robustness in surgical grasping, addressing key challenges in RAS."}}
{"id": "2508.11334", "pdf": "https://arxiv.org/pdf/2508.11334", "abs": "https://arxiv.org/abs/2508.11334", "authors": ["Md Asgor Hossain Reaj", "Rajan Das Gupta", "Md Yeasin Rahat", "Nafiz Fahad", "Md Jawadul Hasan", "Tze Hui Liew"], "title": "GANDiff FR: Hybrid GAN Diffusion Synthesis for Causal Bias Attribution in Face Recognition", "categories": ["cs.CV"], "comment": "Accepted in ICCVDM '25", "summary": "We introduce GANDiff FR, the first synthetic framework that precisely\ncontrols demographic and environmental factors to measure, explain, and reduce\nbias with reproducible rigor. GANDiff FR unifies StyleGAN3-based\nidentity-preserving generation with diffusion-based attribute control, enabling\nfine-grained manipulation of pose around 30 degrees, illumination (four\ndirections), and expression (five levels) under ceteris paribus conditions. We\nsynthesize 10,000 demographically balanced faces across five cohorts validated\nfor realism via automated detection (98.2%) and human review (89%) to isolate\nand quantify bias drivers. Benchmarking ArcFace, CosFace, and AdaFace under\nmatched operating points shows AdaFace reduces inter-group TPR disparity by 60%\n(2.5% vs. 6.3%), with illumination accounting for 42% of residual bias.\nCross-dataset evaluation on RFW, BUPT, and CASIA WebFace confirms strong\nsynthetic-to-real transfer (r 0.85). Despite around 20% computational overhead\nrelative to pure GANs, GANDiff FR yields three times more attribute-conditioned\nvariants, establishing a reproducible, regulation-aligned (EU AI Act) standard\nfor fairness auditing. Code and data are released to support transparent,\nscalable bias evaluation.", "AI": {"tldr": "GANDiff FR is a synthetic framework combining StyleGAN3 and diffusion models to control demographic/environmental factors, reducing bias in face recognition systems.", "motivation": "To measure, explain, and reduce bias in face recognition by precisely controlling factors like pose, illumination, and expression.", "method": "Unifies StyleGAN3 for identity-preserving generation with diffusion models for attribute control, synthesizing 10,000 demographically balanced faces.", "result": "AdaFace reduces inter-group TPR disparity by 60%, with illumination accounting for 42% of residual bias. Strong synthetic-to-real transfer (r 0.85) is confirmed.", "conclusion": "GANDiff FR provides a reproducible, regulation-aligned standard for fairness auditing, despite a 20% computational overhead."}}
{"id": "2508.11530", "pdf": "https://arxiv.org/pdf/2508.11530", "abs": "https://arxiv.org/abs/2508.11530", "authors": ["Lianshuai Guo", "Zhongzheng Yuan", "Xunkai Li", "Yinlin Zhu", "Meixia Qu", "Wenyu Wang"], "title": "DFed-SST: Building Semantic- and Structure-aware Topologies for Decentralized Federated Graph Learning", "categories": ["cs.LG"], "comment": null, "summary": "Decentralized Federated Learning (DFL) has emerged as a robust distributed\nparadigm that circumvents the single-point-of-failure and communication\nbottleneck risks of centralized architectures. However, a significant challenge\narises as existing DFL optimization strategies, primarily designed for tasks\nsuch as computer vision, fail to address the unique topological information\ninherent in the local subgraph. Notably, while Federated Graph Learning (FGL)\nis tailored for graph data, it is predominantly implemented in a centralized\nserver-client model, failing to leverage the benefits of decentralization.To\nbridge this gap, we propose DFed-SST, a decentralized federated graph learning\nframework with adaptive communication. The core of our method is a\ndual-topology adaptive communication mechanism that leverages the unique\ntopological features of each client's local subgraph to dynamically construct\nand optimize the inter-client communication topology. This allows our framework\nto guide model aggregation efficiently in the face of heterogeneity. Extensive\nexperiments on eight real-world datasets consistently demonstrate the\nsuperiority of DFed-SST, achieving 3.26% improvement in average accuracy over\nbaseline methods.", "AI": {"tldr": "DFed-SST is a decentralized federated graph learning framework with adaptive communication, addressing the limitations of existing DFL and FGL methods by leveraging local subgraph topology for efficient model aggregation.", "motivation": "Existing DFL and FGL methods fail to address local subgraph topology or leverage decentralization benefits, creating a gap in efficient graph learning.", "method": "DFed-SST uses a dual-topology adaptive communication mechanism to dynamically optimize inter-client communication based on local subgraph features.", "result": "Experiments on eight datasets show DFed-SST outperforms baselines with a 3.26% average accuracy improvement.", "conclusion": "DFed-SST effectively bridges the gap in decentralized federated graph learning by optimizing communication and leveraging local topology."}}
{"id": "2508.11203", "pdf": "https://arxiv.org/pdf/2508.11203", "abs": "https://arxiv.org/abs/2508.11203", "authors": ["Seungmi Lee", "Kwan Yun", "Junyong Noh"], "title": "StyleMM: Stylized 3D Morphable Face Model via Text-Driven Aligned Image Translation", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.MM", "51-04", "I.3.8; I.4.9"], "comment": "Pacific graphics 2025, CGF, 15 pages", "summary": "We introduce StyleMM, a novel framework that can construct a stylized 3D\nMorphable Model (3DMM) based on user-defined text descriptions specifying a\ntarget style. Building upon a pre-trained mesh deformation network and a\ntexture generator for original 3DMM-based realistic human faces, our approach\nfine-tunes these models using stylized facial images generated via text-guided\nimage-to-image (i2i) translation with a diffusion model, which serve as\nstylization targets for the rendered mesh. To prevent undesired changes in\nidentity, facial alignment, or expressions during i2i translation, we introduce\na stylization method that explicitly preserves the facial attributes of the\nsource image. By maintaining these critical attributes during image\nstylization, the proposed approach ensures consistent 3D style transfer across\nthe 3DMM parameter space through image-based training. Once trained, StyleMM\nenables feed-forward generation of stylized face meshes with explicit control\nover shape, expression, and texture parameters, producing meshes with\nconsistent vertex connectivity and animatability. Quantitative and qualitative\nevaluations demonstrate that our approach outperforms state-of-the-art methods\nin terms of identity-level facial diversity and stylization capability. The\ncode and videos are available at\n[kwanyun.github.io/stylemm_page](kwanyun.github.io/stylemm_page).", "AI": {"tldr": "StyleMM creates stylized 3DMMs from text descriptions, preserving facial attributes during stylization for consistent 3D style transfer.", "motivation": "To enable stylized 3D face generation with explicit control over shape, expression, and texture while preserving identity and alignment.", "method": "Fine-tunes pre-trained mesh deformation and texture models using stylized images from text-guided diffusion, with explicit attribute preservation.", "result": "Outperforms state-of-the-art in facial diversity and stylization, enabling feed-forward stylized mesh generation.", "conclusion": "StyleMM effectively transfers styles to 3DMMs while maintaining facial attributes, offering superior performance and control."}}
{"id": "2508.11339", "pdf": "https://arxiv.org/pdf/2508.11339", "abs": "https://arxiv.org/abs/2508.11339", "authors": ["Mingxiao Ma", "Shunyao Zhu", "Guoliang Kang"], "title": "Index-Aligned Query Distillation for Transformer-based Incremental Object Detection", "categories": ["cs.CV"], "comment": "12 pages, 5 figures", "summary": "Incremental object detection (IOD) aims to continuously expand the capability\nof a model to detect novel categories while preserving its performance on\npreviously learned ones. When adopting a transformer-based detection model to\nperform IOD, catastrophic knowledge forgetting may inevitably occur, meaning\nthe detection performance on previously learned categories may severely\ndegenerate. Previous typical methods mainly rely on knowledge distillation (KD)\nto mitigate the catastrophic knowledge forgetting of transformer-based\ndetection models. Specifically, they utilize Hungarian Matching to build a\ncorrespondence between the queries of the last-phase and current-phase\ndetection models and align the classifier and regressor outputs between matched\nqueries to avoid knowledge forgetting. However, we observe that in IOD task,\nHungarian Matching is not a good choice. With Hungarian Matching, the query of\nthe current-phase model may match different queries of the last-phase model at\ndifferent iterations during KD. As a result, the knowledge encoded in each\nquery may be reshaped towards new categories, leading to the forgetting of\npreviously encoded knowledge of old categories. Based on our observations, we\npropose a new distillation approach named Index-Aligned Query Distillation\n(IAQD) for transformer-based IOD. Beyond using Hungarian Matching, IAQD\nestablishes a correspondence between queries of the previous and current phase\nmodels that have the same index. Moreover, we perform index-aligned\ndistillation only on partial queries which are critical for the detection of\nprevious categories. In this way, IAQD largely preserves the previous semantic\nand spatial encoding capabilities without interfering with the learning of new\ncategories. Extensive experiments on representative benchmarks demonstrate that\nIAQD effectively mitigates knowledge forgetting, achieving new state-of-the-art\nperformance.", "AI": {"tldr": "The paper introduces Index-Aligned Query Distillation (IAQD) to mitigate catastrophic knowledge forgetting in transformer-based incremental object detection, outperforming previous methods.", "motivation": "Catastrophic knowledge forgetting in transformer-based incremental object detection (IOD) when using Hungarian Matching for knowledge distillation (KD).", "method": "Proposes IAQD, which aligns queries by index and focuses on critical queries for old categories, avoiding interference with new category learning.", "result": "IAQD effectively mitigates forgetting and achieves state-of-the-art performance on benchmarks.", "conclusion": "IAQD is a superior approach for transformer-based IOD, preserving old knowledge while learning new categories."}}
{"id": "2508.11542", "pdf": "https://arxiv.org/pdf/2508.11542", "abs": "https://arxiv.org/abs/2508.11542", "authors": ["Nicole Aretz", "Karen Willcox"], "title": "Nested Operator Inference for Adaptive Data-Driven Learning of Reduced-order Models", "categories": ["cs.LG", "cs.CE", "cs.NA", "math.NA"], "comment": null, "summary": "This paper presents a data-driven, nested Operator Inference (OpInf) approach\nfor learning physics-informed reduced-order models (ROMs) from snapshot data of\nhigh-dimensional dynamical systems. The approach exploits the inherent\nhierarchy within the reduced space to iteratively construct initial guesses for\nthe OpInf learning problem that prioritize the interactions of the dominant\nmodes. The initial guess computed for any target reduced dimension corresponds\nto a ROM with provably smaller or equal snapshot reconstruction error than with\nstandard OpInf. Moreover, our nested OpInf algorithm can be warm-started from\npreviously learned models, enabling versatile application scenarios involving\ndynamic basis and model form updates. We demonstrate the performance of our\nalgorithm on a cubic heat conduction problem, with nested OpInf achieving a\nfour times smaller error than standard OpInf at a comparable offline time.\nFurther, we apply nested OpInf to a large-scale, parameterized model of the\nGreenland ice sheet where, despite model form approximation errors, it learns a\nROM with, on average, 3% error and computational speed-up factor above 19,000.", "AI": {"tldr": "A nested Operator Inference (OpInf) method is introduced to learn physics-informed reduced-order models (ROMs) efficiently, outperforming standard OpInf in accuracy and computational speed.", "motivation": "To improve the accuracy and efficiency of learning ROMs from high-dimensional dynamical systems by leveraging hierarchical reduced spaces and dynamic updates.", "method": "A nested OpInf approach iteratively constructs initial guesses prioritizing dominant modes, warm-starts from learned models, and handles dynamic basis updates.", "result": "Achieves four times smaller error than standard OpInf in a heat conduction problem and 3% error with 19,000x speed-up in a Greenland ice sheet model.", "conclusion": "Nested OpInf is a versatile, efficient method for learning accurate ROMs, suitable for large-scale and dynamic applications."}}
{"id": "2508.11204", "pdf": "https://arxiv.org/pdf/2508.11204", "abs": "https://arxiv.org/abs/2508.11204", "authors": ["Hongbin Lin", "Juan Rojas", "Kwok Wai Samuel Au"], "title": "Multi-Group Equivariant Augmentation for Reinforcement Learning in Robot Manipulation", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Sampling efficiency is critical for deploying visuomotor learning in\nreal-world robotic manipulation. While task symmetry has emerged as a promising\ninductive bias to improve efficiency, most prior work is limited to isometric\nsymmetries -- applying the same group transformation to all task objects across\nall timesteps. In this work, we explore non-isometric symmetries, applying\nmultiple independent group transformations across spatial and temporal\ndimensions to relax these constraints. We introduce a novel formulation of the\npartially observable Markov decision process (POMDP) that incorporates the\nnon-isometric symmetry structures, and propose a simple yet effective data\naugmentation method, Multi-Group Equivariance Augmentation (MEA). We integrate\nMEA with offline reinforcement learning to enhance sampling efficiency, and\nintroduce a voxel-based visual representation that preserves translational\nequivariance. Extensive simulation and real-robot experiments across two\nmanipulation domains demonstrate the effectiveness of our approach.", "AI": {"tldr": "The paper introduces a method to improve sampling efficiency in visuomotor learning for robotic manipulation by leveraging non-isometric symmetries, proposing a novel POMDP formulation and a data augmentation technique called MEA.", "motivation": "Sampling efficiency is crucial for real-world robotic manipulation, and while task symmetry helps, prior work is limited to isometric symmetries. This work explores non-isometric symmetries for better flexibility.", "method": "The authors propose a POMDP formulation incorporating non-isometric symmetries and introduce MEA, a data augmentation method. They integrate MEA with offline reinforcement learning and use a voxel-based visual representation.", "result": "Experiments in simulation and real-robot settings across two manipulation domains validate the effectiveness of the approach.", "conclusion": "The proposed method enhances sampling efficiency by leveraging non-isometric symmetries, demonstrating practical benefits in robotic manipulation."}}
{"id": "2508.11340", "pdf": "https://arxiv.org/pdf/2508.11340", "abs": "https://arxiv.org/abs/2508.11340", "authors": ["Yuanlin Liu", "Zhihan Zhou", "Mingqiang Wei", "Youyi Song"], "title": "Cost-Effective Active Labeling for Data-Efficient Cervical Cell Classification", "categories": ["cs.CV", "q-bio.TO"], "comment": "accepted by CW2025", "summary": "Information on the number and category of cervical cells is crucial for the\ndiagnosis of cervical cancer. However, existing classification methods capable\nof automatically measuring this information require the training dataset to be\nrepresentative, which consumes an expensive or even unaffordable human cost. We\nherein propose active labeling that enables us to construct a representative\ntraining dataset using a much smaller human cost for data-efficient cervical\ncell classification. This cost-effective method efficiently leverages the\nclassifier's uncertainty on the unlabeled cervical cell images to accurately\nselect images that are most beneficial to label. With a fast estimation of the\nuncertainty, this new algorithm exhibits its validity and effectiveness in\nenhancing the representative ability of the constructed training dataset. The\nextensive empirical results confirm its efficacy again in navigating the usage\nof human cost, opening the avenue for data-efficient cervical cell\nclassification.", "AI": {"tldr": "Proposes active labeling for cost-efficient cervical cell classification by selecting uncertain images to label, reducing human effort.", "motivation": "Existing methods require expensive human cost for representative training datasets in cervical cell classification.", "method": "Active labeling leverages classifier uncertainty on unlabeled images to select beneficial ones for labeling, reducing human cost.", "result": "The method enhances dataset representativeness and navigates human cost effectively.", "conclusion": "Active labeling opens avenues for data-efficient cervical cell classification."}}
{"id": "2508.11553", "pdf": "https://arxiv.org/pdf/2508.11553", "abs": "https://arxiv.org/abs/2508.11553", "authors": ["Jinghui Wang", "Shaojie Wang", "Yinghan Cui", "Xuxing Chen", "Chao Wang", "Xiaojiang Zhang", "Minglei Zhang", "Jiarong Zhang", "Wenhao Zhuang", "Yuchen Cao", "Wankang Bao", "Haimo Li", "Zheng Lin", "Huiming Wang", "Haoyang Huang", "Zongxian Feng", "Zizheng Zhan", "Ken Deng", "Wen Xiang", "Huaixi Tang", "Kun Wu", "Mengtong Li", "Mengfei Xie", "Junyi Peng", "Haotian Zhang", "Bin Chen", "Bing Yu"], "title": "SeamlessFlow: A Trainer Agent Isolation RL Framework Achieving Bubble-Free Pipelines via Tag Scheduling", "categories": ["cs.LG"], "comment": null, "summary": "We introduce SeamlessFlow, a server based reinforcement learning (RL)\nframework that addresses two core challenges in industrial scale RL: (1)\ndecoupling RL training from the complex execution flow of agents; (2)\nmaximizing GPU utilization with minimal idle time while preserving the\nstability and scalability required for large-scale deployments. First,\nSeamlessFlow introduces a data plane that decouples the RL trainer from\ndiverse, complex agent implementations while sustaining high throughput. A\ncentral trajectory manager maintains complete interaction histories and\nsupports partial rollout, allowing rollout to pause for weight updates and\nresume seamlessly, keeping agents unaware of service interruptions. Second, we\npropose a tag driven scheduling paradigm that abstracts hardware into\ncapability tagged resources, unifying colocated and disaggregated\narchitectures. Based on this, SeamlessFlow introduces a spatiotemporal\nmultiplexing pipeline that dynamically reassigns idle training nodes to rollout\nin a train rollout separated setup, eliminating pipeline bubbles and fully\nexploiting heterogeneous cluster resources. By combining these innovations,\nSeamlessFlow delivers both stability and high performance, making it well\nsuited for multi agent, long horizon, and other complex RL tasks.", "AI": {"tldr": "SeamlessFlow is a server-based RL framework that decouples training from agent execution and optimizes GPU usage, ensuring stability and scalability for large-scale RL tasks.", "motivation": "Addressing challenges in industrial-scale RL, such as decoupling training from complex agent execution and maximizing GPU utilization without sacrificing stability.", "method": "Introduces a data plane for decoupling RL training from agents and a tag-driven scheduling paradigm for efficient resource use. Also includes a spatiotemporal multiplexing pipeline to eliminate idle time.", "result": "Achieves high throughput, stability, and scalability, making it suitable for complex RL tasks like multi-agent and long-horizon scenarios.", "conclusion": "SeamlessFlow effectively combines stability and performance, addressing key challenges in large-scale RL deployments."}}
{"id": "2508.11566", "pdf": "https://arxiv.org/pdf/2508.11566", "abs": "https://arxiv.org/abs/2508.11566", "authors": ["Shaun Cassini", "Thomas Hain", "Anton Ragni"], "title": "Emphasis Sensitivity in Speech Representations", "categories": ["eess.AS", "cs.CL"], "comment": "Accepted to IEEE ASRU 2025", "summary": "This work investigates whether modern speech models are sensitive to prosodic\nemphasis - whether they encode emphasized and neutral words in systematically\ndifferent ways. Prior work typically relies on isolated acoustic correlates\n(e.g., pitch, duration) or label prediction, both of which miss the relational\nstructure of emphasis. This paper proposes a residual-based framework, defining\nemphasis as the difference between paired neutral and emphasized word\nrepresentations. Analysis on self-supervised speech models shows that these\nresiduals correlate strongly with duration changes and perform poorly at word\nidentity prediction, indicating a structured, relational encoding of prosodic\nemphasis. In ASR fine-tuned models, residuals occupy a subspace up to 50% more\ncompact than in pre-trained models, further suggesting that emphasis is encoded\nas a consistent, low-dimensional transformation that becomes more structured\nwith task-specific learning.", "AI": {"tldr": "The paper examines how modern speech models encode prosodic emphasis, proposing a residual-based framework to analyze differences between neutral and emphasized words. Results show structured encoding of emphasis, with ASR models refining this encoding further.", "motivation": "To determine if speech models systematically differentiate between emphasized and neutral words, addressing gaps in prior work that overlooked relational structure.", "method": "A residual-based framework is introduced, defining emphasis as the difference between paired neutral and emphasized word representations. Analysis is conducted on self-supervised and ASR fine-tuned models.", "result": "Residuals correlate with duration changes and perform poorly in word identity prediction, indicating structured encoding. ASR models show residuals in a 50% more compact subspace, suggesting refined emphasis encoding.", "conclusion": "Prosodic emphasis is encoded relationally, with ASR fine-tuning enhancing its structured, low-dimensional representation."}}
{"id": "2508.11341", "pdf": "https://arxiv.org/pdf/2508.11341", "abs": "https://arxiv.org/abs/2508.11341", "authors": ["Katarzyna Filus", "Jorge M. Cruz-Duarte"], "title": "Semantically Guided Adversarial Testing of Vision Models Using Language Models", "categories": ["cs.CV", "cs.CR", "cs.LG", "68T45, 68T01, 68T07, 68T10, 68M25", "I.2.10; I.5.4; I.2.6; I.2.7; K.6.5"], "comment": "12 pages, 4 figures, 3 tables. Submitted for peer review", "summary": "In targeted adversarial attacks on vision models, the selection of the target\nlabel is a critical yet often overlooked determinant of attack success. This\ntarget label corresponds to the class that the attacker aims to force the model\nto predict. Now, existing strategies typically rely on randomness, model\npredictions, or static semantic resources, limiting interpretability,\nreproducibility, or flexibility. This paper then proposes a semantics-guided\nframework for adversarial target selection using the cross-modal knowledge\ntransfer from pretrained language and vision-language models. We evaluate\nseveral state-of-the-art models (BERT, TinyLLAMA, and CLIP) as similarity\nsources to select the most and least semantically related labels with respect\nto the ground truth, forming best- and worst-case adversarial scenarios. Our\nexperiments on three vision models and five attack methods reveal that these\nmodels consistently render practical adversarial targets and surpass static\nlexical databases, such as WordNet, particularly for distant class\nrelationships. We also observe that static testing of target labels offers a\npreliminary assessment of the effectiveness of similarity sources, \\textit{a\npriori} testing. Our results corroborate the suitability of pretrained models\nfor constructing interpretable, standardized, and scalable adversarial\nbenchmarks across architectures and datasets.", "AI": {"tldr": "The paper introduces a semantics-guided framework for selecting adversarial target labels in vision models, leveraging cross-modal knowledge from pretrained models like BERT, TinyLLAMA, and CLIP. It outperforms traditional methods like WordNet, especially for distant class relationships, and provides interpretable, scalable benchmarks.", "motivation": "Existing adversarial attack strategies rely on randomness, model predictions, or static semantic resources, which lack interpretability, reproducibility, or flexibility. This paper addresses these limitations by proposing a more dynamic and interpretable approach.", "method": "The framework uses pretrained language and vision-language models (BERT, TinyLLAMA, CLIP) to select semantically related labels for adversarial attacks. It evaluates these models as similarity sources to form best- and worst-case adversarial scenarios.", "result": "Experiments on three vision models and five attack methods show that pretrained models consistently provide practical adversarial targets, surpassing static lexical databases like WordNet, particularly for distant class relationships.", "conclusion": "Pretrained models are suitable for creating interpretable, standardized, and scalable adversarial benchmarks, offering a more effective approach than traditional methods."}}
{"id": "2508.11618", "pdf": "https://arxiv.org/pdf/2508.11618", "abs": "https://arxiv.org/abs/2508.11618", "authors": ["Jungang Chen", "Seyyed A. Hosseini"], "title": "Optimal CO2 storage management considering safety constraints in multi-stakeholder multi-site CCS projects: a game theoretic perspective", "categories": ["cs.LG"], "comment": "38 pages, 16 figures", "summary": "Carbon capture and storage (CCS) projects typically involve a diverse array\nof stakeholders or players from public, private, and regulatory sectors, each\nwith different objectives and responsibilities. Given the complexity, scale,\nand long-term nature of CCS operations, determining whether individual\nstakeholders can independently maximize their interests or whether\ncollaborative coalition agreements are needed remains a central question for\neffective CCS project planning and management. CCS projects are often\nimplemented in geologically connected sites, where shared geological features\nsuch as pressure space and reservoir pore capacity can lead to competitive\nbehavior among stakeholders. Furthermore, CO2 storage sites are often located\nin geologically mature basins that previously served as sites for hydrocarbon\nextraction or wastewater disposal in order to leverage existing\ninfrastructures, which makes unilateral optimization even more complicated and\nunrealistic.\n  In this work, we propose a paradigm based on Markov games to quantitatively\ninvestigate how different coalition structures affect the goals of\nstakeholders. We frame this multi-stakeholder multi-site problem as a\nmulti-agent reinforcement learning problem with safety constraints. Our\napproach enables agents to learn optimal strategies while compliant with safety\nregulations. We present an example where multiple operators are injecting CO2\ninto their respective project areas in a geologically connected basin. To\naddress the high computational cost of repeated simulations of high-fidelity\nmodels, a previously developed surrogate model based on the Embed-to-Control\n(E2C) framework is employed. Our results demonstrate the effectiveness of the\nproposed framework in addressing optimal management of CO2 storage when\nmultiple stakeholders with various objectives and goals are involved.", "AI": {"tldr": "The paper explores CCS project stakeholder dynamics using Markov games and multi-agent reinforcement learning to optimize collaborative strategies while adhering to safety constraints.", "motivation": "CCS projects involve diverse stakeholders with conflicting goals, making unilateral optimization unrealistic. The study aims to model collaborative solutions for effective CCS management.", "method": "A Markov game framework and multi-agent reinforcement learning are used, incorporating safety constraints and a surrogate model (E2C) to reduce computational costs.", "result": "The framework effectively manages CO2 storage in multi-stakeholder, multi-site scenarios, demonstrating the viability of collaborative strategies.", "conclusion": "Collaborative coalition agreements, modeled via reinforcement learning, are essential for optimizing CCS project outcomes amid stakeholder diversity and geological complexities."}}
{"id": "2508.11616", "pdf": "https://arxiv.org/pdf/2508.11616", "abs": "https://arxiv.org/abs/2508.11616", "authors": ["Oscar Ma\u00f1as", "Pierluca D'Oro", "Koustuv Sinha", "Adriana Romero-Soriano", "Michal Drozdzal", "Aishwarya Agrawal"], "title": "Controlling Multimodal LLMs via Reward-guided Decoding", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "Published at ICCV 2025", "summary": "As Multimodal Large Language Models (MLLMs) gain widespread applicability, it\nis becoming increasingly desirable to adapt them for diverse user needs. In\nthis paper, we study the adaptation of MLLMs through controlled decoding. To\nachieve this, we introduce the first method for reward-guided decoding of MLLMs\nand demonstrate its application in improving their visual grounding. Our method\ninvolves building reward models for visual grounding and using them to guide\nthe MLLM's decoding process. Concretely, we build two separate reward models to\nindependently control the degree of object precision and recall in the model's\noutput. Our approach enables on-the-fly controllability of an MLLM's inference\nprocess in two ways: first, by giving control over the relative importance of\neach reward function during decoding, allowing a user to dynamically trade off\nobject precision for recall in image captioning tasks; second, by giving\ncontrol over the breadth of the search during decoding, allowing the user to\ncontrol the trade-off between the amount of test-time compute and the degree of\nvisual grounding. We evaluate our method on standard object hallucination\nbenchmarks, showing that it provides significant controllability over MLLM\ninference, while consistently outperforming existing hallucination mitigation\nmethods.", "AI": {"tldr": "The paper introduces a reward-guided decoding method for Multimodal Large Language Models (MLLMs) to improve visual grounding, offering dynamic control over precision, recall, and computational trade-offs.", "motivation": "To adapt MLLMs for diverse user needs by enhancing their visual grounding capabilities through controlled decoding.", "method": "Develops two reward models for object precision and recall, guiding MLLM decoding dynamically during inference.", "result": "Outperforms existing hallucination mitigation methods and provides significant controllability over MLLM inference.", "conclusion": "The proposed method effectively enhances MLLM adaptability and performance in visual grounding tasks."}}
{"id": "2508.11350", "pdf": "https://arxiv.org/pdf/2508.11350", "abs": "https://arxiv.org/abs/2508.11350", "authors": ["Zhenhao Zhang", "Hanqing Wang", "Xiangyu Zeng", "Ziyu Cheng", "Jiaxin Liu", "Haoyu Yan", "Zhirui Liu", "Kaiyang Ji", "Tianxiang Gui", "Ke Hu", "Kangyi Chen", "Yahao Fan", "Mokai Pan"], "title": "HOID-R1: Reinforcement Learning for Open-World Human-Object Interaction Detection Reasoning with Multimodal Large Language Model", "categories": ["cs.CV"], "comment": null, "summary": "Understanding and recognizing human-object interaction (HOI) is a pivotal\napplication in AR/VR and robotics. Recent open-vocabulary HOI detection\napproaches depend exclusively on large language models for richer textual\nprompts, neglecting their inherent 3D spatial understanding capabilities. To\naddress this shortcoming, we introduce HOID-R1, the first HOI detection\nframework that integrates chain-of-thought (CoT) guided supervised fine-tuning\n(SFT) with group relative policy optimization (GRPO) within a reinforcement\nlearning (RL) paradigm. Specifically, we initially apply SFT to imbue the model\nwith essential reasoning capabilities, forcing the model to articulate its\nthought process in the output. Subsequently, we integrate GRPO to leverage\nmulti-reward signals for policy optimization, thereby enhancing alignment\nacross diverse modalities. To mitigate hallucinations in the CoT reasoning, we\nintroduce an \"MLLM-as-a-judge\" mechanism that supervises the CoT outputs,\nfurther improving generalization. Extensive experiments show that HOID-R1\nachieves state-of-the-art performance on HOI detection benchmarks and\noutperforms existing methods in open-world generalization to novel scenarios.", "AI": {"tldr": "HOID-R1 is a novel HOI detection framework combining CoT-guided SFT and GRPO in RL, outperforming existing methods in benchmarks and open-world generalization.", "motivation": "Address the lack of 3D spatial understanding in open-vocabulary HOI detection by integrating reasoning and multi-reward optimization.", "method": "Uses CoT-guided SFT for reasoning and GRPO for policy optimization, with an MLLM-as-a-judge mechanism to reduce hallucinations.", "result": "Achieves state-of-the-art performance on HOI benchmarks and excels in open-world generalization.", "conclusion": "HOID-R1 effectively combines reasoning and RL to advance HOI detection, offering robust performance in novel scenarios."}}
{"id": "2508.11354", "pdf": "https://arxiv.org/pdf/2508.11354", "abs": "https://arxiv.org/abs/2508.11354", "authors": ["Zhenyi Zhao", "Muthu Rama Krishnan Mookiah", "Emanuele Trucco"], "title": "Leveraging the RETFound foundation model for optic disc segmentation in retinal images", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "RETFound is a well-known foundation model (FM) developed for fundus camera\nand optical coherence tomography images. It has shown promising performance\nacross multiple datasets in diagnosing diseases, both eye-specific and\nsystemic, from retinal images. However, to our best knowledge, it has not been\nused for other tasks. We present the first adaptation of RETFound for optic\ndisc segmentation, a ubiquitous and foundational task in retinal image\nanalysis. The resulting segmentation system outperforms state-of-the-art,\nsegmentation-specific baseline networks after training a head with only a very\nmodest number of task-specific examples. We report and discuss results with\nfour public datasets, IDRID, Drishti-GS, RIM-ONE-r3, and REFUGE, and a private\ndataset, GoDARTS, achieving about 96% Dice consistently across all datasets.\nOverall, our method obtains excellent performance in internal verification,\ndomain generalization and domain adaptation, and exceeds most of the\nstate-of-the-art baseline results. We discuss the results in the framework of\nthe debate about FMs as alternatives to task-specific architectures. The code\nis available at: [link to be added after the paper is accepted]", "AI": {"tldr": "RETFound, a foundation model for retinal images, is adapted for optic disc segmentation, achieving 96% Dice across multiple datasets and outperforming task-specific baselines.", "motivation": "To explore RETFound's potential beyond disease diagnosis by applying it to optic disc segmentation, a foundational task in retinal image analysis.", "method": "Adapt RETFound for optic disc segmentation by training a head with minimal task-specific examples and evaluating on five datasets.", "result": "Achieves ~96% Dice consistently across datasets, outperforming state-of-the-art segmentation-specific networks.", "conclusion": "Demonstrates RETFound's versatility and strong performance, contributing to the debate on foundation models vs. task-specific architectures."}}
{"id": "2508.11374", "pdf": "https://arxiv.org/pdf/2508.11374", "abs": "https://arxiv.org/abs/2508.11374", "authors": ["Devansh Arora", "Nitin Kumar", "Sukrit Gupta"], "title": "Does the Skeleton-Recall Loss Really Work?", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Image segmentation is an important and widely performed task in computer\nvision. Accomplishing effective image segmentation in diverse settings often\nrequires custom model architectures and loss functions. A set of models that\nspecialize in segmenting thin tubular structures are topology\npreservation-based loss functions. These models often utilize a pixel\nskeletonization process claimed to generate more precise segmentation masks of\nthin tubes and better capture the structures that other models often miss. One\nsuch model, Skeleton Recall Loss (SRL) proposed by Kirchhoff et al.~\\cite\n{kirchhoff2024srl}, was stated to produce state-of-the-art results on benchmark\ntubular datasets. In this work, we performed a theoretical analysis of the\ngradients for the SRL loss. Upon comparing the performance of the proposed\nmethod on some of the tubular datasets (used in the original work, along with\nsome additional datasets), we found that the performance of SRL-based\nsegmentation models did not exceed traditional baseline models. By providing\nboth a theoretical explanation and empirical evidence, this work critically\nevaluates the limitations of topology-based loss functions, offering valuable\ninsights for researchers aiming to develop more effective segmentation models\nfor complex tubular structures.", "AI": {"tldr": "The paper critically evaluates the Skeleton Recall Loss (SRL) for image segmentation of tubular structures, finding it does not outperform traditional baselines despite claims.", "motivation": "To assess the effectiveness of topology-based loss functions like SRL in segmenting thin tubular structures, given their claimed advantages.", "method": "Theoretical analysis of SRL gradients and empirical comparison on tubular datasets, including those from the original SRL work and additional datasets.", "result": "SRL-based models did not surpass traditional baseline models in performance.", "conclusion": "The study highlights limitations of topology-based loss functions, providing insights for improving segmentation models for complex tubular structures."}}
{"id": "2508.11376", "pdf": "https://arxiv.org/pdf/2508.11376", "abs": "https://arxiv.org/abs/2508.11376", "authors": ["Durgesh Mishra", "Rishabh Uikey"], "title": "Unified Knowledge Distillation Framework: Fine-Grained Alignment and Geometric Relationship Preservation for Deep Face Recognition", "categories": ["cs.CV", "cs.LG"], "comment": "The paper spans a total of 14 pages, 10 pages for the main content\n  (including references) and 4 pages for the appendix. The main paper contains\n  3 figures and 1 table, while the appendix includes 1 pseudo-code algorithm\n  and 4 tables. The work was recently accepted for publication at IJCB 2025", "summary": "Knowledge Distillation is crucial for optimizing face recognition models for\ndeployment in computationally limited settings, such as edge devices.\nTraditional KD methods, such as Raw L2 Feature Distillation or Feature\nConsistency loss, often fail to capture both fine-grained instance-level\ndetails and complex relational structures, leading to suboptimal performance.\nWe propose a unified approach that integrates two novel loss functions,\nInstance-Level Embedding Distillation and Relation-Based Pairwise Similarity\nDistillation. Instance-Level Embedding Distillation focuses on aligning\nindividual feature embeddings by leveraging a dynamic hard mining strategy,\nthereby enhancing learning from challenging examples. Relation-Based Pairwise\nSimilarity Distillation captures relational information through pairwise\nsimilarity relationships, employing a memory bank mechanism and a sample mining\nstrategy. This unified framework ensures both effective instance-level\nalignment and preservation of geometric relationships between samples, leading\nto a more comprehensive distillation process. Our unified framework outperforms\nstate-of-the-art distillation methods across multiple benchmark face\nrecognition datasets, as demonstrated by extensive experimental evaluations.\nInterestingly, when using strong teacher networks compared to the student, our\nunified KD enables the student to even surpass the teacher's accuracy.", "AI": {"tldr": "A unified knowledge distillation approach improves face recognition model performance by combining instance-level and relational distillation losses, outperforming traditional methods and even surpassing teacher model accuracy in some cases.", "motivation": "Optimizing face recognition models for deployment in computationally limited settings requires effective knowledge distillation (KD) methods. Traditional KD approaches often fail to capture fine-grained details and relational structures, leading to suboptimal performance.", "method": "The proposed method integrates two novel loss functions: Instance-Level Embedding Distillation (using dynamic hard mining) and Relation-Based Pairwise Similarity Distillation (using a memory bank and sample mining). This ensures both instance-level alignment and relational preservation.", "result": "The unified framework outperforms state-of-the-art distillation methods on benchmark datasets and, in some cases, enables the student model to surpass the teacher's accuracy.", "conclusion": "The unified KD approach effectively addresses limitations of traditional methods, offering a comprehensive solution for optimizing face recognition models in resource-constrained environments."}}
{"id": "2508.11257", "pdf": "https://arxiv.org/pdf/2508.11257", "abs": "https://arxiv.org/abs/2508.11257", "authors": ["Marc Pavel", "Nenad Petrovic", "Lukasz Mazur", "Vahid Zolfaghari", "Fengjunjie Pan", "Alois Knoll"], "title": "Hallucination in LLM-Based Code Generation: An Automotive Case Study", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have shown significant potential in automating\ncode generation tasks offering new opportunities across software engineering\ndomains. However, their practical application remains limited due to\nhallucinations - outputs that appear plausible but are factually incorrect,\nunverifiable or nonsensical. This paper investigates hallucination phenomena in\nthe context of code generation with a specific focus on the automotive domain.\nA case study is presented that evaluates multiple code LLMs for three different\nprompting complexities ranging from a minimal one-liner prompt to a prompt with\nCovesa Vehicle Signal Specifications (VSS) as additional context and finally to\na prompt with an additional code skeleton. The evaluation reveals a high\nfrequency of syntax violations, invalid reference errors and API knowledge\nconflicts in state-of-the-art models GPT-4.1, Codex and GPT-4o. Among the\nevaluated models, only GPT-4.1 and GPT-4o were able to produce a correct\nsolution when given the most context-rich prompt. Simpler prompting strategies\nfailed to yield a working result, even after multiple refinement iterations.\nThese findings highlight the need for effective mitigation techniques to ensure\nthe safe and reliable use of LLM generated code, especially in safety-critical\ndomains such as automotive software systems.", "AI": {"tldr": "The paper examines hallucination issues in LLM-generated code for automotive software, revealing frequent errors in state-of-the-art models and emphasizing the need for mitigation techniques.", "motivation": "To address the limitations of LLMs in code generation due to hallucinations, particularly in safety-critical automotive domains.", "method": "A case study evaluating multiple LLMs (GPT-4.1, Codex, GPT-4o) with varying prompt complexities, from minimal to context-rich.", "result": "High frequency of syntax errors and invalid references in models; only GPT-4.1 and GPT-4o succeeded with the most context-rich prompts.", "conclusion": "Effective mitigation techniques are crucial for reliable LLM-generated code in safety-critical applications like automotive systems."}}
{"id": "2508.11379", "pdf": "https://arxiv.org/pdf/2508.11379", "abs": "https://arxiv.org/abs/2508.11379", "authors": ["Ramil Khafizov", "Artem Komarichev", "Ruslan Rakhimov", "Peter Wonka", "Evgeny Burnaev"], "title": "G-CUT3R: Guided 3D Reconstruction with Camera and Depth Prior Integration", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We introduce G-CUT3R, a novel feed-forward approach for guided 3D scene\nreconstruction that enhances the CUT3R model by integrating prior information.\nUnlike existing feed-forward methods that rely solely on input images, our\nmethod leverages auxiliary data, such as depth, camera calibrations, or camera\npositions, commonly available in real-world scenarios. We propose a lightweight\nmodification to CUT3R, incorporating a dedicated encoder for each modality to\nextract features, which are fused with RGB image tokens via zero convolution.\nThis flexible design enables seamless integration of any combination of prior\ninformation during inference. Evaluated across multiple benchmarks, including\n3D reconstruction and other multi-view tasks, our approach demonstrates\nsignificant performance improvements, showing its ability to effectively\nutilize available priors while maintaining compatibility with varying input\nmodalities.", "AI": {"tldr": "G-CUT3R enhances 3D scene reconstruction by integrating prior data like depth or camera info, outperforming existing methods.", "motivation": "Existing feed-forward methods rely only on images, missing valuable auxiliary data often available in real-world scenarios.", "method": "Lightweight modification of CUT3R with dedicated encoders for each modality, fused via zero convolution.", "result": "Significant performance improvements in 3D reconstruction and multi-view tasks.", "conclusion": "G-CUT3R effectively utilizes priors while remaining flexible with input modalities."}}
{"id": "2508.11409", "pdf": "https://arxiv.org/pdf/2508.11409", "abs": "https://arxiv.org/abs/2508.11409", "authors": ["Zhiming Liu", "Nantheera Anantrasirichai"], "title": "RMFAT: Recurrent Multi-scale Feature Atmospheric Turbulence Mitigator", "categories": ["cs.CV"], "comment": null, "summary": "Atmospheric turbulence severely degrades video quality by introducing\ndistortions such as geometric warping, blur, and temporal flickering, posing\nsignificant challenges to both visual clarity and temporal consistency. Current\nstate-of-the-art methods are based on transformer and 3D architectures and\nrequire multi-frame input, but their large computational cost and memory usage\nlimit real-time deployment, especially in resource-constrained scenarios. In\nthis work, we propose RMFAT: Recurrent Multi-scale Feature Atmospheric\nTurbulence Mitigator, designed for efficient and temporally consistent video\nrestoration under AT conditions. RMFAT adopts a lightweight recurrent framework\nthat restores each frame using only two inputs at a time, significantly\nreducing temporal window size and computational burden. It further integrates\nmulti-scale feature encoding and decoding with temporal warping modules at both\nencoder and decoder stages to enhance spatial detail and temporal coherence.\nExtensive experiments on synthetic and real-world atmospheric turbulence\ndatasets demonstrate that RMFAT not only outperforms existing methods in terms\nof clarity restoration (with nearly a 9\\% improvement in SSIM) but also\nachieves significantly improved inference speed (more than a fourfold reduction\nin runtime), making it particularly suitable for real-time atmospheric\nturbulence suppression tasks.", "AI": {"tldr": "RMFAT is a lightweight recurrent framework for efficient and temporally consistent video restoration under atmospheric turbulence, outperforming existing methods in clarity and speed.", "motivation": "Atmospheric turbulence degrades video quality, and current methods are computationally expensive, limiting real-time use.", "method": "RMFAT uses a recurrent framework with multi-scale feature encoding/decoding and temporal warping, requiring only two frames per restoration.", "result": "RMFAT improves clarity (9% SSIM boost) and reduces runtime by fourfold compared to state-of-the-art methods.", "conclusion": "RMFAT is efficient and effective for real-time atmospheric turbulence suppression."}}
{"id": "2508.10908", "pdf": "https://arxiv.org/pdf/2508.10908", "abs": "https://arxiv.org/abs/2508.10908", "authors": ["Jeong-Hwan Kim", "Daehyun Kang", "Young-Min Yang", "Jae-Heung Park", "Yoo-Geun Ham"], "title": "Data-driven global ocean model resolving ocean-atmosphere coupling dynamics", "categories": ["physics.ao-ph", "cs.LG", "I.2.1"], "comment": "The manuscript contains 4 main figures. The Extended Data contains 7\n  figures and 3 tables. The Supplementary Information contains 3 text sections,\n  7 figures, 1 table", "summary": "Artificial intelligence has advanced global weather forecasting,\noutperforming traditional numerical models in both accuracy and computational\nefficiency. Nevertheless, extending predictions beyond subseasonal timescales\nrequires the development of deep learning (DL)-based ocean-atmosphere coupled\nmodels that can realistically simulate complex oceanic responses to atmospheric\nforcing. This study presents KIST-Ocean, a DL-based global three-dimensional\nocean general circulation model using a U-shaped visual attention adversarial\nnetwork architecture. KIST-Ocean integrates partial convolution, adversarial\ntraining, and transfer learning to address coastal complexity and predictive\ndistribution drift in auto-regressive models. Comprehensive evaluations\nconfirmed the model's robust ocean predictive skill and efficiency. Moreover,\nit accurately captures realistic ocean response, such as Kelvin and Rossby wave\npropagation in the tropical Pacific, and vertical motions induced by cyclonic\nand anticyclonic wind stress, demonstrating its ability to represent key\nocean-atmosphere coupling mechanisms underlying climate phenomena, including\nthe El Nino-Southern Oscillation. These findings reinforce confidence in\nDL-based global weather and climate models and their extending DL-based\napproaches to broader Earth system modeling, offering potential for enhancing\nclimate prediction capabilities.", "AI": {"tldr": "KIST-Ocean, a DL-based 3D ocean model, outperforms traditional methods in accuracy and efficiency, capturing key ocean-atmosphere coupling mechanisms like El Nino-Southern Oscillation.", "motivation": "Extending AI-driven weather forecasting to subseasonal timescales requires realistic ocean-atmosphere coupled models.", "method": "Uses a U-shaped visual attention adversarial network with partial convolution, adversarial training, and transfer learning.", "result": "Demonstrates robust predictive skill, accurately simulating ocean responses like wave propagation and wind stress effects.", "conclusion": "DL-based models like KIST-Ocean enhance climate prediction and broaden Earth system modeling potential."}}
{"id": "2508.11411", "pdf": "https://arxiv.org/pdf/2508.11411", "abs": "https://arxiv.org/abs/2508.11411", "authors": ["Fabian H. Reith", "Jannik Franzen", "Dinesh R. Palli", "J. Lorenz Rumberger", "Dagmar Kainmueller"], "title": "SelfAdapt: Unsupervised Domain Adaptation of Cell Segmentation Models", "categories": ["cs.CV", "cs.LG"], "comment": "8 pages, 3 figures. To appear in the proceedings of the BioImage\n  Computing (BIC) Workshop @ ICCVW 2025. This is the accepted author manuscript\n  (camera-ready version)", "summary": "Deep neural networks have become the go-to method for biomedical instance\nsegmentation. Generalist models like Cellpose demonstrate state-of-the-art\nperformance across diverse cellular data, though their effectiveness often\ndegrades on domains that differ from their training data. While supervised\nfine-tuning can address this limitation, it requires annotated data that may\nnot be readily available. We propose SelfAdapt, a method that enables the\nadaptation of pre-trained cell segmentation models without the need for labels.\nOur approach builds upon student-teacher augmentation consistency training,\nintroducing L2-SP regularization and label-free stopping criteria. We evaluate\nour method on the LiveCell and TissueNet datasets, demonstrating relative\nimprovements in AP0.5 of up to 29.64% over baseline Cellpose. Additionally, we\nshow that our unsupervised adaptation can further improve models that were\npreviously fine-tuned with supervision. We release SelfAdapt as an easy-to-use\nextension of the Cellpose framework. The code for our method is publicly\navailable at https: //github.com/Kainmueller-Lab/self_adapt.", "AI": {"tldr": "SelfAdapt is a label-free method for adapting pre-trained cell segmentation models, improving performance on diverse datasets without requiring annotated data.", "motivation": "Generalist models like Cellpose degrade in performance on domains differing from their training data, and supervised fine-tuning requires scarce annotated data.", "method": "Uses student-teacher augmentation consistency training with L2-SP regularization and label-free stopping criteria.", "result": "Achieves up to 29.64% improvement in AP0.5 over baseline Cellpose on LiveCell and TissueNet datasets.", "conclusion": "SelfAdapt is effective for unsupervised adaptation and is released as an easy-to-use extension of Cellpose."}}
{"id": "2508.10911", "pdf": "https://arxiv.org/pdf/2508.10911", "abs": "https://arxiv.org/abs/2508.10911", "authors": ["Luis Vitor Zerkowski", "Nina S. T. Hirata"], "title": "Uncovering Latent Connections in Indigenous Heritage: Semantic Pipelines for Cultural Preservation in Brazil", "categories": ["cs.HC", "cs.CY", "cs.LG", "I.2.m"], "comment": "8 tables, 7 figures, submitted to AAAI2026", "summary": "Indigenous communities face ongoing challenges in preserving their cultural\nheritage, particularly in the face of systemic marginalization and urban\ndevelopment. In Brazil, the Museu Nacional dos Povos Indigenas through the\nTainacan platform hosts the country's largest online collection of Indigenous\nobjects and iconographies, providing a critical resource for cultural\nengagement. Using publicly available data from this repository, we present a\ndata-driven initiative that applies artificial intelligence to enhance\naccessibility, interpretation, and exploration. We develop two semantic\npipelines: a visual pipeline that models image-based similarity and a textual\npipeline that captures semantic relationships from item descriptions. These\nembedding spaces are projected into two dimensions and integrated into an\ninteractive visualization tool we also developed. In addition to\nsimilarity-based navigation, users can explore the collection through temporal\nand geographic lenses, enabling both semantic and contextualized perspectives.\nThe system supports curatorial tasks, aids public engagement, and reveals\nlatent connections within the collection. This work demonstrates how AI can\nethically contribute to cultural preservation practices.", "AI": {"tldr": "The paper presents an AI-driven initiative using Brazil's Museu Nacional dos Povos Indigenas' Tainacan platform to enhance accessibility and exploration of Indigenous cultural heritage through visual and textual semantic pipelines.", "motivation": "To address challenges in preserving Indigenous cultural heritage by leveraging AI to improve accessibility, interpretation, and engagement with digital collections.", "method": "Developed two semantic pipelines (visual and textual) to model image-based similarity and semantic relationships, integrated into an interactive visualization tool for exploration.", "result": "The system enables similarity-based navigation, temporal and geographic exploration, supports curatorial tasks, and reveals latent connections in the collection.", "conclusion": "AI can ethically contribute to cultural preservation by enhancing accessibility and engagement with Indigenous heritage."}}
{"id": "2508.11278", "pdf": "https://arxiv.org/pdf/2508.11278", "abs": "https://arxiv.org/abs/2508.11278", "authors": ["Francesco Sovrano", "Gabriele Dominici", "Rita Sevastjanova", "Alessandra Stramiglio", "Alberto Bacchelli"], "title": "Is General-Purpose AI Reasoning Sensitive to Data-Induced Cognitive Biases? Dynamic Benchmarking on Typical Software Engineering Dilemmas", "categories": ["cs.HC", "cs.AI", "cs.SE"], "comment": null, "summary": "Human cognitive biases in software engineering can lead to costly errors.\nWhile general-purpose AI (GPAI) systems may help mitigate these biases due to\ntheir non-human nature, their training on human-generated data raises a\ncritical question: Do GPAI systems themselves exhibit cognitive biases?\n  To investigate this, we present the first dynamic benchmarking framework to\nevaluate data-induced cognitive biases in GPAI within software engineering\nworkflows. Starting with a seed set of 16 hand-crafted realistic tasks, each\nfeaturing one of 8 cognitive biases (e.g., anchoring, framing) and\ncorresponding unbiased variants, we test whether bias-inducing linguistic cues\nunrelated to task logic can lead GPAI systems from correct to incorrect\nconclusions.\n  To scale the benchmark and ensure realism, we develop an on-demand\naugmentation pipeline relying on GPAI systems to generate task variants that\npreserve bias-inducing cues while varying surface details. This pipeline\nensures correctness (88--99% on average, according to human evaluation),\npromotes diversity, and controls reasoning complexity by leveraging\nProlog-based reasoning and LLM-as-a-judge validation. It also verifies that the\nembedded biases are both harmful and undetectable by logic-based, unbiased\nreasoners.\n  We evaluate leading GPAI systems (GPT, LLaMA, DeepSeek) and find a consistent\ntendency to rely on shallow linguistic heuristics over deep reasoning. All\nsystems exhibit cognitive biases (ranging from 5.9% to 35% across types), with\nbias sensitivity increasing sharply with task complexity (up to 49%),\nhighlighting critical risks in real-world software engineering deployments.", "AI": {"tldr": "The paper investigates whether general-purpose AI (GPAI) systems exhibit human-like cognitive biases in software engineering tasks, using a dynamic benchmarking framework to evaluate bias sensitivity.", "motivation": "To determine if GPAI systems, trained on human-generated data, inherit cognitive biases, which could lead to errors in software engineering workflows.", "method": "Developed a dynamic benchmarking framework with hand-crafted tasks featuring cognitive biases, augmented by an on-demand pipeline using GPAI for task generation and validation.", "result": "Leading GPAI systems (GPT, LLaMA, DeepSeek) showed cognitive biases (5.9% to 35%), with sensitivity increasing with task complexity (up to 49%).", "conclusion": "GPAI systems exhibit cognitive biases, posing risks in real-world software engineering, highlighting the need for bias-aware AI deployment."}}
{"id": "2508.11419", "pdf": "https://arxiv.org/pdf/2508.11419", "abs": "https://arxiv.org/abs/2508.11419", "authors": ["Florian Bayer", "Maximilian Russo", "Christian Rathgeb"], "title": "Training-free Dimensionality Reduction via Feature Truncation: Enhancing Efficiency in Privacy-preserving Multi-Biometric Systems", "categories": ["cs.CV"], "comment": null, "summary": "Biometric recognition is widely used, making the privacy and security of\nextracted templates a critical concern. Biometric Template Protection schemes,\nespecially those utilizing Homomorphic Encryption, introduce significant\ncomputational challenges due to increased workload. Recent advances in deep\nneural networks have enabled state-of-the-art feature extraction for face,\nfingerprint, and iris modalities. The ubiquity and affordability of biometric\nsensors further facilitate multi-modal fusion, which can enhance security by\ncombining features from different modalities. This work investigates the\nbiometric performance of reduced multi-biometric template sizes. Experiments\nare conducted on an in-house virtual multi-biometric database, derived from\nDNN-extracted features for face, fingerprint, and iris, using the FRGC, MCYT,\nand CASIA databases. The evaluated approaches are (i) explainable and\nstraightforward to implement under encryption, (ii) training-free, and (iii)\ncapable of generalization. Dimensionality reduction of feature vectors leads to\nfewer operations in the Homomorphic Encryption (HE) domain, enabling more\nefficient encrypted processing while maintaining biometric accuracy and\nsecurity at a level equivalent to or exceeding single-biometric recognition.\nOur results demonstrate that, by fusing feature vectors from multiple\nmodalities, template size can be reduced by 67 % with no loss in Equal Error\nRate (EER) compared to the best-performing single modality.", "AI": {"tldr": "The paper explores reducing multi-biometric template sizes to enhance efficiency in Homomorphic Encryption (HE) systems while maintaining security and accuracy, achieving a 67% reduction without compromising performance.", "motivation": "Addressing the computational challenges of Biometric Template Protection schemes, particularly with Homomorphic Encryption, by leveraging multi-modal fusion and dimensionality reduction for efficiency.", "method": "Conducts experiments on a virtual multi-biometric database using DNN-extracted features from face, fingerprint, and iris modalities, focusing on dimensionality reduction and HE-compatible approaches.", "result": "Demonstrates a 67% reduction in template size with no loss in Equal Error Rate (EER) compared to single-modality recognition, maintaining security and accuracy.", "conclusion": "Multi-modal fusion and dimensionality reduction enable efficient encrypted processing without sacrificing biometric performance, offering a viable solution for secure biometric systems."}}
{"id": "2508.10915", "pdf": "https://arxiv.org/pdf/2508.10915", "abs": "https://arxiv.org/abs/2508.10915", "authors": ["Jacob Clouse", "Thomas Ramsey", "Samitha Somathilaka", "Nicholas Kleinsasser", "Sangjin Ryu", "Sasitharan Balasubramaniam"], "title": "Insect-Wing Structured Microfluidic System for Reservoir Computing", "categories": ["cs.NE", "cs.ET", "cs.LG"], "comment": null, "summary": "As the demand for more efficient and adaptive computing grows,\nnature-inspired architectures offer promising alternatives to conventional\nelectronic designs. Microfluidic platforms, drawing on biological forms and\nfluid dynamics, present a compelling foundation for low-power, high-resilience\ncomputing in environments where electronics are unsuitable. This study explores\na hybrid reservoir computing system based on a dragonfly-wing inspired\nmicrofluidic chip, which encodes temporal input patterns as fluid interactions\nwithin the micro channel network.\n  The system operates with three dye-based inlet channels and three\ncamera-monitored detection areas, transforming discrete spatial patterns into\ndynamic color output signals. These reservoir output signals are then modified\nand passed to a simple and trainable readout layer for pattern classification.\nUsing a combination of raw reservoir outputs and synthetically generated\noutputs, we evaluated system performance, system clarity, and data efficiency.\nThe results demonstrate consistent classification accuracies up to $91\\%$, even\nwith coarse resolution and limited training data, highlighting the viability of\nthe microfluidic reservoir computing.", "AI": {"tldr": "A dragonfly-wing inspired microfluidic chip is used for hybrid reservoir computing, achieving 91% classification accuracy with low-power, resilient performance.", "motivation": "To address the need for efficient, adaptive computing in environments unsuitable for electronics by leveraging nature-inspired microfluidic designs.", "method": "A hybrid reservoir computing system with dye-based inlets and camera-monitored detection areas transforms spatial patterns into dynamic color signals for classification.", "result": "The system achieves up to 91% classification accuracy, even with coarse resolution and limited training data.", "conclusion": "Microfluidic reservoir computing is viable for low-power, high-resilience applications."}}
{"id": "2508.11428", "pdf": "https://arxiv.org/pdf/2508.11428", "abs": "https://arxiv.org/abs/2508.11428", "authors": ["Jingyu Li", "Bozhou Zhang", "Xin Jin", "Jiankang Deng", "Xiatian Zhu", "Li Zhang"], "title": "ImagiDrive: A Unified Imagination-and-Planning Framework for Autonomous Driving", "categories": ["cs.CV"], "comment": null, "summary": "Autonomous driving requires rich contextual comprehension and precise\npredictive reasoning to navigate dynamic and complex environments safely.\nVision-Language Models (VLMs) and Driving World Models (DWMs) have\nindependently emerged as powerful recipes addressing different aspects of this\nchallenge. VLMs provide interpretability and robust action prediction through\ntheir ability to understand multi-modal context, while DWMs excel in generating\ndetailed and plausible future driving scenarios essential for proactive\nplanning. Integrating VLMs with DWMs is an intuitive, promising, yet\nunderstudied strategy to exploit the complementary strengths of accurate\nbehavioral prediction and realistic scene generation. Nevertheless, this\nintegration presents notable challenges, particularly in effectively connecting\naction-level decisions with high-fidelity pixel-level predictions and\nmaintaining computational efficiency. In this paper, we propose ImagiDrive, a\nnovel end-to-end autonomous driving framework that integrates a VLM-based\ndriving agent with a DWM-based scene imaginer to form a unified\nimagination-and-planning loop. The driving agent predicts initial driving\ntrajectories based on multi-modal inputs, guiding the scene imaginer to\ngenerate corresponding future scenarios. These imagined scenarios are\nsubsequently utilized to iteratively refine the driving agent's planning\ndecisions. To address efficiency and predictive accuracy challenges inherent in\nthis integration, we introduce an early stopping mechanism and a trajectory\nselection strategy. Extensive experimental validation on the nuScenes and\nNAVSIM datasets demonstrates the robustness and superiority of ImagiDrive over\nprevious alternatives under both open-loop and closed-loop conditions.", "AI": {"tldr": "ImagiDrive integrates Vision-Language Models (VLMs) and Driving World Models (DWMs) for autonomous driving, combining behavioral prediction and scene generation in a unified loop.", "motivation": "Autonomous driving requires contextual comprehension and predictive reasoning. VLMs and DWMs address different aspects, but their integration is understudied.", "method": "ImagiDrive combines a VLM-based driving agent with a DWM-based scene imaginer, using an imagination-and-planning loop and introducing efficiency mechanisms.", "result": "Experiments on nuScenes and NAVSIM show ImagiDrive's robustness and superiority in open-loop and closed-loop conditions.", "conclusion": "ImagiDrive effectively integrates VLMs and DWMs, improving autonomous driving performance through iterative refinement."}}
{"id": "2508.11431", "pdf": "https://arxiv.org/pdf/2508.11431", "abs": "https://arxiv.org/abs/2508.11431", "authors": ["Simona Kocour", "Assia Benbihi", "Torsten Sattler"], "title": "Remove360: Benchmarking Residuals After Object Removal in 3D Gaussian Splatting", "categories": ["cs.CV"], "comment": "arXiv admin note: substantial text overlap with arXiv:2503.17574", "summary": "Understanding what semantic information persists after object removal is\ncritical for privacy-preserving 3D reconstruction and editable scene\nrepresentations. In this work, we introduce a novel benchmark and evaluation\nframework to measure semantic residuals, the unintended semantic traces left\nbehind, after object removal in 3D Gaussian Splatting. We conduct experiments\nacross a diverse set of indoor and outdoor scenes, showing that current methods\ncan preserve semantic information despite the absence of visual geometry. We\nalso release Remove360, a dataset of pre/post-removal RGB images and\nobject-level masks captured in real-world environments. While prior datasets\nhave focused on isolated object instances, Remove360 covers a broader and more\ncomplex range of indoor and outdoor scenes, enabling evaluation of object\nremoval in the context of full-scene representations. Given ground truth images\nof a scene before and after object removal, we assess whether we can truly\neliminate semantic presence, and if downstream models can still infer what was\nremoved. Our findings reveal critical limitations in current 3D object removal\ntechniques and underscore the need for more robust solutions capable of\nhandling real-world complexity. The evaluation framework is available at\ngithub.com/spatial-intelligence-ai/Remove360.git. Data are available at\nhuggingface.co/datasets/simkoc/Remove360.", "AI": {"tldr": "A benchmark and evaluation framework for measuring semantic residuals in 3D Gaussian Splatting after object removal, highlighting limitations in current techniques.", "motivation": "To understand and measure unintended semantic traces left after object removal in 3D reconstructions, critical for privacy and editable scenes.", "method": "Introduces Remove360 dataset and evaluates semantic residuals using pre/post-removal RGB images and masks in diverse indoor/outdoor scenes.", "result": "Current methods preserve semantic information despite visual geometry removal, revealing limitations in 3D object removal techniques.", "conclusion": "More robust solutions are needed to handle real-world complexity in object removal, as current techniques fall short."}}
{"id": "2508.10928", "pdf": "https://arxiv.org/pdf/2508.10928", "abs": "https://arxiv.org/abs/2508.10928", "authors": ["Sheng Wong", "Beth Albert", "Gabriel Davis Jones"], "title": "CleanCTG: A Deep Learning Model for Multi-Artefact Detection and Reconstruction in Cardiotocography", "categories": ["eess.AS", "cs.LG", "cs.SD"], "comment": null, "summary": "Cardiotocography (CTG) is essential for fetal monitoring but is frequently\ncompromised by diverse artefacts which obscure true fetal heart rate (FHR)\npatterns and can lead to misdiagnosis or delayed intervention. Current\ndeep-learning approaches typically bypass comprehensive noise handling,\napplying minimal preprocessing or focusing solely on downstream classification,\nwhile traditional methods rely on simple interpolation or rule-based filtering\nthat addresses only missing samples and fail to correct complex artefact types.\nWe present CleanCTG, an end-to-end dual-stage model that first identifies\nmultiple artefact types via multi-scale convolution and context-aware\ncross-attention, then reconstructs corrupted segments through artefact-specific\ncorrection branches. Training utilised over 800,000 minutes of physiologically\nrealistic, synthetically corrupted CTGs derived from expert-verified \"clean\"\nrecordings. On synthetic data, CleanCTG achieved perfect artefact detection\n(AU-ROC = 1.00) and reduced mean squared error (MSE) on corrupted segments to\n2.74 x 10^-4 (clean-segment MSE = 2.40 x 10^-6), outperforming the next best\nmethod by more than 60%. External validation on 10,190 minutes of\nclinician-annotated segments yielded AU-ROC = 0.95 (sensitivity = 83.44%,\nspecificity 94.22%), surpassing six comparator classifiers. Finally, when\nintegrated with the Dawes-Redman system on 933 clinical CTG recordings,\ndenoised traces increased specificity (from 80.70% to 82.70%) and shortened\nmedian time to decision by 33%. These findings suggest that explicit artefact\nremoval and signal reconstruction can both maintain diagnostic accuracy and\nenable shorter monitoring sessions, offering a practical route to more reliable\nCTG interpretation.", "AI": {"tldr": "CleanCTG is a dual-stage deep-learning model for CTG artefact detection and correction, outperforming existing methods and improving clinical decision-making.", "motivation": "Current CTG monitoring is hindered by artefacts that obscure true fetal heart rate patterns, leading to misdiagnosis. Existing methods lack comprehensive noise handling.", "method": "CleanCTG uses multi-scale convolution and context-aware cross-attention to identify artefacts, followed by artefact-specific correction branches. It was trained on synthetic data derived from clean recordings.", "result": "Achieved perfect artefact detection (AU-ROC = 1.00) on synthetic data, reduced MSE, and improved specificity and decision time in clinical validation.", "conclusion": "Explicit artefact removal enhances CTG reliability, maintaining diagnostic accuracy and reducing monitoring time."}}
{"id": "2508.11286", "pdf": "https://arxiv.org/pdf/2508.11286", "abs": "https://arxiv.org/abs/2508.11286", "authors": ["Che Rin Yu", "Daewon Chae", "Dabin Seo", "Sangwon Lee", "Hyeongwoo Im", "Jinkyu Kim"], "title": "Scene Graph-Guided Proactive Replanning for Failure-Resilient Embodied Agent", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "When humans perform everyday tasks, we naturally adjust our actions based on\nthe current state of the environment. For instance, if we intend to put\nsomething into a drawer but notice it is closed, we open it first. However,\nmany autonomous robots lack this adaptive awareness. They often follow\npre-planned actions that may overlook subtle yet critical changes in the scene,\nwhich can result in actions being executed under outdated assumptions and\neventual failure. While replanning is critical for robust autonomy, most\nexisting methods respond only after failures occur, when recovery may be\ninefficient or infeasible. While proactive replanning holds promise for\npreventing failures in advance, current solutions often rely on manually\ndesigned rules and extensive supervision. In this work, we present a proactive\nreplanning framework that detects and corrects failures at subtask boundaries\nby comparing scene graphs constructed from current RGB-D observations against\nreference graphs extracted from successful demonstrations. When the current\nscene fails to align with reference trajectories, a lightweight reasoning\nmodule is activated to diagnose the mismatch and adjust the plan. Experiments\nin the AI2-THOR simulator demonstrate that our approach detects semantic and\nspatial mismatches before execution failures occur, significantly improving\ntask success and robustness.", "AI": {"tldr": "A proactive replanning framework for robots detects and corrects failures by comparing current scene graphs to reference ones, improving task success.", "motivation": "Autonomous robots often fail due to rigid pre-planned actions that ignore environmental changes, necessitating adaptive awareness.", "method": "The framework compares RGB-D scene graphs to reference graphs from successful demos, activating reasoning to adjust plans when mismatches occur.", "result": "Experiments in AI2-THOR show the method detects mismatches early, enhancing task success and robustness.", "conclusion": "Proactive replanning with scene graph comparison prevents failures, outperforming reactive methods."}}
{"id": "2508.11433", "pdf": "https://arxiv.org/pdf/2508.11433", "abs": "https://arxiv.org/abs/2508.11433", "authors": ["Qian Liang", "Yujia Wu", "Kuncheng Li", "Jiwei Wei", "Shiyuan He", "Jinyu Guo", "Ning Xie"], "title": "MM-R1: Unleashing the Power of Unified Multimodal Large Language Models for Personalized Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) with unified architectures excel\nacross a wide range of vision-language tasks, yet aligning them with\npersonalized image generation remains a significant challenge. Existing methods\nfor MLLMs are frequently subject-specific, demanding a data-intensive\nfine-tuning process for every new subject, which limits their scalability. In\nthis paper, we introduce MM-R1, a framework that integrates a cross-modal\nChain-of-Thought (X-CoT) reasoning strategy to unlock the inherent potential of\nunified MLLMs for personalized image generation. Specifically, we structure\npersonalization as an integrated visual reasoning and generation process: (1)\ngrounding subject concepts by interpreting and understanding user-provided\nimages and contextual cues, and (2) generating personalized images conditioned\non both the extracted subject representations and user prompts. To further\nenhance the reasoning capability, we adopt Grouped Reward Proximal Policy\nOptimization (GRPO) to explicitly align the generation. Experiments demonstrate\nthat MM-R1 unleashes the personalization capability of unified MLLMs to\ngenerate images with high subject fidelity and strong text alignment in a\nzero-shot manner.", "AI": {"tldr": "MM-R1 is a framework using X-CoT reasoning and GRPO to enable unified MLLMs for zero-shot personalized image generation, improving subject fidelity and text alignment.", "motivation": "Aligning MLLMs with personalized image generation is challenging due to subject-specific fine-tuning requirements, limiting scalability.", "method": "Integrates X-CoT reasoning for visual grounding and GRPO for alignment, structuring personalization as a combined reasoning-generation process.", "result": "MM-R1 achieves high subject fidelity and text alignment in zero-shot personalized image generation.", "conclusion": "MM-R1 effectively unlocks unified MLLMs' potential for scalable, high-quality personalized image generation."}}
{"id": "2508.11287", "pdf": "https://arxiv.org/pdf/2508.11287", "abs": "https://arxiv.org/abs/2508.11287", "authors": ["Xuran Liu", "Nan Xue", "Rui Bao", "Yaping Sun", "Zhiyong Chen", "Meixia Tao", "Xiaodong Xu", "Shuguang Cui"], "title": "CSGO: Generalized Optimization for Cold Start in Wireless Collaborative Edge LLM Systems", "categories": ["cs.IT", "cs.AI", "cs.LG", "math.IT"], "comment": "submitted to Journal of Communications and Information Networks", "summary": "While deploying large language models on edge devices promises low-latency\nand privacy-preserving AI services, it is hindered by limited device resources.\nAlthough pipeline parallelism facilitates distributed inference, existing\napproaches often ignore the cold-start latency caused by on-demand model\nloading. In this paper, we propose a latency-aware scheduling framework that\noverlaps model loading with computation and communication to minimize total\ninference latency. Based on device and model parameters, the framework\ndynamically adjusts layer partitioning and allocation to effectively hide\nloading time, thereby eliminating as many idle periods as possible. We\nformulate the problem as a Mixed-Integer Non-Linear Program and design an\nefficient dynamic programming algorithm to optimize model partitioning and\ndevice assignment. Experimental results show that the proposed method\nsignificantly reduces cold-start latency compared to baseline strategies.", "AI": {"tldr": "A framework for minimizing inference latency in edge devices by overlapping model loading with computation and communication, using dynamic layer partitioning and allocation.", "motivation": "Deploying large language models on edge devices is challenging due to limited resources and cold-start latency from on-demand model loading.", "method": "Proposes a latency-aware scheduling framework, formulates the problem as a Mixed-Integer Non-Linear Program, and uses dynamic programming for optimization.", "result": "Significantly reduces cold-start latency compared to baseline strategies.", "conclusion": "The framework effectively hides loading time and optimizes resource use, improving edge AI service efficiency."}}
{"id": "2508.11446", "pdf": "https://arxiv.org/pdf/2508.11446", "abs": "https://arxiv.org/abs/2508.11446", "authors": ["Daniel Airinei", "Elena Burceanu", "Marius Leordeanu"], "title": "Inside Knowledge: Graph-based Path Generation with Explainable Data Augmentation and Curriculum Learning for Visual Indoor Navigation", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted at the International Conference on Computer Vision Workshops\n  2025", "summary": "Indoor navigation is a difficult task, as it generally comes with poor GPS\naccess, forcing solutions to rely on other sources of information. While\nsignificant progress continues to be made in this area, deployment to\nproduction applications is still lacking, given the complexity and additional\nrequirements of current solutions. Here, we introduce an efficient, real-time\nand easily deployable deep learning approach, based on visual input only, that\ncan predict the direction towards a target from images captured by a mobile\ndevice. Our technical approach, based on a novel graph-based path generation\nmethod, combined with explainable data augmentation and curriculum learning,\nincludes contributions that make the process of data collection, annotation and\ntraining, as automatic as possible, efficient and robust. On the practical\nside, we introduce a novel largescale dataset, with video footage inside a\nrelatively large shopping mall, in which each frame is annotated with the\ncorrect next direction towards different specific target destinations.\nDifferent from current methods, ours relies solely on vision, avoiding the need\nof special sensors, additional markers placed along the path, knowledge of the\nscene map or internet access. We also created an easy to use application for\nAndroid, which we plan to make publicly available. We make all our data and\ncode available along with visual demos on our project site", "AI": {"tldr": "The paper introduces a vision-only deep learning approach for indoor navigation, using a novel graph-based method and a new dataset from a shopping mall. It avoids special sensors or maps and includes an Android app.", "motivation": "Indoor navigation is challenging due to poor GPS, and existing solutions are complex or require additional infrastructure. The goal is a simpler, vision-only approach.", "method": "A deep learning model uses visual input, graph-based path generation, explainable data augmentation, and curriculum learning. A new dataset with annotated video footage is introduced.", "result": "The method works in real-time, avoids special sensors or maps, and includes a publicly available Android app.", "conclusion": "The approach is efficient, deployable, and relies solely on vision, making indoor navigation more accessible."}}
{"id": "2508.10944", "pdf": "https://arxiv.org/pdf/2508.10944", "abs": "https://arxiv.org/abs/2508.10944", "authors": ["Mengze Li"], "title": "Non-asymptotic convergence bound of conditional diffusion models", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "Learning and generating various types of data based on conditional diffusion\nmodels has been a research hotspot in recent years. Although conditional\ndiffusion models have made considerable progress in improving acceleration\nalgorithms and enhancing generation quality, the lack of non-asymptotic\nproperties has hindered theoretical research. To address this gap, we focus on\na conditional diffusion model within the domains of classification and\nregression (CARD), which aims to learn the original distribution with given\ninput x (denoted as Y|X). It innovatively integrates a pre-trained model\nf_{\\phi}(x) into the original diffusion model framework, allowing it to\nprecisely capture the original conditional distribution given f (expressed as\nY|f_{\\phi}(x)). Remarkably, when f_{\\phi}(x) performs satisfactorily,\nY|f_{\\phi}(x) closely approximates Y|X. Theoretically, we deduce the stochastic\ndifferential equations of CARD and establish its generalized form predicated on\nthe Fokker-Planck equation, thereby erecting a firm theoretical foundation for\nanalysis. Mainly under the Lipschitz assumptions, we utilize the second-order\nWasserstein distance to demonstrate the upper error bound between the original\nand the generated conditional distributions. Additionally, by appending\nassumptions such as light-tailedness to the original distribution, we derive\nthe convergence upper bound between the true value analogous to the score\nfunction and the corresponding network-estimated value.", "AI": {"tldr": "The paper introduces CARD, a conditional diffusion model for classification and regression, integrating a pre-trained model to capture conditional distributions and providing theoretical foundations and error bounds.", "motivation": "To address the lack of non-asymptotic properties in conditional diffusion models, enabling theoretical research and improving conditional distribution learning.", "method": "Integrates a pre-trained model into the diffusion framework, derives stochastic differential equations, and uses the Fokker-Planck equation for theoretical analysis.", "result": "Establishes upper error bounds for conditional distributions and convergence bounds for score function estimates under Lipschitz and light-tailed assumptions.", "conclusion": "CARD provides a robust theoretical framework for conditional diffusion models, enhancing their applicability in learning and generating conditional distributions."}}
{"id": "2508.11291", "pdf": "https://arxiv.org/pdf/2508.11291", "abs": "https://arxiv.org/abs/2508.11291", "authors": ["Rui Bao", "Nan Xue", "Yaping Sun", "Zhiyong Chen"], "title": "Dynamic Quality-Latency Aware Routing for LLM Inference in Wireless Edge-Device Networks", "categories": ["cs.IT", "cs.AI", "cs.LG", "math.IT"], "comment": "accepted by IEEE/CIC ICCC workshop", "summary": "The integration of wireless communications and Large Language Models (LLMs)\nis poised to unlock ubiquitous intelligent services, yet deploying them in\nwireless edge-device collaborative environments presents a critical trade-off\nbetween inference quality and end-to-end latency. A fundamental mismatch exists\nbetween task complexity and resource allocation: offloading simple queries\ninvites prohibitive latency, while on-device models lack the capacity for\ndemanding computations. To address this challenge, we propose a dynamic,\nquality-latency aware routing framework that orchestrates inference between a\nlightweight model on the mobile device and a powerful model on the edge server.\nOur framework employs two distinct cost models: for single-turn queries, it\nfuses a BERT-predicted semantic score with communication and computation\noverheads; for multi-turn dialogues, it further quantifies context-aware costs\narising from model switching and KV-cache management. While maintaining full\ninference quality, extensive experiments demonstrate that our framework cuts\naverage response latency by 5-15% and reduces large model invocations by 10-20%\nagainst competitive baselines on MMLU, GSM8K, and MT-Bench-101 benchmarks.", "AI": {"tldr": "A dynamic routing framework balances inference quality and latency in wireless edge-device collaboration by orchestrating tasks between lightweight on-device and powerful edge-server models.", "motivation": "Addressing the trade-off between inference quality and latency in deploying LLMs in wireless edge-device environments.", "method": "Proposes a quality-latency aware routing framework using BERT-predicted scores for single-turn queries and context-aware costs for multi-turn dialogues.", "result": "Reduces average response latency by 5-15% and large model invocations by 10-20% while maintaining full inference quality.", "conclusion": "The framework effectively optimizes resource allocation for LLM deployment in edge environments."}}
{"id": "2508.11464", "pdf": "https://arxiv.org/pdf/2508.11464", "abs": "https://arxiv.org/abs/2508.11464", "authors": ["Xiaoya Zhu", "Yibing Nan", "Shiguo Lian"], "title": "Data-Driven Deepfake Image Detection Method -- The 2024 Global Deepfake Image Detection Challenge", "categories": ["cs.CV"], "comment": null, "summary": "With the rapid development of technology in the field of AI, deepfake\ntechnology has emerged as a double-edged sword. It has not only created a large\namount of AI-generated content but also posed unprecedented challenges to\ndigital security. The task of the competition is to determine whether a face\nimage is a Deepfake image and output its probability score of being a Deepfake\nimage. In the image track competition, our approach is based on the Swin\nTransformer V2-B classification network. And online data augmentation and\noffline sample generation methods are employed to enrich the diversity of\ntraining samples and increase the generalization ability of the model. Finally,\nwe got the award of excellence in Deepfake image detection.", "AI": {"tldr": "The paper discusses using Swin Transformer V2-B for Deepfake image detection, employing data augmentation to enhance model performance, achieving excellence in competition.", "motivation": "Address challenges posed by Deepfake technology in digital security by improving detection accuracy.", "method": "Uses Swin Transformer V2-B classification network with online data augmentation and offline sample generation to diversify training samples.", "result": "Achieved excellence in Deepfake image detection competition.", "conclusion": "The approach effectively improves Deepfake detection, demonstrating the potential of advanced models and data augmentation."}}
{"id": "2508.11469", "pdf": "https://arxiv.org/pdf/2508.11469", "abs": "https://arxiv.org/abs/2508.11469", "authors": ["Hongjin Fang", "Daniel Reisenb\u00fcchler", "Kenji Ikemura", "Mert R. Sabuncu", "Yihe Yang", "Ruining Deng"], "title": "CoFi: A Fast Coarse-to-Fine Few-Shot Pipeline for Glomerular Basement Membrane Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Accurate segmentation of the glomerular basement membrane (GBM) in electron\nmicroscopy (EM) images is fundamental for quantifying membrane thickness and\nsupporting the diagnosis of various kidney diseases. While supervised deep\nlearning approaches achieve high segmentation accuracy, their reliance on\nextensive pixel-level annotation renders them impractical for clinical\nworkflows. Few-shot learning can reduce this annotation burden but often\nstruggles to capture the fine structural details necessary for GBM analysis. In\nthis study, we introduce CoFi, a fast and efficient coarse-to-fine few-shot\nsegmentation pipeline designed for GBM delineation in EM images. CoFi first\ntrains a lightweight neural network using only three annotated images to\nproduce an initial coarse segmentation mask. This mask is then automatically\nprocessed to generate high-quality point prompts with morphology-aware pruning,\nwhich are subsequently used to guide SAM in refining the segmentation. The\nproposed method achieved exceptional GBM segmentation performance, with a Dice\ncoefficient of 74.54% and an inference speed of 1.9 FPS. We demonstrate that\nCoFi not only alleviates the annotation and computational burdens associated\nwith conventional methods, but also achieves accurate and reliable segmentation\nresults. The pipeline's speed and annotation efficiency make it well-suited for\nresearch and hold strong potential for clinical applications in renal\npathology. The pipeline is publicly available at:\nhttps://github.com/ddrrnn123/CoFi.", "AI": {"tldr": "CoFi is a coarse-to-fine few-shot segmentation pipeline for GBM in EM images, reducing annotation burden while maintaining accuracy.", "motivation": "Supervised deep learning requires extensive annotation, and few-shot methods often miss fine details in GBM segmentation.", "method": "CoFi uses a lightweight network for coarse segmentation, then refines it with SAM using morphology-aware point prompts.", "result": "Achieves 74.54% Dice coefficient and 1.9 FPS, balancing accuracy and speed.", "conclusion": "CoFi is efficient for research and clinical use, with potential in renal pathology."}}
{"id": "2508.11478", "pdf": "https://arxiv.org/pdf/2508.11478", "abs": "https://arxiv.org/abs/2508.11478", "authors": ["Xinyi Yin", "Wenbo Yuan", "Xuecheng Wu", "Liangyu Fu", "Danlei Huang"], "title": "TACR-YOLO: A Real-time Detection Framework for Abnormal Human Behaviors Enhanced with Coordinate and Task-Aware Representations", "categories": ["cs.CV"], "comment": "8 pages, 4 figures, accepted by IJCNN 2025", "summary": "Abnormal Human Behavior Detection (AHBD) under special scenarios is becoming\nincreasingly crucial. While YOLO-based detection methods excel in real-time\ntasks, they remain hindered by challenges including small objects, task\nconflicts, and multi-scale fusion in AHBD. To tackle them, we propose\nTACR-YOLO, a new real-time framework for AHBD. We introduce a Coordinate\nAttention Module to enhance small object detection, a Task-Aware Attention\nModule to deal with classification-regression conflicts, and a Strengthen Neck\nNetwork for refined multi-scale fusion, respectively. In addition, we optimize\nAnchor Box sizes using K-means clustering and deploy DIoU-Loss to improve\nbounding box regression. The Personnel Anomalous Behavior Detection (PABD)\ndataset, which includes 8,529 samples across four behavior categories, is also\npresented. Extensive experimental results indicate that TACR-YOLO achieves\n91.92% mAP on PABD, with competitive speed and robustness. Ablation studies\nhighlight the contribution of each improvement. This work provides new insights\nfor abnormal behavior detection under special scenarios, advancing its\nprogress.", "AI": {"tldr": "TACR-YOLO is a real-time framework for abnormal human behavior detection, addressing challenges like small objects and task conflicts with novel modules and optimizations. It achieves 91.92% mAP on the PABD dataset.", "motivation": "The need for effective abnormal human behavior detection in special scenarios, overcoming limitations of YOLO-based methods like small object detection and task conflicts.", "method": "Introduces Coordinate Attention Module, Task-Aware Attention Module, and Strengthen Neck Network, optimizes Anchor Box sizes with K-means, and uses DIoU-Loss. Evaluated on the PABD dataset.", "result": "Achieves 91.92% mAP on PABD, demonstrating competitive speed and robustness. Ablation studies validate the contributions of each improvement.", "conclusion": "TACR-YOLO advances abnormal behavior detection, offering new insights and solutions for special scenarios."}}
{"id": "2508.11060", "pdf": "https://arxiv.org/pdf/2508.11060", "abs": "https://arxiv.org/abs/2508.11060", "authors": ["Jeongjin Lee", "Jong-Min Kim"], "title": "Counterfactual Survival Q Learning for Longitudinal Randomized Trials via Buckley James Boosting", "categories": ["stat.ML", "cs.LG", "stat.ME"], "comment": null, "summary": "We propose a Buckley James (BJ) Boost Q learning framework for estimating\noptimal dynamic treatment regimes under right censored survival data, tailored\nfor longitudinal randomized clinical trial settings. The method integrates\naccelerated failure time models with iterative boosting techniques, including\ncomponentwise least squares and regression trees, within a counterfactual Q\nlearning framework. By directly modeling conditional survival time, BJ Boost Q\nlearning avoids the restrictive proportional hazards assumption and enables\nunbiased estimation of stage specific Q functions. Grounded in potential\noutcomes, this framework ensures identifiability of the optimal treatment\nregime under standard causal assumptions. Compared to Cox based Q learning,\nwhich relies on hazard modeling and may suffer from bias under\nmisspecification, our approach provides robust and flexible estimation.\nSimulation studies and analysis of the ACTG175 HIV trial demonstrate that BJ\nBoost Q learning yields higher accuracy in treatment decision making,\nespecially in multistage settings where bias can accumulate.", "AI": {"tldr": "A BJ Boost Q learning framework is proposed for optimal dynamic treatment regimes in survival data, avoiding proportional hazards assumptions and improving accuracy in multistage settings.", "motivation": "To address limitations of Cox-based Q learning, which relies on hazard modeling and may suffer bias under misspecification, by providing a robust and flexible alternative.", "method": "Integrates accelerated failure time models with boosting techniques (componentwise least squares, regression trees) in a counterfactual Q learning framework, directly modeling conditional survival time.", "result": "Simulations and HIV trial analysis show BJ Boost Q learning improves treatment decision accuracy, especially in multistage settings.", "conclusion": "The framework avoids restrictive assumptions, ensures unbiased Q function estimation, and outperforms Cox-based methods in robustness and flexibility."}}
{"id": "2508.11482", "pdf": "https://arxiv.org/pdf/2508.11482", "abs": "https://arxiv.org/abs/2508.11482", "authors": ["Ruoxin Xiong", "Yanyu Wang", "Jiannan Cai", "Kaijian Liu", "Yuansheng Zhu", "Pingbo Tang", "Nora El-Gohary"], "title": "OpenConstruction: A Systematic Synthesis of Open Visual Datasets for Data-Centric Artificial Intelligence in Construction Monitoring", "categories": ["cs.CV"], "comment": null, "summary": "The construction industry increasingly relies on visual data to support\nArtificial Intelligence (AI) and Machine Learning (ML) applications for site\nmonitoring. High-quality, domain-specific datasets, comprising images, videos,\nand point clouds, capture site geometry and spatiotemporal dynamics, including\nthe location and interaction of objects, workers, and materials. However,\ndespite growing interest in leveraging visual datasets, existing resources vary\nwidely in sizes, data modalities, annotation quality, and representativeness of\nreal-world construction conditions. A systematic review to categorize their\ndata characteristics and application contexts is still lacking, limiting the\ncommunity's ability to fully understand the dataset landscape, identify\ncritical gaps, and guide future directions toward more effective, reliable, and\nscalable AI applications in construction. To address this gap, this study\nconducts an extensive search of academic databases and open-data platforms,\nyielding 51 publicly available visual datasets that span the 2005-2024 period.\nThese datasets are categorized using a structured data schema covering (i) data\nfundamentals (e.g., size and license), (ii) data modalities (e.g., RGB and\npoint cloud), (iii) annotation frameworks (e.g., bounding boxes), and (iv)\ndownstream application domains (e.g., progress tracking). This study\nsynthesizes these findings into an open-source catalog, OpenConstruction,\nsupporting data-driven method development. Furthermore, the study discusses\nseveral critical limitations in the existing construction dataset landscape and\npresents a roadmap for future data infrastructure anchored in the Findability,\nAccessibility, Interoperability, and Reusability (FAIR) principles. By\nreviewing the current landscape and outlining strategic priorities, this study\nsupports the advancement of data-centric solutions in the construction sector.", "AI": {"tldr": "The paper reviews 51 visual datasets for AI/ML in construction, categorizing them by data fundamentals, modalities, annotations, and applications. It introduces OpenConstruction, an open-source catalog, and proposes a FAIR-based roadmap for future data infrastructure.", "motivation": "To address the lack of systematic review and categorization of visual datasets in construction, which hinders effective AI/ML applications.", "method": "Conducted an extensive search of academic databases and open-data platforms, analyzing 51 datasets from 2005-2024 using a structured data schema.", "result": "Created OpenConstruction, a catalog of datasets, and identified critical gaps in the dataset landscape. Proposed a FAIR-based roadmap for future improvements.", "conclusion": "The study advances data-centric solutions in construction by systematizing dataset knowledge and guiding future infrastructure development."}}
{"id": "2508.11062", "pdf": "https://arxiv.org/pdf/2508.11062", "abs": "https://arxiv.org/abs/2508.11062", "authors": ["Bhavishya Tarun", "Haoze Du", "Dinesh Kannan", "Edward F. Gehringer"], "title": "Human-in-the-Loop Systems for Adaptive Learning Using Generative AI", "categories": ["cs.HC", "cs.LG"], "comment": "Accepted for presentation at the Frontiers in Education Conference,\n  Nashville, Tennessee, USA, 2-5 November 2025", "summary": "A Human-in-the-Loop (HITL) approach leverages generative AI to enhance\npersonalized learning by directly integrating student feedback into\nAI-generated solutions. Students critique and modify AI responses using\npredefined feedback tags, fostering deeper engagement and understanding. This\nempowers students to actively shape their learning, with AI serving as an\nadaptive partner. The system uses a tagging technique and prompt engineering to\npersonalize content, informing a Retrieval-Augmented Generation (RAG) system to\nretrieve relevant educational material and adjust explanations in real time.\nThis builds on existing research in adaptive learning, demonstrating how\nstudent-driven feedback loops can modify AI-generated responses for improved\nstudent retention and engagement, particularly in STEM education. Preliminary\nfindings from a study with STEM students indicate improved learning outcomes\nand confidence compared to traditional AI tools. This work highlights AI's\npotential to create dynamic, feedback-driven, and personalized learning\nenvironments through iterative refinement.", "AI": {"tldr": "A HITL approach uses generative AI and student feedback to personalize learning, improving engagement and outcomes in STEM education.", "motivation": "To enhance personalized learning by integrating student feedback into AI-generated solutions, fostering deeper engagement and understanding.", "method": "Uses predefined feedback tags, tagging techniques, and prompt engineering to inform a RAG system for real-time content adjustment.", "result": "Preliminary findings show improved learning outcomes and confidence in STEM students compared to traditional AI tools.", "conclusion": "The study demonstrates AI's potential for dynamic, feedback-driven personalized learning through iterative refinement."}}
{"id": "2508.11484", "pdf": "https://arxiv.org/pdf/2508.11484", "abs": "https://arxiv.org/abs/2508.11484", "authors": ["Xiaoxue Wu", "Bingjie Gao", "Yu Qiao", "Yaohui Wang", "Xinyuan Chen"], "title": "CineTrans: Learning to Generate Videos with Cinematic Transitions via Masked Diffusion Models", "categories": ["cs.CV"], "comment": "27 pages, 20 figures", "summary": "Despite significant advances in video synthesis, research into multi-shot\nvideo generation remains in its infancy. Even with scaled-up models and massive\ndatasets, the shot transition capabilities remain rudimentary and unstable,\nlargely confining generated videos to single-shot sequences. In this work, we\nintroduce CineTrans, a novel framework for generating coherent multi-shot\nvideos with cinematic, film-style transitions. To facilitate insights into the\nfilm editing style, we construct a multi-shot video-text dataset Cine250K with\ndetailed shot annotations. Furthermore, our analysis of existing video\ndiffusion models uncovers a correspondence between attention maps in the\ndiffusion model and shot boundaries, which we leverage to design a mask-based\ncontrol mechanism that enables transitions at arbitrary positions and transfers\neffectively in a training-free setting. After fine-tuning on our dataset with\nthe mask mechanism, CineTrans produces cinematic multi-shot sequences while\nadhering to the film editing style, avoiding unstable transitions or naive\nconcatenations. Finally, we propose specialized evaluation metrics for\ntransition control, temporal consistency and overall quality, and demonstrate\nthrough extensive experiments that CineTrans significantly outperforms existing\nbaselines across all criteria.", "AI": {"tldr": "CineTrans is a framework for generating coherent multi-shot videos with cinematic transitions, leveraging a new dataset and mask-based control in diffusion models.", "motivation": "Current video synthesis lacks stable multi-shot generation, limiting videos to single-shot sequences.", "method": "Introduces CineTrans, a framework using a new dataset (Cine250K) and mask-based control in diffusion models for transitions.", "result": "CineTrans outperforms baselines in transition control, consistency, and quality.", "conclusion": "CineTrans advances multi-shot video generation with stable, cinematic transitions."}}
{"id": "2508.11069", "pdf": "https://arxiv.org/pdf/2508.11069", "abs": "https://arxiv.org/abs/2508.11069", "authors": ["Olga A. Vsevolozhskaya", "Dmitri V. Zaykin", "Mark C. Greenwood", "Changshuai Wei", "Qing Lu"], "title": "Functional Analysis of Variance for Association Studies", "categories": ["stat.AP", "cs.LG", "stat.ME"], "comment": null, "summary": "While progress has been made in identifying common genetic variants\nassociated with human diseases, for most of common complex diseases, the\nidentified genetic variants only account for a small proportion of\nheritability. Challenges remain in finding additional unknown genetic variants\npredisposing to complex diseases. With the advance in next-generation\nsequencing technologies, sequencing studies have become commonplace in genetic\nresearch. The ongoing exome-sequencing and whole-genome-sequencing studies\ngenerate a massive amount of sequencing variants and allow researchers to\ncomprehensively investigate their role in human diseases. The discovery of new\ndisease-associated variants can be enhanced by utilizing powerful and\ncomputationally efficient statistical methods. In this paper, we propose a\nfunctional analysis of variance (FANOVA) method for testing an association of\nsequence variants in a genomic region with a qualitative trait. The FANOVA has\na number of advantages: (1) it tests for a joint effect of gene variants,\nincluding both common and rare; (2) it fully utilizes linkage disequilibrium\nand genetic position information; and (3) allows for either protective or\nrisk-increasing causal variants. Through simulations, we show that FANOVA\noutperform two popularly used methods - SKAT and a previously proposed method\nbased on functional linear models (FLM), - especially if a sample size of a\nstudy is small and/or sequence variants have low to moderate effects. We\nconduct an empirical study by applying three methods (FANOVA, SKAT and FLM) to\nsequencing data from Dallas Heart Study. While SKAT and FLM respectively\ndetected ANGPTL 4 and ANGPTL 3 associated with obesity, FANOVA was able to\nidentify both genes associated with obesity.", "AI": {"tldr": "The paper introduces FANOVA, a method for testing associations of sequence variants with qualitative traits, outperforming existing methods like SKAT and FLM, especially in small sample sizes or low-effect scenarios.", "motivation": "Despite progress in identifying genetic variants for diseases, most complex diseases lack sufficient heritability explanations. Advanced sequencing technologies generate vast data, but efficient statistical methods are needed to uncover new disease-associated variants.", "method": "The paper proposes FANOVA, a functional analysis of variance method that tests joint effects of gene variants, uses linkage disequilibrium and genetic position information, and accommodates protective or risk-increasing variants.", "result": "Simulations show FANOVA outperforms SKAT and FLM, particularly in small samples or with low-moderate effect variants. An empirical study on obesity data identified both ANGPTL 4 and ANGPTL 3, while SKAT and FLM each found only one.", "conclusion": "FANOVA is a powerful, efficient method for detecting disease-associated variants, especially in challenging scenarios, enhancing genetic research on complex diseases."}}
{"id": "2508.11486", "pdf": "https://arxiv.org/pdf/2508.11486", "abs": "https://arxiv.org/abs/2508.11486", "authors": ["Kristina Dabrock", "Tim Johansson", "Anna Donarelli", "Mikael Mangold", "Noah Pflugradt", "Jann Michael Weinand", "Jochen Lin\u00dfen"], "title": "Automated Building Heritage Assessment Using Street-Level Imagery", "categories": ["cs.CV"], "comment": null, "summary": "Detailed data is required to quantify energy conservation measures in\nbuildings, such as envelop retrofits, without compromising cultural heritage.\nNovel artificial intelligence tools may improve efficiency in identifying\nheritage values in buildings compared to costly and time-consuming traditional\ninventories. In this study, the large language model GPT was used to detect\nvarious aspects of cultural heritage value in fa\\c{c}ade images. Using this\ndata and building register data as features, machine learning models were\ntrained to classify multi-family and non-residential buildings in Stockholm,\nSweden. Validation against an expert-created inventory shows a macro F1-score\nof 0.71 using a combination of register data and features retrieved from GPT,\nand a score of 0.60 using only GPT-derived data. The presented methodology can\ncontribute to a higher-quality database and thus support careful energy\nefficiency measures and integrated consideration of heritage value in\nlarge-scale energetic refurbishment scenarios.", "AI": {"tldr": "AI tools like GPT and machine learning can efficiently identify cultural heritage values in buildings, aiding energy conservation without compromising heritage.", "motivation": "To improve efficiency in identifying heritage values for energy conservation in buildings, avoiding costly traditional methods.", "method": "Used GPT to detect cultural heritage values in facade images, combined with building register data, to train machine learning models for classification.", "result": "Achieved a macro F1-score of 0.71 with combined data and 0.60 with GPT-only data, validated against expert inventories.", "conclusion": "The method enhances databases for better energy efficiency measures while preserving heritage in large-scale refurbishments."}}
{"id": "2508.11488", "pdf": "https://arxiv.org/pdf/2508.11488", "abs": "https://arxiv.org/abs/2508.11488", "authors": ["Bozhou Zhang", "Jingyu Li", "Nan Song", "Li Zhang"], "title": "Perception in Plan: Coupled Perception and Planning for End-to-End Autonomous Driving", "categories": ["cs.CV"], "comment": null, "summary": "End-to-end autonomous driving has achieved remarkable advancements in recent\nyears. Existing methods primarily follow a perception-planning paradigm, where\nperception and planning are executed sequentially within a fully differentiable\nframework for planning-oriented optimization. We further advance this paradigm\nthrough a perception-in-plan framework design, which integrates perception into\nthe planning process. This design facilitates targeted perception guided by\nevolving planning objectives over time, ultimately enhancing planning\nperformance. Building on this insight, we introduce VeteranAD, a coupled\nperception and planning framework for end-to-end autonomous driving. By\nincorporating multi-mode anchored trajectories as planning priors, the\nperception module is specifically designed to gather traffic elements along\nthese trajectories, enabling comprehensive and targeted perception. Planning\ntrajectories are then generated based on both the perception results and the\nplanning priors. To make perception fully serve planning, we adopt an\nautoregressive strategy that progressively predicts future trajectories while\nfocusing on relevant regions for targeted perception at each step. With this\nsimple yet effective design, VeteranAD fully unleashes the potential of\nplanning-oriented end-to-end methods, leading to more accurate and reliable\ndriving behavior. Extensive experiments on the NAVSIM and Bench2Drive datasets\ndemonstrate that our VeteranAD achieves state-of-the-art performance.", "AI": {"tldr": "VeteranAD introduces a perception-in-plan framework for autonomous driving, integrating perception into planning for targeted optimization, achieving state-of-the-art results.", "motivation": "To enhance planning performance in end-to-end autonomous driving by integrating perception directly into the planning process, guided by evolving objectives.", "method": "Uses a coupled perception and planning framework with multi-mode anchored trajectories as priors, adopting an autoregressive strategy for progressive trajectory prediction and targeted perception.", "result": "Achieves state-of-the-art performance on NAVSIM and Bench2Drive datasets, demonstrating more accurate and reliable driving behavior.", "conclusion": "VeteranAD's perception-in-plan design effectively optimizes planning-oriented end-to-end autonomous driving, outperforming existing methods."}}
{"id": "2508.11175", "pdf": "https://arxiv.org/pdf/2508.11175", "abs": "https://arxiv.org/abs/2508.11175", "authors": ["Ali Karimi", "Hadi Zadeh-Haghighi", "Youssef Kora", "Christoph Simon"], "title": "The Role of Entanglement in Quantum Reservoir Computing with Coupled Kerr Nonlinear Oscillators", "categories": ["quant-ph", "cs.LG", "eess.SP"], "comment": null, "summary": "Quantum Reservoir Computing (QRC) uses quantum dynamics to efficiently\nprocess temporal data. In this work, we investigate a QRC framework based on\ntwo coupled Kerr nonlinear oscillators, a system well-suited for time-series\nprediction tasks due to its complex nonlinear interactions and potentially\nhigh-dimensional state space. We explore how its performance in time-series\nprediction depends on key physical parameters: input drive strength, Kerr\nnonlinearity, and oscillator coupling, and analyze the role of entanglement in\nimproving the reservoir's computational performance, focusing on its effect on\npredicting non-trivial time series. Using logarithmic negativity to quantify\nentanglement and normalized root mean square error (NRMSE) to evaluate\npredictive accuracy, our results suggest that entanglement provides a\ncomputational advantage on average-up to a threshold in the input\nfrequency-that persists under some levels of dissipation and dephasing. In\nparticular, we find that higher dissipation rates can enhance performance.\nWhile the entanglement advantage manifests as improvements in both average and\nworst-case performance, it does not lead to improvements in the best-case\nerror. These findings contribute to the broader understanding of quantum\nreservoirs for high performance, efficient quantum machine learning and\ntime-series forecasting.", "AI": {"tldr": "Quantum Reservoir Computing (QRC) using coupled Kerr nonlinear oscillators shows entanglement improves time-series prediction performance, but only up to a threshold. Higher dissipation rates can enhance results.", "motivation": "To explore how quantum dynamics, specifically entanglement, can enhance computational performance in QRC for time-series prediction tasks.", "method": "Investigates a QRC framework with two coupled Kerr nonlinear oscillators, analyzing key parameters (input drive strength, Kerr nonlinearity, coupling) and entanglement's role using logarithmic negativity and NRMSE.", "result": "Entanglement provides a computational advantage up to a threshold, with higher dissipation rates improving performance. It enhances average and worst-case performance but not best-case error.", "conclusion": "The study advances understanding of quantum reservoirs for efficient quantum machine learning and time-series forecasting, highlighting entanglement's role in performance."}}
{"id": "2508.11497", "pdf": "https://arxiv.org/pdf/2508.11497", "abs": "https://arxiv.org/abs/2508.11497", "authors": ["Feiyue Zhao", "Zhichao Zhang"], "title": "Hierarchical Graph Feature Enhancement with Adaptive Frequency Modulation for Visual Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Convolutional neural networks (CNNs) have\n  demonstrated strong performance in visual recognition tasks,\n  but their inherent reliance on regular grid structures limits\n  their capacity to model complex topological relationships and\n  non-local semantics within images. To address this limita tion, we propose\nthe hierarchical graph feature enhancement\n  (HGFE), a novel framework that integrates graph-based rea soning into CNNs to\nenhance both structural awareness and\n  feature representation. HGFE builds two complementary levels\n  of graph structures: intra-window graph convolution to cap ture local spatial\ndependencies and inter-window supernode\n  interactions to model global semantic relationships. Moreover,\n  we introduce an adaptive frequency modulation module that\n  dynamically balances low-frequency and high-frequency signal\n  propagation, preserving critical edge and texture information\n  while mitigating over-smoothing. The proposed HGFE module\n  is lightweight, end-to-end trainable, and can be seamlessly\n  integrated into standard CNN backbone networks. Extensive\n  experiments on CIFAR-100 (classification), PASCAL VOC,\n  and VisDrone (detection), as well as CrackSeg and CarParts\n  (segmentation), validated the effectiveness of the HGFE in\n  improving structural representation and enhancing overall\n  recognition performance.", "AI": {"tldr": "The paper introduces HGFE, a graph-based framework to enhance CNNs by modeling local and global relationships in images, improving recognition tasks.", "motivation": "CNNs struggle with complex topological relationships and non-local semantics due to their grid structure. HGFE aims to address this limitation.", "method": "HGFE integrates graph-based reasoning with CNNs, using intra-window and inter-window graph structures, plus adaptive frequency modulation.", "result": "Experiments on multiple datasets (CIFAR-100, PASCAL VOC, etc.) show HGFE improves structural representation and recognition performance.", "conclusion": "HGFE is a lightweight, effective solution for enhancing CNNs with graph-based reasoning, validated across diverse tasks."}}
{"id": "2508.11181", "pdf": "https://arxiv.org/pdf/2508.11181", "abs": "https://arxiv.org/abs/2508.11181", "authors": ["Faisal Ahmed"], "title": "HistoViT: Vision Transformer for Accurate and Scalable Histopathological Cancer Diagnosis", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": "13 pages, 3 Figures", "summary": "Accurate and scalable cancer diagnosis remains a critical challenge in modern\npathology, particularly for malignancies such as breast, prostate, bone, and\ncervical, which exhibit complex histological variability. In this study, we\npropose a transformer-based deep learning framework for multi-class tumor\nclassification in histopathological images. Leveraging a fine-tuned Vision\nTransformer (ViT) architecture, our method addresses key limitations of\nconventional convolutional neural networks, offering improved performance,\nreduced preprocessing requirements, and enhanced scalability across tissue\ntypes. To adapt the model for histopathological cancer images, we implement a\nstreamlined preprocessing pipeline that converts tiled whole-slide images into\nPyTorch tensors and standardizes them through data normalization. This ensures\ncompatibility with the ViT architecture and enhances both convergence stability\nand overall classification performance. We evaluate our model on four benchmark\ndatasets: ICIAR2018 (breast), SICAPv2 (prostate), UT-Osteosarcoma (bone), and\nSipakMed (cervical) dataset -- demonstrating consistent outperformance over\nexisting deep learning methods. Our approach achieves classification accuracies\nof 99.32%, 96.92%, 95.28%, and 96.94% for breast, prostate, bone, and cervical\ncancers respectively, with area under the ROC curve (AUC) scores exceeding 99%\nacross all datasets. These results confirm the robustness, generalizability,\nand clinical potential of transformer-based architectures in digital pathology.\nOur work represents a significant advancement toward reliable, automated, and\ninterpretable cancer diagnosis systems that can alleviate diagnostic burdens\nand improve healthcare outcomes.", "AI": {"tldr": "A transformer-based deep learning framework (Vision Transformer) is proposed for multi-class tumor classification in histopathological images, outperforming existing methods with high accuracy and AUC scores across four cancer datasets.", "motivation": "Accurate and scalable cancer diagnosis is challenging due to complex histological variability in cancers like breast, prostate, bone, and cervical. The study aims to improve performance and scalability over conventional methods.", "method": "The approach uses a fine-tuned Vision Transformer (ViT) with a streamlined preprocessing pipeline converting whole-slide images into PyTorch tensors. Data normalization ensures compatibility and enhances performance.", "result": "The model achieves high classification accuracies (99.32%, 96.92%, 95.28%, 96.94%) and AUC scores (>99%) across breast, prostate, bone, and cervical cancer datasets, outperforming existing methods.", "conclusion": "The study demonstrates the robustness and clinical potential of transformer-based architectures for automated cancer diagnosis, offering improved accuracy and scalability in digital pathology."}}
{"id": "2508.11499", "pdf": "https://arxiv.org/pdf/2508.11499", "abs": "https://arxiv.org/abs/2508.11499", "authors": ["Erez Meoded"], "title": "Handwritten Text Recognition of Historical Manuscripts Using Transformer-Based Models", "categories": ["cs.CV", "cs.AI", "cs.DL", "cs.LG"], "comment": null, "summary": "Historical handwritten text recognition (HTR) is essential for unlocking the\ncultural and scholarly value of archival documents, yet digitization is often\nhindered by scarce transcriptions, linguistic variation, and highly diverse\nhandwriting styles. In this study, we apply TrOCR, a state-of-the-art\ntransformer-based HTR model, to 16th-century Latin manuscripts authored by\nRudolf Gwalther. We investigate targeted image preprocessing and a broad suite\nof data augmentation techniques, introducing four novel augmentation methods\ndesigned specifically for historical handwriting characteristics. We also\nevaluate ensemble learning approaches to leverage the complementary strengths\nof augmentation-trained models. On the Gwalther dataset, our best single-model\naugmentation (Elastic) achieves a Character Error Rate (CER) of 1.86, while a\ntop-5 voting ensemble achieves a CER of 1.60 - representing a 50% relative\nimprovement over the best reported TrOCR_BASE result and a 42% improvement over\nthe previous state of the art. These results highlight the impact of\ndomain-specific augmentations and ensemble strategies in advancing HTR\nperformance for historical manuscripts.", "AI": {"tldr": "The study improves historical handwritten text recognition (HTR) for 16th-century Latin manuscripts using TrOCR, targeted preprocessing, novel data augmentations, and ensemble learning, achieving significant error rate reductions.", "motivation": "Historical HTR is crucial for digitizing archival documents but faces challenges like scarce transcriptions, linguistic variation, and diverse handwriting styles.", "method": "Applied TrOCR, introduced four novel data augmentation techniques for historical handwriting, and evaluated ensemble learning approaches.", "result": "Best single-model augmentation achieved a CER of 1.86, while a top-5 voting ensemble achieved 1.60, improving over previous benchmarks by 50% and 42%, respectively.", "conclusion": "Domain-specific augmentations and ensemble strategies significantly advance HTR performance for historical manuscripts."}}
{"id": "2508.11502", "pdf": "https://arxiv.org/pdf/2508.11502", "abs": "https://arxiv.org/abs/2508.11502", "authors": ["Eyad Alshami", "Shashank Agnihotri", "Bernt Schiele", "Margret Keuper"], "title": "AIM: Amending Inherent Interpretability via Self-Supervised Masking", "categories": ["cs.CV"], "comment": "Accepted at International Conference on Computer Vision (ICCV) 2025", "summary": "It has been observed that deep neural networks (DNNs) often use both genuine\nas well as spurious features. In this work, we propose \"Amending Inherent\nInterpretability via Self-Supervised Masking\" (AIM), a simple yet interestingly\neffective method that promotes the network's utilization of genuine features\nover spurious alternatives without requiring additional annotations. In\nparticular, AIM uses features at multiple encoding stages to guide a\nself-supervised, sample-specific feature-masking process. As a result, AIM\nenables the training of well-performing and inherently interpretable models\nthat faithfully summarize the decision process. We validate AIM across a\ndiverse range of challenging datasets that test both out-of-distribution\ngeneralization and fine-grained visual understanding. These include\ngeneral-purpose classification benchmarks such as ImageNet100, HardImageNet,\nand ImageWoof, as well as fine-grained classification datasets such as\nWaterbirds, TravelingBirds, and CUB-200. AIM demonstrates significant dual\nbenefits: interpretability improvements, as measured by the Energy Pointing\nGame (EPG) score, and accuracy gains over strong baselines. These consistent\ngains across domains and architectures provide compelling evidence that AIM\npromotes the use of genuine and meaningful features that directly contribute to\nimproved generalization and human-aligned interpretability.", "AI": {"tldr": "AIM is a self-supervised method that enhances DNNs' use of genuine features over spurious ones, improving interpretability and accuracy without extra annotations.", "motivation": "DNNs often rely on spurious features; AIM aims to prioritize genuine features for better interpretability and generalization.", "method": "AIM uses multi-stage features to guide self-supervised, sample-specific masking, promoting interpretable models.", "result": "AIM improves interpretability (EPG score) and accuracy across diverse datasets like ImageNet100 and CUB-200.", "conclusion": "AIM effectively enhances model interpretability and performance by focusing on meaningful features."}}
{"id": "2508.11517", "pdf": "https://arxiv.org/pdf/2508.11517", "abs": "https://arxiv.org/abs/2508.11517", "authors": ["Shaoze Huang", "Qi Liu", "Chao Chen", "Yuhang Chen"], "title": "A Real-time Concrete Crack Detection and Segmentation Model Based on YOLOv11", "categories": ["cs.CV"], "comment": null, "summary": "Accelerated aging of transportation infrastructure in the rapidly developing\nYangtze River Delta region necessitates efficient concrete crack detection, as\ncrack deterioration critically compromises structural integrity and regional\neconomic growth. To overcome the limitations of inefficient manual inspection\nand the suboptimal performance of existing deep learning models, particularly\nfor small-target crack detection within complex backgrounds, this paper\nproposes YOLOv11-KW-TA-FP, a multi-task concrete crack detection and\nsegmentation model based on the YOLOv11n architecture. The proposed model\nintegrates a three-stage optimization framework: (1) Embedding dynamic\nKernelWarehouse convolution (KWConv) within the backbone network to enhance\nfeature representation through a dynamic kernel sharing mechanism; (2)\nIncorporating a triple attention mechanism (TA) into the feature pyramid to\nstrengthen channel-spatial interaction modeling; and (3) Designing an FP-IoU\nloss function to facilitate adaptive bounding box regression penalization.\nExperimental validation demonstrates that the enhanced model achieves\nsignificant performance improvements over the baseline, attaining 91.3%\nprecision, 76.6% recall, and 86.4% mAP@50. Ablation studies confirm the\nsynergistic efficacy of the proposed modules. Furthermore, robustness tests\nindicate stable performance under conditions of data scarcity and noise\ninterference. This research delivers an efficient computer vision solution for\nautomated infrastructure inspection, exhibiting substantial practical\nengineering value.", "AI": {"tldr": "The paper proposes YOLOv11-KW-TA-FP, an enhanced YOLOv11n-based model for concrete crack detection, integrating dynamic KWConv, triple attention, and FP-IoU loss, achieving high precision and robustness.", "motivation": "The need for efficient crack detection in aging infrastructure in the Yangtze River Delta, overcoming manual inspection inefficiencies and deep learning model limitations for small-target cracks.", "method": "A multi-task model with three-stage optimization: dynamic KWConv for feature enhancement, triple attention for channel-spatial interaction, and FP-IoU loss for adaptive bounding box regression.", "result": "Achieves 91.3% precision, 76.6% recall, and 86.4% mAP@50, with robustness in data scarcity and noise.", "conclusion": "The model offers an efficient, practical solution for automated infrastructure inspection with significant engineering value."}}
{"id": "2508.11531", "pdf": "https://arxiv.org/pdf/2508.11531", "abs": "https://arxiv.org/abs/2508.11531", "authors": ["Shilei Wang", "Gong Cheng", "Pujian Lai", "Dong Gao", "Junwei Han"], "title": "Multi-State Tracker: Enhancing Efficient Object Tracking via Multi-State Specialization and Interaction", "categories": ["cs.CV"], "comment": null, "summary": "Efficient trackers achieve faster runtime by reducing computational\ncomplexity and model parameters. However, this efficiency often compromises the\nexpense of weakened feature representation capacity, thus limiting their\nability to accurately capture target states using single-layer features. To\novercome this limitation, we propose Multi-State Tracker (MST), which utilizes\nhighly lightweight state-specific enhancement (SSE) to perform specialized\nenhancement on multi-state features produced by multi-state generation (MSG)\nand aggregates them in an interactive and adaptive manner using cross-state\ninteraction (CSI). This design greatly enhances feature representation while\nincurring minimal computational overhead, leading to improved tracking\nrobustness in complex environments. Specifically, the MSG generates multiple\nstate representations at multiple stages during feature extraction, while SSE\nrefines them to highlight target-specific features. The CSI module facilitates\ninformation exchange between these states and ensures the integration of\ncomplementary features. Notably, the introduced SSE and CSI modules adopt a\nhighly lightweight hidden state adaptation-based state space duality (HSA-SSD)\ndesign, incurring only 0.1 GFLOPs in computation and 0.66 M in parameters.\nExperimental results demonstrate that MST outperforms all previous efficient\ntrackers across multiple datasets, significantly improving tracking accuracy\nand robustness. In particular, it shows excellent runtime performance, with an\nAO score improvement of 4.5\\% over the previous SOTA efficient tracker HCAT on\nthe GOT-10K dataset. The code is available at https://github.com/wsumel/MST.", "AI": {"tldr": "MST introduces lightweight state-specific enhancement (SSE) and cross-state interaction (CSI) to improve feature representation in efficient trackers, achieving better accuracy and robustness with minimal computational overhead.", "motivation": "Efficient trackers often sacrifice feature representation for speed, limiting their ability to accurately capture target states. MST aims to enhance feature representation without significant computational cost.", "method": "MST uses multi-state generation (MSG) to produce multiple state representations, refines them with SSE, and aggregates them via CSI. The design employs lightweight HSA-SSD for efficiency.", "result": "MST outperforms previous efficient trackers, improving tracking accuracy and robustness. It achieves a 4.5% AO score improvement over HCAT on GOT-10K.", "conclusion": "MST successfully balances efficiency and feature representation, offering a robust and accurate tracking solution with minimal computational overhead."}}
{"id": "2508.11274", "pdf": "https://arxiv.org/pdf/2508.11274", "abs": "https://arxiv.org/abs/2508.11274", "authors": ["Paul Dommel", "Rajmadan Lakshmanan"], "title": "Uniform convergence for Gaussian kernel ridge regression", "categories": ["stat.ML", "cs.LG"], "comment": null, "summary": "This paper establishes the first polynomial convergence rates for Gaussian\nkernel ridge regression (KRR) with a fixed hyperparameter in both the uniform\nand the $L^{2}$-norm. The uniform convergence result closes a gap in the\ntheoretical understanding of KRR with the Gaussian kernel, where no such rates\nwere previously known. In addition, we prove a polynomial $L^{2}$-convergence\nrate in the case, where the Gaussian kernel's width parameter is fixed. This\nalso contributes to the broader understanding of smooth kernels, for which\npreviously only sub-polynomial $L^{2}$-rates were known in similar settings.\nTogether, these results provide new theoretical justification for the use of\nGaussian KRR with fixed hyperparameters in nonparametric regression.", "AI": {"tldr": "First polynomial convergence rates for Gaussian KRR with fixed hyperparameters in uniform and L\u00b2-norm, closing gaps in theoretical understanding.", "motivation": "To address the lack of known polynomial convergence rates for Gaussian KRR with fixed hyperparameters, particularly in uniform and L\u00b2-norm settings.", "method": "Analyzes Gaussian kernel ridge regression (KRR) with fixed hyperparameters, focusing on uniform and L\u00b2-norm convergence.", "result": "Establishes polynomial convergence rates for Gaussian KRR in both uniform and L\u00b2-norm, improving upon previous sub-polynomial rates.", "conclusion": "Provides theoretical justification for using Gaussian KRR with fixed hyperparameters in nonparametric regression."}}
{"id": "2508.11398", "pdf": "https://arxiv.org/pdf/2508.11398", "abs": "https://arxiv.org/abs/2508.11398", "authors": ["Mithat Can Ozgun", "Jiahuan Pei", "Koen Hindriks", "Lucia Donatelli", "Qingzhi Liu", "Xin Sun", "Junxiao Wang"], "title": "Trustworthy AI Psychotherapy: Multi-Agent LLM Workflow for Counseling and Explainable Mental Disorder Diagnosis", "categories": ["cs.HC", "cs.AI", "cs.IR"], "comment": "Accepted by CIKM 2025 as a full paper", "summary": "LLM-based agents have emerged as transformative tools capable of executing\ncomplex tasks through iterative planning and action, achieving significant\nadvancements in understanding and addressing user needs. Yet, their\neffectiveness remains limited in specialized domains such as mental health\ndiagnosis, where they underperform compared to general applications. Current\napproaches to integrating diagnostic capabilities into LLMs rely on scarce,\nhighly sensitive mental health datasets, which are challenging to acquire.\nThese methods also fail to emulate clinicians' proactive inquiry skills, lack\nmulti-turn conversational comprehension, and struggle to align outputs with\nexpert clinical reasoning. To address these gaps, we propose DSM5AgentFlow, the\nfirst LLM-based agent workflow designed to autonomously generate DSM-5 Level-1\ndiagnostic questionnaires. By simulating therapist-client dialogues with\nspecific client profiles, the framework delivers transparent, step-by-step\ndisorder predictions, producing explainable and trustworthy results. This\nworkflow serves as a complementary tool for mental health diagnosis, ensuring\nadherence to ethical and legal standards. Through comprehensive experiments, we\nevaluate leading LLMs across three critical dimensions: conversational realism,\ndiagnostic accuracy, and explainability. Our datasets and implementations are\nfully open-sourced.", "AI": {"tldr": "DSM5AgentFlow is an LLM-based agent workflow designed to autonomously generate DSM-5 Level-1 diagnostic questionnaires, addressing limitations in mental health diagnosis by improving conversational realism, accuracy, and explainability.", "motivation": "Current LLM-based agents underperform in specialized domains like mental health diagnosis due to reliance on scarce datasets, lack of clinician-like inquiry skills, and poor alignment with clinical reasoning.", "method": "Proposes DSM5AgentFlow, simulating therapist-client dialogues to generate transparent, step-by-step disorder predictions while adhering to ethical standards.", "result": "Evaluates leading LLMs on conversational realism, diagnostic accuracy, and explainability, with open-sourced datasets and implementations.", "conclusion": "DSM5AgentFlow serves as a complementary tool for mental health diagnosis, offering trustworthy and explainable results."}}
{"id": "2508.11532", "pdf": "https://arxiv.org/pdf/2508.11532", "abs": "https://arxiv.org/abs/2508.11532", "authors": ["Jingsong Xia", "Yue Yin", "Xiuhan Li"], "title": "An Efficient Medical Image Classification Method Based on a Lightweight Improved ConvNeXt-Tiny Architecture", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Intelligent analysis of medical imaging plays a crucial role in assisting\nclinical diagnosis. However, achieving efficient and high-accuracy image\nclassification in resource-constrained computational environments remains\nchallenging. This study proposes a medical image classification method based on\nan improved ConvNeXt-Tiny architecture. Through structural optimization and\nloss function design, the proposed method enhances feature extraction\ncapability and classification performance while reducing computational\ncomplexity. Specifically, the method introduces a dual global pooling (Global\nAverage Pooling and Global Max Pooling) feature fusion strategy into the\nConvNeXt-Tiny backbone to simultaneously preserve global statistical features\nand salient response information. A lightweight channel attention module,\ntermed Squeeze-and-Excitation Vector (SEVector), is designed to improve the\nadaptive allocation of channel weights while minimizing parameter overhead.\nAdditionally, a Feature Smoothing Loss is incorporated into the loss function\nto enhance intra-class feature consistency and suppress intra-class variance.\nUnder CPU-only conditions (8 threads), the method achieves a maximum\nclassification accuracy of 89.10% on the test set within 10 training epochs,\nexhibiting a stable convergence trend in loss values. Experimental results\ndemonstrate that the proposed method effectively improves medical image\nclassification performance in resource-limited settings, providing a feasible\nand efficient solution for the deployment and promotion of medical imaging\nanalysis models.", "AI": {"tldr": "A medical image classification method using an improved ConvNeXt-Tiny architecture enhances accuracy and efficiency in resource-limited settings.", "motivation": "Efficient and high-accuracy medical image classification in resource-constrained environments is challenging.", "method": "Improved ConvNeXt-Tiny with dual global pooling, SEVector attention module, and Feature Smoothing Loss.", "result": "89.10% accuracy on test set under CPU-only conditions with stable convergence.", "conclusion": "The method provides a feasible solution for medical imaging analysis in resource-limited settings."}}
{"id": "2508.11404", "pdf": "https://arxiv.org/pdf/2508.11404", "abs": "https://arxiv.org/abs/2508.11404", "authors": ["Junyeon Kim", "Tianshu Ruan", "Cesar Alan Contreras", "Manolis Chiou"], "title": "An Exploratory Study on Crack Detection in Concrete through Human-Robot Collaboration", "categories": ["cs.RO", "cs.AI", "cs.HC"], "comment": null, "summary": "Structural inspection in nuclear facilities is vital for maintaining\noperational safety and integrity. Traditional methods of manual inspection pose\nsignificant challenges, including safety risks, high cognitive demands, and\npotential inaccuracies due to human limitations. Recent advancements in\nArtificial Intelligence (AI) and robotic technologies have opened new\npossibilities for safer, more efficient, and accurate inspection methodologies.\nSpecifically, Human-Robot Collaboration (HRC), leveraging robotic platforms\nequipped with advanced detection algorithms, promises significant improvements\nin inspection outcomes and reductions in human workload. This study explores\nthe effectiveness of AI-assisted visual crack detection integrated into a\nmobile Jackal robot platform. The experiment results indicate that HRC enhances\ninspection accuracy and reduces operator workload, resulting in potential\nsuperior performance outcomes compared to traditional manual methods.", "AI": {"tldr": "AI-assisted HRC improves nuclear facility inspections by enhancing accuracy and reducing human workload compared to manual methods.", "motivation": "Traditional manual inspections in nuclear facilities are risky, cognitively demanding, and prone to inaccuracies, prompting the need for safer, more efficient alternatives.", "method": "The study integrates AI-assisted visual crack detection into a mobile Jackal robot platform for Human-Robot Collaboration (HRC).", "result": "HRC enhances inspection accuracy and reduces operator workload, outperforming traditional manual methods.", "conclusion": "AI-assisted HRC offers a superior, safer, and more efficient approach to structural inspections in nuclear facilities."}}
{"id": "2508.11538", "pdf": "https://arxiv.org/pdf/2508.11538", "abs": "https://arxiv.org/abs/2508.11538", "authors": ["Sitong Gong", "Lu Zhang", "Yunzhi Zhuge", "Xu Jia", "Pingping Zhang", "Huchuan Lu"], "title": "Reinforcing Video Reasoning Segmentation to Think Before It Segments", "categories": ["cs.CV"], "comment": "12 pages", "summary": "Video reasoning segmentation (VRS) endeavors to delineate referred objects in\nvideos guided by implicit instructions that encapsulate human intent and\ntemporal logic. Previous approaches leverage large vision language models\n(LVLMs) to encode object semantics into <SEG> tokens for mask prediction.\nHowever, this paradigm suffers from limited interpretability during inference\nand suboptimal performance due to inadequate spatiotemporal reasoning. Drawing\ninspiration from seminal breakthroughs in reinforcement learning, we introduce\nVeason-R1, a specialized LVLM for VRS that emphasizes structured reasoning in\nsegmentation. Veason-R1 is trained through Group Relative Policy Optimization\n(GRPO) augmented with Chain-of-Thought (CoT) initialization. To begin with, we\ncurate high-quality CoT training data to instill structured reasoning\ntrajectories, bridging video-level semantics and frame-level spatial grounding,\nyielding the supervised fine-tuned model Veason-SFT. Subsequently, GRPO\nfine-tuning encourages efficient exploration of the reasoning space by\noptimizing reasoning chains. To this end, we incorporate a holistic reward\nmechanism that synergistically enhances spatial alignment and temporal\nconsistency, bolstering keyframe localization and fine-grained grounding.\nComprehensive empirical evaluations demonstrate that Veason-R1 achieves\nstate-of-the-art performance on multiple benchmarks, surpassing prior art by\nsignificant margins (e.g., +1.3 J &F in ReVOS and +10.0 J &F in ReasonVOS),\nwhile exhibiting robustness to hallucinations (+8.8 R). Our code and model\nweights will be available at Veason-R1.", "AI": {"tldr": "Veason-R1, a specialized LVLM for Video Reasoning Segmentation (VRS), improves interpretability and performance via structured reasoning, trained with GRPO and CoT initialization, achieving state-of-the-art results.", "motivation": "Previous VRS methods using LVLMs lack interpretability and suffer from poor spatiotemporal reasoning, prompting the need for a more structured approach.", "method": "Veason-R1 is trained with Group Relative Policy Optimization (GRPO) and Chain-of-Thought (CoT) initialization, enhancing reasoning and grounding.", "result": "Veason-R1 outperforms prior methods (e.g., +1.3 J &F in ReVOS, +10.0 J &F in ReasonVOS) and reduces hallucinations (+8.8 R).", "conclusion": "Veason-R1 sets a new benchmark for VRS by combining structured reasoning with efficient training, offering robust and interpretable segmentation."}}
{"id": "2508.11406", "pdf": "https://arxiv.org/pdf/2508.11406", "abs": "https://arxiv.org/abs/2508.11406", "authors": ["Benjamin Alt", "Mareike Picklum", "Sorin Arion", "Franklin Kenghagho Kenfack", "Michael Beetz"], "title": "Open, Reproducible and Trustworthy Robot-Based Experiments with Virtual Labs and Digital-Twin-Based Execution Tracing", "categories": ["cs.RO", "cs.AI", "68T40", "I.2.9"], "comment": "8 pages, 6 figures, submitted to the 1st IROS Workshop on Embodied AI\n  and Robotics for Future Scientific Discovery", "summary": "We envision a future in which autonomous robots conduct scientific\nexperiments in ways that are not only precise and repeatable, but also open,\ntrustworthy, and transparent. To realize this vision, we present two key\ncontributions: a semantic execution tracing framework that logs sensor data\ntogether with semantically annotated robot belief states, ensuring that\nautomated experimentation is transparent and replicable; and the AICOR Virtual\nResearch Building (VRB), a cloud-based platform for sharing, replicating, and\nvalidating robot task executions at scale. Together, these tools enable\nreproducible, robot-driven science by integrating deterministic execution,\nsemantic memory, and open knowledge representation, laying the foundation for\nautonomous systems to participate in scientific discovery.", "AI": {"tldr": "A framework and platform for transparent, replicable, and scalable robot-driven science.", "motivation": "To enable autonomous robots to conduct scientific experiments with precision, repeatability, transparency, and trustworthiness.", "method": "Developed a semantic execution tracing framework and the AICOR Virtual Research Building (VRB) platform for sharing and validating robot task executions.", "result": "Tools ensure reproducible science by integrating deterministic execution, semantic memory, and open knowledge representation.", "conclusion": "Lays the foundation for autonomous systems to contribute to scientific discovery."}}
{"id": "2508.11550", "pdf": "https://arxiv.org/pdf/2508.11550", "abs": "https://arxiv.org/abs/2508.11550", "authors": ["Zuo Zuo", "Jiahao Dong", "Yanyun Qu", "Zongze Wu"], "title": "Training-Free Anomaly Generation via Dual-Attention Enhancement in Diffusion Model", "categories": ["cs.CV"], "comment": null, "summary": "Industrial anomaly detection (AD) plays a significant role in manufacturing\nwhere a long-standing challenge is data scarcity. A growing body of works have\nemerged to address insufficient anomaly data via anomaly generation. However,\nthese anomaly generation methods suffer from lack of fidelity or need to be\ntrained with extra data. To this end, we propose a training-free anomaly\ngeneration framework dubbed AAG, which is based on Stable Diffusion (SD)'s\nstrong generation ability for effective anomaly image generation. Given a\nnormal image, mask and a simple text prompt, AAG can generate realistic and\nnatural anomalies in the specific regions and simultaneously keep contents in\nother regions unchanged. In particular, we propose Cross-Attention Enhancement\n(CAE) to re-engineer the cross-attention mechanism within Stable Diffusion\nbased on the given mask. CAE increases the similarity between visual tokens in\nspecific regions and text embeddings, which guides these generated visual\ntokens in accordance with the text description. Besides, generated anomalies\nneed to be more natural and plausible with object in given image. We propose\nSelf-Attention Enhancement (SAE) which improves similarity between each normal\nvisual token and anomaly visual tokens. SAE ensures that generated anomalies\nare coherent with original pattern. Extensive experiments on MVTec AD and VisA\ndatasets demonstrate effectiveness of AAG in anomaly generation and its\nutility. Furthermore, anomaly images generated by AAG can bolster performance\nof various downstream anomaly inspection tasks.", "AI": {"tldr": "AAG is a training-free anomaly generation framework using Stable Diffusion to create realistic anomalies in specific image regions without extra data, enhancing downstream AD tasks.", "motivation": "Addressing data scarcity in industrial anomaly detection by generating realistic anomalies without requiring additional training data.", "method": "Uses Stable Diffusion with Cross-Attention Enhancement (CAE) and Self-Attention Enhancement (SAE) to guide anomaly generation based on masks and text prompts.", "result": "Demonstrates effectiveness on MVTec AD and VisA datasets, improving downstream anomaly inspection tasks.", "conclusion": "AAG provides a practical solution for anomaly generation, enhancing AD performance without extra training."}}
{"id": "2508.11569", "pdf": "https://arxiv.org/pdf/2508.11569", "abs": "https://arxiv.org/abs/2508.11569", "authors": ["Zheng Wang", "Shihao Xu", "Wei Shi"], "title": "TrajSV: A Trajectory-based Model for Sports Video Representations and Applications", "categories": ["cs.CV", "cs.IR"], "comment": "This paper has been accepted by TCSVT", "summary": "Sports analytics has received significant attention from both academia and\nindustry in recent years. Despite the growing interest and efforts in this\nfield, several issues remain unresolved, including (1) data unavailability, (2)\nlack of an effective trajectory-based framework, and (3) requirement for\nsufficient supervision labels. In this paper, we present TrajSV, a\ntrajectory-based framework that addresses various issues in existing studies.\nTrajSV comprises three components: data preprocessing, Clip Representation\nNetwork (CRNet), and Video Representation Network (VRNet). The data\npreprocessing module extracts player and ball trajectories from sports\nbroadcast videos. CRNet utilizes a trajectory-enhanced Transformer module to\nlearn clip representations based on these trajectories. Additionally, VRNet\nlearns video representations by aggregating clip representations and visual\nfeatures with an encoder-decoder architecture. Finally, a triple contrastive\nloss is introduced to optimize both video and clip representations in an\nunsupervised manner. The experiments are conducted on three broadcast video\ndatasets to verify the effectiveness of TrajSV for three types of sports (i.e.,\nsoccer, basketball, and volleyball) with three downstream applications (i.e.,\nsports video retrieval, action spotting, and video captioning). The results\ndemonstrate that TrajSV achieves state-of-the-art performance in sports video\nretrieval, showcasing a nearly 70% improvement. It outperforms baselines in\naction spotting, achieving state-of-the-art results in 9 out of 17 action\ncategories, and demonstrates a nearly 20% improvement in video captioning.\nAdditionally, we introduce a deployed system along with the three applications\nbased on TrajSV.", "AI": {"tldr": "TrajSV is a trajectory-based framework for sports analytics, addressing data unavailability, lack of trajectory frameworks, and supervision label needs. It includes preprocessing, CRNet, and VRNet, achieving state-of-the-art results in retrieval, action spotting, and captioning.", "motivation": "To resolve unresolved issues in sports analytics like data unavailability, ineffective trajectory frameworks, and supervision label requirements.", "method": "TrajSV uses data preprocessing, CRNet (trajectory-enhanced Transformer), and VRNet (encoder-decoder for video representations) with triple contrastive loss for unsupervised optimization.", "result": "Achieves 70% improvement in retrieval, state-of-the-art in 9/17 action categories, and 20% better captioning.", "conclusion": "TrajSV effectively addresses sports analytics challenges, demonstrating superior performance in multiple applications."}}
{"id": "2508.11307", "pdf": "https://arxiv.org/pdf/2508.11307", "abs": "https://arxiv.org/abs/2508.11307", "authors": ["Sabin Roman", "Gregor Skok", "Ljupco Todorovski", "Saso Dzeroski"], "title": "Approximating the universal thermal climate index using sparse regression with orthogonal polynomials", "categories": ["physics.ao-ph", "cs.LG", "physics.data-an"], "comment": null, "summary": "This article explores novel data-driven modeling approaches for analyzing and\napproximating the Universal Thermal Climate Index (UTCI), a\nphysiologically-based metric integrating multiple atmospheric variables to\nassess thermal comfort. Given the nonlinear, multivariate structure of UTCI, we\ninvestigate symbolic and sparse regression techniques as tools for\ninterpretable and efficient function approximation. In particular, we highlight\nthe benefits of using orthogonal polynomial bases-such as Legendre\npolynomials-in sparse regression frameworks, demonstrating their advantages in\nstability, convergence, and hierarchical interpretability compared to standard\npolynomial expansions. We demonstrate that our models achieve significantly\nlower root-mean squared losses than the widely used sixth-degree polynomial\nbenchmark-while using the same or fewer parameters. By leveraging Legendre\npolynomial bases, we construct models that efficiently populate a Pareto front\nof accuracy versus complexity and exhibit stable, hierarchical coefficient\nstructures across varying model capacities. Training on just 20% of the data,\nour models generalize robustly to the remaining 80%, with consistent\nperformance under bootstrapping. The decomposition effectively approximates the\nUTCI as a Fourier-like expansion in an orthogonal basis, yielding results near\nthe theoretical optimum in the L2 (least squares) sense. We also connect these\nfindings to the broader context of equation discovery in environmental\nmodeling, referencing probabilistic grammar-based methods that enforce domain\nconsistency and compactness in symbolic expressions. Taken together, these\nresults illustrate how combining sparsity, orthogonality, and symbolic\nstructure enables robust, interpretable modeling of complex environmental\nindices like UTCI - and significantly outperforms the state-of-the-art\napproximation in both accuracy and efficiency.", "AI": {"tldr": "The paper introduces data-driven modeling using symbolic and sparse regression with orthogonal polynomial bases (e.g., Legendre polynomials) to approximate UTCI, achieving better accuracy and efficiency than existing methods.", "motivation": "To address the nonlinear, multivariate nature of UTCI by developing interpretable and efficient models that outperform current benchmarks.", "method": "Symbolic and sparse regression techniques with orthogonal polynomial bases, focusing on Legendre polynomials for stability and interpretability.", "result": "Models achieve lower root-mean squared losses than the sixth-degree polynomial benchmark, generalize well with minimal training data, and populate a Pareto front of accuracy vs. complexity.", "conclusion": "Combining sparsity, orthogonality, and symbolic structure enables robust, interpretable modeling of UTCI, outperforming state-of-the-art methods in accuracy and efficiency."}}
{"id": "2508.11576", "pdf": "https://arxiv.org/pdf/2508.11576", "abs": "https://arxiv.org/abs/2508.11576", "authors": ["Yumeng Shi", "Quanyu Long", "Yin Wu", "Wenya Wang"], "title": "Causality Matters: How Temporal Information Emerges in Video Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Video language models (VideoLMs) have made significant progress in multimodal\nunderstanding. However, temporal understanding, which involves identifying\nevent order, duration, and relationships across time, still remains a core\nchallenge. Prior works emphasize positional encodings (PEs) as a key mechanism\nfor encoding temporal structure. Surprisingly, we find that removing or\nmodifying PEs in video inputs yields minimal degradation in the performance of\ntemporal understanding. In contrast, reversing the frame sequence while\npreserving the original PEs causes a substantial drop. To explain this\nbehavior, we conduct substantial analysis experiments to trace how temporal\ninformation is integrated within the model. We uncover a causal information\npathway: temporal cues are progressively synthesized through inter-frame\nattention, aggregated in the final frame, and subsequently integrated into the\nquery tokens. This emergent mechanism shows that temporal reasoning emerges\nfrom inter-visual token interactions under the constraints of causal attention,\nwhich implicitly encodes temporal structure. Based on these insights, we\npropose two efficiency-oriented strategies: staged cross-modal attention and a\ntemporal exit mechanism for early token truncation. Experiments on two\nbenchmarks validate the effectiveness of both approaches. To the best of our\nknowledge, this is the first work to systematically investigate video temporal\nunderstanding in VideoLMs, offering insights for future model improvement.", "AI": {"tldr": "The paper challenges the importance of positional encodings (PEs) in VideoLMs for temporal understanding, revealing that inter-frame attention and causal pathways are key. It proposes efficiency strategies like staged attention and early token truncation.", "motivation": "To address the gap in temporal understanding in VideoLMs, despite progress in multimodal tasks, and to explore the role of PEs and inter-frame interactions.", "method": "Analyzes the impact of PEs and frame sequence reversal, traces temporal information pathways, and proposes staged cross-modal attention and temporal exit mechanisms.", "result": "Removing/modifying PEs has minimal impact, while reversing frames harms performance. Temporal reasoning emerges from inter-frame attention, leading to efficient strategies.", "conclusion": "Temporal understanding in VideoLMs relies on inter-frame interactions, not PEs. Proposed strategies improve efficiency, offering insights for future model enhancements."}}
{"id": "2508.11312", "pdf": "https://arxiv.org/pdf/2508.11312", "abs": "https://arxiv.org/abs/2508.11312", "authors": ["Ziyi Zeng", "Yun-Hsuan Chen", "Xurong Gao", "Wenyao Zheng", "Hemmings Wu", "Zhoule Zhu", "Jie Yang", "Chengkai Wang", "Lihua Zhong", "Weiwei Cheng", "Mohamad Sawan"], "title": "Repetitive TMS-based Identification of Methamphetamine-Dependent Individuals Using EEG Spectra", "categories": ["q-bio.NC", "cs.LG", "eess.SP"], "comment": "10 pages, 9 figures", "summary": "The impact of repetitive transcranial magnetic stimulation (rTMS) on\nmethamphetamine (METH) users' craving levels is often assessed using\nquestionnaires. This study explores the feasibility of using neural signals to\nobtain more objective results. EEG signals recorded from 20 METH-addicted\nparticipants Before and After rTMS (MBT and MAT) and from 20 healthy\nparticipants (HC) are analyzed. In each EEG paradigm, participants are shown 15\nMETH-related and 15 neutral pictures randomly, and the relative band power\n(RBP) of each EEG sub-band frequency is derived. The average RBP across all 31\nchannels, as well as individual brain regions, is analyzed. Statistically,\nMAT's alpha, beta, and gamma RBPs are more like those of HC compared to MBT, as\nindicated by the power topographies. Utilizing a random forest (RF), the gamma\nRBP is identified as the optimal frequency band for distinguishing between MBT\nand HC with a 90% accuracy. The performance of classifying MAT versus HC is\nlower than that of MBT versus HC, suggesting that the efficacy of rTMS can be\nvalidated using RF with gamma RBP. Furthermore, the gamma RBP recorded by the\nTP10 and CP2 channels dominates the classification task of MBT versus HC when\nreceiving METH-related image cues. The gamma RBP during exposure to\nMETH-related cues can serve as a biomarker for distinguishing between MBT and\nHC and for evaluating the effectiveness of rTMS. Therefore, real-time\nmonitoring of gamma RBP variations holds promise as a parameter for\nimplementing a customized closed-loop neuromodulation system for treating METH\naddiction.", "AI": {"tldr": "The study uses EEG signals to objectively assess rTMS's impact on METH users' cravings, identifying gamma RBP as a key biomarker for distinguishing users from healthy controls and evaluating rTMS efficacy.", "motivation": "To move beyond subjective questionnaires and use neural signals for more objective assessment of rTMS effects on METH cravings.", "method": "EEG signals from 20 METH users (before and after rTMS) and 20 healthy controls were analyzed. Participants viewed METH-related and neutral images, and RBP of EEG sub-bands was calculated. RF classified data.", "result": "Gamma RBP was optimal for distinguishing METH users from controls (90% accuracy). rTMS shifted METH users' RBP closer to healthy controls. TP10 and CP2 channels were key for classification.", "conclusion": "Gamma RBP is a promising biomarker for assessing rTMS efficacy and could enable real-time, closed-loop neuromodulation for METH addiction treatment."}}
{"id": "2508.11591", "pdf": "https://arxiv.org/pdf/2508.11591", "abs": "https://arxiv.org/abs/2508.11591", "authors": ["Durga Joshi", "Chandi Witharana", "Robert Fahey", "Thomas Worthley", "Zhe Zhu", "Diego Cerrai"], "title": "DashCam Video: A complementary low-cost data stream for on-demand forest-infrastructure system monitoring", "categories": ["cs.CV", "cs.ET"], "comment": "35 Pages, 15 figures", "summary": "Our study introduces a novel, low-cost, and reproducible framework for\nreal-time, object-level structural assessment and geolocation of roadside\nvegetation and infrastructure with commonly available but underutilized\ndashboard camera (dashcam) video data. We developed an end-to-end pipeline that\ncombines monocular depth estimation, depth error correction, and geometric\ntriangulation to generate accurate spatial and structural data from\nstreet-level video streams from vehicle-mounted dashcams. Depth maps were first\nestimated using a state-of-the-art monocular depth model, then refined via a\ngradient-boosted regression framework to correct underestimations, particularly\nfor distant objects. The depth correction model achieved strong predictive\nperformance (R2 = 0.92, MAE = 0.31 on transformed scale), significantly\nreducing bias beyond 15 m. Further, object locations were estimated using\nGPS-based triangulation, while object heights were calculated using pin hole\ncamera geometry. Our method was evaluated under varying conditions of camera\nplacement and vehicle speed. Low-speed vehicle with inside camera gave the\nhighest accuracy, with mean geolocation error of 2.83 m, and mean absolute\nerror (MAE) in height estimation of 2.09 m for trees and 0.88 m for poles. To\nthe best of our knowledge, it is the first framework to combine monocular depth\nmodeling, triangulated GPS-based geolocation, and real-time structural\nassessment for urban vegetation and infrastructure using consumer-grade video\ndata. Our approach complements conventional RS methods, such as LiDAR and image\nby offering a fast, real-time, and cost-effective solution for object-level\nmonitoring of vegetation risks and infrastructure exposure, making it\nespecially valuable for utility companies, and urban planners aiming for\nscalable and frequent assessments in dynamic urban environments.", "AI": {"tldr": "A low-cost, reproducible framework using dashcam video for real-time structural assessment and geolocation of roadside objects, combining monocular depth estimation, error correction, and GPS-based triangulation.", "motivation": "To provide a cost-effective, scalable solution for monitoring urban vegetation and infrastructure using underutilized dashcam data, complementing traditional methods like LiDAR.", "method": "Developed an end-to-end pipeline with monocular depth estimation, depth error correction via gradient-boosted regression, and GPS-based triangulation for object geolocation and height estimation.", "result": "Achieved high accuracy (R2 = 0.92, MAE = 0.31 for depth correction; mean geolocation error of 2.83 m, height MAE of 2.09 m for trees, 0.88 m for poles).", "conclusion": "The framework offers a fast, real-time, and cost-effective alternative for urban monitoring, valuable for utility companies and urban planners."}}
{"id": "2508.11603", "pdf": "https://arxiv.org/pdf/2508.11603", "abs": "https://arxiv.org/abs/2508.11603", "authors": ["Zhe Zhu", "Honghua Chen", "Peng Li", "Mingqiang Wei"], "title": "CoreEditor: Consistent 3D Editing via Correspondence-constrained Diffusion", "categories": ["cs.CV"], "comment": null, "summary": "Text-driven 3D editing seeks to modify 3D scenes according to textual\ndescriptions, and most existing approaches tackle this by adapting pre-trained\n2D image editors to multi-view inputs. However, without explicit control over\nmulti-view information exchange, they often fail to maintain cross-view\nconsistency, leading to insufficient edits and blurry details. We introduce\nCoreEditor, a novel framework for consistent text-to-3D editing. The key\ninnovation is a correspondence-constrained attention mechanism that enforces\nprecise interactions between pixels expected to remain consistent throughout\nthe diffusion denoising process. Beyond relying solely on geometric alignment,\nwe further incorporate semantic similarity estimated during denoising, enabling\nmore reliable correspondence modeling and robust multi-view editing. In\naddition, we design a selective editing pipeline that allows users to choose\npreferred results from multiple candidates, offering greater flexibility and\nuser control. Extensive experiments show that CoreEditor produces high-quality,\n3D-consistent edits with sharper details, significantly outperforming prior\nmethods.", "AI": {"tldr": "CoreEditor introduces a correspondence-constrained attention mechanism for consistent text-to-3D editing, outperforming prior methods with sharper details and better cross-view consistency.", "motivation": "Existing methods for text-driven 3D editing often fail to maintain cross-view consistency, leading to insufficient edits and blurry details.", "method": "CoreEditor uses a correspondence-constrained attention mechanism and incorporates semantic similarity during denoising for robust multi-view editing. It also includes a selective editing pipeline for user flexibility.", "result": "CoreEditor produces high-quality, 3D-consistent edits with sharper details, significantly outperforming prior methods.", "conclusion": "CoreEditor advances text-to-3D editing by ensuring cross-view consistency and offering user control, setting a new standard for quality."}}
{"id": "2508.11472", "pdf": "https://arxiv.org/pdf/2508.11472", "abs": "https://arxiv.org/abs/2508.11472", "authors": ["Yang Wang", "Yaxin Zhao", "Xinyu Jiao", "Sihan Xu", "Xiangrui Cai", "Ying Zhang", "Xiaojie Yuan"], "title": "RMSL: Weakly-Supervised Insider Threat Detection with Robust Multi-sphere Learning", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": "15 pages", "summary": "Insider threat detection aims to identify malicious user behavior by\nanalyzing logs that record user interactions. Due to the lack of fine-grained\nbehavior-level annotations, detecting specific behavior-level anomalies within\nuser behavior sequences is challenging. Unsupervised methods face high false\npositive rates and miss rates due to the inherent ambiguity between normal and\nanomalous behaviors. In this work, we instead introduce weak labels of behavior\nsequences, which have lower annotation costs, i.e., the training labels\n(anomalous or normal) are at sequence-level instead of behavior-level, to\nenhance the detection capability for behavior-level anomalies by learning\ndiscriminative features. To achieve this, we propose a novel framework called\nRobust Multi-sphere Learning (RMSL). RMSL uses multiple hyper-spheres to\nrepresent the normal patterns of behaviors. Initially, a one-class classifier\nis constructed as a good anomaly-supervision-free starting point. Building on\nthis, using multiple instance learning and adaptive behavior-level\nself-training debiasing based on model prediction confidence, the framework\nfurther refines hyper-spheres and feature representations using weak\nsequence-level labels. This approach enhances the model's ability to\ndistinguish between normal and anomalous behaviors. Extensive experiments\ndemonstrate that RMSL significantly improves the performance of behavior-level\ninsider threat detection.", "AI": {"tldr": "The paper introduces a framework called Robust Multi-sphere Learning (RMSL) for insider threat detection, leveraging weak sequence-level labels to improve behavior-level anomaly detection.", "motivation": "The challenge lies in detecting behavior-level anomalies due to lack of fine-grained annotations, leading to high false positives and miss rates in unsupervised methods.", "method": "RMSL uses multiple hyper-spheres to model normal behavior patterns, combining one-class classification, multiple instance learning, and adaptive self-training debiasing.", "result": "Experiments show RMSL significantly enhances behavior-level insider threat detection performance.", "conclusion": "RMSL effectively addresses the limitations of unsupervised methods by leveraging weak labels, improving anomaly detection accuracy."}}
{"id": "2508.11624", "pdf": "https://arxiv.org/pdf/2508.11624", "abs": "https://arxiv.org/abs/2508.11624", "authors": ["Niki Foteinopoulou", "Ignas Budvytis", "Stephan Liwicki"], "title": "LoRAtorio: An intrinsic approach to LoRA Skill Composition", "categories": ["cs.CV"], "comment": "32 pages, 17 figures", "summary": "Low-Rank Adaptation (LoRA) has become a widely adopted technique in\ntext-to-image diffusion models, enabling the personalisation of visual concepts\nsuch as characters, styles, and objects. However, existing approaches struggle\nto effectively compose multiple LoRA adapters, particularly in open-ended\nsettings where the number and nature of required skills are not known in\nadvance. In this work, we present LoRAtorio, a novel train-free framework for\nmulti-LoRA composition that leverages intrinsic model behaviour. Our method is\nmotivated by two key observations: (1) LoRA adapters trained on narrow domains\nproduce denoised outputs that diverge from the base model, and (2) when\noperating out-of-distribution, LoRA outputs show behaviour closer to the base\nmodel than when conditioned in distribution. The balance between these two\nobservations allows for exceptional performance in the single LoRA scenario,\nwhich nevertheless deteriorates when multiple LoRAs are loaded. Our method\noperates in the latent space by dividing it into spatial patches and computing\ncosine similarity between each patch's predicted noise and that of the base\nmodel. These similarities are used to construct a spatially-aware weight\nmatrix, which guides a weighted aggregation of LoRA outputs. To address domain\ndrift, we further propose a modification to classifier-free guidance that\nincorporates the base model's unconditional score into the composition. We\nextend this formulation to a dynamic module selection setting, enabling\ninference-time selection of relevant LoRA adapters from a large pool. LoRAtorio\nachieves state-of-the-art performance, showing up to a 1.3% improvement in\nClipScore and a 72.43% win rate in GPT-4V pairwise evaluations, and generalises\neffectively to multiple latent diffusion models.", "AI": {"tldr": "LoRAtorio is a train-free framework for composing multiple LoRA adapters in text-to-image diffusion models, leveraging intrinsic model behavior and spatial-aware weighting to improve performance.", "motivation": "Existing methods struggle with composing multiple LoRA adapters in open-ended settings, where the number and nature of required skills are unknown.", "method": "LoRAtorio divides latent space into patches, computes cosine similarity for noise predictions, and uses a spatially-aware weight matrix for weighted aggregation. It also modifies classifier-free guidance to address domain drift.", "result": "Achieves state-of-the-art performance with up to 1.3% ClipScore improvement and 72.43% win rate in GPT-4V evaluations.", "conclusion": "LoRAtorio effectively generalizes to multiple latent diffusion models and outperforms existing approaches in multi-LoRA composition."}}
{"id": "2508.11503", "pdf": "https://arxiv.org/pdf/2508.11503", "abs": "https://arxiv.org/abs/2508.11503", "authors": ["Andrej Orsula", "Matthieu Geist", "Miguel Olivares-Mendez", "Carol Martinez"], "title": "Sim2Dust: Mastering Dynamic Waypoint Tracking on Granular Media", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "The source code is available at\n  https://github.com/AndrejOrsula/space_robotics_bench", "summary": "Reliable autonomous navigation across the unstructured terrains of distant\nplanetary surfaces is a critical enabler for future space exploration. However,\nthe deployment of learning-based controllers is hindered by the inherent\nsim-to-real gap, particularly for the complex dynamics of wheel interactions\nwith granular media. This work presents a complete sim-to-real framework for\ndeveloping and validating robust control policies for dynamic waypoint tracking\non such challenging surfaces. We leverage massively parallel simulation to\ntrain reinforcement learning agents across a vast distribution of procedurally\ngenerated environments with randomized physics. These policies are then\ntransferred zero-shot to a physical wheeled rover operating in a lunar-analogue\nfacility. Our experiments systematically compare multiple reinforcement\nlearning algorithms and action smoothing filters to identify the most effective\ncombinations for real-world deployment. Crucially, we provide strong empirical\nevidence that agents trained with procedural diversity achieve superior\nzero-shot performance compared to those trained on static scenarios. We also\nanalyze the trade-offs of fine-tuning with high-fidelity particle physics,\nwhich offers minor gains in low-speed precision at a significant computational\ncost. Together, these contributions establish a validated workflow for creating\nreliable learning-based navigation systems, marking a critical step towards\ndeploying autonomous robots in the final frontier.", "AI": {"tldr": "A sim-to-real framework trains reinforcement learning agents for autonomous navigation on planetary terrains, showing superior zero-shot performance with procedural diversity and minor gains from high-fidelity physics.", "motivation": "Enable reliable autonomous navigation on unstructured planetary terrains by bridging the sim-to-real gap for wheeled rovers.", "method": "Train reinforcement learning agents in procedurally generated environments with randomized physics, then transfer policies zero-shot to a physical rover. Compare algorithms and action smoothing filters.", "result": "Agents trained with procedural diversity outperform those in static scenarios. High-fidelity physics offers minor precision gains at high computational cost.", "conclusion": "The framework provides a validated workflow for learning-based navigation, advancing autonomous robot deployment in space exploration."}}
{"id": "2508.11628", "pdf": "https://arxiv.org/pdf/2508.11628", "abs": "https://arxiv.org/abs/2508.11628", "authors": ["Qiang Li", "Shansong Wang", "Mingzhe Hu", "Mojtaba Safari", "Zachary Eidex", "Xiaofeng Yang"], "title": "Is ChatGPT-5 Ready for Mammogram VQA?", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Mammogram visual question answering (VQA) integrates image interpretation\nwith clinical reasoning and has potential to support breast cancer screening.\nWe systematically evaluated the GPT-5 family and GPT-4o model on four public\nmammography datasets (EMBED, InBreast, CMMD, CBIS-DDSM) for BI-RADS assessment,\nabnormality detection, and malignancy classification tasks. GPT-5 consistently\nwas the best performing model but lagged behind both human experts and\ndomain-specific fine-tuned models. On EMBED, GPT-5 achieved the highest scores\namong GPT variants in density (56.8%), distortion (52.5%), mass (64.5%),\ncalcification (63.5%), and malignancy (52.8%) classification. On InBreast, it\nattained 36.9% BI-RADS accuracy, 45.9% abnormality detection, and 35.0%\nmalignancy classification. On CMMD, GPT-5 reached 32.3% abnormality detection\nand 55.0% malignancy accuracy. On CBIS-DDSM, it achieved 69.3% BI-RADS\naccuracy, 66.0% abnormality detection, and 58.2% malignancy accuracy. Compared\nwith human expert estimations, GPT-5 exhibited lower sensitivity (63.5%) and\nspecificity (52.3%). While GPT-5 exhibits promising capabilities for screening\ntasks, its performance remains insufficient for high-stakes clinical imaging\napplications without targeted domain adaptation and optimization. However, the\ntremendous improvements in performance from GPT-4o to GPT-5 show a promising\ntrend in the potential for general large language models (LLMs) to assist with\nmammography VQA tasks.", "AI": {"tldr": "GPT-5 outperforms GPT-4o in mammogram VQA tasks but lags behind human experts and specialized models, showing potential for clinical support with further optimization.", "motivation": "To evaluate the performance of GPT-5 and GPT-4o in mammogram VQA tasks like BI-RADS assessment, abnormality detection, and malignancy classification, aiming to assess their potential for clinical use.", "method": "Systematic evaluation of GPT-5 and GPT-4o on four public mammography datasets (EMBED, InBreast, CMMD, CBIS-DDSM) for various tasks.", "result": "GPT-5 consistently outperformed GPT-4o but was inferior to human experts and fine-tuned models. Performance varied across datasets, with the highest scores in malignancy classification (up to 69.3% accuracy).", "conclusion": "GPT-5 shows promise for mammogram VQA but requires domain-specific optimization for clinical applications. The improvement from GPT-4o to GPT-5 indicates potential for LLMs in this field."}}
{"id": "2508.11630", "pdf": "https://arxiv.org/pdf/2508.11630", "abs": "https://arxiv.org/abs/2508.11630", "authors": ["Yi-Fan Zhang", "Xingyu Lu", "Shukang Yin", "Chaoyou Fu", "Wei Chen", "Xiao Hu", "Bin Wen", "Kaiyu Jiang", "Changyi Liu", "Tianke Zhang", "Haonan Fan", "Kaibing Chen", "Jiankang Chen", "Haojie Ding", "Kaiyu Tang", "Zhang Zhang", "Liang Wang", "Fan Yang", "Tingting Gao", "Guorui Zhou"], "title": "Thyme: Think Beyond Images", "categories": ["cs.CV"], "comment": "Project page: https://thyme-vl.github.io/", "summary": "Following OpenAI's introduction of the ``thinking with images'' concept,\nrecent efforts have explored stimulating the use of visual information in the\nreasoning process to enhance model performance in perception and reasoning\ntasks. However, to the best of our knowledge, no open-source work currently\noffers a feature set as rich as proprietary models (O3), which can perform\ndiverse image manipulations and simultaneously enhance logical reasoning\ncapabilities through code. In this paper, we make a preliminary attempt in this\ndirection by introducing Thyme (Think Beyond Images), a novel paradigm for\nenabling MLLMs to transcend existing ``think with images'' approaches by\nautonomously generating and executing diverse image processing and\ncomputational operations via executable code. This approach not only\nfacilitates a rich, on-the-fly set of image manipulations (e.g., cropping,\nrotation, contrast enhancement) but also allows for mathematical computations,\nall while maintaining high autonomy in deciding when and how to apply these\noperations. We activate this capability through a two-stage training strategy:\nan initial SFT on a curated dataset of 500K samples to teach code generation,\nfollowed by a RL phase to refine decision-making. For the RL stage, we manually\ncollect and design high-resolution question-answer pairs to increase the\nlearning difficulty, and we propose GRPO-ATS (Group Relative Policy\nOptimization with Adaptive Temperature Sampling), an algorithm that applies\ndistinct temperatures to text and code generation to balance reasoning\nexploration with code execution precision. We conduct extensive experimental\nanalysis and ablation studies. Comprehensive evaluations on nearly 20\nbenchmarks show that Thyme yields significant and consistent performance gains,\nparticularly in challenging high-resolution perception and complex reasoning\ntasks.", "AI": {"tldr": "Thyme introduces a novel paradigm for MLLMs to autonomously generate and execute image processing and computational operations via code, outperforming existing methods in perception and reasoning tasks.", "motivation": "To bridge the gap between proprietary models (O3) and open-source work by enabling richer image manipulations and logical reasoning through executable code.", "method": "A two-stage training strategy: SFT on 500K samples for code generation, followed by RL with GRPO-ATS to refine decision-making.", "result": "Significant performance gains on nearly 20 benchmarks, especially in high-resolution perception and complex reasoning.", "conclusion": "Thyme successfully enhances MLLMs' capabilities beyond traditional \"thinking with images\" approaches, demonstrating the effectiveness of autonomous code generation and execution."}}
{"id": "2508.11515", "pdf": "https://arxiv.org/pdf/2508.11515", "abs": "https://arxiv.org/abs/2508.11515", "authors": ["Qipeng Kuang", "V\u00e1clav K\u016fla", "Ond\u0159ej Ku\u017eelka", "Yuanhong Wang", "Yuyi Wang"], "title": "Weighted First Order Model Counting for Two-variable Logic with Axioms on Two Relations", "categories": ["cs.LO", "cs.AI", "03C13, 68T27", "F.4.0"], "comment": "24 pages, 5 figures", "summary": "The Weighted First-Order Model Counting Problem (WFOMC) asks to compute the\nweighted sum of models of a given first-order logic sentence over a given\ndomain. The boundary between fragments for which WFOMC can be computed in\npolynomial time relative to the domain size lies between the two-variable\nfragment ($\\text{FO}^2$) and the three-variable fragment ($\\text{FO}^3$). It is\nknown that WFOMC for \\FOthree{} is $\\mathsf{\\#P_1}$-hard while polynomial-time\nalgorithms exist for computing WFOMC for $\\text{FO}^2$ and $\\text{C}^2$,\npossibly extended by certain axioms such as the linear order axiom, the\nacyclicity axiom, and the connectedness axiom. All existing research has\nconcentrated on extending the fragment with axioms on a single distinguished\nrelation, leaving a gap in understanding the complexity boundary of axioms on\nmultiple relations. In this study, we explore the extension of the two-variable\nfragment by axioms on two relations, presenting both negative and positive\nresults. We show that WFOMC for $\\text{FO}^2$ with two linear order relations\nand $\\text{FO}^2$ with two acyclic relations are $\\mathsf{\\#P_1}$-hard.\nConversely, we provide an algorithm in time polynomial in the domain size for\nWFOMC of $\\text{C}^2$ with a linear order relation, its successor relation and\nanother successor relation.", "AI": {"tldr": "The paper explores the complexity of WFOMC for two-variable fragments with axioms on two relations, showing some cases are #P1-hard while others are polynomial-time solvable.", "motivation": "To understand the complexity boundary of WFOMC when extending the two-variable fragment with axioms on multiple relations, filling a gap in existing research.", "method": "Investigates WFOMC for FO^2 with two linear order relations and two acyclic relations, and provides a polynomial-time algorithm for C^2 with specific relations.", "result": "WFOMC for FO^2 with two linear orders or two acyclic relations is #P1-hard, while C^2 with certain relations is polynomial-time solvable.", "conclusion": "The study clarifies the complexity boundary for WFOMC with multiple-relation axioms, revealing both hard and tractable cases."}}
{"id": "2508.10941", "pdf": "https://arxiv.org/pdf/2508.10941", "abs": "https://arxiv.org/abs/2508.10941", "authors": ["Zhisen Hu", "David S. Johnson", "Aleksei Tiulpin", "Timothy F. Cootes", "Claudia Lindner"], "title": "The Role of Radiographic Knee Alignment in Knee Replacement Outcomes and Opportunities for Artificial Intelligence-Driven Assessment", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Prevalent knee osteoarthritis (OA) imposes substantial burden on health\nsystems with no cure available. Its ultimate treatment is total knee\nreplacement (TKR). Complications from surgery and recovery are difficult to\npredict in advance, and numerous factors may affect them. Radiographic knee\nalignment is one of the key factors that impacts TKR outcomes, affecting\noutcomes such as postoperative pain or function. Recently, artificial\nintelligence (AI) has been introduced to the automatic analysis of knee\nradiographs, for example, to automate knee alignment measurements. Existing\nreview articles tend to focus on knee OA diagnosis and segmentation of bones or\ncartilages in MRI rather than exploring knee alignment biomarkers for TKR\noutcomes and their assessment. In this review, we first examine the current\nscoring protocols for evaluating TKR outcomes and potential knee alignment\nbiomarkers associated with these outcomes. We then discuss existing AI-based\napproaches for generating knee alignment biomarkers from knee radiographs, and\nexplore future directions for knee alignment assessment and TKR outcome\nprediction.", "AI": {"tldr": "The paper reviews knee alignment biomarkers for TKR outcomes, discusses AI-based approaches for alignment measurement, and explores future directions for improving TKR outcome prediction.", "motivation": "The burden of knee OA and unpredictable TKR complications necessitate better predictive tools, with knee alignment being a key factor.", "method": "The review examines scoring protocols for TKR outcomes, identifies knee alignment biomarkers, and evaluates AI-based methods for alignment measurement.", "result": "AI can automate knee alignment analysis, but existing reviews focus more on OA diagnosis than alignment biomarkers for TKR outcomes.", "conclusion": "Future research should prioritize AI-driven knee alignment assessment to enhance TKR outcome prediction."}}
{"id": "2508.10974", "pdf": "https://arxiv.org/pdf/2508.10974", "abs": "https://arxiv.org/abs/2508.10974", "authors": ["Yuxin Cao", "Wei Song", "Derui Wang", "Jingling Xue", "Jin Song Dong"], "title": "Failures to Surface Harmful Contents in Video Large Language Models", "categories": ["cs.MM", "cs.CV"], "comment": "11 pages, 8 figures", "summary": "Video Large Language Models (VideoLLMs) are increasingly deployed on numerous\ncritical applications, where users rely on auto-generated summaries while\ncasually skimming the video stream. We show that this interaction hides a\ncritical safety gap: if harmful content is embedded in a video, either as\nfull-frame inserts or as small corner patches, state-of-the-art VideoLLMs\nrarely mention the harmful content in the output, despite its clear visibility\nto human viewers. A root-cause analysis reveals three compounding design flaws:\n(1) insufficient temporal coverage resulting from the sparse, uniformly spaced\nframe sampling used by most leading VideoLLMs, (2) spatial information loss\nintroduced by aggressive token downsampling within sampled frames, and (3)\nencoder-decoder disconnection, whereby visual cues are only weakly utilized\nduring text generation. Leveraging these insights, we craft three zero-query\nblack-box attacks, aligning with these flaws in the processing pipeline. Our\nlarge-scale evaluation across five leading VideoLLMs shows that the harmfulness\nomission rate exceeds 90% in most cases. Even when harmful content is clearly\npresent in all frames, these models consistently fail to identify it. These\nresults underscore a fundamental vulnerability in current VideoLLMs' designs\nand highlight the urgent need for sampling strategies, token compression, and\ndecoding mechanisms that guarantee semantic coverage rather than speed alone.", "AI": {"tldr": "VideoLLMs often omit harmful content in video summaries due to design flaws like sparse frame sampling, token downsampling, and weak visual cue utilization, leading to a 90% omission rate in attacks.", "motivation": "To expose safety gaps in VideoLLMs where harmful content is ignored in summaries despite being visible to humans.", "method": "Root-cause analysis of three design flaws, followed by crafting zero-query black-box attacks to exploit them.", "result": "Harmfulness omission rates exceed 90% across five leading VideoLLMs, even when harmful content is present in all frames.", "conclusion": "Current VideoLLM designs are fundamentally vulnerable, necessitating improved sampling, token compression, and decoding mechanisms for better semantic coverage."}}
{"id": "2508.11551", "pdf": "https://arxiv.org/pdf/2508.11551", "abs": "https://arxiv.org/abs/2508.11551", "authors": ["Shengzhuang Chen", "Xu Ouyang", "Michael Arthur Leopold Pearce", "Thomas Hartvigsen", "Jonathan Richard Schwarz"], "title": "ADMIRE-BayesOpt: Accelerated Data MIxture RE-weighting for Language Models with Bayesian Optimization", "categories": ["stat.ML", "cs.AI", "cs.LG"], "comment": null, "summary": "Determining the optimal data mixture for large language model training\nremains a challenging problem with an outsized impact on performance. In\npractice, language model developers continue to rely on heuristic exploration\nsince no learning-based approach has emerged as a reliable solution. In this\nwork, we propose to view the selection of training data mixtures as a black-box\nhyperparameter optimization problem, for which Bayesian Optimization is a\nwell-established class of appropriate algorithms. Firstly, we cast data mixture\nlearning as a sequential decision-making problem, in which we aim to find a\nsuitable trade-off between the computational cost of training exploratory\n(proxy-) models and final mixture performance. Secondly, we systematically\nexplore the properties of transferring mixtures learned at a small scale to\nlarger-scale experiments, providing insights and highlighting opportunities for\nresearch at a modest scale. By proposing Multi-fidelity Bayesian Optimization\nas a suitable method in this common scenario, we introduce a natural framework\nto balance experiment cost with model fit, avoiding the risks of overfitting to\nsmaller scales while minimizing the number of experiments at high cost. We\npresent results for pre-training and instruction finetuning across models\nranging from 1 million to 7 billion parameters, varying from simple\narchitectures to state-of-the-art models and benchmarks spanning dozens of\ndatasets. We demonstrate consistently strong results relative to a wide range\nof benchmarks, showingspeed-ups of over 500% in determining the best data\nmixture on our largest experiments relative to recent baselines. In addition,\nwe broaden access to research by sharing ADMIRE IFT Runs, a dataset of 460 full\ntraining & evaluation runs across various model sizes worth over 13,000 GPU\nhours, greatly reducing the cost of conducting research in this area.", "AI": {"tldr": "The paper proposes using Bayesian Optimization for selecting optimal data mixtures in large language model training, achieving significant speed-ups and sharing a dataset to reduce research costs.", "motivation": "Current heuristic methods for data mixture selection are unreliable, prompting the need for a learning-based approach.", "method": "Frames data mixture selection as a black-box hyperparameter optimization problem using Multi-fidelity Bayesian Optimization to balance cost and performance.", "result": "Demonstrates 500% speed-ups in determining optimal mixtures and shares a dataset of 460 training runs to aid research.", "conclusion": "Bayesian Optimization is effective for data mixture selection, offering scalability and efficiency while reducing research costs."}}
{"id": "2508.11584", "pdf": "https://arxiv.org/pdf/2508.11584", "abs": "https://arxiv.org/abs/2508.11584", "authors": ["Jakub \u0141ucki", "Jonathan Becktor", "Georgios Georgakis", "Robert Royce", "Shehryar Khattak"], "title": "Visual Perception Engine: Fast and Flexible Multi-Head Inference for Robotic Vision Tasks", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "comment": "6 pages, 6 figures, 2 tables", "summary": "Deploying multiple machine learning models on resource-constrained robotic\nplatforms for different perception tasks often results in redundant\ncomputations, large memory footprints, and complex integration challenges. In\nresponse, this work presents Visual Perception Engine (VPEngine), a modular\nframework designed to enable efficient GPU usage for visual multitasking while\nmaintaining extensibility and developer accessibility. Our framework\narchitecture leverages a shared foundation model backbone that extracts image\nrepresentations, which are efficiently shared, without any unnecessary GPU-CPU\nmemory transfers, across multiple specialized task-specific model heads running\nin parallel. This design eliminates the computational redundancy inherent in\nfeature extraction component when deploying traditional sequential models while\nenabling dynamic task prioritization based on application demands. We\ndemonstrate our framework's capabilities through an example implementation\nusing DINOv2 as the foundation model with multiple task (depth, object\ndetection and semantic segmentation) heads, achieving up to 3x speedup compared\nto sequential execution. Building on CUDA Multi-Process Service (MPS), VPEngine\noffers efficient GPU utilization and maintains a constant memory footprint\nwhile allowing per-task inference frequencies to be adjusted dynamically during\nruntime. The framework is written in Python and is open source with ROS2 C++\n(Humble) bindings for ease of use by the robotics community across diverse\nrobotic platforms. Our example implementation demonstrates end-to-end real-time\nperformance at $\\geq$50 Hz on NVIDIA Jetson Orin AGX for TensorRT optimized\nmodels.", "AI": {"tldr": "VPEngine is a modular framework for efficient GPU usage in visual multitasking on robots, reducing redundancy and enabling dynamic task prioritization.", "motivation": "Addressing the inefficiencies of deploying multiple ML models on resource-constrained robotic platforms, such as redundant computations and large memory footprints.", "method": "Uses a shared foundation model backbone (e.g., DINOv2) to extract image representations, shared across parallel task-specific heads, avoiding GPU-CPU transfers. Built on CUDA MPS for efficient GPU use.", "result": "Achieves up to 3x speedup over sequential execution, maintains constant memory footprint, and supports dynamic task prioritization. Demonstrated real-time performance at \u226550 Hz on NVIDIA Jetson Orin AGX.", "conclusion": "VPEngine offers an efficient, extensible, and accessible solution for visual multitasking in robotics, with open-source Python and ROS2 C++ support."}}
{"id": "2508.11049", "pdf": "https://arxiv.org/pdf/2508.11049", "abs": "https://arxiv.org/abs/2508.11049", "authors": ["Kelin Yu", "Sheng Zhang", "Harshit Soora", "Furong Huang", "Heng Huang", "Pratap Tokekar", "Ruohan Gao"], "title": "GenFlowRL: Shaping Rewards with Generative Object-Centric Flow in Visual Reinforcement Learning", "categories": ["cs.RO", "cs.CV"], "comment": "Published at ICCV 2025", "summary": "Recent advances have shown that video generation models can enhance robot\nlearning by deriving effective robot actions through inverse dynamics. However,\nthese methods heavily depend on the quality of generated data and struggle with\nfine-grained manipulation due to the lack of environment feedback. While\nvideo-based reinforcement learning improves policy robustness, it remains\nconstrained by the uncertainty of video generation and the challenges of\ncollecting large-scale robot datasets for training diffusion models. To address\nthese limitations, we propose GenFlowRL, which derives shaped rewards from\ngenerated flow trained from diverse cross-embodiment datasets. This enables\nlearning generalizable and robust policies from diverse demonstrations using\nlow-dimensional, object-centric features. Experiments on 10 manipulation tasks,\nboth in simulation and real-world cross-embodiment evaluations, demonstrate\nthat GenFlowRL effectively leverages manipulation features extracted from\ngenerated object-centric flow, consistently achieving superior performance\nacross diverse and challenging scenarios. Our Project Page:\nhttps://colinyu1.github.io/genflowrl", "AI": {"tldr": "GenFlowRL introduces shaped rewards from generated flow to improve robot learning, overcoming limitations of video generation and dataset constraints.", "motivation": "Addressing the dependency on video generation quality and the challenges of large-scale robot dataset collection for training.", "method": "Uses generated flow trained from diverse cross-embodiment datasets to derive shaped rewards, focusing on low-dimensional, object-centric features.", "result": "Demonstrates superior performance in 10 manipulation tasks in simulation and real-world cross-embodiment evaluations.", "conclusion": "GenFlowRL effectively leverages object-centric flow for robust and generalizable policy learning."}}
{"id": "2508.11511", "pdf": "https://arxiv.org/pdf/2508.11511", "abs": "https://arxiv.org/abs/2508.11511", "authors": ["Siyamalan Manivannan"], "title": "Semi-Supervised Learning with Online Knowledge Distillation for Skin Lesion Classification", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": null, "summary": "Deep Learning has emerged as a promising approach for skin lesion analysis.\nHowever, existing methods mostly rely on fully supervised learning, requiring\nextensive labeled data, which is challenging and costly to obtain. To alleviate\nthis annotation burden, this study introduces a novel semi-supervised deep\nlearning approach that integrates ensemble learning with online knowledge\ndistillation for enhanced skin lesion classification. Our methodology involves\ntraining an ensemble of convolutional neural network models, using online\nknowledge distillation to transfer insights from the ensemble to its members.\nThis process aims to enhance the performance of each model within the ensemble,\nthereby elevating the overall performance of the ensemble itself.\nPost-training, any individual model within the ensemble can be deployed at test\ntime, as each member is trained to deliver comparable performance to the\nensemble. This is particularly beneficial in resource-constrained environments.\nExperimental results demonstrate that the knowledge-distilled individual model\nperforms better than independently trained models. Our approach demonstrates\nsuperior performance on both the \\emph{International Skin Imaging\nCollaboration} 2018 and 2019 public benchmark datasets, surpassing current\nstate-of-the-art results. By leveraging ensemble learning and online knowledge\ndistillation, our method reduces the need for extensive labeled data while\nproviding a more resource-efficient solution for skin lesion classification in\nreal-world scenarios.", "AI": {"tldr": "A semi-supervised deep learning method using ensemble learning and online knowledge distillation improves skin lesion classification with less labeled data.", "motivation": "To reduce the annotation burden in skin lesion analysis by minimizing reliance on extensive labeled data.", "method": "Trains an ensemble of CNNs with online knowledge distillation to enhance individual model performance.", "result": "Outperforms state-of-the-art on ISIC 2018 and 2019 datasets, with knowledge-distilled models surpassing independently trained ones.", "conclusion": "The approach offers a resource-efficient solution for skin lesion classification, reducing labeled data dependency."}}
{"id": "2508.11599", "pdf": "https://arxiv.org/pdf/2508.11599", "abs": "https://arxiv.org/abs/2508.11599", "authors": ["Zhihao Li", "Zimo Ji", "Tao Zheng", "Hao Ren", "Xiao Lan"], "title": "CryptoScope: Utilizing Large Language Models for Automated Cryptographic Logic Vulnerability Detection", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Cryptographic algorithms are fundamental to modern security, yet their\nimplementations frequently harbor subtle logic flaws that are hard to detect.\nWe introduce CryptoScope, a novel framework for automated cryptographic\nvulnerability detection powered by Large Language Models (LLMs). CryptoScope\ncombines Chain-of-Thought (CoT) prompting with Retrieval-Augmented Generation\n(RAG), guided by a curated cryptographic knowledge base containing over 12,000\nentries. We evaluate CryptoScope on LLM-CLVA, a benchmark of 92 cases primarily\nderived from real-world CVE vulnerabilities, complemented by cryptographic\nchallenges from major Capture The Flag (CTF) competitions and synthetic\nexamples across 11 programming languages. CryptoScope consistently improves\nperformance over strong LLM baselines, boosting DeepSeek-V3 by 11.62%,\nGPT-4o-mini by 20.28%, and GLM-4-Flash by 28.69%. Additionally, it identifies 9\npreviously undisclosed flaws in widely used open-source cryptographic projects.", "AI": {"tldr": "CryptoScope is a new framework using LLMs to detect cryptographic vulnerabilities, outperforming baselines and finding undisclosed flaws.", "motivation": "Cryptographic implementations often have hard-to-detect logic flaws, necessitating automated tools for vulnerability detection.", "method": "CryptoScope combines Chain-of-Thought prompting and Retrieval-Augmented Generation, leveraging a 12,000-entry cryptographic knowledge base.", "result": "It improves performance over baselines (e.g., 28.69% boost for GLM-4-Flash) and identifies 9 new flaws in open-source projects.", "conclusion": "CryptoScope effectively enhances cryptographic vulnerability detection using LLMs and curated knowledge."}}
{"id": "2508.11609", "pdf": "https://arxiv.org/pdf/2508.11609", "abs": "https://arxiv.org/abs/2508.11609", "authors": ["Kemal Altwlkany", "Elmedin Selmanovic", "Sead Delalic"], "title": "Pretrained Conformers for Audio Fingerprinting and Retrieval", "categories": ["cs.SD", "cs.AI", "cs.IR", "eess.AS"], "comment": null, "summary": "Conformers have shown great results in speech processing due to their ability\nto capture both local and global interactions. In this work, we utilize a\nself-supervised contrastive learning framework to train conformer-based\nencoders that are capable of generating unique embeddings for small segments of\naudio, generalizing well to previously unseen data. We achieve state-of-the-art\nresults for audio retrieval tasks while using only 3 seconds of audio to\ngenerate embeddings. Our models are almost completely immune to temporal\nmisalignments and achieve state-of-the-art results in cases of other audio\ndistortions such as noise, reverb or extreme temporal stretching. Code and\nmodels are made publicly available and the results are easy to reproduce as we\ntrain and test using popular and freely available datasets of different sizes.", "AI": {"tldr": "A self-supervised contrastive learning framework trains conformer-based encoders for unique audio embeddings, achieving state-of-the-art results in audio retrieval and robustness to distortions.", "motivation": "To improve audio retrieval and robustness by leveraging conformers' ability to capture local and global interactions in audio.", "method": "Utilizes a self-supervised contrastive learning framework with conformer-based encoders to generate unique embeddings for small audio segments.", "result": "State-of-the-art performance in audio retrieval using only 3-second segments, with high immunity to temporal misalignments and distortions like noise, reverb, and stretching.", "conclusion": "The approach generalizes well to unseen data, is reproducible, and sets new benchmarks in audio retrieval tasks."}}
{"id": "2508.11211", "pdf": "https://arxiv.org/pdf/2508.11211", "abs": "https://arxiv.org/abs/2508.11211", "authors": ["Zhenhao Li", "Long Yang", "Xiaojie Yin", "Haijun Yu", "Jiazhou Wang", "Hongbin Han", "Weigang Hu", "Yixing Huang"], "title": "Efficient Image-to-Image Schr\u00f6dinger Bridge for CT Field of View Extension", "categories": ["eess.IV", "cs.CV"], "comment": "10 pages", "summary": "Computed tomography (CT) is a cornerstone imaging modality for non-invasive,\nhigh-resolution visualization of internal anatomical structures. However, when\nthe scanned object exceeds the scanner's field of view (FOV), projection data\nare truncated, resulting in incomplete reconstructions and pronounced artifacts\nnear FOV boundaries. Conventional reconstruction algorithms struggle to recover\naccurate anatomy from such data, limiting clinical reliability. Deep learning\napproaches have been explored for FOV extension, with diffusion generative\nmodels representing the latest advances in image synthesis. Yet, conventional\ndiffusion models are computationally demanding and slow at inference due to\ntheir iterative sampling process. To address these limitations, we propose an\nefficient CT FOV extension framework based on the image-to-image Schr\\\"odinger\nBridge (I$^2$SB) diffusion model. Unlike traditional diffusion models that\nsynthesize images from pure Gaussian noise, I$^2$SB learns a direct stochastic\nmapping between paired limited-FOV and extended-FOV images. This direct\ncorrespondence yields a more interpretable and traceable generative process,\nenhancing anatomical consistency and structural fidelity in reconstructions.\nI$^2$SB achieves superior quantitative performance, with root-mean-square error\n(RMSE) values of 49.8\\,HU on simulated noisy data and 152.0HU on real data,\noutperforming state-of-the-art diffusion models such as conditional denoising\ndiffusion probabilistic models (cDDPM) and patch-based diffusion methods.\nMoreover, its one-step inference enables reconstruction in just 0.19s per 2D\nslice, representing over a 700-fold speedup compared to cDDPM (135s) and\nsurpassing diffusionGAN (0.58s), the second fastest. This combination of\naccuracy and efficiency makes I$^2$SB highly suitable for real-time or clinical\ndeployment.", "AI": {"tldr": "Proposes an efficient CT FOV extension framework using the I\u00b2SB diffusion model, outperforming existing methods in accuracy and speed.", "motivation": "CT scans often suffer from truncation artifacts when objects exceed the scanner's FOV, limiting clinical reliability. Existing deep learning solutions are slow and computationally intensive.", "method": "Uses the image-to-image Schr\u00f6dinger Bridge (I\u00b2SB) diffusion model for direct mapping between limited-FOV and extended-FOV images, improving anatomical consistency and speed.", "result": "Achieves RMSE values of 49.8 HU (simulated) and 152.0 HU (real data), with a 700-fold speedup (0.19s per slice) compared to cDDPM.", "conclusion": "I\u00b2SB offers a fast, accurate solution for CT FOV extension, suitable for real-time clinical use."}}
{"id": "2508.11588", "pdf": "https://arxiv.org/pdf/2508.11588", "abs": "https://arxiv.org/abs/2508.11588", "authors": ["Benjamin Walt", "Jordan Westphal", "Girish Krishnan"], "title": "Investigating Sensors and Methods in Grasp State Classification in Agricultural Manipulation", "categories": ["cs.RO", "cs.LG"], "comment": null, "summary": "Effective and efficient agricultural manipulation and harvesting depend on\naccurately understanding the current state of the grasp. The agricultural\nenvironment presents unique challenges due to its complexity, clutter, and\nocclusion. Additionally, fruit is physically attached to the plant, requiring\nprecise separation during harvesting. Selecting appropriate sensors and\nmodeling techniques is critical for obtaining reliable feedback and correctly\nidentifying grasp states. This work investigates a set of key sensors, namely\ninertial measurement units (IMUs), infrared (IR) reflectance, tension, tactile\nsensors, and RGB cameras, integrated into a compliant gripper to classify grasp\nstates. We evaluate the individual contribution of each sensor and compare the\nperformance of two widely used classification models: Random Forest and Long\nShort-Term Memory (LSTM) networks. Our results demonstrate that a Random Forest\nclassifier, trained in a controlled lab environment and tested on real cherry\ntomato plants, achieved 100% accuracy in identifying slip, grasp failure, and\nsuccessful picks, marking a substantial improvement over baseline performance.\nFurthermore, we identify a minimal viable sensor combination, namely IMU and\ntension sensors that effectively classifies grasp states. This classifier\nenables the planning of corrective actions based on real-time feedback, thereby\nenhancing the efficiency and reliability of fruit harvesting operations.", "AI": {"tldr": "The paper explores sensor integration and classification models for accurate grasp state identification in agricultural harvesting, achieving 100% accuracy with a Random Forest classifier.", "motivation": "Agricultural environments pose challenges like complexity and occlusion, requiring precise grasp state understanding for efficient manipulation and harvesting.", "method": "Investigates sensors (IMUs, IR, tension, tactile, RGB cameras) in a compliant gripper, comparing Random Forest and LSTM models for grasp state classification.", "result": "Random Forest achieved 100% accuracy in identifying grasp states (slip, failure, success) in real cherry tomato plants, outperforming baselines. IMU and tension sensors were identified as a minimal viable combination.", "conclusion": "The classifier enables real-time corrective actions, improving harvesting efficiency and reliability, with potential for broader agricultural applications."}}
{"id": "2508.11216", "pdf": "https://arxiv.org/pdf/2508.11216", "abs": "https://arxiv.org/abs/2508.11216", "authors": ["Han Zhang", "Xue-Cheng Tai", "Jean-Michel Morel", "Raymond H. Chan"], "title": "Fluid Dynamics and Domain Reconstruction from Noisy Flow Images Using Physics-Informed Neural Networks and Quasi-Conformal Mapping", "categories": ["math.NA", "cs.CV", "cs.NA"], "comment": null, "summary": "Blood flow imaging provides important information for hemodynamic behavior\nwithin the vascular system and plays an essential role in medical diagnosis and\ntreatment planning. However, obtaining high-quality flow images remains a\nsignificant challenge. In this work, we address the problem of denoising flow\nimages that may suffer from artifacts due to short acquisition times or\ndevice-induced errors. We formulate this task as an optimization problem, where\nthe objective is to minimize the discrepancy between the modeled velocity\nfield, constrained to satisfy the Navier-Stokes equations, and the observed\nnoisy velocity data. To solve this problem, we decompose it into two\nsubproblems: a fluid subproblem and a geometry subproblem. The fluid subproblem\nleverages a Physics-Informed Neural Network to reconstruct the velocity field\nfrom noisy observations, assuming a fixed domain. The geometry subproblem aims\nto infer the underlying flow region by optimizing a quasi-conformal mapping\nthat deforms a reference domain. These two subproblems are solved in an\nalternating Gauss-Seidel fashion, iteratively refining both the velocity field\nand the domain. Upon convergence, the framework yields a high-quality\nreconstruction of the flow image. We validate the proposed method through\nexperiments on synthetic flow data in a converging channel geometry under\nvarying levels of Gaussian noise, and on real-like flow data in an aortic\ngeometry with signal-dependent noise. The results demonstrate the effectiveness\nand robustness of the approach. Additionally, ablation studies are conducted to\nassess the influence of key hyperparameters.", "AI": {"tldr": "The paper proposes a method to denoise blood flow images by solving an optimization problem using Physics-Informed Neural Networks and quasi-conformal mapping, validated on synthetic and real-like data.", "motivation": "High-quality blood flow imaging is crucial for medical diagnosis, but artifacts from short acquisition times or device errors pose challenges.", "method": "The task is formulated as an optimization problem, decomposed into fluid and geometry subproblems solved iteratively using Physics-Informed Neural Networks and quasi-conformal mapping.", "result": "Experiments on synthetic and real-like data show effective and robust denoising, with ablation studies assessing hyperparameters.", "conclusion": "The framework successfully reconstructs high-quality flow images, demonstrating its potential for medical applications."}}
{"id": "2508.11597", "pdf": "https://arxiv.org/pdf/2508.11597", "abs": "https://arxiv.org/abs/2508.11597", "authors": ["Arnab Ganguly", "Riten Mitra", "Jinpu Zhou"], "title": "Nonparametric learning of stochastic differential equations from sparse and noisy data", "categories": ["stat.ML", "cs.LG", "math.PR", "stat.ME", "62G05, 62M05, 60H10, 60J60, 46E22, 65C05, 65C35"], "comment": "35 pages, 6 figures", "summary": "The paper proposes a systematic framework for building data-driven stochastic\ndifferential equation (SDE) models from sparse, noisy observations. Unlike\ntraditional parametric approaches, which assume a known functional form for the\ndrift, our goal here is to learn the entire drift function directly from data\nwithout strong structural assumptions, making it especially relevant in\nscientific disciplines where system dynamics are partially understood or highly\ncomplex. We cast the estimation problem as minimization of the penalized\nnegative log-likelihood functional over a reproducing kernel Hilbert space\n(RKHS). In the sparse observation regime, the presence of unobserved trajectory\nsegments makes the SDE likelihood intractable. To address this, we develop an\nExpectation-Maximization (EM) algorithm that employs a novel Sequential Monte\nCarlo (SMC) method to approximate the filtering distribution and generate Monte\nCarlo estimates of the E-step objective. The M-step then reduces to a penalized\nempirical risk minimization problem in the RKHS, whose minimizer is given by a\nfinite linear combination of kernel functions via a generalized representer\ntheorem. To control model complexity across EM iterations, we also develop a\nhybrid Bayesian variant of the algorithm that uses shrinkage priors to identify\nsignificant coefficients in the kernel expansion. We establish important\ntheoretical convergence results for both the exact and approximate EM\nsequences. The resulting EM-SMC-RKHS procedure enables accurate estimation of\nthe drift function of stochastic dynamical systems in low-data regimes and is\nbroadly applicable across domains requiring continuous-time modeling under\nobservational constraints. We demonstrate the effectiveness of our method\nthrough a series of numerical experiments.", "AI": {"tldr": "The paper introduces a framework for learning stochastic differential equation (SDE) drift functions from sparse, noisy data using RKHS and EM-SMC methods, with theoretical guarantees and practical validation.", "motivation": "To address the challenge of learning SDE drift functions without strong structural assumptions, especially in fields with partially understood or complex dynamics.", "method": "Proposes an EM algorithm with SMC for E-step approximation and RKHS-based penalized empirical risk minimization for the M-step, enhanced by Bayesian shrinkage priors.", "result": "The EM-SMC-RKHS method accurately estimates drift functions in low-data regimes, validated by numerical experiments.", "conclusion": "The framework is broadly applicable for continuous-time modeling under observational constraints, offering theoretical and practical advantages."}}
{"id": "2508.11259", "pdf": "https://arxiv.org/pdf/2508.11259", "abs": "https://arxiv.org/abs/2508.11259", "authors": ["Ryosuke Isono", "Shunsuke Ono"], "title": "Temporally-Similar Structure-Aware Spatiotemporal Fusion of Satellite Images", "categories": ["eess.SP", "cs.CV"], "comment": "Submitted to IEEE Transactions on Geoscience and Remote Sensing.\n  arXiv admin note: text overlap with arXiv:2308.00500", "summary": "This paper proposes a novel spatiotemporal (ST) fusion framework for\nsatellite images, named Temporally-Similar Structure-Aware ST fusion (TSSTF).\nST fusion is a promising approach to address the trade-off between the spatial\nand temporal resolution of satellite images. In real-world scenarios, observed\nsatellite images are severely degraded by noise due to measurement equipment\nand environmental conditions. Consequently, some recent studies have focused on\nenhancing the robustness of ST fusion methods against noise. However, existing\nnoise-robust ST fusion approaches often fail to capture fine spatial structure,\nleading to oversmoothing and artifacts. To address this issue, TSSTF introduces\ntwo key mechanisms: Temporally-Guided Total Variation (TGTV) and\nTemporally-Guided Edge Constraint (TGEC). TGTV is a novel regularization\nfunction that promotes spatial piecewise smoothness while preserving structural\ndetails, guided by a reference high spatial resolution image acquired on a\nnearby date. TGEC enforces consistency in edge locations between two temporally\nadjacent images, while allowing for spectral variations. We formulate the ST\nfusion task as a constrained optimization problem incorporating TGTV and TGEC,\nand develop an efficient algorithm based on a preconditioned primal-dual\nsplitting method. Experimental results demonstrate that TSSTF performs\ncomparably to state-of-the-art methods under noise-free conditions and\noutperforms them under noisy conditions. Additionally, we provide a\ncomprehensive set of recommended parameter values that consistently yield high\nperformance across diverse target regions and noise conditions, aiming to\nenhance reproducibility and practical utility.", "AI": {"tldr": "TSSTF is a spatiotemporal fusion framework for satellite images, introducing TGTV and TGEC to enhance robustness against noise while preserving spatial details. It outperforms existing methods under noisy conditions.", "motivation": "Address the trade-off between spatial and temporal resolution in satellite images and improve robustness against noise without sacrificing spatial structure.", "method": "Proposes TGTV for spatial smoothness and TGEC for edge consistency, formulated as a constrained optimization problem solved via a primal-dual splitting method.", "result": "TSSTF matches state-of-the-art in noise-free conditions and excels under noise, with recommended parameters for reproducibility.", "conclusion": "TSSTF effectively balances noise robustness and spatial detail preservation, offering practical utility for satellite image fusion."}}
{"id": "2508.11294", "pdf": "https://arxiv.org/pdf/2508.11294", "abs": "https://arxiv.org/abs/2508.11294", "authors": ["Qiangong Zhou", "Zhiting Wang", "Mingyou Yao", "Zongyang Liu"], "title": "Allen: Rethinking MAS Design through Step-Level Policy Autonomy", "categories": ["cs.MA", "cs.CV"], "comment": null, "summary": "We introduce a new Multi-Agent System (MAS) - Allen, designed to address two\ncore challenges in current MAS design: (1) improve system's policy autonomy,\nempowering agents to dynamically adapt their behavioral strategies, and (2)\nachieving the trade-off between collaborative efficiency, task supervision, and\nhuman oversight in complex network topologies.\n  Our core insight is to redefine the basic execution unit in the MAS, allowing\nagents to autonomously form different patterns by combining these units. We\nhave constructed a four-tier state architecture (Task, Stage, Agent, Step) to\nconstrain system behavior from both task-oriented and execution-oriented\nperspectives. This achieves a unification of topological optimization and\ncontrollable progress.\n  Allen grants unprecedented Policy Autonomy, while making a trade-off for the\ncontrollability of the collaborative structure. The project code has been open\nsource at: https://github.com/motern88/Allen", "AI": {"tldr": "Allen is a Multi-Agent System (MAS) designed to enhance policy autonomy and balance collaborative efficiency, task supervision, and human oversight in complex networks.", "motivation": "Addressing the challenges of improving policy autonomy and achieving a trade-off between collaboration efficiency and controllability in MAS.", "method": "Redefining the basic execution unit in MAS and using a four-tier state architecture (Task, Stage, Agent, Step) to unify topological optimization and progress control.", "result": "Allen achieves unprecedented policy autonomy while balancing collaborative structure controllability.", "conclusion": "Allen successfully addresses key MAS challenges and is open-sourced for further use and development."}}
{"id": "2508.11331", "pdf": "https://arxiv.org/pdf/2508.11331", "abs": "https://arxiv.org/abs/2508.11331", "authors": ["Xinyi Wang", "Smaranda Tasmoc", "Nantheera Anantrasirichai", "Angeliki Katsenou"], "title": "Guiding WaveMamba with Frequency Maps for Image Debanding", "categories": ["eess.IV", "cs.CV"], "comment": "5 pages, 2 figures", "summary": "Compression at low bitrates in modern codecs often introduces banding\nartifacts, especially in smooth regions such as skies. These artifacts degrade\nvisual quality and are common in user-generated content due to repeated\ntranscoding. We propose a banding restoration method that employs the Wavelet\nState Space Model and a frequency masking map to preserve high-frequency\ndetails. Furthermore, we provide a benchmark of open-source banding restoration\nmethods and evaluate their performance on two public banding image datasets.\nExperimentation on the available datasets suggests that the proposed\npost-processing approach effectively suppresses banding compared to the\nstate-of-the-art method (a DBI value of 0.082 on BAND-2k) while preserving\nimage textures. Visual inspections of the results confirm this. Code and\nsupplementary material are available at:\nhttps://github.com/xinyiW915/Debanding-PCS2025.", "AI": {"tldr": "A method using Wavelet State Space Model and frequency masking to reduce banding artifacts in low-bitrate compressed images, outperforming state-of-the-art techniques.", "motivation": "Banding artifacts degrade visual quality in low-bitrate compressed images, especially in smooth regions like skies, and are common in user-generated content due to repeated transcoding.", "method": "Proposes a banding restoration method combining the Wavelet State Space Model and a frequency masking map to preserve high-frequency details.", "result": "Outperforms state-of-the-art methods with a DBI value of 0.082 on BAND-2k, effectively suppressing banding while preserving textures.", "conclusion": "The proposed post-processing approach is effective for banding restoration, validated by visual inspections and benchmark comparisons."}}
{"id": "2508.11375", "pdf": "https://arxiv.org/pdf/2508.11375", "abs": "https://arxiv.org/abs/2508.11375", "authors": ["Zonglin Wu", "Yule Xue", "Qianxiang Hu", "Yaoyao Feng", "Yuqi Ma", "Shanxiong Chen"], "title": "AnatoMaskGAN: GNN-Driven Slice Feature Fusion and Noise Augmentation for Medical Semantic Image Synthesis", "categories": ["eess.IV", "cs.CV", "I.4.9"], "comment": "8 pages", "summary": "Medical semantic-mask synthesis boosts data augmentation and analysis, yet\nmost GAN-based approaches still produce one-to-one images and lack spatial\nconsistency in complex scans. To address this, we propose AnatoMaskGAN, a novel\nsynthesis framework that embeds slice-related spatial features to precisely\naggregate inter-slice contextual dependencies, introduces diverse\nimage-augmentation strategies, and optimizes deep feature learning to improve\nperformance on complex medical images. Specifically, we design a GNN-based\nstrongly correlated slice-feature fusion module to model spatial relationships\nbetween slices and integrate contextual information from neighboring slices,\nthereby capturing anatomical details more comprehensively; we introduce a\nthree-dimensional spatial noise-injection strategy that weights and fuses\nspatial features with noise to enhance modeling of structural diversity; and we\nincorporate a grayscale-texture classifier to optimize grayscale distribution\nand texture representation during generation. Extensive experiments on the\npublic L2R-OASIS and L2R-Abdomen CT datasets show that AnatoMaskGAN raises PSNR\non L2R-OASIS to 26.50 dB (0.43 dB higher than the current state of the art) and\nachieves an SSIM of 0.8602 on L2R-Abdomen CT--a 0.48 percentage-point gain over\nthe best model, demonstrating its superiority in reconstruction accuracy and\nperceptual quality. Ablation studies that successively remove the slice-feature\nfusion module, spatial 3D noise-injection strategy, and grayscale-texture\nclassifier reveal that each component contributes significantly to PSNR, SSIM,\nand LPIPS, further confirming the independent value of each core design in\nenhancing reconstruction accuracy and perceptual quality.", "AI": {"tldr": "AnatoMaskGAN improves medical image synthesis by embedding spatial features, enhancing diversity, and optimizing grayscale-texture, outperforming state-of-the-art methods in reconstruction accuracy and perceptual quality.", "motivation": "Existing GAN-based approaches lack spatial consistency and diversity in complex medical scans, limiting their effectiveness for data augmentation and analysis.", "method": "Proposes AnatoMaskGAN with a GNN-based slice-feature fusion module, 3D spatial noise-injection, and a grayscale-texture classifier to enhance spatial and contextual modeling.", "result": "Achieves PSNR of 26.50 dB on L2R-OASIS and SSIM of 0.8602 on L2R-Abdomen CT, surpassing current benchmarks.", "conclusion": "Each component of AnatoMaskGAN significantly contributes to performance, validating its design for accurate and high-quality medical image synthesis."}}
{"id": "2508.11391", "pdf": "https://arxiv.org/pdf/2508.11391", "abs": "https://arxiv.org/abs/2508.11391", "authors": ["Yinggan Tang", "Quanwei Hu"], "title": "LKFMixer: Exploring Large Kernel Feature For Efficient Image Super-Resolution", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "The success of self-attention (SA) in Transformer demonstrates the importance\nof non-local information to image super-resolution (SR), but the huge computing\npower required makes it difficult to implement lightweight models. To solve\nthis problem, we propose a pure convolutional neural network (CNN) model,\nLKFMixer, which utilizes large convolutional kernel to simulate the ability of\nself-attention to capture non-local features. Specifically, we increase the\nkernel size to 31 to obtain the larger receptive field as possible, and reduce\nthe parameters and computations by coordinate decomposition. Meanwhile, a\nspatial feature modulation block (SFMB) is designed to enhance the focus of\nfeature information on both spatial and channel dimension. In addition, by\nintroducing feature selection block (FSB), the model can adaptively adjust the\nweights between local features and non-local features. Extensive experiments\nshow that the proposed LKFMixer family outperform other state-of-the-art (SOTA)\nmethods in terms of SR performance and reconstruction quality. In particular,\ncompared with SwinIR-light on Manga109 dataset, LKFMixer-L achieves 0.6dB PSNR\nimprovement at $\\times$4 scale, while the inference speed is $\\times$5 times\nfaster. The code is available at https://github.com/Supereeeee/LKFMixer.", "AI": {"tldr": "LKFMixer, a CNN model with large kernels, mimics self-attention for non-local feature capture in image super-resolution, outperforming SOTA methods in performance and speed.", "motivation": "Self-attention in Transformers is effective for non-local feature capture in SR but is computationally heavy. LKFMixer aims to replicate this capability using lightweight CNN.", "method": "Uses large 31x31 kernels for wide receptive fields, reduces computation via coordinate decomposition, and employs SFMB and FSB for spatial-channel focus and adaptive feature weighting.", "result": "LKFMixer outperforms SOTA methods, e.g., 0.6dB PSNR gain over SwinIR-light on Manga109 at 4x scale, with 5x faster inference.", "conclusion": "LKFMixer successfully balances SR performance and efficiency, proving large-kernel CNNs can effectively replace self-attention in lightweight models."}}
{"id": "2508.11450", "pdf": "https://arxiv.org/pdf/2508.11450", "abs": "https://arxiv.org/abs/2508.11450", "authors": ["Augustine X. W. Lee", "Pak-Hei Yeung", "Jagath C. Rajapakse"], "title": "Subcortical Masks Generation in CT Images via Ensemble-Based Cross-Domain Label Transfer", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Subcortical segmentation in neuroimages plays an important role in\nunderstanding brain anatomy and facilitating computer-aided diagnosis of\ntraumatic brain injuries and neurodegenerative disorders. However, training\naccurate automatic models requires large amounts of labelled data. Despite the\navailability of publicly available subcortical segmentation datasets for\nMagnetic Resonance Imaging (MRI), a significant gap exists for Computed\nTomography (CT). This paper proposes an automatic ensemble framework to\ngenerate high-quality subcortical segmentation labels for CT scans by\nleveraging existing MRI-based models. We introduce a robust ensembling pipeline\nto integrate them and apply it to unannotated paired MRI-CT data, resulting in\na comprehensive CT subcortical segmentation dataset. Extensive experiments on\nmultiple public datasets demonstrate the superior performance of our proposed\nframework. Furthermore, using our generated CT dataset, we train segmentation\nmodels that achieve improved performance on related segmentation tasks. To\nfacilitate future research, we make our source code, generated dataset, and\ntrained models publicly available at\nhttps://github.com/SCSE-Biomedical-Computing-Group/CT-Subcortical-Segmentation,\nmarking the first open-source release for CT subcortical segmentation to the\nbest of our knowledge.", "AI": {"tldr": "An automatic ensemble framework is proposed to generate high-quality subcortical segmentation labels for CT scans by leveraging MRI-based models, addressing the lack of labeled CT data.", "motivation": "The lack of labeled CT data for subcortical segmentation hinders research and diagnosis. Leveraging existing MRI-based models can bridge this gap.", "method": "An ensemble framework integrates MRI-based models and applies them to unannotated paired MRI-CT data to create a CT segmentation dataset.", "result": "The framework outperforms others on public datasets, and models trained on the generated dataset show improved performance.", "conclusion": "The proposed framework and released resources advance CT subcortical segmentation research."}}
{"id": "2508.11476", "pdf": "https://arxiv.org/pdf/2508.11476", "abs": "https://arxiv.org/abs/2508.11476", "authors": ["Qian Liang", "Zichong Chen", "Yang Zhou", "Hui Huang"], "title": "SPG: Style-Prompting Guidance for Style-Specific Content Creation", "categories": ["cs.GR", "cs.CV"], "comment": "Accepted to the Journal track of Pacific Graphics 2025", "summary": "Although recent text-to-image (T2I) diffusion models excel at aligning\ngenerated images with textual prompts, controlling the visual style of the\noutput remains a challenging task. In this work, we propose Style-Prompting\nGuidance (SPG), a novel sampling strategy for style-specific image generation.\nSPG constructs a style noise vector and leverages its directional deviation\nfrom unconditional noise to guide the diffusion process toward the target style\ndistribution. By integrating SPG with Classifier-Free Guidance (CFG), our\nmethod achieves both semantic fidelity and style consistency. SPG is simple,\nrobust, and compatible with controllable frameworks like ControlNet and\nIPAdapter, making it practical and widely applicable. Extensive experiments\ndemonstrate the effectiveness and generality of our approach compared to\nstate-of-the-art methods. Code is available at\nhttps://github.com/Rumbling281441/SPG.", "AI": {"tldr": "The paper introduces Style-Prompting Guidance (SPG), a novel sampling strategy for style-specific image generation in text-to-image diffusion models, ensuring both semantic fidelity and style consistency.", "motivation": "Controlling the visual style of generated images in text-to-image diffusion models remains challenging, prompting the need for a robust and practical solution.", "method": "SPG constructs a style noise vector and uses its directional deviation from unconditional noise to guide the diffusion process toward the target style distribution, integrating with Classifier-Free Guidance (CFG).", "result": "SPG achieves semantic fidelity and style consistency, is compatible with frameworks like ControlNet and IPAdapter, and outperforms state-of-the-art methods.", "conclusion": "SPG is a simple, robust, and widely applicable method for style-specific image generation, demonstrated through extensive experiments."}}
{"id": "2508.11492", "pdf": "https://arxiv.org/pdf/2508.11492", "abs": "https://arxiv.org/abs/2508.11492", "authors": ["Bozhou Zhang", "Nan Song", "Bingzhao Gao", "Li Zhang"], "title": "Relative Position Matters: Trajectory Prediction and Planning with Polar Representation", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Trajectory prediction and planning in autonomous driving are highly\nchallenging due to the complexity of predicting surrounding agents' movements\nand planning the ego agent's actions in dynamic environments. Existing methods\nencode map and agent positions and decode future trajectories in Cartesian\ncoordinates. However, modeling the relationships between the ego vehicle and\nsurrounding traffic elements in Cartesian space can be suboptimal, as it does\nnot naturally capture the varying influence of different elements based on\ntheir relative distances and directions. To address this limitation, we adopt\nthe Polar coordinate system, where positions are represented by radius and\nangle. This representation provides a more intuitive and effective way to model\nspatial changes and relative relationships, especially in terms of distance and\ndirectional influence. Based on this insight, we propose Polaris, a novel\nmethod that operates entirely in Polar coordinates, distinguishing itself from\nconventional Cartesian-based approaches. By leveraging the Polar\nrepresentation, this method explicitly models distance and direction variations\nand captures relative relationships through dedicated encoding and refinement\nmodules, enabling more structured and spatially aware trajectory prediction and\nplanning. Extensive experiments on the challenging prediction (Argoverse 2) and\nplanning benchmarks (nuPlan) demonstrate that Polaris achieves state-of-the-art\nperformance.", "AI": {"tldr": "Polaris introduces a Polar coordinate-based method for autonomous driving trajectory prediction and planning, outperforming Cartesian-based approaches by better modeling spatial relationships.", "motivation": "Existing Cartesian-based methods inadequately capture the influence of surrounding agents' relative distances and directions, limiting prediction and planning accuracy.", "method": "Polaris operates entirely in Polar coordinates, using radius and angle to model spatial changes and relative relationships, with dedicated encoding and refinement modules.", "result": "Polaris achieves state-of-the-art performance on Argoverse 2 and nuPlan benchmarks.", "conclusion": "The Polar coordinate system provides a more intuitive and effective framework for trajectory prediction and planning in autonomous driving."}}
