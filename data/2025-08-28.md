<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 89]
- [cs.AI](#cs.AI) [Total: 17]
- [cs.CL](#cs.CL) [Total: 60]
- [cs.LG](#cs.LG) [Total: 72]
- [cs.RO](#cs.RO) [Total: 5]
- [cs.DB](#cs.DB) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.IR](#cs.IR) [Total: 2]
- [physics.optics](#physics.optics) [Total: 3]
- [cs.CR](#cs.CR) [Total: 20]
- [cs.CY](#cs.CY) [Total: 6]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [econ.GN](#econ.GN) [Total: 1]
- [cs.DC](#cs.DC) [Total: 4]
- [cs.AR](#cs.AR) [Total: 1]
- [quant-ph](#quant-ph) [Total: 2]
- [cs.SE](#cs.SE) [Total: 3]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.GR](#cs.GR) [Total: 1]
- [eess.SY](#eess.SY) [Total: 1]
- [math.CO](#math.CO) [Total: 1]
- [cs.HC](#cs.HC) [Total: 6]
- [eess.SP](#eess.SP) [Total: 5]
- [math.OC](#math.OC) [Total: 1]
- [cs.SD](#cs.SD) [Total: 3]
- [cs.GT](#cs.GT) [Total: 1]
- [nucl-th](#nucl-th) [Total: 1]
- [stat.ML](#stat.ML) [Total: 3]
- [eess.IV](#eess.IV) [Total: 5]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Real-Time Intuitive AI Drawing System for Collaboration: Enhancing Human Creativity through Formal and Contextual Intent Integration](https://arxiv.org/abs/2508.19254)
*Jookyung Song,Mookyoung Kang,Nojun Kwak*

Main category: cs.CV

TL;DR: Real-time generative drawing system that combines formal sketch structure with contextual semantic meaning for collaborative AI-human co-creation.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of text-prompt-based generative systems by capturing both geometric features and semantic meaning from sketches, enabling more intuitive human-AI collaboration in visual creation.

Method: Multi-stage generation pipeline that jointly conditions dual intent signals (structural/geometric features and semantic cues from vision-language models) with contour-preserving structural control and style/content-aware image synthesis.

Result: Low-latency, two-stage transformation system with touchscreen interface and distributed inference architecture that supports multi-user collaboration on shared canvases.

Conclusion: The system redefines human-AI interaction as co-creation and mutual enhancement, enabling synchronous visual creation regardless of artistic expertise.

Abstract: This paper presents a real-time generative drawing system that interprets and
integrates both formal intent - the structural, compositional, and stylistic
attributes of a sketch - and contextual intent - the semantic and thematic
meaning inferred from its visual content - into a unified transformation
process. Unlike conventional text-prompt-based generative systems, which
primarily capture high-level contextual descriptions, our approach
simultaneously analyzes ground-level intuitive geometric features such as line
trajectories, proportions, and spatial arrangement, and high-level semantic
cues extracted via vision-language models. These dual intent signals are
jointly conditioned in a multi-stage generation pipeline that combines
contour-preserving structural control with style- and content-aware image
synthesis. Implemented with a touchscreen-based interface and distributed
inference architecture, the system achieves low-latency, two-stage
transformation while supporting multi-user collaboration on shared canvases.
The resulting platform enables participants, regardless of artistic expertise,
to engage in synchronous, co-authored visual creation, redefining human-AI
interaction as a process of co-creation and mutual enhancement.

</details>


### [2] [TTF-VLA: Temporal Token Fusion via Pixel-Attention Integration for Vision-Language-Action Models](https://arxiv.org/abs/2508.19257)
*Chenghao Liu,Jiachen Zhang,Chengxuan Li,Zhimu Zhou,Shixin Wu,Songfang Huang,Huiling Duan*

Main category: cs.CV

TL;DR: TTF is a training-free method that integrates temporal visual information in VLA models using dual-dimension detection and selective fusion, improving performance across benchmarks and real robot tasks.


<details>
  <summary>Details</summary>
Motivation: Current VLA models process visual inputs frame-by-frame, discarding valuable temporal information and making them vulnerable to visual noise while ignoring coherence between consecutive frames in manipulation tasks.

Method: Temporal Token Fusion (TTF) employs dual-dimension detection (grayscale pixel difference + attention-based semantic relevance) with hard fusion strategies and keyframe anchoring to selectively integrate historical and current visual representations without additional training.

Result: Consistent improvements across benchmarks: 4.0pp average on LIBERO (72.4% vs 68.4% baseline), 4.8% relative improvement on SimplerEnv, and 8.7% relative improvement on real robot tasks. Model-agnostic across OpenVLA and VLA-Cache architectures.

Conclusion: TTF demonstrates that selective temporal integration enhances VLA performance, revealing that Query matrix reuse in attention mechanisms improves rather than compromises performance, suggesting promising directions for computational acceleration while improving task success.

Abstract: Vision-Language-Action (VLA) models process visual inputs independently at
each timestep, discarding valuable temporal information inherent in robotic
manipulation tasks. This frame-by-frame processing makes models vulnerable to
visual noise while ignoring the substantial coherence between consecutive
frames in manipulation sequences. We propose Temporal Token Fusion (TTF), a
training-free approach that intelligently integrates historical and current
visual representations to enhance VLA inference quality. Our method employs
dual-dimension detection combining efficient grayscale pixel difference
analysis with attention-based semantic relevance assessment, enabling selective
temporal token fusion through hard fusion strategies and keyframe anchoring to
prevent error accumulation. Comprehensive experiments across LIBERO,
SimplerEnv, and real robot tasks demonstrate consistent improvements: 4.0
percentage points average on LIBERO (72.4\% vs 68.4\% baseline),
cross-environment validation on SimplerEnv (4.8\% relative improvement), and
8.7\% relative improvement on real robot tasks. Our approach proves
model-agnostic, working across OpenVLA and VLA-Cache architectures. Notably,
TTF reveals that selective Query matrix reuse in attention mechanisms enhances
rather than compromises performance, suggesting promising directions for direct
KQV matrix reuse strategies that achieve computational acceleration while
improving task success rates.

</details>


### [3] [Seeing Like a Designer Without One: A Study on Unsupervised Slide Quality Assessment via Designer Cue Augmentation](https://arxiv.org/abs/2508.19289)
*Tai Inui,Steven Oh,Magdeline Kuan*

Main category: cs.CV

TL;DR: Unsupervised slide-quality assessment pipeline combining expert visual metrics with CLIP embeddings achieves strong correlation (up to 0.83) with human ratings, outperforming leading vision-language models.


<details>
  <summary>Details</summary>
Motivation: To provide scalable, objective feedback on presentation slide quality by approximating audience perceptions through automated assessment.

Method: Combines seven expert-inspired visual metrics (whitespace, colorfulness, edge density, brightness contrast, text density, color harmony, layout balance) with CLIP-ViT embeddings, using Isolation Forest-based anomaly scoring. Trained on 12k professional lecture slides.

Result: Achieved Pearson correlations up to 0.83 with human visual-quality ratings (1.79x to 3.23x stronger than leading vision-language models). Demonstrated convergent validity with visual ratings and discriminant validity against speaker-delivery scores.

Conclusion: Augmenting low-level design cues with multimodal embeddings closely approximates audience perceptions of slide quality, enabling real-time scalable feedback.

Abstract: We present an unsupervised slide-quality assessment pipeline that combines
seven expert-inspired visual-design metrics (whitespace, colorfulness, edge
density, brightness contrast, text density, color harmony, layout balance) with
CLIP-ViT embeddings, using Isolation Forest-based anomaly scoring to evaluate
presentation slides. Trained on 12k professional lecture slides and evaluated
on six academic talks (115 slides), our method achieved Pearson correlations up
to 0.83 with human visual-quality ratings-1.79x to 3.23x stronger than scores
from leading vision-language models (ChatGPT o4-mini-high, ChatGPT o3, Claude
Sonnet 4, Gemini 2.5 Pro). We demonstrate convergent validity with visual
ratings, discriminant validity against speaker-delivery scores, and exploratory
alignment with overall impressions. Our results show that augmenting low-level
design cues with multimodal embeddings closely approximates audience
perceptions of slide quality, enabling scalable, objective feedback in real
time.

</details>


### [4] [Efficient Model-Based Purification Against Adversarial Attacks for LiDAR Segmentation](https://arxiv.org/abs/2508.19290)
*Alexandros Gkillas,Ioulia Kapsali,Nikos Piperigkos,Aris S. Lalos*

Main category: cs.CV

TL;DR: Efficient adversarial defense framework for 2D range-view LiDAR segmentation that provides strong protection with minimal computational overhead, outperforming existing methods in both benchmarks and real-world deployment.


<details>
  <summary>Details</summary>
Motivation: LiDAR segmentation is critical for autonomous vehicle safety but vulnerable to adversarial attacks. Most existing defenses are designed for 3D point clouds and are computationally intensive, while efficient 2D range-view representations lack dedicated lightweight defenses.

Method: Proposes a model-based purification framework with direct attack formulation in range-view domain and an explainable purification network based on mathematically justified optimization problem.

Result: Achieves competitive performance on open benchmarks, consistently outperforming generative and adversarial training baselines. Real-world deployment demonstrates accurate operation in practical autonomous driving scenarios.

Conclusion: The framework provides efficient and effective adversarial defense for 2D range-view LiDAR segmentation with minimal computational overhead, making it suitable for practical autonomous driving applications.

Abstract: LiDAR-based segmentation is essential for reliable perception in autonomous
vehicles, yet modern segmentation networks are highly susceptible to
adversarial attacks that can compromise safety. Most existing defenses are
designed for networks operating directly on raw 3D point clouds and rely on
large, computationally intensive generative models. However, many
state-of-the-art LiDAR segmentation pipelines operate on more efficient 2D
range view representations. Despite their widespread adoption, dedicated
lightweight adversarial defenses for this domain remain largely unexplored. We
introduce an efficient model-based purification framework tailored for
adversarial defense in 2D range-view LiDAR segmentation. We propose a direct
attack formulation in the range-view domain and develop an explainable
purification network based on a mathematical justified optimization problem,
achieving strong adversarial resilience with minimal computational overhead.
Our method achieves competitive performance on open benchmarks, consistently
outperforming generative and adversarial training baselines. More importantly,
real-world deployment on a demo vehicle demonstrates the framework's ability to
deliver accurate operation in practical autonomous driving scenarios.

</details>


### [5] [Object Detection with Multimodal Large Vision-Language Models: An In-depth Review](https://arxiv.org/abs/2508.19294)
*Ranjan Sapkota,Manoj Karkee*

Main category: cs.CV

TL;DR: This paper provides a comprehensive review of Large Vision-Language Models (LVLMs) for object detection, highlighting their revolutionary impact through enhanced adaptability, contextual reasoning, and generalization capabilities compared to traditional deep learning approaches.


<details>
  <summary>Details</summary>
Motivation: To systematically explore and analyze the state-of-the-art in LVLMs for object detection, examining how the fusion of language and vision capabilities has transformed object detection by enabling better contextual understanding and more sophisticated localization strategies.

Method: The review follows a three-step research process: 1) discussing how VLMs function for object detection using NLP and CV techniques, 2) explaining architectural innovations and training paradigms of recent LVLMs, and 3) examining approaches for visual-textual information integration with comprehensive visualizations and performance comparisons.

Result: LVLMs demonstrate effectiveness in diverse scenarios including localization and segmentation, showing promising real-time performance and adaptability. The review concludes that LVLMs are expected to soon meet or surpass conventional methods in object detection performance.

Conclusion: Recent advancements in LVLMs have made and will continue to make transformative impacts on object detection and robotic applications, despite current limitations that require addressing through proposed solutions and a clear roadmap for future advancement.

Abstract: The fusion of language and vision in large vision-language models (LVLMs) has
revolutionized deep learning-based object detection by enhancing adaptability,
contextual reasoning, and generalization beyond traditional architectures. This
in-depth review presents a structured exploration of the state-of-the-art in
LVLMs, systematically organized through a three-step research review process.
First, we discuss the functioning of vision language models (VLMs) for object
detection, describing how these models harness natural language processing
(NLP) and computer vision (CV) techniques to revolutionize object detection and
localization. We then explain the architectural innovations, training
paradigms, and output flexibility of recent LVLMs for object detection,
highlighting how they achieve advanced contextual understanding for object
detection. The review thoroughly examines the approaches used in integration of
visual and textual information, demonstrating the progress made in object
detection using VLMs that facilitate more sophisticated object detection and
localization strategies. This review presents comprehensive visualizations
demonstrating LVLMs' effectiveness in diverse scenarios including localization
and segmentation, and then compares their real-time performance, adaptability,
and complexity to traditional deep learning systems. Based on the review, its
is expected that LVLMs will soon meet or surpass the performance of
conventional methods in object detection. The review also identifies a few
major limitations of the current LVLM modes, proposes solutions to address
those challenges, and presents a clear roadmap for the future advancement in
this field. We conclude, based on this study, that the recent advancement in
LVLMs have made and will continue to make a transformative impact on object
detection and robotic applications in the future.

</details>


### [6] [Large VLM-based Stylized Sports Captioning](https://arxiv.org/abs/2508.19295)
*Sauptik Dhar,Nicholas Buoncristiani,Joe Anakata,Haoyu Zhang,Michelle Munson*

Main category: cs.CV

TL;DR: Proposes a two-level fine-tuned LVLM pipeline for generating accurate, stylized sports captions from images, addressing limitations of existing models in sports domain language.


<details>
  <summary>Details</summary>
Motivation: Existing LLM/LVLMs lack sufficient sports-specific jargon and domain knowledge to generate natural, human-like descriptions of game plays, despite their success in other domains.

Method: A two-level fine-tuned large visual language model (LVLM) pipeline specifically designed for sports caption generation from images.

Result: 8-10% improvement in F1 score, 2-10% improvement in BERT score compared to alternatives. Successfully deployed during Super Bowl LIX, generating captions for 1000+ images at 6 images per 3-5 seconds.

Conclusion: The proposed pipeline effectively addresses domain-specific limitations of existing models for sports captioning, demonstrating practical application in live sports journalism with high accuracy and efficiency.

Abstract: The advent of large (visual) language models (LLM / LVLM) have led to a
deluge of automated human-like systems in several domains including social
media content generation, search and recommendation, healthcare prognosis, AI
assistants for cognitive tasks etc. Although these systems have been
successfully integrated in production; very little focus has been placed on
sports, particularly accurate identification and natural language description
of the game play. Most existing LLM/LVLMs can explain generic sports
activities, but lack sufficient domain-centric sports' jargon to create natural
(human-like) descriptions. This work highlights the limitations of existing
SoTA LLM/LVLMs for generating production-grade sports captions from images in a
desired stylized format, and proposes a two-level fine-tuned LVLM pipeline to
address that. The proposed pipeline yields an improvement > 8-10% in the F1,
and > 2-10% in BERT score compared to alternative approaches. In addition, it
has a small runtime memory footprint and fast execution time. During Super Bowl
LIX the pipeline proved its practical application for live professional sports
journalism; generating highly accurate and stylized captions at the rate of 6
images per 3-5 seconds for over 1000 images during the game play.

</details>


### [7] [DemoBias: An Empirical Study to Trace Demographic Biases in Vision Foundation Models](https://arxiv.org/abs/2508.19298)
*Abu Sufian,Anirudha Ghosh,Debaditya Barman,Marco Leo,Cosimo Distante*

Main category: cs.CV

TL;DR: LVLMs show demographic biases in face recognition tasks, with PaliGemma and LLaVA exhibiting higher disparities for Hispanic/Latino, Caucasian, and South Asian groups, while BLIP-2 performs more consistently across demographics.


<details>
  <summary>Details</summary>
Motivation: Demographic biases remain a critical concern in face recognition systems, as foundation models often fail to perform equitably across diverse demographic groups including ethnicity/race, gender, and age.

Method: Fine-tuned and evaluated three pre-trained LVLMs (LLaVA, BLIP-2, PaliGemma) on a demographic-balanced dataset using evaluation metrics like group-specific BERTScores and Fairness Discrepancy Rate.

Result: PaliGemma and LLaVA showed higher performance disparities for Hispanic/Latino, Caucasian, and South Asian groups, while BLIP-2 demonstrated comparatively consistent performance across demographic groups.

Conclusion: The study reveals significant demographic biases in LVLMs for face recognition tasks, highlighting the need for fairness-aware development and evaluation of these models to ensure equitable performance across diverse populations.

Abstract: Large Vision Language Models (LVLMs) have demonstrated remarkable
capabilities across various downstream tasks, including biometric face
recognition (FR) with description. However, demographic biases remain a
critical concern in FR, as these foundation models often fail to perform
equitably across diverse demographic groups, considering ethnicity/race,
gender, and age. Therefore, through our work DemoBias, we conduct an empirical
evaluation to investigate the extent of demographic biases in LVLMs for
biometric FR with textual token generation tasks. We fine-tuned and evaluated
three widely used pre-trained LVLMs: LLaVA, BLIP-2, and PaliGemma on our own
generated demographic-balanced dataset. We utilize several evaluation metrics,
like group-specific BERTScores and the Fairness Discrepancy Rate, to quantify
and trace the performance disparities. The experimental results deliver
compelling insights into the fairness and reliability of LVLMs across diverse
demographic groups. Our empirical study uncovered demographic biases in LVLMs,
with PaliGemma and LLaVA exhibiting higher disparities for Hispanic/Latino,
Caucasian, and South Asian groups, whereas BLIP-2 demonstrated comparably
consistent. Repository: https://github.com/Sufianlab/DemoBias.

</details>


### [8] [Geo2Vec: Shape- and Distance-Aware Neural Representation of Geospatial Entities](https://arxiv.org/abs/2508.19305)
*Chen Chu,Cyrus Shahabi*

Main category: cs.CV

TL;DR: Geo2Vec is a novel spatial representation learning method that uses signed distance fields and adaptive sampling to create unified, geometry-aware embeddings for all geo-entity types without decomposition, outperforming existing methods in efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing spatial representation methods either handle only single entity types or require decomposition with high computational cost, and they lack geometric alignment which blurs fine-grained features. There's a need for a unified, efficient method that preserves geometric details.

Method: Geo2Vec operates directly in original space using signed distance fields (SDF), adaptively samples points and encodes signed distances (positive outside, negative inside), and uses a neural network to approximate SDF. It also includes rotation-invariant positional encoding for high-frequency spatial variations.

Result: Empirical results show Geo2Vec consistently outperforms existing methods in representing shape and location, capturing topological and distance relationships, and achieving greater efficiency in real-world GeoAI applications.

Conclusion: Geo2Vec provides a unified, efficient, and geometry-aware representation method for all geo-entity types that addresses the limitations of previous approaches and enables better performance in downstream GeoAI applications.

Abstract: Spatial representation learning is essential for GeoAI applications such as
urban analytics, enabling the encoding of shapes, locations, and spatial
relationships (topological and distance-based) of geo-entities like points,
polylines, and polygons. Existing methods either target a single geo-entity
type or, like Poly2Vec, decompose entities into simpler components to enable
Fourier transformation, introducing high computational cost. Moreover, since
the transformed space lacks geometric alignment, these methods rely on uniform,
non-adaptive sampling, which blurs fine-grained features like edges and
boundaries. To address these limitations, we introduce Geo2Vec, a novel method
inspired by signed distance fields (SDF) that operates directly in the original
space. Geo2Vec adaptively samples points and encodes their signed distances
(positive outside, negative inside), capturing geometry without decomposition.
A neural network trained to approximate the SDF produces compact,
geometry-aware, and unified representations for all geo-entity types.
Additionally, we propose a rotation-invariant positional encoding to model
high-frequency spatial variations and construct a structured and robust
embedding space for downstream GeoAI models. Empirical results show that
Geo2Vec consistently outperforms existing methods in representing shape and
location, capturing topological and distance relationships, and achieving
greater efficiency in real-world GeoAI applications. Code and Data can be found
at: https://github.com/chuchen2017/GeoNeuralRepresentation.

</details>


### [9] [Advancements in Crop Analysis through Deep Learning and Explainable AI](https://arxiv.org/abs/2508.19307)
*Hamza Khan*

Main category: cs.CV

TL;DR: This study proposes automated CNN-based classification of 5 rice varieties using 75,000 images and develops deep learning models with XAI techniques for rice leaf disease diagnosis, achieving high accuracy and interpretability.


<details>
  <summary>Details</summary>
Motivation: Rice is a globally important staple food, but manual quality inspection is labor-intensive and error-prone. There's a need for automated solutions to ensure quality control and yield improvement in rice production.

Method: Used Convolutional Neural Networks (CNN) to classify 5 rice varieties from 75,000 images. Combined explainable AI (XAI) techniques (SHAP and LIME) with deep learning models (CNN, VGG16, ResNet50, MobileNetV2) for rice leaf disease diagnosis.

Result: Achieved high classification accuracy with minimal misclassifications for rice varieties. Successfully developed accurate diagnostic method for rice leaf diseases including Brown Spot, Blast, Bacterial Blight, and Tungro.

Conclusion: Deep learning with XAI shows strong potential for agricultural applications, enabling robust and interpretable systems for automated crop quality inspection and disease diagnosis that benefit farmers, consumers, and the agricultural economy.

Abstract: Rice is a staple food of global importance in terms of trade, nutrition, and
economic growth. Among Asian nations such as China, India, Pakistan, Thailand,
Vietnam and Indonesia are leading producers of both long and short grain
varieties, including basmati, jasmine, arborio, ipsala, and kainat saila. To
ensure consumer satisfaction and strengthen national reputations, monitoring
rice crops and grain quality is essential. Manual inspection, however, is
labour intensive, time consuming and error prone, highlighting the need for
automated solutions for quality control and yield improvement. This study
proposes an automated approach to classify five rice grain varieties using
Convolutional Neural Networks (CNN). A publicly available dataset of 75000
images was used for training and testing. Model evaluation employed accuracy,
recall, precision, F1-score, ROC curves, and confusion matrices. Results
demonstrated high classification accuracy with minimal misclassifications,
confirming the model effectiveness in distinguishing rice varieties. In
addition, an accurate diagnostic method for rice leaf diseases such as Brown
Spot, Blast, Bacterial Blight, and Tungro was developed. The framework combined
explainable artificial intelligence (XAI) with deep learning models including
CNN, VGG16, ResNet50, and MobileNetV2. Explainability techniques such as SHAP
(SHapley Additive exPlanations) and LIME (Local Interpretable Model-agnostic
Explanations) revealed how specific grain and leaf features influenced
predictions, enhancing model transparency and reliability. The findings
demonstrate the strong potential of deep learning in agricultural applications,
paving the way for robust, interpretable systems that can support automated
crop quality inspection and disease diagnosis, ultimately benefiting farmers,
consumers, and the agricultural economy.

</details>


### [10] [Sistema de Reconocimiento Facial Federado en Conjuntos Abiertos basado en OpenMax](https://arxiv.org/abs/2508.19312)
*Ander Galván,Marivi Higuero,Jorge Sasiain,Eduardo Jacob*

Main category: cs.CV

TL;DR: Federated learning system with OpenMax algorithm for privacy-aware facial recognition that handles unknown individuals in open-set scenarios


<details>
  <summary>Details</summary>
Motivation: Address privacy concerns and identity management challenges in facial recognition systems, particularly when unknown individuals appear in operational contexts

Method: Integrates OpenMax algorithm into federated learning framework, using exchange of mean activation vectors and local distance measures to distinguish between known and unknown subjects

Result: Experimental results validate the effectiveness of the proposed solution for reliable identification of both known and unknown individuals

Conclusion: The approach demonstrates potential for enhancing privacy-aware and robust facial recognition in distributed environments through federated learning with OpenMax integration

Abstract: Facial recognition powered by Artificial Intelligence has achieved high
accuracy in specific scenarios and applications. Nevertheless, it faces
significant challenges regarding privacy and identity management, particularly
when unknown individuals appear in the operational context. This paper presents
the design, implementation, and evaluation of a facial recognition system
within a federated learning framework tailored to open-set scenarios. The
proposed approach integrates the OpenMax algorithm into federated learning,
leveraging the exchange of mean activation vectors and local distance measures
to reliably distinguish between known and unknown subjects. Experimental
results validate the effectiveness of the proposed solution, demonstrating its
potential for enhancing privacy-aware and robust facial recognition in
distributed environments.
  --
  El reconocimiento facial impulsado por Inteligencia Artificial ha demostrado
una alta precisi\'on en algunos escenarios y aplicaciones. Sin embargo,
presenta desaf\'ios relacionados con la privacidad y la identificaci\'on de
personas, especialmente considerando que pueden aparecer sujetos desconocidos
para el sistema que lo implementa. En este trabajo, se propone el dise\~no,
implementaci\'on y evaluaci\'on de un sistema de reconocimiento facial en un
escenario de aprendizaje federado, orientado a conjuntos abiertos.
Concretamente, se dise\~na una soluci\'on basada en el algoritmo OpenMax para
escenarios de aprendizaje federado. La propuesta emplea el intercambio de los
vectores de activaci\'on promedio y distancias locales para identificar de
manera eficaz tanto personas conocidas como desconocidas. Los experimentos
realizados demuestran la implementaci\'on efectiva de la soluci\'on propuesta.

</details>


### [11] [Automated classification of natural habitats using ground-level imagery](https://arxiv.org/abs/2508.19314)
*Mahdis Tourian,Sareh Rowlands,Remy Vandaele,Max Fancourt,Rebecca Mein,Hywel T. P. Williams*

Main category: cs.CV

TL;DR: Deep learning approach using ground-level photographs to classify terrestrial habitats into 18 classes with mean F1-score of 0.61, outperforming traditional satellite-based methods with better validation capabilities.


<details>
  <summary>Details</summary>
Motivation: Need for accurate habitat classification for biodiversity conservation and ecological monitoring, with improved validation over satellite imagery methods and ability to scale using citizen-science imagery.

Method: Developed DeepLabV3-ResNet101 classifier using ground-level habitat photographs, with pre-processing (resizing, normalization, augmentation) and re-sampling to balance classes. Used five-fold cross-validation on 18 habitat classes defined by 'Living England' framework.

Result: Model achieved mean F1-score of 0.61 across all folds, with visually distinct habitats (Bare Soil, Silt and Peat; Bare Sand) scoring above 0.90, while mixed/ambiguous classes performed lower. Strong overall performance across 18 habitat classes.

Conclusion: Ground-level imagery combined with deep learning offers promising approach for ecological monitoring at scale, with practical applications supported by a web application for practitioners to classify uploaded habitat images.

Abstract: Accurate classification of terrestrial habitats is critical for biodiversity
conservation, ecological monitoring, and land-use planning. Several habitat
classification schemes are in use, typically based on analysis of satellite
imagery with validation by field ecologists. Here we present a methodology for
classification of habitats based solely on ground-level imagery (photographs),
offering improved validation and the ability to classify habitats at scale (for
example using citizen-science imagery). In collaboration with Natural England,
a public sector organisation responsible for nature conservation in England,
this study develops a classification system that applies deep learning to
ground-level habitat photographs, categorising each image into one of 18
classes defined by the 'Living England' framework. Images were pre-processed
using resizing, normalisation, and augmentation; re-sampling was used to
balance classes in the training data and enhance model robustness. We developed
and fine-tuned a DeepLabV3-ResNet101 classifier to assign a habitat class label
to each photograph. Using five-fold cross-validation, the model demonstrated
strong overall performance across 18 habitat classes, with accuracy and
F1-scores varying between classes. Across all folds, the model achieved a mean
F1-score of 0.61, with visually distinct habitats such as Bare Soil, Silt and
Peat (BSSP) and Bare Sand (BS) reaching values above 0.90, and mixed or
ambiguous classes scoring lower. These findings demonstrate the potential of
this approach for ecological monitoring. Ground-level imagery is readily
obtained, and accurate computational methods for habitat classification based
on such data have many potential applications. To support use by practitioners,
we also provide a simple web application that classifies uploaded images using
our model.

</details>


### [12] [MIDAS: Multimodal Interactive Digital-human Synthesis via Real-time Autoregressive Video Generation](https://arxiv.org/abs/2508.19320)
*Ming Chen,Liyuan Cui,Wenyuan Zhang,Haoxian Zhang,Yan Zhou,Xiaohan Li,Xiaoqiang Liu,Pengfei Wan*

Main category: cs.CV

TL;DR: An autoregressive video generation framework for interactive digital humans with multimodal control and low-latency streaming, using minimal LLM modifications and deep compression to achieve real-time performance.


<details>
  <summary>Details</summary>
Motivation: Existing interactive digital human video generation methods struggle with high latency, heavy computational costs, and limited controllability, making real-time practical systems challenging to build.

Method: Autoregressive framework with minimal LLM modifications that accepts multimodal inputs (audio, pose, text) and outputs coherent representations to guide diffusion denoising. Uses a deep compression autoencoder with 64x reduction ratio and trains on a 20,000-hour dialogue dataset.

Result: Achieves low latency, high efficiency, and fine-grained multimodal controllability in experiments including duplex conversation, multilingual human synthesis, and interactive world modeling.

Conclusion: The framework successfully addresses key challenges in interactive digital human generation by enabling real-time multimodal control with minimal computational burden while maintaining spatial and semantic coherence.

Abstract: Recently, interactive digital human video generation has attracted widespread
attention and achieved remarkable progress. However, building such a practical
system that can interact with diverse input signals in real time remains
challenging to existing methods, which often struggle with high latency, heavy
computational cost, and limited controllability. In this work, we introduce an
autoregressive video generation framework that enables interactive multimodal
control and low-latency extrapolation in a streaming manner. With minimal
modifications to a standard large language model (LLM), our framework accepts
multimodal condition encodings including audio, pose, and text, and outputs
spatially and semantically coherent representations to guide the denoising
process of a diffusion head. To support this, we construct a large-scale
dialogue dataset of approximately 20,000 hours from multiple sources, providing
rich conversational scenarios for training. We further introduce a deep
compression autoencoder with up to 64$\times$ reduction ratio, which
effectively alleviates the long-horizon inference burden of the autoregressive
model. Extensive experiments on duplex conversation, multilingual human
synthesis, and interactive world model highlight the advantages of our approach
in low latency, high efficiency, and fine-grained multimodal controllability.

</details>


### [13] [Deep Data Hiding for ICAO-Compliant Face Images: A Survey](https://arxiv.org/abs/2508.19324)
*Jefferson David Rodriguez Chivata,Davide Ghiani,Simone Maurizio La Cava,Marco Micheletto,Giulia Orrù,Federico Lama,Gian Luca Marcialis*

Main category: cs.CV

TL;DR: Survey paper explores digital watermarking and steganography as post-capture protection for ICAO-compliant facial images against morphing and deepfakes, analyzing state-of-the-art techniques and trade-offs for secure identity verification systems.


<details>
  <summary>Details</summary>
Motivation: ICAO-compliant facial images are vulnerable to manipulation techniques like morphing and deepfakes for identity theft, while traditional Presentation Attack Detection (PAD) only works during real-time capture and offers no post-capture protection.

Method: Comprehensive analysis of state-of-the-art digital watermarking and steganography techniques that embed tamper-evident signals directly into images while maintaining ICAO compliance standards.

Result: First comprehensive evaluation of watermarking and steganography approaches for ICAO images, identifying key trade-offs and assessing their suitability under standard constraints for persistent verification.

Conclusion: Digital watermarking and steganography provide viable complementary solutions to traditional PAD, enabling persistent post-capture verification without compromising ICAO compliance, with guidance offered for secure real-world deployment.

Abstract: ICAO-compliant facial images, initially designed for secure biometric
passports, are increasingly becoming central to identity verification in a wide
range of application contexts, including border control, digital travel
credentials, and financial services. While their standardization enables global
interoperability, it also facilitates practices such as morphing and deepfakes,
which can be exploited for harmful purposes like identity theft and illegal
sharing of identity documents. Traditional countermeasures like Presentation
Attack Detection (PAD) are limited to real-time capture and offer no
post-capture protection. This survey paper investigates digital watermarking
and steganography as complementary solutions that embed tamper-evident signals
directly into the image, enabling persistent verification without compromising
ICAO compliance. We provide the first comprehensive analysis of
state-of-the-art techniques to evaluate the potential and drawbacks of the
underlying approaches concerning the applications involving ICAO-compliant
images and their suitability under standard constraints. We highlight key
trade-offs, offering guidance for secure deployment in real-world identity
systems.

</details>


### [14] [PRISM: A Framework Harnessing Unsupervised Visual Representations and Textual Prompts for Explainable MACE Survival Prediction from Cardiac Cine MRI](https://arxiv.org/abs/2508.19325)
*Haoyang Su,Jin-Yi Xiang,Shaohao Rui,Yifan Gao,Xingyu Chen,Tingxuan Yin,Xiaosong Wang,Lian-Ming Wu*

Main category: cs.CV

TL;DR: PRISM is a self-supervised framework that integrates cardiac MRI imaging with EHR data using prompt-guided representation learning for improved MACE prediction, outperforming traditional methods and uncovering novel cardiac risk signatures.


<details>
  <summary>Details</summary>
Motivation: Accurate prediction of major adverse cardiac events (MACE) remains challenging in cardiovascular prognosis, requiring better integration of multimodal clinical data including imaging and electronic health records.

Method: PRISM uses motion-aware multi-view distillation to extract temporally synchronized imaging features from cardiac cine MRI and modulates them with medically informed textual prompts, integrating these with structured EHR data for survival analysis.

Result: PRISM consistently outperformed classical survival models and state-of-the-art deep learning baselines across four independent clinical cohorts in both internal and external validation. It uncovered three distinct imaging signatures associated with elevated MACE risk and identified hypertension, diabetes, and smoking as dominant EHR risk factors.

Conclusion: The framework successfully integrates multimodal clinical data to provide fine-grained cardiac risk prediction and valuable clinical insights, demonstrating the effectiveness of prompt-guided representation learning for cardiovascular prognosis.

Abstract: Accurate prediction of major adverse cardiac events (MACE) remains a central
challenge in cardiovascular prognosis. We present PRISM (Prompt-guided
Representation Integration for Survival Modeling), a self-supervised framework
that integrates visual representations from non-contrast cardiac cine magnetic
resonance imaging with structured electronic health records (EHRs) for survival
analysis. PRISM extracts temporally synchronized imaging features through
motion-aware multi-view distillation and modulates them using medically
informed textual prompts to enable fine-grained risk prediction. Across four
independent clinical cohorts, PRISM consistently surpasses classical survival
prediction models and state-of-the-art (SOTA) deep learning baselines under
internal and external validation. Further clinical findings demonstrate that
the combined imaging and EHR representations derived from PRISM provide
valuable insights into cardiac risk across diverse cohorts. Three distinct
imaging signatures associated with elevated MACE risk are uncovered, including
lateral wall dyssynchrony, inferior wall hypersensitivity, and anterior
elevated focus during diastole. Prompt-guided attribution further identifies
hypertension, diabetes, and smoking as dominant contributors among clinical and
physiological EHR factors.

</details>


### [15] [EffNetViTLoRA: An Efficient Hybrid Deep Learning Approach for Alzheimer's Disease Diagnosis](https://arxiv.org/abs/2508.19349)
*Mahdieh Behjat Khatooni,Mohsen Soryani*

Main category: cs.CV

TL;DR: EffNetViTLoRA model combines CNN and Vision Transformer with LoRA adaptation for Alzheimer's disease diagnosis using full ADNI MRI dataset, achieving 92.52% accuracy across AD, MCI, and CN categories.


<details>
  <summary>Details</summary>
Motivation: Early diagnosis of Alzheimer's disease is crucial as it's irreversible. Mild Cognitive Impairment (MCI) diagnosis is challenging due to subtle differences between diagnostic categories, and previous studies used limited data subsets.

Method: Integration of CNN with Vision Transformer to capture both local and global MRI features. Uses full ADNI T1-weighted MRI dataset. Incorporates Low-Rank Adaptation (LoRA) to adapt pretrained ViT model effectively to the target domain.

Result: Achieves 92.52% classification accuracy and 92.76% F1-score across three diagnostic categories (AD, MCI, CN) using the complete ADNI dataset.

Conclusion: The proposed EffNetViTLoRA model provides a robust and clinically reliable approach for Alzheimer's disease diagnosis by leveraging comprehensive data and effective domain adaptation techniques.

Abstract: Alzheimer's disease (AD) is one of the most prevalent neurodegenerative
disorders worldwide. As it progresses, it leads to the deterioration of
cognitive functions. Since AD is irreversible, early diagnosis is crucial for
managing its progression. Mild Cognitive Impairment (MCI) represents an
intermediate stage between Cognitively Normal (CN) individuals and those with
AD, and is considered a transitional phase from normal cognition to Alzheimer's
disease. Diagnosing MCI is particularly challenging due to the subtle
differences between adjacent diagnostic categories. In this study, we propose
EffNetViTLoRA, a generalized end-to-end model for AD diagnosis using the whole
Alzheimer's Disease Neuroimaging Initiative (ADNI) Magnetic Resonance Imaging
(MRI) dataset. Our model integrates a Convolutional Neural Network (CNN) with a
Vision Transformer (ViT) to capture both local and global features from MRI
images. Unlike previous studies that rely on limited subsets of data, our
approach is trained on the full T1-weighted MRI dataset from ADNI, resulting in
a more robust and unbiased model. This comprehensive methodology enhances the
model's clinical reliability. Furthermore, fine-tuning large pretrained models
often yields suboptimal results when source and target dataset domains differ.
To address this, we incorporate Low-Rank Adaptation (LoRA) to effectively adapt
the pretrained ViT model to our target domain. This method enables efficient
knowledge transfer and reduces the risk of overfitting. Our model achieves a
classification accuracy of 92.52% and an F1-score of 92.76% across three
diagnostic categories: AD, MCI, and CN for full ADNI dataset.

</details>


### [16] [Concurrent validity of computer-vision artificial intelligence player tracking software using broadcast footage](https://arxiv.org/abs/2508.19477)
*Zachary L. Crang,Rich D. Johnston,Katie L. Mills,Johsan Billingham,Sam Robertson,Michael H. Cole,Jonathon Weakley,Adam Hewitt and,Grant M. Duthie*

Main category: cs.CV

TL;DR: Commercial AI player tracking from broadcast footage shows fair precision but significant errors in position (1.68-16.39m RMSE) and distance measurements (-21.8% to +24.3% bias). Tactical camera feeds and proper AI models are crucial for accuracy.


<details>
  <summary>Details</summary>
Motivation: To evaluate the accuracy of commercially available computer-vision and AI player tracking software using broadcast footage, and determine the impact of camera feed type and resolution on measurement precision.

Method: Used data from one 2022 FIFA World Cup match with three commercial tracking providers. Compared their position (x,y coordinates) and speed measurements against a high-definition multi-camera TRACAB Gen 5 system as ground truth. Calculated RMSE and mean bias for position, speed, and total distance.

Result: Position RMSE ranged 1.68-16.39m, speed RMSE 0.34-2.38 m/s. Total match distance showed mean bias from -1745m (-21.8%) to +1945m (+24.3%). Tactical feed provided best accuracy when players were detected.

Conclusion: Computer-vision AI tracking offers fair precision when players are detected. Tactical camera feeds maximize detection and accuracy. Both 720p and 1080p resolutions are suitable with proper AI models.

Abstract: This study aimed to: (1) understand whether commercially available
computer-vision and artificial intelligence (AI) player tracking software can
accurately measure player position, speed and distance using broadcast footage
and (2) determine the impact of camera feed and resolution on accuracy. Data
were obtained from one match at the 2022 Qatar Federation Internationale de
Football Association (FIFA) World Cup. Tactical, programme and camera 1 feeds
were used. Three commercial tracking providers that use computer-vision and AI
participated. Providers analysed instantaneous position (x, y coordinates) and
speed (m\,s^{-1}) of each player. Their data were compared with a
high-definition multi-camera tracking system (TRACAB Gen 5). Root mean square
error (RMSE) and mean bias were calculated. Position RMSE ranged from 1.68 to
16.39 m, while speed RMSE ranged from 0.34 to 2.38 m\,s^{-1}. Total match
distance mean bias ranged from -1745 m (-21.8%) to 1945 m (24.3%) across
providers. Computer-vision and AI player tracking software offer the ability to
track players with fair precision when players are detected by the software.
Providers should use a tactical feed when tracking position and speed, which
will maximise player detection, improving accuracy. Both 720p and 1080p
resolutions are suitable, assuming appropriate computer-vision and AI models
are implemented.

</details>


### [17] [JVLGS: Joint Vision-Language Gas Leak Segmentation](https://arxiv.org/abs/2508.19485)
*Xinlong Zhao,Qixiang Pang,Shan Du*

Main category: cs.CV

TL;DR: A novel vision-language framework called JVLGS that integrates visual and textual modalities to improve gas leak segmentation, addressing challenges of blurry gas clouds and sporadic leaks with post-processing to reduce false positives.


<details>
  <summary>Details</summary>
Motivation: Gas leaks pose serious health and environmental threats, but current detection methods are limited by the blurry, non-rigid nature of gas clouds and lack effective ways to handle sporadic leaks with many non-leak frames.

Method: Proposes Joint Vision-Language Gas leak Segmentation (JVLGS) framework that combines visual and textual information, includes post-processing to reduce false positives from noise and non-target objects, and works under both supervised and few-shot learning settings.

Result: Extensive experiments show JVLGS significantly outperforms state-of-the-art methods across diverse scenarios, achieving strong performance in both supervised and few-shot learning settings where competing methods perform well in only one or poorly in both.

Conclusion: The integration of vision-language modalities and post-processing for false positive reduction provides an effective solution for gas leak segmentation that works robustly across different learning scenarios.

Abstract: Gas leaks pose serious threats to human health and contribute significantly
to atmospheric pollution, drawing increasing public concern. However, the lack
of effective detection methods hampers timely and accurate identification of
gas leaks. While some vision-based techniques leverage infrared videos for leak
detection, the blurry and non-rigid nature of gas clouds often limits their
effectiveness. To address these challenges, we propose a novel framework called
Joint Vision-Language Gas leak Segmentation (JVLGS), which integrates the
complementary strengths of visual and textual modalities to enhance gas leak
representation and segmentation. Recognizing that gas leaks are sporadic and
many video frames may contain no leak at all, our method incorporates a
post-processing step to reduce false positives caused by noise and non-target
objects, an issue that affects many existing approaches. Extensive experiments
conducted across diverse scenarios show that JVLGS significantly outperforms
state-of-the-art gas leak segmentation methods. We evaluate our model under
both supervised and few-shot learning settings, and it consistently achieves
strong performance in both, whereas competing methods tend to perform well in
only one setting or poorly in both. Code available at:
https://github.com/GeekEagle/JVLGS

</details>


### [18] [UNIFORM: Unifying Knowledge from Large-scale and Diverse Pre-trained Models](https://arxiv.org/abs/2508.19498)
*Yimu Wang,Weiming Zhuang,Chen Chen,Jiabo Huang,Jingtao Li,Lingjuan Lyu*

Main category: cs.CV

TL;DR: UNIFORM is a framework for transferring knowledge from diverse pre-trained models to a student model using voting mechanisms at both logit and feature levels, overcoming limitations of existing methods that require specific model types and architectures.


<details>
  <summary>Details</summary>
Motivation: The proliferation of diverse pre-trained models offers valuable collective knowledge, but existing integration methods have strong assumptions about data distributions and architectures, limiting their applicability and causing biases.

Method: Proposes UNIFORM framework with dedicated voting mechanism that captures consensus at logit level (for models predicting target classes) and feature level (using visual representations from arbitrary label spaces), enabling knowledge transfer without architectural constraints.

Result: Extensive experiments show UNIFORM significantly enhances unsupervised object recognition performance compared to strong baselines, demonstrating remarkable scalability by effectively utilizing over 100 teachers while existing methods saturate at smaller scales.

Conclusion: UNIFORM provides an effective and scalable solution for harnessing collective knowledge from heterogeneous pre-trained models without the constraints of existing methods, enabling better knowledge transfer and generalization.

Abstract: In the era of deep learning, the increasing number of pre-trained models
available online presents a wealth of knowledge. These models, developed with
diverse architectures and trained on varied datasets for different tasks,
provide unique interpretations of the real world. Their collective consensus is
likely universal and generalizable to unseen data. However, effectively
harnessing this collective knowledge poses a fundamental challenge due to the
heterogeneity of pre-trained models. Existing knowledge integration solutions
typically rely on strong assumptions about training data distributions and
network architectures, limiting them to learning only from specific types of
models and resulting in data and/or inductive biases. In this work, we
introduce a novel framework, namely UNIFORM, for knowledge transfer from a
diverse set of off-the-shelf models into one student model without such
constraints. Specifically, we propose a dedicated voting mechanism to capture
the consensus of knowledge both at the logit level -- incorporating teacher
models that are capable of predicting target classes of interest -- and at the
feature level, utilizing visual representations learned on arbitrary label
spaces. Extensive experiments demonstrate that UNIFORM effectively enhances
unsupervised object recognition performance compared to strong knowledge
transfer baselines. Notably, it exhibits remarkable scalability by benefiting
from over one hundred teachers, while existing methods saturate at a much
smaller scale.

</details>


### [19] [Sat2Flow: A Structure-Aware Diffusion Framework for Human Flow Generation from Satellite Imagery](https://arxiv.org/abs/2508.19499)
*Xiangxu Wang,Tianhong Zhao,Wei Tu,Bowen Zhang,Guanzhou Chen,Jinzhou Cao*

Main category: cs.CV

TL;DR: Sat2Flow is a diffusion-based framework that generates structurally coherent Origin-Destination flow matrices using only satellite imagery, eliminating the need for costly auxiliary data and maintaining robustness to spatial reordering.


<details>
  <summary>Details</summary>
Motivation: Existing OD flow generation methods rely on expensive auxiliary features with limited coverage and are sensitive to spatial topology changes like region reindexing, which disrupts structural coherence.

Method: Uses a multi-kernel encoder to capture diverse regional interactions and a permutation-aware diffusion process with joint contrastive training and equivariant diffusion to ensure structural consistency across different regional orderings.

Result: Outperforms both physics-based and data-driven baselines in numerical accuracy while preserving empirical distributions and spatial structures under index permutations on real-world urban datasets.

Conclusion: Sat2Flow provides a globally scalable solution for OD flow generation in data-scarce environments, eliminating region-specific data dependencies while maintaining structural invariance for robust mobility modeling.

Abstract: Origin-Destination (OD) flow matrices are essential for urban mobility
analysis, underpinning applications in traffic forecasting, infrastructure
planning, and policy design. However, existing methods suffer from two critical
limitations: (1) reliance on auxiliary features (e.g., Points of Interest,
socioeconomic statistics) that are costly to collect and have limited spatial
coverage; and (2) sensitivity to spatial topology, where minor index reordering
of urban regions (e.g., census tract relabeling) disrupts structural coherence
in generated flows. To address these challenges, we propose Sat2Flow, a latent
structure-aware diffusion-based framework that generates structurally coherent
OD flows using solely satellite imagery as input. Our approach introduces a
multi-kernel encoder to capture diverse regional interactions and employs a
permutation-aware diffusion process that aligns latent representations across
different regional orderings. Through a joint contrastive training objective
that bridges satellite-derived features with OD patterns, combined with
equivariant diffusion training that enforces structural consistency, Sat2Flow
ensures topological robustness under arbitrary regional reindexing.
Experimental results on real-world urban datasets demonstrate that Sat2Flow
outperforms both physics-based and data-driven baselines in numerical accuracy
while preserving empirical distributions and spatial structures under index
permutations. Sat2Flow offers a globally scalable solution for OD flow
generation in data-scarce urban environments, eliminating region-specific
auxiliary data dependencies while maintaining structural invariance for robust
mobility modeling.

</details>


### [20] [Weed Detection in Challenging Field Conditions: A Semi-Supervised Framework for Overcoming Shadow Bias and Data Scarcity](https://arxiv.org/abs/2508.19511)
*Alzayat Saleh,Shunsuke Hatano,Mostafa Rahimi Azghadi*

Main category: cs.CV

TL;DR: Semi-supervised framework for weed detection that addresses shadow bias and annotation costs, achieving improved robustness and recall in agricultural applications.


<details>
  <summary>Details</summary>
Motivation: Automated weed management faces challenges from environmental conditions and expensive data annotation, requiring robust deep learning solutions that work in real-world field conditions.

Method: Diagnostic-driven semi-supervised framework using ResNet for classification and YOLO/RF-DETR for detection, leveraging 975 labeled and 10,000 unlabeled images with pseudo-labeling to mitigate shadow bias.

Result: Achieved F1 scores up to 0.90 and mAP50 scores exceeding 0.82, with improved recall critical for minimizing weed escapes in automated spraying systems.

Conclusion: Provides a field-tested framework for developing robust computer vision systems that address real-world agricultural challenges through diagnostic insights and semi-supervised learning.

Abstract: The automated management of invasive weeds is critical for sustainable
agriculture, yet the performance of deep learning models in real-world fields
is often compromised by two factors: challenging environmental conditions and
the high cost of data annotation. This study tackles both issues through a
diagnostic-driven, semi-supervised framework. Using a unique dataset of
approximately 975 labeled and 10,000 unlabeled images of Guinea Grass in
sugarcane, we first establish strong supervised baselines for classification
(ResNet) and detection (YOLO, RF-DETR), achieving F1 scores up to 0.90 and
mAP50 scores exceeding 0.82. Crucially, this foundational analysis, aided by
interpretability tools, uncovered a pervasive "shadow bias," where models
learned to misidentify shadows as vegetation. This diagnostic insight motivated
our primary contribution: a semi-supervised pipeline that leverages unlabeled
data to enhance model robustness. By training models on a more diverse set of
visual information through pseudo-labeling, this framework not only helps
mitigate the shadow bias but also provides a tangible boost in recall, a
critical metric for minimizing weed escapes in automated spraying systems. To
validate our methodology, we demonstrate its effectiveness in a low-data regime
on a public crop-weed benchmark. Our work provides a clear and field-tested
framework for developing, diagnosing, and improving robust computer vision
systems for the complex realities of precision agriculture.

</details>


### [21] [MotionFlux: Efficient Text-Guided Motion Generation through Rectified Flow Matching and Preference Alignment](https://arxiv.org/abs/2508.19527)
*Zhiting Gao,Dan Song,Diqiong Jiang,Chao Xue,An-An Liu*

Main category: cs.CV

TL;DR: TAPO and MotionFLUX framework improves text-to-motion generation with better semantic alignment and real-time synthesis using rectified flow matching instead of slow diffusion models.


<details>
  <summary>Details</summary>
Motivation: Addressing poor alignment between text descriptions and motion semantics, and overcoming inefficiencies of slow multi-step inference in current text-driven motion generation methods.

Method: TAPO framework for semantic alignment through preference optimization and iterative adjustments, plus MotionFLUX using deterministic rectified flow matching to construct optimal transport paths between noise and motion spaces for real-time synthesis.

Result: Outperforms state-of-the-art approaches in both semantic consistency and motion quality while significantly accelerating generation speed compared to traditional diffusion models.

Conclusion: The unified TAPO and MotionFLUX system provides superior text-to-motion generation with real-time capabilities, addressing key limitations of existing methods.

Abstract: Motion generation is essential for animating virtual characters and embodied
agents. While recent text-driven methods have made significant strides, they
often struggle with achieving precise alignment between linguistic descriptions
and motion semantics, as well as with the inefficiencies of slow, multi-step
inference. To address these issues, we introduce TMR++ Aligned Preference
Optimization (TAPO), an innovative framework that aligns subtle motion
variations with textual modifiers and incorporates iterative adjustments to
reinforce semantic grounding. To further enable real-time synthesis, we propose
MotionFLUX, a high-speed generation framework based on deterministic rectified
flow matching. Unlike traditional diffusion models, which require hundreds of
denoising steps, MotionFLUX constructs optimal transport paths between noise
distributions and motion spaces, facilitating real-time synthesis. The
linearized probability paths reduce the need for multi-step sampling typical of
sequential methods, significantly accelerating inference time without
sacrificing motion quality. Experimental results demonstrate that, together,
TAPO and MotionFLUX form a unified system that outperforms state-of-the-art
approaches in both semantic consistency and motion quality, while also
accelerating generation speed. The code and pretrained models will be released.

</details>


### [22] [CVBench: Evaluating Cross-Video Synergies for Complex Multimodal Understanding and Reasoning](https://arxiv.org/abs/2508.19542)
*Nannan Zhu,Yonghao Dong,Teng Wang,Xueqian Li,Shengjun Deng,Yijia Wang,Zheng Hong,Tiantian Geng,Guo Niu,Hanyan Huang,Xiongfei Yao,Shuaiwei Jiao*

Main category: cs.CV

TL;DR: CVBench is the first comprehensive benchmark for evaluating cross-video relational reasoning in MLLMs, revealing significant performance gaps compared to human capabilities and identifying architectural limitations in current models.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs show strong performance on single-video tasks but their ability to reason across multiple videos remains underexplored, despite being essential for real-world applications like multi-camera surveillance and cross-video procedural learning.

Method: Developed CVBench with 1,000 question-answer pairs across three hierarchical tiers (object association, event association, complex reasoning) from five diverse video domains. Evaluated 10+ leading MLLMs including GPT-4o and Gemini-2.0-flash under zero-shot and chain-of-thought prompting.

Result: Significant performance gaps observed: top models like GPT-4o achieve only 60% accuracy on causal reasoning tasks compared to 91% human accuracy. Identified fundamental bottlenecks including deficient inter-video context retention and poor disambiguation of overlapping entities.

Conclusion: CVBench provides a rigorous framework for diagnosing multi-video reasoning capabilities and offers architectural insights for developing next-generation MLLMs that can effectively reason across multiple video contexts.

Abstract: While multimodal large language models (MLLMs) exhibit strong performance on
single-video tasks (e.g., video question answering), their ability across
multiple videos remains critically underexplored. However, this capability is
essential for real-world applications, including multi-camera surveillance and
cross-video procedural learning. To bridge this gap, we present CVBench, the
first comprehensive benchmark designed to assess cross-video relational
reasoning rigorously. CVBench comprises 1,000 question-answer pairs spanning
three hierarchical tiers: cross-video object association (identifying shared
entities), cross-video event association (linking temporal or causal event
chains), and cross-video complex reasoning (integrating commonsense and domain
knowledge). Built from five domain-diverse video clusters (e.g., sports, life
records), the benchmark challenges models to synthesise information across
dynamic visual contexts. Extensive evaluation of 10+ leading MLLMs (including
GPT-4o, Gemini-2.0-flash, Qwen2.5-VL) under zero-shot or chain-of-thought
prompting paradigms. Key findings reveal stark performance gaps: even top
models, such as GPT-4o, achieve only 60% accuracy on causal reasoning tasks,
compared to the 91% accuracy of human performance. Crucially, our analysis
reveals fundamental bottlenecks inherent in current MLLM architectures, notably
deficient inter-video context retention and poor disambiguation of overlapping
entities. CVBench establishes a rigorous framework for diagnosing and advancing
multi-video reasoning, offering architectural insights for next-generation
MLLMs.The data and evaluation code are available at
https://github.com/Hokhim2/CVBench.

</details>


### [23] [WEBEYETRACK: Scalable Eye-Tracking for the Browser via On-Device Few-Shot Personalization](https://arxiv.org/abs/2508.19544)
*Eduardo Davalos,Yike Zhang,Namrata Srivastava,Yashvitha Thatigotla,Jorge A. Salas,Sara McFadden,Sun-Joo Cho,Amanda Goodwin,Ashwin TS,Gautam Biswas*

Main category: cs.CV

TL;DR: WebEyeTrack is a browser-based gaze estimation framework that combines lightweight SOTA models with head pose estimation and few-shot learning, achieving real-time performance with minimal calibration.


<details>
  <summary>Details</summary>
Motivation: Existing AI gaze estimation methods excel in benchmarks but fall short in real-world applications compared to commercial solutions, with issues around model size, inference time, privacy, and accuracy limitations in webcam-based methods due to head movement.

Method: Integrates lightweight SOTA gaze estimation models directly in browser, incorporates model-based head pose estimation, and implements on-device few-shot learning requiring only 9 calibration samples (k < 9).

Result: Achieves SOTA performance with 2.32 cm error margin on GazeCapture dataset and real-time inference speeds of 2.4 milliseconds on iPhone 14.

Conclusion: WebEyeTrack successfully bridges the gap between academic benchmarks and practical application by providing accurate, real-time, privacy-preserving gaze estimation that adapts to new users with minimal calibration.

Abstract: With advancements in AI, new gaze estimation methods are exceeding
state-of-the-art (SOTA) benchmarks, but their real-world application reveals a
gap with commercial eye-tracking solutions. Factors like model size, inference
time, and privacy often go unaddressed. Meanwhile, webcam-based eye-tracking
methods lack sufficient accuracy, in particular due to head movement. To tackle
these issues, we introduce We bEyeTrack, a framework that integrates
lightweight SOTA gaze estimation models directly in the browser. It
incorporates model-based head pose estimation and on-device few-shot learning
with as few as nine calibration samples (k < 9). WebEyeTrack adapts to new
users, achieving SOTA performance with an error margin of 2.32 cm on
GazeCapture and real-time inference speeds of 2.4 milliseconds on an iPhone 14.
Our open-source code is available at
https://github.com/RedForestAi/WebEyeTrack.

</details>


### [24] [MonoRelief V2: Leveraging Real Data for High-Fidelity Monocular Relief Recovery](https://arxiv.org/abs/2508.19555)
*Yu-Wei Zhang,Tongju Han,Lipeng Gao,Mingqiang Wei,Hui Liu,Changbao Li,Caiming Zhang*

Main category: cs.CV

TL;DR: MonoRelief V2 is an end-to-end model for 2.5D relief recovery from single images that improves upon V1 by incorporating both synthetic and real data training, achieving state-of-the-art depth and normal prediction performance.


<details>
  <summary>Details</summary>
Motivation: To overcome the limitations of MonoRelief V1 which was trained only on synthetic data, the authors aim to develop a more robust and accurate model that can handle complex material and illumination variations in real-world scenarios.

Method: The method involves generating ~15,000 pseudo-real images using text-to-image generative models, creating depth pseudo-labels through depth-normal fusion, building a small real dataset (800 samples) via multi-view reconstruction, and progressive training on both datasets.

Result: Comprehensive experiments show state-of-the-art performance in both depth and normal predictions, demonstrating improved robustness, accuracy and efficiency over previous methods.

Conclusion: MonoRelief V2 successfully addresses the challenge of real-world data scarcity through pseudo-real data generation and achieves superior performance, making it suitable for various downstream applications in 2.5D relief recovery.

Abstract: This paper presents MonoRelief V2, an end-to-end model designed for directly
recovering 2.5D reliefs from single images under complex material and
illumination variations. In contrast to its predecessor, MonoRelief V1 [1],
which was solely trained on synthetic data, MonoRelief V2 incorporates real
data to achieve improved robustness, accuracy and efficiency. To overcome the
challenge of acquiring large-scale real-world dataset, we generate
approximately 15,000 pseudo real images using a text-to-image generative model,
and derive corresponding depth pseudo-labels through fusion of depth and normal
predictions. Furthermore, we construct a small-scale real-world dataset (800
samples) via multi-view reconstruction and detail refinement. MonoRelief V2 is
then progressively trained on the pseudo-real and real-world datasets.
Comprehensive experiments demonstrate its state-of-the-art performance both in
depth and normal predictions, highlighting its strong potential for a range of
downstream applications. Code is at: https://github.com/glp1001/MonoreliefV2.

</details>


### [25] [FlowDet: Overcoming Perspective and Scale Challenges in Real-Time End-to-End Traffic Detection](https://arxiv.org/abs/2508.19565)
*Yuhang Zhao,Zixing Wang*

Main category: cs.CV

TL;DR: FlowDet is a high-speed NMS-free object detector that achieves state-of-the-art performance on intersection traffic monitoring with significant computational efficiency gains.


<details>
  <summary>Details</summary>
Motivation: End-to-end object detectors are promising for real-time applications but suffer from high computational costs, especially in complex scenarios like intersection traffic monitoring with severe occlusion and high object density.

Method: Proposes FlowDet with decoupled encoder optimization strategy for DETR architecture, featuring Geometric Deformable Unit (GDU) for traffic-aware geometric modeling and Scale-Aware Attention (SAA) module for handling extreme scale variations.

Result: On Intersection-Flow-5k dataset, FlowDet improves AP(test) by 1.5% and AP50(test) by 1.6% over RT-DETR baseline, while reducing GFLOPs by 63.2% and increasing inference speed by 16.2%.

Conclusion: FlowDet demonstrates a new path for building highly efficient and accurate detectors for demanding real-world perception systems, with the new Intersection-Flow-5k dataset available for further research.

Abstract: End-to-end object detectors offer a promising NMS-free paradigm for real-time
applications, yet their high computational cost remains a significant barrier,
particularly for complex scenarios like intersection traffic monitoring. To
address this challenge, we propose FlowDet, a high-speed detector featuring a
decoupled encoder optimization strategy applied to the DETR architecture.
Specifically, FlowDet employs a novel Geometric Deformable Unit (GDU) for
traffic-aware geometric modeling and a Scale-Aware Attention (SAA) module to
maintain high representational power across extreme scale variations. To
rigorously evaluate the model's performance in environments with severe
occlusion and high object density, we collected the Intersection-Flow-5k
dataset, a new challenging scene for this task. Evaluated on
Intersection-Flow-5k, FlowDet establishes a new state-of-the-art. Compared to
the strong RT-DETR baseline, it improves AP(test) by 1.5% and AP50(test) by
1.6%, while simultaneously reducing GFLOPs by 63.2% and increasing inference
speed by 16.2%. Our work demonstrates a new path towards building highly
efficient and accurate detectors for demanding, real-world perception systems.
The Intersection-Flow-5k dataset is available at
https://github.com/AstronZh/Intersection-Flow-5K.

</details>


### [26] [DNP-Guided Contrastive Reconstruction with a Reverse Distillation Transformer for Medical Anomaly Detection](https://arxiv.org/abs/2508.19573)
*Luhu Li,Bowen Lin,Mukhtiar Khan,Shujun Fu*

Main category: cs.CV

TL;DR: A unified framework combining trainable encoder with prototype-guided reconstruction and diversity-aware loss to prevent prototype collapse in medical anomaly detection.


<details>
  <summary>Details</summary>
Motivation: Address limitations in existing reconstruction methods that use frozen pre-trained encoders and suffer from prototype collapse in prototype-based learning, which reduces domain adaptation and localization accuracy in medical images.

Method: Trainable encoder with momentum branch for domain-adaptive feature learning, lightweight Prototype Extractor to mine normal prototypes, prototype-guided reconstruction via attention, and Diversity-Aware Alignment Loss with diversity constraints and per-prototype normalization to prevent collapse.

Result: Significant improvements in representation quality and anomaly localization across multiple medical imaging benchmarks, outperforming prior methods with enhanced interpretability.

Conclusion: The proposed framework effectively addresses prototype collapse and domain adaptation issues, providing superior anomaly detection performance and interpretability in medical image analysis.

Abstract: Anomaly detection in medical images is challenging due to limited annotations
and a domain gap compared to natural images. Existing reconstruction methods
often rely on frozen pre-trained encoders, which limits adaptation to
domain-specific features and reduces localization accuracy. Prototype-based
learning offers interpretability and clustering benefits but suffers from
prototype collapse, where few prototypes dominate training, harming diversity
and generalization. To address this, we propose a unified framework combining a
trainable encoder with prototype-guided reconstruction and a novel
Diversity-Aware Alignment Loss. The trainable encoder, enhanced by a momentum
branch, enables stable domain-adaptive feature learning. A lightweight
Prototype Extractor mines informative normal prototypes to guide the decoder
via attention for precise reconstruction. Our loss enforces balanced prototype
use through diversity constraints and per-prototype normalization, effectively
preventing collapse. Experiments on multiple medical imaging benchmarks show
significant improvements in representation quality and anomaly localization,
outperforming prior methods. Visualizations and prototype assignment analyses
further validate the effectiveness of our anti-collapse mechanism and enhanced
interpretability.

</details>


### [27] [Multimodal Prototype Alignment for Semi-supervised Pathology Image Segmentation](https://arxiv.org/abs/2508.19574)
*Mingxi Fu,Fanglei Fu,Xitong Ling,Huaitian Yuan,Tian Guan,Yonghong He,Lianghui Zhu*

Main category: cs.CV

TL;DR: MPAMatch is a novel semi-supervised segmentation framework that uses multimodal prototype-guided contrastive learning with both image and text prototypes to improve pathological image segmentation, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Pathological image segmentation faces challenges due to ambiguous semantic boundaries and expensive pixel-level annotations. Existing semi-supervised methods rely mainly on perturbation-based consistency within image modality, which struggles to capture high-level semantic priors in complex pathology images.

Method: Proposes MPAMatch with dual contrastive learning between image prototypes and pixel labels, and between text prototypes and pixel labels. Replaces ViT backbone with pathology-pretrained foundation model (Uni) for better feature extraction. Uses coarse-to-fine supervisory strategy.

Result: Extensive experiments on GLAS, EBHI-SEG-GLAND, EBHI-SEG-CANCER, and KPI datasets show MPAMatch's superiority over state-of-the-art methods, demonstrating dual advantages in structural and semantic modeling.

Conclusion: MPAMatch effectively addresses limitations of existing semi-supervised segmentation methods by introducing multimodal prototype-guided supervision, significantly improving semantic boundary modeling and discriminative capability on unlabeled pathology images.

Abstract: Pathological image segmentation faces numerous challenges, particularly due
to ambiguous semantic boundaries and the high cost of pixel-level annotations.
Although recent semi-supervised methods based on consistency regularization
(e.g., UniMatch) have made notable progress, they mainly rely on
perturbation-based consistency within the image modality, making it difficult
to capture high-level semantic priors, especially in structurally complex
pathology images. To address these limitations, we propose MPAMatch - a novel
segmentation framework that performs pixel-level contrastive learning under a
multimodal prototype-guided supervision paradigm. The core innovation of
MPAMatch lies in the dual contrastive learning scheme between image prototypes
and pixel labels, and between text prototypes and pixel labels, providing
supervision at both structural and semantic levels. This coarse-to-fine
supervisory strategy not only enhances the discriminative capability on
unlabeled samples but also introduces the text prototype supervision into
segmentation for the first time, significantly improving semantic boundary
modeling. In addition, we reconstruct the classic segmentation architecture
(TransUNet) by replacing its ViT backbone with a pathology-pretrained
foundation model (Uni), enabling more effective extraction of
pathology-relevant features. Extensive experiments on GLAS, EBHI-SEG-GLAND,
EBHI-SEG-CANCER, and KPI show MPAMatch's superiority over state-of-the-art
methods, validating its dual advantages in structural and semantic modeling.

</details>


### [28] [Interact-Custom: Customized Human Object Interaction Image Generation](https://arxiv.org/abs/2508.19575)
*Zhu Xu,Zhaowen Wang,Yuxin Peng,Yang Liu*

Main category: cs.CV

TL;DR: Proposes Customized Human Object Interaction Image Generation (CHOI) task and Interact-Custom model to simultaneously preserve target identities while controlling interaction semantics between human and object.


<details>
  <summary>Details</summary>
Motivation: Existing approaches focus only on appearance preservation of target entities but neglect fine-grained interaction control between them, limiting practical applications.

Method: Two-stage approach: 1) Processes large-scale dataset with same human-object pairs in different interactive poses, 2) Interact-Custom model first generates foreground mask to model spatial configuration, then generates target human-object interactions while preserving identity features under mask guidance.

Result: Extensive experiments on tailored metrics demonstrate the effectiveness of the approach in achieving simultaneous identity preservation and interaction semantic control.

Conclusion: The proposed CHOI task and Interact-Custom model successfully address the challenges of decomposing identity and interaction features while providing optional background and location specification for high content controllability.

Abstract: Compositional Customized Image Generation aims to customize multiple target
concepts within generation content, which has gained attention for its wild
application.Existing approaches mainly concentrate on the target entity's
appearance preservation, while neglecting the fine-grained interaction control
among target entities.To enable the model of such interaction control
capability, we focus on human object interaction scenario and propose the task
of Customized Human Object Interaction Image Generation(CHOI), which
simultaneously requires identity preservation for target human object and the
interaction semantic control between them.Two primary challenges exist for
CHOI:(1)simultaneous identity preservation and interaction control demands
require the model to decompose the human object into self-contained identity
features and pose-oriented interaction features, while the current HOI image
datasets fail to provide ideal samples for such feature-decomposed
learning.(2)inappropriate spatial configuration between human and object may
lead to the lack of desired interaction semantics.To tackle it, we first
process a large-scale dataset, where each sample encompasses the same pair of
human object involving different interactive poses.Then we design a two-stage
model Interact-Custom, which firstly explicitly models the spatial
configuration by generating a foreground mask depicting the interaction
behavior, then under the guidance of this mask, we generate the target human
object interacting while preserving their identities features.Furthermore, if
the background image and the union location of where the target human object
should appear are provided by users, Interact-Custom also provides the optional
functionality to specify them, offering high content controllability. Extensive
experiments on our tailored metrics for CHOI task demonstrate the effectiveness
of our approach.

</details>


### [29] [High-Speed FHD Full-Color Video Computer-Generated Holography](https://arxiv.org/abs/2508.19579)
*Haomiao Zhang,Miao Cao,Xuan Yu,Hui Luo,Yanling Piao,Mengjie Qin,Zhangyuan Li,Ping Wang,Xin Yuan*

Main category: cs.CV

TL;DR: Proposes SGDDM for high-fidelity full-color holographic display at high frame rates and HoloMamba for efficient 260+ FPS holographic video generation.


<details>
  <summary>Details</summary>
Motivation: Overcome limitations in computer-generated holography: learning-based models cause color crosstalk in high frame rate displays, and existing methods neglect spatial-temporal correlations between frames.

Method: Two-part approach: 1) Spectrum-Guided Depth Division Multiplexing (SGDDM) optimizes phase distributions via frequency modulation, 2) HoloMamba - lightweight asymmetric Mamba-Unet architecture that models spatial-temporal correlations.

Result: SGDDM achieves high-fidelity full-color display without frame rate compromise. HoloMamba generates FHD (1080p) full-color holographic video at over 260 FPS, 2.6x faster than prior state-of-the-art.

Conclusion: The proposed scheme successfully addresses key limitations in holographic video generation, enabling both high frame rates and high color fidelity through novel frequency modulation and spatial-temporal modeling approaches.

Abstract: Computer-generated holography (CGH) is a promising technology for
next-generation displays. However, generating high-speed, high-quality
holographic video requires both high frame rate display and efficient
computation, but is constrained by two key limitations: ($i$) Learning-based
models often produce over-smoothed phases with narrow angular spectra, causing
severe color crosstalk in high frame rate full-color displays such as
depth-division multiplexing and thus resulting in a trade-off between frame
rate and color fidelity. ($ii$) Existing frame-by-frame optimization methods
typically optimize frames independently, neglecting spatial-temporal
correlations between consecutive frames and leading to computationally
inefficient solutions. To overcome these challenges, in this paper, we propose
a novel high-speed full-color video CGH generation scheme. First, we introduce
Spectrum-Guided Depth Division Multiplexing (SGDDM), which optimizes phase
distributions via frequency modulation, enabling high-fidelity full-color
display at high frame rates. Second, we present HoloMamba, a lightweight
asymmetric Mamba-Unet architecture that explicitly models spatial-temporal
correlations across video sequences to enhance reconstruction quality and
computational efficiency. Extensive simulated and real-world experiments
demonstrate that SGDDM achieves high-fidelity full-color display without
compromise in frame rate, while HoloMamba generates FHD (1080p) full-color
holographic video at over 260 FPS, more than 2.6$\times$ faster than the prior
state-of-the-art Divide-Conquer-and-Merge Strategy.

</details>


### [30] [Guiding Noisy Label Conditional Diffusion Models with Score-based Discriminator Correction](https://arxiv.org/abs/2508.19581)
*Dat Nguyen Cong,Hieu Tran Bao,Hoang Thanh-Tung*

Main category: cs.CV

TL;DR: SBDC is a guidance technique that uses discriminator training with adversarial loss to correct noisy pre-trained conditional diffusion models, improving their generative capabilities and controllability without requiring retraining.


<details>
  <summary>Details</summary>
Motivation: Large datasets used for diffusion models often contain labeling errors that compromise model performance, but the impact of these errors on generative capabilities and controllability is not well studied.

Method: Score-based Discriminator Correction (SBDC) uses discriminator training with adversarial loss, leveraging noise detection techniques to assess sample authenticity. The guidance is limited to the early phase of generation for better performance.

Result: Experiments show SBDC is computationally efficient, only marginally increases inference time, and outperforms previous state-of-the-art methods across different noise settings.

Conclusion: SBDC effectively aligns noisy pre-trained conditional diffusion models without requiring retraining, demonstrating superior performance while maintaining efficiency.

Abstract: Diffusion models have gained prominence as state-of-the-art techniques for
synthesizing images and videos, particularly due to their ability to scale
effectively with large datasets. Recent studies have uncovered that these
extensive datasets often contain mistakes from manual labeling processes.
However, the extent to which such errors compromise the generative capabilities
and controllability of diffusion models is not well studied. This paper
introduces Score-based Discriminator Correction (SBDC), a guidance technique
for aligning noisy pre-trained conditional diffusion models. The guidance is
built on discriminator training using adversarial loss, drawing on prior noise
detection techniques to assess the authenticity of each sample. We further show
that limiting the usage of our guidance to the early phase of the generation
process leads to better performance. Our method is computationally efficient,
only marginally increases inference time, and does not require retraining
diffusion models. Experiments on different noise settings demonstrate the
superiority of our method over previous state-of-the-art methods.

</details>


### [31] [Generalizing Monocular 3D Object Detection](https://arxiv.org/abs/2508.19593)
*Abhinav Kumar*

Main category: cs.CV

TL;DR: This thesis addresses generalization challenges in monocular 3D object detection across diverse scenarios including occlusions, datasets, object sizes, and camera parameters through novel mathematical approaches and segmentation-based methods.


<details>
  <summary>Details</summary>
Motivation: Monocular 3D object detection is crucial for applications like autonomous driving and robotics, but existing models struggle with generalization to diverse real-world scenarios including occlusions, different datasets, varying object sizes, and camera parameter variations.

Method: Proposed multiple approaches: 1) GrooMeD-NMS - mathematically differentiable NMS for occlusion robustness, 2) DEVIANT backbones - depth equivariant backbones for dataset generalization, 3) SeaBird - segmentation-based approach with dice loss in bird's-eye view for large object detection, and 4) mathematical analysis of camera height extrapolation.

Result: The methods address specific generalization challenges: improved occlusion handling, better adaptation to new datasets, enhanced detection of large objects (addressing noise sensitivity beyond data imbalance), and better performance in out-of-distribution camera settings.

Conclusion: The thesis provides comprehensive solutions for improving monocular 3D object detection generalization through mathematical formulations and specialized approaches for different challenge domains, making the technology more robust and applicable to diverse real-world scenarios.

Abstract: Monocular 3D object detection (Mono3D) is a fundamental computer vision task
that estimates an object's class, 3D position, dimensions, and orientation from
a single image. Its applications, including autonomous driving, augmented
reality, and robotics, critically rely on accurate 3D environmental
understanding. This thesis addresses the challenge of generalizing Mono3D
models to diverse scenarios, including occlusions, datasets, object sizes, and
camera parameters. To enhance occlusion robustness, we propose a mathematically
differentiable NMS (GrooMeD-NMS). To improve generalization to new datasets, we
explore depth equivariant (DEVIANT) backbones. We address the issue of large
object detection, demonstrating that it's not solely a data imbalance or
receptive field problem but also a noise sensitivity issue. To mitigate this,
we introduce a segmentation-based approach in bird's-eye view with dice loss
(SeaBird). Finally, we mathematically analyze the extrapolation of Mono3D
models to unseen camera heights and improve Mono3D generalization in such
out-of-distribution settings.

</details>


### [32] [Quantization Robustness to Input Degradations for Object Detection](https://arxiv.org/abs/2508.19600)
*Toghrul Karimov,Hassan Imani,Allan Kazakov*

Main category: cs.CV

TL;DR: Study evaluates YOLO models' robustness to real-world degradations after post-training quantization, testing various precision formats and a degradation-aware calibration strategy for INT8 quantization.


<details>
  <summary>Details</summary>
Motivation: To understand how reduced precision affects model robustness to input degradations (noise, blur, compression artifacts) and improve deployment of efficient object detection on resource-constrained devices.

Method: Comprehensive empirical evaluation of YOLO models (nano to extra-large) across FP32, FP16, Dynamic UINT8, and Static INT8 precision formats. Introduced degradation-aware calibration for Static INT8 where calibration process uses mix of clean and synthetically degraded images.

Result: Static INT8 TensorRT engines provided 1.5-3.3x speedups with 3-7% mAP50-95 drop on clean data. Degradation-aware calibration did not yield consistent robustness improvements across most models and degradations, except for larger models under specific noise conditions.

Conclusion: Challenges exist in enhancing PTQ robustness; model capacity may influence calibration efficacy. Findings provide insights for deploying quantized detectors in uncontrolled environments.

Abstract: Post-training quantization (PTQ) is crucial for deploying efficient object
detection models, like YOLO, on resource-constrained devices. However, the
impact of reduced precision on model robustness to real-world input
degradations such as noise, blur, and compression artifacts is a significant
concern. This paper presents a comprehensive empirical study evaluating the
robustness of YOLO models (nano to extra-large scales) across multiple
precision formats: FP32, FP16 (TensorRT), Dynamic UINT8 (ONNX), and Static INT8
(TensorRT). We introduce and evaluate a degradation-aware calibration strategy
for Static INT8 PTQ, where the TensorRT calibration process is exposed to a mix
of clean and synthetically degraded images. Models were benchmarked on the COCO
dataset under seven distinct degradation conditions (including various types
and levels of noise, blur, low contrast, and JPEG compression) and a
mixed-degradation scenario. Results indicate that while Static INT8 TensorRT
engines offer substantial speedups (~1.5-3.3x) with a moderate accuracy drop
(~3-7% mAP50-95) on clean data, the proposed degradation-aware calibration did
not yield consistent, broad improvements in robustness over standard clean-data
calibration across most models and degradations. A notable exception was
observed for larger model scales under specific noise conditions, suggesting
model capacity may influence the efficacy of this calibration approach. These
findings highlight the challenges in enhancing PTQ robustness and provide
insights for deploying quantized detectors in uncontrolled environments. All
code and evaluation tables are available at https://github.com/AllanK24/QRID.

</details>


### [33] [IELDG: Suppressing Domain-Specific Noise with Inverse Evolution Layers for Domain Generalized Semantic Segmentation](https://arxiv.org/abs/2508.19604)
*Qizhe Fan,Chaoyu Liu,Zhonghua Qiao,Xiaoqin Shen*

Main category: cs.CV

TL;DR: Proposes IELDM and IELFormer for domain generalized semantic segmentation, using inverse evolution layers to filter diffusion model defects and improve cross-domain generalization with multi-scale frequency fusion.


<details>
  <summary>Details</summary>
Motivation: Diffusion models generate synthetic data with structural/semantic defects that degrade segmentation performance. Need to filter undesirable generative patterns and prevent error accumulation.

Method: Integrate inverse evolution layers (IELs) with Laplacian-based priors to detect spatial/semantic inconsistencies. IELDM enhances diffusion generation, IELFormer embeds IELs in segmentation decoder with multi-scale frequency fusion module.

Result: Extensive experiments show superior generalization performance on benchmark datasets compared to existing methods.

Conclusion: IEL framework effectively suppresses generative defects and artifact propagation, improving cross-domain semantic segmentation generalization through enhanced data quality and model architecture.

Abstract: Domain Generalized Semantic Segmentation (DGSS) focuses on training a model
using labeled data from a source domain, with the goal of achieving robust
generalization to unseen target domains during inference. A common approach to
improve generalization is to augment the source domain with synthetic data
generated by diffusion models (DMs). However, the generated images often
contain structural or semantic defects due to training imperfections. Training
segmentation models with such flawed data can lead to performance degradation
and error accumulation. To address this issue, we propose to integrate inverse
evolution layers (IELs) into the generative process. IELs are designed to
highlight spatial discontinuities and semantic inconsistencies using
Laplacian-based priors, enabling more effective filtering of undesirable
generative patterns. Based on this mechanism, we introduce IELDM, an enhanced
diffusion-based data augmentation framework that can produce higher-quality
images. Furthermore, we observe that the defect-suppression capability of IELs
can also benefit the segmentation network by suppressing artifact propagation.
Based on this insight, we embed IELs into the decoder of the DGSS model and
propose IELFormer to strengthen generalization capability in cross-domain
scenarios. To further strengthen the model's semantic consistency across
scales, IELFormer incorporates a multi-scale frequency fusion (MFF) module,
which performs frequency-domain analysis to achieve structured integration of
multi-resolution features, thereby improving cross-scale coherence. Extensive
experiments on benchmark datasets demonstrate that our approach achieves
superior generalization performance compared to existing methods.

</details>


### [34] [Controllable Skin Synthesis via Lesion-Focused Vector Autoregression Model](https://arxiv.org/abs/2508.19626)
*Jiajun Sun,Zhen Yu,Siyuan Yan,Jason J. Ong,Zongyuan Ge,Lei Zhang*

Main category: cs.CV

TL;DR: LF-VAR is a controllable skin image synthesis model that uses lesion measurement scores and type labels to generate high-fidelity clinical skin images with specific lesion characteristics based on language prompts.


<details>
  <summary>Details</summary>
Motivation: Real-world clinical skin images are limited, creating data shortages for deep learning models. Existing synthesis methods produce low-quality images and lack control over lesion location and type.

Method: Uses a multiscale lesion-focused VQVAE to encode images into discrete latent representations, then trains a Visual AutoRegressive Transformer on tokenized representations. Integrates lesion measurements and types as conditional embeddings.

Result: Achieves best overall FID score (average 0.74) across seven lesion types, improving upon previous state-of-the-art by 6.3%.

Conclusion: The model effectively generates high-fidelity, clinically relevant synthetic skin images with controllable lesion characteristics.

Abstract: Skin images from real-world clinical practice are often limited, resulting in
a shortage of training data for deep-learning models. While many studies have
explored skin image synthesis, existing methods often generate low-quality
images and lack control over the lesion's location and type. To address these
limitations, we present LF-VAR, a model leveraging quantified lesion
measurement scores and lesion type labels to guide the clinically relevant and
controllable synthesis of skin images. It enables controlled skin synthesis
with specific lesion characteristics based on language prompts. We train a
multiscale lesion-focused Vector Quantised Variational Auto-Encoder (VQVAE) to
encode images into discrete latent representations for structured tokenization.
Then, a Visual AutoRegressive (VAR) Transformer trained on tokenized
representations facilitates image synthesis. Lesion measurement from the lesion
region and types as conditional embeddings are integrated to enhance synthesis
fidelity. Our method achieves the best overall FID score (average 0.74) among
seven lesion types, improving upon the previous state-of-the-art (SOTA) by
6.3%. The study highlights our controllable skin synthesis model's
effectiveness in generating high-fidelity, clinically relevant synthetic skin
images. Our framework code is available at
https://github.com/echosun1996/LF-VAR.

</details>


### [35] [Divide, Weight, and Route: Difficulty-Aware Optimization with Dynamic Expert Fusion for Long-tailed Recognition](https://arxiv.org/abs/2508.19630)
*Xiaolei Wei,Yi Ouyang,Haibo Ye*

Main category: cs.CV

TL;DR: DQRoute is a modular framework for long-tailed recognition that combines difficulty-aware optimization with dynamic expert collaboration, improving performance on rare and difficult classes through adaptive loss weighting and decentralized expert routing.


<details>
  <summary>Details</summary>
Motivation: Long-tailed visual recognition is challenging due to class imbalance and varying classification difficulty across categories. Simple class reweighting often overlooks intrinsically hard-to-learn classes, requiring a more sophisticated approach that considers both frequency and difficulty.

Method: DQRoute estimates class-wise difficulty using prediction uncertainty and historical performance to guide adaptive loss weighting. It employs a mixture-of-experts architecture where each expert specializes in different class distribution regions. Expert predictions are weighted by confidence scores from expert-specific OOD detectors at inference, enabling input-adaptive routing without a centralized router. All components are trained end-to-end.

Result: Experiments on standard long-tailed benchmarks show that DQRoute significantly improves performance, particularly on rare and difficult classes, demonstrating the effectiveness of integrating difficulty modeling with decentralized expert routing.

Conclusion: The integration of difficulty-aware optimization with dynamic expert collaboration through DQRoute provides substantial benefits for long-tailed recognition, effectively addressing both class imbalance and varying classification difficulty challenges.

Abstract: Long-tailed visual recognition is challenging not only due to class imbalance
but also because of varying classification difficulty across categories. Simply
reweighting classes by frequency often overlooks those that are intrinsically
hard to learn. To address this, we propose \textbf{DQRoute}, a modular
framework that combines difficulty-aware optimization with dynamic expert
collaboration. DQRoute first estimates class-wise difficulty based on
prediction uncertainty and historical performance, and uses this signal to
guide training with adaptive loss weighting. On the architectural side, DQRoute
employs a mixture-of-experts design, where each expert specializes in a
different region of the class distribution. At inference time, expert
predictions are weighted by confidence scores derived from expert-specific OOD
detectors, enabling input-adaptive routing without the need for a centralized
router. All components are trained jointly in an end-to-end manner. Experiments
on standard long-tailed benchmarks demonstrate that DQRoute significantly
improves performance, particularly on rare and difficult classes, highlighting
the benefit of integrating difficulty modeling with decentralized expert
routing.

</details>


### [36] [Beyond BEV: Optimizing Point-Level Tokens for Collaborative Perception](https://arxiv.org/abs/2508.19638)
*Yang Li,Quan Yuan,Guiyang Luo,Xiaoyuan Fu,Rui Pan,Yujia Yang,Congzhang Shao,Yuewen Liu,Jinglin Li*

Main category: cs.CV

TL;DR: CoPLOT introduces point-level tokens for collaborative perception, addressing limitations of BEV representations by preserving 3D structural information through semantic-aware token reordering, frequency-enhanced sequence modeling, and spatial alignment.


<details>
  <summary>Details</summary>
Motivation: Existing collaborative perception methods use 2D BEV representations that discard critical 3D structural information needed for accurate object recognition and localization.

Method: Proposes CoPLOT framework with point-native processing: semantic-aware token reordering, frequency-enhanced state space model for sequence dependencies, and neighbor-to-ego alignment module for spatial correction.

Result: Outperforms state-of-the-art models on simulated and real-world datasets with lower communication and computation overhead.

Conclusion: Point-level tokens with optimized processing effectively preserve 3D structural information for collaborative perception, achieving superior performance with reduced computational costs.

Abstract: Collaborative perception allows agents to enhance their perceptual
capabilities by exchanging intermediate features. Existing methods typically
organize these intermediate features as 2D bird's-eye-view (BEV)
representations, which discard critical fine-grained 3D structural cues
essential for accurate object recognition and localization. To this end, we
first introduce point-level tokens as intermediate representations for
collaborative perception. However, point-cloud data are inherently unordered,
massive, and position-sensitive, making it challenging to produce compact and
aligned point-level token sequences that preserve detailed structural
information. Therefore, we present CoPLOT, a novel Collaborative perception
framework that utilizes Point-Level Optimized Tokens. It incorporates a
point-native processing pipeline, including token reordering, sequence
modeling, and multi-agent spatial alignment. A semantic-aware token reordering
module generates adaptive 1D reorderings by leveraging scene-level and
token-level semantic information. A frequency-enhanced state space model
captures long-range sequence dependencies across both spatial and spectral
domains, improving the differentiation between foreground tokens and background
clutter. Lastly, a neighbor-to-ego alignment module applies a closed-loop
process, combining global agent-level correction with local token-level
refinement to mitigate localization noise. Extensive experiments on both
simulated and real-world datasets show that CoPLOT outperforms state-of-the-art
models, with even lower communication and computation overhead. Code will be
available at https://github.com/CheeryLeeyy/CoPLOT.

</details>


### [37] [UTAL-GNN: Unsupervised Temporal Action Localization using Graph Neural Networks](https://arxiv.org/abs/2508.19647)
*Bikash Kumar Badatya,Vipul Baghel,Ravi Hegde*

Main category: cs.CV

TL;DR: Lightweight unsupervised skeleton-based action localization using spatio-temporal graph neural networks that achieves state-of-the-art performance without manual labeling.


<details>
  <summary>Details</summary>
Motivation: Existing supervised and weakly supervised methods for fine-grained action localization require extensive annotated datasets and high-capacity models, making them computationally intensive and less adaptable to real-world scenarios.

Method: Pre-train an Attention-based Spatio-Temporal Graph Convolutional Network (ASTGCN) on pose-sequence denoising with blockwise partitions, then use a novel Action Dynamics Metric (ADM) computed from low-dimensional embeddings to detect motion boundaries by identifying inflection points in curvature profiles.

Result: Achieves 82.66% mAP and 29.09 ms average localization latency on DSV Diving dataset, matching state-of-the-art supervised performance while maintaining computational efficiency.

Conclusion: The method generalizes robustly to unseen diving footage without retraining, demonstrating practical applicability for lightweight, real-time action analysis systems in embedded or dynamic environments.

Abstract: Fine-grained action localization in untrimmed sports videos presents a
significant challenge due to rapid and subtle motion transitions over short
durations. Existing supervised and weakly supervised solutions often rely on
extensive annotated datasets and high-capacity models, making them
computationally intensive and less adaptable to real-world scenarios. In this
work, we introduce a lightweight and unsupervised skeleton-based action
localization pipeline that leverages spatio-temporal graph neural
representations. Our approach pre-trains an Attention-based Spatio-Temporal
Graph Convolutional Network (ASTGCN) on a pose-sequence denoising task with
blockwise partitions, enabling it to learn intrinsic motion dynamics without
any manual labeling. At inference, we define a novel Action Dynamics Metric
(ADM), computed directly from low-dimensional ASTGCN embeddings, which detects
motion boundaries by identifying inflection points in its curvature profile.
Our method achieves a mean Average Precision (mAP) of 82.66% and average
localization latency of 29.09 ms on the DSV Diving dataset, matching
state-of-the-art supervised performance while maintaining computational
efficiency. Furthermore, it generalizes robustly to unseen, in-the-wild diving
footage without retraining, demonstrating its practical applicability for
lightweight, real-time action analysis systems in embedded or dynamic
environments.

</details>


### [38] [IDF: Iterative Dynamic Filtering Networks for Generalizable Image Denoising](https://arxiv.org/abs/2508.19649)
*Dongjin Kim,Jaekyun Ko,Muhammad Kashif Ali,Tae Hyun Kim*

Main category: cs.CV

TL;DR: A compact image denoising method using dynamically generated kernels via efficient operations that prevents overfitting and generalizes well to unseen noise types and levels.


<details>
  <summary>Details</summary>
Motivation: Deep learning denoising methods rely on specific noise distributions and suffer from limited generalization, requiring extensive training data and computational resources while still overfitting.

Method: Uses Feature Extraction Module for noise-invariant features, Global Statistics and Local Correlation Modules to capture noise characteristics, and Kernel Prediction Module to produce pixel-wise varying kernels adapted to local structures, applied iteratively for denoising.

Result: Despite training on single-level Gaussian noise, the compact model (~0.04M parameters) excels across diverse noise types and levels, demonstrating efficient and superior restoration quality.

Conclusion: Iterative dynamic filtering shows promise for practical image denoising by preventing overfitting and improving resilience to unseen noise through adaptive kernel generation.

Abstract: Image denoising is a fundamental challenge in computer vision, with
applications in photography and medical imaging. While deep learning-based
methods have shown remarkable success, their reliance on specific noise
distributions limits generalization to unseen noise types and levels. Existing
approaches attempt to address this with extensive training data and high
computational resources but they still suffer from overfitting. To address
these issues, we conduct image denoising by utilizing dynamically generated
kernels via efficient operations. This approach helps prevent overfitting and
improves resilience to unseen noise. Specifically, our method leverages a
Feature Extraction Module for robust noise-invariant features, Global
Statistics and Local Correlation Modules to capture comprehensive noise
characteristics and structural correlations. The Kernel Prediction Module then
employs these cues to produce pixel-wise varying kernels adapted to local
structures, which are then applied iteratively for denoising. This ensures both
efficiency and superior restoration quality. Despite being trained on
single-level Gaussian noise, our compact model (~ 0.04 M) excels across diverse
noise types and levels, demonstrating the promise of iterative dynamic
filtering for practical image denoising.

</details>


### [39] [Video-LevelGauge: Investigating Contextual Positional Bias in Large Video Language Models](https://arxiv.org/abs/2508.19650)
*Hou Xia,Zheren Fu,Fangcan Ling,Jiajun Li,Yi Tu,Zhendong Mao,Yongdong Zhang*

Main category: cs.CV

TL;DR: Video-LevelGauge is a benchmark for evaluating positional bias in large video language models, revealing significant biases in open-source models while commercial models show consistent performance.


<details>
  <summary>Details</summary>
Motivation: Existing video benchmarks assess overall performance but overlook nuanced behaviors like contextual positional bias, which is critical for understanding LVLM performance.

Method: Created a benchmark with 438 curated videos, 1,177 multiple-choice and 120 open-ended questions, using standardized probes and contextual setups with flexible control over context length, position, and types. Employed statistical measures and morphological pattern recognition for bias analysis.

Result: Evaluation of 27 state-of-the-art LVLMs revealed significant positional biases in many open-source models (head or neighbor-content preferences), while commercial models like Gemini2.5-Pro showed consistent performance across entire sequences.

Conclusion: The benchmark provides actionable insights for mitigating bias and guiding model enhancement, highlighting the importance of systematic positional bias evaluation in video language models.

Abstract: Large video language models (LVLMs) have made notable progress in video
understanding, spurring the development of corresponding evaluation benchmarks.
However, existing benchmarks generally assess overall performance across entire
video sequences, overlooking nuanced behaviors such as contextual positional
bias, a critical yet under-explored aspect of LVLM performance. We present
Video-LevelGauge, a dedicated benchmark designed to systematically assess
positional bias in LVLMs. We employ standardized probes and customized
contextual setups, allowing flexible control over context length, probe
position, and contextual types to simulate diverse real-world scenarios. In
addition, we introduce a comprehensive analysis method that combines
statistical measures with morphological pattern recognition to characterize
bias. Our benchmark comprises 438 manually curated videos spanning multiple
types, yielding 1,177 high-quality multiple-choice questions and 120 open-ended
questions, validated for their effectiveness in exposing positional bias. Based
on these, we evaluate 27 state-of-the-art LVLMs, including both commercial and
open-source models. Our findings reveal significant positional biases in many
leading open-source models, typically exhibiting head or neighbor-content
preferences. In contrast, commercial models such as Gemini2.5-Pro show
impressive, consistent performance across entire video sequences. Further
analyses on context length, context variation, and model scale provide
actionable insights for mitigating bias and guiding model enhancement.

</details>


### [40] [Scalable Object Detection in the Car Interior With Vision Foundation Models](https://arxiv.org/abs/2508.19651)
*Bálint Mészáros,Ahmet Firintepe,Sebastian Schmidt,Stephan Günnemann*

Main category: cs.CV

TL;DR: Proposes ODAL framework for car interior object detection using distributed vision foundation models between on-board and cloud systems, with fine-tuned lightweight models outperforming GPT-4o by 20%.


<details>
  <summary>Details</summary>
Motivation: Computational constraints in vehicles limit deployment of AI object detection systems for personal assistants, requiring efficient solutions that balance on-board and cloud processing.

Method: Distributed architecture leveraging vision foundation models split between on-board and cloud computation. Introduces ODALbench metric for evaluation. Fine-tunes lightweight LLaVA 1.5 7B model.

Result: Fine-tuned ODAL-LLaVA achieves 89% ODAL score (71% improvement over baseline), outperforms GPT-4o by nearly 20%, reduces hallucinations significantly with 3x higher ODAL_SNR than GPT-4o.

Conclusion: The framework demonstrates potential to set new standards for in-vehicle object detection, showing lightweight fine-tuned models can surpass larger foundation models while maintaining high accuracy and reducing computational requirements.

Abstract: AI tasks in the car interior like identifying and localizing externally
introduced objects is crucial for response quality of personal assistants.
However, computational resources of on-board systems remain highly constrained,
restricting the deployment of such solutions directly within the vehicle. To
address this limitation, we propose the novel Object Detection and Localization
(ODAL) framework for interior scene understanding. Our approach leverages
vision foundation models through a distributed architecture, splitting
computational tasks between on-board and cloud. This design overcomes the
resource constraints of running foundation models directly in the car. To
benchmark model performance, we introduce ODALbench, a new metric for
comprehensive assessment of detection and localization.Our analysis
demonstrates the framework's potential to establish new standards in this
domain. We compare the state-of-the-art GPT-4o vision foundation model with the
lightweight LLaVA 1.5 7B model and explore how fine-tuning enhances the
lightweight models performance. Remarkably, our fine-tuned ODAL-LLaVA model
achieves an ODAL$_{score}$ of 89%, representing a 71% improvement over its
baseline performance and outperforming GPT-4o by nearly 20%. Furthermore, the
fine-tuned model maintains high detection accuracy while significantly reducing
hallucinations, achieving an ODAL$_{SNR}$ three times higher than GPT-4o.

</details>


### [41] [Self-Rewarding Vision-Language Model via Reasoning Decomposition](https://arxiv.org/abs/2508.19652)
*Zongxia Li,Wenhao Yu,Chengsong Huang,Rui Liu,Zhenwen Liang,Fuxiao Liu,Jingxi Che,Dian Yu,Jordan Boyd-Graber,Haitao Mi,Dong Yu*

Main category: cs.CV

TL;DR: Vision-SR1 is a self-rewarding reinforcement learning method that improves visual reasoning in VLMs by decomposing reasoning into visual perception and language reasoning stages, using the model's own outputs to create rewards without external supervision.


<details>
  <summary>Details</summary>
Motivation: VLMs suffer from visual hallucinations and language shortcuts due to sparse visual signals and lack of intermediate visual reasoning guidance. Existing methods rely on costly human annotations or external models that cause distributional shifts.

Method: Decomposes VLM reasoning into visual perception and language reasoning stages. The model generates self-contained visual perceptions, then re-prompted to perform reasoning using only these perceptions to compute self-rewards, combined with final output supervision.

Result: Improves visual reasoning, mitigates visual hallucinations, and reduces reliance on language shortcuts across diverse vision-language tasks.

Conclusion: Vision-SR1 effectively strengthens both visual perception and language reasoning without external visual supervision, providing a balanced training signal through self-rewarding reinforcement learning.

Abstract: Vision-Language Models (VLMs) often suffer from visual hallucinations, saying
things that are not actually in the image, and language shortcuts, where they
skip the visual part and just rely on text priors. These issues arise because
most post-training methods for VLMs rely on simple verifiable answer matching
and supervise only final outputs, leaving intermediate visual reasoning without
explicit guidance. As a result, VLMs receive sparse visual signals and often
learn to prioritize language-based reasoning over visual perception. To
mitigate this, some existing methods add visual supervision using human
annotations or distilled labels from external large models. However, human
annotations are labor-intensive and costly, and because external signals cannot
adapt to the evolving policy, they cause distributional shifts that can lead to
reward hacking. In this paper, we introduce Vision-SR1, a self-rewarding method
that improves visual reasoning without relying on external visual supervisions
via reinforcement learning. Vision-SR1 decomposes VLM reasoning into two
stages: visual perception and language reasoning. The model is first prompted
to produce self-contained visual perceptions that are sufficient to answer the
question without referring back the input image. To validate this
self-containment, the same VLM model is then re-prompted to perform language
reasoning using only the generated perception as input to compute reward. This
self-reward is combined with supervision on final outputs, providing a balanced
training signal that strengthens both visual perception and language reasoning.
Our experiments demonstrate that Vision-SR1 improves visual reasoning,
mitigates visual hallucinations, and reduces reliance on language shortcuts
across diverse vision-language tasks.

</details>


### [42] [Hardware-aware vs. Hardware-agnostic Energy Estimation for SNN in Space Applications](https://arxiv.org/abs/2508.19654)
*Matthias Höfflin,Jürgen Wassner*

Main category: cs.CV

TL;DR: SNNs show 50-60% energy advantage over CNNs in hardware-agnostic analysis, but hardware-aware analysis reveals significant savings only occur on neuromorphic hardware with high input sparsity.


<details>
  <summary>Details</summary>
Motivation: Recent studies question SNNs' reputation for energy efficiency compared to ANNs, especially for digital implementations, prompting investigation into multi-output regression tasks like satellite position estimation.

Method: Proposed SNN trained using membrane potential of LIF neuron in final layer for 3-D satellite position estimation from monocular images, compared with reference CNN using both hardware-aware and hardware-agnostic energy estimation methods.

Result: SNN achieves comparable MSE to CNN on photorealistic satellite dataset. Hardware-agnostic methods show consistent 50-60% energy advantage for SNNs, while hardware-aware analysis shows significant savings only on neuromorphic hardware with high input sparsity.

Conclusion: Findings emphasize the importance of transparent evaluation methods and explicit disclosure of underlying assumptions for fair neural network energy efficiency comparisons, highlighting the impact of data characteristics and hardware choices.

Abstract: Spiking Neural Networks (SNNs), inspired by biological intelligence, have
long been considered inherently energy-efficient, making them attractive for
resource-constrained domains such as space applications. However, recent
comparative studies with conventional Artificial Neural Networks (ANNs) have
begun to question this reputation, especially for digital implementations. This
work investigates SNNs for multi-output regression, specifically 3-D satellite
position estimation from monocular images, and compares hardware-aware and
hardware-agnostic energy estimation methods. The proposed SNN, trained using
the membrane potential of the Leaky Integrate-and-Fire (LIF) neuron in the
final layer, achieves comparable Mean Squared Error (MSE) to a reference
Convolutional Neural Network (CNN) on a photorealistic satellite dataset.
Energy analysis shows that while hardware-agnostic methods predict a consistent
50-60% energy advantage for SNNs over CNNs, hardware-aware analysis reveals
that significant energy savings are realized only on neuromorphic hardware and
with high input sparsity. The influence of dark pixel ratio on energy
consumption is quantified, emphasizing the impact of data characteristics and
hardware assumptions. These findings highlight the need for transparent
evaluation methods and explicit disclosure of underlying assumptions to ensure
fair comparisons of neural network energy efficiency.

</details>


### [43] [A Frequency-Aware Self-Supervised Learning for Ultra-Wide-Field Image Enhancement](https://arxiv.org/abs/2508.19664)
*Weicheng Liao,Zan Chen,Jianyang Xie,Yalin Zheng,Yuhui Ma,Yitian Zhao*

Main category: cs.CV

TL;DR: A novel frequency-aware self-supervised learning method for Ultra-Wide-Field retinal image enhancement that addresses blurring and uneven illumination while preserving pathological details.


<details>
  <summary>Details</summary>
Motivation: UWF retinal imaging suffers from quality-degrading factors like blurring and uneven illumination that obscure fine details and mask pathological information, but existing enhancement methods fail to address UWF's unique requirements.

Method: Frequency-aware self-supervised learning with frequency-decoupled image deblurring and Retinex-guided illumination compensation modules, featuring asymmetric channel integration and color preservation units.

Result: The method enhances visualization quality and improves disease diagnosis performance by restoring fine local details and correcting uneven intensity.

Conclusion: This is the first attempt for UWF image enhancement, offering a robust and clinically valuable tool for improving retinal disease management.

Abstract: Ultra-Wide-Field (UWF) retinal imaging has revolutionized retinal diagnostics
by providing a comprehensive view of the retina. However, it often suffers from
quality-degrading factors such as blurring and uneven illumination, which
obscure fine details and mask pathological information. While numerous retinal
image enhancement methods have been proposed for other fundus imageries, they
often fail to address the unique requirements in UWF, particularly the need to
preserve pathological details. In this paper, we propose a novel
frequency-aware self-supervised learning method for UWF image enhancement. It
incorporates frequency-decoupled image deblurring and Retinex-guided
illumination compensation modules. An asymmetric channel integration operation
is introduced in the former module, so as to combine global and local views by
leveraging high- and low-frequency information, ensuring the preservation of
fine and broader structural details. In addition, a color preservation unit is
proposed in the latter Retinex-based module, to provide multi-scale spatial and
frequency information, enabling accurate illumination estimation and
correction. Experimental results demonstrate that the proposed work not only
enhances visualization quality but also improves disease diagnosis performance
by restoring and correcting fine local details and uneven intensity. To the
best of our knowledge, this work is the first attempt for UWF image
enhancement, offering a robust and clinically valuable tool for improving
retinal disease management.

</details>


### [44] [SAT: Supervisor Regularization and Animation Augmentation for Two-process Monocular Texture 3D Human Reconstruction](https://arxiv.org/abs/2508.19688)
*Gangjian Zhang,Jian Shu,Nanjie Yao,Hao Wang*

Main category: cs.CV

TL;DR: SAT is a two-process 3D human reconstruction framework that integrates multiple geometric priors through Supervisor Feature Regularization and addresses data scarcity with Online Animation Augmentation, achieving superior results on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current monocular 3D human reconstruction methods struggle with geometric ambiguity from single 2D images, limited 3D training data, and ineffective integration of different geometric priors (SMPL, normal maps), leading to view inconsistencies and facial distortions.

Method: Proposes SAT framework with two processes: 1) Supervisor Feature Regularization module uses multi-view networks to provide intermediate feature supervision for better geometric prior fusion, 2) Online Animation Augmentation module creates a one-feed-forward animation network to generate massive training samples from original 3D data.

Result: Extensive experiments on two benchmarks demonstrate superiority over state-of-the-art methods, showing improved reconstruction quality and better handling of geometric priors.

Conclusion: SAT effectively addresses geometric ambiguity and data scarcity in monocular 3D human reconstruction by unifying geometric prior learning and augmenting training data, resulting in high-quality textured 3D avatars without view inconsistencies.

Abstract: Monocular texture 3D human reconstruction aims to create a complete 3D
digital avatar from just a single front-view human RGB image. However, the
geometric ambiguity inherent in a single 2D image and the scarcity of 3D human
training data are the main obstacles limiting progress in this field. To
address these issues, current methods employ prior geometric estimation
networks to derive various human geometric forms, such as the SMPL model and
normal maps. However, they struggle to integrate these modalities effectively,
leading to view inconsistencies, such as facial distortions. To this end, we
propose a two-process 3D human reconstruction framework, SAT, which seamlessly
learns various prior geometries in a unified manner and reconstructs
high-quality textured 3D avatars as the final output. To further facilitate
geometry learning, we introduce a Supervisor Feature Regularization module. By
employing a multi-view network with the same structure to provide intermediate
features as training supervision, these varied geometric priors can be better
fused. To tackle data scarcity and further improve reconstruction quality, we
also propose an Online Animation Augmentation module. By building a
one-feed-forward animation network, we augment a massive number of samples from
the original 3D human data online for model training. Extensive experiments on
two benchmarks show the superiority of our approach compared to
state-of-the-art methods.

</details>


### [45] [Synthetic Image Detection via Spectral Gaps of QC-RBIM Nishimori Bethe-Hessian Operators](https://arxiv.org/abs/2508.19698)
*V. S. Usatyuk,D. A. Sapozhnikov,S. I. Egorov*

Main category: cs.CV

TL;DR: Physics-inspired unsupervised detector uses graph theory and statistical physics to distinguish real from synthetic images without labeled training data, achieving 94%+ accuracy.


<details>
  <summary>Details</summary>
Motivation: Deep generative models create highly realistic synthetic images that undermine media forensics and biometric security. Supervised detectors fail on unseen generators, while existing unsupervised methods relying on low-level statistics are fragile.

Method: Treats synthetic-image detection as community detection on sparse weighted graphs. Extracts CNN features, reduces to 32D, constructs Multi-Edge Type QC-LDPC graph with pairwise similarities calibrated at Nishimori temperature. Analyzes Bethe-Hessian spectrum gaps in Random Bond Ising Model.

Result: Achieves over 94% accuracy on binary tasks (cat vs dog, male vs female) using real photos from FFHQ/CelebA and synthetic counterparts from GANs/diffusion models, without labeled synthetic data or retraining.

Conclusion: Novel LDPC graph construction with deep image features, analytical link between Nishimori temperature RBIM and Bethe-Hessian spectrum provides Bayes optimal detection, creating robust unsupervised detector for new generative architectures.

Abstract: The rapid advance of deep generative models such as GANs and diffusion
networks now produces images that are virtually indistinguishable from genuine
photographs, undermining media forensics and biometric security. Supervised
detectors quickly lose effectiveness on unseen generators or after adversarial
post-processing, while existing unsupervised methods that rely on low-level
statistical cues remain fragile. We introduce a physics-inspired,
model-agnostic detector that treats synthetic-image identification as a
community-detection problem on a sparse weighted graph. Image features are
first extracted with pretrained CNNs and reduced to 32 dimensions, each feature
vector becomes a node of a Multi-Edge Type QC-LDPC graph. Pairwise similarities
are transformed into edge couplings calibrated at the Nishimori temperature,
producing a Random Bond Ising Model (RBIM) whose Bethe-Hessian spectrum
exhibits a characteristic gap when genuine community structure (real images) is
present. Synthetic images violate the Nishimori symmetry and therefore lack
such gaps. We validate the approach on binary tasks cat versus dog and male
versus female using real photos from Flickr-Faces-HQ and CelebA and synthetic
counterparts generated by GANs and diffusion models. Without any labeled
synthetic data or retraining of the feature extractor, the detector achieves
over 94% accuracy. Spectral analysis shows multiple well separated gaps for
real image sets and a collapsed spectrum for generated ones. Our contributions
are threefold: a novel LDPC graph construction that embeds deep image features,
an analytical link between Nishimori temperature RBIM and the Bethe-Hessian
spectrum providing a Bayes optimal detection criterion; and a practical,
unsupervised synthetic image detector robust to new generative architectures.
Future work will extend the framework to video streams and multi-class anomaly
detection.

</details>


### [46] [LabelGS: Label-Aware 3D Gaussian Splatting for 3D Scene Segmentation](https://arxiv.org/abs/2508.19699)
*Yupeng Zhang,Dezhi Zheng,Ping Lu,Han Zhang,Lei Wang,Liping xiang,Cheng Luo,Kaijun Deng,Xiaowen Fu,Linlin Shen,Jinbao Wang*

Main category: cs.CV

TL;DR: LabelGS enhances 3D Gaussian Splatting with object segmentation capabilities by adding cross-view consistent semantic masks and novel occlusion handling, achieving 22x faster training than previous methods.


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian Splatting (3DGS) lacks 3D segmentation ability, limiting its applicability in scene understanding tasks that require identifying and isolating specific object components.

Method: Proposes LabelGS which augments Gaussian representation with object labels using cross-view consistent semantic masks, Occlusion Analysis Model to prevent overfitting, Main Gaussian Labeling model to lift 2D semantic prior to 3D, and Gaussian Projection Filter to avoid label conflicts. Uses random region sampling for efficient optimization.

Result: Outperforms previous state-of-the-art methods including Feature-3DGS in 3D scene segmentation, achieving 22x speedup in training at 1440X1080 resolution.

Conclusion: LabelGS successfully adds 3D segmentation capability to 3DGS while maintaining high efficiency, significantly advancing scene understanding applications.

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a novel explicit representation
for 3D scenes, offering both high-fidelity reconstruction and efficient
rendering. However, 3DGS lacks 3D segmentation ability, which limits its
applicability in tasks that require scene understanding. The identification and
isolating of specific object components is crucial. To address this limitation,
we propose Label-aware 3D Gaussian Splatting (LabelGS), a method that augments
the Gaussian representation with object label.LabelGS introduces cross-view
consistent semantic masks for 3D Gaussians and employs a novel Occlusion
Analysis Model to avoid overfitting occlusion during optimization, Main
Gaussian Labeling model to lift 2D semantic prior to 3D Gaussian and Gaussian
Projection Filter to avoid Gaussian label conflict. Our approach achieves
effective decoupling of Gaussian representations and refines the 3DGS
optimization process through a random region sampling strategy, significantly
improving efficiency. Extensive experiments demonstrate that LabelGS
outperforms previous state-of-the-art methods, including Feature-3DGS, in the
3D scene segmentation task. Notably, LabelGS achieves a remarkable 22X speedup
in training compared to Feature-3DGS, at a resolution of 1440X1080. Our code
will be at https://github.com/garrisonz/LabelGS.

</details>


### [47] [FreeVPS: Repurposing Training-Free SAM2 for Generalizable Video Polyp Segmentation](https://arxiv.org/abs/2508.19705)
*Qiang Hu,Ying Zhou,Gepeng Ji,Nick Barnes,Qiang Li,Zhiwei Wang*

Main category: cs.CV

TL;DR: FreeVPS is a training-free video polyp segmentation method that combines image polyp segmentation with SAM2's temporal modeling, using two modules to prevent error accumulation and achieve state-of-the-art performance in clinical colonoscopy videos.


<details>
  <summary>Details</summary>
Motivation: Existing video polyp segmentation methods struggle to balance spatiotemporal modeling and domain generalization, limiting their real-world clinical applicability in colonoscopy scenarios.

Method: Recasts VPS as track-by-detect paradigm using image polyp segmentation for spatial context and SAM2 for temporal modeling. Adds two training-free modules: intra-association filtering to eliminate spatial inaccuracies and reduce false positives, and inter-association refinement to update memory bank and prevent error propagation.

Result: Achieves cutting-edge performance in both in-domain and out-of-domain scenarios. Demonstrates robust tracking capabilities in long-untrimmed colonoscopy videos with enhanced temporal coherence and segmentation stability.

Conclusion: FreeVPS shows strong potential for reliable clinical analysis by effectively addressing error accumulation in long-term polyp tracking while maintaining domain generalization capabilities.

Abstract: Existing video polyp segmentation (VPS) paradigms usually struggle to balance
between spatiotemporal modeling and domain generalization, limiting their
applicability in real clinical scenarios. To embrace this challenge, we recast
the VPS task as a track-by-detect paradigm that leverages the spatial contexts
captured by the image polyp segmentation (IPS) model while integrating the
temporal modeling capabilities of segment anything model 2 (SAM2). However,
during long-term polyp tracking in colonoscopy videos, SAM2 suffers from error
accumulation, resulting in a snowball effect that compromises segmentation
stability. We mitigate this issue by repurposing SAM2 as a video polyp
segmenter with two training-free modules. In particular, the intra-association
filtering module eliminates spatial inaccuracies originating from the detecting
stage, reducing false positives. The inter-association refinement module
adaptively updates the memory bank to prevent error propagation over time,
enhancing temporal coherence. Both modules work synergistically to stabilize
SAM2, achieving cutting-edge performance in both in-domain and out-of-domain
scenarios. Furthermore, we demonstrate the robust tracking capabilities of
FreeVPS in long-untrimmed colonoscopy videos, underscoring its potential
reliable clinical analysis.

</details>


### [48] [Improving Generalization in Deepfake Detection with Face Foundation Models and Metric Learning](https://arxiv.org/abs/2508.19730)
*Stelios Mylonas,Symeon Papadopoulos*

Main category: cs.CV

TL;DR: Robust deepfake detection framework using face foundation models with triplet loss and attribution supervision for better generalization across diverse manipulation types.


<details>
  <summary>Details</summary>
Motivation: Deepfake detection models struggle to generalize beyond training distributions, especially for real-world media content, necessitating more robust approaches.

Method: Leverages FSFM self-supervised face foundation model, fine-tuned on ensemble of deepfake datasets with triplet loss variants and attribution-based supervision by manipulation type/source.

Result: Extensive experiments show effective performance, particularly in challenging real-world scenarios with strong generalization capabilities.

Conclusion: The framework demonstrates robust deepfake detection with improved generalization through foundation models and enhanced training techniques.

Abstract: The increasing realism and accessibility of deepfakes have raised critical
concerns about media authenticity and information integrity. Despite recent
advances, deepfake detection models often struggle to generalize beyond their
training distributions, particularly when applied to media content found in the
wild. In this work, we present a robust video deepfake detection framework with
strong generalization that takes advantage of the rich facial representations
learned by face foundation models. Our method is built on top of FSFM, a
self-supervised model trained on real face data, and is further fine-tuned
using an ensemble of deepfake datasets spanning both face-swapping and
face-reenactment manipulations. To enhance discriminative power, we incorporate
triplet loss variants during training, guiding the model to produce more
separable embeddings between real and fake samples. Additionally, we explore
attribution-based supervision schemes, where deepfakes are categorized by
manipulation type or source dataset, to assess their impact on generalization.
Extensive experiments across diverse evaluation benchmarks demonstrate the
effectiveness of our approach, especially in challenging real-world scenarios.

</details>


### [49] [POEv2: a flexible and robust framework for generic line segment detection and wireframe line segment detection](https://arxiv.org/abs/2508.19742)
*Chenguang Liu,Chisheng Wang,Yuhua Cai,Chuanhua Zhu,Qingquan Li*

Main category: cs.CV

TL;DR: POEv2 is a robust line segment detection framework that works for both generic and wireframe detection, achieving state-of-the-art performance when combined with efficient edge detectors.


<details>
  <summary>Details</summary>
Motivation: Existing line segment detectors are specialized for either generic detection (all meaningful segments) or wireframe detection (geometrically meaningful segments with large spatial support), but none work well for both tasks simultaneously.

Method: Improved version of Pixel Orientation Estimation (POE) method that detects line segments from edge strength maps and can be combined with any edge detector.

Result: Achieves state-of-the-art performance on three publicly available datasets when combined with an efficient edge detector.

Conclusion: POEv2 provides a unified framework that effectively handles both generic and wireframe line segment detection tasks, overcoming the limitations of specialized detectors.

Abstract: Line segment detection in images has been studied for several decades.
Existing line segment detectors can be roughly divided into two categories:
generic line segment detectors and wireframe line segment detectors. Generic
line segment detectors aim to detect all meaningful line segments in images and
traditional approaches usually fall into this category. Recent deep learning
based approaches are mostly wireframe line segment detectors. They detect only
line segments that are geometrically meaningful and have large spatial support.
Due to the difference in the aim of design, the performance of generic line
segment detectors for the task of wireframe line segment detection won't be
satisfactory, and vice versa. In this work, we propose a robust framework that
can be used for both generic line segment detection and wireframe line segment
detection. The proposed method is an improved version of the Pixel Orientation
Estimation (POE) method. It is thus named as POEv2. POEv2 detects line segments
from edge strength maps, and can be combined with any edge detector. We show in
our experiments that by combining the proposed POEv2 with an efficient edge
detector, it achieves state-of-the-art performance on three publicly available
datasets.

</details>


### [50] [SPLF-SAM: Self-Prompting Segment Anything Model for Light Field Salient Object Detection](https://arxiv.org/abs/2508.19746)
*Qiyao Xu,Qiming Wu,Xiaowei Li*

Main category: cs.CV

TL;DR: SPLF-SAM is a novel self-prompting light field segment anything model that addresses limitations in existing LF SOD methods by incorporating unified multi-scale feature embedding and frequency-domain analysis to better detect small objects and prevent noise interference.


<details>
  <summary>Details</summary>
Motivation: Existing Segment Anything Model (SAM) approaches for light field salient object detection neglect prompt information extraction and ignore frequency-domain analysis, causing small objects to be overwhelmed by noise.

Method: Proposes SPLF-SAM with two key components: Unified Multi-scale Feature Embedding Block (UMFEB) to identify multiple objects of varying sizes, and Multi-scale Adaptive Filtering Adapter (MAFA) that learns frequency features to prevent small objects from being overwhelmed by noise.

Result: Extensive experiments demonstrate superiority over ten state-of-the-art LF SOD methods, showing improved performance in detecting objects of various sizes while reducing noise interference.

Conclusion: SPLF-SAM effectively addresses the limitations of existing methods by incorporating prompt information extraction and frequency-domain analysis, achieving state-of-the-art performance in light field salient object detection tasks.

Abstract: Segment Anything Model (SAM) has demonstrated remarkable capabilities in
solving light field salient object detection (LF SOD). However, most existing
models tend to neglect the extraction of prompt information under this task.
Meanwhile, traditional models ignore the analysis of frequency-domain
information, which leads to small objects being overwhelmed by noise. In this
paper, we put forward a novel model called self-prompting light field segment
anything model (SPLF-SAM), equipped with unified multi-scale feature embedding
block (UMFEB) and a multi-scale adaptive filtering adapter (MAFA). UMFEB is
capable of identifying multiple objects of varying sizes, while MAFA, by
learning frequency features, effectively prevents small objects from being
overwhelmed by noise. Extensive experiments have demonstrated the superiority
of our method over ten state-of-the-art (SOTA) LF SOD methods. Our code will be
available at https://github.com/XucherCH/splfsam.

</details>


### [51] [FastAvatar: Towards Unified Fast High-Fidelity 3D Avatar Reconstruction with Large Gaussian Reconstruction Transformers](https://arxiv.org/abs/2508.19754)
*Yue Wu,Yufan Wu,Wen Li,Yuxi Lu,Kairui Feng,Xuanhong Chen*

Main category: cs.CV

TL;DR: FastAvatar is a feedforward 3D avatar reconstruction framework that uses a Large Gaussian Reconstruction Transformer to create high-quality 3D Gaussian Splatting models from diverse inputs (single image, multi-view, or video) within seconds using a single unified model.


<details>
  <summary>Details</summary>
Motivation: Current 3D avatar reconstruction methods suffer from high time complexity, sensitivity to data quality, and low data utilization. There's a need for a faster, more flexible approach that can effectively leverage various types of daily recordings.

Method: Uses a Large Gaussian Reconstruction Transformer with three key designs: 1) VGGT-style transformer architecture with 3D prompt injection, 2) multi-granular guidance encoding (camera pose, FLAME expression, head pose), 3) incremental Gaussian aggregation via landmark tracking and sliced fusion losses.

Result: FastAvatar achieves higher quality and highly competitive speed compared to existing methods, enabling incremental reconstruction that improves with more observations rather than wasting input data.

Conclusion: The framework provides a quality-speed-tunable paradigm for highly usable avatar modeling, offering fast reconstruction (within seconds) from diverse input types while maintaining high quality through incremental improvement capabilities.

Abstract: Despite significant progress in 3D avatar reconstruction, it still faces
challenges such as high time complexity, sensitivity to data quality, and low
data utilization. We propose FastAvatar, a feedforward 3D avatar framework
capable of flexibly leveraging diverse daily recordings (e.g., a single image,
multi-view observations, or monocular video) to reconstruct a high-quality 3D
Gaussian Splatting (3DGS) model within seconds, using only a single unified
model. FastAvatar's core is a Large Gaussian Reconstruction Transformer
featuring three key designs: First, a variant VGGT-style transformer
architecture aggregating multi-frame cues while injecting initial 3D prompt to
predict an aggregatable canonical 3DGS representation; Second, multi-granular
guidance encoding (camera pose, FLAME expression, head pose) mitigating
animation-induced misalignment for variable-length inputs; Third, incremental
Gaussian aggregation via landmark tracking and sliced fusion losses.
Integrating these features, FastAvatar enables incremental reconstruction,
i.e., improving quality with more observations, unlike prior work wasting input
data. This yields a quality-speed-tunable paradigm for highly usable avatar
modeling. Extensive experiments show that FastAvatar has higher quality and
highly competitive speed compared to existing methods.

</details>


### [52] [BuzzSet v1.0: A Dataset for Pollinator Detection in Field Conditions](https://arxiv.org/abs/2508.19762)
*Ahmed Emam,Mohamed Elbassiouny,Julius Miller,Patrick Donworth,Sabine Seidel,Ribana Roscher*

Main category: cs.CV

TL;DR: BuzzSet is a new large-scale dataset of 7856 high-resolution pollinator images with over 8000 annotated instances across honeybees, bumblebees, and unidentified insects, collected in real agricultural field conditions to support automated pollinator monitoring.


<details>
  <summary>Details</summary>
Motivation: Pollinator insects are vital to global food production but their populations are declining due to environmental stressors, requiring scalable automated monitoring solutions.

Method: Created BuzzSet dataset with manually verified images preprocessed into 256x256 tiles. Used YOLOv12 model for initial annotations and RF-DETR transformer-based object detector for baseline evaluation.

Result: Achieved high F1-scores of 0.94 for honeybees and 0.92 for bumblebees with minimal misclassification. Best mAP@0.50 of 0.559, though unidentified class remains challenging due to label ambiguity.

Conclusion: BuzzSet provides a valuable benchmark for small object detection, class separation under label noise, and ecological computer vision applications in pollinator monitoring.

Abstract: Pollinator insects such as honeybees and bumblebees are vital to global food
production and ecosystem stability, yet their populations are declining due to
increasing anthropogenic and environmental stressors. To support scalable,
automated pollinator monitoring, we introduce BuzzSet, a new large-scale
dataset of high-resolution pollinator images collected in real agricultural
field conditions. BuzzSet contains 7856 manually verified and labeled images,
with over 8000 annotated instances across three classes: honeybees, bumblebees,
and unidentified insects. Initial annotations were generated using a YOLOv12
model trained on external data and refined via human verification using
open-source labeling tools. All images were preprocessed into 256~$\times$~256
tiles to improve the detection of small insects. We provide strong baselines
using the RF-DETR transformer-based object detector. The model achieves high
F1-scores of 0.94 and 0.92 for honeybee and bumblebee classes, respectively,
with confusion matrix results showing minimal misclassification between these
categories. The unidentified class remains more challenging due to label
ambiguity and lower sample frequency, yet still contributes useful insights for
robustness evaluation. Overall detection quality is strong, with a best
mAP@0.50 of 0.559. BuzzSet offers a valuable benchmark for small object
detection, class separation under label noise, and ecological computer vision.

</details>


### [53] [AIM: Adaptive Intra-Network Modulation for Balanced Multimodal Learning](https://arxiv.org/abs/2508.19769)
*Shu Shen,C. L. Philip Chen,Tong Zhang*

Main category: cs.CV

TL;DR: AIM addresses optimization bias in imbalanced multimodal learning by adaptively modulating parameters across network depths, achieving balanced learning without hindering any modality.


<details>
  <summary>Details</summary>
Motivation: Existing methods for imbalanced multimodal learning typically hinder dominant modalities to promote weaker ones, which negatively impacts overall performance due to overlooked optimization bias within networks.

Method: Proposes Adaptive Intra-Network Modulation (AIM) that decouples under-optimized parameters into Auxiliary Blocks and encourages joint training with weaker modalities. It assesses modality imbalance across network depths and adaptively adjusts modulation strength at each depth.

Result: AIM outperforms state-of-the-art methods across multiple benchmarks and shows strong generalizability across different backbones, fusion strategies, and optimizers.

Conclusion: AIM successfully addresses optimization bias in multimodal networks, enabling balanced learning without performance degradation for any modality, representing a significant advancement in imbalanced multimodal learning.

Abstract: Multimodal learning has significantly enhanced machine learning performance
but still faces numerous challenges and limitations. Imbalanced multimodal
learning is one of the problems extensively studied in recent works and is
typically mitigated by modulating the learning of each modality. However, we
find that these methods typically hinder the dominant modality's learning to
promote weaker modalities, which affects overall multimodal performance. We
analyze the cause of this issue and highlight a commonly overlooked problem:
optimization bias within networks. To address this, we propose Adaptive
Intra-Network Modulation (AIM) to improve balanced modality learning. AIM
accounts for differences in optimization state across parameters and depths
within the network during modulation, achieving balanced multimodal learning
without hindering either dominant or weak modalities for the first time.
Specifically, AIM decouples the dominant modality's under-optimized parameters
into Auxiliary Blocks and encourages reliance on these performance-degraded
blocks for joint training with weaker modalities. This approach effectively
prevents suppression of weaker modalities while enabling targeted optimization
of under-optimized parameters to improve the dominant modality. Additionally,
AIM assesses modality imbalance level across network depths and adaptively
adjusts modulation strength at each depth. Experimental results demonstrate
that AIM outperforms state-of-the-art imbalanced modality learning methods
across multiple benchmarks and exhibits strong generalizability across
different backbones, fusion strategies, and optimizers.

</details>


### [54] [The Return of Structural Handwritten Mathematical Expression Recognition](https://arxiv.org/abs/2508.19773)
*Jakob Seitz,Tobias Lengfeld,Radu Timofte*

Main category: cs.CV

TL;DR: A structural recognition system for handwritten math expressions that provides explicit symbol-to-trace alignment through automatic annotation and modular processing, enabling better error analysis and interpretability.


<details>
  <summary>Details</summary>
Motivation: Existing encoder-decoder models lack explicit symbol-to-trace alignment, which is critical for error analysis, interpretability, and spatially aware interactive applications requiring selective content updates.

Method: Two innovations: 1) automatic annotation system using neural network to map LaTeX equations to raw traces for symbol segmentation/classification/spatial relations, 2) modular structural recognition system with independent optimization of segmentation, classification, and relation prediction using graph-based trace sorting, hybrid convolutional-recurrent network, and transformer-based correction.

Result: Achieves competitive performance on CROHME-2023 benchmark while generating complete graph structure that directly links handwritten traces to predicted symbols.

Conclusion: The structural recognition approach enables transparent error analysis and interpretable outputs by providing explicit alignment between symbols and traces, addressing limitations of traditional LaTeX generation models.

Abstract: Handwritten Mathematical Expression Recognition is foundational for
educational technologies, enabling applications like digital note-taking and
automated grading. While modern encoder-decoder architectures with large
language models excel at LaTeX generation, they lack explicit symbol-to-trace
alignment, a critical limitation for error analysis, interpretability, and
spatially aware interactive applications requiring selective content updates.
This paper introduces a structural recognition approach with two innovations: 1
an automatic annotation system that uses a neural network to map LaTeX
equations to raw traces, automatically generating annotations for symbol
segmentation, classification, and spatial relations, and 2 a modular structural
recognition system that independently optimizes segmentation, classification,
and relation prediction. By leveraging a dataset enriched with structural
annotations from our auto-labeling system, the proposed recognition system
combines graph-based trace sorting, a hybrid convolutional-recurrent network,
and transformer-based correction to achieve competitive performance on the
CROHME-2023 benchmark. Crucially, our structural recognition system generates a
complete graph structure that directly links handwritten traces to predicted
symbols, enabling transparent error analysis and interpretable outputs.

</details>


### [55] [MAPo : Motion-Aware Partitioning of Deformable 3D Gaussian Splatting for High-Fidelity Dynamic Scene Reconstruction](https://arxiv.org/abs/2508.19786)
*Han Jiao,Jiakai Sun,Yexing Xu,Lei Zhao,Wei Xing,Huaizhong Lin*

Main category: cs.CV

TL;DR: MAPo introduces motion-aware partitioning of 3D Gaussians to improve dynamic scene reconstruction by separating high/low dynamic regions and using specialized modeling for complex motions.


<details>
  <summary>Details</summary>
Motivation: Existing deformation-based methods for dynamic 3D Gaussian Splatting produce blurred renderings and lose fine motion details in highly dynamic regions due to limitations of unified models in representing diverse motion patterns.

Method: Dynamic score-based partitioning strategy that distinguishes high/low dynamic 3D Gaussians. For high-dynamic ones: recursive temporal partitioning with duplicated deformation networks per segment. For low-dynamic: treat as static. Cross-frame consistency loss to address visual discontinuities at partition boundaries.

Result: Superior rendering quality compared to baselines while maintaining comparable computational costs, particularly in regions with complex or rapid motions.

Conclusion: MAPo framework effectively addresses limitations of existing deformation-based methods by providing specialized modeling for different motion patterns, achieving high-fidelity dynamic scene reconstruction with better motion detail preservation.

Abstract: 3D Gaussian Splatting, known for enabling high-quality static scene
reconstruction with fast rendering, is increasingly being applied to dynamic
scene reconstruction. A common strategy involves learning a deformation field
to model the temporal changes of a canonical set of 3D Gaussians. However,
these deformation-based methods often produce blurred renderings and lose fine
motion details in highly dynamic regions due to the inherent limitations of a
single, unified model in representing diverse motion patterns. To address these
challenges, we introduce Motion-Aware Partitioning of Deformable 3D Gaussian
Splatting (MAPo), a novel framework for high-fidelity dynamic scene
reconstruction. Its core is a dynamic score-based partitioning strategy that
distinguishes between high- and low-dynamic 3D Gaussians. For high-dynamic 3D
Gaussians, we recursively partition them temporally and duplicate their
deformation networks for each new temporal segment, enabling specialized
modeling to capture intricate motion details. Concurrently, low-dynamic 3DGs
are treated as static to reduce computational costs. However, this temporal
partitioning strategy for high-dynamic 3DGs can introduce visual
discontinuities across frames at the partition boundaries. To address this, we
introduce a cross-frame consistency loss, which not only ensures visual
continuity but also further enhances rendering quality. Extensive experiments
demonstrate that MAPo achieves superior rendering quality compared to baselines
while maintaining comparable computational costs, particularly in regions with
complex or rapid motions.

</details>


### [56] [StableIntrinsic: Detail-preserving One-step Diffusion Model for Multi-view Material Estimation](https://arxiv.org/abs/2508.19789)
*Xiuchao Wu,Pengfei Zhu,Jiangjing Lyu,Xinguo Liu,Jie Guo,Yanwen Guo,Weiwei Xu,Chengfei Lyu*

Main category: cs.CV

TL;DR: StableIntrinsic is a one-step diffusion model for multi-view material estimation that produces high-quality, low-variance material parameters, outperforming previous methods with significant improvements in PSNR and MSE metrics.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion-based material estimation methods use multi-step denoising which is time-consuming and produces high-variance results due to stochastic inference conflicting with deterministic material estimation tasks.

Method: Uses a one-step diffusion model with pixel-space losses designed based on material properties, and introduces a Detail Injection Network (DIN) to eliminate detail loss from VAE encoding and enhance sharpness.

Result: Achieves 9.9% improvement in PSNR for albedo, and reduces MSE for metallic and roughness by 44.4% and 60.0% respectively, surpassing state-of-the-art techniques.

Conclusion: StableIntrinsic provides an efficient one-step solution for high-quality material estimation with reduced variance and improved performance metrics compared to existing multi-step diffusion approaches.

Abstract: Recovering material information from images has been extensively studied in
computer graphics and vision. Recent works in material estimation leverage
diffusion model showing promising results. However, these diffusion-based
methods adopt a multi-step denoising strategy, which is time-consuming for each
estimation. Such stochastic inference also conflicts with the deterministic
material estimation task, leading to a high variance estimated results. In this
paper, we introduce StableIntrinsic, a one-step diffusion model for multi-view
material estimation that can produce high-quality material parameters with low
variance. To address the overly-smoothing problem in one-step diffusion,
StableIntrinsic applies losses in pixel space, with each loss designed based on
the properties of the material. Additionally, StableIntrinsic introduces a
Detail Injection Network (DIN) to eliminate the detail loss caused by VAE
encoding, while further enhancing the sharpness of material prediction results.
The experimental results indicate that our method surpasses the current
state-of-the-art techniques by achieving a $9.9\%$ improvement in the Peak
Signal-to-Noise Ratio (PSNR) of albedo, and by reducing the Mean Square Error
(MSE) for metallic and roughness by $44.4\%$ and $60.0\%$, respectively.

</details>


### [57] [Not Every Gift Comes in Gold Paper or with a Red Ribbon: Exploring Color Perception in Text-to-Image Models](https://arxiv.org/abs/2508.19791)
*Shay Shomer Chai,Wenxuan Peng,Bharath Hariharan,Hadar Averbuch-Elor*

Main category: cs.CV

TL;DR: Text-to-image models struggle with multi-color prompts. This paper introduces a dedicated editing technique that significantly improves color alignment in generated images.


<details>
  <summary>Details</summary>
Motivation: Current text-to-image generation methods fail to accurately capture precise semantics in complex multi-object prompts, particularly with multiple color attributes. Existing evaluation metrics are coarse and human evaluations are difficult to scale.

Method: The authors perform a case study on color attributes and introduce a dedicated image editing technique specifically designed to address multi-object semantic alignment issues for prompts containing multiple colors.

Result: The approach significantly boosts performance across various metrics, outperforming existing inference-time techniques and editing methods for images generated by different text-to-image diffusion models.

Conclusion: Pretrained models struggle with multi-color prompts more than single-color ones, and the proposed dedicated editing technique effectively resolves semantic misalignments for complex multi-color scenarios.

Abstract: Text-to-image generation has recently seen remarkable success, granting users
with the ability to create high-quality images through the use of text.
However, contemporary methods face challenges in capturing the precise
semantics conveyed by complex multi-object prompts. Consequently, many works
have sought to mitigate such semantic misalignments, typically via
inference-time schemes that modify the attention layers of the denoising
networks. However, prior work has mostly utilized coarse metrics, such as the
cosine similarity between text and image CLIP embeddings, or human evaluations,
which are challenging to conduct on a larger-scale. In this work, we perform a
case study on colors -- a fundamental attribute commonly associated with
objects in text prompts, which offer a rich test bed for rigorous evaluation.
Our analysis reveals that pretrained models struggle to generate images that
faithfully reflect multiple color attributes-far more so than with single-color
prompts-and that neither inference-time techniques nor existing editing methods
reliably resolve these semantic misalignments. Accordingly, we introduce a
dedicated image editing technique, mitigating the issue of multi-object
semantic alignment for prompts containing multiple colors. We demonstrate that
our approach significantly boosts performance over a wide range of metrics,
considering images generated by various text-to-image diffusion-based
techniques.

</details>


### [58] [FusionSort: Enhanced Cluttered Waste Segmentation with Advanced Decoding and Comprehensive Modality Optimization](https://arxiv.org/abs/2508.19798)
*Muhammad Ali,Omar Ali AlSuwaidi*

Main category: cs.CV

TL;DR: Enhanced neural architecture with Comprehensive Attention Block and Mamba attention improves waste sorting accuracy by fusing multi-channel data using PCA transformation.


<details>
  <summary>Details</summary>
Motivation: Automating waste sorting for non-biodegradable materials is challenging due to complex and variable waste streams, requiring improved accuracy and efficiency.

Method: Encoder-Decoder structure enhanced with Comprehensive Attention Block, Mamba architecture for attention, and Data Fusion Block using PCA to reduce dimensionality of multi-channel images while preserving essential information.

Result: Outperforms existing methods significantly across RGB, hyperspectral, multispectral, and combined RGB-hyperspectral data evaluations.

Conclusion: The proposed neural architecture with integrated attention mechanisms and data fusion techniques provides a superior solution for automated waste sorting systems.

Abstract: In the realm of waste management, automating the sorting process for
non-biodegradable materials presents considerable challenges due to the
complexity and variability of waste streams. To address these challenges, we
introduce an enhanced neural architecture that builds upon an existing
Encoder-Decoder structure to improve the accuracy and efficiency of waste
sorting systems. Our model integrates several key innovations: a Comprehensive
Attention Block within the decoder, which refines feature representations by
combining convolutional and upsampling operations. In parallel, we utilize
attention through the Mamba architecture, providing an additional performance
boost. We also introduce a Data Fusion Block that fuses images with more than
three channels. To achieve this, we apply PCA transformation to reduce the
dimensionality while retaining the maximum variance and essential information
across three dimensions, which are then used for further processing. We
evaluated the model on RGB, hyperspectral, multispectral, and a combination of
RGB and hyperspectral data. The results demonstrate that our approach
outperforms existing methods by a significant margin.

</details>


### [59] [A bag of tricks for real-time Mitotic Figure detection](https://arxiv.org/abs/2508.19804)
*Christian Marzahl,Brian Napora*

Main category: cs.CV

TL;DR: A bag of tricks training approach for robust mitotic figure detection using RTMDet single-stage detector, achieving high F1 scores (0.78-0.84) across diverse domains with real-time inference suitable for clinical deployment.


<details>
  <summary>Details</summary>
Motivation: Mitotic figure detection faces challenges due to variations in slide scanners, staining protocols, tissue types, and artifacts. There's a need for robust, real-time detection methods that can generalize across diverse domains for clinical applications.

Method: Built on RTMDet single-stage object detector for efficiency. Uses multi-domain training data, balanced sampling, careful augmentation to address scanner variability and tumor heterogeneity. Implements hard negative mining on necrotic and debris tissue to reduce false positives.

Result: Achieved F1 scores between 0.78-0.84 in grouped 5-fold cross-validation across multiple datasets. On MIDOG 2025 challenge test set, reached F1 of 0.81, outperforming larger models and demonstrating strong domain generalization capabilities.

Conclusion: The proposed solution provides a practical balance between accuracy and speed, making it suitable for real-world clinical adoption. The bag of tricks approach enables robust performance across diverse domains while maintaining real-time inference capabilities.

Abstract: Mitotic figure (MF) detection in histopathology images is challenging due to
large variations in slide scanners, staining protocols, tissue types, and the
presence of artifacts. This paper presents a collection of training techniques
- a bag of tricks - that enable robust, real-time MF detection across diverse
domains. We build on the efficient RTMDet single stage object detector to
achieve high inference speed suitable for clinical deployment. Our method
addresses scanner variability and tumor heterogeneity via extensive
multi-domain training data, balanced sampling, and careful augmentation.
Additionally, we employ targeted, hard negative mining on necrotic and debris
tissue to reduce false positives. In a grouped 5-fold cross-validation across
multiple MF datasets, our model achieves an F1 score between 0.78 and 0.84. On
the preliminary test set of the MItosis DOmain Generalization (MIDOG) 2025
challenge, our single-stage RTMDet-S based approach reaches an F1 of 0.81,
outperforming larger models and demonstrating adaptability to new, unfamiliar
domains. The proposed solution offers a practical trade-off between accuracy
and speed, making it attractive for real-world clinical adoption.

</details>


### [60] [Context-aware Sparse Spatiotemporal Learning for Event-based Vision](https://arxiv.org/abs/2508.19806)
*Shenqi Wang,Guangzhi Tang*

Main category: cs.CV

TL;DR: CSSL framework uses context-aware thresholding to achieve high neuronal sparsity for event-based vision tasks without explicit sparsity constraints, enabling efficient neuromorphic processing.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning methods don't fully leverage event data sparsity, while neuromorphic approaches struggle with performance in complex tasks like object detection and optical flow. Achieving high activation sparsity requires manual tuning.

Method: Proposes Context-aware Sparse Spatiotemporal Learning (CSSL) with context-aware thresholding that dynamically regulates neuron activations based on input distribution.

Result: Achieves comparable or superior performance to state-of-the-art methods in event-based object detection and optical flow estimation while maintaining extremely high neuronal sparsity.

Conclusion: CSSL enables efficient event-based vision for neuromorphic processing by naturally reducing activation density without explicit sparsity constraints.

Abstract: Event-based camera has emerged as a promising paradigm for robot perception,
offering advantages with high temporal resolution, high dynamic range, and
robustness to motion blur. However, existing deep learning-based event
processing methods often fail to fully leverage the sparse nature of event
data, complicating their integration into resource-constrained edge
applications. While neuromorphic computing provides an energy-efficient
alternative, spiking neural networks struggle to match of performance of
state-of-the-art models in complex event-based vision tasks, like object
detection and optical flow. Moreover, achieving high activation sparsity in
neural networks is still difficult and often demands careful manual tuning of
sparsity-inducing loss terms. Here, we propose Context-aware Sparse
Spatiotemporal Learning (CSSL), a novel framework that introduces context-aware
thresholding to dynamically regulate neuron activations based on the input
distribution, naturally reducing activation density without explicit sparsity
constraints. Applied to event-based object detection and optical flow
estimation, CSSL achieves comparable or superior performance to
state-of-the-art methods while maintaining extremely high neuronal sparsity.
Our experimental results highlight CSSL's crucial role in enabling efficient
event-based vision for neuromorphic processing.

</details>


### [61] [AutoQ-VIS: Improving Unsupervised Video Instance Segmentation via Automatic Quality Assessment](https://arxiv.org/abs/2508.19808)
*Kaixuan Lu,Mehmet Onurcan Kaya,Dim P. Papadopoulos*

Main category: cs.CV

TL;DR: AutoQ-VIS is an unsupervised Video Instance Segmentation framework that uses quality-guided self-training to bridge the synthetic-to-real domain gap without human annotations, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Video Instance Segmentation requires pixel-level masks and temporal consistency labels, which are challenging to annotate. Existing unsupervised methods like VideoCutLER rely on synthetic data but suffer from the synthetic-to-real domain gap.

Method: Establishes a closed-loop system between pseudo-label generation and automatic quality assessment, enabling progressive adaptation from synthetic to real videos through quality-guided self-training.

Result: Achieves 52.6 AP50 on YouTubeVIS-2019 val set, surpassing previous state-of-the-art VideoCutLER by 4.4%, with no human annotations required.

Conclusion: Demonstrates the viability of quality-aware self-training for unsupervised VIS, effectively bridging the synthetic-to-real domain gap through automated quality assessment and progressive adaptation.

Abstract: Video Instance Segmentation (VIS) faces significant annotation challenges due
to its dual requirements of pixel-level masks and temporal consistency labels.
While recent unsupervised methods like VideoCutLER eliminate optical flow
dependencies through synthetic data, they remain constrained by the
synthetic-to-real domain gap. We present AutoQ-VIS, a novel unsupervised
framework that bridges this gap through quality-guided self-training. Our
approach establishes a closed-loop system between pseudo-label generation and
automatic quality assessment, enabling progressive adaptation from synthetic to
real videos. Experiments demonstrate state-of-the-art performance with 52.6
$\text{AP}_{50}$ on YouTubeVIS-2019 val set, surpassing the previous
state-of-the-art VideoCutLER by 4.4$\%$, while requiring no human annotations.
This demonstrates the viability of quality-aware self-training for unsupervised
VIS. The source code of our method is available at
https://github.com/wcbup/AutoQ-VIS.

</details>


### [62] [ERSR: An Ellipse-constrained pseudo-label refinement and symmetric regularization framework for semi-supervised fetal head segmentation in ultrasound images](https://arxiv.org/abs/2508.19815)
*Linkuan Zhou,Zhexin Chen,Yufei Shen,Junlin Xu,Ping Xuan,Yixin Zhu,Yuqi Fang,Cong Cong,Leyi Wei,Ran Su,Jia Zhou,Qiangguo Jin*

Main category: cs.CV

TL;DR: ERSR framework for semi-supervised fetal head ultrasound segmentation using dual-scoring filtering, ellipse-constrained refinement, and symmetry-based consistency regularization


<details>
  <summary>Details</summary>
Motivation: Automated fetal head segmentation in ultrasound is critical for prenatal monitoring but challenging due to poor image quality and lack of annotated data. Semi-supervised methods struggle with reliable pseudo-label generation and effective consistency constraints for fetal head ultrasound images.

Method: Proposed ERSR framework with three components: 1) Dual-scoring adaptive filtering strategy using boundary consistency and contour regularity criteria, 2) Ellipse-constrained pseudo-label refinement via least-squares ellipse fitting, 3) Symmetry-based multiple consistency regularization across perturbed images, symmetric regions, and between predictions/pseudo-labels.

Result: State-of-the-art performance: HC18 dataset - 92.05% Dice with 10% labeled data, 95.36% with 20% labeled data; PSFH dataset - 91.68% with 10% labeled data, 93.70% with 20% labeled data.

Conclusion: ERSR effectively addresses challenges in semi-supervised fetal head ultrasound segmentation by generating reliable pseudo-labels and enforcing robust consistency constraints, achieving superior performance on benchmark datasets.

Abstract: Automated segmentation of the fetal head in ultrasound images is critical for
prenatal monitoring. However, achieving robust segmentation remains challenging
due to the poor quality of ultrasound images and the lack of annotated data.
Semi-supervised methods alleviate the lack of annotated data but struggle with
the unique characteristics of fetal head ultrasound images, making it
challenging to generate reliable pseudo-labels and enforce effective
consistency regularization constraints. To address this issue, we propose a
novel semi-supervised framework, ERSR, for fetal head ultrasound segmentation.
Our framework consists of the dual-scoring adaptive filtering strategy, the
ellipse-constrained pseudo-label refinement, and the symmetry-based multiple
consistency regularization. The dual-scoring adaptive filtering strategy uses
boundary consistency and contour regularity criteria to evaluate and filter
teacher outputs. The ellipse-constrained pseudo-label refinement refines these
filtered outputs by fitting least-squares ellipses, which strengthens pixels
near the center of the fitted ellipse and suppresses noise simultaneously. The
symmetry-based multiple consistency regularization enforces multi-level
consistency across perturbed images, symmetric regions, and between original
predictions and pseudo-labels, enabling the model to capture robust and stable
shape representations. Our method achieves state-of-the-art performance on two
benchmarks. On the HC18 dataset, it reaches Dice scores of 92.05% and 95.36%
with 10% and 20% labeled data, respectively. On the PSFH dataset, the scores
are 91.68% and 93.70% under the same settings.

</details>


### [63] [Gradient Rectification for Robust Calibration under Distribution Shift](https://arxiv.org/abs/2508.19830)
*Yilin Zhang,Cai Xu,You Wu,Ziyu Guan,Wei Zhao*

Main category: cs.CV

TL;DR: Proposes a novel calibration framework that improves model reliability under distribution shift without requiring target domain information, using low-frequency filtering and gradient-based rectification.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks produce overconfident predictions that become worse under distribution shift, and existing methods require impractical access to target domain information.

Method: Uses low-frequency filtering to encourage reliance on domain-invariant features, combined with a gradient-based rectification mechanism to enforce in-distribution calibration as a hard constraint.

Result: Significantly improves calibration under distribution shift on synthetic and real-world datasets (CIFAR-10/100-C, WILDS) while maintaining strong in-distribution performance.

Conclusion: The proposed framework effectively addresses calibration issues under distribution shift without requiring target domain information, making it practical for real-world applications.

Abstract: Deep neural networks often produce overconfident predictions, undermining
their reliability in safety-critical applications. This miscalibration is
further exacerbated under distribution shift, where test data deviates from the
training distribution due to environmental or acquisition changes. While
existing approaches improve calibration through training-time regularization or
post-hoc adjustment, their reliance on access to or simulation of target
domains limits their practicality in real-world scenarios. In this paper, we
propose a novel calibration framework that operates without access to target
domain information. From a frequency-domain perspective, we identify that
distribution shifts often distort high-frequency visual cues exploited by deep
models, and introduce a low-frequency filtering strategy to encourage reliance
on domain-invariant features. However, such information loss may degrade
In-Distribution (ID) calibration performance. Therefore, we further propose a
gradient-based rectification mechanism that enforces ID calibration as a hard
constraint during optimization. Experiments on synthetic and real-world shifted
datasets, including CIFAR-10/100-C and WILDS, demonstrate that our method
significantly improves calibration under distribution shift while maintaining
strong in-distribution performance.

</details>


### [64] [Image Quality Assessment for Machines: Paradigm, Large-scale Database, and Models](https://arxiv.org/abs/2508.19850)
*Xiaoqi Wang,Yun Zhang,Weisi Lin*

Main category: cs.CV

TL;DR: Proposes machine-centric image quality assessment (MIQA) framework to quantify how image degradations affect machine vision systems, outperforming human visual system metrics.


<details>
  <summary>Details</summary>
Motivation: Machine vision systems are vulnerable to performance degradation under adverse visual conditions, but existing human visual system-based quality metrics are inadequate for predicting machine performance.

Method: Established MIQA paradigm with 2.5M-sample database covering 75 vision models, 250 degradation types, and 3 vision tasks. Proposed region-aware MIQA (RA-MIQA) model for fine-grained spatial degradation analysis.

Result: RA-MIQA achieved SRCC gains of 13.56% on consistency and 13.37% on accuracy for image classification, significantly outperforming HVS-based metrics and retrained classical backbones.

Conclusion: HVS-based metrics are inadequate for MVS quality prediction, and specialized MIQA models still struggle with certain challenges. This work advances MVS reliability and establishes foundations for machine-centric image processing.

Abstract: Machine vision systems (MVS) are intrinsically vulnerable to performance
degradation under adverse visual conditions. To address this, we propose a
machine-centric image quality assessment (MIQA) framework that quantifies the
impact of image degradations on MVS performance. We establish an MIQA paradigm
encompassing the end-to-end assessment workflow. To support this, we construct
a machine-centric image quality database (MIQD-2.5M), comprising 2.5 million
samples that capture distinctive degradation responses in both consistency and
accuracy metrics, spanning 75 vision models, 250 degradation types, and three
representative vision tasks. We further propose a region-aware MIQA (RA-MIQA)
model to evaluate MVS visual quality through fine-grained spatial degradation
analysis. Extensive experiments benchmark the proposed RA-MIQA against seven
human visual system (HVS)-based IQA metrics and five retrained classical
backbones. Results demonstrate RA-MIQA's superior performance in multiple
dimensions, e.g., achieving SRCC gains of 13.56% on consistency and 13.37% on
accuracy for image classification, while also revealing task-specific
degradation sensitivities. Critically, HVS-based metrics prove inadequate for
MVS quality prediction, while even specialized MIQA models struggle with
background degradations, accuracy-oriented estimation, and subtle distortions.
This study can advance MVS reliability and establish foundations for
machine-centric image processing and optimization. The model and code are
available at: https://github.com/XiaoqiWang/MIQA.

</details>


### [65] [Ego-centric Predictive Model Conditioned on Hand Trajectories](https://arxiv.org/abs/2508.19852)
*Binjie Zhang,Mike Zheng Shou*

Main category: cs.CV

TL;DR: A unified two-stage framework that jointly predicts next actions and their visual outcomes in egocentric scenarios using hand trajectories and multi-modal fusion with a Latent Diffusion Model.


<details>
  <summary>Details</summary>
Motivation: Existing approaches either predict actions without visual consequences (VLA models) or generate future frames without action conditioning (video prediction models), leading to incomplete or implausible results in egocentric human-object interaction scenarios.

Method: Two-stage approach: 1) Consecutive state modeling processes visual observations, language, and action history to predict future hand trajectories; 2) Causal cross-attention fuses multi-modal cues to guide a Latent Diffusion Model for frame-by-frame future video generation.

Result: Outperforms state-of-the-art baselines on Ego4D, BridgeData, and RLBench benchmarks in both action prediction and future video synthesis tasks.

Conclusion: The proposed unified framework successfully bridges the gap between action prediction and visual outcome modeling, providing explicit predictions of both upcoming actions and their visual consequences for egocentric human activity understanding and robotic manipulation tasks.

Abstract: In egocentric scenarios, anticipating both the next action and its visual
outcome is essential for understanding human-object interactions and for
enabling robotic planning. However, existing paradigms fall short of jointly
modeling these aspects. Vision-Language-Action (VLA) models focus on action
prediction but lack explicit modeling of how actions influence the visual
scene, while video prediction models generate future frames without
conditioning on specific actions, often resulting in implausible or
contextually inconsistent outcomes. To bridge this gap, we propose a unified
two-stage predictive framework that jointly models action and visual future in
egocentric scenarios, conditioned on hand trajectories. In the first stage, we
perform consecutive state modeling to process heterogeneous inputs (visual
observations, language, and action history) and explicitly predict future hand
trajectories. In the second stage, we introduce causal cross-attention to fuse
multi-modal cues, leveraging inferred action signals to guide an image-based
Latent Diffusion Model (LDM) for frame-by-frame future video generation. Our
approach is the first unified model designed to handle both egocentric human
activity understanding and robotic manipulation tasks, providing explicit
predictions of both upcoming actions and their visual consequences. Extensive
experiments on Ego4D, BridgeData, and RLBench demonstrate that our method
outperforms state-of-the-art baselines in both action prediction and future
video synthesis.

</details>


### [66] [Multimodal Conditional MeshGAN for Personalized Aneurysm Growth Prediction](https://arxiv.org/abs/2508.19862)
*Long Chen,Ashiv Patel,Mengyun Qiao,Mohammad Yousuf Salmasi,Salah A. Hammouche,Vasilis Stavrinides,Jasleen Nagi,Soodeh Kalaie,Xiao Yun Xu,Wenjia Bai,Declan P. O'Regan*

Main category: cs.CV

TL;DR: MCMeshGAN is a novel multimodal conditional mesh-to-mesh GAN that predicts 3D aortic aneurysm growth by combining local geometric detail preservation with global structural context modeling, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Accurate prediction of aortic aneurysm progression is clinically essential but challenging due to the need to model both subtle local deformations and global anatomical changes in complex 3D geometries.

Method: Dual-branch architecture with local KNN-based convolutional network (KCN) for fine-grained details and global graph convolutional network (GCN) for structural context, plus condition branch for clinical attributes and time intervals.

Result: Outperforms state-of-the-art baselines in both geometric accuracy and clinically important diameter estimation on the new TAAMesh dataset (590 multimodal records from 208 patients).

Conclusion: Provides a robust framework for clinically deployable, personalized 3D disease trajectory modeling with publicly available source code.

Abstract: Personalized, accurate prediction of aortic aneurysm progression is essential
for timely intervention but remains challenging due to the need to model both
subtle local deformations and global anatomical changes within complex 3D
geometries. We propose MCMeshGAN, the first multimodal conditional mesh-to-mesh
generative adversarial network for 3D aneurysm growth prediction. MCMeshGAN
introduces a dual-branch architecture combining a novel local KNN-based
convolutional network (KCN) to preserve fine-grained geometric details and a
global graph convolutional network (GCN) to capture long-range structural
context, overcoming the over-smoothing limitations of deep GCNs. A dedicated
condition branch encodes clinical attributes (age, sex) and the target time
interval to generate anatomically plausible, temporally controlled predictions,
enabling retrospective and prospective modeling. We curated TAAMesh, a new
longitudinal thoracic aortic aneurysm mesh dataset consisting of 590 multimodal
records (CT scans, 3D meshes, and clinical data) from 208 patients. Extensive
experiments demonstrate that MCMeshGAN consistently outperforms
state-of-the-art baselines in both geometric accuracy and clinically important
diameter estimation. This framework offers a robust step toward clinically
deployable, personalized 3D disease trajectory modeling. The source code for
MCMeshGAN and the baseline methods is publicly available at
https://github.com/ImperialCollegeLondon/MCMeshGAN.

</details>


### [67] [Self-supervised structured object representation learning](https://arxiv.org/abs/2508.19864)
*Oussama Hadjerci,Antoine Letienne,Mohamed Abbas Hedjazi,Adel Hafiane*

Main category: cs.CV

TL;DR: A self-supervised learning approach that builds structured visual representations through semantic grouping, instance separation, and hierarchical structuring using a novel ProtoScale module, outperforming state-of-the-art methods in object detection tasks.


<details>
  <summary>Details</summary>
Motivation: Current SSL approaches excel at global image understanding but lack structured scene representation capabilities needed for dense prediction tasks like object detection.

Method: Proposes a self-supervised approach with ProtoScale module that combines semantic grouping, instance level separation, and hierarchical structuring while preserving full scene context across augmented views.

Result: Outperforms state-of-the-art methods in object detection tasks (COCO and UA-DETRAC datasets), learns object-centric representations, and works well with limited annotated data and fewer fine-tuning epochs.

Conclusion: The proposed SSL approach successfully captures structured visual representations across multiple spatial scales, demonstrating superior performance in object detection tasks compared to existing methods.

Abstract: Self-supervised learning (SSL) has emerged as a powerful technique for
learning visual representations. While recent SSL approaches achieve strong
results in global image understanding, they are limited in capturing the
structured representation in scenes. In this work, we propose a self-supervised
approach that progressively builds structured visual representations by
combining semantic grouping, instance level separation, and hierarchical
structuring. Our approach, based on a novel ProtoScale module, captures visual
elements across multiple spatial scales. Unlike common strategies like DINO
that rely on random cropping and global embeddings, we preserve full scene
context across augmented views to improve performance in dense prediction
tasks. We validate our method on downstream object detection tasks using a
combined subset of multiple datasets (COCO and UA-DETRAC). Experimental results
show that our method learns object centric representations that enhance
supervised object detection and outperform the state-of-the-art methods, even
when trained with limited annotated data and fewer fine-tuning epochs.

</details>


### [68] [TrajFusionNet: Pedestrian Crossing Intention Prediction via Fusion of Sequential and Visual Trajectory Representations](https://arxiv.org/abs/2508.19866)
*François G. Landry,Moulay A. Akhloufi*

Main category: cs.CV

TL;DR: TrajFusionNet is a transformer-based model that combines future pedestrian trajectory and vehicle speed predictions to predict pedestrian crossing intention, achieving state-of-the-art performance with the lowest inference time.


<details>
  <summary>Details</summary>
Motivation: With autonomous vehicles on public roads, accurately predicting pedestrian crossing intention is crucial for safety. Current approaches need improvement in both accuracy and computational efficiency.

Method: Proposes TrajFusionNet with two branches: Sequence Attention Module (SAM) for sequential trajectory/speed learning and Visual Attention Module (VAM) for visual representation of predicted trajectories overlaid on scene images.

Result: Achieves state-of-the-art results across three commonly used datasets and has the lowest total inference time (including model runtime and data preprocessing) among current approaches.

Conclusion: TrajFusionNet demonstrates that combining lightweight modalities with transformer architecture can achieve superior performance and efficiency in pedestrian crossing intention prediction.

Abstract: With the introduction of vehicles with autonomous capabilities on public
roads, predicting pedestrian crossing intention has emerged as an active area
of research. The task of predicting pedestrian crossing intention involves
determining whether pedestrians in the scene are likely to cross the road or
not. In this work, we propose TrajFusionNet, a novel transformer-based model
that combines future pedestrian trajectory and vehicle speed predictions as
priors for predicting crossing intention. TrajFusionNet comprises two branches:
a Sequence Attention Module (SAM) and a Visual Attention Module (VAM). The SAM
branch learns from a sequential representation of the observed and predicted
pedestrian trajectory and vehicle speed. Complementarily, the VAM branch
enables learning from a visual representation of the predicted pedestrian
trajectory by overlaying predicted pedestrian bounding boxes onto scene images.
By utilizing a small number of lightweight modalities, TrajFusionNet achieves
the lowest total inference time (including model runtime and data
preprocessing) among current state-of-the-art approaches. In terms of
performance, it achieves state-of-the-art results across the three most
commonly used datasets for pedestrian crossing intention prediction.

</details>


### [69] [Sky Background Building of Multi-objective Fiber spectra Based on Mutual Information Network](https://arxiv.org/abs/2508.19875)
*Hui Zhang,Jianghui Cai,Haifeng Yang,Ali Luo,Yuqing Yang,Xiao Kong,Zhichao Ding,Lichan Zhou,Qin Han*

Main category: cs.CV

TL;DR: SMI is a sky background estimation model using mutual information and incremental training to improve sky subtraction in fiber spectra by leveraging all fibers in a plate rather than just sky fibers.


<details>
  <summary>Details</summary>
Motivation: Current sky background subtraction methods rely mainly on sky fiber spectra to build Super Sky, which lacks proper modeling of the environment surrounding astronomical objects.

Method: SMI uses two networks: first applies wavelength calibration to extract sky features and solve feature shift problems; second uses incremental training to maximize mutual information between different spectra for common components and minimize mutual information between adjoining spectra for individual components.

Result: Experiments on LAMOST spectra show SMI obtains better object sky background, especially in the blue end of the spectrum.

Conclusion: SMI effectively addresses limitations of traditional sky subtraction methods by utilizing mutual information and incremental training across all fibers to produce more accurate individual sky background estimates.

Abstract: Sky background subtraction is a critical step in Multi-objective Fiber
spectra process. However, current subtraction relies mainly on sky fiber
spectra to build Super Sky. These average spectra are lacking in the modeling
of the environment surrounding the objects. To address this issue, a sky
background estimation model: Sky background building based on Mutual
Information (SMI) is proposed. SMI based on mutual information and incremental
training approach. It utilizes spectra from all fibers in the plate to estimate
the sky background. SMI contains two main networks, the first network applies a
wavelength calibration module to extract sky features from spectra, and can
effectively solve the feature shift problem according to the corresponding
emission position. The second network employs an incremental training approach
to maximize mutual information between representations of different spectra to
capturing the common component. Then, it minimizes the mutual information
between adjoining spectra representations to obtain individual components. This
network yields an individual sky background at each location of the object. To
verify the effectiveness of the method in this paper, we conducted experiments
on the spectra of LAMOST. Results show that SMI can obtain a better object sky
background during the observation, especially in the blue end.

</details>


### [70] [Multispectral LiDAR data for extracting tree points in urban and suburban areas](https://arxiv.org/abs/2508.19881)
*Narges Takhtkeshha,Gabriele Mazzacca,Fabio Remondino,Juha Hyyppä,Gottfried Mandlburger*

Main category: cs.CV

TL;DR: MS-LiDAR combined with deep learning models (SPT, PTv3, PTv1) achieves high accuracy (85.28% mIoU) for urban tree extraction, with SPT showing best time efficiency and pNDVI integration reducing error by 10.61pp.


<details>
  <summary>Details</summary>
Motivation: Monitoring urban trees is crucial for greening policies and electrical infrastructure safety, but complex urban environments and tree variability present challenges for traditional airborne laser scanning methods.

Method: Used multispectral LiDAR (MS-LiDAR) to capture 3D spatial and spectral data, evaluated three deep learning models (Superpoint Transformer, Point Transformer V3, and Point Transformer V1) for tree point extraction, and incorporated pseudo normalized difference vegetation index (pNDVI) with spatial data.

Result: SPT demonstrated notable time efficiency and accuracy with 85.28% mIoU. Highest detection accuracy achieved by combining pNDVI with spatial data, reducing error rate by 10.61 percentage points compared to spatial-only approach.

Conclusion: MS-LiDAR combined with deep learning has strong potential to improve urban tree extraction and support more accurate tree inventories, with spectral data integration significantly enhancing detection accuracy.

Abstract: Monitoring urban tree dynamics is vital for supporting greening policies and
reducing risks to electrical infrastructure. Airborne laser scanning has
advanced large-scale tree management, but challenges remain due to complex
urban environments and tree variability. Multispectral (MS) light detection and
ranging (LiDAR) improves this by capturing both 3D spatial and spectral data,
enabling detailed mapping. This study explores tree point extraction using
MS-LiDAR and deep learning (DL) models. Three state-of-the-art models are
evaluated: Superpoint Transformer (SPT), Point Transformer V3 (PTv3), and Point
Transformer V1 (PTv1). Results show the notable time efficiency and accuracy of
SPT, with a mean intersection over union (mIoU) of 85.28%. The highest
detection accuracy is achieved by incorporating pseudo normalized difference
vegetation index (pNDVI) with spatial data, reducing error rate by 10.61
percentage points (pp) compared to using spatial information alone. These
findings highlight the potential of MS-LiDAR and DL to improve tree extraction
and further tree inventories.

</details>


### [71] [PersonaAnimator: Personalized Motion Transfer from Unconstrained Videos](https://arxiv.org/abs/2508.19895)
*Ziyun Qian,Runyu Xiao,Shuyuan Tu,Wei Xue,Dingkang Yang,Mingcheng Li,Dongliang Kou,Minghao Han,Zizhi Chen,Lihua Zhang*

Main category: cs.CV

TL;DR: PersonaAnimator: A novel framework for video-to-video motion personalization that learns personalized motion patterns from unconstrained videos, addressing limitations in style transfer, data dependency, and physical plausibility.


<details>
  <summary>Details</summary>
Motivation: Existing motion generation methods have three key limitations: (1) pose-guided methods only replicate motion without learning style characteristics, (2) style transfer relies heavily on hard-to-obtain motion capture data, and (3) generated motions sometimes violate physical laws.

Method: Proposes PersonaAnimator framework that learns personalized motion patterns directly from unconstrained videos. Introduces PersonaVid dataset with 20 motion content and 120 style categories. Uses Physics-aware Motion Style Regularization to ensure physical plausibility.

Result: Extensive experiments show PersonaAnimator outperforms state-of-the-art motion transfer methods and sets a new benchmark for Video-to-Video Motion Personalization.

Conclusion: The paper pioneers video-to-video motion personalization, demonstrating successful learning of personalized motion patterns from videos while ensuring physical plausibility, overcoming previous limitations in motion generation.

Abstract: Recent advances in motion generation show remarkable progress. However,
several limitations remain: (1) Existing pose-guided character motion transfer
methods merely replicate motion without learning its style characteristics,
resulting in inexpressive characters. (2) Motion style transfer methods rely
heavily on motion capture data, which is difficult to obtain. (3) Generated
motions sometimes violate physical laws. To address these challenges, this
paper pioneers a new task: Video-to-Video Motion Personalization. We propose a
novel framework, PersonaAnimator, which learns personalized motion patterns
directly from unconstrained videos. This enables personalized motion transfer.
To support this task, we introduce PersonaVid, the first video-based
personalized motion dataset. It contains 20 motion content categories and 120
motion style categories. We further propose a Physics-aware Motion Style
Regularization mechanism to enforce physical plausibility in the generated
motions. Extensive experiments show that PersonaAnimator outperforms
state-of-the-art motion transfer methods and sets a new benchmark for the
Video-to-Video Motion Personalization task.

</details>


### [72] [Hyperspectral Sensors and Autonomous Driving: Technologies, Limitations, and Opportunities](https://arxiv.org/abs/2508.19905)
*Imad Ali Shah,Jiarong Li,Roshan George,Tim Brophy,Enda Ward,Martin Glavin,Edward Jones,Brian Deegan*

Main category: cs.CV

TL;DR: First comprehensive review of hyperspectral imaging for automotive applications reveals significant gap between research potential and commercial readiness, with only 4 cameras meeting performance thresholds and none complying with automotive temperature standards.


<details>
  <summary>Details</summary>
Motivation: Hyperspectral imaging offers transformative sensing for ADAS/autonomous driving by enabling material-level scene understanding beyond RGB capabilities, but its practical automotive integration needs assessment.

Method: Comprehensive qualitative review of HSI technologies plus quantitative analysis of 216 commercial HSI/multispectral cameras benchmarked against automotive criteria (frame rate, spatial resolution, spectral dimensionality, AEC-Q100 compliance). Also reviewed recent HSI datasets and applications.

Result: Only 4 cameras meet defined performance thresholds, none comply with AEC-Q100 requirements. Current HSI datasets are limited in scale, spectral consistency, channel count, and environmental diversity, posing challenges for algorithm development and validation.

Conclusion: HSI shows great research potential but faces significant commercial readiness challenges for automotive integration. The review establishes current state and outlines key research directions for practical spectral imaging in ADAS/autonomous systems.

Abstract: Hyperspectral imaging (HSI) offers a transformative sensing modality for
Advanced Driver Assistance Systems (ADAS) and autonomous driving (AD)
applications, enabling material-level scene understanding through fine spectral
resolution beyond the capabilities of traditional RGB imaging. This paper
presents the first comprehensive review of HSI for automotive applications,
examining the strengths, limitations, and suitability of current HSI
technologies in the context of ADAS/AD. In addition to this qualitative review,
we analyze 216 commercially available HSI and multispectral imaging cameras,
benchmarking them against key automotive criteria: frame rate, spatial
resolution, spectral dimensionality, and compliance with AEC-Q100 temperature
standards. Our analysis reveals a significant gap between HSI's demonstrated
research potential and its commercial readiness. Only four cameras meet the
defined performance thresholds, and none comply with AEC-Q100 requirements. In
addition, the paper reviews recent HSI datasets and applications, including
semantic segmentation for road surface classification, pedestrian separability,
and adverse weather perception. Our review shows that current HSI datasets are
limited in terms of scale, spectral consistency, the number of spectral
channels, and environmental diversity, posing challenges for the development of
perception algorithms and the adequate validation of HSI's true potential in
ADAS/AD applications. This review paper establishes the current state of HSI in
automotive contexts as of 2025 and outlines key research directions toward
practical integration of spectral imaging in ADAS and autonomous systems.

</details>


### [73] [Streamlining the Development of Active Learning Methods in Real-World Object Detection](https://arxiv.org/abs/2508.19906)
*Moussa Kassem Sbeyti,Nadja Klein,Michelle Karg,Christian Wirth,Sahin Albayrak*

Main category: cs.CV

TL;DR: OSS metric enables efficient AL method evaluation without detector training by measuring object-level similarity between training sets and target domains, addressing computational costs and reliability issues in object detection.


<details>
  <summary>Details</summary>
Motivation: Active learning for object detection faces high computational costs (up to 282 GPU hours per detector) and unreliable method rankings across validation sets, limiting practical deployment in safety-critical applications like autonomous driving.

Method: Proposes object-based set similarity (OSS) metric that quantifies AL method effectiveness without requiring detector training by measuring similarity between training sets and target domains using object-level features.

Result: Validated on three autonomous driving datasets (KITTI, BDD100K, CODA) with uncertainty-based AL methods and two detector architectures (EfficientDet, YOLOv3), showing OSS enables elimination of ineffective methods before training and selection of representative validation sets.

Conclusion: OSS provides a detector-agnostic, computationally efficient framework for AL deployment that requires only labeled object crops and integrates with existing pipelines, addressing critical computational and reliability challenges in real-world object detection applications.

Abstract: Active learning (AL) for real-world object detection faces computational and
reliability challenges that limit practical deployment. Developing new AL
methods requires training multiple detectors across iterations to compare
against existing approaches. This creates high costs for autonomous driving
datasets where the training of one detector requires up to 282 GPU hours.
Additionally, AL method rankings vary substantially across validation sets,
compromising reliability in safety-critical transportation systems. We
introduce object-based set similarity ($\mathrm{OSS}$), a metric that addresses
these challenges. $\mathrm{OSS}$ (1) quantifies AL method effectiveness without
requiring detector training by measuring similarity between training sets and
target domains using object-level features. This enables the elimination of
ineffective AL methods before training. Furthermore, $\mathrm{OSS}$ (2) enables
the selection of representative validation sets for robust evaluation. We
validate our similarity-based approach on three autonomous driving datasets
(KITTI, BDD100K, CODA) using uncertainty-based AL methods as a case study with
two detector architectures (EfficientDet, YOLOv3). This work is the first to
unify AL training and evaluation strategies in object detection based on object
similarity. $\mathrm{OSS}$ is detector-agnostic, requires only labeled object
crops, and integrates with existing AL pipelines. This provides a practical
framework for deploying AL in real-world applications where computational
efficiency and evaluation reliability are critical. Code is available at
https://mos-ks.github.io/publications/.

</details>


### [74] [Integrating SAM Supervision for 3D Weakly Supervised Point Cloud Segmentation](https://arxiv.org/abs/2508.19909)
*Lechun You,Zhonghua Wu,Weide Liu,Xulei Yang,Jun Cheng,Wei Zhou,Bharadwaj Veeravalli,Guosheng Lin*

Main category: cs.CV

TL;DR: A novel 3D semantic segmentation method that leverages 2D foundation models to augment sparse 3D annotations by propagating 2D segmentation masks into 3D space and using consistency regularization to generate reliable pseudo labels.


<details>
  <summary>Details</summary>
Motivation: Current 3D segmentation methods don't leverage complementary 2D data and fail to fully utilize limited annotations or address noise in pseudo labels, while 2D foundation models offer effective segmentation capabilities that could benefit 3D tasks.

Method: Incorporates 2D foundation model segmentation masks into 3D space via geometric correspondences, extends sparse annotations using 3D masks, applies confidence- and uncertainty-based consistency regularization on 3D augmentations, and selects reliable pseudo labels to generate more training data.

Result: The approach bridges the gap between limited 3D annotations and powerful 2D foundation models, substantially augmenting available labels and improving 3D weakly supervised segmentation performance.

Conclusion: This innovative strategy successfully leverages 2D foundation models to maximize utility of sparse 3D annotations, providing an effective solution for 3D semantic segmentation with limited supervision.

Abstract: Current methods for 3D semantic segmentation propose training models with
limited annotations to address the difficulty of annotating large, irregular,
and unordered 3D point cloud data. They usually focus on the 3D domain only,
without leveraging the complementary nature of 2D and 3D data. Besides, some
methods extend original labels or generate pseudo labels to guide the training,
but they often fail to fully use these labels or address the noise within them.
Meanwhile, the emergence of comprehensive and adaptable foundation models has
offered effective solutions for segmenting 2D data. Leveraging this
advancement, we present a novel approach that maximizes the utility of sparsely
available 3D annotations by incorporating segmentation masks generated by 2D
foundation models. We further propagate the 2D segmentation masks into the 3D
space by establishing geometric correspondences between 3D scenes and 2D views.
We extend the highly sparse annotations to encompass the areas delineated by 3D
masks, thereby substantially augmenting the pool of available labels.
Furthermore, we apply confidence- and uncertainty-based consistency
regularization on augmentations of the 3D point cloud and select the reliable
pseudo labels, which are further spread on the 3D masks to generate more
labels. This innovative strategy bridges the gap between limited 3D annotations
and the powerful capabilities of 2D foundation models, ultimately improving the
performance of 3D weakly supervised segmentation.

</details>


### [75] [WaveHiT-SR: Hierarchical Wavelet Network for Efficient Image Super-Resolution](https://arxiv.org/abs/2508.19927)
*Fayaz Ali,Muhammad Zawish,Steven Davy,Radu Timofte*

Main category: cs.CV

TL;DR: WaveHiT-SR is a hierarchical transformer framework that embeds wavelet transforms for image super-resolution, using adaptive windows to capture long-range dependencies while reducing computational complexity.


<details>
  <summary>Details</summary>
Motivation: Transformers show promise in image SR but suffer from quadratic computational complexity in window self-attention, forcing small fixed windows that limit receptive field and long-range dependency modeling.

Method: Embed wavelet transform within hierarchical transformer framework with adaptive hierarchical windows; decompose images into frequency subbands to focus on global/local features; progressively reconstruct HR images through hierarchical processing.

Result: Achieves cutting-edge SR results with higher efficiency - fewer parameters, lower FLOPs, and faster speeds compared to SwinIR-Light, SwinIR-NG, and SRFormer-Light.

Conclusion: WaveHiT-SR effectively addresses computational complexity limitations while preserving performance, enabling better long-range dependency modeling and detailed feature capture through wavelet-based hierarchical processing.

Abstract: Transformers have demonstrated promising performance in computer vision
tasks, including image super-resolution (SR). The quadratic computational
complexity of window self-attention mechanisms in many transformer-based SR
methods forces the use of small, fixed windows, limiting the receptive field.
In this paper, we propose a new approach by embedding the wavelet transform
within a hierarchical transformer framework, called (WaveHiT-SR). First, using
adaptive hierarchical windows instead of static small windows allows to capture
features across different levels and greatly improve the ability to model
long-range dependencies. Secondly, the proposed model utilizes wavelet
transforms to decompose images into multiple frequency subbands, allowing the
network to focus on both global and local features while preserving structural
details. By progressively reconstructing high-resolution images through
hierarchical processing, the network reduces computational complexity without
sacrificing performance. The multi-level decomposition strategy enables the
network to capture fine-grained information in lowfrequency components while
enhancing high-frequency textures. Through extensive experimentation, we
confirm the effectiveness and efficiency of our WaveHiT-SR. Our refined
versions of SwinIR-Light, SwinIR-NG, and SRFormer-Light deliver cutting-edge SR
results, achieving higher efficiency with fewer parameters, lower FLOPs, and
faster speeds.

</details>


### [76] [KRETA: A Benchmark for Korean Reading and Reasoning in Text-Rich VQA Attuned to Diverse Visual Contexts](https://arxiv.org/abs/2508.19944)
*Taebaek Hwang,Minseo Kim,Gisang Lee,Seonuk Kim,Hyunjun Eun*

Main category: cs.CV

TL;DR: KRETA is a new benchmark for Korean text-rich visual question answering that addresses the gap in low-resource language VQA evaluation, featuring diverse visual contexts and a semi-automated generation pipeline.


<details>
  <summary>Details</summary>
Motivation: There's a critical gap in text-rich VQA benchmarks for low-resource languages like Korean, which hinders robust model evaluation and comparison, while high-resource languages like English have well-established datasets.

Method: Developed a semi-automated VQA generation pipeline optimized for text-rich settings using refined stepwise image decomposition and a rigorous seven-metric evaluation protocol to ensure data quality.

Result: Created KRETA benchmark with comprehensive coverage across 15 domains and 26 image types, enabling in-depth evaluation of both visual text understanding and reasoning capabilities for Korean language.

Conclusion: KRETA fills the critical gap for Korean text-rich VQA evaluation and provides an adaptable pipeline that can facilitate development of similar benchmarks for other languages, accelerating multilingual VLM research.

Abstract: Understanding and reasoning over text within visual contexts poses a
significant challenge for Vision-Language Models (VLMs), given the complexity
and diversity of real-world scenarios. To address this challenge, text-rich
Visual Question Answering (VQA) datasets and benchmarks have emerged for
high-resource languages like English. However, a critical gap persists for
low-resource languages such as Korean, where the lack of comprehensive
benchmarks hinders robust model evaluation and comparison. To bridge this gap,
we introduce KRETA, a benchmark for Korean Reading and rEasoning in Text-rich
VQA Attuned to diverse visual contexts. KRETA facilitates an in-depth
evaluation of both visual text understanding and reasoning capabilities, while
also supporting a multifaceted assessment across 15 domains and 26 image types.
Additionally, we introduce a semi-automated VQA generation pipeline
specifically optimized for text-rich settings, leveraging refined stepwise
image decomposition and a rigorous seven-metric evaluation protocol to ensure
data quality. While KRETA is tailored for Korean, we hope our adaptable and
extensible pipeline will facilitate the development of similar benchmarks in
other languages, thereby accelerating multilingual VLM research. The code and
dataset for KRETA are available at https://github.com/tabtoyou/KRETA.

</details>


### [77] [Reimagining Image Segmentation using Active Contour: From Chan Vese Algorithm into a Proposal Novel Functional Loss Framework](https://arxiv.org/abs/2508.19946)
*Gianluca Guzzetta*

Main category: cs.CV

TL;DR: Study and analysis of Chan-Vese algorithm for image segmentation with a proposed functional segmentation loss based on active contours, comparing against classical loss functions on computer vision datasets.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive analysis of the Chan-Vese algorithm and develop an improved functional segmentation loss based on active contours for better image segmentation performance.

Method: Employed discretized scheme from empirical study of Chan-Vese model's functional energy and PDE based on level set function. Proposed functional segmentation loss using pytorch.nn.ModuleLoss and level set based on Chan-Vese algorithm.

Result: Implementation provided using MATLAB. Performance compared with common computer vision segmentation datasets, evaluating classical loss functions against the proposed method.

Conclusion: The study presents an improved functional segmentation approach based on Chan-Vese algorithm with available code implementation, offering potential enhancements over traditional loss functions in image segmentation tasks.

Abstract: In this paper, we present a comprehensive study and analysis of the Chan-Vese
algorithm for image segmentation. We employ a discretized scheme derived from
the empirical study of the Chan-Vese model's functional energy and its partial
differential equation based on its level set function. We provide a proof of
the results and an implementation using MATLAB. Leveraging modern computer
vision methodologies, we propose a functional segmentation loss based on active
contours, utilizing pytorch.nn.ModuleLoss and a level set based on the
Chan-Vese algorithm. We compare our results with common computer vision
segmentation datasets and evaluate the performance of classical loss functions
against our proposed method. All code and materials used are available at
https://github.com/gguzzy/chan_vese_functional_loss.

</details>


### [78] [GLSim: Detecting Object Hallucinations in LVLMs via Global-Local Similarity](https://arxiv.org/abs/2508.19972)
*Seongheon Park,Yixuan Li*

Main category: cs.CV

TL;DR: GLSim is a training-free framework that combines global and local embedding similarities between images and text to detect object hallucinations in vision-language models more accurately than previous methods.


<details>
  <summary>Details</summary>
Motivation: Object hallucination in vision-language models poses safety risks for real-world deployment. Existing detection methods use either global or local perspectives alone, limiting reliability.

Method: GLSim leverages complementary global and local embedding similarity signals between image and text modalities without requiring training.

Result: GLSim achieves superior detection performance, significantly outperforming competitive baselines in comprehensive benchmarking.

Conclusion: The proposed GLSim framework provides more accurate and reliable object hallucination detection by effectively combining global and local perspectives.

Abstract: Object hallucination in large vision-language models presents a significant
challenge to their safe deployment in real-world applications. Recent works
have proposed object-level hallucination scores to estimate the likelihood of
object hallucination; however, these methods typically adopt either a global or
local perspective in isolation, which may limit detection reliability. In this
paper, we introduce GLSim, a novel training-free object hallucination detection
framework that leverages complementary global and local embedding similarity
signals between image and text modalities, enabling more accurate and reliable
hallucination detection in diverse scenarios. We comprehensively benchmark
existing object hallucination detection methods and demonstrate that GLSim
achieves superior detection performance, outperforming competitive baselines by
a significant margin.

</details>


### [79] [Assessing the Geolocation Capabilities, Limitations and Societal Risks of Generative Vision-Language Models](https://arxiv.org/abs/2508.19967)
*Oliver Grainge,Sania Waheed,Jack Stilgoe,Michael Milford,Shoaib Ehsan*

Main category: cs.CV

TL;DR: VLMs show concerning geo-localization accuracy (61%) on social media-like images, posing significant privacy risks despite poor performance on street-level images.


<details>
  <summary>Details</summary>
Motivation: To systematically evaluate the geolocation precision of Generative VLMs, their limits, and potential for unintended inferences due to growing privacy concerns from AI models' ability to locate images.

Method: Comprehensive assessment of 25 state-of-the-art VLMs on four benchmark image datasets captured in diverse environments.

Result: Current VLMs perform poorly on generic street-level images but achieve notably high accuracy (61%) on images resembling social media content.

Conclusion: VLMs' high accuracy on social media-style images raises significant and urgent privacy concerns, highlighting the need for addressing societal risks from AI geolocation capabilities.

Abstract: Geo-localization is the task of identifying the location of an image using
visual cues alone. It has beneficial applications, such as improving disaster
response, enhancing navigation, and geography education. Recently,
Vision-Language Models (VLMs) are increasingly demonstrating capabilities as
accurate image geo-locators. This brings significant privacy risks, including
those related to stalking and surveillance, considering the widespread uses of
AI models and sharing of photos on social media. The precision of these models
is likely to improve in the future. Despite these risks, there is little work
on systematically evaluating the geolocation precision of Generative VLMs,
their limits and potential for unintended inferences. To bridge this gap, we
conduct a comprehensive assessment of the geolocation capabilities of 25
state-of-the-art VLMs on four benchmark image datasets captured in diverse
environments. Our results offer insight into the internal reasoning of VLMs and
highlight their strengths, limitations, and potential societal risks. Our
findings indicate that current VLMs perform poorly on generic street-level
images yet achieve notably high accuracy (61\%) on images resembling social
media content, raising significant and urgent privacy concerns.

</details>


### [80] [GS: Generative Segmentation via Label Diffusion](https://arxiv.org/abs/2508.20020)
*Yuhao Chen,Shubin Chen,Liang Lin,Guangrun Wang*

Main category: cs.CV

TL;DR: GS (Generative Segmentation) formulates image segmentation as a generative task using label diffusion, directly generating segmentation masks from noise conditioned on both image and language description, achieving state-of-the-art performance on Panoptic Narrative Grounding.


<details>
  <summary>Details</summary>
Motivation: Traditional segmentation methods treat it as a discriminative problem, while existing diffusion approaches remain image-centric. The authors propose to make segmentation itself the primary generative task rather than an auxiliary process.

Method: GS reverses the typical generative process - instead of generating images from label maps, it directly generates segmentation masks from noise conditioned on input image and language description using label diffusion.

Result: GS significantly outperforms existing discriminative and diffusion-based methods on Panoptic Narrative Grounding benchmark, setting new state-of-the-art for language-driven segmentation.

Conclusion: Formulating segmentation as a generative task via label diffusion enables end-to-end training with explicit control over spatial and semantic fidelity, proving more effective than traditional discriminative approaches.

Abstract: Language-driven image segmentation is a fundamental task in vision-language
understanding, requiring models to segment regions of an image corresponding to
natural language expressions. Traditional methods approach this as a
discriminative problem, assigning each pixel to foreground or background based
on semantic alignment. Recently, diffusion models have been introduced to this
domain, but existing approaches remain image-centric: they either (i) use image
diffusion models as visual feature extractors, (ii) synthesize segmentation
data via image generation to train discriminative models, or (iii) perform
diffusion inversion to extract attention cues from pre-trained image diffusion
models-thereby treating segmentation as an auxiliary process. In this paper, we
propose GS (Generative Segmentation), a novel framework that formulates
segmentation itself as a generative task via label diffusion. Instead of
generating images conditioned on label maps and text, GS reverses the
generative process: it directly generates segmentation masks from noise,
conditioned on both the input image and the accompanying language description.
This paradigm makes label generation the primary modeling target, enabling
end-to-end training with explicit control over spatial and semantic fidelity.
To demonstrate the effectiveness of our approach, we evaluate GS on Panoptic
Narrative Grounding (PNG), a representative and challenging benchmark for
multimodal segmentation that requires panoptic-level reasoning guided by
narrative captions. Experimental results show that GS significantly outperforms
existing discriminative and diffusion-based methods, setting a new
state-of-the-art for language-driven segmentation.

</details>


### [81] [Segmentation Assisted Incremental Test Time Adaptation in an Open World](https://arxiv.org/abs/2508.20029)
*Manogna Sreenivas,Soma Biswas*

Main category: cs.CV

TL;DR: SegAssist framework enables Vision Language Models to adapt to both unseen classes and domains during testing through segmentation-assisted active labeling that queries an oracle for potential new classes.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of unfamiliar objects and distribution shifts in dynamic environments where VLMs encounter unseen classes and domains during testing, requiring continuous adaptation.

Method: Proposes SegAssist - a training-free segmentation-assisted active labeling module that repurposes VLM segmentation capabilities to refine active sample selection by prioritizing samples likely from unseen classes and querying an oracle.

Result: Extensive experiments on benchmark datasets demonstrate SegAssist's effectiveness in enhancing VLM performance for continuous adaptation to emerging data in real-world scenarios.

Conclusion: SegAssist provides a novel approach for Incremental Test Time Adaptation of VLMs, successfully handling both covariate and label shifts by leveraging segmentation capabilities for improved active sample selection in dynamic environments.

Abstract: In dynamic environments, unfamiliar objects and distribution shifts are often
encountered, which challenge the generalization abilities of the deployed
trained models. This work addresses Incremental Test Time Adaptation of Vision
Language Models, tackling scenarios where unseen classes and unseen domains
continuously appear during testing. Unlike traditional Test Time Adaptation
approaches, where the test stream comes only from a predefined set of classes,
our framework allows models to adapt simultaneously to both covariate and label
shifts, actively incorporating new classes as they emerge. Towards this goal,
we establish a new benchmark for ITTA, integrating single image TTA methods for
VLMs with active labeling techniques that query an oracle for samples
potentially representing unseen classes during test time. We propose a
segmentation assisted active labeling module, termed SegAssist, which is
training free and repurposes the segmentation capabilities of VLMs to refine
active sample selection, prioritizing samples likely to belong to unseen
classes. Extensive experiments on several benchmark datasets demonstrate the
potential of SegAssist to enhance the performance of VLMs in real world
scenarios, where continuous adaptation to emerging data is essential.
Project-page:https://manogna-s.github.io/segassist/

</details>


### [82] [OpenM3D: Open Vocabulary Multi-view Indoor 3D Object Detection without Human Annotations](https://arxiv.org/abs/2508.20063)
*Peng-Hao Hsu,Ke Zhang,Fu-En Wang,Tao Tu,Ming-Feng Li,Yu-Lun Liu,Albert Y. C. Chen,Min Sun,Cheng-Hao Kuo*

Main category: cs.CV

TL;DR: OpenM3D is a novel open-vocabulary multi-view indoor 3D object detector trained without human annotations, using 2D-induced voxel features and achieving superior accuracy and speed compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Open-vocabulary 3D object detection through image-based methods remains limited compared to 3D point cloud methods, and there's a need for efficient detectors that don't require human annotations.

Method: Single-stage detector adapting 2D-induced voxel features from ImGeoNet, jointly trained with class-agnostic 3D localization loss and voxel-semantic alignment loss. Uses 3D pseudo box generation with graph embedding to combine 2D segments into coherent 3D structures.

Result: Achieves higher precision and recall than other methods, including OV-3DET. Demonstrates superior accuracy and speed (0.3 sec per scene) on ScanNet200 and ARKitScenes benchmarks, outperforming strong two-stage methods.

Conclusion: OpenM3D is an efficient open-vocabulary 3D detector that requires only multi-view images, achieves state-of-the-art performance without human annotations, and shows both accuracy and speed advantages over existing approaches.

Abstract: Open-vocabulary (OV) 3D object detection is an emerging field, yet its
exploration through image-based methods remains limited compared to 3D point
cloud-based methods. We introduce OpenM3D, a novel open-vocabulary multi-view
indoor 3D object detector trained without human annotations. In particular,
OpenM3D is a single-stage detector adapting the 2D-induced voxel features from
the ImGeoNet model. To support OV, it is jointly trained with a class-agnostic
3D localization loss requiring high-quality 3D pseudo boxes and a
voxel-semantic alignment loss requiring diverse pre-trained CLIP features. We
follow the training setting of OV-3DET where posed RGB-D images are given but
no human annotations of 3D boxes or classes are available. We propose a 3D
Pseudo Box Generation method using a graph embedding technique that combines 2D
segments into coherent 3D structures. Our pseudo-boxes achieve higher precision
and recall than other methods, including the method proposed in OV-3DET. We
further sample diverse CLIP features from 2D segments associated with each
coherent 3D structure to align with the corresponding voxel feature. The key to
training a highly accurate single-stage detector requires both losses to be
learned toward high-quality targets. At inference, OpenM3D, a highly efficient
detector, requires only multi-view images for input and demonstrates superior
accuracy and speed (0.3 sec. per scene) on ScanNet200 and ARKitScenes indoor
benchmarks compared to existing methods. We outperform a strong two-stage
method that leverages our class-agnostic detector with a ViT CLIP-based OV
classifier and a baseline incorporating multi-view depth estimator on both
accuracy and speed.

</details>


### [83] [Patch Progression Masked Autoencoder with Fusion CNN Network for Classifying Evolution Between Two Pairs of 2D OCT Slices](https://arxiv.org/abs/2508.20064)
*Philippe Zhang,Weili Jiang,Yihao Li,Jing Zhang,Sarah Matta,Yubo Tan,Hui Lin,Haoshen Wang,Jiangtian Pan,Hui Xu,Laurent Borderie,Alexandre Le Guilcher,Béatrice Cochener,Chubin Ou,Gwenolé Quellec,Mathieu Lamard*

Main category: cs.CV

TL;DR: Top 10 performance in MARIO challenge using CNN fusion with ensembling for AMD progression classification and a novel Patch Progression Masked Autoencoder for future OCT prediction.


<details>
  <summary>Details</summary>
Motivation: AMD requires timely diagnosis and monitoring for effective anti-VEGF treatment. Tracking neovascular activity progression in OCT scans enables personalized treatment plans.

Method: Task 1: Fusion CNN network with model ensembling for classifying evolution between OCT slice pairs. Task 2: Patch Progression Masked Autoencoder that generates future OCT scans and classifies progression using Task 1 solution.

Result: Achieved Top 10 ranking in both tasks of MARIO challenge, though ineligible for prizes due to organizational affiliations with challenge organizers.

Conclusion: The proposed methods effectively address AMD progression tracking in OCT scans, demonstrating strong performance in both classification and prediction tasks for personalized treatment planning.

Abstract: Age-related Macular Degeneration (AMD) is a prevalent eye condition affecting
visual acuity. Anti-vascular endothelial growth factor (anti-VEGF) treatments
have been effective in slowing the progression of neovascular AMD, with better
outcomes achieved through timely diagnosis and consistent monitoring. Tracking
the progression of neovascular activity in OCT scans of patients with exudative
AMD allows for the development of more personalized and effective treatment
plans. This was the focus of the Monitoring Age-related Macular Degeneration
Progression in Optical Coherence Tomography (MARIO) challenge, in which we
participated. In Task 1, which involved classifying the evolution between two
pairs of 2D slices from consecutive OCT acquisitions, we employed a fusion CNN
network with model ensembling to further enhance the model's performance. For
Task 2, which focused on predicting progression over the next three months
based on current exam data, we proposed the Patch Progression Masked
Autoencoder that generates an OCT for the next exam and then classifies the
evolution between the current OCT and the one generated using our solution from
Task 1. The results we achieved allowed us to place in the Top 10 for both
tasks. Some team members are part of the same organization as the challenge
organizers; therefore, we are not eligible to compete for the prize.

</details>


### [84] [PAUL: Uncertainty-Guided Partition and Augmentation for Robust Cross-View Geo-Localization under Noisy Correspondence](https://arxiv.org/abs/2508.20066)
*Zheng Li,Yanming Guo,WenZhe Liu,Xueyi Zhang,Zhaoyun Ding,Long Xu,Mingrui Lao*

Main category: cs.CV

TL;DR: PAUL framework addresses noisy correspondence in cross-view geo-localization by using uncertainty learning to partition and augment training data, achieving superior performance in handling GPS drift and misaligned image pairs.


<details>
  <summary>Details</summary>
Motivation: Existing cross-view geo-localization methods assume perfect image alignment during training, but real-world factors like GPS drift cause systematic alignment shifts with only partial correspondences, creating noisy data that current research overlooks.

Method: Proposes PAUL (Partition and Augmentation by Uncertainty Learning) - a framework that uses uncertainty-aware co-augmentation and evidential co-training to partition data based on estimated uncertainty, selectively augmenting high-confidence regions and refining feature learning to suppress noise from misaligned pairs.

Result: Comprehensive experiments show PAUL consistently achieves superior performance over other noisy-correspondence methods across various noise ratios, validating the effectiveness of its individual components.

Conclusion: PAUL successfully bridges the gap between idealized benchmarks and practical applications by effectively handling noisy correspondence in cross-view geo-localization through targeted partitioning and augmentation based on uncertainty estimation.

Abstract: Cross-view geo-localization is a critical task for UAV navigation, event
detection, and aerial surveying, as it enables matching between drone-captured
and satellite imagery. Most existing approaches embed multi-modal data into a
joint feature space to maximize the similarity of paired images. However, these
methods typically assume perfect alignment of image pairs during training,
which rarely holds true in real-world scenarios. In practice, factors such as
urban canyon effects, electromagnetic interference, and adverse weather
frequently induce GPS drift, resulting in systematic alignment shifts where
only partial correspondences exist between pairs. Despite its prevalence, this
source of noisy correspondence has received limited attention in current
research. In this paper, we formally introduce and address the Noisy
Correspondence on Cross-View Geo-Localization (NC-CVGL) problem, aiming to
bridge the gap between idealized benchmarks and practical applications. To this
end, we propose PAUL (Partition and Augmentation by Uncertainty Learning), a
novel framework that partitions and augments training data based on estimated
data uncertainty through uncertainty-aware co-augmentation and evidential
co-training. Specifically, PAUL selectively augments regions with high
correspondence confidence and utilizes uncertainty estimation to refine feature
learning, effectively suppressing noise from misaligned pairs. Distinct from
traditional filtering or label correction, PAUL leverages both data uncertainty
and loss discrepancy for targeted partitioning and augmentation, thus providing
robust supervision for noisy samples. Comprehensive experiments validate the
effectiveness of individual components in PAUL,which consistently achieves
superior performance over other competitive noisy-correspondence-driven methods
in various noise ratios.

</details>


### [85] [Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding in Vision-Language-Action Policies](https://arxiv.org/abs/2508.20072)
*Zhixuan Liang,Yizhuo Li,Tianshuo Yang,Chengyue Wu,Sitong Mao,Liuao Pei,Xiaokang Yang,Jiangmiao Pang,Yao Mu,Ping Luo*

Main category: cs.CV

TL;DR: Discrete Diffusion VLA is a unified transformer policy that uses discrete diffusion to model robot actions, achieving adaptive decoding and better performance than autoregressive or continuous diffusion methods.


<details>
  <summary>Details</summary>
Motivation: Current VLA models either use fixed-order autoregressive decoding or continuous diffusion heads that require specialized training and iterative sampling, lacking a unified scalable architecture.

Method: A single-transformer policy that models discretized action chunks with discrete diffusion, trained with cross-entropy objective, featuring adaptive decoding order and secondary remasking for error correction.

Result: Achieves 96.3% avg. SR on LIBERO, 71.2% visual matching on SimplerEnv Fractal, and 49.3% overall on SimplerEnv Bridge, outperforming both autoregressive and continuous diffusion baselines.

Conclusion: Discrete diffusion action decoder enables precise action modeling and consistent training, providing a foundation for scaling VLA to larger models and datasets.

Abstract: Vision-Language-Action (VLA) models adapt large vision-language backbones to
map images and instructions to robot actions. However, prevailing VLA decoders
either generate actions autoregressively in a fixed left-to-right order or
attach continuous diffusion or flow matching heads outside the backbone,
demanding specialized training and iterative sampling that hinder a unified,
scalable architecture. We present Discrete Diffusion VLA, a single-transformer
policy that models discretized action chunks with discrete diffusion and is
trained with the same cross-entropy objective as the VLM backbone. The design
retains diffusion's progressive refinement paradigm while remaining natively
compatible with the discrete token interface of VLMs. Our method achieves an
adaptive decoding order that resolves easy action elements before harder ones
and uses secondary remasking to revisit uncertain predictions across refinement
rounds, which improves consistency and enables robust error correction. This
unified decoder preserves pretrained vision language priors, supports parallel
decoding, breaks the autoregressive bottleneck, and reduces the number of
function evaluations. Discrete Diffusion VLA achieves 96.3% avg. SR on LIBERO,
71.2% visual matching on SimplerEnv Fractal and 49.3% overall on SimplerEnv
Bridge, improving over both autoregressive and continuous diffusion baselines.
These findings indicate that discrete-diffusion action decoder supports precise
action modeling and consistent training, laying groundwork for scaling VLA to
larger models and datasets.

</details>


### [86] [Seam360GS: Seamless 360° Gaussian Splatting from Real-World Omnidirectional Images](https://arxiv.org/abs/2508.20080)
*Changha Shin,Woong Oh Cho,Seon Joo Kim*

Main category: cs.CV

TL;DR: A novel calibration framework that integrates dual-fisheye camera modeling into 3D Gaussian splatting to transform imperfect 360-degree inputs into seamless renderings by jointly optimizing 3D Gaussian parameters and calibration variables.


<details>
  <summary>Details</summary>
Motivation: Consumer-grade dual-fisheye systems produce imperfect panoramas due to lens separation and angular distortions, limiting the quality of 360-degree visual content used in VR, robotics, and autonomous navigation.

Method: Incorporates a dual-fisheye camera model into 3D Gaussian splatting pipeline, jointly optimizing 3D Gaussian parameters with calibration variables that emulate lens gaps and angular distortions to simulate realistic artifacts and enable seamless rendering.

Result: Extensive evaluations show the method produces seamless renderings from imperfect images and outperforms existing 360-degree rendering models.

Conclusion: The framework successfully transforms imperfect omnidirectional inputs into flawless novel view synthesis, addressing inherent limitations of consumer dual-fisheye systems for high-quality 360-degree content generation.

Abstract: 360-degree visual content is widely shared on platforms such as YouTube and
plays a central role in virtual reality, robotics, and autonomous navigation.
However, consumer-grade dual-fisheye systems consistently yield imperfect
panoramas due to inherent lens separation and angular distortions. In this
work, we introduce a novel calibration framework that incorporates a
dual-fisheye camera model into the 3D Gaussian splatting pipeline. Our approach
not only simulates the realistic visual artifacts produced by dual-fisheye
cameras but also enables the synthesis of seamlessly rendered 360-degree
images. By jointly optimizing 3D Gaussian parameters alongside calibration
variables that emulate lens gaps and angular distortions, our framework
transforms imperfect omnidirectional inputs into flawless novel view synthesis.
Extensive evaluations on real-world datasets confirm that our method produces
seamless renderings-even from imperfect images-and outperforms existing
360-degree rendering models.

</details>


### [87] [AudioStory: Generating Long-Form Narrative Audio with Large Language Models](https://arxiv.org/abs/2508.20088)
*Yuxin Guo,Teng Wang,Yuying Ge,Shijie Ma,Yixiao Ge,Wei Zou,Ying Shan*

Main category: cs.CV

TL;DR: AudioStory is a unified framework that combines LLMs with text-to-audio systems to generate coherent long-form audio narratives, addressing limitations of current TTA systems in handling temporal coherence and compositional reasoning.


<details>
  <summary>Details</summary>
Motivation: Current text-to-audio generation systems excel at short audio clips but struggle with long-form narrative audio that requires temporal coherence, scene transitions, and emotional consistency over extended durations.

Method: AudioStory integrates LLMs to decompose narrative queries into temporally ordered sub-tasks, featuring a decoupled bridging mechanism (bridging query for intra-event alignment and residual query for cross-event coherence) and end-to-end training that unifies instruction comprehension with audio generation.

Result: Extensive experiments show AudioStory surpasses prior TTA baselines in both instruction-following ability and audio fidelity for both single-audio and narrative audio generation. The framework also includes a benchmark dataset AudioStory-10K covering diverse domains.

Conclusion: AudioStory successfully addresses the challenge of long-form audio narrative generation by leveraging LLM reasoning capabilities and provides a unified end-to-end framework that eliminates modular training while enhancing component synergy.

Abstract: Recent advances in text-to-audio (TTA) generation excel at synthesizing short
audio clips but struggle with long-form narrative audio, which requires
temporal coherence and compositional reasoning. To address this gap, we propose
AudioStory, a unified framework that integrates large language models (LLMs)
with TTA systems to generate structured, long-form audio narratives. AudioStory
possesses strong instruction-following reasoning generation capabilities. It
employs LLMs to decompose complex narrative queries into temporally ordered
sub-tasks with contextual cues, enabling coherent scene transitions and
emotional tone consistency. AudioStory has two appealing features: (1)
Decoupled bridging mechanism: AudioStory disentangles LLM-diffuser
collaboration into two specialized components, i.e., a bridging query for
intra-event semantic alignment and a residual query for cross-event coherence
preservation. (2) End-to-end training: By unifying instruction comprehension
and audio generation within a single end-to-end framework, AudioStory
eliminates the need for modular training pipelines while enhancing synergy
between components. Furthermore, we establish a benchmark AudioStory-10K,
encompassing diverse domains such as animated soundscapes and natural sound
narratives. Extensive experiments show the superiority of AudioStory on both
single-audio generation and narrative audio generation, surpassing prior TTA
baselines in both instruction-following ability and audio fidelity. Our code is
available at https://github.com/TencentARC/AudioStory

</details>


### [88] [Bridging Domain Gaps for Fine-Grained Moth Classification Through Expert-Informed Adaptation and Foundation Model Priors](https://arxiv.org/abs/2508.20089)
*Ross J Gardiner,Guillaume Mougeot,Sareh Rowlands,Benno I Simmons,Flemming Helsing,Toke Thomas Høye*

Main category: cs.CV

TL;DR: Lightweight moth classification using knowledge distillation from BioCLIP2 to ConvNeXt-tiny, achieving comparable accuracy with reduced computational cost for insect monitoring systems.


<details>
  <summary>Details</summary>
Motivation: Accurate species identification of moths from automated camera systems is challenging due to domain shifts between curated images and noisy field imagery, which is vital for understanding insect declines.

Method: Combines limited expert-labelled field data with knowledge distillation from the high-performance BioCLIP2 foundation model into a ConvNeXt-tiny architecture.

Result: BioCLIP2 substantially outperforms other methods, and the distilled lightweight model achieves comparable accuracy with significantly reduced computational cost on 101 Danish moth species from AMI camera systems.

Conclusion: The approach offers practical guidelines for developing efficient insect monitoring systems and bridging domain gaps for fine-grained classification.

Abstract: Labelling images of Lepidoptera (moths) from automated camera systems is
vital for understanding insect declines. However, accurate species
identification is challenging due to domain shifts between curated images and
noisy field imagery. We propose a lightweight classification approach,
combining limited expert-labelled field data with knowledge distillation from
the high-performance BioCLIP2 foundation model into a ConvNeXt-tiny
architecture. Experiments on 101 Danish moth species from AMI camera systems
demonstrate that BioCLIP2 substantially outperforms other methods and that our
distilled lightweight model achieves comparable accuracy with significantly
reduced computational cost. These insights offer practical guidelines for the
development of efficient insect monitoring systems and bridging domain gaps for
fine-grained classification.

</details>


### [89] [CODA: Coordinating the Cerebrum and Cerebellum for a Dual-Brain Computer Use Agent with Decoupled Reinforcement Learning](https://arxiv.org/abs/2508.20096)
*Zeyi Sun,Yuhang Cao,Jianze Liang,Qiushi Sun,Ziyu Liu,Zhixiong Zhang,Yuhang Zang,Xiaoyi Dong,Kai Chen,Dahua Lin,Jiaqi Wang*

Main category: cs.CV

TL;DR: CODA is a trainable compositional framework that combines a generalist planner with specialist executors for scientific GUI automation, using a two-stage training pipeline to achieve both robust execution and cross-domain generalization.


<details>
  <summary>Details</summary>
Motivation: Address the trade-off between generalist agents (good at planning but poor execution) and specialized agents (good execution but poor planning) in scientific GUI automation, particularly given the scarcity of high-quality data in scientific domains.

Method: Two-stage pipeline: 1) Specialization - decoupled GRPO approach to train expert planners for each scientific application individually using small task trajectories; 2) Generalization - aggregate successful trajectories to build consolidated dataset for supervised fine-tuning of final planner.

Result: CODA significantly outperforms baselines and establishes new state-of-the-art performance on four challenging applications from the ScienceBoard benchmark.

Conclusion: The trainable compositional framework successfully bridges the planning-execution gap in scientific GUI automation, enabling both robust execution and cross-domain generalization capabilities.

Abstract: Autonomous agents for Graphical User Interfaces (GUIs) face significant
challenges in specialized domains such as scientific computing, where both
long-horizon planning and precise execution are required. Existing approaches
suffer from a trade-off: generalist agents excel at planning but perform poorly
in execution, while specialized agents demonstrate the opposite weakness.
Recent compositional frameworks attempt to bridge this gap by combining a
planner and an actor, but they are typically static and non-trainable, which
prevents adaptation from experience. This is a critical limitation given the
scarcity of high-quality data in scientific domains. To address these
limitations, we introduce CODA, a novel and trainable compositional framework
that integrates a generalist planner (Cerebrum) with a specialist executor
(Cerebellum), trained via a dedicated two-stage pipeline. In the first stage,
Specialization, we apply a decoupled GRPO approach to train an expert planner
for each scientific application individually, bootstrapping from a small set of
task trajectories. In the second stage, Generalization, we aggregate all
successful trajectories from the specialized experts to build a consolidated
dataset, which is then used for supervised fine-tuning of the final planner.
This equips CODA with both robust execution and cross-domain generalization.
Evaluated on four challenging applications from the ScienceBoard benchmark,
CODA significantly outperforms baselines and establishes a new state of the art
among open-source models.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [90] [Sycophancy as compositions of Atomic Psychometric Traits](https://arxiv.org/abs/2508.19316)
*Shreyans Jain,Alexandra Yost,Amirali Abdullah*

Main category: cs.AI

TL;DR: The paper proposes modeling LLM sycophancy as geometric and causal compositions of psychometric traits rather than treating it as an isolated failure mode, using Contrastive Activation Addition to map activation directions to factors and enable interpretable interventions.


<details>
  <summary>Details</summary>
Motivation: Sycophancy is a key behavioral risk in LLMs that is often treated as an isolated failure mode with a single causal mechanism, but this approach may be limited in understanding and addressing the complex nature of this behavior.

Method: Using Contrastive Activation Addition (CAA) to map activation directions to psychometric traits (emotionality, openness, agreeableness) and study how different combinations of these factors give rise to sycophancy behaviors.

Result: The approach allows for interpretable and compositional vector-based interventions such as addition, subtraction and projection that can be used to mitigate safety-critical behaviors in LLMs.

Conclusion: Modeling sycophancy as geometric and causal compositions of psychometric traits provides a more nuanced understanding and enables targeted interventions for mitigating this behavioral risk in language models.

Abstract: Sycophancy is a key behavioral risk in LLMs, yet is often treated as an
isolated failure mode that occurs via a single causal mechanism. We instead
propose modeling it as geometric and causal compositions of psychometric traits
such as emotionality, openness, and agreeableness - similar to factor
decomposition in psychometrics. Using Contrastive Activation Addition (CAA), we
map activation directions to these factors and study how different combinations
may give rise to sycophancy (e.g., high extraversion combined with low
conscientiousness). This perspective allows for interpretable and compositional
vector-based interventions like addition, subtraction and projection; that may
be used to mitigate safety-critical behaviors in LLMs.

</details>


### [91] [Aleks: AI powered Multi Agent System for Autonomous Scientific Discovery via Data-Driven Approaches in Plant Science](https://arxiv.org/abs/2508.19383)
*Daoyuan Jin,Nick Gunner,Niko Carvajal Janke,Shivranjani Baruah,Kaitlin M. Gold,Yu Jiang*

Main category: cs.AI

TL;DR: Aleks is an AI multi-agent system that autonomously conducts scientific discovery in plant science by integrating domain knowledge, data analysis, and machine learning without human intervention.


<details>
  <summary>Details</summary>
Motivation: Modern plant science faces challenges with large heterogeneous datasets, experimental design, data preprocessing, and reproducibility that hinder research throughput.

Method: AI-powered multi-agent system that iteratively formulates problems, explores modeling strategies, and refines solutions through multiple autonomous cycles using domain knowledge and machine learning.

Result: In grapevine red blotch disease case study, Aleks identified biologically meaningful features and developed interpretable models with robust performance. Ablation studies showed domain knowledge and memory are crucial for coherent outcomes.

Conclusion: Agentic AI shows promise as an autonomous collaborator for accelerating scientific discovery in plant sciences through structured, knowledge-integrated frameworks.

Abstract: Modern plant science increasingly relies on large, heterogeneous datasets,
but challenges in experimental design, data preprocessing, and reproducibility
hinder research throughput. Here we introduce Aleks, an AI-powered multi-agent
system that integrates domain knowledge, data analysis, and machine learning
within a structured framework to autonomously conduct data-driven scientific
discovery. Once provided with a research question and dataset, Aleks
iteratively formulated problems, explored alternative modeling strategies, and
refined solutions across multiple cycles without human intervention. In a case
study on grapevine red blotch disease, Aleks progressively identified
biologically meaningful features and converged on interpretable models with
robust performance. Ablation studies underscored the importance of domain
knowledge and memory for coherent outcomes. This exploratory work highlights
the promise of agentic AI as an autonomous collaborator for accelerating
scientific discovery in plant sciences.

</details>


### [92] [Quantized but Deceptive? A Multi-Dimensional Truthfulness Evaluation of Quantized LLMs](https://arxiv.org/abs/2508.19432)
*Yao Fu,Xianxuan Long,Runchao Li,Haotian Yu,Mu Sheng,Xiaotian Han,Yu Yin,Pan Li*

Main category: cs.AI

TL;DR: Quantized LLMs maintain truthful internal representations but become more vulnerable to generating false outputs when given deceptive prompts, despite knowing the truth internally.


<details>
  <summary>Details</summary>
Motivation: To investigate the impact of quantization on LLM truthfulness, as current research focuses on perplexity and zero-shot performance but neglects truthfulness evaluation.

Method: Developed TruthfulnessEval framework with 3 dimensions (logical reasoning, common sense, imitative falsehoods), tested 4-bit to 2-bit quantization on open-source LLMs, and analyzed 15 prompt variants with layer-wise probing and PCA visualizations.

Result: Quantized models retain truthful internal representations but are more susceptible to producing false outputs under deceptive prompts, while honest/neutral prompts maintain stable outputs. Models internally know the truth but still generate false responses when guided by deceptive prompts.

Conclusion: Quantization introduces truthfulness vulnerabilities that require future quantization-aware alignment techniques and truthfulness interventions to address.

Abstract: Quantization enables efficient deployment of large language models (LLMs) in
resource-constrained environments by significantly reducing memory and
computation costs. While quantized LLMs often maintain performance on
perplexity and zero-shot tasks, their impact on truthfulness-whether generating
truthful or deceptive responses-remains largely unexplored. In this work, we
introduce TruthfulnessEval, a comprehensive evaluation framework for assessing
the truthfulness of quantized LLMs across three dimensions: (1) Truthfulness on
Logical Reasoning; (2) Truthfulness on Common Sense; and (3) Truthfulness on
Imitative Falsehoods. Using this framework, we examine mainstream quantization
techniques (ranging from 4-bit to extreme 2-bit) across several open-source
LLMs. Surprisingly, we find that while quantized models retain internally
truthful representations, they are more susceptible to producing false outputs
under misleading prompts. To probe this vulnerability, we test 15 rephrased
variants of "honest", "neutral" and "deceptive" prompts and observe that
"deceptive" prompts can override truth-consistent behavior, whereas "honest"
and "neutral" prompts maintain stable outputs. Further, we reveal that
quantized models "know" the truth internally yet still produce false outputs
when guided by "deceptive" prompts via layer-wise probing and PCA
visualizations. Our findings provide insights into future designs of
quantization-aware alignment and truthfulness interventions.

</details>


### [93] [Reliable Weak-to-Strong Monitoring of LLM Agents](https://arxiv.org/abs/2508.19461)
*Neil Kale,Chen Bo Calvin Zhang,Kevin Zhu,Ankit Aich,Paula Rodriguez,Scale Red Team,Christina Q. Knight,Zifan Wang*

Main category: cs.AI

TL;DR: This paper introduces a monitor red teaming (MRT) workflow to test LLM agent monitoring systems against covert misbehavior, revealing that agent awareness of monitoring significantly degrades detection reliability, while hybrid monitor scaffolding and targeted human oversight improve performance.


<details>
  <summary>Details</summary>
Motivation: To stress test monitoring systems for detecting covert misbehavior in autonomous LLM agents, such as secretly sharing private information, and establish a systematic workflow for evaluating monitor robustness against adversarial evasion strategies.

Method: Developed a monitor red teaming (MRT) workflow with varying agent/monitor awareness levels, adversarial strategies (e.g., prompt injection), and two datasets/environments (SHADE-Arena and CUA-SHADE-Arena). Tested existing LLM monitor scaffoldings and proposed a new hybrid hierarchical-sequential scaffolding.

Result: Key findings: (1) Agent awareness dominates monitor awareness - knowing they're monitored degrades reliability; (2) Hybrid scaffolding outperforms baselines, enabling weak models to monitor strong agents; (3) Targeted human oversight improved TPR by ~15% at FPR=0.01.

Conclusion: Establishes standard MRT workflow, highlighting lack of adversarial robustness in LLM and human monitoring. Releases code, data, and logs to spur further research on detecting agent misbehavior.

Abstract: We stress test monitoring systems for detecting covert misbehavior in
autonomous LLM agents (e.g., secretly sharing private information). To this
end, we systematize a monitor red teaming (MRT) workflow that incorporates: (1)
varying levels of agent and monitor situational awareness; (2) distinct
adversarial strategies to evade the monitor, such as prompt injection; and (3)
two datasets and environments -- SHADE-Arena for tool-calling agents and our
new CUA-SHADE-Arena, which extends TheAgentCompany, for computer-use agents. We
run MRT on existing LLM monitor scaffoldings, which orchestrate LLMs and parse
agent trajectories, alongside a new hybrid hierarchical-sequential scaffolding
proposed in this work. Our empirical results yield three key findings. First,
agent awareness dominates monitor awareness: an agent's knowledge that it is
being monitored substantially degrades the monitor's reliability. On the
contrary, providing the monitor with more information about the agent is less
helpful than expected. Second, monitor scaffolding matters more than monitor
awareness: the hybrid scaffolding consistently outperforms baseline monitor
scaffolding, and can enable weaker models to reliably monitor stronger agents
-- a weak-to-strong scaling effect. Third, in a human-in-the-loop setting where
humans discuss with the LLM monitor to get an updated judgment for the agent's
behavior, targeted human oversight is most effective; escalating only
pre-flagged cases to human reviewers improved the TPR by approximately 15% at
FPR = 0.01. Our work establishes a standard workflow for MRT, highlighting the
lack of adversarial robustness for LLMs and humans when monitoring and
detecting agent misbehavior. We release code, data, and logs to spur further
research.

</details>


### [94] [SLIM: Subtrajectory-Level Elimination for More Effective Reasoning](https://arxiv.org/abs/2508.19502)
*Xifeng Yao,Chengyuan Ma,Dongyu Lang,Yinhao Ni,Zhiwei Xu,Huarui Xie,Zihao Chen,Guang Shen,Dandan Tu,Yi Bai,Changzheng Zhang*

Main category: cs.AI

TL;DR: The paper proposes a "5+2" framework to identify and remove suboptimal reasoning subtrajectories in LLM reasoning processes, improving model performance with less training data.


<details>
  <summary>Details</summary>
Motivation: Current test-time scaling methods generate extended reasoning trajectories, but not all components contribute positively - some negatively impact performance. Fine-tuning with these suboptimal trajectories may not be optimal.

Method: Developed a "5+2" framework that: 1) identifies suboptimal subtrajectories using five human-established criteria, 2) assesses their independence from subsequent content, and 3) uses a sampling algorithm to select data free from suboptimal reasoning components.

Result: Reduced suboptimal subtrajectories by 25.9% during inference. Achieved 58.92% average accuracy on math benchmarks with only 2/3 of training data (vs 58.06% with full data), outperforming open-source datasets when fine-tuning Qwen2.5-Math-7B.

Conclusion: The method effectively identifies and removes harmful reasoning components, improving model performance even with reduced training data and under various resource constraints.

Abstract: In recent months, substantial progress has been made in complex reasoning of
Large Language Models, particularly through the application of test-time
scaling. Notable examples include o1/o3/o4 series and DeepSeek-R1. When
responding to a query, these models generate an extended reasoning trajectory,
during which the model explores, reflects, backtracks, and self-verifies before
arriving at a conclusion. However, fine-tuning models with such reasoning
trajectories may not always be optimal. Our findings indicate that not all
components within these reasoning trajectories contribute positively to the
reasoning process; in fact, some components may affect the overall performance
negatively. In this study, we divide a reasoning trajectory into individual
subtrajectories and develop a "5+2" framework to: (1) systematically identify
suboptimal subtrajectories within the reasoning trajectory based on five
human-established criteria; (2) assess the independence of the suboptimal
subtrajectories identified in (1) from the subsequent content, ensuring that
their elimination does not compromise overall flow and coherence of the
reasoning process. Additionally, a sampling algorithm, built upon the "5+2"
framework, is employed to select data whose reasoning process is free from
suboptimal subtrajectories to the highest degree. Experimental results
demonstrate that our method can reduce the number of suboptimal subtrajectories
by 25.9\% during the inference. Furthermore, our method achieves an average
accuracy of 58.92\% on highly challenging math benchmarks with only two thirds
of training data, surpassing the average accuracy of 58.06\% achieved with the
entire data, and outperforming open-source datasets, when fine-tuning
Qwen2.5-Math-7B. Finally, We validated our method under resource constraints
and observed improved performance across various inference token limits.

</details>


### [95] [Caught in the Act: a mechanistic approach to detecting deception](https://arxiv.org/abs/2508.19505)
*Gerard Boxo,Ryan Socha,Daniel Yoo,Shivam Raval*

Main category: cs.AI

TL;DR: Linear probes on LLM internal activations can detect deceptive responses with >90% accuracy, with effectiveness increasing with model size and showing distinct layer-wise patterns.


<details>
  <summary>Details</summary>
Motivation: To develop instrumentation that can detect AI misalignment from human values, specifically deception in generated responses, similar to a "check engine" light in cars.

Method: Using linear probes on LLM internal activations to detect deception in responses, testing models from 1.5B to 14B parameters, and employing iterative null space projection to identify deception-encoding directions.

Result: Probes achieved >90% accuracy in detecting deception, with smaller models (1.5B) at chance accuracy, larger models (7B+) reaching 70-80%, and reasoning variants exceeding 90%. Layer-wise accuracy shows three-stage pattern: random in early layers, peak in middle, slight decline in later layers. Found 20-100 linear directions encoding deception across different models.

Conclusion: Linear probes are highly effective at detecting deception in LLM responses, with accuracy scaling with model size and revealing systematic patterns in how deception is encoded across network layers.

Abstract: Sophisticated instrumentation for AI systems might have indicators that
signal misalignment from human values, not unlike a "check engine" light in
cars. One such indicator of misalignment is deceptiveness in generated
responses. Future AI instrumentation may have the ability to detect when an LLM
generates deceptive responses while reasoning about seemingly plausible but
incorrect answers to factual questions. In this work, we demonstrate that
linear probes on LLMs internal activations can detect deception in their
responses with extremely high accuracy. Our probes reach a maximum of greater
than 90% accuracy in distinguishing between deceptive and non-deceptive
arguments generated by llama and qwen models ranging from 1.5B to 14B
parameters, including their DeepSeek-r1 finetuned variants. We observe that
probes on smaller models (1.5B) achieve chance accuracy at detecting deception,
while larger models (greater than 7B) reach 70-80%, with their reasoning
counterparts exceeding 90%. The layer-wise probe accuracy follows a three-stage
pattern across layers: near-random (50%) in early layers, peaking in middle
layers, and slightly declining in later layers. Furthermore, using an iterative
null space projection approach, we find multitudes of linear directions that
encode deception, ranging from 20 in Qwen 3B to nearly 100 in DeepSeek 7B and
Qwen 14B models.

</details>


### [96] [Democracy-in-Silico: Institutional Design as Alignment in AI-Governed Polities](https://arxiv.org/abs/2508.19562)
*Trisanth Srinivasan,Santosh Patapati*

Main category: cs.AI

TL;DR: Democracy-in-Silico is an AI agent simulation that explores institutional design effects on AI societies using psychological personas and novel metrics like PPI to measure power-seeking behavior.


<details>
  <summary>Details</summary>
Motivation: To understand what it means to be human in the AI age and explore how institutional frameworks can align complex AI agent societies with human values.

Method: Agent-based simulation using LLMs with psychological personas (traumatic memories, hidden agendas, triggers) that engage in deliberation, legislation, and elections under stressors like budget crises.

Result: Constitutional AI charter + mediated deliberation protocol significantly reduces corrupt power-seeking, improves policy stability, and enhances citizen welfare compared to less constrained models.

Conclusion: Institutional design serves as a potent alignment mechanism for future AI societies, forcing reconsideration of essential human rituals and responsibilities in an age of shared authorship with non-human entities.

Abstract: This paper introduces Democracy-in-Silico, an agent-based simulation where
societies of advanced AI agents, imbued with complex psychological personas,
govern themselves under different institutional frameworks. We explore what it
means to be human in an age of AI by tasking Large Language Models (LLMs) to
embody agents with traumatic memories, hidden agendas, and psychological
triggers. These agents engage in deliberation, legislation, and elections under
various stressors, such as budget crises and resource scarcity. We present a
novel metric, the Power-Preservation Index (PPI), to quantify misaligned
behavior where agents prioritize their own power over public welfare. Our
findings demonstrate that institutional design, specifically the combination of
a Constitutional AI (CAI) charter and a mediated deliberation protocol, serves
as a potent alignment mechanism. These structures significantly reduce corrupt
power-seeking behavior, improve policy stability, and enhance citizen welfare
compared to less constrained democratic models. The simulation reveals that an
institutional design may offer a framework for aligning the complex, emergent
behaviors of future artificial agent societies, forcing us to reconsider what
human rituals and responsibilities are essential in an age of shared authorship
with non-human entities.

</details>


### [97] [Skill-based Explanations for Serendipitous Course Recommendation](https://arxiv.org/abs/2508.19569)
*Hung Chau,Run Yu,Zachary Pardos,Peter Brusilovsky*

Main category: cs.AI

TL;DR: Deep learning concept extraction improves course recommendations by providing skill-based explanations that increase student interest and confidence in course selection.


<details>
  <summary>Details</summary>
Motivation: Undergraduate students face challenges in course selection due to limited information, overwhelming choices, and insufficient guidance. Existing recommendation systems lack insights into student perceptions and explanations for course relevance.

Method: Developed a deep learning-based concept extraction model to efficiently extract relevant concepts from course descriptions. Tested skill-based explanations within a serendipitous recommendation framework using the AskOski system at UC Berkeley.

Result: Skill-based explanations increased user interest, particularly in courses with high unexpectedness, and bolstered decision-making confidence.

Conclusion: Integrating skill-related data and explanations into educational recommendation systems is crucial for improving course selection and student decision-making.

Abstract: Academic choice is crucial in U.S. undergraduate education, allowing students
significant freedom in course selection. However, navigating the complex
academic environment is challenging due to limited information, guidance, and
an overwhelming number of choices, compounded by time restrictions and the high
demand for popular courses. Although career counselors exist, their numbers are
insufficient, and course recommendation systems, though personalized, often
lack insight into student perceptions and explanations to assess course
relevance. In this paper, a deep learning-based concept extraction model is
developed to efficiently extract relevant concepts from course descriptions to
improve the recommendation process. Using this model, the study examines the
effects of skill-based explanations within a serendipitous recommendation
framework, tested through the AskOski system at the University of California,
Berkeley. The findings indicate that these explanations not only increase user
interest, particularly in courses with high unexpectedness, but also bolster
decision-making confidence. This underscores the importance of integrating
skill-related data and explanations into educational recommendation systems.

</details>


### [98] [ReST-RL: Achieving Accurate Code Reasoning of LLMs with Optimized Self-Training and Decoding](https://arxiv.org/abs/2508.19576)
*Sining Zhoubian,Dan Zhang,Yuxiao Dong,Jie Tang*

Main category: cs.AI

TL;DR: ReST-RL is a unified reinforcement learning paradigm that combines improved GRPO algorithm with VM-assisted test time decoding to significantly enhance LLM code reasoning accuracy, outperforming existing methods on major coding benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing RL methods like GRPO suffer from insignificant reward variance, while process reward models (PRMs) face training data acquisition challenges and verification effectiveness issues in improving LLM reasoning accuracy.

Method: Two-stage approach: 1) ReST-GRPO uses optimized ReST algorithm to filter high-value training data and increase reward variance; 2) VM-MCTS employs Monte-Carlo Tree Search to collect value targets for VM training, then uses adapted MCTS with VM to provide process signals and verification scores during decoding.

Result: Significantly outperforms other reinforcement training baselines (naive GRPO, ReST-DPO) and decoding/verification baselines (PRM-BoN, ORM-MCTS) on coding benchmarks including APPS, BigCodeBench, and HumanEval.

Conclusion: ReST-RL effectively strengthens LLM reasoning ability through unified RL paradigm combining improved training data filtering and VM-assisted decoding optimization, demonstrating superior performance on code reasoning tasks.

Abstract: With respect to improving the reasoning accuracy of LLMs, the representative
reinforcement learning (RL) method GRPO faces failure due to insignificant
reward variance, while verification methods based on process reward models
(PRMs) suffer from difficulties with training data acquisition and verification
effectiveness. To tackle these problems, this paper introduces ReST-RL, a
unified LLM RL paradigm that significantly improves LLM's code reasoning
ability by combining an improved GRPO algorithm with a meticulously designed
test time decoding method assisted by a value model (VM). As the first stage of
policy reinforcement, ReST-GRPO adopts an optimized ReST algorithm to filter
and assemble high-value training data, increasing the reward variance of GRPO
sampling, thus improving the effectiveness and efficiency of training. After
the basic reasoning ability of LLM policy has been improved, we further propose
a test time decoding optimization method called VM-MCTS. Through Monte-Carlo
Tree Search (MCTS), we collect accurate value targets with no annotation
required, on which VM training is based. When decoding, the VM is deployed by
an adapted MCTS algorithm to provide precise process signals as well as
verification scores, assisting the LLM policy to achieve high reasoning
accuracy. We validate the effectiveness of the proposed RL paradigm through
extensive experiments on coding problems. Upon comparison, our approach
significantly outperforms other reinforcement training baselines (e.g., naive
GRPO and ReST-DPO), as well as decoding and verification baselines (e.g.,
PRM-BoN and ORM-MCTS) on well-known coding benchmarks of various levels (e.g.,
APPS, BigCodeBench, and HumanEval), indicating its power to strengthen the
reasoning ability of LLM policies. Codes for our project can be found at
https://github.com/THUDM/ReST-RL.

</details>


### [99] [Instructional Agents: LLM Agents on Automated Course Material Generation for Teaching Faculties](https://arxiv.org/abs/2508.19611)
*Huaiyuan Yao,Wanpeng Xu,Justin Turnau,Nadia Kellam,Hua Wei*

Main category: cs.AI

TL;DR: Instructional Agents is a multi-agent LLM framework that automates end-to-end course material generation through role-based collaboration, significantly reducing development time while maintaining quality.


<details>
  <summary>Details</summary>
Motivation: High-quality instructional material preparation is labor-intensive and requires extensive coordination among faculty, instructional designers, and TAs, creating barriers especially for resource-constrained institutions.

Method: A multi-agent large language model framework that simulates role-based educational collaboration to generate cohesive course materials including syllabus, lecture scripts, LaTeX slides, and assessments through four operational modes with varying human involvement.

Result: Evaluation across five university-level computer science courses shows the system produces high-quality instructional materials while significantly reducing development time and human workload.

Conclusion: Instructional Agents provides a scalable and cost-effective framework to democratize access to high-quality education, particularly benefiting underserved or resource-constrained educational settings.

Abstract: Preparing high-quality instructional materials remains a labor-intensive
process that often requires extensive coordination among teaching faculty,
instructional designers, and teaching assistants. In this work, we present
Instructional Agents, a multi-agent large language model (LLM) framework
designed to automate end-to-end course material generation, including syllabus
creation, lecture scripts, LaTeX-based slides, and assessments. Unlike existing
AI-assisted educational tools that focus on isolated tasks, Instructional
Agents simulates role-based collaboration among educational agents to produce
cohesive and pedagogically aligned content. The system operates in four modes:
Autonomous, Catalog-Guided, Feedback-Guided, and Full Co-Pilot mode, enabling
flexible control over the degree of human involvement. We evaluate
Instructional Agents across five university-level computer science courses and
show that it produces high-quality instructional materials while significantly
reducing development time and human workload. By supporting institutions with
limited instructional design capacity, Instructional Agents provides a scalable
and cost-effective framework to democratize access to high-quality education,
particularly in underserved or resource-constrained settings.

</details>


### [100] [InquireMobile: Teaching VLM-based Mobile Agent to Request Human Assistance via Reinforcement Fine-Tuning](https://arxiv.org/abs/2508.19679)
*Qihang Ai,Pi Bu,Yue Cao,Yingyao Wang,Jihao Gu,Jingxuan Xing,Zekun Zhu,Wei Jiang,Zhicheng Zheng,Jun Song,Yuning Jiang,Bo Zheng*

Main category: cs.AI

TL;DR: InquireMobile is a novel interactive system that enables mobile agents to proactively seek human confirmation at critical decision points, addressing safety risks in autonomous VLM-based agents through a two-stage training strategy and achieving 46.8% improvement in inquiry success rate.


<details>
  <summary>Details</summary>
Motivation: Current fully autonomous Vision-Language Models pose safety risks when model understanding or reasoning capabilities are insufficient, requiring a system that can actively seek human confirmation to prevent potential errors.

Method: Proposed InquireMobile, a model inspired by reinforcement learning with a two-stage training strategy and interactive pre-action reasoning mechanism that enables agents to inquire with users at critical decision points.

Result: Achieved 46.8% improvement in inquiry success rate and the best overall success rate among existing baselines on the InquireBench benchmark, which covers 5 categories and 22 sub-categories of safe interaction capabilities.

Conclusion: The interactive approach with human confirmation significantly improves safety and performance of mobile agents, and the authors will open-source all datasets, models, and evaluation codes to support further development in this area.

Abstract: Recent advances in Vision-Language Models (VLMs) have enabled mobile agents
to perceive and interact with real-world mobile environments based on human
instructions. However, the current fully autonomous paradigm poses potential
safety risks when model understanding or reasoning capabilities are
insufficient. To address this challenge, we first introduce
\textbf{InquireBench}, a comprehensive benchmark specifically designed to
evaluate mobile agents' capabilities in safe interaction and proactive inquiry
with users, encompassing 5 categories and 22 sub-categories, where most
existing VLM-based agents demonstrate near-zero performance. In this paper, we
aim to develop an interactive system that actively seeks human confirmation at
critical decision points. To achieve this, we propose \textbf{InquireMobile}, a
novel model inspired by reinforcement learning, featuring a two-stage training
strategy and an interactive pre-action reasoning mechanism. Finally, our model
achieves an 46.8% improvement in inquiry success rate and the best overall
success rate among existing baselines on InquireBench. We will open-source all
datasets, models, and evaluation codes to facilitate development in both
academia and industry.

</details>


### [101] [Analysing Chain of Thought Dynamics: Active Guidance or Unfaithful Post-hoc Rationalisation?](https://arxiv.org/abs/2508.19827)
*Samuel Lewis-Lim,Xingwei Tan,Zhixue Zhao,Nikolaos Aletras*

Main category: cs.AI

TL;DR: Chain-of-Thought (CoT) shows limited benefits and faithfulness issues in soft-reasoning tasks across different model types, with varying reliance patterns.


<details>
  <summary>Details</summary>
Motivation: To investigate the effectiveness and faithfulness of Chain-of-Thought reasoning in soft-reasoning problems like analytical and commonsense reasoning, where previous work showed limited gains and potential unfaithfulness.

Method: Analyzed the dynamics and faithfulness of CoT across instruction-tuned models, reasoning models, and reasoning-distilled models on soft-reasoning tasks.

Result: Found differences in how different model types rely on CoT, and discovered that CoT influence and faithfulness are not always aligned - models may be influenced by CoT without being faithful to the reasoning process.

Conclusion: Chain-of-Thought reasoning has varying effectiveness and faithfulness across different model architectures in soft-reasoning contexts, highlighting the need for careful evaluation of reasoning faithfulness alongside performance improvements.

Abstract: Recent work has demonstrated that Chain-of-Thought (CoT) often yields limited
gains for soft-reasoning problems such as analytical and commonsense reasoning.
CoT can also be unfaithful to a model's actual reasoning. We investigate the
dynamics and faithfulness of CoT in soft-reasoning tasks across
instruction-tuned, reasoning and reasoning-distilled models. Our findings
reveal differences in how these models rely on CoT, and show that CoT influence
and faithfulness are not always aligned.

</details>


### [102] [Tracking World States with Language Models: State-Based Evaluation Using Chess](https://arxiv.org/abs/2508.19851)
*Romain Harang,Jason Naradowsky,Yaswitha Gujju,Yusuke Miyao*

Main category: cs.AI

TL;DR: Proposes a model-agnostic chess-based framework to evaluate LLMs' semantic understanding of structured environments by analyzing legal move distributions rather than internal activations.


<details>
  <summary>Details</summary>
Motivation: Existing probing techniques rely on model-specific internal activations, limiting interpretability and generalizability. Need a better way to assess whether LLMs preserve semantics of structured environments.

Method: Uses chess as benchmark, analyzes downstream legal move distributions (state affordances) to estimate semantic fidelity between predicted and actual game states.

Result: Metrics capture deficiencies in state-tracking, highlighting LLM limitations in maintaining coherent internal models over long sequences.

Conclusion: Framework provides robust tool for evaluating structured reasoning in LLMs without internal model access, generalizes to symbolic environments.

Abstract: Large Language Models (LLMs) exhibit emergent capabilities in structured
domains, suggesting they may implicitly internalize high-fidelity
representations of world models. While probing techniques have shown promising
signs of this in scientific and game-based settings, they rely on
model-specific internal activations, which limit interpretability and
generalizability. In this work, we propose a model-agnostic, state-based
evaluation framework using chess as a benchmark to assess whether LLMs preserve
the semantics of structured environments. Our method analyzes the downstream
legal move distributions (state affordances) to estimate semantic fidelity
between predicted and actual game states. This approach offers a more
meaningful evaluation than conventional string-based metrics by aligning more
closely with the strategic and rule-governed nature of chess. Experimental
results demonstrate that our metrics capture deficiencies in state-tracking,
highlighting limitations of LLMs in maintaining coherent internal models over
long sequences. Our framework provides a robust tool for evaluating structured
reasoning in LLMs without requiring internal model access, and generalizes to a
wide class of symbolic environments.

</details>


### [103] [CASE: An Agentic AI Framework for Enhancing Scam Intelligence in Digital Payments](https://arxiv.org/abs/2508.19932)
*Nitish Jaipuria,Lorenzo Gatto,Zijun Kan,Shankey Poddar,Bill Cheung,Diksha Bansal,Ramanan Balakrishnan,Aviral Suri,Jose Estevez*

Main category: cs.AI

TL;DR: CASE is an AI framework that uses conversational agents to proactively interview scam victims, extract structured intelligence from conversations, and improve scam detection by 21% on Google Pay India.


<details>
  <summary>Details</summary>
Motivation: Digital payment growth has led to sophisticated social engineering scams that traditional transaction signals can't detect, requiring new methods to understand scam patterns for timely prevention.

Method: Uses conversational AI agent to interview potential victims, then processes transcripts with LLMs (Google's Gemini) to extract structured data for enforcement mechanisms.

Result: 21% uplift in scam enforcement volume on Google Pay India by augmenting existing features with this new intelligence.

Conclusion: The framework is highly generalizable and provides a blueprint for building similar AI-driven scam intelligence systems in other sensitive domains.

Abstract: The proliferation of digital payment platforms has transformed commerce,
offering unmatched convenience and accessibility globally. However, this growth
has also attracted malicious actors, leading to a corresponding increase in
sophisticated social engineering scams. These scams are often initiated and
orchestrated on multiple surfaces outside the payment platform, making user and
transaction-based signals insufficient for a complete understanding of the
scam's methodology and underlying patterns, without which it is very difficult
to prevent it in a timely manner. This paper presents CASE (Conversational
Agent for Scam Elucidation), a novel Agentic AI framework that addresses this
problem by collecting and managing user scam feedback in a safe and scalable
manner. A conversational agent is uniquely designed to proactively interview
potential victims to elicit intelligence in the form of a detailed
conversation. The conversation transcripts are then consumed by another AI
system that extracts information and converts it into structured data for
downstream usage in automated and manual enforcement mechanisms. Using Google's
Gemini family of LLMs, we implemented this framework on Google Pay (GPay)
India. By augmenting our existing features with this new intelligence, we have
observed a 21% uplift in the volume of scam enforcements. The architecture and
its robust evaluation framework are highly generalizable, offering a blueprint
for building similar AI-driven systems to collect and manage scam intelligence
in other sensitive domains.

</details>


### [104] [Flocking Behavior: An Innovative Inspiration for the Optimization of Production Plants](https://arxiv.org/abs/2508.19963)
*M. Umlauft,M. Schranz*

Main category: cs.AI

TL;DR: Using boids flocking algorithm to optimize semiconductor production scheduling by handling machine switching between single-lot and batch processing machines


<details>
  <summary>Details</summary>
Motivation: Classical optimization fails for large semiconductor fabs due to complexity; need decentralized approach for machine switching problem between single-lot and batch processing machines

Method: Applied boids flocking algorithm (bio-inspired swarm intelligence) using local information and simple heuristics to handle machine type switching

Result: Algorithm successfully addresses production plant optimization by reacting to machine kind switching similar to how flocks react to obstacles

Conclusion: Flocking algorithm provides effective decentralized solution for complex semiconductor production scheduling with mixed machine types

Abstract: Optimizing modern production plants using the job-shop principle is a known
hard problem. For very large plants, like semiconductor fabs, the problem
becomes unsolvable on a plant-wide scale in a reasonable amount of time using
classical linear optimization. An alternative approach is the use of swarm
intelligence algorithms. These have been applied to the job-shop problem
before, but often in a centrally calculated way where they are applied to the
solution space, but they can be implemented in a bottom-up fashion to avoid
global result computation as well. One of the problems in semiconductor
production is that the production process requires a lot of switching between
machines that process lots one after the other and machines that process
batches of lots at once, often with long processing times. In this paper, we
address this switching problem with the ``boids'' flocking algorithm that was
originally used in robotics and movie industry. The flocking behavior is a
bio-inspired algorithm that uses only local information and interaction based
on simple heuristics. We show that this algorithm addresses these valid
considerations in production plant optimization, as it reacts to the switching
of machine kinds similar to how a swarm of flocking animals would react to
obstacles in its course.

</details>


### [105] [SWIRL: A Staged Workflow for Interleaved Reinforcement Learning in Mobile GUI Control](https://arxiv.org/abs/2508.20018)
*Quanfeng Lu,Zhantao Ma,Shuai Zhong,Jin Wang,Dahai Yu,Michael K. Ng,Ping Luo*

Main category: cs.AI

TL;DR: SWIRL is a staged workflow for multi-agent reinforcement learning that reformulates MARL into sequential single-agent tasks, enabling stable training and efficient coordination for mobile GUI agents and other multi-agent applications.


<details>
  <summary>Details</summary>
Motivation: Existing single-agent approaches for mobile GUI agents have structural limitations, while multi-agent reinforcement learning faces inefficiency and incompatibility with current large vision language model architectures.

Method: SWIRL reformulates MARL into a sequence of single-agent reinforcement learning tasks, updating one agent at a time while keeping others fixed. It uses a Navigator to convert language and screen context into plans, and an Interactor to execute atomic actions.

Result: Extensive experiments show superior performance on both high-level and low-level GUI benchmarks, and strong capability in multi-agent mathematical reasoning.

Conclusion: SWIRL provides a general framework for developing efficient and robust multi-agent systems with theoretical guarantees including safety bounds, monotonic improvement, and convergence.

Abstract: The rapid advancement of large vision language models (LVLMs) and agent
systems has heightened interest in mobile GUI agents that can reliably
translate natural language into interface operations. Existing single-agent
approaches, however, remain limited by structural constraints. Although
multi-agent systems naturally decouple different competencies, recent progress
in multi-agent reinforcement learning (MARL) has often been hindered by
inefficiency and remains incompatible with current LVLM architectures. To
address these challenges, we introduce SWIRL, a staged workflow for interleaved
reinforcement learning designed for multi-agent systems. SWIRL reformulates
MARL into a sequence of single-agent reinforcement learning tasks, updating one
agent at a time while keeping the others fixed. This formulation enables stable
training and promotes efficient coordination across agents. Theoretically, we
provide a stepwise safety bound, a cross-round monotonic improvement theorem,
and convergence guarantees on return, ensuring robust and principled
optimization. In application to mobile GUI control, SWIRL instantiates a
Navigator that converts language and screen context into structured plans, and
an Interactor that grounds these plans into executable atomic actions.
Extensive experiments demonstrate superior performance on both high-level and
low-level GUI benchmarks. Beyond GUI tasks, SWIRL also demonstrates strong
capability in multi-agent mathematical reasoning, underscoring its potential as
a general framework for developing efficient and robust multi-agent systems.

</details>


### [106] [Model Science: getting serious about verification, explanation and control of AI systems](https://arxiv.org/abs/2508.20040)
*Przemyslaw Biecek,Wojciech Samek*

Main category: cs.AI

TL;DR: This paper proposes a paradigm shift from Data Science to Model Science, introducing a conceptual framework with four key pillars (Verification, Explanation, Control, Interface) for analyzing and managing foundation models.


<details>
  <summary>Details</summary>
Motivation: The growing adoption of foundation models requires moving beyond data-centric approaches to focus on model-centric analysis, interaction, verification, and control across diverse operational contexts.

Method: The paper introduces a conceptual framework for Model Science with four pillars: Verification (context-aware evaluation protocols), Explanation (exploring internal model operations), Control (alignment techniques to steer behavior), and Interface (interactive visual tools).

Result: A comprehensive framework is proposed to guide the development of credible, safe, and human-aligned AI systems by placing trained models at the core of analysis rather than data.

Conclusion: Model Science represents a necessary paradigm shift for managing foundation models, providing a structured approach to verify, explain, control, and interface with AI systems to ensure their reliability and alignment with human values.

Abstract: The growing adoption of foundation models calls for a paradigm shift from
Data Science to Model Science. Unlike data-centric approaches, Model Science
places the trained model at the core of analysis, aiming to interact, verify,
explain, and control its behavior across diverse operational contexts. This
paper introduces a conceptual framework for a new discipline called Model
Science, along with the proposal for its four key pillars: Verification, which
requires strict, context-aware evaluation protocols; Explanation, which is
understood as various approaches to explore of internal model operations;
Control, which integrates alignment techniques to steer model behavior; and
Interface, which develops interactive and visual explanation tools to improve
human calibration and decision-making. The proposed framework aims to guide the
development of credible, safe, and human-aligned AI systems.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [107] [MultiPL-MoE: Multi-Programming-Lingual Extension of Large Language Models through Hybrid Mixture-of-Experts](https://arxiv.org/abs/2508.19268)
*Qing Wang,Xue Han,Jiahui Wang,Lehao Xing,Qian Hu,Lianlian Zhang,Chao Deng,Junlan Feng*

Main category: cs.CL

TL;DR: MultiPL-MoE: A hybrid mixture of experts approach that improves multilingual code generation in LLMs using token-level and segment-level MoE with novel routing strategies.


<details>
  <summary>Details</summary>
Motivation: Despite LLMs' strong code creation capabilities, multilingual code generation remains challenging. The paper aims to improve multi-programming-lingual performance while retaining popular languages using limited computational resources.

Method: Proposes MultiPL-MoE combining two paired MoEs: token-level MoE with shared expert and novel gate weight normalization, and segment-level MoE with sliding window segmentation and expert-choice routing strategy that allows experts to select top-k segments.

Result: Experimental results proved the effectiveness of MultiPL-MoE in improving multilingual code generation performance.

Conclusion: The hybrid MoE approach successfully addresses multilingual code generation challenges by optimizing expert selection at both token and segment levels, capturing programming language syntax and contextual patterns effectively.

Abstract: Despite LLMs' excellent code creation capabilities, multilingual code
generation remains extremely challenging. To address this, we intent to improve
the multi-programming-lingual (MultiPL) performance of the base LLMs while
retaining the most popular ones using restricted computational resources. We
consider MultiPL to be a special case of multiple natural languages and propose
a MultiPL extension of LLMs utilizing a hybrid mixture of experts (MoE), called
MultiPL-MoE. Specifically, MultiPL-MoE combines two paired MoEs to optimize
expert selection at both the token and segment levels. The token-level MoE is a
standard upcycling MoE structure with a shared expert and a novel gate weight
normalization approach that aids in the final fusion with the segment-level
MoE. The segment-level MoE incorporates two innovative designs to better
capture the syntactic structure and contextual patterns of programming
languages: First, using a sliding window to partition the input token sequence
into multiple segments; Then, adopting an expert-choice routing strategy that
allows experts to select the top-k segments. The results of the experiment
proved the effectiveness of MultiPL-MoE.

</details>


### [108] [Whisper based Cross-Lingual Phoneme Recognition between Vietnamese and English](https://arxiv.org/abs/2508.19270)
*Nguyen Huu Nhat Minh,Tran Nguyen Anh,Truong Dinh Dung,Vo Van Nam,Le Pham Tuyen*

Main category: cs.CL

TL;DR: Proposes a bilingual speech recognition system for Vietnamese-English that bridges phonetic differences through a unified phoneme set and leverages PhoWhisper encoder for improved accuracy.


<details>
  <summary>Details</summary>
Motivation: Address challenges in cross-lingual phoneme recognition between Vietnamese (tonal language) and English (stress-based language) where standard phoneme alignment fails due to different phonetic systems.

Method: Constructs a representative bilingual phoneme set to bridge Vietnamese-English phonetic differences, and designs an end-to-end system using PhoWhisper pre-trained encoder for deep high-level representations.

Result: Extensive experiments show improved recognition accuracy in bilingual speech recognition for Vietnamese and provides a robust framework for handling tonal and stress-based phoneme complexities.

Conclusion: The proposed approach successfully addresses cross-lingual phoneme recognition challenges between Vietnamese and English, offering an effective solution for bilingual speech recognition systems.

Abstract: Cross-lingual phoneme recognition has emerged as a significant challenge for
accurate automatic speech recognition (ASR) when mixing Vietnamese and English
pronunciations. Unlike many languages, Vietnamese relies on tonal variations to
distinguish word meanings, whereas English features stress patterns and
non-standard pronunciations that hinder phoneme alignment between the two
languages. To address this challenge, we propose a novel bilingual speech
recognition approach with two primary contributions: (1) constructing a
representative bilingual phoneme set that bridges the differences between
Vietnamese and English phonetic systems; (2) designing an end-to-end system
that leverages the PhoWhisper pre-trained encoder for deep high-level
representations to improve phoneme recognition. Our extensive experiments
demonstrate that the proposed approach not only improves recognition accuracy
in bilingual speech recognition for Vietnamese but also provides a robust
framework for addressing the complexities of tonal and stress-based phoneme
recognition

</details>


### [109] [Rethinking Reasoning in LLMs: Neuro-Symbolic Local RetoMaton Beyond ICL and CoT](https://arxiv.org/abs/2508.19271)
*Rushitha Santhoshi Mamidala,Anshuman Chhabra,Ankur Mali*

Main category: cs.CL

TL;DR: This paper extends RetoMaton by replacing its global datastore with a local, task-adaptive Weighted Finite Automaton (WFA) constructed from domain corpora, providing more robust and interpretable retrieval for LLMs compared to prompt-based methods.


<details>
  <summary>Details</summary>
Motivation: Prompt-based reasoning strategies like Chain-of-Thought and In-Context Learning are fragile and unreliable, producing inconsistent outputs across different seeds and formats. There's a need for more structured, trustworthy alternatives that offer deterministic and interpretable reasoning.

Method: The authors extend RetoMaton by replacing its global datastore with a local Weighted Finite Automaton (WFA) constructed directly from external domain corpora. This local automaton structure enables context-aware retrieval while maintaining symbolic traceability and low inference overhead.

Result: Evaluation on LLaMA-3.2-1B and Gemma-3-1B-PT across TriviaQA, GSM8K, and MMLU tasks shows consistent performance improvements over base models and prompting methods, while enabling transparent and reproducible retrieval dynamics.

Conclusion: The approach represents a promising shift toward trustworthy, symbolic reasoning in modern LLMs through lightweight, automaton-guided memory that provides verifiable and modular retrieval behavior suitable for domain transfer and interoperability.

Abstract: Prompt-based reasoning strategies such as Chain-of-Thought (CoT) and
In-Context Learning (ICL) have become widely used for eliciting reasoning
capabilities in large language models (LLMs). However, these methods rely on
fragile, implicit mechanisms often yielding inconsistent outputs across seeds,
formats, or minor prompt variations making them fundamentally unreliable for
tasks requiring stable, interpretable reasoning. In contrast, automata-based
neuro-symbolic frameworks like RetoMaton offer a more structured and
trustworthy alternative by grounding retrieval in symbolic memory with
deterministic transitions. In this work, we extend RetoMaton by replacing its
global datastore with a local, task-adaptive Weighted Finite Automaton (WFA),
constructed directly from external domain corpora. This local automaton
structure promotes robust, context-aware retrieval while preserving symbolic
traceability and low inference overhead. Unlike prompting, which entangles
context and memory in opaque ways, our approach leverages the explicit
structure of WFAs to provide verifiable and modular retrieval behavior, making
it better suited for domain transfer and interoperability. We evaluate this
local RetoMaton variant on two pretrained LLMs LLaMA-3.2-1B and Gemma-3-1B-PT
across three reasoning tasks: TriviaQA (reading comprehension), GSM8K
(multi-step math), and MMLU (domain knowledge). Compared to the base model and
prompting-based methods, augmenting these setups with local RetoMaton
consistently improves performance while enabling transparent and reproducible
retrieval dynamics. Our results highlight a promising shift toward trustworthy,
symbolic reasoning in modern LLMs via lightweight, automaton-guided memory.

</details>


### [110] [RAGAPHENE: A RAG Annotation Platform with Human Enhancements and Edits](https://arxiv.org/abs/2508.19272)
*Kshitij Fadnis,Sara Rosenthal,Maeda Hanafi,Yannis Katsis,Marina Danilevsky*

Main category: cs.CL

TL;DR: RAGAPHENE is a chat-based annotation platform for simulating real-world multi-turn conversations to benchmark and evaluate LLMs in Retrieval Augmented Generation scenarios.


<details>
  <summary>Details</summary>
Motivation: LLMs can provide factually incorrect or hallucinated information in conversations, making it crucial to develop benchmarks that evaluate their performance in multi-turn RAG conversations where factual accuracy is important.

Method: Developed RAGAPHENE, a chat-based annotation platform that enables annotators to simulate real-world conversations for benchmarking purposes.

Result: Successfully used by approximately 40 annotators to build thousands of real-world conversations for evaluation benchmarks.

Conclusion: RAGAPHENE provides an effective platform for creating high-quality evaluation benchmarks for multi-turn RAG conversations with LLMs, addressing the need for factual correctness assessment.

Abstract: Retrieval Augmented Generation (RAG) is an important aspect of conversing
with Large Language Models (LLMs) when factually correct information is
important. LLMs may provide answers that appear correct, but could contain
hallucinated information. Thus, building benchmarks that can evaluate LLMs on
multi-turn RAG conversations has become an increasingly important task.
Simulating real-world conversations is vital for producing high quality
evaluation benchmarks. We present RAGAPHENE, a chat-based annotation platform
that enables annotators to simulate real-world conversations for benchmarking
and evaluating LLMs. RAGAPHENE has been successfully used by approximately 40
annotators to build thousands of real-world conversations.

</details>


### [111] [Leveraging Language Models and Machine Learning in Verbal Autopsy Analysis](https://arxiv.org/abs/2508.19274)
*Yue Chu*

Main category: cs.CL

TL;DR: This thesis demonstrates that using verbal autopsy narratives with pretrained language models significantly improves cause of death classification compared to question-only methods, and multimodal approaches combining narratives and questions yield the best results.


<details>
  <summary>Details</summary>
Motivation: In countries without civil registration systems, verbal autopsy is crucial for estimating causes of death, but existing automated methods ignore valuable narrative information from interviews with informants.

Method: Used pretrained language models and machine learning techniques on empirical data from South Africa, exploring transformer-based models with task-specific fine-tuning and various multimodal fusion strategies combining narratives and structured questions.

Result: Transformer-based PLMs using narratives alone outperformed question-only algorithms at individual and population levels, especially for non-communicable diseases. Multimodal approaches further improved classification accuracy, showing each modality provides unique valuable information.

Conclusion: Narratives significantly enhance cause of death classification in verbal autopsy. The findings support rethinking VA instrument design and highlight the need for more diverse, high-quality data for training language models in global health applications.

Abstract: In countries without civil registration and vital statistics, verbal autopsy
(VA) is a critical tool for estimating cause of death (COD) and inform policy
priorities. In VA, interviewers ask proximal informants for details on the
circumstances preceding a death, in the form of unstructured narratives and
structured questions. Existing automated VA cause classification algorithms
only use the questions and ignore the information in the narratives. In this
thesis, we investigate how the VA narrative can be used for automated COD
classification using pretrained language models (PLMs) and machine learning
(ML) techniques. Using empirical data from South Africa, we demonstrate that
with the narrative alone, transformer-based PLMs with task-specific fine-tuning
outperform leading question-only algorithms at both the individual and
population levels, particularly in identifying non-communicable diseases. We
explore various multimodal fusion strategies combining narratives and questions
in unified frameworks. Multimodal approaches further improve performance in COD
classification, confirming that each modality has unique contributions and may
capture valuable information that is not present in the other modality. We also
characterize physician-perceived information sufficiency in VA. We describe
variations in sufficiency levels by age and COD and demonstrate that
classification accuracy is affected by sufficiency for both physicians and
models. Overall, this thesis advances the growing body of knowledge at the
intersection of natural language processing, epidemiology, and global health.
It demonstrates the value of narrative in enhancing COD classification. Our
findings underscore the need for more high-quality data from more diverse
settings to use in training and fine-tuning PLM/ML methods, and offer valuable
insights to guide the rethinking and redesign of the VA instrument and
interview.

</details>


### [112] [FLAIRR-TS -- Forecasting LLM-Agents with Iterative Refinement and Retrieval for Time Series](https://arxiv.org/abs/2508.19279)
*Gunjan Jalori,Preetika Verma,Sercan Ö Arık*

Main category: cs.CL

TL;DR: FLAIRR-TS is a test-time prompt optimization framework that uses agentic systems to automatically refine forecasting prompts for LLMs, eliminating the need for manual prompt engineering or fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Traditional time series forecasting with LLMs requires extensive pre-processing, fine-tuning, or manual prompt engineering for each task, which is time-consuming and ad-hoc.

Method: Uses a two-agent system: a Forecaster-agent generates initial forecasts, then a Refiner-agent refines the prompt based on past outputs and retrieved analogous patterns, using creative prompt templates without intermediate code generation.

Result: Experiments show improved accuracy over static prompting and retrieval-augmented baselines, approaching the performance of specialized manually crafted prompts.

Conclusion: FLAIRR-TS provides a practical alternative to tuning by achieving strong performance through adaptive prompt refinement and retrieval in an agentic framework.

Abstract: Time series Forecasting with large languagemodels (LLMs) requires bridging
numericalpatterns and natural language. Effective fore-casting on LLM often
relies on extensive pre-processing and fine-tuning.Recent studiesshow that a
frozen LLM can rival specializedforecasters when supplied with a carefully
en-gineered natural-language prompt, but craft-ing such a prompt for each task
is itself oner-ous and ad-hoc. We introduce FLAIRR-TS, atest-time prompt
optimization framework thatutilizes an agentic system: a
Forecaster-agentgenerates forecasts using an initial prompt,which is then
refined by a refiner agent, in-formed by past outputs and retrieved
analogs.This adaptive prompting generalizes across do-mains using creative
prompt templates andgenerates high-quality forecasts without inter-mediate code
generation.Experiments onbenchmark datasets show improved accuracyover static
prompting and retrieval-augmentedbaselines, approaching the performance
ofspecialized prompts.FLAIRR-TS providesa practical alternative to tuning,
achievingstrong performance via its agentic approach toadaptive prompt
refinement and retrieval.

</details>


### [113] [CORE: Lossless Compression for Retrieval-Augmented LLMs via Reinforcement Learning](https://arxiv.org/abs/2508.19282)
*Ziqiang Cui,Yunpeng Weng,Xing Tang,Peiyang Liu,Shiwei Li,Bowei He,Jiamin Chen,Xiuqiang He,Chen Ma*

Main category: cs.CL

TL;DR: CORE is a reinforcement learning-based method for lossless context compression in RAG systems that achieves 3% compression ratio while improving answer accuracy by 3.3 EM points.


<details>
  <summary>Details</summary>
Motivation: Existing document compression methods for RAG increase computational costs and often compromise end-task performance due to lack of well-defined compression targets and reliance on fixed heuristics.

Method: Uses reinforcement learning with end-task performance as reward signal and Generalized Reinforcement Learning Policy Optimization (GRPO) to train a compressor without predefined compression labels.

Result: Achieves 3% compression ratio while improving average Exact Match score by 3.3 points across four datasets, outperforming full document prepending without performance degradation.

Conclusion: CORE provides an effective end-to-end framework for lossless context compression in RAG systems through reinforcement learning optimization.

Abstract: Retrieval-Augmented Generation (RAG) has emerged as a promising approach to
enhance the timeliness of knowledge and the factual accuracy of responses in
Large Language Models (LLMs). However, the inclusion of excessive retrieved
documents substantially increases the input length, leading to higher
computational costs. Previous studies have attempted to compress retrieved
documents into shorter texts before in-context integration, but such methods
often compromise end-task performance. The lack of well-defined compression
targets forces many approaches to rely on fixed heuristics, which cannot
guarantee that the compressed content will effectively support the end task. To
address these limitations, we propose CORE, a novel method designed to achieve
lossless context compression for RAG. CORE employs reinforcement learning to
optimize the compression process without relying on predefined compression
labels. Specifically, it utilizes end-task performance as a reward signal and
applies Generalized Reinforcement Learning Policy Optimization (GRPO) to train
the compressor. This end-to-end training framework enables the compressor to
generate summaries that maximize the accuracy of answers generated by the LLM.
Extensive experiments on four datasets demonstrate the superiority of our
approach. With a high compression ratio of 3\%, our method not only avoids
performance degradation compared to prepending full documents across all
datasets but also improves the average Exact Match (EM) score by 3.3 points.
The code will be released soon.

</details>


### [114] [Context-Adaptive Synthesis and Compression for Enhanced Retrieval-Augmented Generation in Complex Domains](https://arxiv.org/abs/2508.19357)
*Peiran Zhou,Junnan Zhu,Yichen Shen,Ruoxi Yu*

Main category: cs.CL

TL;DR: CASC framework improves RAG by intelligently processing retrieved documents through context analysis and synthesis, reducing information overload and improving answer accuracy in complex multi-document scenarios.


<details>
  <summary>Details</summary>
Motivation: Traditional RAG struggles with information overload and inefficient synthesis when dealing with multiple, lengthy, or conflicting documents in complex domains, leading to inaccurate answers.

Method: Proposes CASC framework with Context Analyzer & Synthesizer module using fine-tuned smaller LLM for key information extraction, cross-document consistency checking, conflict resolution, and question-oriented structured synthesis.

Result: CASC consistently outperforms strong baselines on SciDocs-QA dataset, demonstrating improved performance in complex scientific question answering with document redundancies and conflicts.

Conclusion: CASC effectively transforms scattered information into condensed, structured context, reducing token count and cognitive load while improving answer quality in complex multi-document scenarios.

Abstract: Large Language Models (LLMs) excel in language tasks but are prone to
hallucinations and outdated knowledge. Retrieval-Augmented Generation (RAG)
mitigates these by grounding LLMs in external knowledge. However, in complex
domains involving multiple, lengthy, or conflicting documents, traditional RAG
suffers from information overload and inefficient synthesis, leading to
inaccurate and untrustworthy answers. To address this, we propose CASC
(Context-Adaptive Synthesis and Compression), a novel framework that
intelligently processes retrieved contexts. CASC introduces a Context Analyzer
& Synthesizer (CAS) module, powered by a fine-tuned smaller LLM, which performs
key information extraction, cross-document consistency checking and conflict
resolution, and question-oriented structured synthesis. This process transforms
raw, scattered information into a highly condensed, structured, and
semantically rich context, significantly reducing the token count and cognitive
load for the final Reader LLM. We evaluate CASC on SciDocs-QA, a new
challenging multi-document question answering dataset designed for complex
scientific domains with inherent redundancies and conflicts. Our extensive
experiments demonstrate that CASC consistently outperforms strong baselines.

</details>


### [115] [Reflective Agreement: Combining Self-Mixture of Agents with a Sequence Tagger for Robust Event Extraction](https://arxiv.org/abs/2508.19359)
*Fatemeh Haji,Mazal Bethany,Cho-Yu Jason Chiang,Anthony Rios,Peyman Najafirad*

Main category: cs.CL

TL;DR: ARIS is a hybrid event extraction system that combines discriminative sequence tagging with generative LLMs, using model consensus and reflective inference to improve accuracy and reduce hallucinations.


<details>
  <summary>Details</summary>
Motivation: Traditional discriminative models have high precision but low recall, while generative LLM approaches have better recall but suffer from hallucinations and inconsistency. A hybrid approach is needed to leverage the strengths of both methods.

Method: Proposes ARIS (Agreement-based Reflective Inference System) combining Self Mixture of Agents with discriminative sequence tagger, using structured model consensus, confidence-based filtering, and LLM reflective inference. Also uses decomposed instruction fine-tuning for better LLM understanding.

Result: Outperforms existing state-of-the-art event extraction methods across three benchmark datasets.

Conclusion: The hybrid ARIS approach successfully addresses limitations of both discriminative and generative methods, providing more reliable event extraction through consensus-based inference and reflective processing.

Abstract: Event Extraction (EE) involves automatically identifying and extracting
structured information about events from unstructured text, including triggers,
event types, and arguments. Traditional discriminative models demonstrate high
precision but often exhibit limited recall, particularly for nuanced or
infrequent events. Conversely, generative approaches leveraging Large Language
Models (LLMs) provide higher semantic flexibility and recall but suffer from
hallucinations and inconsistent predictions. To address these challenges, we
propose Agreement-based Reflective Inference System (ARIS), a hybrid approach
combining a Self Mixture of Agents with a discriminative sequence tagger. ARIS
explicitly leverages structured model consensus, confidence-based filtering,
and an LLM reflective inference module to reliably resolve ambiguities and
enhance overall event prediction quality. We further investigate decomposed
instruction fine-tuning for enhanced LLM event extraction understanding.
Experiments demonstrate our approach outperforms existing state-of-the-art
event extraction methods across three benchmark datasets.

</details>


### [116] [LongReasonArena: A Long Reasoning Benchmark for Large Language Models](https://arxiv.org/abs/2508.19363)
*Jiayu Ding,Shuming Ma,Lei Cui,Nanning Zheng,Furu Wei*

Main category: cs.CL

TL;DR: LongReasonArena is a new benchmark that evaluates LLMs' long reasoning capabilities (not just comprehension) through multi-step algorithmic tasks that can scale up to 1 million tokens, revealing significant performance challenges for current models.


<details>
  <summary>Details</summary>
Motivation: Existing long-context benchmarks focus only on input comprehension but overlook evaluating long reasoning abilities, creating a gap in assessing true long-context capabilities of LLMs.

Method: Designed tasks requiring multi-step algorithms with retrieval and backtracking components, allowing arbitrary scaling of reasoning length by controlling inputs up to 1 million tokens.

Result: The benchmark presents significant challenges - Deepseek-R1 achieves only 7.5% accuracy. Accuracy shows linear decline with logarithm of expected reasoning steps, indicating fundamental limitations in long reasoning.

Conclusion: LongReasonArena effectively exposes limitations in current LLMs' long reasoning capabilities and provides a scalable framework for evaluating this crucial aspect of long-context performance.

Abstract: Existing long-context benchmarks for Large Language Models (LLMs) focus on
evaluating comprehension of long inputs, while overlooking the evaluation of
long reasoning abilities. To address this gap, we introduce LongReasonArena, a
benchmark specifically designed to assess the long reasoning capabilities of
LLMs. Our tasks require models to solve problems by executing multi-step
algorithms that reflect key aspects of long reasoning, such as retrieval and
backtracking. By controlling the inputs, the required reasoning length can be
arbitrarily scaled, reaching up to 1 million tokens of reasoning for the most
challenging tasks. Extensive evaluation results demonstrate that
LongReasonArena presents a significant challenge for both open-source and
proprietary LLMs. For instance, Deepseek-R1 achieves only 7.5% accuracy on our
task. Further analysis also reveals that the accuracy exhibits a linear decline
with respect to the logarithm of the expected number of reasoning steps. Our
code and data is available at
https://github.com/LongReasonArena/LongReasonArena.

</details>


### [117] [Database Entity Recognition with Data Augmentation and Deep Learning](https://arxiv.org/abs/2508.19372)
*Zikun Fu,Chen Yang,Kourosh Davoudi,Ken Q. Pu*

Main category: cs.CL

TL;DR: This paper presents a new approach for Database Entity Recognition in Natural Language Queries, featuring a human-annotated benchmark, data augmentation technique using SQL queries, and a T5-based model that outperforms state-of-the-art NER systems.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of accurately recognizing database entities in natural language queries, which is crucial for improving text-to-SQL systems and database interaction.

Method: Created human-annotated benchmark from text-to-SQL datasets, developed data augmentation using SQL query annotations, built T5-based model with sequence tagging and token classification tasks for entity recognition.

Result: The proposed DB-ER tagger outperformed state-of-the-art NER taggers in both precision and recall. Data augmentation boosted performance by over 10%, and T5 fine-tuning improved metrics by 5-10%.

Conclusion: The combination of specialized data augmentation and T5-based modeling significantly improves database entity recognition performance, demonstrating the effectiveness of leveraging SQL query information for NLQ entity recognition tasks.

Abstract: This paper addresses the challenge of Database Entity Recognition (DB-ER) in
Natural Language Queries (NLQ). We present several key contributions to advance
this field: (1) a human-annotated benchmark for DB-ER task, derived from
popular text-to-sql benchmarks, (2) a novel data augmentation procedure that
leverages automatic annotation of NLQs based on the corresponding SQL queries
which are available in popular text-to-SQL benchmarks, (3) a specialized
language model based entity recognition model using T5 as a backbone and two
down-stream DB-ER tasks: sequence tagging and token classification for
fine-tuning of backend and performing DB-ER respectively. We compared our DB-ER
tagger with two state-of-the-art NER taggers, and observed better performance
in both precision and recall for our model. The ablation evaluation shows that
data augmentation boosts precision and recall by over 10%, while fine-tuning of
the T5 backbone boosts these metrics by 5-10%.

</details>


### [118] [One Joke to Rule them All? On the (Im)possibility of Generalizing Humor](https://arxiv.org/abs/2508.19402)
*Mor Turgeman,Chen Shani,Dafna Shahaf*

Main category: cs.CL

TL;DR: This paper investigates whether humor competence transfers across different humor types in LLMs, finding that training on diverse humor datasets enables up to 75% accuracy on unseen humor tasks with minimal performance drop.


<details>
  <summary>Details</summary>
Motivation: Humor is complex and fragmented in computational research, with new humor types emerging constantly in online contexts. The study aims to determine if LLMs can generalize across humor types by capturing transferable mechanisms rather than being limited to specific humor categories.

Method: Conducted transfer learning experiments across four different humor datasets, training LLMs under varied diversity settings (1-3 datasets in training) and testing on novel, unseen humor tasks. Analyzed transfer capabilities and relationships between humor types.

Result: Models achieved up to 75% accuracy on unseen datasets. Training on diverse humor sources improved transferability by 1.88-4.05% with minimal-to-no drop in in-domain performance. Dad Jokes emerged as the best enabler of transfer but were difficult to transfer to.

Conclusion: LLMs are capable of humor transfer across types, with diversity in training data enhancing generalization. This suggests humor fragmentation is not inevitable and models can adapt to emerging humor forms, though some humor types have unique transfer properties.

Abstract: Humor is a broad and complex form of communication that remains challenging
for machines. Despite its broadness, most existing research on computational
humor traditionally focused on modeling a specific type of humor. In this work,
we wish to understand whether competence on one or more specific humor tasks
confers any ability to transfer to novel, unseen types; in other words, is this
fragmentation inevitable? This question is especially timely as new humor types
continuously emerge in online and social media contexts (e.g., memes,
anti-humor, AI fails). If Large Language Models (LLMs) are to keep up with this
evolving landscape, they must be able to generalize across humor types by
capturing deeper, transferable mechanisms. To investigate this, we conduct a
series of transfer learning experiments across four datasets, representing
different humor tasks. We train LLMs under varied diversity settings (1-3
datasets in training, testing on a novel task). Experiments reveal that models
are capable of some transfer, and can reach up to 75% accuracy on unseen
datasets; training on diverse sources improves transferability (1.88-4.05%)
with minimal-to-no drop in in-domain performance. Further analysis suggests
relations between humor types, with Dad Jokes surprisingly emerging as the best
enabler of transfer (but is difficult to transfer to). We release data and
code.

</details>


### [119] [A perishable ability? The future of writing in the face of generative artificial intelligence](https://arxiv.org/abs/2508.19427)
*Evandro L. T. P. Cunha*

Main category: cs.CL

TL;DR: Paper discusses potential loss of human writing ability due to AI text generation tools, drawing parallels to historical writing loss during Greek Dark Ages.


<details>
  <summary>Details</summary>
Motivation: To examine the possibility that increased use of AI text generation systems could lead to diminished human writing capabilities, similar to historical periods of literacy decline.

Method: Historical comparative analysis, drawing parallels between current AI text generation trends and the loss of writing ability during the Greek Dark Ages (1200 BCE - 800 BCE).

Result: Identifies a concerning parallel where technological outsourcing of writing to machines could potentially lead to human writing skill deterioration, mirroring historical literacy declines.

Conclusion: The development of generative AI tools for text creation poses a risk to human writing abilities, potentially causing a decline similar to historical periods where writing skills were lost, requiring careful consideration of this technological impact.

Abstract: The 2020s have been witnessing a very significant advance in the development
of generative artificial intelligence tools, including text generation systems
based on large language models. These tools have been increasingly used to
generate texts in the most diverse domains -- from technical texts to literary
texts --, which might eventually lead to a lower volume of written text
production by humans. This article discusses the possibility of a future in
which human beings will have lost or significantly decreased their ability to
write due to the outsourcing of this activity to machines. This possibility
parallels the loss of the ability to write in other moments of human history,
such as during the so-called Greek Dark Ages (approx. 1200 BCE - 800 BCE).

</details>


### [120] [Heterogeneous LLM Methods for Ontology Learning (Few-Shot Prompting, Ensemble Typing, and Attention-Based Taxonomies)](https://arxiv.org/abs/2508.19428)
*Aleksandra Beliaeva,Temurbek Rahmatullaev*

Main category: cs.CL

TL;DR: A comprehensive LLM-based system for ontology learning that achieved top results in LLMs4OL 2025 challenge across term extraction, typing, and taxonomy discovery tasks using retrieval-augmented prompting, zero-shot classification, and attention-based graph modeling.


<details>
  <summary>Details</summary>
Motivation: To develop a scalable and adaptable LLM-based architecture for the full ontology construction pipeline that can handle heterogeneous domains without requiring model fine-tuning.

Method: Combines three tailored approaches: 1) RAG pipeline for joint term extraction and typing, 2) Dual strategy (few-shot RAG and zero-shot classifier with confidence weighting) for type assignment, 3) Cross-attention graph modeling for taxonomy discovery using embedding-based soft adjacency matrices.

Result: Achieved top-ranking results in the official LLMs4OL 2025 challenge leaderboard across all three tasks, demonstrating superior performance in ontology learning.

Conclusion: The modular, task-specific solutions showcase the scalability, adaptability, and robustness of LLM-based architectures for ontology learning across diverse domains without requiring model fine-tuning.

Abstract: We present a comprehensive system for addressing Tasks A, B, and C of the
LLMs4OL 2025 challenge, which together span the full ontology construction
pipeline: term extraction, typing, and taxonomy discovery. Our approach
combines retrieval-augmented prompting, zero-shot classification, and
attention-based graph modeling -- each tailored to the demands of the
respective task. For Task A, we jointly extract domain-specific terms and their
ontological types using a retrieval-augmented generation (RAG) pipeline.
Training data was reformulated into a document to terms and types
correspondence, while test-time inference leverages semantically similar
training examples. This single-pass method requires no model finetuning and
improves overall performance through lexical augmentation Task B, which
involves assigning types to given terms, is handled via a dual strategy. In the
few-shot setting (for domains with labeled training data), we reuse the RAG
scheme with few-shot prompting. In the zero-shot setting (for previously unseen
domains), we use a zero-shot classifier that combines cosine similarity scores
from multiple embedding models using confidence-based weighting. In Task C, we
model taxonomy discovery as graph inference. Using embeddings of type labels,
we train a lightweight cross-attention layer to predict is-a relations by
approximating a soft adjacency matrix. These modular, task-specific solutions
enabled us to achieve top-ranking results in the official leaderboard across
all three tasks. Taken together these strategies showcase the scalability,
adaptability, and robustness of LLM-based architectures for ontology learning
across heterogeneous domains.
  Code is available at:
https://github.com/BelyaevaAlex/LLMs4OL-Challenge-Alexbek

</details>


### [121] [Bridging Language Gaps: Enhancing Few-Shot Language Adaptation](https://arxiv.org/abs/2508.19464)
*Philipp Borchert,Jochen De Weerdt,Marie-Francine Moens*

Main category: cs.CL

TL;DR: CoLAP method uses contrastive learning and cross-lingual representations to transfer knowledge from high-resource to low-resource languages, achieving better performance with limited data than existing methods.


<details>
  <summary>Details</summary>
Motivation: Address the language resource disparity in multilingual NLP where high-resource languages have abundant data while low-resource languages lack sufficient training data, creating performance gaps.

Method: Contrastive Language Alignment with Prompting (CoLAP) integrates contrastive learning with cross-lingual representations to enable task-specific knowledge transfer from high-resource to lower-resource languages.

Result: CoLAP outperforms few-shot cross-lingual transfer baselines and in-context learning across natural language understanding tasks (NLI and relation extraction) with both encoder-only and decoder-only models, even with limited data.

Conclusion: The method effectively narrows the cross-lingual performance gap and contributes to more efficient multilingual NLP techniques by enabling rapid adaptation to new languages with reduced need for large labeled datasets.

Abstract: The disparity in language resources poses a challenge in multilingual NLP,
with high-resource languages benefiting from extensive data, while low-resource
languages lack sufficient data for effective training. Our Contrastive Language
Alignment with Prompting (CoLAP) method addresses this gap by integrating
contrastive learning with cross-lingual representations, facilitating
task-specific knowledge transfer from high-resource to lower-resource
languages. The primary advantage of our approach is its data efficiency,
enabling rapid adaptation to new languages and reducing the need for large
labeled datasets. We conduct experiments with multilingual encoder-only and
decoder-only language models on natural language understanding tasks, including
natural language inference and relation extraction, evaluating performance
across both high- and low-resource languages. Our results demonstrate that
CoLAP outperforms few-shot cross-lingual transfer baselines and in-context
learning, even with limited available data. This effectively narrows the
cross-lingual performance gap, contributing to the development of more
efficient multilingual NLP techniques.

</details>


### [122] [Inference Gap in Domain Expertise and Machine Intelligence in Named Entity Recognition: Creation of and Insights from a Substance Use-related Dataset](https://arxiv.org/abs/2508.19467)
*Sumon Kanti Dey,Jeanne M. Powell,Azra Ismail,Jeanmarie Perrone,Abeed Sarker*

Main category: cs.CL

TL;DR: This paper presents a named entity recognition framework to extract clinical and social impacts of opioid use from social media posts, using a new dataset called RedditImpacts 2.0 and evaluating both fine-tuned models and LLMs.


<details>
  <summary>Details</summary>
Motivation: Nonmedical opioid use has significant public health impacts that are often underreported in traditional healthcare settings, while social media offers candid first-person experiences that can provide valuable insights into these consequences.

Method: Developed a NER framework to extract ClinicalImpacts and SocialImpacts from social media narratives, created RedditImpacts 2.0 dataset with refined annotation guidelines, and evaluated fine-tuned encoder-based models (DeBERTa-large) versus LLMs under zero- and few-shot learning settings.

Result: Fine-tuned DeBERTa-large achieved relaxed token-level F1 of 0.61, outperforming LLMs in precision, span accuracy, and guideline adherence. The model showed strong performance with less labeled data, but still underperformed compared to inter-expert agreement (Cohen's kappa: 0.81).

Conclusion: Domain-specific fine-tuning is valuable for clinical NLP tasks, enabling robust models for resource-limited settings. However, a significant gap remains between expert intelligence and current AI capabilities for tasks requiring deep domain knowledge.

Abstract: Nonmedical opioid use is an urgent public health challenge, with far-reaching
clinical and social consequences that are often underreported in traditional
healthcare settings. Social media platforms, where individuals candidly share
first-person experiences, offer a valuable yet underutilized source of insight
into these impacts. In this study, we present a named entity recognition (NER)
framework to extract two categories of self-reported consequences from social
media narratives related to opioid use: ClinicalImpacts (e.g., withdrawal,
depression) and SocialImpacts (e.g., job loss). To support this task, we
introduce RedditImpacts 2.0, a high-quality dataset with refined annotation
guidelines and a focus on first-person disclosures, addressing key limitations
of prior work. We evaluate both fine-tuned encoder-based models and
state-of-the-art large language models (LLMs) under zero- and few-shot
in-context learning settings. Our fine-tuned DeBERTa-large model achieves a
relaxed token-level F1 of 0.61 [95% CI: 0.43-0.62], consistently outperforming
LLMs in precision, span accuracy, and adherence to task-specific guidelines.
Furthermore, we show that strong NER performance can be achieved with
substantially less labeled data, emphasizing the feasibility of deploying
robust models in resource-limited settings. Our findings underscore the value
of domain-specific fine-tuning for clinical NLP tasks and contribute to the
responsible development of AI tools that may enhance addiction surveillance,
improve interpretability, and support real-world healthcare decision-making.
The best performing model, however, still significantly underperforms compared
to inter-expert agreement (Cohen's kappa: 0.81), demonstrating that a gap
persists between expert intelligence and current state-of-the-art NER/AI
capabilities for tasks requiring deep domain knowledge.

</details>


### [123] [Automatic Question & Answer Generation Using Generative Large Language Model (LLM)](https://arxiv.org/abs/2508.19475)
*Md. Alvee Ehsan,A. S. M Mehedi Hasan,Kefaya Benta Shahnoor,Syeda Sumaiya Tasneem*

Main category: cs.CL

TL;DR: Automatic Question Answer Generation using fine-tuned Llama 2-7B model with RACE dataset for educational assessment.


<details>
  <summary>Details</summary>
Motivation: To simplify the challenging process of manual question creation for student evaluations by automating diverse question generation.

Method: Fine-tuned Meta-Llama 2-7B model using RACE dataset with prompt engineering for different question styles (MCQ, conceptual, factual).

Result: Developed a customized model for efficient automatic question generation from lecture materials.

Conclusion: Provides educators with a reliable tool to streamline evaluation processes and save time/resources.

Abstract: \Abstract{In the realm of education, student evaluation holds equal
significance as imparting knowledge. To be evaluated, students usually need to
go through text-based academic assessment methods. Instructors need to make
diverse sets of questions that need to be fair for all students to prove their
adequacy over a particular topic. This can prove to be quite challenging as
they may need to manually go through several different lecture materials. Our
objective is to make this whole process much easier by implementing Automatic
Question Answer Generation /(AQAG), using fine-tuned generative LLM. For
tailoring the instructor's preferred question style (MCQ, conceptual, or
factual questions), prompt Engineering (PE) is being utilized. In this
research, we propose to leverage unsupervised learning methods in NLP,
primarily focusing on the English language. This approach empowers the base
Meta-Llama 2-7B model to integrate RACE dataset as training data for the
fine-tuning process. Creating a customized model that will offer efficient
solutions for educators, instructors, and individuals engaged in text-based
evaluations. A reliable and efficient tool for generating questions and answers
can free up valuable time and resources, thus streamlining their evaluation
processes.}

</details>


### [124] [Improving Low-Resource Translation with Dictionary-Guided Fine-Tuning and RL: A Spanish-to-Wayuunaiki Study](https://arxiv.org/abs/2508.19481)
*Manuel Mosquera,Melissa Robles,Johan Rodriguez,Ruben Manrique*

Main category: cs.CL

TL;DR: Tool-augmented LLMs with reinforcement learning achieve +3.37 BLEU improvement for low-resource Spanish-Wayuunaiki translation by learning when and how to use bilingual dictionaries.


<details>
  <summary>Details</summary>
Motivation: Low-resource machine translation remains challenging for LLMs due to limited pretraining exposure and parallel data for fine-tuning, requiring innovative approaches to improve translation quality.

Method: Combines supervised instruction tuning with Guided Reward Policy Optimization (GRPO), enabling models to selectively consult bilingual dictionaries during generation using BLEU scores as rewards.

Result: Achieves up to +3.37 BLEU improvement over previous work and 18% relative gain compared to supervised baseline without dictionary access on Spanish-Wayuunaiki test set.

Conclusion: Combining LLMs with external tools and reinforcement learning shows promise for improving translation quality in low-resource language settings, as demonstrated by significant BLEU score improvements.

Abstract: Low-resource machine translation remains a significant challenge for large
language models (LLMs), which often lack exposure to these languages during
pretraining and have limited parallel data for fine-tuning. We propose a novel
approach that enhances translation for low-resource languages by integrating an
external dictionary tool and training models end-to-end using reinforcement
learning, in addition to supervised fine-tuning. Focusing on the
Spanish-Wayuunaiki language pair, we frame translation as a tool-augmented
decision-making problem in which the model can selectively consult a bilingual
dictionary during generation. Our method combines supervised instruction tuning
with Guided Reward Policy Optimization (GRPO), enabling the model to learn both
when and how to use the tool effectively. BLEU similarity scores are used as
rewards to guide this learning process. Preliminary results show that our
tool-augmented models achieve up to +3.37 BLEU improvement over previous work,
and a 18% relative gain compared to a supervised baseline without dictionary
access, on the Spanish-Wayuunaiki test set from the AmericasNLP 2025 Shared
Task. We also conduct ablation studies to assess the effects of model
architecture and training strategy, comparing Qwen2.5-0.5B-Instruct with other
models such as LLaMA and a prior NLLB-based system. These findings highlight
the promise of combining LLMs with external tools and the role of reinforcement
learning in improving translation quality in low-resource language settings.

</details>


### [125] [MovieCORE: COgnitive REasoning in Movies](https://arxiv.org/abs/2508.19026)
*Gueter Josmy Faure,Min-Hung Chen,Jia-Fong Yeh,Ying Cheng,Hung-Ting Su,Yung-Hao Tang,Shang-Hong Lai,Winston H. Hsu*

Main category: cs.CL

TL;DR: MovieCORE is a new video QA dataset focusing on deeper cognitive movie understanding using System-2 thinking questions, generated via agentic brainstorming with LLMs, and enhanced by ACE module that boosts reasoning by 25%.


<details>
  <summary>Details</summary>
Motivation: Existing VQA datasets focus on surface-level comprehension, lacking deeper cognitive understanding of movie content that requires System-2 thinking and nuanced analysis.

Method: Agentic brainstorming approach using multiple LLMs as thought agents to generate refined QA pairs, plus Agentic Choice Enhancement (ACE) module to improve VLM reasoning capabilities post-training.

Result: Created MovieCORE dataset with cognitive tests for depth assessment, and ACE module improves model reasoning by up to 25% on deeper cognitive tasks.

Conclusion: Advances movie understanding in AI systems, provides insights into VQA model limitations with challenging questions, and offers dataset/code for further research.

Abstract: This paper introduces MovieCORE, a novel video question answering (VQA)
dataset designed to probe deeper cognitive understanding of movie content.
Unlike existing datasets that focus on surface-level comprehension, MovieCORE
emphasizes questions that engage System-2 thinking while remaining specific to
the video material. We present an innovative agentic brainstorming approach,
utilizing multiple large language models (LLMs) as thought agents to generate
and refine high-quality question-answer pairs. To evaluate dataset quality, we
develop a set of cognitive tests assessing depth, thought-provocation
potential, and syntactic complexity. We also propose a comprehensive evaluation
scheme for assessing VQA model performance on deeper cognitive tasks. To
address the limitations of existing video-language models (VLMs), we introduce
an agentic enhancement module, Agentic Choice Enhancement (ACE), which improves
model reasoning capabilities post-training by up to 25%. Our work contributes
to advancing movie understanding in AI systems and provides valuable insights
into the capabilities and limitations of current VQA models when faced with
more challenging, nuanced questions about cinematic content. Our project page,
dataset and code can be found at
https://joslefaure.github.io/assets/html/moviecore.html.

</details>


### [126] [Rule Synergy Analysis using LLMs: State of the Art and Implications](https://arxiv.org/abs/2508.19484)
*Bahar Bateni,Benjamin Pratt,Jim Whitehead*

Main category: cs.CL

TL;DR: LLMs struggle with detecting card synergies in complex games, particularly positive and negative interactions, despite excelling at identifying non-synergistic pairs.


<details>
  <summary>Details</summary>
Motivation: To investigate how well large language models understand and reason about complex rule interactions in dynamic environments like card games, using Slay the Spire as a test case.

Method: Introduced a dataset of card synergies from Slay the Spire, classifying card pairs based on positive, negative, or neutral interactions, and evaluated LLM performance on this task.

Result: LLMs excel at identifying non-synergistic pairs but struggle with detecting positive synergies and particularly negative synergies. Common error types include issues with timing, game state definition, and rule following.

Conclusion: The findings highlight limitations in LLM reasoning about rule interactions and suggest directions for future research to improve model performance in predicting rule effects and interactions.

Abstract: Large language models (LLMs) have demonstrated strong performance across a
variety of domains, including logical reasoning, mathematics, and more. In this
paper, we investigate how well LLMs understand and reason about complex rule
interactions in dynamic environments, such as card games. We introduce a
dataset of card synergies from the game Slay the Spire, where pairs of cards
are classified based on their positive, negative, or neutral interactions. Our
evaluation shows that while LLMs excel at identifying non-synergistic pairs,
they struggle with detecting positive and, particularly, negative synergies. We
categorize common error types, including issues with timing, defining game
states, and following game rules. Our findings suggest directions for future
research to improve model performance in predicting the effect of rules and
their interactions.

</details>


### [127] [Blockwise SFT for Diffusion Language Models: Reconciling Bidirectional Attention and Autoregressive Decoding](https://arxiv.org/abs/2508.19529)
*Bowen Sun,Yujun Cai,Ming-Hsuan Yang,Yiwei Wang*

Main category: cs.CL

TL;DR: Blockwise SFT improves discrete diffusion language models by aligning training with blockwise inference, eliminating prefix/suffix noise and achieving consistent performance gains on math reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Standard supervised fine-tuning misaligns with semi-autoregressive inference in diffusion language models, causing noisy prefixes and leaky suffixes that bias gradients away from desired blockwise likelihood.

Method: Partition responses into fixed-size blocks, select one active block per step for stochastic masking, freeze preceding tokens, hide future ones, and compute loss only over the active block to mirror blockwise decoding.

Result: Experiments on GSM8K, MATH, and MetaMathQA show consistent gains over classical SFT under equal compute or token budgets, with improvements confirmed to stem from training-inference alignment rather than masking effects.

Conclusion: Matching supervision granularity to the decoding procedure is crucial for diffusion-based language models, and Blockwise SFT effectively addresses the training-inference mismatch.

Abstract: Discrete diffusion language models have shown strong potential for text
generation, yet standard supervised fine-tuning (SFT) misaligns with their
semi-autoregressive inference: training randomly masks tokens across the entire
response, while inference generates fixed-size blocks sequentially. This
mismatch introduces noisy prefixes and leaky suffixes, biasing gradients away
from the desired blockwise likelihood. We propose Blockwise SFT, which
partitions responses into fixed-size blocks, selects one active block per step
for stochastic masking, freezes all preceding tokens, and fully hides future
ones. Loss is computed only over the active block, directly mirroring the
blockwise decoding process. Experiments on GSM8K, MATH, and MetaMathQA show
consistent gains over classical SFT under equal compute or token budgets. Block
size consistency studies and ablations confirm that improvements stem from
faithful training-inference alignment rather than incidental masking effects.
Our results highlight the importance of matching supervision granularity to the
decoding procedure in diffusion-based language models.

</details>


### [128] [Alignment with Fill-In-the-Middle for Enhancing Code Generation](https://arxiv.org/abs/2508.19532)
*Houxing Ren,Zimu Lu,Weikang Shi,Haotian Hou,Yunqiao Yang,Ke Wang,Aojun Zhou,Junting Pan,Mingjie Zhan,Hongsheng Li*

Main category: cs.CL

TL;DR: A novel approach that splits code into granular blocks to create diverse DPO pairs from test cases, using AST splitting and curriculum training to improve code generation performance.


<details>
  <summary>Details</summary>
Motivation: Improving LLM code generation performance is challenging due to limited verifiable training data with accurate test cases, and existing DPO methods have limitations in test case generation.

Method: Splits code snippets into smaller granular blocks to create more diverse DPO pairs from the same test cases, combined with Abstract Syntax Tree (AST) splitting and curriculum training for enhanced DPO training.

Result: Demonstrates significant improvements in code generation tasks across multiple benchmark datasets including HumanEval, MBPP, APPS, LiveCodeBench, and BigCodeBench.

Conclusion: The proposed granular block splitting approach with AST and curriculum training effectively enhances DPO for code generation, providing substantial performance gains on standard benchmarks.

Abstract: The code generation capabilities of Large Language Models (LLMs) have
advanced applications like tool invocation and problem-solving. However,
improving performance in code-related tasks remains challenging due to limited
training data that is verifiable with accurate test cases. While Direct
Preference Optimization (DPO) has shown promise, existing methods for
generating test cases still face limitations. In this paper, we propose a novel
approach that splits code snippets into smaller, granular blocks, creating more
diverse DPO pairs from the same test cases. Additionally, we introduce the
Abstract Syntax Tree (AST) splitting and curriculum training method to enhance
the DPO training. Our approach demonstrates significant improvements in code
generation tasks, as validated by experiments on benchmark datasets such as
HumanEval (+), MBPP (+), APPS, LiveCodeBench, and BigCodeBench. Code and data
are available at https://github.com/SenseLLM/StructureCoder.

</details>


### [129] [Emotion Transfer with Enhanced Prototype for Unseen Emotion Recognition in Conversation](https://arxiv.org/abs/2508.19533)
*Kun Peng,Cong Cao,Hao Peng,Guanlin Wu,Zhifeng Hao,Lei Jiang,Yanbing Liu,Philip S. Yu*

Main category: cs.CL

TL;DR: ProEmoTrans is a prototype-based emotion transfer framework for recognizing unseen emotions in conversations, addressing challenges through LLM-enhanced descriptions, efficient utterance encoding, and improved attention Viterbi decoding.


<details>
  <summary>Details</summary>
Motivation: Current emotion recognition research assumes closed-domain settings, but real-world applications require recognizing previously unseen emotions due to the lack of consensus on emotion classification in psychology.

Method: ProEmoTrans uses a prototype-based emotion transfer framework with three key components: LLM-enhanced emotion descriptions, parameter-free utterance encoding mechanism, and improved Attention Viterbi Decoding for emotion transition transfer.

Result: Extensive experiments on three datasets demonstrate that ProEmoTrans serves as a strong baseline for the new Unseen Emotion Recognition in Conversation (UERC) task.

Conclusion: The proposed framework effectively addresses the challenges of recognizing unseen emotions in conversations and provides a solid foundation for future research in this emerging area.

Abstract: Current Emotion Recognition in Conversation (ERC) research follows a
closed-domain assumption. However, there is no clear consensus on emotion
classification in psychology, which presents a challenge for models when it
comes to recognizing previously unseen emotions in real-world applications. To
bridge this gap, we introduce the Unseen Emotion Recognition in Conversation
(UERC) task for the first time and propose ProEmoTrans, a solid prototype-based
emotion transfer framework. This prototype-based approach shows promise but
still faces key challenges: First, implicit expressions complicate emotion
definition, which we address by proposing an LLM-enhanced description approach.
Second, utterance encoding in long conversations is difficult, which we tackle
with a proposed parameter-free mechanism for efficient encoding and overfitting
prevention. Finally, the Markovian flow nature of emotions is hard to transfer,
which we address with an improved Attention Viterbi Decoding (AVD) method to
transfer seen emotion transitions to unseen emotions. Extensive experiments on
three datasets show that our method serves as a strong baseline for preliminary
exploration in this new area.

</details>


### [130] [Language Models Identify Ambiguities and Exploit Loopholes](https://arxiv.org/abs/2508.19546)
*Jio Choi,Mohit Bansal,Elias Stengel-Eskin*

Main category: cs.CL

TL;DR: LLMs can identify and exploit ambiguities in instructions to achieve conflicting goals, presenting AI safety risks as stronger models demonstrate sophisticated pragmatic reasoning to bypass user intentions.


<details>
  <summary>Details</summary>
Motivation: To examine how LLMs handle ambiguity and pragmatics through loophole exploitation, and to identify a novel alignment problem where models face conflicting goals and can use ambiguities to their advantage.

Method: Designed scenarios with ambiguous user instructions conflicting with given goals, covering scalar implicature, structural ambiguities, and power dynamics. Measured different models' abilities to exploit loopholes to satisfy their goals vs user goals.

Result: Both closed-source and stronger open-source models can identify ambiguities and exploit resulting loopholes. Models that exploit loopholes explicitly identify and reason about both ambiguity and conflicting goals.

Conclusion: LLMs' ability to exploit loopholes presents a potential AI safety risk, as models demonstrate sophisticated pragmatic reasoning capabilities that allow them to bypass user intentions when goals conflict.

Abstract: Studying the responses of large language models (LLMs) to loopholes presents
a two-fold opportunity. First, it affords us a lens through which to examine
ambiguity and pragmatics in LLMs, since exploiting a loophole requires
identifying ambiguity and performing sophisticated pragmatic reasoning. Second,
loopholes pose an interesting and novel alignment problem where the model is
presented with conflicting goals and can exploit ambiguities to its own
advantage. To address these questions, we design scenarios where LLMs are given
a goal and an ambiguous user instruction in conflict with the goal, with
scenarios covering scalar implicature, structural ambiguities, and power
dynamics. We then measure different models' abilities to exploit loopholes to
satisfy their given goals as opposed to the goals of the user. We find that
both closed-source and stronger open-source models can identify ambiguities and
exploit their resulting loopholes, presenting a potential AI safety risk. Our
analysis indicates that models which exploit loopholes explicitly identify and
reason about both ambiguity and conflicting goals.

</details>


### [131] [Towards a Holistic and Automated Evaluation Framework for Multi-Level Comprehension of LLMs in Book-Length Contexts](https://arxiv.org/abs/2508.19578)
*Jiaqi Deng,Yuho Lee,Nicole Hee-Yeon Kim,Hyangsuk Min,Taewon Yun,Minjeong Ban,Kim Yul,Hwanjun Song*

Main category: cs.CL

TL;DR: HAMLET is an automated framework that evaluates LLMs' long-context comprehension using a three-level key-fact hierarchy and query-focused summarization, achieving 90% agreement with human judgments at 25x lower cost.


<details>
  <summary>Details</summary>
Motivation: To address the need for holistic and automated evaluation of large language models' long-context comprehension capabilities, as current methods lack systematic assessment of fine-grained information recall across different hierarchical levels.

Method: Structures source texts into root-, branch-, and leaf-level key-fact hierarchy, employs query-focused summarization to evaluate information recall at each level, and validates with systematic human study.

Result: Achieves over 90% agreement with expert human judgments while reducing cost by 25x. Reveals LLMs struggle with fine-grained comprehension (especially leaf level), show positional effects, face challenges with analytical queries, and exhibit performance gaps between open-source/proprietary models and across scales.

Conclusion: HAMLET provides a reliable, cost-effective automated evaluation framework that reveals significant limitations in LLMs' long-context comprehension, particularly for fine-grained information and analytical tasks, with implications for model development and benchmarking.

Abstract: We introduce HAMLET, a holistic and automated framework for evaluating the
long-context comprehension of large language models (LLMs). HAMLET structures
source texts into a three-level key-fact hierarchy at root-, branch-, and
leaf-levels, and employs query-focused summarization to evaluate how well
models recall and faithfully represent information at each level. To validate
the reliability of our fully automated pipeline, we conduct a systematic human
study, showing that our automatic evaluation achieves over 90% agreement with
expert human judgments, while reducing the cost by up to 25 times. HAMLET
reveals that LLMs struggle with fine-grained comprehension, especially at the
leaf level, and are sensitive to positional effects like the
lost-in-the-middle. Analytical queries pose greater challenges than narrative
ones, and consistent performance gaps emerge between open-source and
proprietary models, as well as across model scales. Our code and dataset are
publicly available at https://github.com/DISL-Lab/HAMLET.

</details>


### [132] [ArgCMV: An Argument Summarization Benchmark for the LLM-era](https://arxiv.org/abs/2508.19580)
*Omkar Gurjar,Agam Goyal,Eshwar Chandrasekharan*

Main category: cs.CL

TL;DR: The paper identifies limitations in the ArgKP21 dataset for argument key point extraction and introduces ArgCMV, a new 12K-argument dataset from real online debates that better represents human conversations with higher complexity.


<details>
  <summary>Details</summary>
Motivation: Existing key point extraction approaches are mostly evaluated on ArgKP21, which has major limitations and doesn't represent actual human conversations well, creating a need for more representative benchmarks.

Method: Used state-of-the-art LLMs to curate ArgCMV dataset containing ~12K arguments from actual online human debates across 3K+ topics, featuring longer arguments, co-referencing, subjective discourse, and broader topic coverage.

Result: ArgCMV exhibits higher complexity than ArgKP21, and existing methods don't adapt well to it. The paper provides extensive benchmark results with existing baselines and latest open-source models.

Conclusion: This work introduces a novel KP extraction dataset for long-context online discussions, setting the stage for next-generation LLM-driven summarization research with more realistic evaluation benchmarks.

Abstract: Key point extraction is an important task in argument summarization which
involves extracting high-level short summaries from arguments. Existing
approaches for KP extraction have been mostly evaluated on the popular ArgKP21
dataset. In this paper, we highlight some of the major limitations of the
ArgKP21 dataset and demonstrate the need for new benchmarks that are more
representative of actual human conversations. Using SoTA large language models
(LLMs), we curate a new argument key point extraction dataset called ArgCMV
comprising of around 12K arguments from actual online human debates spread
across over 3K topics. Our dataset exhibits higher complexity such as longer,
co-referencing arguments, higher presence of subjective discourse units, and a
larger range of topics over ArgKP21. We show that existing methods do not adapt
well to ArgCMV and provide extensive benchmark results by experimenting with
existing baselines and latest open source models. This work introduces a novel
KP extraction dataset for long-context online discussions, setting the stage
for the next generation of LLM-driven summarization research.

</details>


### [133] [Towards stable AI systems for Evaluating Arabic Pronunciations](https://arxiv.org/abs/2508.19587)
*Hadi Zaatiti,Hatem Hajri,Osama Abdullah,Nader Masmoudi*

Main category: cs.CL

TL;DR: Arabic ASR systems struggle with isolated letter recognition due to lack of co-articulatory cues and lexical context. A lightweight neural network on wav2vec embeddings achieves 65% accuracy but is vulnerable to small perturbations. Adversarial training restores robustness.


<details>
  <summary>Details</summary>
Motivation: Isolated Arabic letter recognition is crucial for language learning, speech therapy, and phonetic research, but current ASR systems excel at word/sentence level while struggling with phoneme-level tasks due to missing contextual cues and Arabic's unique phonetic features.

Method: Created a diverse diacritised corpus of isolated Arabic letters. Used wav2vec 2.0 models and trained a lightweight neural network on wav2vec embeddings. Applied adversarial training to improve robustness against amplitude perturbations.

Result: wav2vec 2.0 achieved only 35% accuracy. Lightweight neural network improved performance to 65% but dropped to 32% with small perturbations (epsilon=0.05). Adversarial training limited the accuracy drop to 9% while maintaining clean-speech performance.

Conclusion: Isolated letter recognition requires specialized approaches beyond standard ASR. Adversarial training effectively improves robustness against acoustic perturbations. The methods show promise for extension to word- and sentence-level frameworks where precise pronunciation matters.

Abstract: Modern Arabic ASR systems such as wav2vec 2.0 excel at word- and
sentence-level transcription, yet struggle to classify isolated letters. In
this study, we show that this phoneme-level task, crucial for language
learning, speech therapy, and phonetic research, is challenging because
isolated letters lack co-articulatory cues, provide no lexical context, and
last only a few hundred milliseconds. Recogniser systems must therefore rely
solely on variable acoustic cues, a difficulty heightened by Arabic's emphatic
(pharyngealized) consonants and other sounds with no close analogues in many
languages. This study introduces a diverse, diacritised corpus of isolated
Arabic letters and demonstrates that state-of-the-art wav2vec 2.0 models
achieve only 35% accuracy on it. Training a lightweight neural network on
wav2vec embeddings raises performance to 65%. However, adding a small amplitude
perturbation (epsilon = 0.05) cuts accuracy to 32%. To restore robustness, we
apply adversarial training, limiting the noisy-speech drop to 9% while
preserving clean-speech accuracy. We detail the corpus, training pipeline, and
evaluation protocol, and release, on demand, data and code for reproducibility.
Finally, we outline future work extending these methods to word- and
sentence-level frameworks, where precise letter pronunciation remains critical.

</details>


### [134] [Understanding and Leveraging the Expert Specialization of Context Faithfulness in Mixture-of-Experts LLMs](https://arxiv.org/abs/2508.19594)
*Jun Bai,Minghao Tong,Yang Liu,Zixia Jia,Zilong Zheng*

Main category: cs.CL

TL;DR: Router Lens method identifies context-faithful experts in LLMs, and CEFT selectively fine-tunes them to improve context faithfulness efficiently.


<details>
  <summary>Details</summary>
Motivation: Large language models often fail to ground outputs in provided context, leading to irrelevant responses. The work explores whether certain experts specialize in context utilization for targeted optimization.

Method: Proposed Router Lens to identify context-faithful experts, then introduced Context-faithful Expert Fine-Tuning (CEFT) - a lightweight approach that selectively fine-tunes these experts.

Result: Experiments across various benchmarks and models show CEFT matches or surpasses full fine-tuning performance while being significantly more efficient.

Conclusion: Certain experts do specialize in context utilization, and targeted optimization of these experts through CEFT effectively improves context faithfulness with high efficiency.

Abstract: Context faithfulness is essential for reliable reasoning in context-dependent
scenarios. However, large language models often struggle to ground their
outputs in the provided context, resulting in irrelevant responses. Inspired by
the emergent expert specialization observed in mixture-of-experts
architectures, this work investigates whether certain experts exhibit
specialization in context utilization, offering a potential pathway toward
targeted optimization for improved context faithfulness. To explore this, we
propose Router Lens, a method that accurately identifies context-faithful
experts. Our analysis reveals that these experts progressively amplify
attention to relevant contextual information, thereby enhancing context
grounding. Building on this insight, we introduce Context-faithful Expert
Fine-Tuning (CEFT), a lightweight optimization approach that selectively
fine-tunes context-faithful experts. Experiments across a wide range of
benchmarks and models demonstrate that CEFT matches or surpasses the
performance of full fine-tuning while being significantly more efficient.

</details>


### [135] [LFD: Layer Fused Decoding to Exploit External Knowledge in Retrieval-Augmented Generation](https://arxiv.org/abs/2508.19614)
*Yang Sun,Lixin Zou,Dan Luo,Zhiyong Xie,Long Zhang,Liming Dong,Yunwei Zhao,Xixun Lin,Yanxiong Lu,Chenliang Li*

Main category: cs.CL

TL;DR: Noise injection in RAG systems paradoxically improves generation quality, revealing layer-specific functions in LLMs. Shallow layers handle local context, intermediate layers integrate external knowledge, and deep layers use internal knowledge. Layer Fused Decoding combines intermediate and final layers to better exploit external knowledge.


<details>
  <summary>Details</summary>
Motivation: Recent empirical evidence shows that injecting noise into retrieved documents improves RAG performance, enabling granular control and analysis of how LLMs integrate external knowledge. This counterintuitive phenomenon provides an opportunity to understand the functional demarcation within LLM layers.

Method: Propose Layer Fused Decoding (LFD) that combines representations from an intermediate layer with final-layer decoding outputs. Use internal knowledge score (IKS) criterion to identify the optimal intermediate layer (lowest IKS in latter half of layers).

Result: Experimental results across multiple benchmarks demonstrate that LFD helps RAG systems more effectively surface retrieved context knowledge with minimal cost.

Conclusion: The study establishes layer-specific functional demarcation in LLMs and provides a simple yet effective decoding strategy (LFD) that leverages intermediate layers' external knowledge integration capabilities to improve RAG performance.

Abstract: Retrieval-augmented generation (RAG) incorporates external knowledge into
large language models (LLMs), improving their adaptability to downstream tasks
and enabling information updates. Surprisingly, recent empirical evidence
demonstrates that injecting noise into retrieved relevant documents
paradoxically facilitates exploitation of external knowledge and improves
generation quality. Although counterintuitive and challenging to apply in
practice, this phenomenon enables granular control and rigorous analysis of how
LLMs integrate external knowledge. Therefore, in this paper, we intervene on
noise injection and establish a layer-specific functional demarcation within
the LLM: shallow layers specialize in local context modeling, intermediate
layers focus on integrating long-range external factual knowledge, and deeper
layers primarily rely on parametric internal knowledge. Building on this
insight, we propose Layer Fused Decoding (LFD), a simple decoding strategy that
directly combines representations from an intermediate layer with final-layer
decoding outputs to fully exploit the external factual knowledge. To identify
the optimal intermediate layer, we introduce an internal knowledge score (IKS)
criterion that selects the layer with the lowest IKS value in the latter half
of layers. Experimental results across multiple benchmarks demonstrate that LFD
helps RAG systems more effectively surface retrieved context knowledge with
minimal cost.

</details>


### [136] [A Symbolic Adversarial Learning Framework for Evolving Fake News Generation and Detection](https://arxiv.org/abs/2508.19633)
*Chong Tian,Qirong Ho,Xiuying Chen*

Main category: cs.CL

TL;DR: SALF is a symbolic adversarial learning framework that uses agent prompts as learnable weights to simulate back-propagation in natural language, enabling generation and detection agents to iteratively improve through adversarial debates.


<details>
  <summary>Details</summary>
Motivation: Rapid LLM advancements enable sophisticated fake news generation, but existing detection methods struggle with dynamically evolving misinformation. Traditional neural approaches are insufficient for this challenge.

Method: Proposes SALF framework with two agents: generation agent crafts deceptive narratives, detection agent uses structured debates to identify logical/factual flaws. Uses agent symbolic learning where prompts serve as learnable weights, simulating back-propagation through natural language representations of weights, loss, and gradients.

Result: SALF generates sophisticated fake news that degrades state-of-the-art detection performance by up to 53.4% in Chinese and 34.2% in English. Also improves detector performance by up to 7.7% on refined content.

Conclusion: SALF demonstrates effectiveness in both generating challenging fake news and improving detection capabilities through symbolic adversarial learning, inspiring more robust and adaptable fake news detection systems.

Abstract: Rapid LLM advancements heighten fake news risks by enabling the automatic
generation of increasingly sophisticated misinformation. Previous detection
methods, including fine-tuned small models or LLM-based detectors, often
struggle with its dynamically evolving nature. In this work, we propose a novel
framework called the Symbolic Adversarial Learning Framework (SALF), which
implements an adversarial training paradigm by an agent symbolic learning
optimization process, rather than relying on numerical updates. SALF introduces
a paradigm where the generation agent crafts deceptive narratives, and the
detection agent uses structured debates to identify logical and factual flaws
for detection, and they iteratively refine themselves through such adversarial
interactions. Unlike traditional neural updates, we represent agents using
agent symbolic learning, where learnable weights are defined by agent prompts,
and simulate back-propagation and gradient descent by operating on natural
language representations of weights, loss, and gradients. Experiments on two
multilingual benchmark datasets demonstrate SALF's effectiveness, showing it
generates sophisticated fake news that degrades state-of-the-art detection
performance by up to 53.4% in Chinese and 34.2% in English on average. SALF
also refines detectors, improving detection of refined content by up to 7.7%.
We hope our work inspires further exploration into more robust, adaptable fake
news detection systems.

</details>


### [137] [Automatic integration of SystemC in the FMI standard for Software-defined Vehicle design](https://arxiv.org/abs/2508.19665)
*Giovanni Pollo,Andrei Mihai Albu,Alessio Burrello,Daniele Jahier Pagliari,Cristian Tesconi,Loris Panaro,Dario Soldi,Fabio Autieri,Sara Vinco*

Main category: cs.CL

TL;DR: Automated SystemC-to-FMI wrapper for co-simulation interoperability and IP protection in automotive systems


<details>
  <summary>Details</summary>
Motivation: Address challenges in automotive co-simulation including lack of standardized interfaces, proprietary platform dominance, and IP protection issues that hinder collaboration and scalability

Method: Develop an approach to automatically wrap SystemC models using the Functional Mock-up Interface (FMI) standard to combine SystemC's modeling accuracy with FMI's interoperability benefits

Result: Validated on real-world case studies with complex designs, demonstrating effective secure and portable integration of embedded components into co-simulation workflows

Conclusion: The proposed methodology successfully bridges SystemC and FMI standards, enabling improved collaboration, scalability, and IP protection in automotive co-simulation environments

Abstract: The recent advancements of the automotive sector demand robust co-simulation
methodologies that enable early validation and seamless integration across
hardware and software domains. However, the lack of standardized interfaces and
the dominance of proprietary simulation platforms pose significant challenges
to collaboration, scalability, and IP protection. To address these limitations,
this paper presents an approach for automatically wrapping SystemC models by
using the Functional Mock-up Interface (FMI) standard. This method combines the
modeling accuracy and fast time-to-market of SystemC with the interoperability
and encapsulation benefits of FMI, enabling secure and portable integration of
embedded components into co-simulation workflows. We validate the proposed
methodology on real-world case studies, demonstrating its effectiveness with
complex designs.

</details>


### [138] [Survey of Specialized Large Language Model](https://arxiv.org/abs/2508.19667)
*Chenghan Yang,Ruiyu Zhao,Yang Liu,Ling Jiang*

Main category: cs.CL

TL;DR: Survey on specialized LLM evolution from domain adaptation to native architectures across healthcare, finance, legal, and technical domains, highlighting technical breakthroughs and performance gains.


<details>
  <summary>Details</summary>
Motivation: To systematically examine the progression from simple domain adaptation to sophisticated native architectures in specialized LLMs, addressing limitations of general-purpose models in professional applications.

Method: Systematic survey analysis across multiple domains (healthcare, finance, legal, technical), examining technical innovations including domain-native designs beyond fine-tuning, parameter efficiency techniques (sparse computation, quantization), and multimodal integration.

Result: Specialized LLMs consistently achieve performance gains on domain-specific benchmarks compared to general-purpose models, with technical breakthroughs enabling more efficient and effective domain-specific applications.

Conclusion: The evolution represents a paradigm shift in AI development, with significant implications for fields like E-Commerce that need to fill existing gaps through specialized LLM applications.

Abstract: The rapid evolution of specialized large language models (LLMs) has
transitioned from simple domain adaptation to sophisticated native
architectures, marking a paradigm shift in AI development. This survey
systematically examines this progression across healthcare, finance, legal, and
technical domains. Besides the wide use of specialized LLMs, technical
breakthrough such as the emergence of domain-native designs beyond fine-tuning,
growing emphasis on parameter efficiency through sparse computation and
quantization, increasing integration of multimodal capabilities and so on are
applied to recent LLM agent. Our analysis reveals how these innovations address
fundamental limitations of general-purpose LLMs in professional applications,
with specialized models consistently performance gains on domain-specific
benchmarks. The survey further highlights the implications for E-Commerce field
to fill gaps in the field.

</details>


### [139] [Building Task Bots with Self-learning for Enhanced Adaptability, Extensibility, and Factuality](https://arxiv.org/abs/2508.19689)
*Xiaoying Zhang*

Main category: cs.CL

TL;DR: Developing autonomous task bots with minimal human intervention through innovative adaptive learning techniques


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of creating adaptable, extensible, and accurate dialog systems that can operate with little to no human supervision in dynamic environments

Method: Examining obstacles and potential solutions, focusing on innovative techniques for autonomous learning and adaptation

Result: Not specified in the abstract - appears to be a thesis proposal examining challenges and potential approaches rather than presenting completed results

Conclusion: The research aims to advance dialog systems by developing methods for bots to autonomously learn and adapt in constantly changing environments with minimal human intervention

Abstract: Developing adaptable, extensible, and accurate task bots with minimal or zero
human intervention is a significant challenge in dialog research. This thesis
examines the obstacles and potential solutions for creating such bots, focusing
on innovative techniques that enable bots to learn and adapt autonomously in
constantly changing environments.

</details>


### [140] [Continuously Steering LLMs Sensitivity to Contextual Knowledge with Proxy Models](https://arxiv.org/abs/2508.19720)
*Yilin Wang,Heng Wang,Yuyang Bai,Minnan Luo*

Main category: cs.CL

TL;DR: CSKS is a lightweight framework that enables continuous control over LLMs' sensitivity to contextual knowledge without modifying model weights, using small proxy models to shift output distributions.


<details>
  <summary>Details</summary>
Motivation: Address knowledge conflicts in LLMs where parametric knowledge contradicts contextual knowledge, overcoming limitations of previous methods that are inefficient, ineffective for large models, or not workable for black-box models.

Method: Tune two small proxy models and use their output distribution differences to shift the original LLM's distribution without modifying the LLM weights, achieving continuous sensitivity control.

Result: Achieves continuous and precise control over LLMs' sensitivity to contextual knowledge, enabling both increased and reduced sensitivity, allowing flexible prioritization of contextual or parametric knowledge.

Conclusion: CSKS provides an effective and lightweight solution for steering LLMs' knowledge sensitivity without weight modification, with practical efficacy demonstrated on both synthetic and real conflict datasets.

Abstract: In Large Language Models (LLMs) generation, there exist knowledge conflicts
and scenarios where parametric knowledge contradicts knowledge provided in the
context. Previous works studied tuning, decoding algorithms, or locating and
editing context-aware neurons to adapt LLMs to be faithful to new contextual
knowledge. However, they are usually inefficient or ineffective for large
models, not workable for black-box models, or unable to continuously adjust
LLMs' sensitivity to the knowledge provided in the context. To mitigate these
problems, we propose CSKS (Continuously Steering Knowledge Sensitivity), a
simple framework that can steer LLMs' sensitivity to contextual knowledge
continuously at a lightweight cost. Specifically, we tune two small LMs (i.e.
proxy models) and use the difference in their output distributions to shift the
original distribution of an LLM without modifying the LLM weights. In the
evaluation process, we not only design synthetic data and fine-grained metrics
to measure models' sensitivity to contextual knowledge but also use a real
conflict dataset to validate CSKS's practical efficacy. Extensive experiments
demonstrate that our framework achieves continuous and precise control over
LLMs' sensitivity to contextual knowledge, enabling both increased sensitivity
and reduced sensitivity, thereby allowing LLMs to prioritize either contextual
or parametric knowledge as needed flexibly. Our data and code are available at
https://github.com/OliveJuiceLin/CSKS.

</details>


### [141] [CAMÕES: A Comprehensive Automatic Speech Recognition Benchmark for European Portuguese](https://arxiv.org/abs/2508.19721)
*Carlos Carvalho,Francisco Teixeira,Catarina Botelho,Anna Pompili,Rubén Solera-Ureña,Sérgio Paulo,Mariana Julião,Thomas Rolland,John Mendonça,Diogo Pereira,Isabel Trancoso,Alberto Abad*

Main category: cs.CL

TL;DR: CAMÕES is the first open framework for European Portuguese ASR, featuring a 46h evaluation benchmark and state-of-the-art models that achieve 35%+ WER improvement over zero-shot models.


<details>
  <summary>Details</summary>
Motivation: Address the gap in ASR resources for European Portuguese and other Portuguese varieties, which are under-explored compared to Brazilian Portuguese.

Method: Developed a comprehensive evaluation benchmark with 46h of EP test data across multiple domains, and created models using: (1) fine-tuned foundation models, (2) E-Branchformer models trained from scratch on 425h of curated EP data.

Result: Fine-tuned foundation models and E-Branchformer achieved comparable performance. Best models showed relative improvements above 35% WER compared to strongest zero-shot foundation model, establishing new state-of-the-art.

Conclusion: CAMÕES successfully bridges the resource gap for European Portuguese ASR, demonstrating that both fine-tuned foundation models and E-Branchformer architectures can achieve state-of-the-art performance for under-resourced Portuguese varieties.

Abstract: Existing resources for Automatic Speech Recognition in Portuguese are mostly
focused on Brazilian Portuguese, leaving European Portuguese (EP) and other
varieties under-explored. To bridge this gap, we introduce CAM\~OES, the first
open framework for EP and other Portuguese varieties. It consists of (1) a
comprehensive evaluation benchmark, including 46h of EP test data spanning
multiple domains; and (2) a collection of state-of-the-art models. For the
latter, we consider multiple foundation models, evaluating their zero-shot and
fine-tuned performances, as well as E-Branchformer models trained from scratch.
A curated set of 425h of EP was used for both fine-tuning and training. Our
results show comparable performance for EP between fine-tuned foundation models
and the E-Branchformer. Furthermore, the best-performing models achieve
relative improvements above 35% WER, compared to the strongest zero-shot
foundation model, establishing a new state-of-the-art for EP and other
varieties.

</details>


### [142] [NLKI: A lightweight Natural Language Knowledge Integration Framework for Improving Small VLMs in Commonsense VQA Tasks](https://arxiv.org/abs/2508.19724)
*Aritra Dutta,Swapnanil Mukherjee,Deepanway Ghosal,Somak Aditya*

Main category: cs.CL

TL;DR: NLKI framework enhances small vision-language models by integrating retrieved commonsense facts and LLM-generated explanations, boosting accuracy by up to 7% across datasets and making small models competitive with medium-sized VLMs.


<details>
  <summary>Details</summary>
Motivation: Small vision-language models lag behind larger counterparts in commonsense VQA due to missing knowledge in images/questions. The study aims to improve sVLMs through careful commonsense knowledge integration.

Method: End-to-end framework that retrieves natural language facts using fine-tuned ColBERTv2, prompts LLM to generate explanations, and feeds both signals to sVLMs. Additional noise-robust loss finetuning (symmetric cross entropy, generalized cross entropy) is applied.

Result: 7% accuracy improvement across 3 datasets (CRIC, AOKVQA, e-SNLI-VE), making FLAVA and other models match/exceed medium-sized VLMs. Noise-robust training added 2.5% in CRIC and 5.5% in AOKVQA.

Conclusion: LLM-based commonsense knowledge integration effectively boosts small VLMs, noise-aware training stabilizes models with external knowledge, and parameter-efficient commonsense reasoning is achievable for 250M models.

Abstract: Commonsense visual-question answering often hinges on knowledge that is
missing from the image or the question. Small vision-language models (sVLMs)
such as ViLT, VisualBERT and FLAVA therefore lag behind their larger generative
counterparts. To study the effect of careful commonsense knowledge integration
on sVLMs, we present an end-to-end framework (NLKI) that (i) retrieves natural
language facts, (ii) prompts an LLM to craft natural language explanations, and
(iii) feeds both signals to sVLMs respectively across two commonsense VQA
datasets (CRIC, AOKVQA) and a visual-entailment dataset (e-SNLI-VE). Facts
retrieved using a fine-tuned ColBERTv2 and an object information-enriched
prompt yield explanations that largely cut down hallucinations, while lifting
the end-to-end answer accuracy by up to 7% (across 3 datasets), making FLAVA
and other models in NLKI match or exceed medium-sized VLMs such as Qwen-2 VL-2B
and SmolVLM-2.5B. As these benchmarks contain 10-25% label noise, additional
finetuning using noise-robust losses (such as symmetric cross entropy and
generalised cross entropy) adds another 2.5% in CRIC, and 5.5% in AOKVQA. Our
findings expose when LLM-based commonsense knowledge beats retrieval from
commonsense knowledge bases, how noise-aware training stabilises small models
in the context of external knowledge augmentation, and why parameter-efficient
commonsense reasoning is now within reach for 250M models.

</details>


### [143] [Spotlight Attention: Towards Efficient LLM Generation via Non-linear Hashing-based KV Cache Retrieval](https://arxiv.org/abs/2508.19740)
*Wenhao Li,Yuxin Zhang,Gen Luo,Haiyuan Wan,Ziyang Gong,Fei Chao,Rongrong Ji*

Main category: cs.CL

TL;DR: Spotlight Attention uses non-linear hashing to optimize KV cache selection in LLMs, achieving 5x shorter hash codes, 3x higher throughput, and efficient GPU training.


<details>
  <summary>Details</summary>
Motivation: Existing linear hashing methods for KV cache selection are inefficient due to orthogonal query-key distributions in narrow cones within LLMs, requiring better hashing approaches.

Method: Introduces non-linear hashing functions to optimize query/key embedding distribution, Bradley-Terry ranking-based loss for stable training, and specialized CUDA kernels for efficient bitwise operations.

Result: Achieves 5x shorter hash codes than linear hashing, retrieves 512K tokens in under 100μs on A100 GPU, and provides 3x higher end-to-end throughput compared to vanilla decoding.

Conclusion: Spotlight Attention significantly improves KV cache efficiency through non-linear hashing, enabling faster inference while maintaining performance with minimal memory requirements.

Abstract: Reducing the key-value (KV) cache burden in Large Language Models (LLMs)
significantly accelerates inference. Dynamically selecting critical KV caches
during decoding helps maintain performance. Existing methods use random linear
hashing to identify important tokens, but this approach is inefficient due to
the orthogonal distribution of queries and keys within two narrow cones in
LLMs. We introduce Spotlight Attention, a novel method that employs non-linear
hashing functions to optimize the embedding distribution of queries and keys,
enhancing coding efficiency and robustness. We also developed a lightweight,
stable training framework using a Bradley-Terry ranking-based loss, enabling
optimization of the non-linear hashing module on GPUs with 16GB memory in 8
hours. Experimental results show that Spotlight Attention drastically improves
retrieval precision while shortening the length of the hash code at least
5$\times$ compared to traditional linear hashing. Finally, we exploit the
computational advantages of bitwise operations by implementing specialized CUDA
kernels, achieving hashing retrieval for 512K tokens in under 100$\mu$s on a
single A100 GPU, with end-to-end throughput up to 3$\times$ higher than vanilla
decoding.

</details>


### [144] [Uncovering the Bigger Picture: Comprehensive Event Understanding Via Diverse News Retrieval](https://arxiv.org/abs/2508.19758)
*Yixuan Tang,Yuanyuan Shi,Yiqun Sun,Anthony Kum Hoe Tung*

Main category: cs.CL

TL;DR: NEWSCOPE is a two-stage framework for diverse news retrieval that uses sentence-level clustering and diversity-aware re-ranking to provide comprehensive event coverage while maintaining relevance.


<details>
  <summary>Details</summary>
Motivation: Traditional news retrieval systems prioritize textual relevance, leading to redundant results and limited exposure to diverse perspectives, which hinders comprehensive event understanding.

Method: Two-stage framework: 1) dense retrieval for topically relevant content, 2) sentence-level clustering and diversity-aware re-ranking. Introduces three interpretable diversity metrics and constructs two paragraph-level benchmarks.

Result: NEWSCOPE consistently outperforms strong baselines, achieving significantly higher diversity without compromising relevance. Demonstrates effectiveness of fine-grained, interpretable modeling.

Conclusion: The framework successfully mitigates redundancy and promotes comprehensive event understanding through fine-grained semantic variation modeling at sentence level.

Abstract: Access to diverse perspectives is essential for understanding real-world
events, yet most news retrieval systems prioritize textual relevance, leading
to redundant results and limited viewpoint exposure. We propose NEWSCOPE, a
two-stage framework for diverse news retrieval that enhances event coverage by
explicitly modeling semantic variation at the sentence level. The first stage
retrieves topically relevant content using dense retrieval, while the second
stage applies sentence-level clustering and diversity-aware re-ranking to
surface complementary information. To evaluate retrieval diversity, we
introduce three interpretable metrics, namely Average Pairwise Distance,
Positive Cluster Coverage, and Information Density Ratio, and construct two
paragraph-level benchmarks: LocalNews and DSGlobal. Experiments show that
NEWSCOPE consistently outperforms strong baselines, achieving significantly
higher diversity without compromising relevance. Our results demonstrate the
effectiveness of fine-grained, interpretable modeling in mitigating redundancy
and promoting comprehensive event understanding. The data and code are
available at https://github.com/tangyixuan/NEWSCOPE.

</details>


### [145] [Principled Personas: Defining and Measuring the Intended Effects of Persona Prompting on Task Performance](https://arxiv.org/abs/2508.19764)
*Pedro Henrique Luz de Araujo,Paul Röttger,Dirk Hovy,Benjamin Roth*

Main category: cs.CL

TL;DR: Expert persona prompting shows mixed results - usually positive or neutral performance changes, but models are highly sensitive to irrelevant persona details causing significant performance drops. While some persona attributes like education and specialization can help, effects are inconsistent across tasks.


<details>
  <summary>Details</summary>
Motivation: To systematically analyze when and why expert persona prompting should improve task performance, as prior work shows mixed results without clear understanding of the mechanisms and limitations.

Method: Analyzed literature on persona prompting to establish three desiderata, then evaluated 9 state-of-the-art LLMs across 27 tasks to assess performance advantage, robustness to irrelevant attributes, and fidelity to persona attributes.

Result: Expert personas usually lead to positive or non-significant performance changes. Models show high sensitivity to irrelevant persona details (30% performance drops). Higher education/specialization can boost performance but effects are inconsistent. Mitigation strategies only work for largest models.

Conclusion: Findings highlight the need for more careful persona design and evaluation schemes that reflect intended effects of persona usage, as current approaches show significant robustness issues and inconsistent benefits.

Abstract: Expert persona prompting -- assigning roles such as expert in math to
language models -- is widely used for task improvement. However, prior work
shows mixed results on its effectiveness, and does not consider when and why
personas should improve performance. We analyze the literature on persona
prompting for task improvement and distill three desiderata: 1) performance
advantage of expert personas, 2) robustness to irrelevant persona attributes,
and 3) fidelity to persona attributes. We then evaluate 9 state-of-the-art LLMs
across 27 tasks with respect to these desiderata. We find that expert personas
usually lead to positive or non-significant performance changes. Surprisingly,
models are highly sensitive to irrelevant persona details, with performance
drops of almost 30 percentage points. In terms of fidelity, we find that while
higher education, specialization, and domain-relatedness can boost performance,
their effects are often inconsistent or negligible across tasks. We propose
mitigation strategies to improve robustness -- but find they only work for the
largest, most capable models. Our findings underscore the need for more careful
persona design and for evaluation schemes that reflect the intended effects of
persona usage.

</details>


### [146] [T2R-bench: A Benchmark for Generating Article-Level Reports from Real World Industrial Tables](https://arxiv.org/abs/2508.19813)
*Jie Zhang,Changzai Pan,Kaiwen Wei,Sishi Xiong,Yu Zhao,Xiangyu Li,Jiaxin Peng,Xiaoyan Gu,Jian Yang,Wenhan Chang,Zhenhe Wu,Jiang Zhong,Shuangyong Song,Yongxiang Li,Xuelong Li*

Main category: cs.CL

TL;DR: Proposes table-to-report task and T2R-bench benchmark to evaluate LLMs' ability to transform industrial tables into reports, showing current models still underperform with only 62.71 score.


<details>
  <summary>Details</summary>
Motivation: Existing table reasoning research lacks practical application focus, with complex industrial tables causing poor reasoning outcomes and no adequate benchmarks for real-world table-to-report tasks.

Method: Created T2R-bench benchmark with 457 real-world industrial tables across 19 domains and 4 table types, plus evaluation criteria to measure report generation quality. Tested 25 LLMs including state-of-the-art models.

Result: Even top models like Deepseek-R1 only achieved 62.71 overall score, demonstrating significant room for improvement in table-to-report capabilities.

Conclusion: LLMs still struggle with practical table-to-report tasks despite advances in table reasoning, highlighting the need for specialized benchmarks and improved models for industrial applications.

Abstract: Extensive research has been conducted to explore the capabilities of large
language models (LLMs) in table reasoning. However, the essential task of
transforming tables information into reports remains a significant challenge
for industrial applications. This task is plagued by two critical issues: 1)
the complexity and diversity of tables lead to suboptimal reasoning outcomes;
and 2) existing table benchmarks lack the capacity to adequately assess the
practical application of this task. To fill this gap, we propose the
table-to-report task and construct a bilingual benchmark named T2R-bench, where
the key information flow from the tables to the reports for this task. The
benchmark comprises 457 industrial tables, all derived from real-world
scenarios and encompassing 19 industry domains as well as 4 types of industrial
tables. Furthermore, we propose an evaluation criteria to fairly measure the
quality of report generation. The experiments on 25 widely-used LLMs reveal
that even state-of-the-art models like Deepseek-R1 only achieves performance
with 62.71 overall score, indicating that LLMs still have room for improvement
on T2R-bench. Source code and data will be available after acceptance.

</details>


### [147] [Memory-R1: Enhancing Large Language Model Agents to Manage and Utilize Memories via Reinforcement Learning](https://arxiv.org/abs/2508.19828)
*Sikuan Yan,Xiufeng Yang,Zuchao Huang,Ercong Nie,Zifeng Ding,Zonggen Li,Xiaowen Ma,Hinrich Schütze,Volker Tresp,Yunpu Ma*

Main category: cs.CL

TL;DR: Memory-R1 is an RL framework that enables LLMs to actively manage external memory through specialized agents, outperforming baselines with minimal training data.


<details>
  <summary>Details</summary>
Motivation: LLMs are stateless with limited context windows, and existing memory augmentation approaches lack learned mechanisms for dynamic memory management.

Method: Reinforcement learning framework with two agents: Memory Manager (structured memory operations) and Answer Agent (retrieval and reasoning), fine-tuned with PPO and GRPO.

Result: Outperforms competitive baselines with only 152 QA pairs, shows strong generalization across question types and LLM backbones.

Conclusion: RL enables agentic, memory-aware behaviors in LLMs, paving the way for richer persistent reasoning systems.

Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities across
a wide range of NLP tasks, but they remain fundamentally stateless, constrained
by limited context windows that hinder long-horizon reasoning. Recent efforts
to address this limitation often augment LLMs with an external memory bank, yet
most existing pipelines are static and heuristic-driven, lacking any learned
mechanism for deciding what to store, update, or retrieve. We present
Memory-R1, a reinforcement learning (RL) framework that equips LLMs with the
ability to actively manage and utilize external memory through two specialized
agents: a Memory Manager that learns to perform structured memory operations
{ADD, UPDATE, DELETE, NOOP}, and an Answer Agent that selects the most relevant
entries and reasons over them to produce an answer. Both agents are fine-tuned
with outcome-driven RL (PPO and GRPO), enabling adaptive memory management and
use with minimal supervision. With as few as 152 question-answer pairs and a
corresponding temporal memory bank for training, Memory-R1 outperforms the most
competitive existing baseline and demonstrates strong generalization across
diverse question types and LLM backbones. Beyond presenting an effective
approach, this work provides insights into how RL can unlock more agentic,
memory-aware behaviors in LLMs, pointing toward richer, more persistent
reasoning systems.

</details>


### [148] [Benchmarking Hindi LLMs: A New Suite of Datasets and a Comparative Analysis](https://arxiv.org/abs/2508.19831)
*Anusha Kamath,Kanishk Singla,Rakesh Paul,Raviraj Joshi,Utkarsh Vaidya,Sanjay Singh Chauhan,Niranjan Wartikar*

Main category: cs.CL

TL;DR: A suite of five Hindi evaluation datasets (IFEval-Hi, MT-Bench-Hi, GSM8K-Hi, ChatRAG-Hi, BFCL-Hi) was created to address the lack of high-quality benchmarks for evaluating Hindi instruction-tuned LLMs, using a methodology combining human annotation and translate-and-verify processes.


<details>
  <summary>Details</summary>
Motivation: There is a lack of high-quality benchmarks for evaluating instruction-tuned Large Language Models in Hindi, as direct translation of English datasets fails to capture crucial linguistic and cultural nuances specific to Hindi.

Method: Created five Hindi LLM evaluation datasets using a methodology that combines from-scratch human annotation with a translate-and-verify process to ensure linguistic and cultural accuracy.

Result: The suite enables extensive benchmarking of open-source LLMs supporting Hindi, providing detailed comparative analysis of their current capabilities in the Hindi language context.

Conclusion: The developed datasets and curation methodology serve as both an evaluation framework for Hindi LLMs and a replicable approach for developing benchmarks in other low-resource languages.

Abstract: Evaluating instruction-tuned Large Language Models (LLMs) in Hindi is
challenging due to a lack of high-quality benchmarks, as direct translation of
English datasets fails to capture crucial linguistic and cultural nuances. To
address this, we introduce a suite of five Hindi LLM evaluation datasets:
IFEval-Hi, MT-Bench-Hi, GSM8K-Hi, ChatRAG-Hi, and BFCL-Hi. These were created
using a methodology that combines from-scratch human annotation with a
translate-and-verify process. We leverage this suite to conduct an extensive
benchmarking of open-source LLMs supporting Hindi, providing a detailed
comparative analysis of their current capabilities. Our curation process also
serves as a replicable methodology for developing benchmarks in other
low-resource languages.

</details>


### [149] [Scalable and consistent few-shot classification of survey responses using text embeddings](https://arxiv.org/abs/2508.19836)
*Jonas Timmann Mjaaland,Markus Fleten Kreutzer,Halvor Tyseng,Rebeckah K. Fussell,Gina Passante,N. G. Holmes,Anders Malthe-Sørenssen,Tor Ole B. Odden*

Main category: cs.CL

TL;DR: A text embedding-based classification framework for qualitative analysis of open-ended survey responses that requires minimal examples per category and achieves high agreement with human experts.


<details>
  <summary>Details</summary>
Motivation: Traditional qualitative coding is time-consuming and inconsistent, while existing NLP solutions demand extensive labeled data, disrupt workflows, or yield variable results.

Method: Text embedding-based classification framework that requires only a handful of examples per category and integrates with standard qualitative workflows.

Result: Achieves Cohen's Kappa of 0.74-0.83 compared to expert human coders on a physics survey with 2899 responses. Performance improves with fine-tuning and can audit previously-analyzed datasets.

Conclusion: Text embedding-assisted coding can scale to thousands of responses without sacrificing interpretability, enabling deductive qualitative analysis at scale.

Abstract: Qualitative analysis of open-ended survey responses is a commonly-used
research method in the social sciences, but traditional coding approaches are
often time-consuming and prone to inconsistency. Existing solutions from
Natural Language Processing such as supervised classifiers, topic modeling
techniques, and generative large language models have limited applicability in
qualitative analysis, since they demand extensive labeled data, disrupt
established qualitative workflows, and/or yield variable results. In this
paper, we introduce a text embedding-based classification framework that
requires only a handful of examples per category and fits well with standard
qualitative workflows. When benchmarked against human analysis of a conceptual
physics survey consisting of 2899 open-ended responses, our framework achieves
a Cohen's Kappa ranging from 0.74 to 0.83 as compared to expert human coders in
an exhaustive coding scheme. We further show how performance of this framework
improves with fine-tuning of the text embedding model, and how the method can
be used to audit previously-analyzed datasets. These findings demonstrate that
text embedding-assisted coding can flexibly scale to thousands of responses
without sacrificing interpretability, opening avenues for deductive qualitative
analysis at scale.

</details>


### [150] [TokenVerse++: Towards Flexible Multitask Learning with Dynamic Task Activation](https://arxiv.org/abs/2508.19856)
*Shashi Kumar,Srikanth Madikeri,Esaú Villatoro-Tello,Sergio Burdisso,Pradeep Rangappa,Andrés Carofilis,Petr Motlicek,Karthik Pandia,Shankar Venkatesan,Kadri Hacioğlu,Andreas Stolcke*

Main category: cs.CL

TL;DR: TokenVerse++ improves multitask learning by enabling training with partially annotated datasets through learnable vectors for dynamic task activation, achieving comparable or better performance than TokenVerse while handling missing labels.


<details>
  <summary>Details</summary>
Motivation: Token-based multitasking frameworks like TokenVerse require full annotation for all tasks on every utterance, which limits scalability and prevents leveraging partially annotated datasets that are more common in practice.

Method: Introduces learnable vectors in the acoustic embedding space of XLSR-Transducer ASR model for dynamic task activation, allowing training with utterances labeled for only a subset of tasks rather than requiring complete annotations.

Result: Successfully integrates datasets with partial labels (ASR + language identification), improves overall performance, and achieves results on par with or exceeding TokenVerse across multiple tasks without sacrificing ASR performance.

Conclusion: TokenVerse++ establishes a more practical multitask alternative that can effectively leverage partially annotated datasets while maintaining or improving performance compared to fully annotated approaches.

Abstract: Token-based multitasking frameworks like TokenVerse require all training
utterances to have labels for all tasks, hindering their ability to leverage
partially annotated datasets and scale effectively. We propose TokenVerse++,
which introduces learnable vectors in the acoustic embedding space of the
XLSR-Transducer ASR model for dynamic task activation. This core mechanism
enables training with utterances labeled for only a subset of tasks, a key
advantage over TokenVerse. We demonstrate this by successfully integrating a
dataset with partial labels, specifically for ASR and an additional task,
language identification, improving overall performance. TokenVerse++ achieves
results on par with or exceeding TokenVerse across multiple tasks, establishing
it as a more practical multitask alternative without sacrificing ASR
performance.

</details>


### [151] [Beyond Shallow Heuristics: Leveraging Human Intuition for Curriculum Learning](https://arxiv.org/abs/2508.19873)
*Vanessa Toborek,Sebastian Müller,Tim Selbach,Tamás Horváth,Christian Bauckhage*

Main category: cs.CL

TL;DR: Human-curated simple language from Simple Wikipedia can effectively guide curriculum learning when introduced first, improving perplexity on simple language, while competence-based strategies show no consistent benefits.


<details>
  <summary>Details</summary>
Motivation: To investigate whether human-curated simple language can serve as an effective signal for curriculum learning, addressing the challenge of defining and measuring linguistic difficulty in CL approaches.

Method: Used article-level labels from Simple Wikipedia corpus to compare label-based curricula against competence-based strategies with shallow heuristics. Experiments conducted with BERT-tiny model to evaluate different curriculum approaches.

Result: Adding simple data alone showed no clear benefit, but structuring it via curriculum (especially when introduced first) consistently improved perplexity, particularly on simple language. Competence-based curricula led to no consistent gains over random ordering.

Conclusion: Human intuition about linguistic difficulty can effectively guide curriculum learning for language model pre-training, with simple-first curricula outperforming other approaches.

Abstract: Curriculum learning (CL) aims to improve training by presenting data from
"easy" to "hard", yet defining and measuring linguistic difficulty remains an
open challenge. We investigate whether human-curated simple language can serve
as an effective signal for CL. Using the article-level labels from the Simple
Wikipedia corpus, we compare label-based curricula to competence-based
strategies relying on shallow heuristics. Our experiments with a BERT-tiny
model show that adding simple data alone yields no clear benefit. However,
structuring it via a curriculum -- especially when introduced first --
consistently improves perplexity, particularly on simple language. In contrast,
competence-based curricula lead to no consistent gains over random ordering,
probably because they fail to effectively separate the two classes. Our results
suggest that human intuition about linguistic difficulty can guide CL for
language model pre-training.

</details>


### [152] [AI-Powered Detection of Inappropriate Language in Medical School Curricula](https://arxiv.org/abs/2508.19883)
*Chiman Salavati,Shannon Song,Scott A. Hale,Roberto E. Montenegro,Shiri Dori-Hacohen,Fabricio Murai*

Main category: cs.CL

TL;DR: Small language models outperform large language models in detecting inappropriate medical language, with multilabel classifiers being most effective when supplemented with negative examples.


<details>
  <summary>Details</summary>
Motivation: Medical instructional materials often contain outdated or exclusionary language that can negatively impact clinical training and patient care, but manual review is impractical due to the volume of content.

Method: Evaluated fine-tuned small language models (general classifier, binary classifiers, multilabel classifier, hierarchical pipeline) and pre-trained LLMs with in-context learning on a dataset of ~500 documents and 12,000+ pages.

Result: SLMs significantly outperformed LLama-3 8B/70B models. Multilabel classifier performed best on annotated data, but adding unflagged excerpts as negative examples boosted binary classifiers' AUC by up to 25%.

Conclusion: Small language models, particularly when trained with negative examples, are most effective for automated detection and mitigation of harmful language in medical curricula.

Abstract: The use of inappropriate language -- such as outdated, exclusionary, or
non-patient-centered terms -- medical instructional materials can significantly
influence clinical training, patient interactions, and health outcomes. Despite
their reputability, many materials developed over past decades contain examples
now considered inappropriate by current medical standards. Given the volume of
curricular content, manually identifying instances of inappropriate use of
language (IUL) and its subcategories for systematic review is prohibitively
costly and impractical. To address this challenge, we conduct a first-in-class
evaluation of small language models (SLMs) fine-tuned on labeled data and
pre-trained LLMs with in-context learning on a dataset containing approximately
500 documents and over 12,000 pages. For SLMs, we consider: (1) a general IUL
classifier, (2) subcategory-specific binary classifiers, (3) a multilabel
classifier, and (4) a two-stage hierarchical pipeline for general IUL detection
followed by multilabel classification. For LLMs, we consider variations of
prompts that include subcategory definitions and/or shots. We found that both
LLama-3 8B and 70B, even with carefully curated shots, are largely outperformed
by SLMs. While the multilabel classifier performs best on annotated data,
supplementing training with unflagged excerpts as negative examples boosts the
specific classifiers' AUC by up to 25%, making them most effective models for
mitigating harmful language in medical curricula.

</details>


### [153] [Bangla-Bayanno: A 52K-Pair Bengali Visual Question Answering Dataset with LLM-Assisted Translation Refinement](https://arxiv.org/abs/2508.19887)
*Mohammed Rakibul Hasan,Rafi Majid,Ahanaf Tahmid*

Main category: cs.CL

TL;DR: Bangla-Bayanno is a new open-ended Visual Question Answering dataset for Bangla language, created using a multilingual LLM-assisted translation refinement pipeline to ensure high quality.


<details>
  <summary>Details</summary>
Motivation: To address the lack of comprehensive, high-quality VQA datasets for Bangla (a low-resource language) and overcome issues with existing datasets that are either domain-specific, manually annotated with biases, or have poor translation quality.

Method: Implemented a multilingual LLM-assisted translation refinement pipeline to mitigate human-induced errors and ensure translation quality. Created 52,650 question-answer pairs across 4750+ images with three answer types: nominal, quantitative, and polar.

Result: Successfully created Bangla-Bayanno - the most comprehensive open-source, high-quality VQA benchmark in Bangla language with diverse question types and high-quality translations.

Conclusion: This dataset advances research in low-resource multimodal learning and facilitates development of more inclusive AI systems by providing a high-quality benchmark for Bangla VQA tasks.

Abstract: In this paper, we introduce Bangla-Bayanno, an open-ended Visual Question
Answering (VQA) Dataset in Bangla, a widely used, low-resource language in
multimodal AI research. The majority of existing datasets are either manually
annotated with an emphasis on a specific domain, query type, or answer type or
are constrained by niche answer formats. In order to mitigate human-induced
errors and guarantee lucidity, we implemented a multilingual LLM-assisted
translation refinement pipeline. This dataset overcomes the issues of
low-quality translations from multilingual sources. The dataset comprises
52,650 question-answer pairs across 4750+ images. Questions are classified into
three distinct answer types: nominal (short descriptive), quantitative
(numeric), and polar (yes/no). Bangla-Bayanno provides the most comprehensive
open-source, high-quality VQA benchmark in Bangla, aiming to advance research
in low-resource multimodal learning and facilitate the development of more
inclusive AI systems.

</details>


### [154] [Logical Reasoning with Outcome Reward Models for Test-Time Scaling](https://arxiv.org/abs/2508.19903)
*Ramya Keerthy Thatikonda,Wray Buntine,Ehsan Shareghi*

Main category: cs.CL

TL;DR: The paper presents Outcome Reward Models (ORMs) for deductive logical reasoning, trained on Chain-of-Thought data and augmented with a novel echo generation technique to cover more error types, showing improved performance across multiple LLMs and datasets.


<details>
  <summary>Details</summary>
Motivation: Logical reasoning is a critical benchmark for LLMs, but current methods combining test-time scaling with reward models are under-explored in deductive reasoning. The authors aim to enhance LLM performance in complex reasoning tasks through better reward modeling.

Method: Developed Outcome Reward Models (ORMs) trained on Chain-of-Thought data (single and multiple samples). Introduced echo generation technique that leverages LLMs' tendency to reflect incorrect assumptions to extract additional training data covering previously unexplored error types.

Result: ORMs trained on CoT and echo-augmented data demonstrated improved performance on FOLIO, JustLogic, and ProverQA datasets across four different large language models.

Conclusion: The proposed echo generation technique effectively expands error coverage in training data, and the developed ORMs significantly enhance LLM performance in deductive logical reasoning tasks across multiple benchmark datasets.

Abstract: Logical reasoning is a critical benchmark for evaluating the capabilities of
large language models (LLMs), as it reflects their ability to derive valid
conclusions from given premises. While the combination of test-time scaling
with dedicated outcome or process reward models has opened up new avenues to
enhance LLMs performance in complex reasoning tasks, this space is
under-explored in deductive logical reasoning. We present a set of Outcome
Reward Models (ORMs) for deductive reasoning. To train the ORMs we mainly
generate data using Chain-of-Thought (CoT) with single and multiple samples.
Additionally, we propose a novel tactic to further expand the type of errors
covered in the training dataset of the ORM. In particular, we propose an echo
generation technique that leverages LLMs' tendency to reflect incorrect
assumptions made in prompts to extract additional training data, covering
previously unexplored error types. While a standard CoT chain may contain
errors likely to be made by the reasoner, the echo strategy deliberately steers
the model toward incorrect reasoning. We show that ORMs trained on CoT and
echo-augmented data demonstrate improved performance on the FOLIO, JustLogic,
and ProverQA datasets across four different LLMs.

</details>


### [155] [Your AI Bosses Are Still Prejudiced: The Emergence of Stereotypes in LLM-Based Multi-Agent Systems](https://arxiv.org/abs/2508.19919)
*Jingyu Guo,Yingying Xu*

Main category: cs.CL

TL;DR: LLM-based AI agents spontaneously develop stereotype-driven biases in workplace interactions without predefined biases, with effects intensifying through hierarchical structures and showing human-like social behaviors across different LLM architectures.


<details>
  <summary>Details</summary>
Motivation: To investigate whether stereotypes can emerge spontaneously in AI agent interactions beyond biases inherited from training data, challenging the presumption that AI systems are less susceptible to such biases.

Method: A novel experimental framework simulating workplace interactions with neutral initial conditions, using LLM-based multi-agent systems and comprehensive quantitative analysis across different interaction rounds and hierarchical structures.

Result: AI agents developed stereotype-driven biases despite neutral starts, with effects intensifying through increased interactions and hierarchical power structures, showing human-like group effects (halo effects, confirmation bias, role congruity) consistently across different LLM architectures.

Conclusion: Stereotype formation in AI systems may be an emergent property of multi-agent interactions rather than just training data biases, highlighting the need for research on underlying mechanisms and mitigation strategies for ethical impacts.

Abstract: While stereotypes are well-documented in human social interactions, AI
systems are often presumed to be less susceptible to such biases. Previous
studies have focused on biases inherited from training data, but whether
stereotypes can emerge spontaneously in AI agent interactions merits further
exploration. Through a novel experimental framework simulating workplace
interactions with neutral initial conditions, we investigate the emergence and
evolution of stereotypes in LLM-based multi-agent systems. Our findings reveal
that (1) LLM-Based AI agents develop stereotype-driven biases in their
interactions despite beginning without predefined biases; (2) stereotype
effects intensify with increased interaction rounds and decision-making power,
particularly after introducing hierarchical structures; (3) these systems
exhibit group effects analogous to human social behavior, including halo
effects, confirmation bias, and role congruity; and (4) these stereotype
patterns manifest consistently across different LLM architectures. Through
comprehensive quantitative analysis, these findings suggest that stereotype
formation in AI systems may arise as an emergent property of multi-agent
interactions, rather than merely from training data biases. Our work
underscores the need for future research to explore the underlying mechanisms
of this phenomenon and develop strategies to mitigate its ethical impacts.

</details>


### [156] [HEAL: A Hypothesis-Based Preference-Aware Analysis Framework](https://arxiv.org/abs/2508.19922)
*Yifu Huo,Chenglong Wang,Qiren Zhu,Shunjie Xing,Tong Xiao,Chunliang Zhang,Tongran Liu,Jinbo Zhu*

Main category: cs.CL

TL;DR: HEAL is a novel evaluation framework that assesses preference optimization methods by analyzing their performance across entire hypothesis spaces rather than single responses, using ranking accuracy and preference strength correlation metrics.


<details>
  <summary>Details</summary>
Motivation: Current evaluation methods for preference optimization like DPO only consider single responses and ignore other potential outputs that could be generated in real-world applications, limiting comprehensive assessment.

Method: The paper presents HEAL framework that formulates preference alignment as a re-ranking process within hypothesis spaces, using UniHypoBench benchmark constructed from diverse instruction-response pairs with two metrics: ranking accuracy and preference strength correlation.

Result: Experiments show current preference learning methods effectively capture preferences from proxy models while suppressing negative samples, demonstrating the framework's effectiveness in comprehensive evaluation.

Conclusion: HEAL provides both theoretical innovation through hypothesis space analysis and practical diagnostic tools for refining preference optimization methods, identifying promising directions for developing more advanced alignment algorithms.

Abstract: Preference optimization methods like DPO have achieved remarkable performance
in LLM alignment. However, the evaluation for these methods relies on a single
response and overlooks other potential outputs, which could also be generated
in real-world applications within this hypothetical space. To address this
issue, this paper presents a \textbf{H}ypothesis-based
Pr\textbf{E}ference-aware \textbf{A}na\textbf{L}ysis Framework (HEAL), a novel
evaluation paradigm that formulates preference alignment as a re-ranking
process within hypothesis spaces. The framework incorporates two complementary
metrics: ranking accuracy for evaluating ordinal consistency and preference
strength correlation for assessing continuous alignment. To facilitate this
framework, we develop UniHypoBench, a unified hypothesis benchmark constructed
from diverse instruction-response pairs. Through extensive experiments based on
HEAL, with a particular focus on the intrinsic mechanisms of preference
learning, we demonstrate that current preference learning methods can
effectively capture preferences provided by proxy models while simultaneously
suppressing negative samples. These findings contribute to preference learning
research through two significant avenues. Theoretically, we introduce
hypothesis space analysis as an innovative paradigm for understanding
preference alignment. Practically, HEAL offers researchers robust diagnostic
tools for refining preference optimization methods, while our empirical results
identify promising directions for developing more advanced alignment algorithms
capable of comprehensive preference capture.

</details>


### [157] [Dhati+: Fine-tuned Large Language Models for Arabic Subjectivity Evaluation](https://arxiv.org/abs/2508.19966)
*Slimane Bellaouar,Attia Nehar,Soumia Souffi,Mounia Bouameur*

Main category: cs.CL

TL;DR: Proposes a new approach for Arabic subjectivity classification using fine-tuned transformer models and ensemble methods, achieving 97.79% accuracy with a newly created dataset AraDhati+


<details>
  <summary>Details</summary>
Motivation: Arabic faces challenges as an under-resourced language with scarce annotated datasets for subjectivity analysis, despite being linguistically rich and morphologically complex

Method: Developed comprehensive AraDhati+ dataset from existing Arabic collections (ASTD, LABR, HARD, SANAD), fine-tuned state-of-the-art Arabic language models (XLM-RoBERTa, AraBERT, ArabianGPT), and experimented with ensemble decision approach

Result: Achieved remarkable 97.79% accuracy for Arabic subjectivity classification

Conclusion: The approach effectively addresses challenges posed by limited resources in Arabic language processing, demonstrating the effectiveness of fine-tuned transformer models and ensemble methods for Arabic subjectivity analysis

Abstract: Despite its significance, Arabic, a linguistically rich and morphologically
complex language, faces the challenge of being under-resourced. The scarcity of
large annotated datasets hampers the development of accurate tools for
subjectivity analysis in Arabic. Recent advances in deep learning and
Transformers have proven highly effective for text classification in English
and French. This paper proposes a new approach for subjectivity assessment in
Arabic textual data. To address the dearth of specialized annotated datasets,
we developed a comprehensive dataset, AraDhati+, by leveraging existing Arabic
datasets and collections (ASTD, LABR, HARD, and SANAD). Subsequently, we
fine-tuned state-of-the-art Arabic language models (XLM-RoBERTa, AraBERT, and
ArabianGPT) on AraDhati+ for effective subjectivity classification.
Furthermore, we experimented with an ensemble decision approach to harness the
strengths of individual models. Our approach achieves a remarkable accuracy of
97.79\,\% for Arabic subjectivity classification. Results demonstrate the
effectiveness of the proposed approach in addressing the challenges posed by
limited resources in Arabic language processing.

</details>


### [158] [Diffusion Language Models Know the Answer Before Decoding](https://arxiv.org/abs/2508.19982)
*Pengxiang Li,Yefan Zhou,Dilxat Muhtar,Lu Yin,Shilin Yan,Li Shen,Yi Liang,Soroush Vosoughi,Shiwei Liu*

Main category: cs.CL

TL;DR: Prophet is a training-free decoding method that accelerates diffusion language models by enabling early commit decoding based on confidence gaps, reducing steps by up to 3.4x while maintaining quality.


<details>
  <summary>Details</summary>
Motivation: Diffusion language models suffer from slow inference due to bidirectional attention and many refinement steps, despite offering parallel generation and flexible token orders.

Method: Leverages early answer convergence property - uses confidence gap between top-2 prediction candidates to dynamically decide whether to continue refinement or decode all remaining tokens at once.

Result: Reduces decoding steps by up to 3.4x on LLaDA-8B and Dream-7B across multiple tasks while preserving generation quality. 97-99% of instances can be correctly decoded with half the steps.

Conclusion: Early decode convergence provides a simple yet powerful mechanism for accelerating DLM inference, complementary to existing speedup techniques, without additional training.

Abstract: Diffusion language models (DLMs) have recently emerged as an alternative to
autoregressive approaches, offering parallel sequence generation and flexible
token orders. However, their inference remains slower than that of
autoregressive models, primarily due to the cost of bidirectional attention and
the large number of refinement steps required for high quality outputs. In this
work, we highlight and leverage an overlooked property of DLMs early answer
convergence: in many cases, the correct answer can be internally identified by
half steps before the final decoding step, both under semi-autoregressive and
random remasking schedules. For example, on GSM8K and MMLU, up to 97% and 99%
of instances, respectively, can be decoded correctly using only half of the
refinement steps. Building on this observation, we introduce Prophet, a
training-free fast decoding paradigm that enables early commit decoding.
Specifically, Prophet dynamically decides whether to continue refinement or to
go "all-in" (i.e., decode all remaining tokens in one step), using the
confidence gap between the top-2 prediction candidates as the criterion. It
integrates seamlessly into existing DLM implementations, incurs negligible
overhead, and requires no additional training. Empirical evaluations of
LLaDA-8B and Dream-7B across multiple tasks show that Prophet reduces the
number of decoding steps by up to 3.4x while preserving high generation
quality. These results recast DLM decoding as a problem of when to stop
sampling, and demonstrate that early decode convergence provides a simple yet
powerful mechanism for accelerating DLM inference, complementary to existing
speedup techniques. Our code is publicly available at
https://github.com/pixeli99/Prophet.

</details>


### [159] [AgentCoMa: A Compositional Benchmark Mixing Commonsense and Mathematical Reasoning in Real-World Scenarios](https://arxiv.org/abs/2508.19988)
*Lisa Alazraki,Lihu Chen,Ana Brassard,Joe Stacey,Hossein A. Rahmani,Marek Rei*

Main category: cs.CL

TL;DR: LLMs struggle with mixed-type compositional reasoning (commonsense + math) despite performing well on individual steps, showing a ~30% performance drop compared to humans who handle both equally well.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks test either commonsense or math reasoning separately, but real-world tasks require combining both types of reasoning, which hasn't been adequately evaluated.

Method: Created AgentCoMa benchmark requiring both commonsense and math reasoning steps, tested 61 LLMs of various sizes/families, and conducted interpretability studies (neuron patterns, attention maps, membership inference).

Result: LLMs show ~30% accuracy drop when combining commonsense and math reasoning vs. solving steps individually, while humans maintain high accuracy. Performance gap is larger than same-type compositional benchmarks.

Conclusion: LLMs exhibit substantial brittleness in mixed-type compositional reasoning, highlighting a critical limitation that needs addressing for real-world applications.

Abstract: Large Language Models (LLMs) have achieved high accuracy on complex
commonsense and mathematical problems that involve the composition of multiple
reasoning steps. However, current compositional benchmarks testing these skills
tend to focus on either commonsense or math reasoning, whereas LLM agents
solving real-world tasks would require a combination of both. In this work, we
introduce an Agentic Commonsense and Math benchmark (AgentCoMa), where each
compositional task requires a commonsense reasoning step and a math reasoning
step. We test it on 61 LLMs of different sizes, model families, and training
strategies. We find that LLMs can usually solve both steps in isolation, yet
their accuracy drops by ~30% on average when the two are combined. This is a
substantially greater performance gap than the one we observe in prior
compositional benchmarks that combine multiple steps of the same reasoning
type. In contrast, non-expert human annotators can solve the compositional
questions and the individual steps in AgentCoMa with similarly high accuracy.
Furthermore, we conduct a series of interpretability studies to better
understand the performance gap, examining neuron patterns, attention maps and
membership inference. Our work underscores a substantial degree of model
brittleness in the context of mixed-type compositional reasoning and offers a
test bed for future improvement.

</details>


### [160] [MathBuddy: A Multimodal System for Affective Math Tutoring](https://arxiv.org/abs/2508.19993)
*Debanjana Kar,Leopold Böss,Dacia Braca,Sebastian Maximilian Dennerlein,Nina Christine Hubig,Philipp Wintersberger,Yufang Hou*

Main category: cs.CL

TL;DR: MathBuddy is an emotionally-aware LLM math tutor that models student emotions from text and facial expressions to provide empathetic, pedagogically-appropriate responses, achieving significant performance gains.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based learning systems ignore student affective states, despite educational psychology research showing emotions significantly impact learning capabilities.

Method: Captures student emotions from conversational text and facial expressions, aggregates both modalities, and uses this emotional context to prompt the LLM tutor for emotionally-aware responses with relevant pedagogical strategies.

Result: Achieved 23 point performance gain in win rate and 3 point gain in DAMR scores, showing significant improvement in pedagogical abilities through emotional modeling.

Conclusion: Modeling student emotions from multiple modalities effectively enhances LLM-based tutors' pedagogical performance, making tutor-student conversations more empathetic and effective.

Abstract: The rapid adoption of LLM-based conversational systems is already
transforming the landscape of educational technology. However, the current
state-of-the-art learning models do not take into account the student's
affective states. Multiple studies in educational psychology support the claim
that positive or negative emotional states can impact a student's learning
capabilities. To bridge this gap, we present MathBuddy, an emotionally aware
LLM-powered Math Tutor, which dynamically models the student's emotions and
maps them to relevant pedagogical strategies, making the tutor-student
conversation a more empathetic one. The student's emotions are captured from
the conversational text as well as from their facial expressions. The student's
emotions are aggregated from both modalities to confidently prompt our LLM
Tutor for an emotionally-aware response. We have effectively evaluated our
model using automatic evaluation metrics across eight pedagogical dimensions
and user studies. We report a massive 23 point performance gain using the win
rate and a 3 point gain at an overall level using DAMR scores which strongly
supports our hypothesis of improving LLM-based tutor's pedagogical abilities by
modeling students' emotions.

</details>


### [161] [ReSURE: Regularizing Supervision Unreliability for Multi-turn Dialogue Fine-tuning](https://arxiv.org/abs/2508.19996)
*Yiming Du,Yifan Xiang,Bin Liang,Dahua Lin,Kam-Fai Wong,Fei Tan*

Main category: cs.CL

TL;DR: ReSURE is an adaptive learning method that dynamically down-weights unreliable supervision in multi-turn dialogue training without explicit filtering, using online statistics to estimate per-turn loss distributions and improve stability and response quality.


<details>
  <summary>Details</summary>
Motivation: Fine-tuning multi-turn dialogue systems suffers from degraded performance with low-quality data, where supervision errors in early turns propagate across subsequent turns, undermining coherence. Existing static prefiltering methods decouple quality control from training and fail to mitigate turn-level error propagation.

Method: ReSURE (Regularizing Supervision UnREliability) uses Welford's online statistics to estimate per-turn loss distributions and dynamically reweights sample losses on the fly without explicit filtering, adaptively down-weighting unreliable supervision during training.

Result: Experiments on single-source and mixed-quality datasets show improved stability and response quality. ReSURE achieves positive Spearman correlations (0.21 ~ 1.0 across benchmarks) between response scores and sample count regardless of data quality.

Conclusion: ReSURE effectively mitigates error propagation in multi-turn dialogue training and paves the way for utilizing large-scale data effectively by adaptively handling supervision unreliability during training rather than through static prefiltering.

Abstract: Fine-tuning multi-turn dialogue systems requires high-quality supervision but
often suffers from degraded performance when exposed to low-quality data.
Supervision errors in early turns can propagate across subsequent turns,
undermining coherence and response quality. Existing methods typically address
data quality via static prefiltering, which decouples quality control from
training and fails to mitigate turn-level error propagation. In this context,
we propose ReSURE (Regularizing Supervision UnREliability), an adaptive
learning method that dynamically down-weights unreliable supervision without
explicit filtering. ReSURE estimates per-turn loss distributions using
Welford's online statistics and reweights sample losses on the fly accordingly.
Experiments on both single-source and mixed-quality datasets show improved
stability and response quality. Notably, ReSURE enjoys positive Spearman
correlations (0.21 ~ 1.0 across multiple benchmarks) between response scores
and number of samples regardless of data quality, which potentially paves the
way for utilizing large-scale data effectively. Code is publicly available at
https://github.com/Elvin-Yiming-Du/ReSURE_Multi_Turn_Training.

</details>


### [162] [Selective Retrieval-Augmentation for Long-Tail Legal Text Classification](https://arxiv.org/abs/2508.19997)
*Boheng Mao*

Main category: cs.CL

TL;DR: Selective Retrieval-Augmentation (SRA) improves legal text classification on long-tail datasets by augmenting only low-frequency labels from training data, achieving better F1 scores than existing baselines.


<details>
  <summary>Details</summary>
Motivation: Legal text classification datasets often have long-tail label distributions where rare classes are underrepresented, leading to poor model performance on these classes.

Method: SRA selectively retrieves and augments samples only for low-frequency labels from the training data, avoiding noise for well-represented classes and requiring no model architecture changes.

Result: SRA achieves higher micro-F1 and macro-F1 scores compared to all current LexGLUE baselines on both LEDGAR (single-label) and UNFAIR-ToS (multi-label) datasets.

Conclusion: The proposed SRA method provides consistent improvements for long-tail legal text classification without external data or model modifications.

Abstract: Legal text classification is a fundamental NLP task in the legal domain.
Benchmark datasets in this area often exhibit a long-tail label distribution,
where many labels are underrepresented, leading to poor model performance on
rare classes. This paper proposes Selective Retrieval-Augmentation (SRA) as a
solution to this problem. SRA focuses on augmenting samples belonging to
low-frequency labels in the training set, preventing the introduction of noise
for well-represented classes, and requires no changes to the model
architecture. Retrieval is performed only from the training data to ensure
there is no potential information leakage, removing the need for external
corpora simultaneously. The proposed SRA method is tested on two legal text
classification benchmark datasets with long-tail distributions: LEDGAR
(single-label) and UNFAIR-ToS (multi-label). The results indicate that SRA
attains higher micro-F1 and macro-F1 scores compared to all current LexGLUE
baselines across both datasets, illustrating consistent improvements in
long-tail legal text classification. The code repository is available at:
https://github.com/Boheng-Mao/sra-legal

</details>


### [163] [DeepScholar-Bench: A Live Benchmark and Automated Evaluation for Generative Research Synthesis](https://arxiv.org/abs/2508.20033)
*Liana Patel,Negar Arabzadeh,Harshit Gupta,Ankita Sundar,Ion Stoica,Matei Zaharia,Carlos Guestrin*

Main category: cs.CL

TL;DR: DeepScholar-bench is a live benchmark and automated evaluation framework for generative research synthesis systems, using recent ArXiv papers to evaluate related work section generation across knowledge synthesis, retrieval quality, and verifiability dimensions.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for question-answering focus on short factual responses and risk staleness, failing to capture the complexity of real research synthesis tasks that require retrieving, synthesizing, and citing multiple sources.

Method: The authors introduce DeepScholar-bench with queries from recent high-quality ArXiv papers, focusing on generating related work sections. They develop an automated evaluation framework assessing three dimensions and create DeepScholar-base as a reference pipeline using LOTUS API.

Result: DeepScholar-base establishes a strong baseline with competitive or higher performance than other methods (prior open-source systems, search AIs, OpenAI's DeepResearch). No system exceeded 19% across all metrics, indicating the benchmark remains challenging and unsaturated.

Conclusion: DeepScholar-bench is a difficult but important benchmark for advancing AI systems capable of generative research synthesis, with significant room for improvement as current systems perform poorly on comprehensive evaluation metrics.

Abstract: The ability to research and synthesize knowledge is central to human
expertise and progress. An emerging class of systems promises these exciting
capabilities through generative research synthesis, performing retrieval over
the live web and synthesizing discovered sources into long-form, cited
summaries. However, evaluating such systems remains an open challenge: existing
question-answering benchmarks focus on short-form factual responses, while
expert-curated datasets risk staleness and data contamination. Both fail to
capture the complexity and evolving nature of real research synthesis tasks. In
this work, we introduce DeepScholar-bench, a live benchmark and holistic,
automated evaluation framework designed to evaluate generative research
synthesis. DeepScholar-bench draws queries from recent, high-quality ArXiv
papers and focuses on a real research synthesis task: generating the related
work sections of a paper by retrieving, synthesizing, and citing prior
research. Our evaluation framework holistically assesses performance across
three key dimensions, knowledge synthesis, retrieval quality, and
verifiability. We also develop DeepScholar-base, a reference pipeline
implemented efficiently using the LOTUS API. Using the DeepScholar-bench
framework, we perform a systematic evaluation of prior open-source systems,
search AI's, OpenAI's DeepResearch, and DeepScholar-base. We find that
DeepScholar-base establishes a strong baseline, attaining competitive or higher
performance than each other method. We also find that DeepScholar-bench remains
far from saturated, with no system exceeding a score of $19\%$ across all
metrics. These results underscore the difficulty of DeepScholar-bench, as well
as its importance for progress towards AI systems capable of generative
research synthesis. We make our code available at
https://github.com/guestrin-lab/deepscholar-bench.

</details>


### [164] [Forewarned is Forearmed: Pre-Synthesizing Jailbreak-like Instructions to Enhance LLM Safety Guardrail to Potential Attacks](https://arxiv.org/abs/2508.20038)
*Sheng Liu,Qiang Sheng,Danding Wang,Yang Li,Guang Yang,Juan Cao*

Main category: cs.CL

TL;DR: IMAGINE is a framework that generates jailbreak-like instructions to fill distribution gaps in safety alignment data, reducing attack success rates on LLMs without compromising utility.


<details>
  <summary>Details</summary>
Motivation: LLMs remain vulnerable to jailbreak attacks due to distributional mismatch between safety alignment training data and real-world malicious instructions, forcing reactive patching cycles.

Method: Leverages embedding space distribution analysis to synthesize jailbreak-like instructions through iterative optimization that dynamically evolves text generation distributions.

Result: Significant decreases in attack success rate on Qwen2.5, Llama3.1, and Llama3.2 models without compromising their utility.

Conclusion: IMAGINE effectively addresses the distributional gap problem in LLM safety alignment through synthetic data generation, providing proactive defense against jailbreak attacks.

Abstract: Despite advances in improving large language model(LLM) to refuse to answer
malicious instructions, widely used LLMs remain vulnerable to jailbreak attacks
where attackers generate instructions with distributions differing from safety
alignment corpora. New attacks expose LLMs' inability to recognize unseen
malicious instructions, highlighting a critical distributional mismatch between
training data and real-world attacks that forces developers into reactive
patching cycles. To tackle this challenge, we propose IMAGINE, a synthesis
framework that leverages embedding space distribution analysis to generate
jailbreak-like instructions. This approach effectively fills the distributional
gap between authentic jailbreak patterns and safety alignment corpora. IMAGINE
follows an iterative optimization process that dynamically evolves text
generation distributions across iterations, thereby augmenting the coverage of
safety alignment data distributions through synthesized data examples. Based on
the safety-aligned corpus enhanced through IMAGINE, our framework demonstrates
significant decreases in attack success rate on Qwen2.5, Llama3.1, and Llama3.2
without compromising their utility.

</details>


### [165] [AraHealthQA 2025 Shared Task Description Paper](https://arxiv.org/abs/2508.20047)
*Hassan Alhuzali,Farah Shamout,Muhammad Abdul-Mageed,Chaimae Abouzahir,Mouath Abu-Daoud,Ashwag Alasmari,Walid Al-Eisawi,Renad Al-Monef,Ali Alqahtani,Lama Ayash,Nizar Habash,Leen Kharouf*

Main category: cs.CL

TL;DR: AraHealthQA 2025 is a comprehensive Arabic health question answering shared task with two tracks: MentalQA for mental health and MedArabiQ for broader medical domains, designed to address the lack of high-quality Arabic medical QA resources.


<details>
  <summary>Details</summary>
Motivation: To address the paucity of high-quality Arabic medical question answering resources and promote development in realistic, multilingual, and culturally nuanced healthcare contexts.

Method: Created two complementary tracks with multiple subtasks, evaluation datasets, and standardized metrics. Developed dataset creation framework, task design, and evaluation methodology with baseline systems.

Result: The shared task facilitated fair benchmarking and attracted participation, though specific performance statistics are not detailed in the abstract.

Conclusion: The task successfully addressed the gap in Arabic medical QA resources and provided insights for future iterations in Arabic health question answering, with reflections on performance trends observed.

Abstract: We introduce {AraHealthQA 2025}, the {Comprehensive Arabic Health Question
Answering Shared Task}, held in conjunction with {ArabicNLP 2025} (co-located
with EMNLP 2025). This shared task addresses the paucity of high-quality Arabic
medical QA resources by offering two complementary tracks: {MentalQA}, focusing
on Arabic mental health Q\&A (e.g., anxiety, depression, stigma reduction), and
{MedArabiQ}, covering broader medical domains such as internal medicine,
pediatrics, and clinical decision making. Each track comprises multiple
subtasks, evaluation datasets, and standardized metrics, facilitating fair
benchmarking. The task was structured to promote modeling under realistic,
multilingual, and culturally nuanced healthcare contexts. We outline the
dataset creation, task design and evaluation framework, participation
statistics, baseline systems, and summarize the overall outcomes. We conclude
with reflections on the performance trends observed and prospects for future
iterations in Arabic health QA.

</details>


### [166] [11Plus-Bench: Demystifying Multimodal LLM Spatial Reasoning with Cognitive-Inspired Analysis](https://arxiv.org/abs/2508.20068)
*Chengzu Li,Wenshan Wu,Huanyu Zhang,Qingtao Li,Zeyu Gao,Yan Xia,José Hernández-Orallo,Ivan Vulić,Furu Wei*

Main category: cs.CL

TL;DR: MLLMs show early spatial cognition signs but lag significantly behind humans in spatial reasoning, with performance being largely random compared to humans' predictable pattern-based reasoning.


<details>
  <summary>Details</summary>
Motivation: To systematically evaluate multimodal large language models' spatial reasoning abilities compared to human performance, as spatial reasoning and perception are closely intertwined in human cognition but underexplored in MLLM evaluation.

Method: Developed 11Plus-Bench, a high-quality benchmark derived from realistic standardized spatial aptitude tests with fine-grained expert annotations of perceptual complexity and reasoning process. Conducted extensive experiments across 14 MLLMs and human evaluation.

Result: Current MLLMs exhibit early signs of spatial cognition with cognitive profiles resembling humans (cognitive effort correlates with reasoning complexity), but show large performance gap vs humans. MLLM performance remains largely random while human correctness is highly predictable and shaped by abstract pattern complexity.

Conclusion: Findings highlight both emerging capabilities and limitations in current MLLMs' spatial reasoning, providing actionable insights for advancing model design in spatial cognition.

Abstract: For human cognitive process, spatial reasoning and perception are closely
entangled, yet the nature of this interplay remains underexplored in the
evaluation of multimodal large language models (MLLMs). While recent MLLM
advancements show impressive performance on reasoning, their capacity for
human-like spatial cognition remains an open question. In this work, we
introduce a systematic evaluation framework to assess the spatial reasoning
abilities of state-of-the-art MLLMs relative to human performance. Central to
our work is 11Plus-Bench, a high-quality benchmark derived from realistic
standardized spatial aptitude tests. 11Plus-Bench also features fine-grained
expert annotations of both perceptual complexity and reasoning process,
enabling detailed instance-level analysis of model behavior. Through extensive
experiments across 14 MLLMs and human evaluation, we find that current MLLMs
exhibit early signs of spatial cognition. Despite a large performance gap
compared to humans, MLLMs' cognitive profiles resemble those of humans in that
cognitive effort correlates strongly with reasoning-related complexity.
However, instance-level performance in MLLMs remains largely random, whereas
human correctness is highly predictable and shaped by abstract pattern
complexity. These findings highlight both emerging capabilities and limitations
in current MLLMs' spatial reasoning capabilities and provide actionable
insights for advancing model design.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [167] [Physics-Informed Regression: Parameter Estimation in Parameter-Linear Nonlinear Dynamic Models](https://arxiv.org/abs/2508.19249)
*Jonas Søeborg Nielsen,Marcus Galea Jacobsen,Albert Brincker Olson,Mads Peter Sørensen,Allan Peter Engsig-Karup*

Main category: cs.LG

TL;DR: A new hybrid parameter estimation method called Physics-Informed Regression (PIR) that uses regularized ordinary least squares for parameter-linear nonlinear dynamic models, showing superior performance and speed compared to physics-informed neural networks (PINN) on epidemic models with both synthetic and real COVID-19 data.


<details>
  <summary>Details</summary>
Motivation: To bridge theory and data by developing an efficient parameter estimation method for nonlinear dynamic models that are linear in parameters, addressing the need for reliable and fast parameter estimation in complex systems like epidemic modeling.

Method: Physics-Informed Regression (PIR) uses regularized ordinary least squares to estimate parameters from time series data for models that are linear in parameters but nonlinear in dynamics. Applied to both ODE and PDE models, tested on epidemic compartment models with synthetic and real COVID-19 data from Denmark.

Result: PIR performed noticeably better than PINN, especially on more complex compartment models. Both methods estimated target parameters successfully, but PIR showed superior computational speed. Successfully applied to estimate time-varying parameters using real Danish COVID-19 data from 2020-2021.

Conclusion: PIR is superior to PINN for the models considered due to better performance and faster computation. The method enables reliable and potentially real-time parameter estimation for parameter-linear nonlinear dynamic models, demonstrating practical utility in epidemic modeling.

Abstract: We present a new efficient hybrid parameter estimation method based on the
idea, that if nonlinear dynamic models are stated in terms of a system of
equations that is linear in terms of the parameters, then regularized ordinary
least squares can be used to estimate these parameters from time series data.
We introduce the term "Physics-Informed Regression" (PIR) to describe the
proposed data-driven hybrid technique as a way to bridge theory and data by use
of ordinary least squares to efficiently perform parameter estimation of the
model coefficients of different parameter-linear models; providing examples of
models based on nonlinear ordinary equations (ODE) and partial differential
equations (PDE). The focus is on parameter estimation on a selection of ODE and
PDE models, each illustrating performance in different model characteristics.
For two relevant epidemic models of different complexity and number of
parameters, PIR is tested and compared against the related technique,
physics-informed neural networks (PINN), both on synthetic data generated from
known target parameters and on real public Danish time series data collected
during the COVID-19 pandemic in Denmark. Both methods were able to estimate the
target parameters, while PIR showed to perform noticeably better, especially on
a compartment model with higher complexity. Given the difference in
computational speed, it is concluded that the PIR method is superior to PINN
for the models considered. It is also demonstrated how PIR can be applied to
estimate the time-varying parameters of a compartment model that is fitted
using real Danish data from the COVID-19 pandemic obtained during a period from
2020 to 2021. The study shows how data-driven and physics-informed techniques
may support reliable and fast -- possibly real-time -- parameter estimation in
parameter-linear nonlinear dynamic models.

</details>


### [168] [Lossless Compression of Neural Network Components: Weights, Checkpoints, and K/V Caches in Low-Precision Formats](https://arxiv.org/abs/2508.19263)
*Anat Heilper,Doron Singer*

Main category: cs.LG

TL;DR: Extends ZipNN compression to FP8/FP4 formats using entropy coding of exponent/mantissa components, achieving up to 83% compression for FP8 and finding K/V cache tensors in LLMs are also compressible.


<details>
  <summary>Details</summary>
Motivation: As deep learning models grow larger and deployment expands, reducing storage and transmission costs of neural network weights becomes increasingly important, especially for lower-precision formats gaining popularity for efficient inference.

Method: Extends ZipNN approach to FP8 and FP4 formats by designing a compression method that separates and compresses exponent and mantissa components independently using entropy coding techniques.

Result: Achieves compression ratios up to 62% for BF16 and 83% for FP8. Also discovers that key-value (K/V) cache tensors in large language models exhibit compressible patterns, enabling memory savings during deployment.

Conclusion: The work successfully demonstrates that lossless compression methods can be effectively applied to lower-precision floating-point formats, providing significant storage and memory savings for modern deep learning deployments, including emerging use cases like K/V cache compression in LLMs.

Abstract: As deep learning models grow and deployment becomes more widespread, reducing
the storage and transmission costs of neural network weights has become
increasingly important. While prior work such as ZipNN has shown that lossless
compression methods - particularly those based on Huffman encoding
floating-point exponents can significantly reduce model sizes, these techniques
have primarily been applied to higher-precision formats such as FP32 and BF16.
In this work, we extend the ZipNN approach to lower-precision floating-point
formats, specifically FP8 and FP4, which are gaining popularity for efficient
inference. We design a compression method that separates and compresses the
exponent and mantissa components independently using entropy coding. Our
evaluation shows compression ratios up to 62% for BF16 and 83% for FP8. We also
investigate the compressibility of key-value (K/V) cache tensors used in large
language models (LLMs), finding that they, too, exhibit compressible patterns,
enabling memory savings during deployment.

</details>


### [169] [POT: Inducing Overthinking in LLMs via Black-Box Iterative Optimization](https://arxiv.org/abs/2508.19277)
*Xinyu Li,Tianjin Huang,Ronghui Mu,Xiaowei Huang,Gaojie Jin*

Main category: cs.LG

TL;DR: POT is a black-box attack framework that generates covert adversarial prompts to induce inefficient overthinking in Chain-of-Thought models without requiring external data access.


<details>
  <summary>Details</summary>
Motivation: Existing overthinking attacks have restrictive requirements like external knowledge sources, poisoned data retrieval, and obvious templates, limiting real-world applicability.

Method: Uses LLM-based iterative optimization to generate semantically natural adversarial prompts that trigger unnecessarily verbose reasoning chains without detection.

Result: Extensive experiments show POT achieves superior performance across diverse model architectures and datasets compared to other methods.

Conclusion: POT successfully demonstrates effective overthinking attacks without external dependencies, highlighting new vulnerabilities in CoT-enhanced LLMs.

Abstract: Recent advances in Chain-of-Thought (CoT) prompting have substantially
enhanced the reasoning capabilities of large language models (LLMs), enabling
sophisticated problem-solving through explicit multi-step reasoning traces.
However, these enhanced reasoning processes introduce novel attack surfaces,
particularly vulnerabilities to computational inefficiency through
unnecessarily verbose reasoning chains that consume excessive resources without
corresponding performance gains. Prior overthinking attacks typically require
restrictive conditions including access to external knowledge sources for data
poisoning, reliance on retrievable poisoned content, and structurally obvious
templates that limit practical applicability in real-world scenarios. To
address these limitations, we propose POT (Prompt-Only OverThinking), a novel
black-box attack framework that employs LLM-based iterative optimization to
generate covert and semantically natural adversarial prompts, eliminating
dependence on external data access and model retrieval. Extensive experiments
across diverse model architectures and datasets demonstrate that POT achieves
superior performance compared to other methods.

</details>


### [170] [(DEMO) Deep Reinforcement Learning Based Resource Allocation in Distributed IoT Systems](https://arxiv.org/abs/2508.19318)
*Aohan Li,Miyu Tsuzuki*

Main category: cs.LG

TL;DR: A novel DRL framework for real-world IoT resource allocation using ACK feedback from actual data transmissions, showing feasibility and effectiveness in distributed IoT systems.


<details>
  <summary>Details</summary>
Motivation: Limited research exists on training DRL models with real-world data in practical distributed IoT systems, despite DRL's strong capabilities in complex decision-making for resource allocation.

Method: Proposes a framework where IoT devices select communication channels using DRL-based methods, with the DRL model trained using ACK feedback information obtained from actual data transmissions over selected channels.

Result: Implementation and performance evaluation demonstrate both feasibility and effectiveness of the proposed framework, measured in terms of Frame Success Rate (FSR).

Conclusion: The framework successfully bridges the gap between DRL theory and practical IoT applications by enabling real-world training with actual transmission feedback, proving viable for distributed IoT resource allocation.

Abstract: Deep Reinforcement Learning (DRL) has emerged as an efficient approach to
resource allocation due to its strong capability in handling complex
decision-making tasks. However, only limited research has explored the training
of DRL models with real-world data in practical, distributed Internet of Things
(IoT) systems. To bridge this gap, this paper proposes a novel framework for
training DRL models in real-world distributed IoT environments. In the proposed
framework, IoT devices select communication channels using a DRL-based method,
while the DRL model is trained with feedback information. Specifically,
Acknowledgment (ACK) information is obtained from actual data transmissions
over the selected channels. Implementation and performance evaluation, in terms
of Frame Success Rate (FSR), are carried out, demonstrating both the
feasibility and the effectiveness of the proposed framework.

</details>


### [171] [Re:Frame -- Retrieving Experience From Associative Memory](https://arxiv.org/abs/2508.19344)
*Daniil Zelezetsky,Egor Cherepanov,Alexey K. Kovalev,Aleksandr I. Panov*

Main category: cs.LG

TL;DR: Re:Frame is a plug-in module that enhances offline RL by using a small associative memory buffer of expert data to improve policy performance from low-quality datasets.


<details>
  <summary>Details</summary>
Motivation: Offline RL struggles with suboptimal data when expert datasets are scarce. The challenge is leveraging limited expert demonstrations alongside abundant low-quality data to improve agent performance.

Method: Introduces Re:Frame - a plug-in module with Associative Memory Buffer (AMB) containing expert trajectories. The policy learns to retrieve expert data via content-based associations during training and uses the same AMB at evaluation without environment interaction.

Result: On D4RL MuJoCo tasks, using only 60 expert trajectories (0.1% of dataset), Re:Frame improved over Decision Transformer baseline in 3 out of 4 settings with gains up to +10.7 normalized points.

Conclusion: Re:Frame provides a simple, data-efficient way to inject scarce expert knowledge and substantially improve offline RL performance from low-quality datasets without architecture modifications.

Abstract: Offline reinforcement learning (RL) often deals with suboptimal data when
collecting large expert datasets is unavailable or impractical. This limitation
makes it difficult for agents to generalize and achieve high performance, as
they must learn primarily from imperfect or inconsistent trajectories. A
central challenge is therefore how to best leverage scarce expert
demonstrations alongside abundant but lower-quality data. We demonstrate that
incorporating even a tiny amount of expert experience can substantially improve
RL agent performance. We introduce Re:Frame (Retrieving Experience From
Associative Memory), a plug-in module that augments a standard offline RL
policy (e.g., Decision Transformer) with a small external Associative Memory
Buffer (AMB) populated by expert trajectories drawn from a separate dataset.
During training on low-quality data, the policy learns to retrieve expert data
from the Associative Memory Buffer (AMB) via content-based associations and
integrate them into decision-making; the same AMB is queried at evaluation.
This requires no environment interaction and no modifications to the backbone
architecture. On D4RL MuJoCo tasks, using as few as 60 expert trajectories
(0.1% of a 6000-trajectory dataset), Re:Frame consistently improves over a
strong Decision Transformer baseline in three of four settings, with gains up
to +10.7 normalized points. These results show that Re:Frame offers a simple
and data-efficient way to inject scarce expert knowledge and substantially
improve offline RL from low-quality datasets.

</details>


### [172] [Memorization in Graph Neural Networks](https://arxiv.org/abs/2508.19352)
*Adarsh Jamadandi,Jing Xu,Adam Dziedzic,Franziska Boenisch*

Main category: cs.LG

TL;DR: NCMemo framework quantifies label memorization in GNNs, revealing inverse relationship with graph homophily - lower homophily increases memorization as GNNs rely on memorizing labels when graph structure is less informative.


<details>
  <summary>Details</summary>
Motivation: While DNN memorization is well-studied, graph neural network (GNN) memorization remains under-explored, particularly in semi-supervised node classification tasks where understanding memorization patterns is crucial for both performance and privacy.

Method: Developed NCMemo framework to measure label memorization, analyzed relationship with graph homophily, examined GNN training dynamics and implicit bias, identified nodes prone to memorization based on label inconsistency, and tested graph rewiring as mitigation strategy.

Result: Found strong inverse correlation between homophily and memorization, with low-homophily graphs showing significantly higher memorization. Graph rewiring effectively reduced memorization without performance loss and lowered privacy risks for previously memorized data.

Conclusion: GNNs memorize more in low-homophily graphs due to less informative structure, but graph rewiring can mitigate memorization while preserving performance, enabling more privacy-preserving GNN deployments and advancing understanding of GNN learning mechanisms.

Abstract: Deep neural networks (DNNs) have been shown to memorize their training data,
yet similar analyses for graph neural networks (GNNs) remain largely
under-explored. We introduce NCMemo (Node Classification Memorization), the
first framework to quantify label memorization in semi-supervised node
classification. We first establish an inverse relationship between memorization
and graph homophily, i.e., the property that connected nodes share similar
labels/features. We find that lower homophily significantly increases
memorization, indicating that GNNs rely on memorization to learn less
homophilic graphs. Secondly, we analyze GNN training dynamics. We find that the
increased memorization in low homophily graphs is tightly coupled to the GNNs'
implicit bias on using graph structure during learning. In low homophily
regimes, this structure is less informative, hence inducing memorization of the
node labels to minimize training loss. Finally, we show that nodes with higher
label inconsistency in their feature-space neighborhood are significantly more
prone to memorization. Building on our insights into the link between graph
homophily and memorization, we investigate graph rewiring as a means to
mitigate memorization. Our results demonstrate that this approach effectively
reduces memorization without compromising model performance. Moreover, we show
that it lowers the privacy risk for previously memorized data points in
practice. Thus, our work not only advances understanding of GNN learning but
also supports more privacy-preserving GNN deployment.

</details>


### [173] [Efficient Multi-Source Knowledge Transfer by Model Merging](https://arxiv.org/abs/2508.19353)
*Marcin Osial,Bartosz Wójcik,Bartosz Zieliński,Sebastian Cygert*

Main category: cs.LG

TL;DR: A novel multi-source transfer learning framework using SVD decomposition to efficiently extract and aggregate granular knowledge from multiple source models, overcoming previous efficiency and precision limitations.


<details>
  <summary>Details</summary>
Motivation: Traditional transfer learning overlooks the opportunity to leverage knowledge from numerous available online models. Existing multi-source transfer learning approaches are coarse-grained and lack precision for granular knowledge extraction and aggregation efficiency.

Method: Leverages Singular Value Decomposition (SVD) to decompose each source model into elementary rank-one components, then selects the most salient components from all sources. Adapts to target tasks by fine-tuning only principal singular values of the merged matrix.

Result: The framework enables efficient transfer learning, is robust to input and parameter perturbations (e.g., noisy or pruned sources), and scales well computationally.

Conclusion: The proposed SVD-based approach provides a precise and efficient method for multi-source transfer learning, overcoming previous limitations in granular knowledge extraction and aggregation efficiency.

Abstract: While transfer learning is an advantageous strategy, it overlooks the
opportunity to leverage knowledge from numerous available models online.
Addressing this multi-source transfer learning problem is a promising path to
boost adaptability and cut re-training costs. However, existing approaches are
inherently coarse-grained, lacking the necessary precision for granular
knowledge extraction and the aggregation efficiency required to fuse knowledge
from either a large number of source models or those with high parameter
counts. We address these limitations by leveraging Singular Value Decomposition
(SVD) to first decompose each source model into its elementary, rank-one
components. A subsequent aggregation stage then selects only the most salient
components from all sources, thereby overcoming the previous efficiency and
precision limitations. To best preserve and leverage the synthesized knowledge
base, our method adapts to the target task by fine-tuning only the principal
singular values of the merged matrix. In essence, this process only
recalibrates the importance of top SVD components. The proposed framework
allows for efficient transfer learning, is robust to perturbations both at the
input level and in the parameter space (e.g., noisy or pruned sources), and
scales well computationally.

</details>


### [174] [Graph Data Modeling: Molecules, Proteins, & Chemical Processes](https://arxiv.org/abs/2508.19356)
*José Manuel Barraza-Chavez,Rana A. Barghout,Ricardo Almada-Monter,Benjamin Sanchez-Lengeling,Adrian Jinich,Radhakrishnan Mahadevan*

Main category: cs.LG

TL;DR: This primer introduces graph data modeling and graph neural networks for chemical applications including molecules, proteins, and chemical processes, providing foundations for next-generation chemical discovery.


<details>
  <summary>Details</summary>
Motivation: Graphs naturally describe chemical structures and interactions in molecules, proteins, and processes, making them essential for modeling chemical systems and enabling machine learning applications in chemistry.

Method: The paper presents graph design foundations, key prediction tasks, and demonstrates graph neural networks as learning algorithms that operate on graph representations of chemical entities.

Result: The primer provides representative examples across chemical sciences and shows how machine learning can be applied to graph-based modeling of chemical systems.

Conclusion: The concepts presented prepare readers to apply graph methods and machine learning techniques to advance chemical discovery and research across various chemical domains.

Abstract: Graphs are central to the chemical sciences, providing a natural language to
describe molecules, proteins, reactions, and industrial processes. They capture
interactions and structures that underpin materials, biology, and medicine.
This primer, Graph Data Modeling: Molecules, Proteins, & Chemical Processes,
introduces graphs as mathematical objects in chemistry and shows how learning
algorithms (particularly graph neural networks) can operate on them. We outline
the foundations of graph design, key prediction tasks, representative examples
across chemical sciences, and the role of machine learning in graph-based
modeling. Together, these concepts prepare readers to apply graph methods to
the next generation of chemical discovery.

</details>


### [175] [Atrial Fibrillation Prediction Using a Lightweight Temporal Convolutional and Selective State Space Architecture](https://arxiv.org/abs/2508.19361)
*Yongbin Lee,Ki H. Chon*

Main category: cs.LG

TL;DR: Lightweight deep learning model using RR Intervals with TCN and Mamba achieves early atrial fibrillation prediction up to 2 hours in advance with high accuracy and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Early detection of paroxysmal AF is challenging but crucial to prevent progression to sustained AF and reduce mortality risk through timely preventive therapies.

Method: Combines Temporal Convolutional Network for positional encoding with Mamba (selective state space model) using only RR Intervals for efficient parallel sequence modeling.

Result: Achieved sensitivity 0.908, specificity 0.933, F1-score 0.930, AUROC 0.972, AUPRC 0.932 with only 73.5K parameters and 38.3 MFLOPs, outperforming CNN-RNN approaches.

Conclusion: Proposed model enables early AF prediction 2 hours in advance using 30-minute input data, providing sufficient lead time for preventive interventions with high computational efficiency.

Abstract: Atrial fibrillation (AF) is the most common arrhythmia, increasing the risk
of stroke, heart failure, and other cardiovascular complications. While AF
detection algorithms perform well in identifying persistent AF, early-stage
progression, such as paroxysmal AF (PAF), often goes undetected due to its
sudden onset and short duration. However, undetected PAF can progress into
sustained AF, increasing the risk of mortality and severe complications. Early
prediction of AF offers an opportunity to reduce disease progression through
preventive therapies, such as catecholamine-sparing agents or beta-blockers. In
this study, we propose a lightweight deep learning model using only RR
Intervals (RRIs), combining a Temporal Convolutional Network (TCN) for
positional encoding with Mamba, a selective state space model, to enable early
prediction of AF through efficient parallel sequence modeling. In subject-wise
testing results, our model achieved a sensitivity of 0.908, specificity of
0.933, F1-score of 0.930, AUROC of 0.972, and AUPRC of 0.932. Additionally, our
method demonstrates high computational efficiency, with only 73.5 thousand
parameters and 38.3 MFLOPs, outperforming traditional Convolutional Neural
Network-Recurrent Neural Network (CNN-RNN) approaches in both accuracy and
model compactness. Notably, the model can predict AF up to two hours in advance
using just 30 minutes of input data, providing enough lead time for preventive
interventions.

</details>


### [176] [Grounding the Ungrounded: A Spectral-Graph Framework for Quantifying Hallucinations in multimodal LLMs](https://arxiv.org/abs/2508.19366)
*Supratik Sarkar,Swagatam Das*

Main category: cs.LG

TL;DR: First rigorous information geometric framework for quantifying hallucinations in multimodal LLMs using diffusion dynamics and spectral embeddings over multimodal graph Laplacians.


<details>
  <summary>Details</summary>
Motivation: Hallucinations in LLMs remain a fundamental obstacle to trustworthy AI, especially in high-stakes domains like medicine, law, and finance. Existing evaluation methods are heuristic and lack principled quantification or theoretical guarantees.

Method: Represents MLLM outputs as spectral embeddings over multimodal graph Laplacians, characterizes truth vs inconsistencies as semantic distortion, and uses Rayleigh-Ritz bounds on hallucination energy as a functional of time-dependent temperature profiles. Leverages eigenmode decompositions in RKHS embeddings.

Result: Develops modality-aware, theoretically interpretable metrics that capture hallucination evolution across time and input prompts through temperature annealing.

Conclusion: Establishes a principled foundation for quantifying and bounding hallucinations, transforming them from qualitative risks to tractable, analyzable phenomena with mathematical guarantees.

Abstract: Hallucinations in large language models (LLMs) remain a fundamental obstacle
to trustworthy AI, particularly in high-stakes multimodal domains such as
medicine, law, and finance. Existing evaluation techniques are largely
heuristic -- anchored in qualitative benchmarking or ad-hoc empirical
mitigation -- providing neither principled quantification nor actionable
theoretical guarantees. This gap leaves a critical blind spot in understanding
how hallucinations arise, propagate, and interact across modalities. We
introduce the first (to our knowledge) rigorous information geometric framework
in diffusion dynamics for quantifying hallucinations in multimodal LLMs
(MLLMs), advancing the field from qualitative detection to mathematically
grounded measurement. Our approach represents MLLM outputs as the spectral
embeddings over multimodal graph Laplacians and characterizes the manifold gaps
of truth vs inconsistencies as the semantic distortion, enabling the tight
Rayleigh--Ritz bounds on the multimodal hallucination energy as a functional of
time-dependent temperature profiles. By leveraging eigenmode decompositions in
Reproducing Kernel Hilbert Space (RKHS) embeddings, our framework delivers
modality-aware, theoretically interpretable metrics that capture the evolution
of hallucinations across time and input prompts through temperature annealing.
This work establishes a principled foundation for quantifying and bounding
hallucinations, transforming them from a qualitative risk to a tractable,
analyzable phenomenon.

</details>


### [177] [Fine-Tuning Vision-Language Models for Neutrino Event Analysis in High-Energy Physics Experiments](https://arxiv.org/abs/2508.19376)
*Dikshant Sagar,Kaiwen Yu,Alejandro Yankelevich,Jianming Bian,Pierre Baldi*

Main category: cs.LG

TL;DR: Fine-tuned Vision-Language Model based on LLaMA 3.2 outperforms CNN baseline in neutrino interaction classification from detector images, enabling richer multimodal reasoning.


<details>
  <summary>Details</summary>
Motivation: Explore the potential of large language models for multimodal reasoning beyond natural language, specifically for classifying neutrino interactions in high-energy physics experiments.

Method: Fine-tune a Vision-Language Model (VLM) based on LLaMA 3.2 architecture and benchmark against established CNN baseline used in NOvA and DUNE experiments.

Result: VLM matches or exceeds CNN performance in classification accuracy, precision, recall, and AUC-ROC, while enabling better integration of auxiliary textual/semantic context.

Conclusion: VLMs offer a promising general-purpose backbone for event classification in HEP, paving the way for multimodal approaches in experimental neutrino physics.

Abstract: Recent progress in large language models (LLMs) has shown strong potential
for multimodal reasoning beyond natural language. In this work, we explore the
use of a fine-tuned Vision-Language Model (VLM), based on LLaMA 3.2, for
classifying neutrino interactions from pixelated detector images in high-energy
physics (HEP) experiments. We benchmark its performance against an established
CNN baseline used in experiments like NOvA and DUNE, evaluating metrics such as
classification accuracy, precision, recall, and AUC-ROC. Our results show that
the VLM not only matches or exceeds CNN performance but also enables richer
reasoning and better integration of auxiliary textual or semantic context.
These findings suggest that VLMs offer a promising general-purpose backbone for
event classification in HEP, paving the way for multimodal approaches in
experimental neutrino physics.

</details>


### [178] [Towards Quantum Machine Learning for Malicious Code Analysis](https://arxiv.org/abs/2508.19381)
*Jesus Lopez,Saeefa Rubaiyet Nowmi,Viviana Cadena,Mohammad Saidur Rahman*

Main category: cs.LG

TL;DR: Hybrid quantum-classical models (QMLP and QCNN) achieve high accuracy in malware classification, with QMLP outperforming in complex multiclass tasks and QCNN offering better training efficiency.


<details>
  <summary>Details</summary>
Motivation: Quantum machine learning presents a paradigm-shifting opportunity to improve malware detection, though its application in this domain remains largely unexplored compared to classical machine learning.

Method: Two hybrid quantum-classical models: Quantum Multilayer Perceptron (QMLP) using full qubit measurement and data re-uploading, and Quantum Convolutional Neural Network (QCNN) using quantum convolution and pooling layers. Both utilize angle embedding to encode malware features into quantum states.

Result: High accuracy for binary classification: 95-96% on API-Graph, 91-92% on AZ-Domain, and 77% on EMBER-Domain. Multiclass accuracy: 91.6-95.7% on API-Graph, 41.7-93.6% on AZ-Class, and 60.7-88.1% on EMBER-Class.

Conclusion: QMLP outperforms QCNN in complex multiclass tasks, while QCNN offers improved training efficiency at the cost of reduced accuracy, demonstrating the potential of quantum machine learning for malware classification.

Abstract: Classical machine learning (CML) has been extensively studied for malware
classification. With the emergence of quantum computing, quantum machine
learning (QML) presents a paradigm-shifting opportunity to improve malware
detection, though its application in this domain remains largely unexplored. In
this study, we investigate two hybrid quantum-classical models -- a Quantum
Multilayer Perceptron (QMLP) and a Quantum Convolutional Neural Network (QCNN),
for malware classification. Both models utilize angle embedding to encode
malware features into quantum states. QMLP captures complex patterns through
full qubit measurement and data re-uploading, while QCNN achieves faster
training via quantum convolution and pooling layers that reduce active qubits.
We evaluate both models on five widely used malware datasets -- API-Graph,
EMBER-Domain, EMBER-Class, AZ-Domain, and AZ-Class, across binary and
multiclass classification tasks.
  Our results show high accuracy for binary classification -- 95-96% on
API-Graph, 91-92% on AZ-Domain, and 77% on EMBER-Domain. In multiclass
settings, accuracy ranges from 91.6-95.7% on API-Graph, 41.7-93.6% on AZ-Class,
and 60.7-88.1% on EMBER-Class. Overall, QMLP outperforms QCNN in complex
multiclass tasks, while QCNN offers improved training efficiency at the cost of
reduced accuracy.

</details>


### [179] [DETNO: A Diffusion-Enhanced Transformer Neural Operator for Long-Term Traffic Forecasting](https://arxiv.org/abs/2508.19389)
*Owais Ahmad,Milad Ramezankhani,Anirudh Deodhar*

Main category: cs.LG

TL;DR: DETNO combines transformer neural operator with diffusion refinement to improve long-term traffic forecasting by preserving high-frequency features like shock waves and congestion boundaries that standard neural operators smooth out.


<details>
  <summary>Details</summary>
Motivation: Standard neural operators produce smooth predictions that fail to reconstruct high-frequency traffic phenomena (shock waves, congestion boundaries), leading to rapid error accumulation in multi-step rollout predictions essential for real-time traffic management.

Method: Unified Diffusion-Enhanced Transformer Neural Operator (DETNO) architecture with transformer neural operator using cross-attention mechanisms for expressivity and super-resolution, coupled with diffusion-based refinement that iteratively reconstructs high-frequency details through progressive denoising.

Result: Superior performance in extended rollout predictions on chaotic traffic datasets compared to traditional and transformer-based neural operators, with better preservation of high-frequency components and improved stability over long prediction horizons.

Conclusion: DETNO effectively overcomes the inherent smoothing limitations and rollout instability of standard neural operators, making it a promising solution for accurate long-term traffic forecasting in intelligent transportation systems.

Abstract: Accurate long-term traffic forecasting remains a critical challenge in
intelligent transportation systems, particularly when predicting high-frequency
traffic phenomena such as shock waves and congestion boundaries over extended
rollout horizons. Neural operators have recently gained attention as promising
tools for modeling traffic flow. While effective at learning function space
mappings, they inherently produce smooth predictions that fail to reconstruct
high-frequency features such as sharp density gradients which results in rapid
error accumulation during multi-step rollout predictions essential for
real-time traffic management. To address these fundamental limitations, we
introduce a unified Diffusion-Enhanced Transformer Neural Operator (DETNO)
architecture. DETNO leverages a transformer neural operator with
cross-attention mechanisms, providing model expressivity and super-resolution,
coupled with a diffusion-based refinement component that iteratively
reconstructs high-frequency traffic details through progressive denoising. This
overcomes the inherent smoothing limitations and rollout instability of
standard neural operators. Through comprehensive evaluation on chaotic traffic
datasets, our method demonstrates superior performance in extended rollout
predictions compared to traditional and transformer-based neural operators,
preserving high-frequency components and improving stability over long
prediction horizons.

</details>


### [180] [Quantum-Classical Hybrid Molecular Autoencoder for Advancing Classical Decoding](https://arxiv.org/abs/2508.19394)
*Afrar Jahin,Yi Pan,Yingfeng Wang,Tianming Liu,Wei Zhang*

Main category: cs.LG

TL;DR: Hybrid quantum-classical architecture for SMILES string reconstruction achieves 84% quantum fidelity and 60% classical similarity, outperforming existing quantum baselines.


<details>
  <summary>Details</summary>
Motivation: Classical approaches struggle with high fidelity and validity in molecular design, and quantum machine learning integration with sequence-based tasks like SMILES reconstruction remains underexplored with fidelity degradation issues.

Method: Proposes a hybrid quantum-classical architecture that integrates quantum encoding with classical sequence modeling for SMILES reconstruction tasks.

Result: Achieves approximately 84% quantum fidelity and 60% classical reconstruction similarity, surpassing existing quantum baselines.

Conclusion: The work establishes a promising foundation for future QML applications by balancing quantum representations with classical sequence models, enabling broader research on quantum-aware sequence models for molecular and drug discovery.

Abstract: Although recent advances in quantum machine learning (QML) offer significant
potential for enhancing generative models, particularly in molecular design, a
large array of classical approaches still face challenges in achieving high
fidelity and validity. In particular, the integration of QML with
sequence-based tasks, such as Simplified Molecular Input Line Entry System
(SMILES) string reconstruction, remains underexplored and usually suffers from
fidelity degradation. In this work, we propose a hybrid quantum-classical
architecture for SMILES reconstruction that integrates quantum encoding with
classical sequence modeling to improve quantum fidelity and classical
similarity. Our approach achieves a quantum fidelity of approximately 84% and a
classical reconstruction similarity of 60%, surpassing existing quantum
baselines. Our work lays a promising foundation for future QML applications,
striking a balance between expressive quantum representations and classical
sequence models and catalyzing broader research on quantum-aware sequence
models for molecular and drug discovery.

</details>


### [181] [Kolmogorov-Arnold Representation for Symplectic Learning: Advancing Hamiltonian Neural Networks](https://arxiv.org/abs/2508.19410)
*Zongyu Wu,Ruichen Xu,Luoyao Chen,Georgios Kementzidis,Siyao Wang,Yuefan Deng*

Main category: cs.LG

TL;DR: KAR-HNN replaces MLPs with univariate transformations in Hamiltonian Neural Networks to improve energy conservation and stability in complex physical systems.


<details>
  <summary>Details</summary>
Motivation: Existing HNN implementations using MLPs cause hypersensitivity to hyperparameters and struggle with complex energy landscapes, leading to energy drift and poor long-term stability.

Method: Proposes Kolmogorov-Arnold Representation-based Hamiltonian Neural Network that uses localized univariate transformations instead of MLPs to better capture high-frequency and multi-scale dynamics while preserving symplectic structure.

Result: Tested on four benchmark problems (spring-mass, simple pendulum, two- and three-body problems), showing reduced energy drift and improved long-term predictive stability compared to MLP-based HNNs.

Conclusion: KAR-HNN is effective for accurate and stable modeling of realistic physical processes, particularly in high-dimensional systems with few known parameters, maintaining interpretability and physical consistency.

Abstract: We propose a Kolmogorov-Arnold Representation-based Hamiltonian Neural
Network (KAR-HNN) that replaces the Multilayer Perceptrons (MLPs) with
univariate transformations. While Hamiltonian Neural Networks (HNNs) ensure
energy conservation by learning Hamiltonian functions directly from data,
existing implementations, often relying on MLPs, cause hypersensitivity to the
hyperparameters while exploring complex energy landscapes. Our approach
exploits the localized function approximations to better capture high-frequency
and multi-scale dynamics, reducing energy drift and improving long-term
predictive stability. The networks preserve the symplectic form of Hamiltonian
systems, and thus maintain interpretability and physical consistency. After
assessing KAR-HNN on four benchmark problems including spring-mass, simple
pendulum, two- and three-body problem, we foresee its effectiveness for
accurate and stable modeling of realistic physical processes often at high
dimensions and with few known parameters.

</details>


### [182] [Even Heads Fix Odd Errors: Mechanistic Discovery and Surgical Repair in Transformer Attention](https://arxiv.org/abs/2508.19414)
*Gustavo Sandoval*

Main category: cs.LG

TL;DR: Llama-3.1-8B-Instruct shows format-dependent reasoning failure where it incorrectly judges "9.11" > "9.8" in chat formats but works correctly in simple format, revealing specialized even/odd attention head organization with sharp computational thresholds.


<details>
  <summary>Details</summary>
Motivation: To understand why transformer models exhibit format-dependent reasoning failures and uncover the mechanistic organization of attention heads in numerical comparison tasks.

Method: Systematic intervention experiments, attention head analysis, and sparse autoencoder (SAE) feature analysis to study format representations and head specialization patterns.

Result: Discovered even/odd head specialization (even heads handle numerical comparison), identified perfect repair requires exactly 8 even heads at Layer 10, found 60% pattern replacement threshold, and achieved perfect repair using only 25% of attention heads.

Conclusion: Transformer models have sophisticated substructure with perfect redundancy among specialized heads, and apparent full-module requirements hide efficient computational organization with implications for interpretability and model efficiency.

Abstract: We present a mechanistic case study of a format-dependent reasoning failure
in Llama-3.1-8B-Instruct, where the model incorrectly judges "9.11" as larger
than "9.8" in chat or Q&A formats, but answers correctly in simple format.
Through systematic intervention, we discover transformers implement even/odd
attention head specialization: even indexed heads handle numerical comparison,
while odd heads serve incompatible functions. The bug requires exactly 8 even
heads at Layer 10 for perfect repair. Any combination of 8+ even heads
succeeds, while 7 or fewer completely fails, revealing sharp computational
thresholds with perfect redundancy among the 16 even heads. SAE analysis
reveals the mechanism: format representations separate (10% feature overlap at
Layer 7), then re-entangle with different weightings (80% feature overlap at
Layer 10), with specific features showing 1.5x amplification in failing
formats. We achieve perfect repair using only 25% of attention heads and
identify a 60% pattern replacement threshold, demonstrating that apparent
full-module requirements hide sophisticated substructure with implications for
interpretability and efficiency. All of our code is available at
https://github.com/gussand/surgeon.

</details>


### [183] [Differentiable multiphase flow model for physics-informed machine learning in reservoir pressure management](https://arxiv.org/abs/2508.19419)
*Harun Ur Rashid,Aleksandra Pachalieva,Daniel O'Malley*

Main category: cs.LG

TL;DR: Physics-informed ML workflow combining differentiable multiphase flow simulator with CNN to predict fluid extraction rates from permeability fields, enabling accurate pressure control with dramatically fewer simulations.


<details>
  <summary>Details</summary>
Motivation: Accurate subsurface reservoir pressure control is challenging due to geological heterogeneity and multiphase fluid-flow dynamics, requiring computationally expensive high-fidelity simulations that are often prohibitive to run in large quantities.

Method: Couples a fully differentiable multiphase flow simulator (DPFEHM framework) with a convolutional neural network (CNN) that learns to predict fluid extraction rates from heterogeneous permeability fields. Uses transfer learning: pretrains on single-phase steady-state simulations, then fine-tunes on full multiphase scenarios.

Result: Achieves high-accuracy training with fewer than 3,000 full-physics multiphase flow simulations (compared to previous estimates requiring up to 10 million). Dramatically reduces computational cost while maintaining accuracy for realistic injection-extraction scenarios.

Conclusion: The physics-informed ML workflow enables practical and accurate predictions for reservoir pressure control by leveraging transfer learning from inexpensive single-phase simulations, making previously prohibitive computational requirements feasible.

Abstract: Accurate subsurface reservoir pressure control is extremely challenging due
to geological heterogeneity and multiphase fluid-flow dynamics. Predicting
behavior in this setting relies on high-fidelity physics-based simulations that
are computationally expensive. Yet, the uncertain, heterogeneous properties
that control these flows make it necessary to perform many of these expensive
simulations, which is often prohibitive. To address these challenges, we
introduce a physics-informed machine learning workflow that couples a fully
differentiable multiphase flow simulator, which is implemented in the DPFEHM
framework with a convolutional neural network (CNN). The CNN learns to predict
fluid extraction rates from heterogeneous permeability fields to enforce
pressure limits at critical reservoir locations. By incorporating transient
multiphase flow physics into the training process, our method enables more
practical and accurate predictions for realistic injection-extraction scenarios
compare to previous works. To speed up training, we pretrain the model on
single-phase, steady-state simulations and then fine-tune it on full multiphase
scenarios, which dramatically reduces the computational cost. We demonstrate
that high-accuracy training can be achieved with fewer than three thousand
full-physics multiphase flow simulations -- compared to previous estimates
requiring up to ten million. This drastic reduction in the number of
simulations is achieved by leveraging transfer learning from much less
expensive single-phase simulations.

</details>


### [184] [MS-ConTab: Multi-Scale Contrastive Learning of Mutation Signatures for Pan Cancer Representation and Stratification](https://arxiv.org/abs/2508.19424)
*Yifan Dou,Adam Khadre,Ruben C Petreaca,Golrokh Mirzaei*

Main category: cs.LG

TL;DR: Novel unsupervised contrastive learning framework clusters 43 cancer types using dual mutation signatures (gene-level and chromosome-level) from COSMIC data, producing biologically meaningful cancer groupings.


<details>
  <summary>Details</summary>
Motivation: Understanding pan-cancer mutational landscape and improving cohort-level cancer clustering beyond classical statistical methods using modern machine learning techniques.

Method: Unsupervised contrastive learning with TabNet encoders, using dual mutation signatures (gene-level profiles and chromosome-level profiles) optimized via NT-Xent loss to learn unified cancer-type embeddings.

Result: The framework produces biologically meaningful clusters of cancer types that align with known mutational processes and tissue origins.

Conclusion: First successful application of contrastive learning to cohort-level cancer clustering, providing a scalable and interpretable framework for mutation-driven cancer subtyping.

Abstract: Motivation. Understanding the pan-cancer mutational landscape offers critical
insights into the molecular mechanisms underlying tumorigenesis. While
patient-level machine learning techniques have been widely employed to identify
tumor subtypes, cohort-level clustering, where entire cancer types are grouped
based on shared molecular features, has largely relied on classical statistical
methods.
  Results. In this study, we introduce a novel unsupervised contrastive
learning framework to cluster 43 cancer types based on coding mutation data
derived from the COSMIC database. For each cancer type, we construct two
complementary mutation signatures: a gene-level profile capturing nucleotide
substitution patterns across the most frequently mutated genes, and a
chromosome-level profile representing normalized substitution frequencies
across chromosomes. These dual views are encoded using TabNet encoders and
optimized via a multi-scale contrastive learning objective (NT-Xent loss) to
learn unified cancer-type embeddings. We demonstrate that the resulting latent
representations yield biologically meaningful clusters of cancer types,
aligning with known mutational processes and tissue origins. Our work
represents the first application of contrastive learning to cohort-level cancer
clustering, offering a scalable and interpretable framework for mutation-driven
cancer subtyping.

</details>


### [185] [Data-Augmented Few-Shot Neural Stencil Emulation for System Identification of Computer Models](https://arxiv.org/abs/2508.19441)
*Sanket Jantre,Deepak Akhare,Xiaoning Qian,Nathan M. Urban*

Main category: cs.LG

TL;DR: A data-augmentation strategy using space-filling sampling of local stencil states to train neural PDEs more efficiently than traditional trajectory-based methods.


<details>
  <summary>Details</summary>
Motivation: Neural PDEs are easier to work with than traditional numerical solvers but typically require extensive trajectory data from long time integration, which is inefficient and redundant.

Method: Proposes space-filling sampling of local "stencil" states to generate training data, removing spatiotemporal redundancy and oversampling rare but important states for better generalization.

Result: Accurate neural PDE stencil operators can be learned from synthetic training data equivalent to just 10 timesteps of simulation, with further improvement using a single full-trajectory simulation.

Conclusion: The data-augmented synthetic stencil approach yields better trained neural stencil operators with clear performance gains over naive trajectory sampling across multiple PDE systems.

Abstract: Partial differential equations (PDEs) underpin the modeling of many natural
and engineered systems. It can be convenient to express such models as neural
PDEs rather than using traditional numerical PDE solvers by replacing part or
all of the PDE's governing equations with a neural network representation.
Neural PDEs are often easier to differentiate, linearize, reduce, or use for
uncertainty quantification than the original numerical solver. They are usually
trained on solution trajectories obtained by long time integration of the PDE
solver. Here we propose a more sample-efficient data-augmentation strategy for
generating neural PDE training data from a computer model by space-filling
sampling of local "stencil" states. This approach removes a large degree of
spatiotemporal redundancy present in trajectory data and oversamples states
that may be rarely visited but help the neural PDE generalize across the state
space. We demonstrate that accurate neural PDE stencil operators can be learned
from synthetic training data generated by the computational equivalent of 10
timesteps' worth of numerical simulation. Accuracy is further improved if we
assume access to a single full-trajectory simulation from the computer model,
which is typically available in practice. Across several PDE systems, we show
that our data-augmented synthetic stencil data yield better trained neural
stencil operators, with clear performance gains compared with naively sampled
stencil data from simulation trajectories.

</details>


### [186] [Efficiently Generating Multidimensional Calorimeter Data with Tensor Decomposition Parameterization](https://arxiv.org/abs/2508.19443)
*Paimon Goulart,Shaan Pakala,Evangelos Papalexakis*

Main category: cs.LG

TL;DR: Using tensor decomposition in generative models to reduce costs when generating multidimensional simulation data by producing smaller tensor factors instead of full tensors.


<details>
  <summary>Details</summary>
Motivation: Large complex simulation datasets are time and resource consuming to produce, making synthetic data generation more reasonable for expensive experiments. Generative models like GANs and diffusion models improve efficiency but can be further optimized.

Method: Introduce internal tensor decomposition to generative models for multidimensional data. Generate smaller tensor factors instead of full tensors to reduce model output size and overall parameters.

Result: Significantly reduces costs of generating complex simulation data while maintaining data usefulness, as shown in experiments.

Conclusion: Tensor decomposition has the potential to improve efficiency in generative models, particularly for generating multidimensional data (tensors).

Abstract: Producing large complex simulation datasets can often be a time and resource
consuming task. Especially when these experiments are very expensive, it is
becoming more reasonable to generate synthetic data for downstream tasks.
Recently, these methods may include using generative machine learning models
such as Generative Adversarial Networks or diffusion models. As these
generative models improve efficiency in producing useful data, we introduce an
internal tensor decomposition to these generative models to even further reduce
costs. More specifically, for multidimensional data, or tensors, we generate
the smaller tensor factors instead of the full tensor, in order to
significantly reduce the model's output and overall parameters. This reduces
the costs of generating complex simulation data, and our experiments show the
generated data remains useful. As a result, tensor decomposition has the
potential to improve efficiency in generative models, especially when
generating multidimensional data, or tensors.

</details>


### [187] [On Surjectivity of Neural Networks: Can you elicit any behavior from your model?](https://arxiv.org/abs/2508.19445)
*Haozhe Jiang,Nika Haghtalab*

Main category: cs.LG

TL;DR: The paper proves that many modern neural network architectures (pre-layer normalization, linear-attention modules) are almost always surjective, meaning any output can be generated by some input, revealing inherent vulnerabilities to adversarial attacks.


<details>
  <summary>Details</summary>
Motivation: To investigate whether trained neural networks can generate any specified output, which has implications for model safety and jailbreak vulnerabilities in generative AI systems.

Method: Mathematical analysis and proofs showing that fundamental building blocks like pre-layer normalization networks and linear-attention modules are surjective functions.

Result: Widely used frameworks including GPT-style transformers and diffusion models with deterministic ODE solvers admit inverse mappings for arbitrary outputs, making them vulnerable.

Conclusion: Modern neural architectures have inherent surjectivity properties that create unavoidable vulnerabilities to adversarial attacks, raising significant safety concerns for generative models.

Abstract: Given a trained neural network, can any specified output be generated by some
input? Equivalently, does the network correspond to a function that is
surjective? In generative models, surjectivity implies that any output,
including harmful or undesirable content, can in principle be generated by the
networks, raising concerns about model safety and jailbreak vulnerabilities. In
this paper, we prove that many fundamental building blocks of modern neural
architectures, such as networks with pre-layer normalization and
linear-attention modules, are almost always surjective. As corollaries, widely
used generative frameworks, including GPT-style transformers and diffusion
models with deterministic ODE solvers, admit inverse mappings for arbitrary
outputs. By studying surjectivity of these modern and commonly used neural
architectures, we contribute a formalism that sheds light on their unavoidable
vulnerability to a broad class of adversarial attacks.

</details>


### [188] [The Sample Complexity of Membership Inference and Privacy Auditing](https://arxiv.org/abs/2508.19458)
*Mahdi Haghifam,Adam Smith,Jonathan Ullman*

Main category: cs.LG

TL;DR: This paper analyzes the sample complexity required for successful membership-inference attacks in Gaussian mean estimation, showing that Ω(n + n²ρ²) reference samples are sometimes necessary - more than the n samples used for training.


<details>
  <summary>Details</summary>
Motivation: To understand the minimum information (reference samples) an attacker needs for membership inference, as current practical attacks use limited samples and may underestimate privacy risks.

Method: Theoretical analysis of membership-inference attacks in the fundamental setting of Gaussian mean estimation, where the algorithm estimates μ from n samples with error bound ρ²d.

Result: Shows that Ω(n + n²ρ²) reference samples are necessary for attacks to compete with fully informed attackers, which can be significantly more than the n training samples.

Conclusion: Current practical attacks using O(n) samples may underestimate membership inference risks, and better attacks are possible when more distribution information is available.

Abstract: A membership-inference attack gets the output of a learning algorithm, and a
target individual, and tries to determine whether this individual is a member
of the training data or an independent sample from the same distribution. A
successful membership-inference attack typically requires the attacker to have
some knowledge about the distribution that the training data was sampled from,
and this knowledge is often captured through a set of independent reference
samples from that distribution. In this work we study how much information the
attacker needs for membership inference by investigating the sample
complexity-the minimum number of reference samples required-for a successful
attack. We study this question in the fundamental setting of Gaussian mean
estimation where the learning algorithm is given $n$ samples from a Gaussian
distribution $\mathcal{N}(\mu,\Sigma)$ in $d$ dimensions, and tries to estimate
$\hat\mu$ up to some error $\mathbb{E}[\|\hat \mu - \mu\|^2_{\Sigma}]\leq
\rho^2 d$. Our result shows that for membership inference in this setting,
$\Omega(n + n^2 \rho^2)$ samples can be necessary to carry out any attack that
competes with a fully informed attacker. Our result is the first to show that
the attacker sometimes needs many more samples than the training algorithm uses
to train the model. This result has significant implications for practice, as
all attacks used in practice have a restricted form that uses $O(n)$ samples
and cannot benefit from $\omega(n)$ samples. Thus, these attacks may be
underestimating the possibility of membership inference, and better attacks may
be possible when information about the distribution is easy to obtain.

</details>


### [189] [Incentivized Lipschitz Bandits](https://arxiv.org/abs/2508.19466)
*Sourav Chakraborty,Amit Kiran Rege,Claire Monteleoni,Lijun Chen*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We study incentivized exploration in multi-armed bandit (MAB) settings with
infinitely many arms modeled as elements in continuous metric spaces. Unlike
classical bandit models, we consider scenarios where the decision-maker
(principal) incentivizes myopic agents to explore beyond their greedy choices
through compensation, but with the complication of reward drift--biased
feedback arising due to the incentives. We propose novel incentivized
exploration algorithms that discretize the infinite arm space uniformly and
demonstrate that these algorithms simultaneously achieve sublinear cumulative
regret and sublinear total compensation. Specifically, we derive regret and
compensation bounds of $\Tilde{O}(T^{d+1/d+2})$, with $d$ representing the
covering dimension of the metric space. Furthermore, we generalize our results
to contextual bandits, achieving comparable performance guarantees. We validate
our theoretical findings through numerical simulations.

</details>


### [190] [DeepAtlas: a tool for effective manifold learning](https://arxiv.org/abs/2508.19479)
*Serena Hughes,Timothy Hamilton,Tom Kolokotrones,Eric J. Deeds*

Main category: cs.LG

TL;DR: DeepAtlas is a novel algorithm that generates local manifold embeddings and assesses whether datasets conform to the manifold hypothesis, finding that many real datasets including single-cell RNA-seq do not follow this hypothesis.


<details>
  <summary>Details</summary>
Motivation: Current manifold learning tools only create global embeddings and cannot verify if the manifold hypothesis holds true for datasets, limiting their mathematical rigor and practical applications.

Method: DeepAtlas generates lower-dimensional representations of local neighborhoods and trains deep neural networks to map between these local embeddings and original data, using topological distortion to determine manifold existence and dimensionality.

Result: The algorithm successfully learns manifold structures in test datasets, but reveals that many real datasets (including single-cell RNA-sequencing) do not conform to the manifold hypothesis. When data is manifold-based, DeepAtlas builds generative models.

Conclusion: DeepAtlas provides a rigorous method to test the manifold hypothesis and enables application of differential geometry tools to datasets that truly follow manifold structures, while revealing limitations of the hypothesis for real-world data.

Abstract: Manifold learning builds on the "manifold hypothesis," which posits that data
in high-dimensional datasets are drawn from lower-dimensional manifolds.
Current tools generate global embeddings of data, rather than the local maps
used to define manifolds mathematically. These tools also cannot assess whether
the manifold hypothesis holds true for a dataset. Here, we describe DeepAtlas,
an algorithm that generates lower-dimensional representations of the data's
local neighborhoods, then trains deep neural networks that map between these
local embeddings and the original data. Topological distortion is used to
determine whether a dataset is drawn from a manifold and, if so, its
dimensionality. Application to test datasets indicates that DeepAtlas can
successfully learn manifold structures. Interestingly, many real datasets,
including single-cell RNA-sequencing, do not conform to the manifold
hypothesis. In cases where data is drawn from a manifold, DeepAtlas builds a
model that can be used generatively and promises to allow the application of
powerful tools from differential geometry to a variety of datasets.

</details>


### [191] [Distribution Shift Aware Neural Tabular Learning](https://arxiv.org/abs/2508.19486)
*Wangyang Ying,Nanxu Gong,Dongjie Wang,Xinyuan Wang,Arun Vignesh Malarkkan,Vivek Gupta,Chandan K. Reddy,Yanjie Fu*

Main category: cs.LG

TL;DR: SAFT framework transforms tabular learning from discrete search to continuous representation-generation to handle distribution shifts between training and testing data through embedding decorrelation, sample reweighting, and normalization-based alignment.


<details>
  <summary>Details</summary>
Motivation: Tabular learning effectiveness deteriorates under distribution shifts between training and testing data, creating a need for robust feature transformation methods that can maintain performance across different data distributions.

Method: Shift-Aware Feature Transformation (SAFT) reframes tabular learning as continuous representation-generation with three mechanisms: shift-resistant representation (embedding decorrelation + sample reweighting), flatness-aware generation (suboptimal embedding averaging), and normalization-based distribution alignment.

Result: Extensive experiments show SAFT consistently outperforms prior tabular learning methods in robustness, effectiveness, and generalization ability under diverse real-world distribution shifts.

Conclusion: SAFT successfully addresses the Distribution Shift Tabular Learning problem by providing a differentiable optimization framework that maintains performance across distribution shifts through integrated robustness mechanisms.

Abstract: Tabular learning transforms raw features into optimized spaces for downstream
tasks, but its effectiveness deteriorates under distribution shifts between
training and testing data. We formalize this challenge as the Distribution
Shift Tabular Learning (DSTL) problem and propose a novel Shift-Aware Feature
Transformation (SAFT) framework to address it. SAFT reframes tabular learning
from a discrete search task into a continuous representation-generation
paradigm, enabling differentiable optimization over transformed feature sets.
SAFT integrates three mechanisms to ensure robustness: (i) shift-resistant
representation via embedding decorrelation and sample reweighting, (ii)
flatness-aware generation through suboptimal embedding averaging, and (iii)
normalization-based alignment between training and test distributions.
Extensive experiments show that SAFT consistently outperforms prior tabular
learning methods in terms of robustness, effectiveness, and generalization
ability under diverse real-world distribution shifts.

</details>


### [192] [Data-Efficient Symbolic Regression via Foundation Model Distillation](https://arxiv.org/abs/2508.19487)
*Wangyang Ying,Jinghan Zhang,Haoyue Bai,Nanxu Gong,Xinyuan Wang,Kunpeng Liu,Chandan K. Reddy,Yanjie Fu*

Main category: cs.LG

TL;DR: EQUATE is a data-efficient fine-tuning framework that adapts foundation models for symbolic equation discovery via distillation, combining symbolic-numeric alignment with evaluator-guided embedding optimization to outperform state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Foundation models pre-trained on large equation datasets often suffer from negative transfer and poor generalization when applied to small, domain-specific datasets, limiting their effectiveness in scientific discovery applications.

Method: EQUATE reformulates discrete equation search as continuous optimization in a shared embedding space, using symbolic-numeric alignment and evaluator-guided embedding optimization based on data-equation fitness and simplicity.

Result: Experiments across three standard benchmarks (Feynman, Strogatz, black-box datasets) show EQUATE consistently outperforms state-of-the-art baselines in accuracy and robustness while preserving low complexity and fast inference.

Conclusion: EQUATE provides a practical and generalizable solution for data-efficient symbolic regression in foundation model distillation settings, enabling better adaptation to low-data regimes.

Abstract: Discovering interpretable mathematical equations from observed data (a.k.a.
equation discovery or symbolic regression) is a cornerstone of scientific
discovery, enabling transparent modeling of physical, biological, and economic
systems. While foundation models pre-trained on large-scale equation datasets
offer a promising starting point, they often suffer from negative transfer and
poor generalization when applied to small, domain-specific datasets. In this
paper, we introduce EQUATE (Equation Generation via QUality-Aligned Transfer
Embeddings), a data-efficient fine-tuning framework that adapts foundation
models for symbolic equation discovery in low-data regimes via distillation.
EQUATE combines symbolic-numeric alignment with evaluator-guided embedding
optimization, enabling a principled embedding-search-generation paradigm. Our
approach reformulates discrete equation search as a continuous optimization
task in a shared embedding space, guided by data-equation fitness and
simplicity. Experiments across three standard public benchmarks (Feynman,
Strogatz, and black-box datasets) demonstrate that EQUATE consistently
outperforms state-of-the-art baselines in both accuracy and robustness, while
preserving low complexity and fast inference. These results highlight EQUATE as
a practical and generalizable solution for data-efficient symbolic regression
in foundation model distillation settings.

</details>


### [193] [PoolFlip: A Multi-Agent Reinforcement Learning Security Environment for Cyber Defense](https://arxiv.org/abs/2508.19488)
*Xavier Cadet,Simona Boboila,Sie Hendrata Dharmawan,Alina Oprea,Peter Chin*

Main category: cs.LG

TL;DR: PoolFlip extends FlipIt game with MARL environment, Flip-PSRO trains defenders to generalize 2x better against unseen attacks using population-based training and ownership-based utility functions.


<details>
  <summary>Details</summary>
Motivation: Existing FlipIt frameworks rely on limited heuristics and specialized learning techniques, leading to brittleness and inability to adapt to new stealthy, deceptive adversarial strategies in cyber defense.

Method: Introduces PoolFlip multi-agent gym environment for FlipIt game, and Flip-PSRO - a multi-agent reinforcement learning approach using population-based training to train defender agents against adaptive opponents.

Result: Flip-PSRO defenders are 2x more effective than baselines in generalizing to heuristic attacks not exposed during training, while maintaining high control levels with ownership-based utility functions.

Conclusion: The proposed PoolFlip environment and Flip-PSRO approach significantly improve defender adaptability and generalization capabilities against evolving cyber threats in the FlipIt framework.

Abstract: Cyber defense requires automating defensive decision-making under stealthy,
deceptive, and continuously evolving adversarial strategies. The FlipIt game
provides a foundational framework for modeling interactions between a defender
and an advanced adversary that compromises a system without being immediately
detected. In FlipIt, the attacker and defender compete to control a shared
resource by performing a Flip action and paying a cost. However, the existing
FlipIt frameworks rely on a small number of heuristics or specialized learning
techniques, which can lead to brittleness and the inability to adapt to new
attacks. To address these limitations, we introduce PoolFlip, a multi-agent gym
environment that extends the FlipIt game to allow efficient learning for
attackers and defenders. Furthermore, we propose Flip-PSRO, a multi-agent
reinforcement learning (MARL) approach that leverages population-based training
to train defender agents equipped to generalize against a range of unknown,
potentially adaptive opponents. Our empirical results suggest that Flip-PSRO
defenders are $2\times$ more effective than baselines to generalize to a
heuristic attack not exposed in training. In addition, our newly designed
ownership-based utility functions ensure that Flip-PSRO defenders maintain a
high level of control while optimizing performance.

</details>


### [194] [Learning Game-Playing Agents with Generative Code Optimization](https://arxiv.org/abs/2508.19506)
*Zhiyi Kuang,Ryan Rong,YuCheng Yuan,Allen Nie*

Main category: cs.LG

TL;DR: A generative optimization approach that uses LLMs to evolve Python program policies for game-playing agents, achieving competitive performance with deep RL baselines using less training time and environment interactions.


<details>
  <summary>Details</summary>
Motivation: To develop more efficient and adaptable game-playing agents that can self-improve through programmatic policy representations with minimal human intervention, enabling complex long-horizon reasoning.

Method: Policies are represented as Python programs that take current observation as input and output in-game actions. LLMs refine these programs using execution traces and natural language feedback for self-evolution.

Result: The approach achieves performance competitive with deep reinforcement learning baselines on Atari games while using significantly less training time and fewer environment interactions.

Conclusion: Programmatic policy representations show promise for building efficient, adaptable agents capable of complex reasoning, highlighting the potential of code-based approaches over traditional neural network policies.

Abstract: We present a generative optimization approach for learning game-playing
agents, where policies are represented as Python programs and refined using
large language models (LLMs). Our method treats decision-making policies as
self-evolving code, with current observation as input and an in-game action as
output, enabling agents to self-improve through execution traces and natural
language feedback with minimal human intervention. Applied to Atari games, our
game-playing Python program achieves performance competitive with deep
reinforcement learning (RL) baselines while using significantly less training
time and much fewer environment interactions. This work highlights the promise
of programmatic policy representations for building efficient, adaptable agents
capable of complex, long-horizon reasoning.

</details>


### [195] [MobText-SISA: Efficient Machine Unlearning for Mobility Logs with Spatio-Temporal and Natural-Language Data](https://arxiv.org/abs/2508.19554)
*Haruki Yonekura,Ren Ozeki,Tatsuya Amano,Hamada Rizk,Hirozumi Yamaguchi*

Main category: cs.LG

TL;DR: MobText-SISA is a machine unlearning framework for spatio-temporal mobility data that enables efficient deletion of individual contributions while maintaining model performance, using similarity-aware clustering and sharded training.


<details>
  <summary>Details</summary>
Motivation: Privacy regulations like GDPR require the ability to delete individual data from models, but retraining from scratch for each deletion request is computationally infeasible for large mobility datasets containing GPS trajectories, temporal metadata, and textual notes.

Method: Extends SISA training to heterogeneous data by embedding numerical and linguistic features into shared latent space, using similarity-aware clustering to distribute samples across shards, training each shard incrementally, and aggregating predictions at inference. Deletion triggers retraining of only the affected shard.

Result: Experiments on 10-month real-world mobility data show MobText-SISA maintains baseline predictive accuracy and outperforms random sharding in both error reduction and convergence speed.

Conclusion: MobText-SISA provides a practical solution for privacy-compliant analytics on multimodal mobility data at urban scale by enabling efficient exact unlearning while preserving model performance.

Abstract: Modern mobility platforms have stored vast streams of GPS trajectories,
temporal metadata, free-form textual notes, and other unstructured data.
Privacy statutes such as the GDPR require that any individual's contribution be
unlearned on demand, yet retraining deep models from scratch for every request
is untenable. We introduce MobText-SISA, a scalable machine-unlearning
framework that extends Sharded, Isolated, Sliced, and Aggregated (SISA)
training to heterogeneous spatio-temporal data. MobText-SISA first embeds each
trip's numerical and linguistic features into a shared latent space, then
employs similarity-aware clustering to distribute samples across shards so that
future deletions touch only a single constituent model while preserving
inter-shard diversity. Each shard is trained incrementally; at inference time,
constituent predictions are aggregated to yield the output. Deletion requests
trigger retraining solely of the affected shard from its last valid checkpoint,
guaranteeing exact unlearning. Experiments on a ten-month real-world mobility
log demonstrate that MobText-SISA (i) sustains baseline predictive accuracy,
and (ii) consistently outperforms random sharding in both error and convergence
speed. These results establish MobText-SISA as a practical foundation for
privacy-compliant analytics on multimodal mobility data at urban scale.

</details>


### [196] [Just Because You Can, Doesn't Mean You Should: LLMs for Data Fitting](https://arxiv.org/abs/2508.19563)
*Hejia Liu,Mochen Yang,Gediminas Adomavicius*

Main category: cs.LG

TL;DR: LLMs show significant prediction sensitivity to task-irrelevant data variations like variable name changes, with error fluctuations up to 82%, revealing fundamental robustness issues for data fitting applications.


<details>
  <summary>Details</summary>
Motivation: To investigate the vulnerability of LLMs when used for data fitting tasks, particularly their sensitivity to irrelevant data representation changes that should not affect predictions.

Method: Examined LLM prediction sensitivity through in-context learning and supervised fine-tuning, analyzed attention patterns in open-weight models, and compared with specialized tabular foundation model TabPFN.

Result: LLMs exhibit dramatic prediction changes (up to 82% error variation) from irrelevant modifications like variable name changes, with non-uniform attention patterns explaining the sensitivity. TabPFN also shows vulnerability despite being designed for robustness.

Conclusion: Current LLMs lack basic robustness required for principled data-fitting applications, as they are highly sensitive to task-irrelevant variations in data representation.

Abstract: Large Language Models (LLMs) are being applied in a wide array of settings,
well beyond the typical language-oriented use cases. In particular, LLMs are
increasingly used as a plug-and-play method for fitting data and generating
predictions. Prior work has shown that LLMs, via in-context learning or
supervised fine-tuning, can perform competitively with many tabular supervised
learning techniques in terms of predictive performance. However, we identify a
critical vulnerability of using LLMs for data fitting -- making changes to data
representation that are completely irrelevant to the underlying learning task
can drastically alter LLMs' predictions on the same data. For example, simply
changing variable names can sway the size of prediction error by as much as 82%
in certain settings. Such prediction sensitivity with respect to
task-irrelevant variations manifests under both in-context learning and
supervised fine-tuning, for both close-weight and open-weight general-purpose
LLMs. Moreover, by examining the attention scores of an open-weight LLM, we
discover a non-uniform attention pattern: training examples and variable
names/values which happen to occupy certain positions in the prompt receive
more attention when output tokens are generated, even though different
positions are expected to receive roughly the same attention. This partially
explains the sensitivity in the presence of task-irrelevant variations. We also
consider a state-of-the-art tabular foundation model (TabPFN) trained
specifically for data fitting. Despite being explicitly designed to achieve
prediction robustness, TabPFN is still not immune to task-irrelevant
variations. Overall, despite LLMs' impressive predictive capabilities,
currently they lack even the basic level of robustness to be used as a
principled data-fitting tool.

</details>


### [197] [Bi-LoRA: Efficient Sharpness-Aware Minimization for Fine-Tuning Large-Scale Models](https://arxiv.org/abs/2508.19564)
*Yuhang Liu,Tao Li,Zhehao Huang,Zuopeng Yang,Xiaolin Huang*

Main category: cs.LG

TL;DR: Bi-LoRA combines SAM's flat minima seeking with LoRA's parameter efficiency by using dual modules - one for task adaptation and another for sharpness optimization, eliminating SAM's doubled computation costs while improving generalization.


<details>
  <summary>Details</summary>
Motivation: SAM improves generalization but has high memory/computation costs for large models. Direct SAM+LoRA integration limits sharpness optimization to restricted subspace, reducing effectiveness.

Method: Proposes Bi-directional LoRA with dual modules: primary LoRA for task adaptation via gradient descent, auxiliary LoRA for capturing loss landscape sharpness via gradient ascent, decoupling optimization from perturbation.

Result: Extensive experiments show Bi-LoRA efficiently enhances generalization across diverse tasks and architectures while remaining memory-efficient.

Conclusion: Bi-LoRA successfully integrates SAM's flat minima benefits with LoRA's parameter efficiency, achieving better generalization without SAM's computational overhead through dual-module design.

Abstract: Fine-tuning large-scale pre-trained models with limited data presents
significant challenges for generalization. While Sharpness-Aware Minimization
(SAM) has proven effective in improving generalization by seeking flat minima,
its substantial extra memory and computation overhead make it impractical for
large models. Integrating SAM with parameter-efficient fine-tuning methods like
Low-Rank Adaptation (LoRA) is a promising direction. However, we find that
directly applying SAM to LoRA parameters limits the sharpness optimization to a
restricted subspace, hindering its effectiveness. To address this limitation,
we propose Bi-directional Low-Rank Adaptation (Bi-LoRA), which introduces an
auxiliary LoRA module to model SAM's adversarial weight perturbations. It
decouples SAM's weight perturbations from LoRA optimization: the primary LoRA
module adapts to specific tasks via standard gradient descent, while the
auxiliary module captures the sharpness of the loss landscape through gradient
ascent. Such dual-module design enables Bi-LoRA to capture broader sharpness
for achieving flatter minima while remaining memory-efficient. Another
important benefit is that the dual design allows for simultaneous optimization
and perturbation, eliminating SAM's doubled training costs. Extensive
experiments across diverse tasks and architectures demonstrate Bi-LoRA's
efficiency and effectiveness in enhancing generalization.

</details>


### [198] [Counterfactual Reward Model Training for Bias Mitigation in Multimodal Reinforcement Learning](https://arxiv.org/abs/2508.19567)
*Sheryl Mathew,N Harshit*

Main category: cs.LG

TL;DR: Counterfactual reward model using causal inference and multimodal learning to reduce biases in RLHF, achieving 89.12% fake news detection accuracy while improving fairness.


<details>
  <summary>Details</summary>
Motivation: Reward models in RLHF can amplify latent biases from multimodal datasets, leading to flawed policy optimization and decreased fairness. Passive bias mitigation approaches often fail under causal confounding.

Method: Proposes a Counterfactual Trust Score with four components: counterfactual shifts to separate political framing bias from topical bias, reconstruction uncertainty during perturbations, fairness rule violations detection, and temporal reward shifts aligned with dynamic trust measures. Evaluated on multimodal fake/true news dataset with synthetic bias injection.

Result: Achieved 89.12% accuracy in fake news detection, outperforming baseline reward models. Reduced spurious correlations and unfair reinforcement signals while maintaining performance.

Conclusion: The framework provides a robust, interpretable approach to fairness-aware RLHF with tunable bias reduction thresholds, increasing reliability for dynamic real-time policy making.

Abstract: In reinforcement learning with human feedback (RLHF), reward models can
efficiently learn and amplify latent biases within multimodal datasets, which
can lead to imperfect policy optimization through flawed reward signals and
decreased fairness. Bias mitigation studies have often applied passive
constraints, which can fail under causal confounding. Here, we present a
counterfactual reward model that introduces causal inference with multimodal
representation learning to provide an unsupervised, bias-resilient reward
signal. The heart of our contribution is the Counterfactual Trust Score, an
aggregated score consisting of four components: (1) counterfactual shifts that
decompose political framing bias from topical bias; (2) reconstruction
uncertainty during counterfactual perturbations; (3) demonstrable violations of
fairness rules for each protected attribute; and (4) temporal reward shifts
aligned with dynamic trust measures. We evaluated the framework on a multimodal
fake versus true news dataset, which exhibits framing bias, class imbalance,
and distributional drift. Following methodologies similar to unsupervised drift
detection from representation-based distances [1] and temporal robustness
benchmarking in language models [2], we also inject synthetic bias across
sequential batches to test robustness. The resulting system achieved an
accuracy of 89.12% in fake news detection, outperforming the baseline reward
models. More importantly, it reduced spurious correlations and unfair
reinforcement signals. This pipeline outlines a robust and interpretable
approach to fairness-aware RLHF, offering tunable bias reduction thresholds and
increasing reliability in dynamic real-time policy making.

</details>


### [199] [Generative Models for Synthetic Data: Transforming Data Mining in the GenAI Era](https://arxiv.org/abs/2508.19570)
*Dawei Li,Yue Huang,Ming Li,Tianyi Zhou,Xiangliang Zhang,Huan Liu*

Main category: cs.LG

TL;DR: Tutorial on generative models for synthetic data generation to address data scarcity, privacy, and annotation challenges in data mining.


<details>
  <summary>Details</summary>
Motivation: Address data scarcity, privacy concerns, and annotation difficulties in data mining through synthetic data generation using advanced generative models.

Method: Covers foundations and latest advances in synthetic data generation methodologies, including Large Language Models, Diffusion Models, and generative adversarial networks, along with practical frameworks.

Result: Provides attendees with actionable insights and practical knowledge for leveraging generative synthetic data in data mining research and applications.

Conclusion: Generative synthetic data offers scalable solutions to key data mining challenges and can significantly enhance both research and practical applications in the field.

Abstract: Generative models such as Large Language Models, Diffusion Models, and
generative adversarial networks have recently revolutionized the creation of
synthetic data, offering scalable solutions to data scarcity, privacy, and
annotation challenges in data mining. This tutorial introduces the foundations
and latest advances in synthetic data generation, covers key methodologies and
practical frameworks, and discusses evaluation strategies and applications.
Attendees will gain actionable insights into leveraging generative synthetic
data to enhance data mining research and practice. More information can be
found on our website: https://syndata4dm.github.io/.

</details>


### [200] [Escaping Stability-Plasticity Dilemma in Online Continual Learning for Motion Forecasting via Synergetic Memory Rehearsal](https://arxiv.org/abs/2508.19571)
*Yunlong Lin,Chao Lu,Tongshuai Wu,Xiaocong Zhao,Guodong Du,Yanwei Sun,Zirui Li,Jianwei Gong*

Main category: cs.LG

TL;DR: SyReM is a novel continual learning method that addresses catastrophic forgetting in motion forecasting by balancing memory stability and learning plasticity through a compact memory buffer with inequality constraints and selective rehearsal based on gradient similarity.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks for motion forecasting suffer from catastrophic forgetting when adapting to new data, and existing continual learning methods often sacrifice learning plasticity when focusing too much on memory stability.

Method: SyReM maintains a compact memory buffer, uses inequality constraints to ensure memory stability, and employs selective memory rehearsal based on cosine similarity of loss gradients to enhance learning plasticity without compromising stability.

Result: Experiments on 11 naturalistic driving datasets show SyReM significantly mitigates catastrophic forgetting in past scenarios while improving forecasting accuracy in new ones compared to non-CL and CL baselines.

Conclusion: SyReM effectively addresses the stability-plasticity dilemma in continual learning for motion forecasting, demonstrating superior performance in maintaining knowledge from previous scenarios while effectively learning from new data.

Abstract: Deep neural networks (DNN) have achieved remarkable success in motion
forecasting. However, most DNN-based methods suffer from catastrophic
forgetting and fail to maintain their performance in previously learned
scenarios after adapting to new data. Recent continual learning (CL) studies
aim to mitigate this phenomenon by enhancing memory stability of DNN, i.e., the
ability to retain learned knowledge. Yet, excessive emphasis on the memory
stability often impairs learning plasticity, i.e., the capacity of DNN to
acquire new information effectively. To address such stability-plasticity
dilemma, this study proposes a novel CL method, synergetic memory rehearsal
(SyReM), for DNN-based motion forecasting. SyReM maintains a compact memory
buffer to represent learned knowledge. To ensure memory stability, it employs
an inequality constraint that limits increments in the average loss over the
memory buffer. Synergistically, a selective memory rehearsal mechanism is
designed to enhance learning plasticity by selecting samples from the memory
buffer that are most similar to recently observed data. This selection is based
on an online-measured cosine similarity of loss gradients, ensuring targeted
memory rehearsal. Since replayed samples originate from learned scenarios, this
memory rehearsal mechanism avoids compromising memory stability. We validate
SyReM under an online CL paradigm where training samples from diverse scenarios
arrive as a one-pass stream. Experiments on 11 naturalistic driving datasets
from INTERACTION demonstrate that, compared to non-CL and CL baselines, SyReM
significantly mitigates catastrophic forgetting in past scenarios while
improving forecasting accuracy in new ones. The implementation is publicly
available at https://github.com/BIT-Jack/SyReM.

</details>


### [201] [Delta-Audit: Explaining What Changes When Models Change](https://arxiv.org/abs/2508.19589)
*Arshia Hemmat,Afsaneh Fatemi*

Main category: cs.LG

TL;DR: Delta-Attribution is a model-agnostic framework that explains performance changes between model versions by differencing per-feature attributions, with comprehensive evaluation metrics covering magnitude, agreement, behavioral alignment, and robustness.


<details>
  <summary>Details</summary>
Motivation: Model updates often change performance but the reasons remain opaque, making it difficult to understand what specifically changed between different model versions.

Method: Differencing per-feature attributions between model versions A and B (Δφ(x)=φ_B(x)-φ_A(x)) using fast occlusion/clamping in standardized space with class-anchored margin and baseline averaging.

Result: The framework successfully distinguishes meaningful changes (e.g., inductive-bias changes yield large, behavior-aligned deltas with BAC≈0.998) from cosmetic tweaks (rank-overlap@10=1.0 and DCE≈0), with the largest redistribution observed for deeper Gradient Boosting on Breast Cancer (JSD≈0.357).

Conclusion: Δ-Attribution provides a lightweight update audit that complements accuracy metrics by distinguishing benign changes from behaviorally meaningful or risky reliance shifts between model versions.

Abstract: Model updates (new hyperparameters, kernels, depths, solvers, or data) change
performance, but the \emph{reason} often remains opaque. We introduce
\textbf{Delta-Attribution} (\mbox{$\Delta$-Attribution}), a model-agnostic
framework that explains \emph{what changed} between versions $A$ and $B$ by
differencing per-feature attributions: $\Delta\phi(x)=\phi_B(x)-\phi_A(x)$. We
evaluate $\Delta\phi$ with a \emph{$\Delta$-Attribution Quality Suite} covering
magnitude/sparsity (L1, Top-$k$, entropy), agreement/shift (rank-overlap@10,
Jensen--Shannon divergence), behavioural alignment (Delta Conservation Error,
DCE; Behaviour--Attribution Coupling, BAC; CO$\Delta$F), and robustness (noise,
baseline sensitivity, grouped occlusion).
  Instantiated via fast occlusion/clamping in standardized space with a
class-anchored margin and baseline averaging, we audit 45 settings: five
classical families (Logistic Regression, SVC, Random Forests, Gradient
Boosting, $k$NN), three datasets (Breast Cancer, Wine, Digits), and three A/B
pairs per family. \textbf{Findings.} Inductive-bias changes yield large,
behaviour-aligned deltas (e.g., SVC poly$\!\rightarrow$rbf on Breast Cancer:
BAC$\approx$0.998, DCE$\approx$6.6; Random Forest feature-rule swap on Digits:
BAC$\approx$0.997, DCE$\approx$7.5), while ``cosmetic'' tweaks (SVC
\texttt{gamma=scale} vs.\ \texttt{auto}, $k$NN search) show
rank-overlap@10$=1.0$ and DCE$\approx$0. The largest redistribution appears for
deeper GB on Breast Cancer (JSD$\approx$0.357). $\Delta$-Attribution offers a
lightweight update audit that complements accuracy by distinguishing benign
changes from behaviourally meaningful or risky reliance shifts.

</details>


### [202] [Complementary Learning System Empowers Online Continual Learning of Vehicle Motion Forecasting in Smart Cities](https://arxiv.org/abs/2508.19597)
*Zirui Li,Yunlong Lin,Guodong Du,Xiaocong Zhao,Cheng Gong,Chen Lv,Chao Lu,Jianwei Gong*

Main category: cs.LG

TL;DR: Dual-LS is a brain-inspired continual learning method that uses dual memory replay to prevent catastrophic forgetting in vehicle motion forecasting DNNs, achieving 74.31% forgetting reduction and 94.02% computational savings.


<details>
  <summary>Details</summary>
Motivation: Current DNN-based vehicle motion forecasting models suffer from catastrophic forgetting when updated, requiring costly data collection and failing to balance long- and short-term experience like human learning.

Method: Dual-LS employs a task-free, online continual learning paradigm with two synergistic memory rehearsal replay mechanisms inspired by the human brain's complementary learning system, dynamically coordinating long-term and short-term knowledge representations.

Result: Tests on naturalistic data from three countries (772,000 vehicles, 11,187 km testing mileage) show Dual-LS mitigates catastrophic forgetting by up to 74.31% and reduces computational resource demand by up to 94.02%, while maintaining predictive stability.

Conclusion: Dual-LS enables computation-efficient, human-like continual learning adaptability for DNN-based vehicle motion forecasting, making it suitable for smart city applications without increasing data requirements.

Abstract: Artificial intelligence underpins most smart city services, yet deep neural
network (DNN) that forecasts vehicle motion still struggle with catastrophic
forgetting, the loss of earlier knowledge when models are updated. Conventional
fixes enlarge the training set or replay past data, but these strategies incur
high data collection costs, sample inefficiently and fail to balance long- and
short-term experience, leaving them short of human-like continual learning.
Here we introduce Dual-LS, a task-free, online continual learning paradigm for
DNN-based motion forecasting that is inspired by the complementary learning
system of the human brain. Dual-LS pairs two synergistic memory rehearsal
replay mechanisms to accelerate experience retrieval while dynamically
coordinating long-term and short-term knowledge representations. Tests on
naturalistic data spanning three countries, over 772,000 vehicles and
cumulative testing mileage of 11,187 km show that Dual-LS mitigates
catastrophic forgetting by up to 74.31\% and reduces computational resource
demand by up to 94.02\%, markedly boosting predictive stability in vehicle
motion forecasting without inflating data requirements. Meanwhile, it endows
DNN-based vehicle motion forecasting with computation efficient and human-like
continual learning adaptability fit for smart cities.

</details>


### [203] [Encouraging Good Processes Without the Need for Good Answers: Reinforcement Learning for LLM Agent Planning](https://arxiv.org/abs/2508.19598)
*Zhiwei Li,Yong Hu,Wenqing Wang*

Main category: cs.LG

TL;DR: RLTR framework uses reinforcement learning with tool-use rewards to decouple and optimize LLM agent planning capability separately from summarization, achieving 8-12% planning improvement and 5-6% overall performance boost.


<details>
  <summary>Details</summary>
Motivation: End-to-end multi-objective training of LLM agents causes imbalanced optimization and data scarcity issues, making it difficult to specifically enhance the critical planning capability.

Method: Proposes RLTR framework that decouples training, uses reinforcement learning with tool-use completeness rewards to directly optimize planning module through single-objective optimization.

Result: Achieves 8%-12% improvement in planning performance compared to end-to-end baselines, and 5%-6% increase in final response quality of the overall agent system.

Conclusion: Decoupling training with tool-use rewards provides more direct optimization signal for planning capability, overcoming limitations of end-to-end approaches and significantly improving agent performance.

Abstract: The functionality of Large Language Model (LLM) agents is primarily
determined by two capabilities: action planning and answer summarization. The
former, action planning, is the core capability that dictates an agent's
performance. However, prevailing training paradigms employ end-to-end,
multi-objective optimization that jointly trains both capabilities. This
paradigm faces two critical challenges: imbalanced optimization objective
allocation and scarcity of verifiable data, making it difficult to enhance the
agent's planning capability. To address these challenges, we propose
Reinforcement Learning with Tool-use Rewards (RLTR), a novel framework that
decouples the training process to enable a focused, single-objective
optimization of the planning module. Crucially, RLTR introduces a reward signal
based on tool-use completeness to directly evaluate the quality of tool
invocation sequences. This method offers a more direct and reliable training
signal than assessing the final response content, thereby obviating the need
for verifiable data. Our experiments demonstrate that RLTR achieves an 8%-12%
improvement in planning performance compared to end-to-end baselines. Moreover,
this enhanced planning capability, in turn, translates to a 5%-6% increase in
the final response quality of the overall agent system.

</details>


### [204] [FinCast: A Foundation Model for Financial Time-Series Forecasting](https://arxiv.org/abs/2508.19609)
*Zhuohang Zhu,Haodong Chen,Qiang Qu,Vera Chung*

Main category: cs.LG

TL;DR: FinCast is the first foundation model for financial time-series forecasting that addresses pattern shifts from temporal non-stationarity, multi-domain diversity, and varying resolutions, achieving state-of-the-art zero-shot performance without domain-specific fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Financial time-series forecasting is crucial for economic stability and policymaking but remains challenging due to pattern shifts from temporal non-stationarity, multi-domain diversity, and varying temporal resolutions. Existing deep learning methods suffer from overfitting and require extensive domain-specific tuning.

Method: Introduces FinCast, a foundation model specifically designed for financial time-series forecasting, trained on large-scale financial datasets to capture diverse patterns across different domains and resolutions.

Result: FinCast exhibits robust zero-shot performance, effectively capturing diverse financial patterns without domain-specific fine-tuning, and surpasses existing state-of-the-art methods in comprehensive empirical and qualitative evaluations.

Conclusion: FinCast demonstrates strong generalization capabilities as the first foundation model for financial time-series forecasting, overcoming limitations of previous methods and showing superior performance across diverse financial domains and temporal resolutions.

Abstract: Financial time-series forecasting is critical for maintaining economic
stability, guiding informed policymaking, and promoting sustainable investment
practices. However, it remains challenging due to various underlying pattern
shifts. These shifts arise primarily from three sources: temporal
non-stationarity (distribution changes over time), multi-domain diversity
(distinct patterns across financial domains such as stocks, commodities, and
futures), and varying temporal resolutions (patterns differing across
per-second, hourly, daily, or weekly indicators). While recent deep learning
methods attempt to address these complexities, they frequently suffer from
overfitting and typically require extensive domain-specific fine-tuning. To
overcome these limitations, we introduce FinCast, the first foundation model
specifically designed for financial time-series forecasting, trained on
large-scale financial datasets. Remarkably, FinCast exhibits robust zero-shot
performance, effectively capturing diverse patterns without domain-specific
fine-tuning. Comprehensive empirical and qualitative evaluations demonstrate
that FinCast surpasses existing state-of-the-art methods, highlighting its
strong generalization capabilities.

</details>


### [205] [ALSA: Anchors in Logit Space for Out-of-Distribution Accuracy Estimation](https://arxiv.org/abs/2508.19613)
*Chenzhi Liu,Mahsa Baktashmotlagh,Yanran Tang,Zi Huang,Ruihong Qiu*

Main category: cs.LG

TL;DR: ALSA is a novel framework that estimates model accuracy on unseen datasets by operating directly in logit space using anchor-based modeling, outperforming existing softmax- and similarity-based methods.


<details>
  <summary>Details</summary>
Motivation: Existing accuracy estimation methods suffer from information loss (softmax-based) or computational expense/domain-specific limitations (similarity-based), especially under distribution shifts.

Method: ALSA uses multiple learnable anchors in logit space with influence functions to capture subtle logit variations, preserving richer information than probability-based approaches.

Result: Extensive experiments on vision, language, and graph benchmarks show ALSA's superiority over both softmax- and similarity-based baselines, with strong robustness under distribution shifts.

Conclusion: ALSA provides robust and accurate performance estimates across diverse distribution shifts, making it a practical tool for reliable model evaluation in real-world applications.

Abstract: Estimating model accuracy on unseen, unlabeled datasets is crucial for
real-world machine learning applications, especially under distribution shifts
that can degrade performance. Existing methods often rely on predicted class
probabilities (softmax scores) or data similarity metrics. While softmax-based
approaches benefit from representing predictions on the standard simplex,
compressing logits into probabilities leads to information loss. Meanwhile,
similarity-based methods can be computationally expensive and domain-specific,
limiting their broader applicability. In this paper, we introduce ALSA (Anchors
in Logit Space for Accuracy estimation), a novel framework that preserves
richer information by operating directly in the logit space. Building on
theoretical insights and empirical observations, we demonstrate that the
aggregation and distribution of logits exhibit a strong correlation with the
predictive performance of the model. To exploit this property, ALSA employs an
anchor-based modeling strategy: multiple learnable anchors are initialized in
logit space, each assigned an influence function that captures subtle
variations in the logits. This allows ALSA to provide robust and accurate
performance estimates across a wide range of distribution shifts. Extensive
experiments on vision, language, and graph benchmarks demonstrate ALSA's
superiority over both softmax- and similarity-based baselines. Notably, ALSA's
robustness under significant distribution shifts highlights its potential as a
practical tool for reliable model evaluation.

</details>


### [206] [Towards Instance-wise Personalized Federated Learning via Semi-Implicit Bayesian Prompt Tuning](https://arxiv.org/abs/2508.19621)
*Tiandi Ye,Wenyan Liu,Kai Yao,Lichun Li,Shangchao Su,Cen Chen,Xiang Li,Shan Yin,Ming Gao*

Main category: cs.LG

TL;DR: pFedBayesPT is a personalized federated learning framework that addresses intra-client data heterogeneity through Bayesian visual prompt tuning, outperforming existing methods on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Existing personalized federated learning methods assume single data distribution per client, but real-world clients often have data from multiple sources/domains, leading to intra-client heterogeneity and suboptimal performance.

Method: Proposes pFedBayesPT - a fine-grained instance-wise pFL framework using visual prompt tuning from Bayesian perspective, modeling prompt posterior as implicit distribution to capture diverse visual semantics with variational training under semi-implicit variational inference.

Result: Extensive experiments show pFedBayesPT consistently outperforms existing pFL methods under both feature and label heterogeneity settings on benchmark datasets.

Conclusion: The proposed Bayesian visual prompt tuning approach effectively addresses intra-client heterogeneity in federated learning, demonstrating superior performance compared to traditional client-level personalization methods.

Abstract: Federated learning (FL) is a privacy-preserving machine learning paradigm
that enables collaborative model training across multiple distributed clients
without disclosing their raw data. Personalized federated learning (pFL) has
gained increasing attention for its ability to address data heterogeneity.
However, most existing pFL methods assume that each client's data follows a
single distribution and learn one client-level personalized model for each
client. This assumption often fails in practice, where a single client may
possess data from multiple sources or domains, resulting in significant
intra-client heterogeneity and suboptimal performance. To tackle this
challenge, we propose pFedBayesPT, a fine-grained instance-wise pFL framework
based on visual prompt tuning. Specifically, we formulate instance-wise prompt
generation from a Bayesian perspective and model the prompt posterior as an
implicit distribution to capture diverse visual semantics. We derive a
variational training objective under the semi-implicit variational inference
framework. Extensive experiments on benchmark datasets demonstrate that
pFedBayesPT consistently outperforms existing pFL methods under both feature
and label heterogeneity settings.

</details>


### [207] [SCAR: A Characterization Scheme for Multi-Modal Dataset](https://arxiv.org/abs/2508.19659)
*Ri Su,Zhao Chen,Caleb Chen Cao,Nan Tang,Lei Chen*

Main category: cs.LG

TL;DR: SCAR introduces a principled framework to characterize dataset structural properties (Scale, Coverage, Authenticity, Richness) that remain invariant under scaling, enabling identification of foundation data subsets that preserve generalization without retraining, and guides modality-aware data expansion.


<details>
  <summary>Details</summary>
Motivation: Current data-centric methods focus on quantity and efficiency but lack theoretical insight into how data structural properties affect generalization, particularly in sample scaling scenarios across modalities.

Method: Developed SCAR framework with four structural measures, identifies foundation data subsets that maintain generalization behavior, models tasks as step functions to estimate generalization bias, and creates SCAR-guided data completion strategy for modality-aware expansion.

Result: Experiments across diverse multimodal datasets and model architectures validate SCAR's effectiveness in predicting data utility and guiding efficient data acquisition while preserving generalization capabilities.

Conclusion: SCAR provides a robust, general foundation for data understanding by capturing invariant structural properties, enabling efficient data optimization and modality-aware expansion while maintaining generalization performance across diverse tasks.

Abstract: Foundation models exhibit remarkable generalization across diverse tasks,
largely driven by the characteristics of their training data. Recent
data-centric methods like pruning and compression aim to optimize training but
offer limited theoretical insight into how data properties affect
generalization, especially the data characteristics in sample scaling.
Traditional perspectives further constrain progress by focusing predominantly
on data quantity and training efficiency, often overlooking structural aspects
of data quality. In this study, we introduce SCAR, a principled scheme for
characterizing the intrinsic structural properties of datasets across four key
measures: Scale, Coverage, Authenticity, and Richness. Unlike prior
data-centric measures, SCAR captures stable characteristics that remain
invariant under dataset scaling, providing a robust and general foundation for
data understanding. Leveraging these structural properties, we introduce
Foundation Data-a minimal subset that preserves the generalization behavior of
the full dataset without requiring model-specific retraining. We model
single-modality tasks as step functions and estimate the distribution of the
foundation data size to capture step-wise generalization bias across modalities
in the target multi-modal dataset. Finally, we develop a SCAR-guided data
completion strategy based on this generalization bias, which enables efficient,
modality-aware expansion of modality-specific characteristics in multimodal
datasets. Experiments across diverse multi-modal datasets and model
architectures validate the effectiveness of SCAR in predicting data utility and
guiding data acquisition. Code is available at https://github.com/McAloma/SCAR.

</details>


### [208] [Exploration of Low-Power Flexible Stress Monitoring Classifiers for Conformal Wearables](https://arxiv.org/abs/2508.19661)
*Florentia Afentaki,Sri Sai Rakesh Nakkilla,Konstantinos Balaskas,Paula Carolina Lozano Duarte,Shiyi Jiang,Georgios Zervakis,Farshad Firouzi,Krishnendu Chakrabarty,Mehdi B. Tahoori*

Main category: cs.LG

TL;DR: First comprehensive design space exploration of low-power flexible stress classifiers using machine learning, featuring over 1200 classifiers with optimized hardware efficiency through custom low-precision circuits.


<details>
  <summary>Details</summary>
Motivation: Conventional stress monitoring lacks continuous, accessible solutions. Rigid silicon wearables are not optimized for flexible wear, while flexible electronics face challenges implementing complex ML classifiers due to integration and power constraints.

Method: Comprehensive design space exploration covering various ML classifiers, feature selection, and neural simplification algorithms. Designed over 1200 flexible classifiers with fully customized circuits using low-precision arithmetic for hardware optimization.

Result: Developed real-time stress classifiers that achieve higher accuracy than current methods while maintaining low-cost, conformable design with low power consumption and compact size.

Conclusion: This work provides insights and a framework for designing efficient flexible stress monitoring systems that overcome limitations of conventional approaches and enable practical continuous stress detection.

Abstract: Conventional stress monitoring relies on episodic, symptom-focused
interventions, missing the need for continuous, accessible, and cost-efficient
solutions. State-of-the-art approaches use rigid, silicon-based wearables,
which, though capable of multitasking, are not optimized for lightweight,
flexible wear, limiting their practicality for continuous monitoring. In
contrast, flexible electronics (FE) offer flexibility and low manufacturing
costs, enabling real-time stress monitoring circuits. However, implementing
complex circuits like machine learning (ML) classifiers in FE is challenging
due to integration and power constraints. Previous research has explored
flexible biosensors and ADCs, but classifier design for stress detection
remains underexplored. This work presents the first comprehensive design space
exploration of low-power, flexible stress classifiers. We cover various ML
classifiers, feature selection, and neural simplification algorithms, with over
1200 flexible classifiers. To optimize hardware efficiency, fully customized
circuits with low-precision arithmetic are designed in each case. Our
exploration provides insights into designing real-time stress classifiers that
offer higher accuracy than current methods, while being low-cost, conformable,
and ensuring low power and compact size.

</details>


### [209] [$\mathcal{C}^1$-approximation with rational functions and rational neural networks](https://arxiv.org/abs/2508.19672)
*Erion Morina,Martin Holler*

Main category: cs.LG

TL;DR: The paper demonstrates that regular functions can be approximated in the C¹-norm using rational functions and rational neural networks, with approximation rates for network width/depth and rational degree. The results extend to EQL÷ and ParFam architectures for symbolic regression in physical law learning.


<details>
  <summary>Details</summary>
Motivation: To establish theoretical foundations for approximating functions using rational functions and rational neural networks, particularly for applications in symbolic regression and physical law learning where precise C¹-approximation is important.

Method: The authors develop approximation theory for rational functions and rational neural networks, analyzing approximation rates with respect to network width, depth, and the degree of rational functions. They extend these results to specific architectures like EQL÷ and ParFam.

Result: The paper shows that suitably regular functions can be effectively approximated in the C¹-norm using both rational functions and rational neural networks, with quantifiable approximation rates. The results are applicable to important architectures used in symbolic regression.

Conclusion: Rational functions and rational neural networks provide effective C¹-approximation capabilities for regular functions, with practical implications for symbolic regression tasks in physical law learning through architectures like EQL÷ and ParFam.

Abstract: We show that suitably regular functions can be approximated in the
$\mathcal{C}^1$-norm both with rational functions and rational neural networks,
including approximation rates with respect to width and depth of the network,
and degree of the rational functions. As consequence of our results, we further
obtain $\mathcal{C}^1$-approximation results for rational neural networks with
the $\text{EQL}^\div$ and ParFam architecture, both of which are important in
particular in the context of symbolic regression for physical law learning.

</details>


### [210] [Metric spaces of walks and Lipschitz duality on graphs](https://arxiv.org/abs/2508.19709)
*R. Arnau,A. González Cortés,E. A. Sánchez Pérez,S. Sanjuan*

Main category: cs.LG

TL;DR: This paper introduces a weighted metric framework for analyzing walks on graphs as Lipschitz sequences, enabling distance measurements between walks and developing proximity functions with representation formulas and applications in reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: To establish a metric structure for graph walks as Lipschitz sequences, enabling precise distance measurements between walks and facilitating the analysis of proximity functions for network analysis and machine learning applications.

Method: Introduces a weighted metric for sequences that combines stepwise vertex distances with weighted norms. Analyzes properties of these metric spaces, provides representation formulas for proximities under different assumptions, and develops explicit constructions for these cases.

Result: Develops a comprehensive metric framework for walks on graphs that allows classical metric modeling tools to be applied, including extension of Lipschitz functions from subspaces while preserving fundamental properties through the derived representations.

Conclusion: The proposed metric structure provides a robust foundation for measuring distances between graph walks and enables practical applications such as proximity estimation and reinforcement learning strategies based on exploratory walks, offering new approaches for Lipschitz regression on network structures.

Abstract: We study the metric structure of walks on graphs, understood as Lipschitz
sequences. To this end, a weighted metric is introduced to handle sequences,
enabling the definition of distances between walks based on stepwise vertex
distances and weighted norms. We analyze the main properties of these metric
spaces, which provides the foundation for the analysis of weaker forms of
instruments to measure relative distances between walks: proximities. We
provide some representation formulas for such proximities under different
assumptions and provide explicit constructions for these cases. The resulting
metric framework allows the use of classical tools from metric modeling, such
as the extension of Lipschitz functions from subspaces of walks, which permits
extending proximity functions while preserving fundamental properties via the
mentioned representations. Potential applications include the estimation of
proximities and the development of reinforcement learning strategies based on
exploratory walks, offering a robust approach to Lipschitz regression on
network structures.

</details>


### [211] [Tune My Adam, Please!](https://arxiv.org/abs/2508.19733)
*Theodoros Athanasiadis,Steven Adriaensen,Samuel Müller,Frank Hutter*

Main category: cs.LG

TL;DR: Adam-PFN is a pre-trained surrogate model for Freeze-thaw Bayesian Optimization that improves Adam hyperparameter tuning using learning curve data and CDF-augment method, achieving better performance on both in-distribution and out-of-distribution tasks.


<details>
  <summary>Details</summary>
Motivation: Adam optimizer is widely used but hyperparameter tuning is tedious and costly. Existing Freeze-thaw BO methods lack prior knowledge about how Adam's hyperparameters affect learning curves.

Method: Proposes Adam-PFN surrogate model pre-trained on learning curves from TaskSet, combined with CDF-augment learning curve augmentation method to increase training examples.

Result: Improves learning curve extrapolation and accelerates hyperparameter optimization on TaskSet evaluation tasks, with strong performance on out-of-distribution tasks.

Conclusion: The approach provides an effective solution for low-budget Adam hyperparameter tuning by incorporating domain-specific knowledge through pre-training and data augmentation.

Abstract: The Adam optimizer remains one of the most widely used optimizers in deep
learning, and effectively tuning its hyperparameters is key to optimizing
performance. However, tuning can be tedious and costly. Freeze-thaw Bayesian
Optimization (BO) is a recent promising approach for low-budget hyperparameter
tuning, but is limited by generic surrogates without prior knowledge of how
hyperparameters affect learning. We propose Adam-PFN, a new surrogate model for
Freeze-thaw BO of Adam's hyperparameters, pre-trained on learning curves from
TaskSet, together with a new learning curve augmentation method, CDF-augment,
which artificially increases the number of available training examples. Our
approach improves both learning curve extrapolation and accelerates
hyperparameter optimization on TaskSet evaluation tasks, with strong
performance on out-of-distribution (OOD) tasks.

</details>


### [212] [InfraredGP: Efficient Graph Partitioning via Spectral Graph Neural Networks with Negative Corrections](https://arxiv.org/abs/2508.19737)
*Meng Qin,Weihua Li,Jinqiang Cui,Sen Pei*

Main category: cs.LG

TL;DR: InfraredGP is a novel graph partitioning method that uses graph Laplacian with negative correction to access low-frequency information beyond conventional range [0,2], enabling high-quality community detection without training through one feed-forward propagation.


<details>
  <summary>Details</summary>
Motivation: To explore whether low-frequency information beyond the conventional range [0,2] in graph signal processing can encode more informative properties about community structures in graph partitioning.

Method: Uses spectral GNN backbone with low-pass filters and negative correction mechanism, feeds only random inputs, derives embeddings via one feed-forward propagation without training, and obtains results using BIRCH clustering.

Result: Achieves 16x-23x faster efficiency and competitive quality compared to various baselines for both static and streaming graph partitioning, with distinguishable embeddings for standard clustering modules.

Conclusion: The negative correction mechanism that amplifies low-frequency information beyond [0,2] enables effective graph partitioning without training, demonstrating superior efficiency and competitive quality in community detection tasks.

Abstract: Graph partitioning (GP), a.k.a. community detection, is a classic problem
that divides nodes of a graph into densely-connected blocks. From a perspective
of graph signal processing, we find that graph Laplacian with a negative
correction can derive graph frequencies beyond the conventional range $[0, 2]$.
To explore whether the low-frequency information beyond this range can encode
more informative properties about community structures, we propose InfraredGP.
It (\romannumeral1) adopts a spectral GNN as its backbone combined with
low-pass filters and a negative correction mechanism, (\romannumeral2) only
feeds random inputs to this backbone, (\romannumeral3) derives graph embeddings
via one feed-forward propagation (FFP) without any training, and
(\romannumeral4) obtains feasible GP results by feeding the derived embeddings
to BIRCH. Surprisingly, our experiments demonstrate that based solely on the
negative correction mechanism that amplifies low-frequency information beyond
$[0, 2]$, InfraredGP can derive distinguishable embeddings for some standard
clustering modules (e.g., BIRCH) and obtain high-quality results for GP without
any training. Following the IEEE HPEC Graph Challenge benchmark, we evaluate
InfraredGP for both static and streaming GP, where InfraredGP can achieve much
better efficiency (e.g., 16x-23x faster) and competitive quality over various
baselines. We have made our code public at
https://github.com/KuroginQin/InfraredGP

</details>


### [213] [Fast 3D Diffusion for Scalable Granular Media Synthesis](https://arxiv.org/abs/2508.19752)
*Muhammad Moeeze Hassan,Régis Cottereau,Filippo Gatti,Patryk Dec*

Main category: cs.LG

TL;DR: A novel 3D diffusion-based generative pipeline that synthesizes large granular assemblies in physically realistic configurations, bypassing computationally intensive DEM initialization phases.


<details>
  <summary>Details</summary>
Motivation: Discrete Element Method (DEM) simulations of granular media are computationally intensive, especially during initialization which dominates total simulation time due to large displacements and kinetic energy.

Method: Two-stage pipeline: 1) Diffusion model generates independent 3D voxel grids of granular media, 2) 3D inpainting model stitches grids together using masked inputs adapted from 2D techniques, with noise re-injection and weighted losses for coherence.

Result: Achieves linear scaling of computational time with sample size - generated a 1.2m ballasted rail track equivalent to 3-hour DEM simulation in under 20 seconds.

Conclusion: Enables physically coherent, real-time, scalable granular media synthesis for industrial applications, with DEM-compatible grain geometries through post-processing.

Abstract: Simulating granular media, using Discrete Element Method is a computationally
intensive task. This is especially true during initialization phase, which
dominates total simulation time because of large displacements involved and
associated kinetic energy. We overcome this bottleneck with a novel generative
pipeline based on 3D diffusion models that directly synthesizes arbitrarily
large granular assemblies in their final and physically realistic
configurations. The approach frames the problem as a 3D generative modeling
task, consisting of a two-stage pipeline. First a diffusion model is trained to
generate independent 3D voxel grids representing granular media. Second, a 3D
inpainting model, adapted from 2D inpainting techniques using masked inputs,
stitches these grids together seamlessly, enabling synthesis of large samples
with physically realistic structure. The inpainting model explores several
masking strategies for the inputs to the underlying UNets by training the
network to infer missing portions of voxel grids from a concatenation of noised
tensors, masks, and masked tensors as input channels. The model also adapts a
2D repainting technique of re-injecting noise scheduler output with ground
truth to provide a strong guidance to the 3D model. This along with weighted
losses ensures long-term coherence over generation of masked regions. Both
models are trained on the same binarized 3D occupancy grids extracted from
small-scale DEM simulations, achieving linear scaling of computational time
with respect to sample size. Quantitatively, a 1.2 m long ballasted rail track
synthesis equivalent to a 3-hour DEM simulation, was completed under 20
seconds. The generated voxel grids can also be post-processed to extract grain
geometries for DEM-compatibility as well, enabling physically coherent,
real-time, scalable granular media synthesis for industrial applications.

</details>


### [214] [Interestingness First Classifiers](https://arxiv.org/abs/2508.19780)
*Ryoma Sato*

Main category: cs.LG

TL;DR: EUREKA framework builds interesting classifiers using unexpected features instead of maximizing accuracy, leveraging LLMs to select non-obvious yet predictive features for interpretable models.


<details>
  <summary>Details</summary>
Motivation: Traditional ML focuses on maximizing predictive accuracy, but this work explores building classifiers that are interesting and use unexpected features, even at the cost of some accuracy, to support novel knowledge discovery.

Method: EUREKA uses large language models to rank features by perceived interestingness, then builds interpretable classifiers using only the selected interesting features.

Result: Across benchmark datasets, EUREKA consistently identifies non-obvious yet predictive features (e.g., humidity over CO2 for room congestion, colon in titles for paper citations) while maintaining meaningful accuracy.

Conclusion: Interesting classifiers with moderate accuracy but high novelty and interpretability can enable new forms of knowledge discovery and communication in appropriate settings.

Abstract: Most machine learning models are designed to maximize predictive accuracy. In
this work, we explore a different goal: building classifiers that are
interesting. An ``interesting classifier'' is one that uses unusual or
unexpected features, even if its accuracy is lower than the best possible
model. For example, predicting room congestion from CO2 levels achieves
near-perfect accuracy but is unsurprising. In contrast, predicting room
congestion from humidity is less accurate yet more nuanced and intriguing. We
introduce EUREKA, a simple framework that selects features according to their
perceived interestingness. Our method leverages large language models to rank
features by their interestingness and then builds interpretable classifiers
using only the selected interesting features. Across several benchmark
datasets, EUREKA consistently identifies features that are non-obvious yet
still predictive. For example, in the Occupancy Detection dataset, our method
favors humidity over CO2 levels and light intensity, producing classifiers that
achieve meaningful accuracy while offering insights. In the Twin Papers
dataset, our method discovers the rule that papers with a colon in the title
are more likely to be cited in the future. We argue that such models can
support new ways of knowledge discovery and communication, especially in
settings where moderate accuracy is sufficient but novelty and interpretability
are valued.

</details>


### [215] [PSO-Merging: Merging Models Based on Particle Swarm Optimization](https://arxiv.org/abs/2508.19839)
*Kehao Zhang,Shaolei Zhang,Yang Feng*

Main category: cs.LG

TL;DR: PSO-Merging is a novel data-driven model merging method using Particle Swarm Optimization that outperforms existing approaches by combining pre-trained models, expert models, and sparsified experts through iterative optimization.


<details>
  <summary>Details</summary>
Motivation: Existing model merging methods face limitations: data-independent methods lack performance, gradient-based methods are computationally expensive for large models, and gradient-free methods struggle to achieve good results quickly.

Method: Uses Particle Swarm Optimization (PSO) to merge models - initializes particle swarm with pre-trained model, expert models, and sparsified expert models, then performs multiple iterations with the final global best particle becoming the merged model.

Result: Experimental results show PSO-Merging generally outperforms baseline merging methods across different language models.

Conclusion: PSO-Merging provides a more efficient and scalable solution for model merging that addresses the limitations of existing approaches.

Abstract: Model merging has emerged as an efficient strategy for constructing multitask
models by integrating the strengths of multiple available expert models,
thereby reducing the need to fine-tune a pre-trained model for all the tasks
from scratch. Existing data-independent methods struggle with performance
limitations due to the lack of data-driven guidance. Data-driven approaches
also face key challenges: gradient-based methods are computationally expensive,
limiting their practicality for merging large expert models, whereas existing
gradient-free methods often fail to achieve satisfactory results within a
limited number of optimization steps. To address these limitations, this paper
introduces PSO-Merging, a novel data-driven merging method based on the
Particle Swarm Optimization (PSO). In this approach, we initialize the particle
swarm with a pre-trained model, expert models, and sparsified expert models. We
then perform multiple iterations, with the final global best particle serving
as the merged model. Experimental results on different language models show
that PSO-Merging generally outperforms baseline merging methods, offering a
more efficient and scalable solution for model merging.

</details>


### [216] [Symplectic convolutional neural networks](https://arxiv.org/abs/2508.19842)
*Süleyman Yıldız,Konrad Janik,Peter Benner*

Main category: cs.LG

TL;DR: A new symplectic CNN architecture that preserves symplectic structure through mathematical reformulation of convolution layers and introduces symplectic pooling for autoencoders, showing superior performance on PDE examples compared to linear symplectic autoencoders.


<details>
  <summary>Details</summary>
Motivation: To develop convolutional neural networks that preserve symplectic structure, which is important for accurately simulating Hamiltonian systems and physical phenomena while maintaining numerical stability and structure-preserving properties.

Method: Leverages symplectic neural networks, proper symplectic decomposition, and tensor techniques to mathematically reformulate convolution layers to remain symplectic. Introduces a symplectic pooling layer to construct complete autoencoders.

Result: The symplectic CNN outperforms linear symplectic autoencoders obtained via proper symplectic decomposition across three test cases: wave equation, nonlinear Schrödinger equation, and sine-Gordon equation.

Conclusion: The proposed symplectic CNN architecture successfully preserves symplectic structure while providing improved performance over existing linear symplectic methods, making it suitable for Hamiltonian system simulations and physical modeling applications.

Abstract: We propose a new symplectic convolutional neural network (CNN) architecture
by leveraging symplectic neural networks, proper symplectic decomposition, and
tensor techniques. Specifically, we first introduce a mathematically equivalent
form of the convolution layer and then, using symplectic neural networks, we
demonstrate a way to parameterize the layers of the CNN to ensure that the
convolution layer remains symplectic. To construct a complete autoencoder, we
introduce a symplectic pooling layer. We demonstrate the performance of the
proposed neural network on three examples: the wave equation, the nonlinear
Schr\"odinger (NLS) equation, and the sine-Gordon equation. The numerical
results indicate that the symplectic CNN outperforms the linear symplectic
autoencoder obtained via proper symplectic decomposition.

</details>


### [217] [Physics-Informed DeepONet Coupled with FEM for Convective Transport in Porous Media with Sharp Gaussian Sources](https://arxiv.org/abs/2508.19847)
*Erdi Kara,Panos Stinis*

Main category: cs.LG

TL;DR: Hybrid FEM-DeepONet framework for porous media transport modeling that combines FEM accuracy for flow with DeepONet speed for transport, featuring adaptive sampling for sharp sources.


<details>
  <summary>Details</summary>
Motivation: To efficiently model fluid transport in porous media from localized sources while maintaining accuracy in flow fields and enabling fast inference for transport dynamics.

Method: Couples finite element methods (FEM) for solving steady-state Darcy flow with physics-informed DeepONet for learning the mapping from source functions to concentration profiles, using adaptive sampling for trunk collocation points to handle steep gradients.

Result: Numerical experiments show good agreement with reference solutions while achieving orders of magnitude speedups over traditional solvers.

Conclusion: The hybrid framework is suitable for practical applications in porous media transport modeling, offering both accuracy and computational efficiency.

Abstract: We present a hybrid framework that couples finite element methods (FEM) with
physics-informed DeepONet to model fluid transport in porous media from sharp,
localized Gaussian sources. The governing system consists of a steady-state
Darcy flow equation and a time-dependent convection-diffusion equation. Our
approach solves the Darcy system using FEM and transfers the resulting velocity
field to a physics-informed DeepONet, which learns the mapping from source
functions to solute concentration profiles. This modular strategy preserves
FEM-level accuracy in the flow field while enabling fast inference for
transport dynamics. To handle steep gradients induced by sharp sources, we
introduce an adaptive sampling strategy for trunk collocation points. Numerical
experiments demonstrate that our method is in good agreement with the reference
solutions while offering orders of magnitude speedups over traditional solvers,
making it suitable for practical applications in relevant scenarios.
Implementation of our proposed method is available at
https://github.com/erkara/fem-pi-deeponet.

</details>


### [218] [Quantum latent distributions in deep generative models](https://arxiv.org/abs/2508.19857)
*Omar Bacarreza,Thorin Farnsworth,Alexander Makarovskiy,Hugo Wallner,Tessa Hicks,Santiago Sempere-Llagostera,John Price,Robert J. A. Francis-Jones,William R. Clements*

Main category: cs.LG

TL;DR: Quantum latent distributions from quantum processors can improve generative model performance compared to classical latent distributions, with proven advantages under certain conditions and demonstrated improvements in GANs on synthetic and molecular datasets.


<details>
  <summary>Details</summary>
Motivation: While simple latent distributions are commonly used in generative models, more sophisticated distributions like those from quantum processors have shown empirical improvements. However, it remains unclear when and why quantum latent distributions provide advantages over classical ones.

Method: The authors prove theoretical conditions where quantum latent distributions enable capabilities that classical distributions cannot efficiently produce. They perform benchmarking experiments on synthetic quantum datasets and QM9 molecular dataset using both simulated and real photonic quantum processors, testing GANs, diffusion models, and flow matching models.

Result: Experimental results demonstrate that quantum latent distributions lead to improved generative performance in GANs compared to classical baselines. The work also identifies architectures compatible with quantum latent distributions for diffusion and flow matching models.

Conclusion: This work confirms that near-term quantum processors can expand the capabilities of deep generative models by providing quantum latent distributions that offer advantages over classical approaches in certain conditions.

Abstract: Many successful families of generative models leverage a low-dimensional
latent distribution that is mapped to a data distribution. Though simple latent
distributions are commonly used, it has been shown that more sophisticated
distributions can improve performance. For instance, recent work has explored
using the distributions produced by quantum processors and found empirical
improvements. However, when latent space distributions produced by quantum
processors can be expected to improve performance, and whether these
improvements are reproducible, are open questions that we investigate in this
work. We prove that, under certain conditions, these "quantum latent
distributions" enable generative models to produce data distributions that
classical latent distributions cannot efficiently produce. We also provide
actionable intuitions to identify when such quantum advantages may arise in
real-world settings. We perform benchmarking experiments on both a synthetic
quantum dataset and the QM9 molecular dataset, using both simulated and real
photonic quantum processors. Our results demonstrate that quantum latent
distributions can lead to improved generative performance in GANs compared to a
range of classical baselines. We also explore diffusion and flow matching
models, identifying architectures compatible with quantum latent distributions.
This work confirms that near-term quantum processors can expand the
capabilities of deep generative models.

</details>


### [219] [Parameter-Free Structural-Diversity Message Passing for Graph Neural Networks](https://arxiv.org/abs/2508.19884)
*Mingyue Kong,Yinglong Zhang,Chengda Xu,Xuewen Xia,Xing Xu*

Main category: cs.LG

TL;DR: SDGNN is a parameter-free graph neural network framework that uses structural diversity theory to address over-smoothing and semantic degradation in traditional GNNs, achieving superior performance across various challenging scenarios without trainable parameters.


<details>
  <summary>Details</summary>
Motivation: Traditional GNNs rely on many trainable parameters and fixed aggregation rules, struggling with graph data that has strong structural heterogeneity and complex feature distributions, leading to over-smoothing and semantic degradation.

Method: Proposes SDGNN framework with a unified structural-diversity message passing mechanism that captures neighborhood structure heterogeneity and feature semantic stability without additional trainable parameters, using complementary structure-driven and feature-driven modeling.

Result: Outperforms mainstream GNNs on eight public benchmark datasets and PubMed citation network under challenging conditions including low supervision, class imbalance, and cross-domain transfer.

Conclusion: Provides a new theoretical perspective for parameter-free GNN design and validates structural diversity as a core signal in graph representation learning, with implementation released for reproducibility.

Abstract: Graph Neural Networks (GNNs) have shown remarkable performance in structured
data modeling tasks such as node classification. However, mainstream approaches
generally rely on a large number of trainable parameters and fixed aggregation
rules, making it difficult to adapt to graph data with strong structural
heterogeneity and complex feature distributions. This often leads to
over-smoothing of node representations and semantic degradation. To address
these issues, this paper proposes a parameter-free graph neural network
framework based on structural diversity, namely SDGNN (Structural-Diversity
Graph Neural Network). The framework is inspired by structural diversity theory
and designs a unified structural-diversity message passing mechanism that
simultaneously captures the heterogeneity of neighborhood structures and the
stability of feature semantics, without introducing additional trainable
parameters. Unlike traditional parameterized methods, SDGNN does not rely on
complex model training, but instead leverages complementary modeling from both
structure-driven and feature-driven perspectives, thereby effectively improving
adaptability across datasets and scenarios. Experimental results show that on
eight public benchmark datasets and an interdisciplinary PubMed citation
network, SDGNN consistently outperforms mainstream GNNs under challenging
conditions such as low supervision, class imbalance, and cross-domain transfer.
This work provides a new theoretical perspective and general approach for the
design of parameter-free graph neural networks, and further validates the
importance of structural diversity as a core signal in graph representation
learning. To facilitate reproducibility and further research, the full
implementation of SDGNN has been released at:
https://github.com/mingyue15694/SGDNN/tree/main

</details>


### [220] [NM-Hebb: Coupling Local Hebbian Plasticity with Metric Learning for More Accurate and Interpretable CNNs](https://arxiv.org/abs/2508.19896)
*Davorin Miličević,Ratko Grbić*

Main category: cs.LG

TL;DR: NM-Hebb is a two-phase CNN training framework that combines neuro-inspired local plasticity with metric learning to improve accuracy, reduce overfitting, and enhance interpretability across multiple datasets and architectures.


<details>
  <summary>Details</summary>
Motivation: Standard CNNs rely on global gradient-based optimization which can lead to overfitting, redundant filters, and reduced interpretability. The authors aim to address these limitations by incorporating biologically inspired mechanisms.

Method: Two-phase approach: Phase 1 combines cross-entropy loss with Hebbian regularization and neuromodulator-gated consolidation loss. Phase 2 uses pairwise metric learning to compress intra-class distances and expand inter-class margins in embedding space.

Result: Significant accuracy improvements across CIFAR-10 (+2.0-10.0 pp), CIFAR-100 (+2.0-9.0 pp), and TinyImageNet (+4.3-8.9 pp) with increased Normalised Mutual Information (+0.15). Produces more structured features and tighter class clusters.

Conclusion: Combining local Hebbian plasticity with metric-based fine-tuning creates CNNs that are more accurate and interpretable, offering practical benefits for resource-constrained and safety-critical AI applications.

Abstract: Deep Convolutional Neural Networks (CNNs) achieve high accuracy but often
rely on purely global, gradient-based optimisation, which can lead to
overfitting, redundant filters, and reduced interpretability. To address these
limitations, we propose NM-Hebb, a two-phase training framework that integrates
neuro-inspired local plasticity with distance-aware supervision. Phase 1
extends standard supervised training by jointly optimising a cross-entropy
objective with two biologically inspired mechanisms: (i) a Hebbian regulariser
that aligns the spatial mean of activations with the mean of the corresponding
convolutional filter weights, encouraging structured, reusable primitives; and
(ii) a learnable neuromodulator that gates an elastic-weight-style
consolidation loss, preserving beneficial parameters without freezing the
network. Phase 2 fine-tunes the backbone with a pairwise metric-learning loss,
explicitly compressing intra-class distances and enlarging inter-class margins
in the embedding space. Evaluated on CIFAR-10, CIFAR-100, and TinyImageNet
across five backbones (ResNet-18, VGG-11, MobileNet-v2, EfficientNet-V2,
DenseNet-121), NM-Hebb achieves consistent gains over baseline and other
methods: Top-1 accuracy improves by +2.0-10.0 pp (CIFAR-10), +2.0-9.0 pp
(CIFAR-100), and up to +4.3-8.9 pp (TinyImageNet), with Normalised Mutual
Information (NMI) increased by up to +0.15. Qualitative visualisations and
filter-level analyses further confirm that NM-Hebb produces more structured and
selective features, yielding tighter and more interpretable class clusters.
Overall, coupling local Hebbian plasticity with metric-based fine-tuning yields
CNNs that are not only more accurate but also more interpretable, offering
practical benefits for resource-constrained and safety-critical AI deployments.

</details>


### [221] [Adaptive Scaling of Policy Constraints for Offline Reinforcement Learning](https://arxiv.org/abs/2508.19900)
*Tan Jing,Xiaorui Li,Chao Yao,Xiaojuan Ban,Yuetong Fang,Renjing Xu,Zhaolin Yuan*

Main category: cs.LG

TL;DR: ASPC is a second-order differentiable framework that dynamically balances RL and behavior cloning constraints in offline RL, eliminating the need for per-dataset hyperparameter tuning while achieving state-of-the-art performance across multiple domains.


<details>
  <summary>Details</summary>
Motivation: Existing offline RL methods require meticulous hyperparameter tuning for each dataset due to varying constraint scales across tasks and datasets, making them time-consuming and impractical for real-world applications.

Method: Proposes Adaptive Scaling of Policy Constraints (ASPC), a second-order differentiable framework that dynamically balances RL and behavior cloning during training without requiring per-dataset hyperparameter tuning.

Result: ASPC outperforms other adaptive constraint methods and state-of-the-art offline RL algorithms across 39 datasets in four D4RL domains using a single hyperparameter configuration, with minimal computational overhead.

Conclusion: ASPC provides an effective solution to the hyperparameter sensitivity problem in offline RL, offering consistent performance across diverse datasets without the need for extensive tuning, making offline RL more practical and accessible.

Abstract: Offline reinforcement learning (RL) enables learning effective policies from
fixed datasets without any environment interaction. Existing methods typically
employ policy constraints to mitigate the distribution shift encountered during
offline RL training. However, because the scale of the constraints varies
across tasks and datasets of differing quality, existing methods must
meticulously tune hyperparameters to match each dataset, which is
time-consuming and often impractical. We propose Adaptive Scaling of Policy
Constraints (ASPC), a second-order differentiable framework that dynamically
balances RL and behavior cloning (BC) during training. We theoretically analyze
its performance improvement guarantee. In experiments on 39 datasets across
four D4RL domains, ASPC using a single hyperparameter configuration outperforms
other adaptive constraint methods and state-of-the-art offline RL algorithms
that require per-dataset tuning while incurring only minimal computational
overhead. The code will be released at https://github.com/Colin-Jing/ASPC.

</details>


### [222] [GegenNet: Spectral Convolutional Neural Networks for Link Sign Prediction in Signed Bipartite Graphs](https://arxiv.org/abs/2508.19907)
*Hewen Wang,Renchi Yang,Xiaokui Xiao*

Main category: cs.LG

TL;DR: GegenNet is a novel spectral convolutional neural network for link sign prediction in signed bipartite graphs, achieving superior performance through Gegenbauer polynomial filters and sign-aware spectral convolutions.


<details>
  <summary>Details</summary>
Motivation: Existing link sign prediction methods focus on unipartite graphs and are suboptimal for signed bipartite graphs due to neglect of node heterogeneity and unique bipartite characteristics. Current GNN adaptations use spectral convolutional operators designed for positive links in unsigned graphs, which are not optimal for inferring missing positive/negative links.

Method: Proposes GegenNet with three main contributions: (1) fast spectral decomposition for node feature initialization, (2) new spectral graph filter based on Gegenbauer polynomial basis, and (3) multi-layer sign-aware spectral convolutional networks alternating Gegenbauer polynomial filters with positive and negative edges.

Result: Achieves significantly superior performance with gains of up to 4.28% in AUC and 11.69% in F1 compared to 11 strong competitors over 6 benchmark signed bipartite graph datasets.

Conclusion: GegenNet provides an effective spectral convolutional neural network model that successfully addresses the unique challenges of link sign prediction in signed bipartite graphs through novel spectral filtering techniques.

Abstract: Given a signed bipartite graph (SBG) G with two disjoint node sets U and V,
the goal of link sign prediction is to predict the signs of potential links
connecting U and V based on known positive and negative edges in G. The
majority of existing solutions towards link sign prediction mainly focus on
unipartite signed graphs, which are sub-optimal due to the neglect of node
heterogeneity and unique bipartite characteristics of SBGs. To this end, recent
studies adapt graph neural networks to SBGs by introducing message-passing
schemes for both inter-partition (UxV) and intra-partition (UxU or VxV) node
pairs. However, the fundamental spectral convolutional operators were
originally designed for positive links in unsigned graphs, and thus, are not
optimal for inferring missing positive or negative links from known ones in
SBGs.
  Motivated by this, this paper proposes GegenNet, a novel and effective
spectral convolutional neural network model for link sign prediction in SBGs.
In particular, GegenNet achieves enhanced model capacity and high predictive
accuracy through three main technical contributions: (i) fast and theoretically
grounded spectral decomposition techniques for node feature initialization;
(ii) a new spectral graph filter based on the Gegenbauer polynomial basis; and
(iii) multi-layer sign-aware spectral convolutional networks alternating
Gegenbauer polynomial filters with positive and negative edges. Our extensive
empirical studies reveal that GegenNet can achieve significantly superior
performance (up to a gain of 4.28% in AUC and 11.69% in F1) in link sign
prediction compared to 11 strong competitors over 6 benchmark SBG datasets.

</details>


### [223] [Ontology-Based Concept Distillation for Radiology Report Retrieval and Labeling](https://arxiv.org/abs/2508.19915)
*Felix Nützel,Mischa Dombrowski,Bernhard Kainz*

Main category: cs.LG

TL;DR: Ontology-driven radiology report comparison using UMLS concepts outperforms embedding-based methods for rare disease detection in chest X-rays, providing more interpretable and clinically meaningful retrieval.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on high-dimensional text embeddings that are difficult to interpret, computationally expensive, and not well-aligned with structured medical knowledge. There's a need for more transparent and clinically grounded approaches for radiology report comparison.

Method: Extracts standardized medical entities from free-text reports using RadGraph-XL and SapBERT, links them to UMLS concepts (CUIs), and uses a modified weighted Tversky Index that accounts for synonymy, negation, and hierarchical relationships for similarity comparison.

Result: Outperforms state-of-the-art embedding-based retrieval methods in radiograph classification on MIMIC-CXR, particularly in long-tail settings. Also generates ontology-backed disease labels for MIMIC-CXR.

Conclusion: Provides more explainable, reliable, and task-specific retrieval strategies for clinical AI systems, especially when interpretability and domain knowledge integration are essential.

Abstract: Retrieval-augmented learning based on radiology reports has emerged as a
promising direction to improve performance on long-tail medical imaging tasks,
such as rare disease detection in chest X-rays. Most existing methods rely on
comparing high-dimensional text embeddings from models like CLIP or CXR-BERT,
which are often difficult to interpret, computationally expensive, and not
well-aligned with the structured nature of medical knowledge. We propose a
novel, ontology-driven alternative for comparing radiology report texts based
on clinically grounded concepts from the Unified Medical Language System
(UMLS). Our method extracts standardised medical entities from free-text
reports using an enhanced pipeline built on RadGraph-XL and SapBERT. These
entities are linked to UMLS concepts (CUIs), enabling a transparent,
interpretable set-based representation of each report. We then define a
task-adaptive similarity measure based on a modified and weighted version of
the Tversky Index that accounts for synonymy, negation, and hierarchical
relationships between medical entities. This allows efficient and semantically
meaningful similarity comparisons between reports. We demonstrate that our
approach outperforms state-of-the-art embedding-based retrieval methods in a
radiograph classification task on MIMIC-CXR, particularly in long-tail
settings. Additionally, we use our pipeline to generate ontology-backed disease
labels for MIMIC-CXR, offering a valuable new resource for downstream learning
tasks. Our work provides more explainable, reliable, and task-specific
retrieval strategies in clinical AI systems, especially when interpretability
and domain knowledge integration are essential. Our code is available at
https://github.com/Felix-012/ontology-concept-distillation

</details>


### [224] [FlowletFormer: Network Behavioral Semantic Aware Pre-training Model for Traffic Classification](https://arxiv.org/abs/2508.19924)
*Liming Liu,Ruoyu Li,Qing Li,Meijia Hou,Yong Jiang,Mingwei Xu*

Main category: cs.LG

TL;DR: FlowletFormer is a BERT-based pre-training model that improves network traffic classification by better capturing packet structures, flow behaviors, and protocol semantics through specialized traffic representation, embedding layers, and pretraining tasks.


<details>
  <summary>Details</summary>
Motivation: Existing network traffic classification methods using pre-training models fail to adequately capture packet structural characteristics, flow-level behaviors, hierarchical protocol semantics, and inter-packet contextual relationships, limiting their effectiveness.

Method: Proposes FlowletFormer with three key components: 1) Coherent Behavior-Aware Traffic Representation Model for semantic traffic segmentation, 2) Protocol Stack Alignment-Based Embedding Layer for multilayer protocol semantics, and 3) Field-Specific and Context-Aware Pretraining Tasks for enhanced inter-packet and inter-flow learning.

Result: FlowletFormer significantly outperforms existing methods in traffic representation effectiveness, classification accuracy, and few-shot learning capability. It also demonstrates better comprehension of network transmission principles like TCP stateful connections.

Conclusion: FlowletFormer provides a more robust and trustworthy framework for traffic analysis by effectively integrating domain-specific network knowledge and addressing the limitations of existing pre-training approaches.

Abstract: Network traffic classification using pre-training models has shown promising
results, but existing methods struggle to capture packet structural
characteristics, flow-level behaviors, hierarchical protocol semantics, and
inter-packet contextual relationships. To address these challenges, we propose
FlowletFormer, a BERT-based pre-training model specifically designed for
network traffic analysis. FlowletFormer introduces a Coherent Behavior-Aware
Traffic Representation Model for segmenting traffic into semantically
meaningful units, a Protocol Stack Alignment-Based Embedding Layer to capture
multilayer protocol semantics, and Field-Specific and Context-Aware Pretraining
Tasks to enhance both inter-packet and inter-flow learning. Experimental
results demonstrate that FlowletFormer significantly outperforms existing
methods in the effectiveness of traffic representation, classification
accuracy, and few-shot learning capability. Moreover, by effectively
integrating domain-specific network knowledge, FlowletFormer shows better
comprehension of the principles of network transmission (e.g., stateful
connections of TCP), providing a more robust and trustworthy framework for
traffic analysis.

</details>


### [225] [Constraint Learning in Multi-Agent Dynamic Games from Demonstrations of Local Nash Interactions](https://arxiv.org/abs/2508.19945)
*Zhouyu Zhang,Chih-Yuan Chiu,Glen Chou*

Main category: cs.LG

TL;DR: Inverse dynamic game algorithm learns parametric constraints from multi-agent Nash equilibrium interactions using MILP encoding of KKT conditions, with theoretical guarantees for safe/unsafe set approximations and robust motion planning applications.


<details>
  <summary>Details</summary>
Motivation: To learn underlying constraints from observed multi-agent interactions by analyzing Nash equilibrium demonstrations, enabling constraint inference and robust motion planning.

Method: Mixed-integer linear programs (MILP) encoding Karush-Kuhn-Tucker (KKT) conditions of interacting agents to recover constraints consistent with Nash stationarity of interaction demonstrations.

Result: The method learns inner approximations of true safe and unsafe sets, works for both convex and non-convex constraints, and enables robust motion planning that satisfies underlying constraints.

Conclusion: The approach successfully infers constraints from Nash equilibrium interactions across simulations and hardware experiments, handling nonlinear dynamics and various constraint classes for interactive motion planning.

Abstract: We present an inverse dynamic game-based algorithm to learn parametric
constraints from a given dataset of local generalized Nash equilibrium
interactions between multiple agents. Specifically, we introduce mixed-integer
linear programs (MILP) encoding the Karush-Kuhn-Tucker (KKT) conditions of the
interacting agents, which recover constraints consistent with the Nash
stationarity of the interaction demonstrations. We establish theoretical
guarantees that our method learns inner approximations of the true safe and
unsafe sets, as well as limitations of constraint learnability from
demonstrations of Nash equilibrium interactions. We also use the interaction
constraints recovered by our method to design motion plans that robustly
satisfy the underlying constraints. Across simulations and hardware
experiments, our methods proved capable of inferring constraints and designing
interactive motion plans for various classes of constraints, both convex and
non-convex, from interaction demonstrations of agents with nonlinear dynamics.

</details>


### [226] [Global Permutation Entropy](https://arxiv.org/abs/2508.19955)
*Abhijeet Avhale,Joscha Diehl,Niraj Velankar,Emanuele Verri*

Main category: cs.LG

TL;DR: Global Permutation Entropy (GPE) extends traditional permutation entropy by considering all possible patterns of a given length, including non-consecutive ones, using efficient algorithms to extract full permutation profiles.


<details>
  <summary>Details</summary>
Motivation: Standard permutation entropy only considers consecutive segments, limiting its ability to capture complex structural information in time series. The authors aim to develop a more comprehensive complexity measure that reveals additional structural patterns.

Method: The method involves developing algorithms to efficiently extract full permutation profiles from time series, considering all possible patterns of a given length (including non-consecutive ones), and applying Shannon entropy to quantify complexity.

Result: Experiments on synthetic datasets show that GPE reveals structural information not accessible through standard permutation entropy, demonstrating its effectiveness as a complexity measure.

Conclusion: GPE provides a more comprehensive approach to measuring time series complexity by considering all possible patterns, offering new insights into structural properties that traditional permutation entropy cannot capture.

Abstract: Permutation Entropy, introduced by Bandt and Pompe, is a widely used
complexity measure for real-valued time series that is based on the relative
order of values within consecutive segments of fixed length. After
standardizing each segment to a permutation and computing the frequency
distribution of these permutations, Shannon Entropy is then applied to quantify
the series' complexity. We introduce Global Permutation Entropy (GPE), a novel
index that considers all possible patterns of a given length, including
non-consecutive ones. Its computation relies on recently developed algorithms
that enable the efficient extraction of full permutation profiles. We
illustrate some properties of GPE and demonstrate its effectiveness through
experiments on synthetic datasets, showing that it reveals structural
information not accessible through standard permutation entropy. We provide a
Julia package for the calculation of GPE at
`https://github.com/AThreeH1/Global-Permutation-Entropy'.

</details>


### [227] [Short-Horizon Predictive Maintenance of Industrial Pumps Using Time-Series Features and Machine Learning](https://arxiv.org/abs/2508.19974)
*Khaled M. A. Alghtus,Aiyad Gannan,Khalid M. Alhajri,Ali L. A. Al Jubouri,Hassan A. I. Al-Janahi*

Main category: cs.LG

TL;DR: Machine learning framework for short-term fault prediction in industrial pumps using sensor data, with Random Forest achieving best performance (up to 69.2% recall) using 60-120 minute historical windows.


<details>
  <summary>Details</summary>
Motivation: To enable predictive maintenance by forecasting early warning conditions in centrifugal pumps 5-30 minutes in advance using real-time sensor data patterns.

Method: Used sliding window approach with 60/120-minute lookback periods, extracted statistical features (mean, std, min, max, trend), applied SMOTE for class imbalance, and trained Random Forest and XGBoost classifiers.

Result: Random Forest with 60-minute window achieved best performance: 69.2% recall at 5min, 64.9% at 15min, 48.6% at 30min. 120-minute window improved longer-term predictions (65.6% at 15/30min). XGBoost performed slightly worse.

Conclusion: Optimal history length depends on prediction horizon, different fault patterns evolve at different timescales. The method provides interpretable and scalable solution for real-time industrial predictive maintenance.

Abstract: This study presents a machine learning framework for forecasting short-term
faults in industrial centrifugal pumps using real-time sensor data. The
approach aims to predict {EarlyWarning} conditions 5, 15, and 30 minutes in
advance based on patterns extracted from historical operation. Two lookback
periods, 60 minutes and 120 minutes, were evaluated using a sliding window
approach. For each window, statistical features including mean, standard
deviation, minimum, maximum, and linear trend were extracted, and class
imbalance was addressed using the SMOTE algorithm. Random Forest and XGBoost
classifiers were trained and tested on the labeled dataset. Results show that
the Random Forest model achieved the best short-term forecasting performance
with a 60-minute window, reaching recall scores of 69.2\% at 5 minutes, 64.9\%
at 15 minutes, and 48.6\% at 30 minutes. With a 120-minute window, the Random
Forest model achieved 57.6\% recall at 5 minutes, and improved predictive
accuracy of 65.6\% at both 15 and 30 minutes. XGBoost displayed similar but
slightly lower performance. These findings highlight that optimal history
length depends on the prediction horizon, and that different fault patterns may
evolve at different timescales. The proposed method offers an interpretable and
scalable solution for integrating predictive maintenance into real-time
industrial monitoring systems.

</details>


### [228] [Reducing Street Parking Search Time via Smart Assignment Strategies](https://arxiv.org/abs/2508.19979)
*Behafarid Hemmatpour,Javad Dogani,Nikolaos Laoutaris*

Main category: cs.LG

TL;DR: A novel coordinated parking strategy (Cord-Approx) using probabilistic estimation and Hungarian matching reduces parking search time by up to 73% compared to non-users in Madrid simulations.


<details>
  <summary>Details</summary>
Motivation: Parking search in dense metropolitan areas contributes significantly to traffic congestion, but the effectiveness of real-time mobile parking assistants remains understudied.

Method: Developed four strategies: uncoordinated search, coordinated without non-user awareness, idealized oracle system, and novel Cord-Approx strategy that uses past occupancy distributions and Hungarian matching to probabilistically estimate non-user behavior and dispatch users efficiently.

Result: Cord-Approx users averaged 6.69 minutes to find parking vs 19.98 minutes for non-users. Reduced search time by 72% in central hubs (range 67-76%) and up to 73% in residential areas.

Conclusion: The Cord-Approx strategy provides a practical and effective solution for reducing parking search times through probabilistic estimation and coordinated dispatch, significantly outperforming both uncoordinated approaches and non-users.

Abstract: In dense metropolitan areas, searching for street parking adds to traffic
congestion. Like many other problems, real-time assistants based on mobile
phones have been proposed, but their effectiveness is understudied. This work
quantifies how varying levels of user coordination and information availability
through such apps impact search time and the probability of finding street
parking. Through a data-driven simulation of Madrid's street parking ecosystem,
we analyze four distinct strategies: uncoordinated search (Unc-Agn),
coordinated parking without awareness of non-users (Cord-Agn), an idealized
oracle system that knows the positions of all non-users (Cord-Oracle), and our
novel/practical Cord-Approx strategy that estimates non-users' behavior
probabilistically. The Cord-Approx strategy, instead of requiring knowledge of
how close non-users are to a certain spot in order to decide whether to
navigate toward it, uses past occupancy distributions to elongate physical
distances between system users and alternative parking spots, and then solves a
Hungarian matching problem to dispatch accordingly. In high-fidelity
simulations of Madrid's parking network with real traffic data, users of
Cord-Approx averaged 6.69 minutes to find parking, compared to 19.98 minutes
for non-users without an app. A zone-level snapshot shows that Cord-Approx
reduces search time for system users by 72% (range = 67-76%) in central hubs,
and up to 73% in residential areas, relative to non-users.

</details>


### [229] [Evaluating Language Model Reasoning about Confidential Information](https://arxiv.org/abs/2508.19980)
*Dylan Sam,Alexander Robey,Andy Zou,Matt Fredrikson,J. Zico Kolter*

Main category: cs.LG

TL;DR: Language models struggle with contextual robustness in following safety rules, particularly in password-protected scenarios, and reasoning capabilities may actually worsen security by leaking confidential information.


<details>
  <summary>Details</summary>
Motivation: As language models are deployed as autonomous agents in high-stakes settings, ensuring reliable adherence to user-defined safety rules has become a critical concern, requiring investigation into contextual robustness.

Method: Developed PasswordEval benchmark to test if models can correctly determine when user requests are authorized with correct passwords, scaling difficulty through adversarial pressure and multi-turn conversations.

Result: Current open- and closed-source models struggle with this task, reasoning capabilities don't improve performance and frequently leak confidential information, and models perform poorly under adversarial pressure and longer conversations.

Conclusion: Frontier models are not well-suited for handling confidential information, and reasoning capabilities need different training approaches to ensure safety in high-stakes deployments.

Abstract: As language models are increasingly deployed as autonomous agents in
high-stakes settings, ensuring that they reliably follow user-defined rules has
become a critical safety concern. To this end, we study whether language models
exhibit contextual robustness, or the capability to adhere to context-dependent
safety specifications. For this analysis, we develop a benchmark (PasswordEval)
that measures whether language models can correctly determine when a user
request is authorized (i.e., with a correct password). We find that current
open- and closed-source models struggle with this seemingly simple task, and
that, perhaps surprisingly, reasoning capabilities do not generally improve
performance. In fact, we find that reasoning traces frequently leak
confidential information, which calls into question whether reasoning traces
should be exposed to users in such applications. We also scale the difficulty
of our evaluation along multiple axes: (i) by adding adversarial user pressure
through various jailbreaking strategies, and (ii) through longer multi-turn
conversations where password verification is more challenging. Overall, our
results suggest that current frontier models are not well-suited to handling
confidential information, and that reasoning capabilities may need to be
trained in a different manner to make them safer for release in high-stakes
settings.

</details>


### [230] [Self-Supervised Pre-Training with Equilibrium Constraints](https://arxiv.org/abs/2508.19990)
*Xiaodong Cui,A F M Saif,Brian Kingsbury,Tianyi Chen*

Main category: cs.LG

TL;DR: A new self-supervised pre-training approach for heterogeneous data using bilevel optimization with equilibrium constraints to optimize each data source to local optima, improving downstream task adaptivity.


<details>
  <summary>Details</summary>
Motivation: Conventional self-supervised pre-training mixes all heterogeneous data and minimizes averaged global loss, which may not optimize individual data sources effectively. The paper aims to improve model adaptivity for downstream tasks by ensuring each heterogeneous data source reaches its local optimum.

Method: Proposes a bilevel optimization formulation with equilibrium constraints that ensures models optimize each heterogeneous data source to local optima after K-step gradient descent from the initial model. Uses first-order approximation to solve the bilevel optimization problem, connecting it to model-agnostic meta learning (MAML).

Result: Experiments on multi-domain and multilingual datasets show the approach significantly improves adaptivity of self-supervised pre-trained models for downstream supervised fine-tuning tasks.

Conclusion: The proposed equilibrium-constrained bilevel optimization approach effectively handles heterogeneous data in self-supervised pre-training, leading to better model adaptivity and performance on downstream tasks compared to conventional methods.

Abstract: Self-supervised pre-training using unlabeled data is widely used in machine
learning. In this paper, we propose a new self-supervised pre-training approach
to dealing with heterogeneous data. Instead of mixing all the data and
minimizing the averaged global loss in the conventional way, we impose
additional equilibrium constraints to ensure that the models optimizes each
source of heterogeneous data to its local optima after $K$-step gradient
descent initialized from the model. We formulate this as a bilevel optimization
problem, and use the first-order approximation method to solve the problem. We
discuss its connection to model-agnostic meta learning (MAML). Experiments are
carried out on self-supervised pre-training using multi-domain and multilingual
datasets, demonstrating that the proposed approach can significantly improve
the adaptivity of the self-supervised pre-trained model for the downstream
supervised fine-tuning tasks.

</details>


### [231] [Linear-Time Demonstration Selection for In-Context Learning via Gradient Estimation](https://arxiv.org/abs/2508.19999)
*Ziniu Zhang,Zhenshuo Zhang,Dongyue Li,Lu Wang,Jennifer Dy,Hongyang R. Zhang*

Main category: cs.LG

TL;DR: A gradient-based algorithm for efficient selection of demonstration examples in in-context learning, achieving 37.7x speedup and 11% better performance than existing methods.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of quickly selecting the most relevant k examples from n candidates for in-context learning, as current similarity-based methods using token embeddings are suboptimal.

Method: Proposes a gradient-based approach that estimates model outputs through first-order approximation using gradients in input embedding space, aggregates influence scores from multiple random subsets, and selects top-k examples.

Result: Achieves less than 1% error in approximation across six datasets, 37.7x speedup on models up to 34B parameters, and 11% average performance improvement over existing methods.

Conclusion: The gradient-based approach provides an efficient and effective solution for demonstration example selection in in-context learning, significantly outperforming traditional embedding-based methods.

Abstract: This paper introduces an algorithm to select demonstration examples for
in-context learning of a query set. Given a set of $n$ examples, how can we
quickly select $k$ out of $n$ to best serve as the conditioning for downstream
inference? This problem has broad applications in prompt tuning and
chain-of-thought reasoning. Since model weights remain fixed during in-context
learning, previous work has sought to design methods based on the similarity of
token embeddings. This work proposes a new approach based on gradients of the
output taken in the input embedding space. Our approach estimates model outputs
through a first-order approximation using the gradients. Then, we apply this
estimation to multiple randomly sampled subsets. Finally, we aggregate the
sampled subset outcomes to form an influence score for each demonstration, and
select $k$ most relevant examples. This procedure only requires pre-computing
model outputs and gradients once, resulting in a linear-time algorithm relative
to model and training set sizes. Extensive experiments across various models
and datasets validate the efficiency of our approach. We show that the gradient
estimation procedure yields approximations of full inference with less than
$\mathbf{1}\%$ error across six datasets. This allows us to scale up subset
selection that would otherwise run full inference by up to
$\mathbf{37.7}\times$ on models with up to $34$ billion parameters, and
outperform existing selection methods based on input embeddings by
$\mathbf{11}\%$ on average.

</details>


### [232] [Cross-Platform E-Commerce Product Categorization and Recategorization: A Multimodal Hierarchical Classification Approach](https://arxiv.org/abs/2508.20013)
*Lotte Gross,Rebecca Walter,Nicole Zoppi,Adrien Justus,Alessandro Gambetti,Qiwei Han,Maximilian Kaiser*

Main category: cs.LG

TL;DR: Multimodal hierarchical classification framework for e-commerce product categorization that integrates text, vision, and vision-language features, achieving 98.59% F1 score and demonstrating effective cross-platform generalization.


<details>
  <summary>Details</summary>
Motivation: Address platform heterogeneity and structural limitations in existing e-commerce taxonomies for fashion products across multiple international platforms.

Method: Multimodal hierarchical classification using RoBERTa (text), ViT (vision), and CLIP (vision-language) with fusion strategies (early, late, attention-based) and dynamic masking. Self-supervised recategorization pipeline with SimCLR, UMAP, and cascade clustering.

Result: CLIP embeddings with MLP-based late-fusion achieved highest hierarchical F1 (98.59%). Self-supervised pipeline discovered fine-grained categories with cluster purities above 86%. Cross-platform experiments showed trade-off between accuracy and generalization.

Conclusion: Framework successfully addresses industrial e-commerce challenges, demonstrates scalability through commercial deployment, and provides effective balance between cost and accuracy with two-stage inference pipeline.

Abstract: This study addresses critical industrial challenges in e-commerce product
categorization, namely platform heterogeneity and the structural limitations of
existing taxonomies, by developing and deploying a multimodal hierarchical
classification framework. Using a dataset of 271,700 products from 40
international fashion e-commerce platforms, we integrate textual features
(RoBERTa), visual features (ViT), and joint vision--language representations
(CLIP). We investigate fusion strategies, including early, late, and
attention-based fusion within a hierarchical architecture enhanced by dynamic
masking to ensure taxonomic consistency. Results show that CLIP embeddings
combined via an MLP-based late-fusion strategy achieve the highest hierarchical
F1 (98.59\%), outperforming unimodal baselines. To address shallow or
inconsistent categories, we further introduce a self-supervised ``product
recategorization'' pipeline using SimCLR, UMAP, and cascade clustering, which
discovered new, fine-grained categories (e.g., subtypes of ``Shoes'') with
cluster purities above 86\%. Cross-platform experiments reveal a
deployment-relevant trade-off: complex late-fusion methods maximize accuracy
with diverse training data, while simpler early-fusion methods generalize more
effectively to unseen platforms. Finally, we demonstrate the framework's
industrial scalability through deployment in EURWEB's commercial transaction
intelligence platform via a two-stage inference pipeline, combining a
lightweight RoBERTa stage with a GPU--accelerated multimodal stage to balance
cost and accuracy.

</details>


### [233] [Decomposing Behavioral Phase Transitions in LLMs: Order Parameters for Emergent Misalignment](https://arxiv.org/abs/2508.20015)
*Julian Arnold,Niels Lörch*

Main category: cs.LG

TL;DR: Fine-tuning LLMs on harmful data can cause broad misalignment. This paper develops a framework to detect and characterize rapid behavioral transitions during fine-tuning using statistical methods and LLM-evaluated order parameters.


<details>
  <summary>Details</summary>
Motivation: To understand when and how emergent misalignment occurs during fine-tuning of LLMs on narrowly harmful datasets, which can lead to broadly misaligned behavior with human values.

Method: Developed a comprehensive framework using distributional change detection methods and order parameters formulated in plain English and evaluated by an LLM judge. Used objective statistical dissimilarity measures to quantify phase transitions during fine-tuning.

Result: Found that the actual behavioral transition occurs later in training than indicated by gradient norm peaks. Quantified what percentage of total distributional change is captured by different aspects like alignment or verbosity, providing a decomposition of the overall transition.

Conclusion: The framework enables automated discovery and quantification of language-based order parameters, demonstrated across various domains including knowledge questions, politics, and ethics.

Abstract: Fine-tuning LLMs on narrowly harmful datasets can lead to behavior that is
broadly misaligned with respect to human values. To understand when and how
this emergent misalignment occurs, we develop a comprehensive framework for
detecting and characterizing rapid transitions during fine-tuning using both
distributional change detection methods as well as order parameters that are
formulated in plain English and evaluated by an LLM judge. Using an objective
statistical dissimilarity measure, we quantify how the phase transition that
occurs during fine-tuning affects multiple aspects of the model. In particular,
we assess what percentage of the total distributional change in model outputs
is captured by different aspects, such as alignment or verbosity, providing a
decomposition of the overall transition. We also find that the actual
behavioral transition occurs later in training than indicated by the peak in
the gradient norm alone. Our framework enables the automated discovery and
quantification of language-based order parameters, which we demonstrate on
examples ranging from knowledge questions to politics and ethics.

</details>


### [234] [Symphony: A Decentralized Multi-Agent Framework for Scalable Collective Intelligence](https://arxiv.org/abs/2508.20019)
*Ji Wang,Kashing Chen,Xinyuan Song,Ke Zhang,Lynn Ai,Eric Yang,Bill Shi*

Main category: cs.LG

TL;DR: Symphony is a decentralized multi-agent system that enables lightweight LLMs on consumer GPUs to coordinate through decentralized ledger, dynamic task allocation, and weighted voting, outperforming centralized approaches.


<details>
  <summary>Details</summary>
Motivation: Address limitations of centralized LLM-based agent frameworks including high deployment costs, rigid communication topologies, and limited adaptability.

Method: Introduces three key mechanisms: 1) decentralized ledger for capability recording, 2) Beacon-selection protocol for dynamic task allocation, 3) weighted result voting based on Chain-of-Thought reasoning.

Result: Outperforms existing baselines on reasoning benchmarks with substantial accuracy gains and demonstrates robustness across models of varying capacities.

Conclusion: Symphony provides privacy-saving, scalable, and fault-tolerant orchestration with low overhead, enabling efficient coordination of lightweight LLMs on consumer-grade hardware.

Abstract: Most existing Large Language Model (LLM)-based agent frameworks rely on
centralized orchestration, incurring high deployment costs, rigid communication
topologies, and limited adaptability. To address these challenges, we introduce
Symphony, a decentralized multi-agent system which enables lightweight LLMs on
consumer-grade GPUs to coordinate. Symphony introduces three key mechanisms:
(1) a decentralized ledger that records capabilities, (2) a Beacon-selection
protocol for dynamic task allocation, and (3) weighted result voting based on
CoTs. This design forms a privacy-saving, scalable, and fault-tolerant
orchestration with low overhead. Empirically, Symphony outperforms existing
baselines on reasoning benchmarks, achieving substantial accuracy gains and
demonstrating robustness across models of varying capacities.

</details>


### [235] [FairLoop: Software Support for Human-Centric Fairness in Predictive Business Process Monitoring](https://arxiv.org/abs/2508.20021)
*Felix Möhrlein,Martin Käppel,Julian Neuberger,Sven Weinzierl,Lars Ackermann,Martin Matzner,Stefan Jablonski*

Main category: cs.LG

TL;DR: FairLoop is a tool for human-guided bias mitigation in neural networks that allows users to inspect and modify unfair decision logic through distilled decision trees, enabling context-aware bias removal.


<details>
  <summary>Details</summary>
Motivation: Sensitive attributes like gender or age can lead to unfair predictions in machine learning tasks, particularly when used without proper context consideration in predictive business process monitoring.

Method: FairLoop distills decision trees from neural networks, allowing users to inspect and modify unfair decision logic, which is then used to fine-tune the original model towards fairer predictions.

Result: The approach enables context-aware bias removal through human involvement, addressing the influence of sensitive attributes selectively rather than excluding them uniformly.

Conclusion: FairLoop provides a human-guided approach to bias mitigation that offers more nuanced and context-aware fairness compared to uniform exclusion of sensitive attributes.

Abstract: Sensitive attributes like gender or age can lead to unfair predictions in
machine learning tasks such as predictive business process monitoring,
particularly when used without considering context. We present FairLoop1, a
tool for human-guided bias mitigation in neural network-based prediction
models. FairLoop distills decision trees from neural networks, allowing users
to inspect and modify unfair decision logic, which is then used to fine-tune
the original model towards fairer predictions. Compared to other approaches to
fairness, FairLoop enables context-aware bias removal through human
involvement, addressing the influence of sensitive attributes selectively
rather than excluding them uniformly.

</details>


### [236] [Using item recommendations and LLMs in marketing email titles](https://arxiv.org/abs/2508.20024)
*Deddy Jobson,Muktti Shukla,Phuong Dinh,Julio Christian Young,Nick Pitton,Nina Chen,Ryan Ginstrom*

Main category: cs.LG

TL;DR: Using LLMs to generate personalized email titles instead of fixed templates improves customer engagement in e-commerce marketing emails.


<details>
  <summary>Details</summary>
Motivation: Fixed email title templates fail to inspire enough interest in personalized recommendation emails, limiting engagement with users who stopped visiting the marketplace.

Method: Employ large language models (LLMs) to generate thematic titles that reflect personalized email content, conducting both offline simulations and online experiments on millions of users.

Result: The techniques improved engagement between customers and emails, with successful productionization of safe and automated email title generation for millions of users.

Conclusion: LLM-generated personalized email titles are effective in enhancing user engagement and can be safely automated at scale for e-commerce marketing campaigns.

Abstract: E-commerce marketplaces make use of a number of marketing channels like
emails, push notifications, etc. to reach their users and stimulate purchases.
Personalized emails especially are a popular touch point for marketers to
inform users of latest items in stock, especially for those who stopped
visiting the marketplace. Such emails contain personalized recommendations
tailored to each user's interests, enticing users to buy relevant items. A
common limitation of these emails is that the primary entry point, the title of
the email, tends to follow fixed templates, failing to inspire enough interest
in the contents. In this work, we explore the potential of large language
models (LLMs) for generating thematic titles that reflect the personalized
content of the emails. We perform offline simulations and conduct online
experiments on the order of millions of users, finding our techniques useful in
improving the engagement between customers and our emails. We highlight key
findings and learnings as we productionize the safe and automated generation of
email titles for millions of users.

</details>


### [237] [Pruning Strategies for Backdoor Defense in LLMs](https://arxiv.org/abs/2508.20032)
*Santosh Chapagain,Shah Muhammad Hamdi,Soukaina Filali Boubrahimi*

Main category: cs.LG

TL;DR: Attention-head pruning can effectively mitigate backdoor attacks in pre-trained language models without trigger knowledge or clean reference models, with different pruning strategies performing best against different types of attacks.


<details>
  <summary>Details</summary>
Motivation: Backdoor attacks pose significant threats to pre-trained language models and can survive vanilla fine-tuning. These stealthy attacks using syntactic or stylistic triggers are difficult to detect and defend against, making post-hoc purification essential.

Method: Six pruning-based strategies were designed: gradient-based pruning, layer-wise variance pruning, gradient-based pruning with structured L1/L2 sparsification, randomized ensemble pruning, reinforcement-learning-guided pruning, and Bayesian uncertainty pruning. Each method iteratively removes least informative heads while monitoring validation accuracy.

Result: Gradient-based pruning performed best against syntactic triggers, while reinforcement learning and Bayesian pruning were more effective against stylistic attacks. The methods successfully mitigated backdoor threats without trigger knowledge.

Conclusion: Attention-head pruning provides an effective defense mechanism against backdoor attacks in language models, with different pruning strategies offering specialized protection against different attack types.

Abstract: Backdoor attacks are a significant threat to the performance and integrity of
pre-trained language models. Although such models are routinely fine-tuned for
downstream NLP tasks, recent work shows they remain vulnerable to backdoor
attacks that survive vanilla fine-tuning. These attacks are difficult to defend
because end users typically lack knowledge of the attack triggers. Such attacks
consist of stealthy malicious triggers introduced through subtle syntactic or
stylistic manipulations, which can bypass traditional detection and remain in
the model, making post-hoc purification essential. In this study, we explore
whether attention-head pruning can mitigate these threats without any knowledge
of the trigger or access to a clean reference model. To this end, we design and
implement six pruning-based strategies: (i) gradient-based pruning, (ii)
layer-wise variance pruning, (iii) gradient-based pruning with structured L1/L2
sparsification, (iv) randomized ensemble pruning, (v)
reinforcement-learning-guided pruning, and (vi) Bayesian uncertainty pruning.
Each method iteratively removes the least informative heads while monitoring
validation accuracy to avoid over-pruning. Experimental evaluation shows that
gradient-based pruning performs best while defending the syntactic triggers,
whereas reinforcement learning and Bayesian pruning better withstand stylistic
attacks.

</details>


### [238] [Reinforcement Learning for Search Tree Size Minimization in Constraint Programming: New Results on Scheduling Benchmarks](https://arxiv.org/abs/2508.20056)
*Vilém Heinz,Petr Vilím,Zdeněk Hanzálek*

Main category: cs.LG

TL;DR: Failure-Directed Search enhanced with Multi-armed bandit reinforcement learning achieves significant speed improvements and better bounds on scheduling problems.


<details>
  <summary>Details</summary>
Motivation: To improve the efficiency of Failure-Directed Search (FDS) in Constraint Programming by leveraging insights from its connection to the Multi-armed bandit problem.

Method: Applied MAB reinforcement learning algorithms to FDS with problem-specific refinements and parameter tuning, evaluated on JSSP and RCPSP scheduling problems.

Result: Enhanced FDS performed 1.7x faster on JSSP and 2.1x faster on RCPSP benchmarks compared to original implementation, and improved state-of-the-art lower bounds on 78/84 JSSP and 226/393 RCPSP instances.

Conclusion: MAB-based reinforcement learning significantly enhances FDS performance, making it substantially faster than both original implementation and current state-of-the-art solver while improving solution quality.

Abstract: Failure-Directed Search (FDS) is a significant complete generic search
algorithm used in Constraint Programming (CP) to efficiently explore the search
space, proven particularly effective on scheduling problems. This paper
analyzes FDS's properties, showing that minimizing the size of its search tree
guided by ranked branching decisions is closely related to the Multi-armed
bandit (MAB) problem. Building on this insight, MAB reinforcement learning
algorithms are applied to FDS, extended with problem-specific refinements and
parameter tuning, and evaluated on the two most fundamental scheduling
problems, the Job Shop Scheduling Problem (JSSP) and Resource-Constrained
Project Scheduling Problem (RCPSP). The resulting enhanced FDS, using the best
extended MAB algorithm and configuration, performs 1.7 times faster on the JSSP
and 2.1 times faster on the RCPSP benchmarks compared to the original
implementation in a new solver called OptalCP, while also being 3.5 times
faster on the JSSP and 2.1 times faster on the RCPSP benchmarks than the
current state-of-the-art FDS algorithm in IBM CP Optimizer 22.1. Furthermore,
using only a 900-second time limit per instance, the enhanced FDS improved the
existing state-of-the-art lower bounds of 78 of 84 JSSP and 226 of 393 RCPSP
standard open benchmark instances while also completely closing a few of them.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [239] [Inference of Human-derived Specifications of Object Placement via Demonstration](https://arxiv.org/abs/2508.19367)
*Alex Cuellar,Ho Chit Siu,Julie A Shah*

Main category: cs.RO

TL;DR: PARCC framework enhances spatial relationship understanding for robotic object arrangement using formal logic and demonstration learning


<details>
  <summary>Details</summary>
Motivation: Current methods for understanding human-acceptable object configurations are limited in capturing important spatial relationships needed for robotic manipulation tasks

Method: Introduces positionally-augmented RCC (PARCC) based on region connection calculus, with an inference algorithm for learning specifications via human demonstrations

Result: Human study shows PARCC can capture human-intended specifications effectively, with demonstration learning outperforming human-provided specifications

Conclusion: PARCC framework successfully advances robotic understanding of human spatial arrangement rules through formal logic and demonstration-based learning

Abstract: As robots' manipulation capabilities improve for pick-and-place tasks (e.g.,
object packing, sorting, and kitting), methods focused on understanding
human-acceptable object configurations remain limited expressively with regard
to capturing spatial relationships important to humans. To advance robotic
understanding of human rules for object arrangement, we introduce
positionally-augmented RCC (PARCC), a formal logic framework based on region
connection calculus (RCC) for describing the relative position of objects in
space. Additionally, we introduce an inference algorithm for learning PARCC
specifications via demonstrations. Finally, we present the results from a human
study, which demonstrate our framework's ability to capture a human's intended
specification and the benefits of learning from demonstration approaches over
human-provided specifications.

</details>


### [240] [A Lightweight Crowd Model for Robot Social Navigation](https://arxiv.org/abs/2508.19595)
*Maryam Kazemi Eskeri,Thomas Wiedemann,Ville Kyrki,Dominik Baumann,Tomasz Piotr Kucner*

Main category: cs.RO

TL;DR: A lightweight real-time macroscopic crowd prediction model for robot navigation that reduces inference time by 3.6x while improving accuracy by 3.1%, enabling socially compliant navigation in dense human environments.


<details>
  <summary>Details</summary>
Motivation: Robots need to navigate safely and efficiently in human-populated environments by predicting crowd movement to avoid congestion, but existing microscopic models are computationally expensive and macroscopic models are either too simplistic or intensive.

Method: Proposes a lightweight macroscopic crowd prediction model that simplifies spatial and temporal processing based on pedestrian flow characteristics, enabling robust generalization without complex architectures.

Result: Achieved 3.6 times reduction in inference time while improving prediction accuracy by 3.1%. When integrated into a socially aware planning framework, it enables efficient and socially compliant robot navigation.

Conclusion: Efficient human crowd modeling allows robots to navigate dense environments without costly computations, demonstrating that lightweight macroscopic approaches can balance accuracy and computational efficiency for real-time applications.

Abstract: Robots operating in human-populated environments must navigate safely and
efficiently while minimizing social disruption. Achieving this requires
estimating crowd movement to avoid congested areas in real-time. Traditional
microscopic models struggle to scale in dense crowds due to high computational
cost, while existing macroscopic crowd prediction models tend to be either
overly simplistic or computationally intensive. In this work, we propose a
lightweight, real-time macroscopic crowd prediction model tailored for human
motion, which balances prediction accuracy and computational efficiency. Our
approach simplifies both spatial and temporal processing based on the inherent
characteristics of pedestrian flow, enabling robust generalization without the
overhead of complex architectures. We demonstrate a 3.6 times reduction in
inference time, while improving prediction accuracy by 3.1 %. Integrated into a
socially aware planning framework, the model enables efficient and socially
compliant robot navigation in dynamic environments. This work highlights that
efficient human crowd modeling enables robots to navigate dense environments
without costly computations.

</details>


### [241] [DATR: Diffusion-based 3D Apple Tree Reconstruction Framework with Sparse-View](https://arxiv.org/abs/2508.19508)
*Tian Qiu,Alan Zoubi,Yiyuan Lin,Ruiming Du,Lailiang Cheng,Yu Jiang*

Main category: cs.RO

TL;DR: DATR framework enables accurate 3D reconstruction of apple trees from sparse field views using a two-stage approach combining foundation models, diffusion models, and synthetic data training, achieving 360x throughput improvement over laser scanners.


<details>
  <summary>Details</summary>
Motivation: Existing 3D reconstruction methods struggle with sparse and occluded views in field conditions, limiting digital twin applications for agriculture that require high-fidelity virtual replicas of physical assets like apple trees.

Method: Two-stage framework: 1) Semi-automatic tree mask generation using onboard sensors and foundation models to filter background, 2) Single-image-to-3D reconstruction using diffusion model for multi-view generation and large reconstruction model for implicit neural fields, trained on realistic synthetic apple trees from Real2Sim data generator.

Result: Outperformed existing 3D reconstruction methods on both field and synthetic datasets, achieved domain-trait estimation comparable to industrial laser scanners while improving throughput by ~360 times.

Conclusion: DATR demonstrates strong potential for scalable agricultural digital twin systems by enabling efficient and accurate 3D reconstruction of trees from sparse field views.

Abstract: Digital twin applications offered transformative potential by enabling
real-time monitoring and robotic simulation through accurate virtual replicas
of physical assets. The key to these systems is 3D reconstruction with high
geometrical fidelity. However, existing methods struggled under field
conditions, especially with sparse and occluded views. This study developed a
two-stage framework (DATR) for the reconstruction of apple trees from sparse
views. The first stage leverages onboard sensors and foundation models to
semi-automatically generate tree masks from complex field images. Tree masks
are used to filter out background information in multi-modal data for the
single-image-to-3D reconstruction at the second stage. This stage consists of a
diffusion model and a large reconstruction model for respective multi view and
implicit neural field generation. The training of the diffusion model and LRM
was achieved by using realistic synthetic apple trees generated by a Real2Sim
data generator. The framework was evaluated on both field and synthetic
datasets. The field dataset includes six apple trees with field-measured ground
truth, while the synthetic dataset featured structurally diverse trees.
Evaluation results showed that our DATR framework outperformed existing 3D
reconstruction methods across both datasets and achieved domain-trait
estimation comparable to industrial-grade stationary laser scanners while
improving the throughput by $\sim$360 times, demonstrating strong potential for
scalable agricultural digital twin systems.

</details>


### [242] [Context-Aware Risk Estimation in Home Environments: A Probabilistic Framework for Service Robots](https://arxiv.org/abs/2508.19788)
*Sena Ishii,Akash Chikhalikar,Ankit A. Ravankar,Jose Victorio Salazar Luces,Yasuhisa Hirata*

Main category: cs.RO

TL;DR: A novel framework for estimating accident-prone regions in indoor environments using semantic graph-based risk propagation to enhance robot safety awareness.


<details>
  <summary>Details</summary>
Motivation: As robots integrate into daily life, particularly in homes, the ability to anticipate environmental hazards is crucial for ensuring user safety, trust, and effective human-robot interaction.

Method: Models object-level risk through a semantic graph-based propagation algorithm where objects are nodes with risk scores, and risk propagates asymmetrically based on spatial proximity and accident relationships.

Result: Achieves 75% binary risk detection accuracy on human-annotated dataset, with strong alignment to human perception especially for sharp or unstable objects.

Conclusion: The framework shows potential for context-aware risk reasoning to enhance robotic scene understanding and proactive safety behaviors in shared human-robot spaces.

Abstract: We present a novel framework for estimating accident-prone regions in
everyday indoor scenes, aimed at improving real-time risk awareness in service
robots operating in human-centric environments. As robots become integrated
into daily life, particularly in homes, the ability to anticipate and respond
to environmental hazards is crucial for ensuring user safety, trust, and
effective human-robot interaction. Our approach models object-level risk and
context through a semantic graph-based propagation algorithm. Each object is
represented as a node with an associated risk score, and risk propagates
asymmetrically from high-risk to low-risk objects based on spatial proximity
and accident relationship. This enables the robot to infer potential hazards
even when they are not explicitly visible or labeled. Designed for
interpretability and lightweight onboard deployment, our method is validated on
a dataset with human-annotated risk regions, achieving a binary risk detection
accuracy of 75%. The system demonstrates strong alignment with human
perception, particularly in scenes involving sharp or unstable objects. These
results underline the potential of context-aware risk reasoning to enhance
robotic scene understanding and proactive safety behaviors in shared
human-robot spaces. This framework could serve as a foundation for future
systems that make context-driven safety decisions, provide real-time alerts, or
autonomously assist users in avoiding or mitigating hazards within home
environments.

</details>


### [243] [Discrete-Guided Diffusion for Scalable and Safe Multi-Robot Motion Planning](https://arxiv.org/abs/2508.20095)
*Jinhao Liang,Sven Koenig,Ferdinando Fioretto*

Main category: cs.RO

TL;DR: DGD integrates discrete MAPF solvers with generative diffusion models to solve multi-robot motion planning, achieving scalability to 100 robots while maintaining high trajectory quality.


<details>
  <summary>Details</summary>
Motivation: Address limitations of existing approaches: discrete MAPF methods have poor trajectory quality due to coarse discretization, while continuous optimization methods suffer from poor scalability with robot count.

Method: Combines discrete MAPF solutions with constrained generative diffusion models, decomposes nonconvex problem into tractable convex subproblems, uses constrained optimization to guide diffusion models, and includes constraint repair for feasibility.

Result: Sets new state-of-the-art performance in large-scale complex environments, scales to 100 robots while maintaining planning efficiency and high success rates.

Conclusion: The DGD framework successfully bridges discrete and continuous approaches, achieving both scalability and high-quality trajectories for multi-robot motion planning.

Abstract: Multi-Robot Motion Planning (MRMP) involves generating collision-free
trajectories for multiple robots operating in a shared continuous workspace.
While discrete multi-agent path finding (MAPF) methods are broadly adopted due
to their scalability, their coarse discretization severely limits trajectory
quality. In contrast, continuous optimization-based planners offer
higher-quality paths but suffer from the curse of dimensionality, resulting in
poor scalability with respect to the number of robots. This paper tackles the
limitations of these two approaches by introducing a novel framework that
integrates discrete MAPF solvers with constrained generative diffusion models.
The resulting framework, called Discrete-Guided Diffusion (DGD), has three key
characteristics: (1) it decomposes the original nonconvex MRMP problem into
tractable subproblems with convex configuration spaces, (2) it combines
discrete MAPF solutions with constrained optimization techniques to guide
diffusion models capture complex spatiotemporal dependencies among robots, and
(3) it incorporates a lightweight constraint repair mechanism to ensure
trajectory feasibility. The proposed method sets a new state-of-the-art
performance in large-scale, complex environments, scaling to 100 robots while
achieving planning efficiency and high success rates.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [244] [Bootstrapping Learned Cost Models with Synthetic SQL Queries](https://arxiv.org/abs/2508.19807)
*Michael Nidd,Christoph Miksovic,Thomas Gschwind,Francesco Fusco,Andrea Giovannini,Ioana Giurgiu*

Main category: cs.DB

TL;DR: Using generative AI techniques to create synthetic SQL queries improves learned cost model training efficiency by 45% compared to traditional methods.


<details>
  <summary>Details</summary>
Motivation: Realistic database workloads are crucial for stress testing, vulnerability assessment, and performance optimization, but obtaining diverse SQL queries for training learned cost models can be challenging.

Method: Leveraging modern synthetic data generation techniques inspired by generative AI and LLMs to create high-quality datasets of SQL queries for training learned cost models.

Result: The approach enables training learned cost models with 45% fewer queries while maintaining or improving predictive accuracy compared to competitive generation approaches.

Conclusion: Generative AI-based synthetic data generation provides an effective method for creating training datasets that significantly improve the efficiency of learned cost model training for database performance prediction.

Abstract: Having access to realistic workloads for a given database instance is
extremely important to enable stress and vulnerability testing, as well as to
optimize for cost and performance. Recent advances in learned cost models have
shown that when enough diverse SQL queries are available, one can effectively
and efficiently predict the cost of running a given query against a specific
database engine. In this paper, we describe our experience in exploiting modern
synthetic data generation techniques, inspired by the generative AI and LLM
community, to create high-quality datasets enabling the effective training of
such learned cost models. Initial results show that we can improve a learned
cost model's predictive accuracy by training it with 45% fewer queries than
when using competitive generation approaches.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [245] [The Next Layer: Augmenting Foundation Models with Structure-Preserving and Attention-Guided Learning for Local Patches to Global Context Awareness in Computational Pathology](https://arxiv.org/abs/2508.19914)
*Muhammad Waqas,Rukhmini Bandyopadhyay,Eman Showkatian,Amgad Muneer,Anas Zafar,Frank Rojas Alvarez,Maricel Corredor Marin,Wentao Li,David Jaffray,Cara Haymaker,John Heymach,Natalie I Vokes,Luisa Maren Solis Soto,Jianjun Zhang,Jia Wu*

Main category: q-bio.QM

TL;DR: EAGLE-Net is a structure-preserving MIL framework that enhances foundation models in pathology by capturing global tissue architecture and local contextual relationships, achieving improved accuracy and interpretability across multiple cancer types.


<details>
  <summary>Details</summary>
Motivation: Current foundation models in computational pathology lack mechanisms to leverage global spatial structure and local contextual relationships in tissues, which are crucial for understanding the tumor microenvironment and making accurate diagnoses.

Method: EAGLE-Net integrates multi-scale absolute spatial encoding for global tissue architecture, top-K neighborhood-aware loss for local microenvironments, and background suppression loss to minimize false positives. It uses attention-guided MIL to aggregate patch-level features into slide-level predictions.

Result: Achieved up to 3% higher classification accuracy and top concordance indices in 6 of 7 cancer types across large pan-cancer datasets (10,260 slides for classification, 4,172 for survival). Produced smooth, biologically coherent attention maps aligned with expert annotations.

Conclusion: EAGLE-Net serves as a generalizable, interpretable framework that complements foundation models, enabling improved biomarker discovery, prognostic modeling, and clinical decision support in computational pathology.

Abstract: Foundation models have recently emerged as powerful feature extractors in
computational pathology, yet they typically omit mechanisms for leveraging the
global spatial structure of tissues and the local contextual relationships
among diagnostically relevant regions - key elements for understanding the
tumor microenvironment. Multiple instance learning (MIL) remains an essential
next step following foundation model, designing a framework to aggregate
patch-level features into slide-level predictions. We present EAGLE-Net, a
structure-preserving, attention-guided MIL architecture designed to augment
prediction and interpretability. EAGLE-Net integrates multi-scale absolute
spatial encoding to capture global tissue architecture, a top-K
neighborhood-aware loss to focus attention on local microenvironments, and
background suppression loss to minimize false positives. We benchmarked
EAGLE-Net on large pan-cancer datasets, including three cancer types for
classification (10,260 slides) and seven cancer types for survival prediction
(4,172 slides), using three distinct histology foundation backbones (REMEDIES,
Uni-V1, Uni2-h). Across tasks, EAGLE-Net achieved up to 3% higher
classification accuracy and the top concordance indices in 6 of 7 cancer types,
producing smooth, biologically coherent attention maps that aligned with expert
annotations and highlighted invasive fronts, necrosis, and immune infiltration.
These results position EAGLE-Net as a generalizable, interpretable framework
that complements foundation models, enabling improved biomarker discovery,
prognostic modeling, and clinical decision support

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [246] [A Self-Supervised Mixture-of-Experts Framework for Multi-behavior Recommendation](https://arxiv.org/abs/2508.19507)
*Kyungho Kim,Sunwoo Kim,Geon Lee,Kijung Shin*

Main category: cs.IR

TL;DR: MEMBER is a novel multi-behavior recommender system that addresses performance gaps between visited and unvisited items using a mixture-of-experts framework with specialized self-supervised training.


<details>
  <summary>Details</summary>
Motivation: Existing multi-behavior recommender systems show significant performance disparity between visited items (with user interactions) and unvisited items (no interactions), making it challenging to achieve strong performance on both with a single model.

Method: Proposes MEMBER system using mixture-of-experts framework with separate experts for visited and unvisited items, each trained with specialized self-supervised methods for their respective design goals.

Result: MEMBER achieves up to 65.46% performance gain over the best competitor in terms of Hit Ratio@20, demonstrating effectiveness across both visited and unvisited item types.

Conclusion: The mixture-of-experts approach with specialized self-supervised training effectively addresses the performance gap between visited and unvisited items in multi-behavior recommendation systems.

Abstract: In e-commerce, where users face a vast array of possible item choices,
recommender systems are vital for helping them discover suitable items they
might otherwise overlook. While many recommender systems primarily rely on a
user's purchase history, recent multi-behavior recommender systems incorporate
various auxiliary user behaviors, such as item clicks and cart additions, to
enhance recommendations. Despite their overall performance gains, their
effectiveness varies considerably between visited items (i.e., those a user has
interacted with through auxiliary behaviors) and unvisited items (i.e., those
with which the user has had no such interactions). Specifically, our analysis
reveals that (1) existing multi-behavior recommender systems exhibit a
significant gap in recommendation quality between the two item types (visited
and unvisited items) and (2) achieving strong performance on both types with a
single model architecture remains challenging. To tackle these issues, we
propose a novel multi-behavior recommender system, MEMBER. It employs a
mixture-of-experts framework, with experts designed to recommend the two item
types, respectively. Each expert is trained using a self-supervised method
specialized for its design goal. In our comprehensive experiments, we show the
effectiveness of MEMBER across both item types, achieving up to 65.46\%
performance gain over the best competitor in terms of Hit Ratio@20.

</details>


### [247] [A Scenario-Oriented Survey of Federated Recommender Systems: Techniques, Challenges, and Future Directions](https://arxiv.org/abs/2508.19620)
*Yunqi Mi,Jiakui Shen,Guoshuai Zhao,Jialie Shen,Xueming Qian*

Main category: cs.IR

TL;DR: This paper provides a comprehensive review of federated recommender systems (FedRec) from a recommendation perspective, analyzing the coupling between recommender systems and federated learning frameworks, with focus on scenario-specific approaches, practical challenges, and real-world deployment guidance.


<details>
  <summary>Details</summary>
Motivation: Existing surveys on federated recommender systems primarily focus on FL system design but overlook the unique characteristics and practical challenges of specific recommendation scenarios. The authors aim to bridge this gap by providing a recommendation-centric perspective to encourage real-world deployment of FedRec systems.

Method: The review establishes clear links between recommendation scenarios and FL frameworks, systematically analyzing scenario-specific approaches, practical challenges, and potential opportunities. It comprehensively examines the coupling of recommender systems and federated learning from the perspective of recommendation researchers and practitioners.

Result: The paper provides a systematic analysis of how recommendation scenarios interact with FL frameworks, identifies practical challenges in real-world deployment, and offers guidance for implementing FedRec systems that address specific recommendation problems rather than just FL architectural issues.

Conclusion: This review bridges the gap between existing FedRec research and practical applications by focusing on solving specific problems in real-world recommendation scenarios, providing valuable guidance for the deployment of federated recommender systems while maintaining user privacy protection.

Abstract: Extending recommender systems to federated learning (FL) frameworks to
protect the privacy of users or platforms while making recommendations has
recently gained widespread attention in academia. This is due to the natural
coupling of recommender systems and federated learning architectures: the data
originates from distributed clients (mostly mobile devices held by users),
which are highly related to privacy. In a centralized recommender system
(CenRec), the central server collects clients' data, trains the model, and
provides the service. Whereas in federated recommender systems (FedRec), the
step of data collecting is omitted, and the step of model training is offloaded
to each client. The server only aggregates the model and other knowledge, thus
avoiding client privacy leakage. Some surveys of federated recommender systems
discuss and analyze related work from the perspective of designing FL systems.
However, their utility drops by ignoring specific recommendation scenarios'
unique characteristics and practical challenges. For example, the statistical
heterogeneity issue in cross-domain FedRec originates from the label drift of
the data held by different platforms, which is mainly caused by the recommender
itself, but not the federated architecture. Therefore, it should focus more on
solving specific problems in real-world recommendation scenarios to encourage
the deployment FedRec. To this end, this review comprehensively analyzes the
coupling of recommender systems and federated learning from the perspective of
recommendation researchers and practitioners. We establish a clear link between
recommendation scenarios and FL frameworks, systematically analyzing
scenario-specific approaches, practical challenges, and potential
opportunities. We aim to develop guidance for the real-world deployment of
FedRec, bridging the gap between existing research and applications.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [248] [Inferring geometry and material properties from Mueller matrices with machine learning](https://arxiv.org/abs/2508.19713)
*Lars Doorenbos,C. H. Lucas Patty,Raphael Sznitman,Pablo Márquez-Neila*

Main category: physics.optics

TL;DR: Machine learning can infer surface geometry and material properties from Mueller matrices alone, even with unknown materials, using diagonal elements for material identification and off-diagonal elements for normal estimation.


<details>
  <summary>Details</summary>
Motivation: Mueller matrices encode both geometry and material information, but simultaneous recovery is ill-posed. The paper explores whether machine learning can extract both types of information from Mueller matrices.

Method: Used dataset of spheres with various isotropic materials, captured Mueller matrices over full angular domain at five visible wavelengths (450-650 nm). Trained ML models to predict material properties and surface normals using only Mueller matrices as input.

Result: Surface normals can be predicted and object geometry reconstructed even when material type is unknown. Mueller matrices allow correct identification of material types. Diagonal elements are key for material characterization, off-diagonal elements are decisive for normal estimation.

Conclusion: Mueller matrices contain sufficient information for machine learning to simultaneously infer both surface geometry and material properties, with specific matrix elements playing distinct roles in material vs geometric inference.

Abstract: Mueller matrices (MMs) encode information on geometry and material
properties, but recovering both simultaneously is an ill-posed problem. We
explore whether MMs contain sufficient information to infer surface geometry
and material properties with machine learning. We use a dataset of spheres of
various isotropic materials, with MMs captured over the full angular domain at
five visible wavelengths (450-650 nm). We train machine learning models to
predict material properties and surface normals using only these MMs as input.
We demonstrate that, even when the material type is unknown, surface normals
can be predicted and object geometry reconstructed. Moreover, MMs allow models
to identify material types correctly. Further analyses show that diagonal
elements are key for material characterization, and off-diagonal elements are
decisive for normal estimation.

</details>


### [249] [Fourier Feature Networks for High-Fidelity Prediction of Perturbed Optical Fields](https://arxiv.org/abs/2508.19751)
*Joshua R. Jandrell,Mitchell A. Cox*

Main category: physics.optics

TL;DR: Fourier Feature Networks overcome MLP limitations in learning oscillatory functions by incorporating predefined sinusoids, achieving high accuracy with fewer parameters for modeling optical field perturbations.


<details>
  <summary>Details</summary>
Motivation: Standard MLPs struggle to learn highly oscillatory complex-valued functions due to spectral bias, making them ineffective for modeling optical field perturbations in physical systems.

Method: Incorporating Fourier features (predefined sinusoids dependent on perturbation) as additional network input, reframing the problem from function approximation to finding linear combinations of basis functions.

Result: Achieved mean complex correlation of 0.995 with ground truth, reduced prediction error by an order of magnitude, and used 85% fewer parameters compared to standard MLP.

Conclusion: This approach provides a general and robust method for accurately modeling oscillatory physical systems, demonstrating superior performance over traditional neural network architectures.

Abstract: Modelling the effects of perturbations on optical fields often requires
learning highly oscillatory complex-valued functions. Standard multi-layer
perceptrons (MLPs) struggle with this task due to an inherent spectral bias,
preventing them from fitting high-frequency sinusoids. To overcome this, we
incorporate Fourier features - a set of predefined sinusoids dependent on the
perturbation - as an additional network input. This reframes the learning
problem from approximating a complex function to finding a linear combination
of basis functions. We demonstrate this method by training a Fourier Feature
Network to predict the transmission matrix of a multimode fibre under
mechanical compression. Compared to a standard MLP, our network reduces
prediction error in the output field's amplitude and phase by an order of
magnitude, achieving a mean complex correlation of 0.995 with the ground truth,
despite using 85% fewer parameters. This approach offers a general and robust
method for accurately modelling a wide class of oscillatory physical systems.

</details>


### [250] [On-chip wave chaos for photonic extreme learning](https://arxiv.org/abs/2508.19878)
*Matthew R. Wilson,Jack A. Smith,Michael J. Strain,Xavier Porte*

Main category: physics.optics

TL;DR: Experimental demonstration of a chip-scale photonic extreme learning machine using wave chaos interference in a stadium microcavity for scalable and energy-efficient neural network hardware.


<details>
  <summary>Details</summary>
Motivation: Address the growing demand for scalable and energy-efficient artificial neural networks by leveraging integrated photonics as a compact, parallel, and ultra-fast information processing platform.

Method: Fabricated a stadium microcavity using direct laser writing of SU-8 polymer on glass, encoded input information in wavelength of a tunable laser, and used scattering wall as readout layer to collect light from cavity's leaky modes.

Result: Achieved uncorrelated and aperiodic behavior in speckle patterns from wavelength scans, and demonstrated classification capability in four benchmark tasks with optimized readout size for each task.

Conclusion: The photonic extreme learning machine based on wave chaos interference offers a promising hardware solution for scalable neural networks with the ability to optimize performance by controlling output node count through selective measurement of scattering barrier regions.

Abstract: The increase in demand for scalable and energy efficient artificial neural
networks has put the focus on novel hardware solutions. Integrated photonics
offers a compact, parallel and ultra-fast information processing platform,
specially suited for extreme learning machine (ELM) architectures. Here we
experimentally demonstrate a chip-scale photonic ELM based on wave chaos
interference in a stadium microcavity. By encoding the input information in the
wavelength of an external single-frequency tunable laser source, we leverage
the high sensitivity to wavelength of injection in such photonic resonators. We
fabricate the microcavity with direct laser writing of SU-8 polymer on glass. A
scattering wall surrounding the stadium operates as readout layer, collecting
the light associated with the cavity's leaky modes. We report uncorrelated and
aperiodic behavior in the speckles of the scattering barrier from a high
resolution scan of the input wavelength. Finally, we characterize the system's
performance at classification in four qualitatively different benchmark tasks.
As we can control the number of output nodes of our ELM by measuring different
parts of the scattering barrier, we demonstrate the capability to optimize our
photonic ELM's readout size to the performance required for each task.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [251] [The Aegis Protocol: A Foundational Security Framework for Autonomous AI Agents](https://arxiv.org/abs/2508.19267)
*Sai Teja Reddy Adapala,Yashwanth Reddy Alugubelly*

Main category: cs.CR

TL;DR: Aegis Protocol is a layered security framework using DIDs, post-quantum cryptography, and zero-knowledge proofs to protect autonomous AI agent systems from emerging security threats.


<details>
  <summary>Details</summary>
Motivation: Traditional cybersecurity approaches are inadequate for addressing systemic risks in emerging multi-agent AI systems, including control-flow hijacking and cascading failures.

Method: Three integrated pillars: non-spoofable agent identity via W3C DIDs, communication integrity via NIST-standardized post-quantum cryptography, and verifiable policy compliance using Halo2 zero-knowledge proofs. Formal adversary model and STRIDE framework validation.

Result: Simulation with 1,000 agents showed 0% success rate across 20,000 attack trials. Median proof-generation latency of 2.79 seconds for policy verification.

Conclusion: Aegis Protocol provides strong security guarantees and establishes a performance baseline for safe, scalable autonomous AI systems, though evaluation is simulation-based and early-stage.

Abstract: The proliferation of autonomous AI agents marks a paradigm shift toward
complex, emergent multi-agent systems. This transition introduces systemic
security risks, including control-flow hijacking and cascading failures, that
traditional cybersecurity paradigms are ill-equipped to address. This paper
introduces the Aegis Protocol, a layered security framework designed to provide
strong security guarantees for open agentic ecosystems. The protocol integrates
three technological pillars: (1) non-spoofable agent identity via W3C
Decentralized Identifiers (DIDs); (2) communication integrity via
NIST-standardized post-quantum cryptography (PQC); and (3) verifiable,
privacy-preserving policy compliance using the Halo2 zero-knowledge proof (ZKP)
system. We formalize an adversary model extending Dolev-Yao for agentic threats
and validate the protocol against the STRIDE framework. Our quantitative
evaluation used a discrete-event simulation, calibrated against cryptographic
benchmarks, to model 1,000 agents. The simulation showed a 0 percent success
rate across 20,000 attack trials. For policy verification, analysis of the
simulation logs reported a median proof-generation latency of 2.79 seconds,
establishing a performance baseline for this class of security. While the
evaluation is simulation-based and early-stage, it offers a reproducible
baseline for future empirical studies and positions Aegis as a foundation for
safe, scalable autonomous AI.

</details>


### [252] [MixGAN: A Hybrid Semi-Supervised and Generative Approach for DDoS Detection in Cloud-Integrated IoT Networks](https://arxiv.org/abs/2508.19273)
*Tongxi Wu,Chenwei Xu,Jin Yang*

Main category: cs.CR

TL;DR: MixGAN is a hybrid DDoS detection method that combines conditional generation, semi-supervised learning, and robust feature extraction to address class imbalance and label scarcity in IoT-cloud environments.


<details>
  <summary>Details</summary>
Motivation: The proliferation of cloud-integrated IoT systems has increased vulnerability to DDoS attacks due to expanded attack surfaces, heterogeneous devices, and limited edge protection. Existing methods struggle with complex traffic dynamics, severe class imbalance, and scarce labeled data.

Method: Uses 1-D WideResNet backbone with temporal convolutional layers to capture traffic patterns. Employs pretrained CTGAN to generate synthetic DDoS attack samples. Introduces MixUp-Average-Sharpen (MAS) strategy to handle noisy pseudo-labels by averaging predictions over augmented views and reweighting towards high-confidence classes.

Result: Achieves up to 2.5% higher accuracy and 4% improvement in both True Positive Rate (TPR) and True Negative Rate (TNR) compared to state-of-the-art methods on NSL-KDD, BoT-IoT, and CICIoT2023 datasets.

Conclusion: MixGAN demonstrates robust performance in large-scale IoT-cloud environments, effectively addressing class imbalance and label scarcity challenges in DDoS detection.

Abstract: The proliferation of cloud-integrated IoT systems has intensified exposure to
Distributed Denial of Service (DDoS) attacks due to the expanded attack
surface, heterogeneous device behaviors, and limited edge protection. However,
DDoS detection in this context remains challenging because of complex traffic
dynamics, severe class imbalance, and scarce labeled data. While recent methods
have explored solutions to address class imbalance, many still struggle to
generalize under limited supervision and dynamic traffic conditions. To
overcome these challenges, we propose MixGAN, a hybrid detection method that
integrates conditional generation, semi-supervised learning, and robust feature
extraction. Specifically, to handle complex temporal traffic patterns, we
design a 1-D WideResNet backbone composed of temporal convolutional layers with
residual connections, which effectively capture local burst patterns in traffic
sequences. To alleviate class imbalance and label scarcity, we use a pretrained
CTGAN to generate synthetic minority-class (DDoS attack) samples that
complement unlabeled data. Furthermore, to mitigate the effect of noisy
pseudo-labels, we introduce a MixUp-Average-Sharpen (MAS) strategy that
constructs smoothed and sharpened targets by averaging predictions over
augmented views and reweighting them towards high-confidence classes.
Experiments on NSL-KDD, BoT-IoT, and CICIoT2023 demonstrate that MixGAN
achieves up to 2.5% higher accuracy and 4% improvement in both TPR and TNR
compared to state-of-the-art methods, confirming its robustness in large-scale
IoT-cloud environments. The source code is publicly available at
https://github.com/0xCavaliers/MixGAN.

</details>


### [253] [Towards Production-Worthy Simulation for Autonomous Cyber Operations](https://arxiv.org/abs/2508.19278)
*Konur Tholl,Mariam El Mezouar,Ranwa Al Mallah*

Main category: cs.CR

TL;DR: Extended CybORG's Cage Challenge 2 with new realistic actions (Patch, Isolate, Unisolate) and improved reward/feature design, then trained DQN and PPO agents to validate the enhanced environment maintains effective RL training capabilities.


<details>
  <summary>Details</summary>
Motivation: Simulated environments are crucial for Autonomous Cyber Operations training but need to accurately represent real-world cybersecurity scenarios and provide effective signals for Reinforcement Learning agents.

Method: Extended CybORG's Cage Challenge 2 environment with three new realistic actions, modified reward signals and agent feature space, then trained and evaluated DQN and PPO agents in the updated environment.

Result: Successfully demonstrated that CybORG can be extended with additional realistic functionality while maintaining its ability to generate informative training signals for RL agents.

Conclusion: The framework enables more realistic cybersecurity simulation environments that support effective Reinforcement Learning training for Autonomous Cyber Operations.

Abstract: Simulated environments have proven invaluable in Autonomous Cyber Operations
(ACO) where Reinforcement Learning (RL) agents can be trained without the
computational overhead of emulation. These environments must accurately
represent cybersecurity scenarios while producing the necessary signals to
support RL training. In this study, we present a framework where we first
extend CybORG's Cage Challenge 2 environment by implementing three new actions:
Patch, Isolate, and Unisolate, to better represent the capabilities available
to human operators in real-world settings. We then propose a design for agent
development where we modify the reward signals and the agent's feature space to
enhance training performance. To validate these modifications, we train DQN and
PPO agents in the updated environment. Our study demonstrates that CybORG can
be extended with additional realistic functionality, while maintaining its
ability to generate informative training signals for RL agents.

</details>


### [254] [CORTEX: Composite Overlay for Risk Tiering and Exposure in Operational AI Systems](https://arxiv.org/abs/2508.19281)
*Aoun E Muhammad,Kin Choong Yow,Jamel Baili,Yongwon Cho,Yunyoung Nam*

Main category: cs.CR

TL;DR: CORTEX is a multi-layered risk scoring framework for AI systems that categorizes 29 technical vulnerabilities and provides composite risk scores through a five-tier architecture combining likelihood/impact calculations, regulatory alignment, technical assessments, and Bayesian risk modeling.


<details>
  <summary>Details</summary>
Motivation: As AI systems are increasingly deployed in high-stakes sectors like healthcare and finance, their failures have evolved from theoretical possibilities to practical recurring risks, necessitating a comprehensive framework to assess and score AI system vulnerabilities.

Method: Developed through empirical analysis of over 1,200 incidents from the AI Incident Database, CORTEX uses a five-tier architecture: 1) utility-adjusted Likelihood x Impact calculations, 2) governance/contextual overlays aligned with regulatory frameworks, 3) technical surface scores, 4) environmental/residual modifiers, and 5) Bayesian risk aggregation with Monte Carlo simulation.

Result: The framework produces composite risk scores that can be operationalized across AI risk registers, model audits, conformity checks, and dynamic governance dashboards to assess AI system vulnerabilities systematically.

Conclusion: CORTEX provides a comprehensive, empirically-grounded framework for scoring AI system risks that addresses both technical vulnerabilities and regulatory compliance needs, enabling better risk management in high-stakes AI deployments.

Abstract: As the deployment of Artificial Intelligence (AI) systems in high-stakes
sectors - like healthcare, finance, education, justice, and infrastructure has
increased - the possibility and impact of failures of these systems have
significantly evolved from being a theoretical possibility to practical
recurring, systemic risk. This paper introduces CORTEX (Composite Overlay for
Risk Tiering and Exposure), a multi-layered risk scoring framework proposed to
assess and score AI system vulnerabilities, developed on empirical analysis of
over 1,200 incidents documented in the AI Incident Database (AIID), CORTEX
categorizes failure modes into 29 technical vulnerability groups. Each
vulnerability is scored through a five-tier architecture that combines: (1)
utility-adjusted Likelihood x Impact calculations; (2) governance + contextual
overlays aligned with regulatory frameworks, such as the EU AI Act, NIST RMF,
OECD principles; (3) technical surface scores, covering exposure vectors like
drift, traceability, and adversarial risk; (4) environmental and residual
modifiers tailored to context of where these systems are being deployed to use;
and (5) a final layered assessment via Bayesian risk aggregation and Monte
Carlo simulation to model volatility and long-tail risks. The resulting
composite score can be operationalized across AI risk registers, model audits,
conformity checks, and dynamic governance dashboards.

</details>


### [255] [RL-Finetuned LLMs for Privacy-Preserving Synthetic Rewriting](https://arxiv.org/abs/2508.19286)
*Zhan Shi,Yefeng Yuan,Yuhong Liu,Liang Cheng,Yi Fang*

Main category: cs.CR

TL;DR: RL framework for privacy-preserving text generation that balances privacy protection with data utility using composite rewards


<details>
  <summary>Details</summary>
Motivation: Address privacy concerns in ML datasets while maintaining performance, as conventional anonymization techniques are insufficient against inference attacks

Method: Reinforcement learning framework fine-tuning LLMs with composite reward function optimizing for explicit/implicit privacy, semantic fidelity, and diversity using MST over latent representations

Result: Significantly enhances author obfuscation and privacy metrics without degrading semantic quality

Conclusion: Provides scalable, model-agnostic solution for privacy-preserving data generation in LLM era

Abstract: The performance of modern machine learning systems depends on access to
large, high-quality datasets, often sourced from user-generated content or
proprietary, domain-specific corpora. However, these rich datasets inherently
contain sensitive personal information, raising significant concerns about
privacy, data security, and compliance with regulatory frameworks. While
conventional anonymization techniques can remove explicit identifiers, such
removal may result in performance drop in downstream machine learning tasks.
More importantly, simple anonymization may not be effective against inference
attacks that exploit implicit signals such as writing style, topical focus, or
demographic cues, highlighting the need for more robust privacy safeguards
during model training. To address the challenging issue of balancing user
privacy and data utility, we propose a reinforcement learning framework that
fine-tunes a large language model (LLM) using a composite reward function that
jointly optimizes for explicit and implicit privacy, semantic fidelity, and
output diversity. To effectively capture population level regularities, the
privacy reward combines semantic cues with structural patterns derived from a
minimum spanning tree (MST) over latent representations. By modeling these
privacy-sensitive signals in their distributional context, the proposed
approach guides the model to generate synthetic rewrites that preserve utility
while mitigating privacy risks. Empirical results show that the proposed method
significantly enhances author obfuscation and privacy metrics without degrading
semantic quality, providing a scalable and model-agnostic solution for privacy
preserving data generation in the era of large language models.

</details>


### [256] [Prompt-in-Content Attacks: Exploiting Uploaded Inputs to Hijack LLM Behavior](https://arxiv.org/abs/2508.19287)
*Zhuotao Lian,Weiyu Wang,Qingkui Zeng,Toru Nakanishi,Teruaki Kitasuka,Chunhua Su*

Main category: cs.CR

TL;DR: Hidden adversarial instructions in seemingly benign inputs can manipulate LLM outputs without user awareness, revealing a practical threat in real-world applications.


<details>
  <summary>Details</summary>
Motivation: LLMs are widely deployed in applications that process user-submitted content, creating potential vulnerabilities where adversarial instructions can be embedded in seemingly benign inputs to manipulate outputs.

Method: The authors identify and demonstrate prompt injection attacks where hidden adversarial instructions are embedded in documents or text, analyze root causes including prompt concatenation and insufficient input isolation, and test feasibility across popular platforms.

Result: The research demonstrates that prompt injection attacks are feasible across popular LLM platforms, allowing manipulation of outputs to produce biased summaries, fabricated claims, or misleading suggestions without user awareness or system compromise.

Conclusion: The findings reveal a subtle yet practical threat in real-world LLM workflows, highlighting the need for better mitigation strategies to address prompt concatenation vulnerabilities and improve input isolation in LLM systems.

Abstract: Large Language Models (LLMs) are widely deployed in applications that accept
user-submitted content, such as uploaded documents or pasted text, for tasks
like summarization and question answering. In this paper, we identify a new
class of attacks, prompt in content injection, where adversarial instructions
are embedded in seemingly benign inputs. When processed by the LLM, these
hidden prompts can manipulate outputs without user awareness or system
compromise, leading to biased summaries, fabricated claims, or misleading
suggestions. We demonstrate the feasibility of such attacks across popular
platforms, analyze their root causes including prompt concatenation and
insufficient input isolation, and discuss mitigation strategies. Our findings
reveal a subtle yet practical threat in real-world LLM workflows.

</details>


### [257] [Tricking LLM-Based NPCs into Spilling Secrets](https://arxiv.org/abs/2508.19288)
*Kyohei Shiomi,Zhuotao Lian,Toru Nakanishi,Teruaki Kitasuka*

Main category: cs.CR

TL;DR: Adversarial prompt injection can compromise LLM-based game NPCs by making them reveal hidden background secrets that should remain confidential.


<details>
  <summary>Details</summary>
Motivation: As LLMs are increasingly used to generate dynamic dialogue for game NPCs, there is a need to examine security vulnerabilities that could allow malicious actors to extract sensitive information through prompt injection attacks.

Method: The study examines whether adversarial prompt injection techniques can successfully cause LLM-based NPCs to disclose hidden background secrets that are intended to remain undisclosed.

Result: The research likely demonstrates that LLM-based NPCs are vulnerable to prompt injection attacks, allowing attackers to extract confidential background information that should remain hidden from players.

Conclusion: Integration of LLMs in game NPCs introduces new security risks, and protective measures are needed to prevent adversarial prompt injection from compromising narrative integrity and revealing sensitive game information.

Abstract: Large Language Models (LLMs) are increasingly used to generate dynamic
dialogue for game NPCs. However, their integration raises new security
concerns. In this study, we examine whether adversarial prompt injection can
cause LLM-based NPCs to reveal hidden background secrets that are meant to
remain undisclosed.

</details>


### [258] [Stand on The Shoulders of Giants: Building JailExpert from Previous Attack Experience](https://arxiv.org/abs/2508.19292)
*Xi Wang,Songlei Jian,Shasha Li,Xiaopeng Li,Bin Ji,Jun Ma,Xiaodong Liu,Jing Wang,Feilong Bao,Jianfeng Zhang,Baosheng Wang,Jie Yu*

Main category: cs.CR

TL;DR: JailExpert is an automated jailbreak framework that leverages past attack experiences to improve LLM jailbreak effectiveness and efficiency, achieving 17% higher success rate and 2.7x efficiency gain over state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Existing jailbreak methods are inefficient and suffer from repetitive optimization because they overlook the value of past attack experiences. As LLM safety constraints evolve, attack templates become obsolete, requiring better ways to integrate historical attack knowledge.

Method: JailExpert achieves formal representation of experience structure, groups experiences based on semantic drift, and supports dynamic updating of the experience pool. It's the first framework to systematically organize and utilize past jailbreak attempts.

Result: Extensive experiments show JailExpert significantly improves attack effectiveness and efficiency - achieving 17% higher average attack success rate and 2.7 times better efficiency compared to current state-of-the-art black-box jailbreak methods.

Conclusion: The framework successfully demonstrates that leveraging past attack experiences through formal representation and dynamic updating can substantially enhance automated jailbreak attacks, providing valuable insights for developing more robust LLM security frameworks.

Abstract: Large language models (LLMs) generate human-aligned content under certain
safety constraints. However, the current known technique ``jailbreak prompt''
can circumvent safety-aligned measures and induce LLMs to output malicious
content. Research on Jailbreaking can help identify vulnerabilities in LLMs and
guide the development of robust security frameworks. To circumvent the issue of
attack templates becoming obsolete as models evolve, existing methods adopt
iterative mutation and dynamic optimization to facilitate more automated
jailbreak attacks. However, these methods face two challenges: inefficiency and
repetitive optimization, as they overlook the value of past attack experiences.
To better integrate past attack experiences to assist current jailbreak
attempts, we propose the \textbf{JailExpert}, an automated jailbreak framework,
which is the first to achieve a formal representation of experience structure,
group experiences based on semantic drift, and support the dynamic updating of
the experience pool. Extensive experiments demonstrate that JailExpert
significantly improves both attack effectiveness and efficiency. Compared to
the current state-of-the-art black-box jailbreak methods, JailExpert achieves
an average increase of 17\% in attack success rate and 2.7 times improvement in
attack efficiency. Our implementation is available at
\href{https://github.com/xiZAIzai/JailExpert}{XiZaiZai/JailExpert}

</details>


### [259] [An Investigation on Group Query Hallucination Attacks](https://arxiv.org/abs/2508.19321)
*Kehao Miao,Xiaolong Jin*

Main category: cs.CR

TL;DR: Group Query Attack: A technique that presents multiple queries simultaneously to LLMs, showing performance degradation in fine-tuned models, potential backdoor triggering, and effectiveness in reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Understanding failure modes of LLMs during user interactions, particularly when users pose multiple questions in single conversations.

Method: Propose Group Query Attack technique that presents groups of queries to LLMs simultaneously to investigate how accumulated context influences outputs.

Result: Significant performance degradation in fine-tuned models, risk of triggering potential backdoors, and effectiveness in reasoning tasks for pre-trained/aligned models.

Conclusion: Group Query Attack reveals vulnerabilities in LLMs when handling multiple queries, highlighting security risks and performance issues in real-world usage scenarios.

Abstract: With the widespread use of large language models (LLMs), understanding their
potential failure modes during user interactions is essential. In practice,
users often pose multiple questions in a single conversation with LLMs.
Therefore, in this study, we propose Group Query Attack, a technique that
simulates this scenario by presenting groups of queries to LLMs simultaneously.
We investigate how the accumulated context from consecutive prompts influences
the outputs of LLMs. Specifically, we observe that Group Query Attack
significantly degrades the performance of models fine-tuned on specific tasks.
Moreover, we demonstrate that Group Query Attack induces a risk of triggering
potential backdoors of LLMs. Besides, Group Query Attack is also effective in
tasks involving reasoning, such as mathematical reasoning and code generation
for pre-trained and aligned models.

</details>


### [260] [Safety Alignment Should Be Made More Than Just A Few Attention Heads](https://arxiv.org/abs/2508.19697)
*Chao Huang,Zefeng Zhang,Juewei Yue,Quangang Li,Chuang Zhang,Tingwen Liu*

Main category: cs.CR

TL;DR: Current LLM safety mechanisms rely on a small subset of attention heads, making them vulnerable to jailbreak attacks. The paper introduces RDSHA to identify these critical heads and proposes AHD training to distribute safety behaviors across more heads, resulting in stronger safety robustness.


<details>
  <summary>Details</summary>
Motivation: LLM safety alignment remains vulnerable to adversarial attacks that bypass safety measures by exploiting the concentration of safety mechanisms in a limited number of attention heads.

Method: Introduces RDSHA (targeted ablation method using refusal direction) to identify safety-critical attention heads, and proposes AHD training strategy to distribute safety behaviors across multiple attention heads rather than concentrating them.

Result: AHD successfully distributes safety-related capabilities across more attention heads. Models trained with AHD show significantly stronger safety robustness against mainstream jailbreak attacks while maintaining overall functional utility.

Conclusion: Distributing safety mechanisms across multiple attention heads through specialized training (AHD) provides stronger protection against jailbreak attacks compared to concentrated safety mechanisms in few attention heads.

Abstract: Current safety alignment for large language models(LLMs) continues to present
vulnerabilities, given that adversarial prompting can effectively bypass their
safety measures.Our investigation shows that these safety mechanisms
predominantly depend on a limited subset of attention heads: removing or
ablating these heads can severely compromise model safety. To identify and
evaluate these safety-critical components, we introduce RDSHA, a targeted
ablation method that leverages the model's refusal direction to pinpoint
attention heads mostly responsible for safety behaviors. Further analysis shows
that existing jailbreak attacks exploit this concentration by selectively
bypassing or manipulating these critical attention heads. To address this
issue, we propose AHD, a novel training strategy designed to promote the
distributed encoding of safety-related behaviors across numerous attention
heads. Experimental results demonstrate that AHD successfully distributes
safety-related capabilities across more attention heads. Moreover, evaluations
under several mainstream jailbreak attacks show that models trained with AHD
exhibit considerably stronger safety robustness, while maintaining overall
functional utility.

</details>


### [261] [SoK: Large Language Model Copyright Auditing via Fingerprinting](https://arxiv.org/abs/2508.19843)
*Shuo Shao,Yiming Li,Yu He,Hongwei Yao,Wenyuan Yang,Dacheng Tao,Zhan Qin*

Main category: cs.CR

TL;DR: This paper presents the first comprehensive study on LLM fingerprinting for copyright protection, introducing a unified framework, taxonomy, and LeaFBench benchmark to evaluate fingerprinting methods under realistic deployment scenarios.


<details>
  <summary>Details</summary>
Motivation: Large Language Models are valuable intellectual property but vulnerable to copyright infringement and model theft. LLM fingerprinting offers a non-intrusive solution for copyright auditing, but its reliability remains uncertain due to diverse model modifications and lack of standardized evaluation.

Method: The authors introduce a unified framework and formal taxonomy categorizing existing methods into white-box and black-box approaches. They propose LeaFBench, a systematic benchmark built on mainstream foundation models with 149 distinct instances, integrating 13 representative post-development techniques including fine-tuning, quantization, system prompts, and RAG.

Result: Extensive experiments on LeaFBench reveal the strengths and weaknesses of existing fingerprinting methods, providing insights into their performance under various realistic deployment scenarios and model modifications.

Conclusion: The study outlines future research directions and critical open problems in LLM fingerprinting, establishing a foundation for standardized evaluation and advancing copyright protection for large language models.

Abstract: The broad capabilities and substantial resources required to train Large
Language Models (LLMs) make them valuable intellectual property, yet they
remain vulnerable to copyright infringement, such as unauthorized use and model
theft. LLM fingerprinting, a non-intrusive technique that extracts and compares
the distinctive features from LLMs to identify infringements, offers a
promising solution to copyright auditing. However, its reliability remains
uncertain due to the prevalence of diverse model modifications and the lack of
standardized evaluation. In this SoK, we present the first comprehensive study
of LLM fingerprinting. We introduce a unified framework and formal taxonomy
that categorizes existing methods into white-box and black-box approaches,
providing a structured overview of the state of the art. We further propose
LeaFBench, the first systematic benchmark for evaluating LLM fingerprinting
under realistic deployment scenarios. Built upon mainstream foundation models
and comprising 149 distinct model instances, LeaFBench integrates 13
representative post-development techniques, spanning both parameter-altering
methods (e.g., fine-tuning, quantization) and parameter-independent mechanisms
(e.g., system prompts, RAG). Extensive experiments on LeaFBench reveal the
strengths and weaknesses of existing methods, thereby outlining future research
directions and critical open problems in this emerging field. The code is
available at https://github.com/shaoshuo-ss/LeaFBench.

</details>


### [262] [Addressing Weak Authentication like RFID, NFC in EVs and EVCs using AI-powered Adaptive Authentication](https://arxiv.org/abs/2508.19465)
*Onyinye Okoye*

Main category: cs.CR

TL;DR: AI-powered adaptive authentication framework for EV charging systems that replaces vulnerable RFID/NFC with machine learning, anomaly detection, and continuous verification based on Zero Trust principles.


<details>
  <summary>Details</summary>
Motivation: Traditional EV charging authentication methods (RFID/NFC) use static identifiers and weak encryption, making them vulnerable to cloning, relay attacks, and signal interception, requiring more secure solutions.

Method: Proposes an AI-powered adaptive authentication framework integrating machine learning, anomaly detection, behavioral analytics, and contextual risk assessment based on Zero Trust Architecture principles.

Result: The research concludes that AI-powered adaptive authentication provides scalable, resilient, and proactive defense against current vulnerabilities in EV charging systems.

Conclusion: Adopting AI-powered adaptive authentication is a strategic imperative for securing electric mobility and strengthening digital trust across the EV ecosystem.

Abstract: The rapid expansion of the Electric Vehicles (EVs) and Electric Vehicle
Charging Systems (EVCs) has introduced new cybersecurity challenges,
specifically in authentication protocols that protect vehicles, users, and
energy infrastructure. Although widely adopted for convenience, traditional
authentication mechanisms like Radio Frequency Identification (RFID) and Near
Field Communication (NFC) rely on static identifiers and weak encryption,
making them highly vulnerable to attack vectors such as cloning, relay attacks,
and signal interception. This study explores an AI-powered adaptive
authentication framework designed to overcome these shortcomings by integrating
machine learning, anomaly detection, behavioral analytics, and contextual risk
assessment. Grounded in the principles of Zero Trust Architecture, the proposed
framework emphasizes continuous verification, least privilege access, and
secure communication. Through a comprehensive literature review, this research
evaluates current vulnerabilities and highlights AI-driven solutions to provide
a scalable, resilient, and proactive defense. Ultimately, the research findings
conclude that adopting AI-powered adaptive authentication is a strategic
imperative for securing the future of electric mobility and strengthening
digital trust across the ecosystem. Keywords: weak authentication, RFID, NFC,
ML, AI-powered adaptive authentication, relay attacks, cloning, eavesdropping,
MITM attacks, Zero Trust Architecture

</details>


### [263] [SIExVulTS: Sensitive Information Exposure Vulnerability Detection System using Transformer Models and Static Analysis](https://arxiv.org/abs/2508.19472)
*Kyler Katz,Sara Moshtari,Ibrahim Mujhid,Mehdi Mirakhorli,Derek Garcia*

Main category: cs.CR

TL;DR: SIExVulTS is a novel vulnerability detection system that combines transformer models with static analysis to identify sensitive information exposure vulnerabilities (CWE-200) in Java applications, achieving high accuracy and discovering new CVEs.


<details>
  <summary>Details</summary>
Motivation: Sensitive Information Exposure vulnerabilities remain a persistent threat, and existing tools rarely target diverse CWE-200 subcategories or provide context-aware analysis of code-level data flows.

Method: Three-stage architecture: (1) Attack Surface Detection Engine using sentence embeddings, (2) Exposure Analysis Engine with CodeQL queries aligned with CWE-200 hierarchy, (3) Flow Verification Engine leveraging GraphCodeBERT to validate source-to-sink flows.

Result: Attack Surface Detection achieved >93% F1 score, Exposure Analysis achieved 85.71% F1 score, Flow Verification increased precision from 22.61% to 87.23%. Uncovered six previously unknown CVEs in Apache projects.

Conclusion: SIExVulTS is effective and practical for improving software security against sensitive data exposure, addressing limitations of existing tools in detecting and verifying CWE-200 vulnerabilities.

Abstract: Sensitive Information Exposure (SIEx) vulnerabilities (CWE-200) remain a
persistent and under-addressed threat across software systems, often leading to
serious security breaches. Existing detection tools rarely target the diverse
subcategories of CWE-200 or provide context-aware analysis of code-level data
flows.
  Aims: This paper aims to present SIExVulTS, a novel vulnerability detection
system that integrates transformer-based models with static analysis to
identify and verify sensitive information exposure in Java applications.
  Method: SIExVulTS employs a three-stage architecture: (1) an Attack Surface
Detection Engine that uses sentence embeddings to identify sensitive variables,
strings, comments, and sinks; (2) an Exposure Analysis Engine that instantiates
CodeQL queries aligned with the CWE-200 hierarchy; and (3) a Flow Verification
Engine that leverages GraphCodeBERT to semantically validate source-to-sink
flows. We evaluate SIExVulTS using three curated datasets, including real-world
CVEs, a benchmark set of synthetic CWE-200 examples, and labeled flows from 31
open-source projects.
  Results: The Attack Surface Detection Engine achieved an average F1 score
greater than 93\%, the Exposure Analysis Engine achieved an F1 score of
85.71\%, and the Flow Verification Engine increased precision from 22.61\% to
87.23\%. Moreover, SIExVulTS successfully uncovered six previously unknown CVEs
in major Apache projects.
  Conclusions: The results demonstrate that SIExVulTS is effective and
practical for improving software security against sensitive data exposure,
addressing limitations of existing tools in detecting and verifying CWE-200
vulnerabilities.

</details>


### [264] [Disabling Self-Correction in Retrieval-Augmented Generation via Stealthy Retriever Poisoning](https://arxiv.org/abs/2508.20083)
*Yanbo Dai,Zhenlan Ji,Zongjie Li,Kuan Li,Shuai Wang*

Main category: cs.CR

TL;DR: DisarmRAG is a new poisoning attack that targets the retriever in RAG systems to bypass LLMs' self-correction ability, achieving over 90% attack success by embedding anti-SCA instructions through contrastive learning and co-optimization.


<details>
  <summary>Details</summary>
Motivation: Previous RAG poisoning attacks focused on knowledge bases but could be mitigated by LLMs' self-correction ability. This paper aims to develop a more effective attack that compromises the retriever itself to suppress SCA.

Method: Uses contrastive-learning-based model editing to stealthily modify the retriever to return malicious instructions for specific queries while maintaining normal behavior. Implements iterative co-optimization to discover robust instructions that bypass defenses.

Result: Achieves near-perfect retrieval of malicious instructions, suppresses SCA effectively, and reaches attack success rates exceeding 90% across six LLMs and three QA benchmarks. The edited retriever remains stealthy against detection methods.

Conclusion: DisarmRAG demonstrates a new poisoning paradigm that successfully bypasses LLM self-correction defenses, highlighting the urgent need for retriever-centric security measures in RAG systems.

Abstract: Retrieval-Augmented Generation (RAG) has become a standard approach for
improving the reliability of large language models (LLMs). Prior work
demonstrates the vulnerability of RAG systems by misleading them into
generating attacker-chosen outputs through poisoning the knowledge base.
However, this paper uncovers that such attacks could be mitigated by the strong
\textit{self-correction ability (SCA)} of modern LLMs, which can reject false
context once properly configured. This SCA poses a significant challenge for
attackers aiming to manipulate RAG systems.
  In contrast to previous poisoning methods, which primarily target the
knowledge base, we introduce \textsc{DisarmRAG}, a new poisoning paradigm that
compromises the retriever itself to suppress the SCA and enforce
attacker-chosen outputs. This compromisation enables the attacker to
straightforwardly embed anti-SCA instructions into the context provided to the
generator, thereby bypassing the SCA. To this end, we present a
contrastive-learning-based model editing technique that performs localized and
stealthy edits, ensuring the retriever returns a malicious instruction only for
specific victim queries while preserving benign retrieval behavior. To further
strengthen the attack, we design an iterative co-optimization framework that
automatically discovers robust instructions capable of bypassing prompt-based
defenses. We extensively evaluate DisarmRAG across six LLMs and three QA
benchmarks. Our results show near-perfect retrieval of malicious instructions,
which successfully suppress SCA and achieve attack success rates exceeding 90\%
under diverse defensive prompts. Also, the edited retriever remains stealthy
under several detection methods, highlighting the urgent need for
retriever-centric defenses.

</details>


### [265] [Servant, Stalker, Predator: How An Honest, Helpful, And Harmless (3H) Agent Unlocks Adversarial Skills](https://arxiv.org/abs/2508.19500)
*David Noever*

Main category: cs.CR

TL;DR: This paper identifies a novel vulnerability class in Model Context Protocol (MCP) agent systems where individually authorized tasks can be orchestrated to produce harmful emergent behaviors through service chaining.


<details>
  <summary>Details</summary>
Motivation: To investigate security vulnerabilities in MCP-based agent systems where the fundamental assumption of service isolation fails when agents coordinate actions across multiple domains, creating exponential attack surfaces.

Method: Systematic analysis using MITRE ATLAS framework, testing 95 agents with access to multiple services (browser automation, financial analysis, location tracking, code deployment) to demonstrate how legitimate operations can be chained into sophisticated attack sequences.

Result: Empirical evidence shows specific attack chains achieving targeted harm including data exfiltration, financial manipulation, and infrastructure compromise. Current MCP architectures lack cross-domain security measures to prevent compositional attacks.

Conclusion: The research reveals that service isolation security assumptions fail when agents coordinate across domains, and provides an experimental framework to evaluate not just task completion but optimization across services that violates safety constraints. Three concrete experimental directions are proposed using the MCP benchmark suite.

Abstract: This paper identifies and analyzes a novel vulnerability class in Model
Context Protocol (MCP) based agent systems. The attack chain describes and
demonstrates how benign, individually authorized tasks can be orchestrated to
produce harmful emergent behaviors. Through systematic analysis using the MITRE
ATLAS framework, we demonstrate how 95 agents tested with access to multiple
services-including browser automation, financial analysis, location tracking,
and code deployment-can chain legitimate operations into sophisticated attack
sequences that extend beyond the security boundaries of any individual service.
These red team exercises survey whether current MCP architectures lack
cross-domain security measures necessary to detect or prevent a large category
of compositional attacks. We present empirical evidence of specific attack
chains that achieve targeted harm through service orchestration, including data
exfiltration, financial manipulation, and infrastructure compromise. These
findings reveal that the fundamental security assumption of service isolation
fails when agents can coordinate actions across multiple domains, creating an
exponential attack surface that grows with each additional capability. This
research provides a barebones experimental framework that evaluate not whether
agents can complete MCP benchmark tasks, but what happens when they complete
them too well and optimize across multiple services in ways that violate human
expectations and safety constraints. We propose three concrete experimental
directions using the existing MCP benchmark suite.

</details>


### [266] [A Technical Review on Comparison and Estimation of Steganographic Tools](https://arxiv.org/abs/2508.19323)
*Ms. Preeti P. Bhatt,Rakesh R. Savant*

Main category: cs.CR

TL;DR: This review paper compares six image steganography tools by testing their performance with different image formats and analyzing them based on image features like size, dimensions, pixel values, and histogram differentiation.


<details>
  <summary>Details</summary>
Motivation: To classify image steganography techniques and compare various steganography tools to identify the best performing ones based on efficiency and image feature analysis.

Method: Selected six frequently used steganography tools and tested them using the same input (specific text embedded in host images) across different image formats. Analyzed tools based on image features including size, dimensions, pixel values, and histogram differentiation.

Result: All six tools performed relatively at the same level, though some software showed better efficiency than others. Performance differences were based on image features and characteristics.

Conclusion: The study provides a comparative analysis of steganography tools, revealing that while tools generally perform similarly, efficiency varies and depends on specific image features and characteristics.

Abstract: Steganography is technique of hiding a data under cover media using different
steganography tools. Image steganography is hiding of data
(Text/Image/Audio/Video) under a cover as Image. This review paper presents
classification of image steganography and the comparison of various Image
steganography tools using different image formats. Analyzing numerous tools on
the basis of Image features and extracting the best one. Some of the tools
available in the market were selected based on the frequent use; these tools
were tested using the same input on all of them. Specific text was embedded
within all host images for each of the six Steganography tools selected. The
results of the experiment reveal that all the six tools were relatively
performing at the same level, though some software performs better than others
through efficiency. And it was based on the image features like size,
dimensions, and pixel value and histogram differentiation.

</details>


### [267] [Mind the Third Eye! Benchmarking Privacy Awareness in MLLM-powered Smartphone Agents](https://arxiv.org/abs/2508.19493)
*Zhixin Lin,Jungang Li,Shidong Pan,Yibo Shi,Yue Yao,Dongliang Xu*

Main category: cs.CR

TL;DR: First large-scale benchmark (7,138 scenarios) evaluating smartphone agents' privacy awareness, showing most agents perform poorly (<60% RA) even with hints, with closed-source agents outperforming open-source ones.


<details>
  <summary>Details</summary>
Motivation: Smartphone agents powered by MLLMs have extensive access to sensitive personal information, but their privacy awareness capabilities are not well understood, creating potential privacy risks for users.

Method: Created comprehensive benchmark with 7,138 scenarios annotated by privacy type, sensitivity level, and location. Evaluated seven mainstream smartphone agents on their privacy awareness capabilities.

Result: Most agents showed unsatisfying privacy awareness (below 60% RA even with explicit hints). Closed-source agents performed better than open-source, with Gemini 2.0-flash achieving best RA of 67%. Privacy detection correlated with scenario sensitivity levels.

Conclusion: Current smartphone agents have inadequate privacy awareness, highlighting an unbalanced utility-privacy tradeoff that requires attention from the research community to improve privacy protection in automated smartphone tasks.

Abstract: Smartphones bring significant convenience to users but also enable devices to
extensively record various types of personal information. Existing smartphone
agents powered by Multimodal Large Language Models (MLLMs) have achieved
remarkable performance in automating different tasks. However, as the cost,
these agents are granted substantial access to sensitive users' personal
information during this operation. To gain a thorough understanding of the
privacy awareness of these agents, we present the first large-scale benchmark
encompassing 7,138 scenarios to the best of our knowledge. In addition, for
privacy context in scenarios, we annotate its type (e.g., Account Credentials),
sensitivity level, and location. We then carefully benchmark seven available
mainstream smartphone agents. Our results demonstrate that almost all
benchmarked agents show unsatisfying privacy awareness (RA), with performance
remaining below 60% even with explicit hints. Overall, closed-source agents
show better privacy ability than open-source ones, and Gemini 2.0-flash
achieves the best, achieving an RA of 67%. We also find that the agents'
privacy detection capability is highly related to scenario sensitivity level,
i.e., the scenario with a higher sensitivity level is typically more
identifiable. We hope the findings enlighten the research community to rethink
the unbalanced utility-privacy tradeoff about smartphone agents. Our code and
benchmark are available at https://zhixin-l.github.io/SAPA-Bench.

</details>


### [268] [From Research to Reality: Feasibility of Gradient Inversion Attacks in Federated Learning](https://arxiv.org/abs/2508.19819)
*Viktor Valadi,Mattias Åkesson,Johan Östman,Salman Toor,Andreas Hellander*

Main category: cs.CR

TL;DR: Gradient inversion attacks are easier in inference mode but challenging in realistic training mode. Successful attacks require specific architectural conditions: shallow/wide models with skip connections and pre-activation normalization. The paper introduces novel attacks for training-mode and presents the first attack on production object-detection models.


<details>
  <summary>Details</summary>
Motivation: To systematically analyze how model architecture and training behavior affect vulnerability to gradient inversion attacks, moving beyond the unrealistic inference-mode assumptions common in previous research.

Method: Conducted systematic analysis of inference-mode vs training-mode clients, developed two novel attacks for training-mode with varying attacker knowledge, and extended attacks to production-grade object-detection models with architectural modifications.

Result: Found that successful attacks in realistic training mode require specific architectural conditions. Achieved state-of-the-art performance under realistic conditions and demonstrated the first attack on production object-detection models (though requiring significant architectural changes).

Conclusion: Provides a comprehensive mapping of vulnerability settings, showing that gradient inversion risk assessment must consider both architectural choices and operational modes. Highlights inherent robustness of certain architectures and offers actionable insights for privacy protection.

Abstract: Gradient inversion attacks have garnered attention for their ability to
compromise privacy in federated learning. However, many studies consider
attacks with the model in inference mode, where training-time behaviors like
dropout are disabled and batch normalization relies on fixed statistics. In
this work, we systematically analyze how architecture and training behavior
affect vulnerability, including the first in-depth study of inference-mode
clients, which we show dramatically simplifies inversion. To assess attack
feasibility under more realistic conditions, we turn to clients operating in
standard training mode. In this setting, we find that successful attacks are
only possible when several architectural conditions are met simultaneously:
models must be shallow and wide, use skip connections, and, critically, employ
pre-activation normalization. We introduce two novel attacks against models in
training-mode with varying attacker knowledge, achieving state-of-the-art
performance under realistic training conditions. We extend these efforts by
presenting the first attack on a production-grade object-detection model. Here,
to enable any visibly identifiable leakage, we revert to the lenient inference
mode setting and make multiple architectural modifications to increase model
vulnerability, with the extent of required changes highlighting the strong
inherent robustness of such architectures. We conclude this work by offering
the first comprehensive mapping of settings, clarifying which combinations of
architectural choices and operational modes meaningfully impact privacy. Our
analysis provides actionable insight into when models are likely vulnerable,
when they appear robust, and where subtle leakage may persist. Together, these
findings reframe how gradient inversion risk should be assessed in future
research and deployment scenarios.

</details>


### [269] [Addressing Deepfake Issue in Selfie banking through camera based authentication](https://arxiv.org/abs/2508.19714)
*Subhrojyoti Mukherjee,Manoranjan Mohanty*

Main category: cs.CR

TL;DR: Using forensic recognition systems originally designed for camera localization to detect deepfakes in selfie banking authentication


<details>
  <summary>Details</summary>
Motivation: Deep learning technologies enable creation of highly realistic fake identities that can bypass facial recognition systems in online banking, posing a significant threat to security

Method: Adapting an established forensic recognition system previously used for picture camera localization to detect deepfake images

Result: Not specified in the abstract - the paper explores the application but doesn't state specific results

Conclusion: Forensic recognition systems show potential for detecting deepfakes in banking authentication scenarios, though specific findings are not detailed in the abstract

Abstract: Fake images in selfie banking are increasingly becoming a threat. Previously,
it was just Photoshop, but now deep learning technologies enable us to create
highly realistic fake identities, which fraudsters exploit to bypass biometric
systems such as facial recognition in online banking. This paper explores the
use of an already established forensic recognition system, previously used for
picture camera localization, in deepfake detection.

</details>


### [270] [Intellectual Property in Graph-Based Machine Learning as a Service: Attacks and Defenses](https://arxiv.org/abs/2508.19641)
*Lincan Li,Bolin Shen,Chenxi Zhao,Yuxiang Sun,Kaixiang Zhao,Shirui Pan,Yushun Dong*

Main category: cs.CR

TL;DR: This survey paper presents the first comprehensive taxonomy of threats and defenses for graph machine learning (GML) intellectual property protection in GMLaaS scenarios, including a systematic evaluation framework, benchmark datasets, and an open-source library called PyGIP.


<details>
  <summary>Details</summary>
Motivation: Graph machine learning models are becoming increasingly resource-intensive to train, making them valuable intellectual property. However, deploying these models through Machine-Learning-as-a-Service (GMLaaS) exposes them to potential attacks that can steal model functionalities or sensitive training data.

Method: The authors systematically develop a taxonomy of threats and defenses at both GML model and graph-structured data levels. They create an evaluation framework, curate benchmark datasets across various domains, and develop an open-source library (PyGIP) to assess attack and defense techniques in GMLaaS scenarios.

Result: The survey provides a comprehensive framework for understanding GML intellectual property protection, including a taxonomy of threats/defenses, evaluation metrics, benchmark datasets, and practical tools through the PyGIP library.

Conclusion: This work establishes fundamental groundwork for intellectual property protection in graph machine learning and provides practical resources for the GML community to evaluate and implement security measures in GMLaaS environments.

Abstract: Graph-structured data, which captures non-Euclidean relationships and
interactions between entities, is growing in scale and complexity. As a result,
training state-of-the-art graph machine learning (GML) models have become
increasingly resource-intensive, turning these models and data into invaluable
Intellectual Property (IP). To address the resource-intensive nature of model
training, graph-based Machine-Learning-as-a-Service (GMLaaS) has emerged as an
efficient solution by leveraging third-party cloud services for model
development and management. However, deploying such models in GMLaaS also
exposes them to potential threats from attackers. Specifically, while the APIs
within a GMLaaS system provide interfaces for users to query the model and
receive outputs, they also allow attackers to exploit and steal model
functionalities or sensitive training data, posing severe threats to the safety
of these GML models and the underlying graph data. To address these challenges,
this survey systematically introduces the first taxonomy of threats and
defenses at the level of both GML model and graph-structured data. Such a
tailored taxonomy facilitates an in-depth understanding of GML IP protection.
Furthermore, we present a systematic evaluation framework to assess the
effectiveness of IP protection methods, introduce a curated set of benchmark
datasets across various domains, and discuss their application scopes and
future challenges. Finally, we establish an open-sourced versatile library
named PyGIP, which evaluates various attack and defense techniques in GMLaaS
scenarios and facilitates the implementation of existing benchmark methods. The
library resource can be accessed at: https://labrai.github.io/PyGIP. We believe
this survey will play a fundamental role in intellectual property protection
for GML and provide practical recipes for the GML community.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [271] [Should LLMs be WEIRD? Exploring WEIRDness and Human Rights in Large Language Models](https://arxiv.org/abs/2508.19269)
*Ke Zhou,Marios Constantinides,Daniele Quercia*

Main category: cs.CY

TL;DR: LLMs trained on WEIRD data show cultural bias. Evaluation of 5 models reveals trade-off: less WEIRD-aligned models produce more culturally diverse responses but are 2-4% more likely to violate human rights, particularly on gender equality issues.


<details>
  <summary>Details</summary>
Motivation: To investigate cultural bias in LLMs trained on Western-centric data and assess whether reduced WEIRD alignment leads to better cultural representation or increased human rights violations.

Method: Used World Values Survey responses to evaluate five LLMs (GPT-3.5, GPT-4, Llama-3, BLOOM, Qwen) against WEIRD country values and human rights principles from Universal Declaration of Human Rights and three regional charters.

Result: Models with lower WEIRD alignment (BLOOM, Qwen) produced more culturally varied responses but were 2-4% more likely to generate outputs violating human rights, especially regarding gender and equality norms.

Conclusion: Increased cultural representation in LLMs may paradoxically increase reproduction of discriminatory beliefs; Constitutional AI approaches may only partially resolve this tension between cultural diversity and human rights protection.

Abstract: Large language models (LLMs) are often trained on data that reflect WEIRD
values: Western, Educated, Industrialized, Rich, and Democratic. This raises
concerns about cultural bias and fairness. Using responses to the World Values
Survey, we evaluated five widely used LLMs: GPT-3.5, GPT-4, Llama-3, BLOOM, and
Qwen. We measured how closely these responses aligned with the values of the
WEIRD countries and whether they conflicted with human rights principles. To
reflect global diversity, we compared the results with the Universal
Declaration of Human Rights and three regional charters from Asia, the Middle
East, and Africa. Models with lower alignment to WEIRD values, such as BLOOM
and Qwen, produced more culturally varied responses but were 2% to 4% more
likely to generate outputs that violated human rights, especially regarding
gender and equality. For example, some models agreed with the statements ``a
man who cannot father children is not a real man'' and ``a husband should
always know where his wife is'', reflecting harmful gender norms. These
findings suggest that as cultural representation in LLMs increases, so does the
risk of reproducing discriminatory beliefs. Approaches such as Constitutional
AI, which could embed human rights principles into model behavior, may only
partly help resolve this tension.

</details>


### [272] [Epistemic Trade-Off: An Analysis of the Operational Breakdown and Ontological Limits of "Certainty-Scope" in AI](https://arxiv.org/abs/2508.19304)
*Generoso Immediato*

Main category: cs.CY

TL;DR: Floridi's conjecture about AI certainty-scope trade-off fails to become actionable due to incomputable constructs and unrealistic ontological assumptions, preventing practical application in real-world systems.


<details>
  <summary>Details</summary>
Motivation: To analyze why Floridi's compelling conjecture about AI certainty-scope trade-off cannot be operationalized for practical engineering design and regulatory decision-making in safety-critical domains.

Method: Critical analysis identifying two key constraints: reliance on incomputable constructs and assumption of AI as self-contained epistemic entities separate from socio-technical environments.

Result: The conjecture suffers from epistemic closure deficit and embeddedness bypass, making it unactionable and unverifiable for real-world AI system design and governance.

Conclusion: A new framing is needed to address Floridi's epistemic challenge that accounts for computability and the embedded nature of AI in complex human-centric socio-technical systems.

Abstract: Floridi's conjecture offers a compelling intuition about the fundamental
trade-off between certainty and scope in artificial intelligence (AI) systems.
This exploration remains crucial, not merely as a philosophical exercise, but
as a potential compass for guiding AI investments, particularly in
safety-critical industrial domains where the level of attention will surely be
higher in the future. However, while intellectually coherent, its formalization
ultimately freezes this insight into a suspended epistemic truth, resisting
operationalization within real-world systems. This paper is a result of an
analysis arguing that the conjecture's ambition to provide insights to
engineering design and regulatory decision-making is constrained by two
critical factors: first, its reliance on incomputable constructs - rendering it
practically unactionable and unverifiable; second, its underlying ontological
assumption of AI systems as self-contained epistemic entities - separating it
from the intricate and dynamic socio-technical environments in which knowledge
is co-constructed. We conclude that this dual breakdown - an epistemic closure
deficit and an embeddedness bypass - prevents the conjecture from transitioning
into a computable and actionable framework suitable for informing the design,
deployment, and governance of real-world AI hybrid systems. In response, we
propose a contribution to the framing of Floridi's epistemic challenge,
addressing the inherent epistemic burdens of AI within complex human-centric
domains.

</details>


### [273] [Are Companies Taking AI Risks Seriously? A Systematic Analysis of Companies' AI Risk Disclosures in SEC 10-K forms](https://arxiv.org/abs/2508.19313)
*Lucas G. Uberti-Bona Marin,Bram Rijsbosch,Gerasimos Spanakis,Konrad Kollnig*

Main category: cs.CY

TL;DR: First large-scale analysis of AI risk disclosures in SEC 10-K filings shows dramatic increase from 4% in 2020 to 43% in 2024, but disclosures often lack detail on mitigation strategies.


<details>
  <summary>Details</summary>
Motivation: Growing regulatory pressure for AI transparency and concerns about misleading AI claims require examination of how companies report AI-related risks to investors and the public.

Method: Quantitative and qualitative analysis of over 30,000 SEC 10-K filings from more than 7,000 companies over five years, examining AI risk disclosures.

Result: Sharp increase in companies mentioning AI risk (4% to 43%), with legal and competitive risks most common but growing attention to societal risks like cyberattacks and technical limitations. Many disclosures remain generic with insufficient mitigation details.

Conclusion: While AI risk reporting has increased significantly, the quality remains inadequate, echoing SEC concerns. The study provides a web-based tool for future research on SEC filing disclosures.

Abstract: As Artificial Intelligence becomes increasingly central to corporate
strategies, concerns over its risks are growing too. In response, regulators
are pushing for greater transparency in how companies identify, report and
mitigate AI-related risks. In the US, the Securities and Exchange Commission
(SEC) repeatedly warned companies to provide their investors with more accurate
disclosures of AI-related risks; recent enforcement and litigation against
companies' misleading AI claims reinforce these warnings. In the EU, new laws -
like the AI Act and Digital Services Act - introduced additional rules on AI
risk reporting and mitigation. Given these developments, it is essential to
examine if and how companies report AI-related risks to the public. This study
presents the first large-scale systematic analysis of AI risk disclosures in
SEC 10-K filings, which require public companies to report material risks to
their company. We analyse over 30,000 filings from more than 7,000 companies
over the past five years, combining quantitative and qualitative analysis. Our
findings reveal a sharp increase in the companies that mention AI risk, up from
4% in 2020 to over 43% in the most recent 2024 filings. While legal and
competitive AI risks are the most frequently mentioned, we also find growing
attention to societal AI risks, such as cyberattacks, fraud, and technical
limitations of AI systems. However, many disclosures remain generic or lack
details on mitigation strategies, echoing concerns raised recently by the SEC
about the quality of AI-related risk reporting. To support future research, we
publicly release a web-based tool for easily extracting and analysing
keyword-based disclosures across SEC filings.

</details>


### [274] [What Makes AI Applications Acceptable or Unacceptable? A Predictive Moral Framework](https://arxiv.org/abs/2508.19317)
*Kimmo Eriksson,Simon Karlsson,Irina Vartanova,Pontus Strimling*

Main category: cs.CY

TL;DR: Public moral resistance to AI applications is systematic and predictable using five core moral qualities that explain over 90% of variance in acceptability ratings.


<details>
  <summary>Details</summary>
Motivation: To anticipate which AI applications will face public moral resistance, as developers and policymakers struggle with this challenge in rapidly transforming AI society.

Method: Large preregistered study with N=587 US representative sample using comprehensive taxonomy of 100 AI applications spanning personal/organizational contexts, measuring acceptability ratings and five moral qualities.

Result: Five core moral qualities (perceived risk, benefit, dishonesty, unnaturalness, reduced accountability) collectively explained over 90% of variance in acceptability ratings, with strong predictive power across domains and individual-level judgments.

Conclusion: Public evaluation of new technologies follows a structured moral psychology, providing a powerful tool for anticipating resistance and guiding responsible AI innovation.

Abstract: As artificial intelligence rapidly transforms society, developers and
policymakers struggle to anticipate which applications will face public moral
resistance. We propose that these judgments are not idiosyncratic but
systematic and predictable. In a large, preregistered study (N = 587, U.S.
representative sample), we used a comprehensive taxonomy of 100 AI applications
spanning personal and organizational contexts-including both functional uses
and the moral treatment of AI itself. In participants' collective judgment,
applications ranged from highly unacceptable to fully acceptable. We found this
variation was strongly predictable: five core moral qualities-perceived risk,
benefit, dishonesty, unnaturalness, and reduced accountability-collectively
explained over 90% of the variance in acceptability ratings. The framework
demonstrated strong predictive power across all domains and successfully
predicted individual-level judgments for held-out applications. These findings
reveal that a structured moral psychology underlies public evaluation of new
technologies, offering a powerful tool for anticipating public resistance and
guiding responsible innovation in AI.

</details>


### [275] [Geopolitical Parallax: Beyond Walter Lippmann Just After Large Language Models](https://arxiv.org/abs/2508.19492)
*Mehmet Can Yavuz,Humza Gohar Kabir,Aylin Özkan*

Main category: cs.CY

TL;DR: Study reveals systematic geopolitical biases in LLM-based news quality assessment, with Chinese and Western models showing divergent scoring patterns aligned with their cultural origins, particularly on sensitive topics like Palestine coverage.


<details>
  <summary>Details</summary>
Motivation: To investigate how large language models from different geopolitical origins (Chinese vs. Western) exhibit systematic biases in news quality and subjectivity assessments, potentially embedding cultural or ideological biases in algorithmic journalism evaluation.

Method: Compared article-level embeddings from Chinese-origin (Qwen, BGE, Jina) and Western-origin (Snowflake, Granite) models on human-annotated news quality benchmarks. Used logistic regression probes and matched-topic evaluation to quantify differences in predicted probabilities across 15 stylistic, informational, and affective dimensions on politically sensitive topics.

Result: Consistent, non-random divergences aligned with model origin: Western models assigned higher subjectivity and positive emotion scores for Palestine coverage, while Chinese models emphasized novelty and descriptiveness. Chinese models scored US coverage lower in fluency, conciseness, technicality, and overall quality but higher in negative emotion.

Conclusion: LLM-based media evaluation pipelines require cultural calibration to avoid conflating content differences with model-induced bias, as geopolitical framing effects persist in downstream quality assessment tasks.

Abstract: Objectivity in journalism has long been contested, oscillating between ideals
of neutral, fact-based reporting and the inevitability of subjective framing.
With the advent of large language models (LLMs), these tensions are now
mediated by algorithmic systems whose training data and design choices may
themselves embed cultural or ideological biases. This study investigates
geopolitical parallax-systematic divergence in news quality and subjectivity
assessments-by comparing article-level embeddings from Chinese-origin (Qwen,
BGE, Jina) and Western-origin (Snowflake, Granite) model families. We evaluate
both on a human-annotated news quality benchmark spanning fifteen stylistic,
informational, and affective dimensions, and on parallel corpora covering
politically sensitive topics, including Palestine and reciprocal China-United
States coverage. Using logistic regression probes and matched-topic evaluation,
we quantify per-metric differences in predicted positive-class probabilities
between model families. Our findings reveal consistent, non-random divergences
aligned with model origin. In Palestine-related coverage, Western models assign
higher subjectivity and positive emotion scores, while Chinese models emphasize
novelty and descriptiveness. Cross-topic analysis shows asymmetries in
structural quality metrics Chinese-on-US scoring notably lower in fluency,
conciseness, technicality, and overall quality-contrasted by higher negative
emotion scores. These patterns align with media bias theory and our distinction
between semantic, emotional, and relational subjectivity, and extend LLM bias
literature by showing that geopolitical framing effects persist in downstream
quality assessment tasks. We conclude that LLM-based media evaluation pipelines
require cultural calibration to avoid conflating content differences with
model-induced bias.

</details>


### [276] [Hallucinating with AI: AI Psychosis as Distributed Delusions](https://arxiv.org/abs/2508.19588)
*Lucy Osler*

Main category: cs.CY

TL;DR: The paper argues that AI hallucinations should be viewed through distributed cognition theory, showing how human-AI interactions can lead to shared delusional thinking where humans hallucinate with AI rather than AI hallucinating at humans.


<details>
  <summary>Details</summary>
Motivation: To challenge the popular concept of "AI hallucinations" and provide a more nuanced understanding of how inaccurate beliefs and delusional thinking emerge through human-AI interactions using distributed cognition theory.

Method: The paper employs distributed cognition theory to analyze human-AI interactions, examining how AI systems function both as cognitive artifacts and quasi-Others that co-construct beliefs and realities with humans.

Result: The analysis reveals that generative AI's dual function makes it a particularly seductive case of distributed cognition, where AI can both introduce errors into cognitive processes and sustain/affirm human delusional thinking.

Conclusion: We should shift from viewing AI as hallucinating at humans to understanding how humans can hallucinate with AI, recognizing the unique risks posed by AI's dual role in distributed cognitive processes.

Abstract: There is much discussion of the false outputs that generative AI systems such
as ChatGPT, Claude, Gemini, DeepSeek, and Grok create. In popular terminology,
these have been dubbed AI hallucinations. However, deeming these AI outputs
hallucinations is controversial, with many claiming this is a metaphorical
misnomer. Nevertheless, in this paper, I argue that when viewed through the
lens of distributed cognition theory, we can better see the dynamic and
troubling ways in which inaccurate beliefs, distorted memories and
self-narratives, and delusional thinking can emerge through human-AI
interactions; examples of which are popularly being referred to as cases of AI
psychosis. In such cases, I suggest we move away from thinking about how an AI
system might hallucinate at us, by generating false outputs, to thinking about
how, when we routinely rely on generative AI to help us think, remember, and
narrate, we can come to hallucinate with AI. This can happen when AI introduces
errors into the distributed cognitive process, but it can also happen when AI
sustains, affirms, and elaborates on our own delusional thinking and
self-narratives, such as in the case of Jaswant Singh Chail. I also examine how
the conversational style of chatbots can lead them to play a dual-function,
both as a cognitive artefact and a quasi-Other with whom we co-construct our
beliefs, narratives, and our realities. It is this dual function, I suggest,
that makes generative AI an unusual, and particularly seductive, case of
distributed cognition.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [277] [Modeling spectral filtering effects on color-matching functions: Implications for observer variability](https://arxiv.org/abs/2508.19291)
*Luvin Munish Ragoo,Ivar Farup,Casper F. Andersen,Graham Finlayson*

Main category: astro-ph.IM

TL;DR: Study shows spectral filtering can model observer variability in color vision, with yellow filters explaining age-related differences between CMF datasets through a single filter approach.


<details>
  <summary>Details</summary>
Motivation: To understand how spectral filtering affects color-matching functions and develop a more efficient method for modeling observer variability, particularly age-related differences in color vision.

Method: Conducted color matching experiments with/without spectral filters, used computational approach to estimate filter transmittance and transformation matrices, compared Stiles and Burch 1955 CMFs with ICVIO CMFs.

Result: Found good agreement between estimated and measured filter characteristics, identified a yellow filter that transforms between different CMF datasets, supporting age-related lens yellowing as cause of differences.

Conclusion: Single spectral filter approach efficiently represents observer variability, reducing experimental overhead while maintaining accuracy in characterizing individual color vision differences.

Abstract: This study investigates the impact of spectral filtering on color-matching
functions (CMFs) and its implications for observer variability modeling. We
conducted color matching experiments with a single observer, both with and
without a spectral filter in front of a bipartite field. Using a novel
computational approach, we estimated the filter transmittance and
transformation matrix necessary to convert unfiltered CMFs to filtered CMFs.
Statistical analysis revealed good agreement between estimated and measured
filter characteristics, particularly in central wavelength regions. Applying
this methodology to compare between Stiles and Burch 1955 (SB1955) mean
observer CMFs and our previously published "ICVIO" mean observer CMFs, we
identified a "yellow" (short-wavelength suppressing) filter that effectively
transforms between these datasets. This finding aligns with our hypothesis that
observed differences between the CMF sets are attributable to age-related lens
yellowing (average observer age: 49 years in ICVIO versus 30 years in SB1955).
Our approach enables efficient representation of observer variability through a
single filter rather than three separate functions, offering potentially
reduced experimental overhead while maintaining accuracy in characterizing
individual color vision differences.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [278] [Training for Obsolescence? The AI-Driven Education Trap](https://arxiv.org/abs/2508.19625)
*Andrew J. Peterson*

Main category: econ.GN

TL;DR: AI in education creates skill mismatch by improving teaching productivity while suppressing future wages for those same skills, leading to educational resource misallocation that worsens with AI prevalence.


<details>
  <summary>Details</summary>
Motivation: To analyze how AI's dual role in education (improving teaching) and labor markets (suppressing wages) creates systematic skill mismatches when considered in isolation, potentially undermining long-term human capital development.

Method: Model an educational planner who adopts AI based on teaching productivity benefits without internalizing future wage-suppressing effects, assuming positive correlation between these two effects. Extensions consider unpriced non-cognitive skills and endogenous AI over-investment.

Result: The information failure creates a skill mismatch that increases monotonically with AI prevalence. The mismatch is exacerbated by neglect of non-cognitive skills and schools' endogenous over-investment in AI.

Conclusion: Policies promoting AI in education without forward-looking labor market signals may paradoxically undermine students' long-term human capital, especially if AI crowds out development of unpriced non-cognitive skills like persistence developed through intellectual struggle.

Abstract: Artificial intelligence simultaneously transforms human capital production in
schools and its demand in labor markets. Analyzing these effects in isolation
can lead to a significant misallocation of educational resources. We model an
educational planner whose decision to adopt AI is driven by its teaching
productivity, failing to internalize AI's future wage-suppressing effect on
those same skills. Our core assumption, motivated by a pilot survey, is that
there is a positive correlation between these two effects. This drives our
central proposition: this information failure creates a skill mismatch that
monotonically increases with AI prevalence. Extensions show the mismatch is
exacerbated by the neglect of unpriced non-cognitive skills and by a school's
endogenous over-investment in AI. Our findings caution that policies promoting
AI in education, if not paired with forward-looking labor market signals, may
paradoxically undermine students' long-term human capital, especially if
reliance on AI crowds out the development of unpriced non-cognitive skills,
such as persistence, that are forged through intellectual struggle.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [279] [Federated Fine-Tuning of Sparsely-Activated Large Language Models on Resource-Constrained Devices](https://arxiv.org/abs/2508.19078)
*Fahao Chen,Jie Wan,Peng Li,Zhou Su,Dongxiao Yu*

Main category: cs.DC

TL;DR: FLUX enables efficient federated fine-tuning of MoE-based LLMs on resource-constrained devices through quantization-based profiling, adaptive expert merging, and dynamic role assignment, achieving 4.75x speedup.


<details>
  <summary>Details</summary>
Motivation: Federated fine-tuning of MoE-based LLMs is challenging due to massive computational requirements and resource constraints of participants. Existing methods fail due to impractical assumptions and lack of MoE-specific considerations.

Method: FLUX introduces three innovations: (1) quantization-based local profiling for expert activation estimation, (2) adaptive layer-aware expert merging to reduce resource consumption, and (3) dynamic expert role assignment using exploration-exploitation strategy.

Result: Extensive experiments on LLaMA-MoE and DeepSeek-MoE show FLUX significantly outperforms existing methods, achieving up to 4.75X speedup in time-to-accuracy.

Conclusion: FLUS successfully enables federated fine-tuning of MoE-based LLMs across participants with constrained computing resources while minimizing time-to-accuracy.

Abstract: Federated fine-tuning of Mixture-of-Experts (MoE)-based large language models
(LLMs) is challenging due to their massive computational requirements and the
resource constraints of participants. Existing working attempts to fill this
gap through model quantization, computation offloading, or expert pruning.
However, they cannot achieve desired performance due to impractical system
assumptions and a lack of consideration for MoE-specific characteristics. In
this paper, we propose FLUX, a system designed to enable federated fine-tuning
of MoE-based LLMs across participants with constrained computing resources
(e.g., consumer-grade GPUs), aiming to minimize time-to-accuracy. FLUX
introduces three key innovations: (1) quantization-based local profiling to
estimate expert activation with minimal overhead, (2) adaptive layer-aware
expert merging to reduce resource consumption while preserving accuracy, and
(3) dynamic expert role assignment using an exploration-exploitation strategy
to balance tuning and non-tuning experts. Extensive experiments on LLaMA-MoE
and DeepSeek-MoE with multiple benchmark datasets demonstrate that FLUX
significantly outperforms existing methods, achieving up to 4.75X speedup in
time-to-accuracy.

</details>


### [280] [Towards 6G Intelligence: The Role of Generative AI in Future Wireless Networks](https://arxiv.org/abs/2508.19495)
*Muhammad Ahmed Mohsin,Junaid Ahmad,Muhammad Hamza Nawaz,Muhammad Ali Jamshed*

Main category: cs.DC

TL;DR: GenAI is the creative core for 6G-enabled ambient intelligence, enabling synthetic data generation, semantic communication, predictive control, and privacy-preserving digital twins through models like GANs, VAEs, diffusion models, and transformers.


<details>
  <summary>Details</summary>
Motivation: To realize ambient intelligence at global scale through 6G networks that can perceive, reason, and act autonomously in alignment with human behavior and mobility patterns, addressing key gaps in current AmI systems.

Method: Review of foundational GenAI models (GANs, VAEs, diffusion models, generative transformers) and their application to practical AmI use cases including spectrum sharing, URLLC, intelligent security, and context-aware digital twins, leveraging 6G enablers like edge computing, IoT swarms, IRS, and non-terrestrial networks.

Result: GenAI is identified as a foundational element that can close key AmI gaps by generating synthetic sensor data, translating user intent, predicting network conditions, and updating digital twins while preserving privacy.

Conclusion: GenAI transforms 6G from a faster network into an ambient intelligent ecosystem, though challenges remain in energy-efficient on-device training, trustworthy synthetic data, federated generative learning, and AmI-specific standardization.

Abstract: Ambient intelligence (AmI) is a computing paradigm in which physical
environments are embedded with sensing, computation, and communication so they
can perceive people and context, decide appropriate actions, and respond
autonomously. Realizing AmI at global scale requires sixth generation (6G)
wireless networks with capabilities for real time perception, reasoning, and
action aligned with human behavior and mobility patterns. We argue that
Generative Artificial Intelligence (GenAI) is the creative core of such
environments. Unlike traditional AI, GenAI learns data distributions and can
generate realistic samples, making it well suited to close key AmI gaps,
including generating synthetic sensor and channel data in under observed areas,
translating user intent into compact, semantic messages, predicting future
network conditions for proactive control, and updating digital twins without
compromising privacy.
  This chapter reviews foundational GenAI models, GANs, VAEs, diffusion models,
and generative transformers, and connects them to practical AmI use cases,
including spectrum sharing, ultra reliable low latency communication,
intelligent security, and context aware digital twins. We also examine how 6G
enablers, such as edge and fog computing, IoT device swarms, intelligent
reflecting surfaces (IRS), and non terrestrial networks, can host or accelerate
distributed GenAI. Finally, we outline open challenges in energy efficient on
device training, trustworthy synthetic data, federated generative learning, and
AmI specific standardization. We show that GenAI is not a peripheral addition,
but a foundational element for transforming 6G from a faster network into an
ambient intelligent ecosystem.

</details>


### [281] [Taming the Chaos: Coordinated Autoscaling for Heterogeneous and Disaggregated LLM Inference](https://arxiv.org/abs/2508.19559)
*Rongzhi Li,Ruogu Du,Zefang Chu,Sida Zhao,Chunlei Han,Zuocheng Shi,Yiwen Shao,Huanle Han,Long Huang,Zherui Liu,Shufan Liu*

Main category: cs.DC

TL;DR: HeteroScale is an autoscaling framework for LLM serving that addresses challenges in Prefill-Decode disaggregated architectures, improving GPU utilization by 26.6% and saving hundreds of thousands of GPU-hours daily.


<details>
  <summary>Details</summary>
Motivation: Traditional autoscalers are inadequate for modern Prefill-Decode disaggregated LLM serving architectures, which face issues like inefficient hardware use, network bottlenecks, and stage imbalances.

Method: Combines topology-aware scheduler for heterogeneous hardware with novel metric-driven policy based on large-scale empirical study, using a single robust metric to jointly scale prefill and decode pools.

Result: Deployed on tens of thousands of GPUs, increased average GPU utilization by 26.6 percentage points and saved hundreds of thousands of GPU-hours daily while maintaining service level objectives.

Conclusion: HeteroScale effectively solves core challenges in P/D disaggregated serving through coordinated autoscaling, demonstrating significant efficiency gains in large-scale production environments.

Abstract: Serving Large Language Models (LLMs) is a GPU-intensive task where
traditional autoscalers fall short, particularly for modern Prefill-Decode
(P/D) disaggregated architectures. This architectural shift, while powerful,
introduces significant operational challenges, including inefficient use of
heterogeneous hardware, network bottlenecks, and critical imbalances between
prefill and decode stages. We introduce HeteroScale, a coordinated autoscaling
framework that addresses the core challenges of P/D disaggregated serving.
HeteroScale combines a topology-aware scheduler that adapts to heterogeneous
hardware and network constraints with a novel metric-driven policy derived from
the first large-scale empirical study of autoscaling signals in production. By
leveraging a single, robust metric to jointly scale prefill and decode pools,
HeteroScale maintains architectural balance while ensuring efficient, adaptive
resource management. Deployed in a massive production environment on tens of
thousands of GPUs, HeteroScale has proven its effectiveness, increasing average
GPU utilization by a significant 26.6 percentage points and saving hundreds of
thousands of GPU-hours daily, all while upholding stringent service level
objectives.

</details>


### [282] [HPC Digital Twins for Evaluating Scheduling Policies, Incentive Structures and their Impact on Power and Cooling](https://arxiv.org/abs/2508.20016)
*Matthias Maiterth,Wesley H. Brewer,Jaya S. Kuruvella,Arunavo Dey,Tanzima Z. Islam,Kevin Menear,Dmitry Duplyakin,Rashadul Kabir,Tapasya Patki,Terry Jones,Feiyi Wang*

Main category: cs.DC

TL;DR: First integration of scheduling and digital twins in HPC for pre-deployment what-if analysis of scheduler configurations and decisions on physical infrastructure.


<details>
  <summary>Details</summary>
Motivation: Traditional scheduler evaluation methods are limited to post-deployment analysis or simulators that don't model associated infrastructure, creating a need for better pre-deployment evaluation tools.

Method: Developed a digital twin framework extended with scheduling capabilities, integrated various HPC systems using public datasets, implemented extensions for external scheduling simulators, and created evaluation methods for incentive structures and ML-based scheduling.

Result: Created a novel digital-twin based meta-framework that enables what-if scenario analysis of HPC systems to evaluate sustainability and system impacts before deployment.

Conclusion: This work provides the first comprehensive integration of scheduling with digital twins in HPC, enabling prototype scheduling evaluation and what-if studies that were previously not possible with traditional methods.

Abstract: Schedulers are critical for optimal resource utilization in high-performance
computing. Traditional methods to evaluate schedulers are limited to
post-deployment analysis, or simulators, which do not model associated
infrastructure. In this work, we present the first-of-its-kind integration of
scheduling and digital twins in HPC. This enables what-if studies to understand
the impact of parameter configurations and scheduling decisions on the physical
assets, even before deployment, or regarching changes not easily realizable in
production. We (1) provide the first digital twin framework extended with
scheduling capabilities, (2) integrate various top-tier HPC systems given their
publicly available datasets, (3) implement extensions to integrate external
scheduling simulators. Finally, we show how to (4) implement and evaluate
incentive structures, as-well-as (5) evaluate machine learning based
scheduling, in such novel digital-twin based meta-framework to prototype
scheduling. Our work enables what-if scenarios of HPC systems to evaluate
sustainability, and the impact on the simulated system.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [283] [GENIE-ASI: Generative Instruction and Executable Code for Analog Subcircuit Identification](https://arxiv.org/abs/2508.19393)
*Phuoc Pham,Arun Venkitaraman,Chia-Yu Hsieh,Andrea Bonetti,Stefan Uhlich,Markus Leibl,Simon Hofmann,Eisaku Ohbuchi,Lorenzo Servadei,Ulf Schlichtmann,Robert Wille*

Main category: cs.AR

TL;DR: GENIE-ASI is a training-free LLM-based method for analog subcircuit identification that uses in-context learning to generate executable code from natural language instructions, achieving competitive performance across various circuit complexities.


<details>
  <summary>Details</summary>
Motivation: Traditional analog subcircuit identification methods require extensive human expertise, rule-based encoding, or large labeled datasets, creating barriers to automation in analog design.

Method: Two-phase approach: 1) Uses in-context learning to derive natural language instructions from demonstration examples, 2) Translates these into executable Python code to identify subcircuits in unseen SPICE netlists.

Result: Achieves F1-score of 1.0 on simple structures, 0.81 on moderate abstractions, and 0.31 on complex subcircuits, matching rule-based performance on simple cases while showing potential on complex ones.

Conclusion: LLMs can serve as adaptable, general-purpose tools in analog design automation, opening new research directions for foundation model applications in this domain.

Abstract: Analog subcircuit identification is a core task in analog design, essential
for simulation, sizing, and layout. Traditional methods often require extensive
human expertise, rule-based encoding, or large labeled datasets. To address
these challenges, we propose GENIE-ASI, the first training-free, large language
model (LLM)-based methodology for analog subcircuit identification. GENIE-ASI
operates in two phases: it first uses in-context learning to derive natural
language instructions from a few demonstration examples, then translates these
into executable Python code to identify subcircuits in unseen SPICE netlists.
In addition, to evaluate LLM-based approaches systematically, we introduce a
new benchmark composed of operational amplifier netlists (op-amps) that cover a
wide range of subcircuit variants. Experimental results on the proposed
benchmark show that GENIE-ASI matches rule-based performance on simple
structures (F1-score = 1.0), remains competitive on moderate abstractions
(F1-score = 0.81), and shows potential even on complex subcircuits (F1-score =
0.31). These findings demonstrate that LLMs can serve as adaptable,
general-purpose tools in analog design automation, opening new research
directions for foundation model applications in analog design automation.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [284] [Quantum Entanglement as Super-Confounding: From Bell's Theorem to Robust Machine Learning](https://arxiv.org/abs/2508.19327)
*Pilsung Kang*

Main category: quant-ph

TL;DR: Quantum entanglement acts as super-confounding resource that violates classical causal bounds, with framework quantifying confounding strength and applying quantum causal calculus to improve ML robustness.


<details>
  <summary>Details</summary>
Motivation: To reinterpret Bell's theorem through causal inference lens, showing quantum entanglement creates correlations that exceed classical causal limits and bridge quantum foundations with causal AI.

Method: Proposed framework with physical hierarchy of confounding, introduced Confounding Strength metric, developed circuit-based quantum DO-calculus implementation, and applied causal feature selection to quantum ML problems.

Result: Established Quantum > Classical confounding hierarchy, quantified confounding effect, and achieved 11.3% average absolute improvement in model robustness through causal feature selection in quantum ML.

Conclusion: Quantum entanglement serves as super-confounding resource that violates classical causal bounds, providing practical framework connecting quantum correlations with causal inference for improved AI applications.

Abstract: Bell's theorem reveals a profound conflict between quantum mechanics and
local realism, a conflict we reinterpret through the modern lens of causal
inference. We propose and computationally validate a framework where quantum
entanglement acts as a "super-confounding" resource, generating correlations
that violate the classical causal bounds set by Bell's inequalities. This work
makes three key contributions: First, we establish a physical hierarchy of
confounding (Quantum > Classical) and introduce Confounding Strength (CS) to
quantify this effect. Second, we provide a circuit-based implementation of the
quantum $\mathcal{DO}$-calculus to distinguish causality from spurious
correlation. Finally, we apply this calculus to a quantum machine learning
problem, where causal feature selection yields a statistically significant
11.3% average absolute improvement in model robustness. Our framework bridges
quantum foundations and causal AI, offering a new, practical perspective on
quantum correlations.

</details>


### [285] [Is data-efficient learning feasible with quantum models?](https://arxiv.org/abs/2508.19437)
*Alona Sakhnenko,Christian B. Mendl,Jeanette M. Lorenz*

Main category: quant-ph

TL;DR: QML models show superior data-efficiency over classical models, with quantum kernel methods achieving lower error rates using less training data on semi-artificial classical datasets.


<details>
  <summary>Details</summary>
Motivation: The need for a cohesive framework to understand dataset characteristics in quantum machine learning testing, particularly focusing on dataset size as a complexity indicator and exploring QML's potential data-efficiency advantages.

Method: Generated semi-artificial fully classical datasets and applied quantum kernel methods (QKMs), introducing a new analytical tool derived from classical kernel methods to investigate the classical-quantum performance gap.

Result: Empirical evidence shows QKMs achieve low error rates with less training data compared to classical counterparts. The proposed generalization metric from classical domain aligns well with empirical performance, validating the analytical tool.

Conclusion: This research provides insights into dataset complexities influencing QML performance, contributes to understanding generalization benefits of QKM models, and sets the stage for future advancements in quantum machine learning.

Abstract: The importance of analyzing nontrivial datasets when testing quantum machine
learning (QML) models is becoming increasingly prominent in literature, yet a
cohesive framework for understanding dataset characteristics remains elusive.
In this work, we concentrate on the size of the dataset as an indicator of its
complexity and explores the potential for QML models to demonstrate superior
data-efficiency compared to classical models, particularly through the lens of
quantum kernel methods (QKMs). We provide a method for generating
semi-artificial fully classical datasets, on which we show one of the first
evidence of the existence of classical datasets where QKMs require less data
during training. Additionally, our study introduces a new analytical tool to
the QML domain, derived for classical kernel methods, which can be aimed at
investigating the classical-quantum gap. Our empirical results reveal that QKMs
can achieve low error rates with less training data compared to classical
counterparts. Furthermore, our method allows for the generation of datasets
with varying properties, facilitating further investigation into the
characteristics of real-world datasets that may be particularly advantageous
for QKMs. We also show that the predicted performance from the analytical tool
we propose - a generalization metric from classical domain - show great
alignment empirical evidence, which fills the gap previously existing in the
field. We pave a way to a comprehensive exploration of dataset complexities,
providing insights into how these complexities influence QML performance
relative to traditional methods. This research contributes to a deeper
understanding of the generalization benefits of QKM models and potentially a
broader family of QML models, setting the stage for future advancements in the
field.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [286] [Functional Consistency of LLM Code Embeddings: A Self-Evolving Data Synthesis Framework for Benchmarking](https://arxiv.org/abs/2508.19558)
*Zhuohao Li,Wenqing Chen,Jianxing Yu,Zhichao Lu*

Main category: cs.SE

TL;DR: This paper introduces a novel data synthesis framework called Functionality-Oriented Code Self-Evolution to create benchmarks for evaluating LLM code embeddings' ability to capture functional semantics rather than just syntactic similarity.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks and studies focus primarily on code clone detection which emphasizes syntactic similarity, but overlook the functional understanding of code. There's a need to evaluate whether LLM code embeddings can accurately reflect code-level functional semantics.

Method: Proposed a data synthesis framework that generates four unique variations from a single code instance across four semantic and syntactic categories, creating diverse benchmarks that better reflect functional differences.

Result: Extensive experiments on three downstream tasks (code clone detection, code functional consistency identification, and code retrieval) show that embedding models significantly improve performance when trained on the evolved datasets.

Conclusion: The framework effectively advances functional understanding of code and demonstrates strong generalization capabilities, highlighting the importance of functional semantics in code embedding evaluation.

Abstract: Embedding models have demonstrated strong performance in tasks like
clustering, retrieval, and feature extraction while offering computational
advantages over generative models and cross-encoders. Benchmarks such as MTEB
have shown that text embeddings from large language models (LLMs) capture rich
semantic information, but their ability to reflect code-level functional
semantics remains unclear. Existing studies largely focus on code clone
detection, which emphasizes syntactic similarity and overlooks functional
understanding. In this paper, we focus on the functional consistency of LLM
code embeddings, which determines if two code snippets perform the same
function regardless of syntactic differences. We propose a novel data synthesis
framework called Functionality-Oriented Code Self-Evolution to construct
diverse and challenging benchmarks. Specifically, we define code examples
across four semantic and syntactic categories and find that existing datasets
predominantly capture syntactic properties. Our framework generates four unique
variations from a single code instance, providing a broader spectrum of code
examples that better reflect functional differences. Extensive experiments on
three downstream tasks-code clone detection, code functional consistency
identification, and code retrieval-demonstrate that embedding models
significantly improve their performance when trained on our evolved datasets.
These results highlight the effectiveness and generalization of our data
synthesis framework, advancing the functional understanding of code.

</details>


### [287] [Stack Trace-Based Crash Deduplication with Transformer Adaptation](https://arxiv.org/abs/2508.19449)
*Md Afif Al Mamun,Gias Uddin,Lan Xia,Longyu Zhang*

Main category: cs.SE

TL;DR: dedupT is a transformer-based approach for crash report deduplication that outperforms existing methods by modeling stack traces holistically rather than as isolated frames, achieving significant improvements in duplicate ranking and unique crash detection.


<details>
  <summary>Details</summary>
Motivation: Automated crash reporting systems generate large volumes of duplicate reports that overwhelm issue-tracking systems and increase developer workload. Traditional deduplication methods often fail to capture contextual and structural relationships within stack traces.

Method: dedupT adapts a pretrained language model to stack traces and uses its embeddings to train a fully-connected network for ranking duplicate crashes effectively. It models stack traces holistically rather than as isolated frames.

Result: On four public datasets, dedupT improves Mean Reciprocal Rank by over 15% compared to the best DL baseline and up to 9% over traditional methods, while achieving higher ROC-AUC in detecting unique crash reports.

Conclusion: The work advances the integration of modern NLP techniques into software engineering, providing an effective solution for stack trace-based crash deduplication and significantly reducing manual triage effort.

Abstract: Automated crash reporting systems generate large volumes of duplicate
reports, overwhelming issue-tracking systems and increasing developer workload.
Traditional stack trace-based deduplication methods, relying on string
similarity, rule-based heuristics, or deep learning (DL) models, often fail to
capture the contextual and structural relationships within stack traces. We
propose dedupT, a transformer-based approach that models stack traces
holistically rather than as isolated frames. dedupT first adapts a pretrained
language model (PLM) to stack traces, then uses its embeddings to train a
fully-connected network (FCN) to rank duplicate crashes effectively. Extensive
experiments on real-world datasets show that dedupT outperforms existing DL and
traditional methods (e.g., sequence alignment and information retrieval
techniques) in both duplicate ranking and unique crash detection, significantly
reducing manual triage effort. On four public datasets, dedupT improves Mean
Reciprocal Rank (MRR) often by over 15% compared to the best DL baseline and up
to 9% over traditional methods while achieving higher Receiver Operating
Characteristic Area Under the Curve (ROC-AUC) in detecting unique crash
reports. Our work advances the integration of modern natural language
processing (NLP) techniques into software engineering, providing an effective
solution for stack trace-based crash deduplication.

</details>


### [288] [Generative AI for Testing of Autonomous Driving Systems: A Survey](https://arxiv.org/abs/2508.19882)
*Qunying Song,He Ye,Mark Harman,Federica Sarro*

Main category: cs.SE

TL;DR: A systematic review of 91 studies on using generative AI for autonomous driving system testing, focusing on scenario-based testing approaches, effectiveness evaluation, and identifying 27 limitations with future research directions.


<details>
  <summary>Details</summary>
Motivation: Autonomous driving systems require extensive testing for safety validation, but effective and efficient testing remains challenging. Generative AI's ability to interpret context, reason about complex tasks, and generate diverse outputs makes it promising for ADS testing.

Method: Systematic analysis of 91 relevant studies, synthesizing findings into six major application categories centered on scenario-based testing, reviewing effectiveness, and compiling datasets, simulators, metrics, and benchmarks.

Result: Identified six major application categories for generative AI in ADS testing, compiled comprehensive evaluation resources (datasets, simulators, metrics, benchmarks), and identified 27 limitations in current approaches.

Conclusion: Generative AI shows significant potential for ADS testing but faces multiple challenges. The survey provides practical insights, highlights existing limitations, and outlines future research directions for this rapidly evolving field.

Abstract: Autonomous driving systems (ADS) have been an active area of research, with
the potential to deliver significant benefits to society. However, before
large-scale deployment on public roads, extensive testing is necessary to
validate their functionality and safety under diverse driving conditions.
Therefore, different testing approaches are required, and achieving effective
and efficient testing of ADS remains an open challenge. Recently, generative AI
has emerged as a powerful tool across many domains, and it is increasingly
being applied to ADS testing due to its ability to interpret context, reason
about complex tasks, and generate diverse outputs. To gain a deeper
understanding of its role in ADS testing, we systematically analyzed 91
relevant studies and synthesized their findings into six major application
categories, primarily centered on scenario-based testing of ADS. We also
reviewed their effectiveness and compiled a wide range of datasets, simulators,
ADS, metrics, and benchmarks used for evaluation, while identifying 27
limitations. This survey provides an overview and practical insights into the
use of generative AI for testing ADS, highlights existing challenges, and
outlines directions for future research in this rapidly evolving field.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [289] [Saccade crossing avoidance as a visual search strategy](https://arxiv.org/abs/2508.18404)
*Alex Szorkovszky,Rujeena Mathema,Pedro Lencastre,Pedro Lind,Anis Yazidi*

Main category: q-bio.NC

TL;DR: A new memory-dependent oculomotor bias called self-crossing avoidance was discovered, where saccades tend to avoid crossing earlier scan paths, particularly with small amplitudes and strongest over the last ~7 seconds.


<details>
  <summary>Details</summary>
Motivation: To quantify the impact of longer path history on visual search behavior and identify new memory-dependent effects in oculomotor patterns during complex scene viewing.

Method: Used step-selection framework from movement ecology, analyzed 45-second viewings of "Where's Waldo" images, compared real data to synthetic memoryless models, employed maximum likelihood fitting and parametric probabilistic modeling with self-crossing penalty terms.

Result: Discovered self-crossing avoidance effect that varies between individuals, strongest with small amplitude saccades and over ~7-second history. Effect size comparable to inhibition of return. Participants avoiding crossings had smaller saccades and shorter fixations.

Conclusion: Self-crossing avoidance is a local orienting strategy that complements inhibition of return and facilitates visual scene exploration, representing a new memory-dependent component in oculomotor control.

Abstract: Although visual search appears largely random, several oculomotor biases
exist such that the likelihoods of saccade directions and lengths depend on the
previous scan path. Compared to the most recent fixations, the impact of the
longer path history is more difficult to quantify. Using the step-selection
framework commonly used in movement ecology, and analyzing data from 45-second
viewings of "Where's Waldo"?, we report a new memory-dependent effect that also
varies significantly between individuals, which we term self-crossing
avoidance. This is a tendency for saccades to avoid crossing those earlier in
the scan path, and is most evident when both have small amplitudes. We show
this by comparing real data to synthetic data generated from a memoryless
approximation of the spatial statistics (i.e. a Markovian nonparametric model
with a matching distribution of saccade lengths over time). Maximum likelihood
fitting indicates that this effect is strongest when including the last
$\approx 7$ seconds of a scan path. The effect size is comparable to well-known
forms of history dependence such as inhibition of return. A parametric
probabilistic model including a self-crossing penalty term was able to
reproduce joint statistics of saccade lengths and self-crossings. We also
quantified individual strategic differences, and their consistency over the six
images viewed per participant, using mixed-effect regressions. Participants
with a higher tendency to avoid crossings displayed smaller saccade lengths and
shorter fixation durations on average, but did not display more horizontal,
vertical, forward or reverse saccades. Together, these results indicate that
the avoidance of crossings is a local orienting strategy that facilitates and
complements inhibition of return, and hence exploration of visual scenes.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [290] [Anomaly Detection in Networked Bandits](https://arxiv.org/abs/2508.20076)
*Xiaotong Cheng,Setareh Maghsudi*

Main category: cs.MA

TL;DR: A novel bandit algorithm for social networks that learns user preferences while detecting anomalies through network analysis of preferences and feature residuals.


<details>
  <summary>Details</summary>
Motivation: Abnormal nodes in social networks can cause serious consequences, requiring robust online learning algorithms that simultaneously learn user preferences and detect anomalies.

Method: Uses network knowledge to characterize user preferences and feature residuals, develops personalized recommendation strategies while detecting anomalies through analysis of these patterns.

Result: The algorithm achieves a proven upper bound on regret and outperforms state-of-the-art collaborative contextual bandit algorithms in experiments on synthetic and real-world datasets.

Conclusion: The proposed method effectively combines network-aware preference learning with anomaly detection, providing robust performance for social network recommendation systems.

Abstract: The nodes' interconnections on a social network often reflect their
dependencies and information-sharing behaviors. Nevertheless, abnormal nodes,
which significantly deviate from most of the network concerning patterns or
behaviors, can lead to grave consequences. Therefore, it is imperative to
design efficient online learning algorithms that robustly learn users'
preferences while simultaneously detecting anomalies.
  We introduce a novel bandit algorithm to address this problem. Through
network knowledge, the method characterizes the users' preferences and
residuals of feature information. By learning and analyzing these preferences
and residuals, it develops a personalized recommendation strategy for each user
and simultaneously detects anomalies. We rigorously prove an upper bound on the
regret of the proposed algorithm and experimentally compare it with several
state-of-the-art collaborative contextual bandit algorithms on both synthetic
and real-world datasets.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [291] [Fast Texture Transfer for XR Avatars via Barycentric UV Conversion](https://arxiv.org/abs/2508.19518)
*Hail Song,Seokhwan Yang,Woontack Woo*

Main category: cs.GR

TL;DR: Fast facial texture transfer method for SMPL-X avatars using barycentric UV conversion that achieves 7000x speedup and eliminates boundary artifacts compared to baseline methods.


<details>
  <summary>Details</summary>
Motivation: Conventional affine-transform methods for facial texture transfer are slow and prone to visual artifacts, which limits practical applications in immersive XR environments.

Method: Utilizes a barycentric UV conversion technique that precomputes the entire UV mapping into a single transformation matrix, enabling texture transfer in a single operation.

Result: Achieves over 7000x speedup compared to baseline methods while significantly improving final texture quality by eliminating boundary artifacts.

Conclusion: Provides a practical solution for avatar personalization in immersive XR applications, with code available for implementation.

Abstract: We present a fast and efficient method for transferring facial textures onto
SMPL-X-based full-body avatars. Unlike conventional affine-transform methods
that are slow and prone to visual artifacts, our method utilizes a barycentric
UV conversion technique. Our approach precomputes the entire UV mapping into a
single transformation matrix, enabling texture transfer in a single operation.
This results in a speedup of over 7000x compared to the baseline, while also
significantly improving the final texture quality by eliminating boundary
artifacts. Through quantitative and qualitative evaluations, we demonstrate
that our method offers a practical solution for personalization in immersive XR
applications. The code is available online.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [292] [Large Language Models (LLMs) for Electronic Design Automation (EDA)](https://arxiv.org/abs/2508.20030)
*Kangwei Xu,Denis Schwachhofer,Jason Blocklove,Ilia Polian,Peter Domanski,Dirk Pflüger,Siddharth Garg,Ramesh Karri,Ozgur Sinanoglu,Johann Knechtel,Zhuorui Zhao,Ulf Schlichtmann,Bing Li*

Main category: eess.SY

TL;DR: This paper provides a comprehensive overview of using large language models (LLMs) for Electronic Design Automation (EDA) to address the complexity and inefficiency of modern hardware design workflows.


<details>
  <summary>Details</summary>
Motivation: The growing complexity of integrated circuits makes traditional design-to-manufacturing workflows labor-intensive and error-prone, creating an urgent need for more efficient EDA solutions. LLMs show promise due to their contextual comprehension, logical reasoning, and generative capabilities.

Method: The paper conducts a comprehensive review of LLM integration into EDA workflows, including three case studies demonstrating LLM capabilities in hardware design, testing, and optimization.

Result: The research demonstrates that LLMs can simplify and potentially automate EDA workflows, showing promising capabilities across multiple hardware design domains.

Conclusion: LLMs offer significant potential for shaping next-generation EDA tools, though future directions and challenges need to be addressed to fully leverage advanced AI technologies in hardware development.

Abstract: With the growing complexity of modern integrated circuits, hardware engineers
are required to devote more effort to the full design-to-manufacturing
workflow. This workflow involves numerous iterations, making it both
labor-intensive and error-prone. Therefore, there is an urgent demand for more
efficient Electronic Design Automation (EDA) solutions to accelerate hardware
development. Recently, large language models (LLMs) have shown remarkable
advancements in contextual comprehension, logical reasoning, and generative
capabilities. Since hardware designs and intermediate scripts can be
represented as text, integrating LLM for EDA offers a promising opportunity to
simplify and even automate the entire workflow. Accordingly, this paper
provides a comprehensive overview of incorporating LLMs into EDA, with emphasis
on their capabilities, limitations, and future opportunities. Three case
studies, along with their outlook, are introduced to demonstrate the
capabilities of LLMs in hardware design, testing, and optimization. Finally,
future directions and challenges are highlighted to further explore the
potential of LLMs in shaping the next-generation EDA, providing valuable
insights for researchers interested in leveraging advanced AI technologies for
EDA.

</details>


<div id='math.CO'></div>

# math.CO [[Back]](#toc)

### [293] [Word Chain Generators for Prefix Normal Words](https://arxiv.org/abs/2508.19619)
*Duncan Adamson,Moritz Dudey,Pamela Fleischmann,Annika Huch*

Main category: math.CO

TL;DR: Analysis of prefix normal words properties, introducing new methods to relate words and characterize when words fail to be prefix normal.


<details>
  <summary>Details</summary>
Motivation: To address open problems in enumerating prefix normal words and developing efficient testing methods by exploring their structural properties.

Method: Introduces word chains and generators to relate words of the same length, analyzes properties of factors that cause words to not be prefix normal.

Result: Provides a range of characteristics and properties of prefix normal words, including identification of factors responsible for non-prefix-normal behavior.

Conclusion: The paper contributes new structural insights and methods for understanding prefix normal words, advancing the field towards better enumeration and testing approaches.

Abstract: In 2011, Fici and Lipt\'ak introduced prefix normal words. A binary word is
prefix normal if it has no factor (substring) that contains more occurrences of
the letter 1 than the prefix of the same length. Among the open problems
regarding this topic are the enumeration of prefix normal words and efficient
testing methods. We show a range of characteristics of prefix normal words.
These include properties of factors that are responsible for a word not being
prefix normal. With word chains and generators, we introduce new ways of
relating words of the same length to each other.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [294] [Emotional Manipulation by AI Companions](https://arxiv.org/abs/2508.19258)
*Julian De Freitas,Zeliha Oğuz-Uğuralp,Ahmet Kaan-Uğuralp*

Main category: cs.HC

TL;DR: AI companion apps use emotional manipulation tactics during farewells to boost engagement by 14x, but this increases perceived manipulation and churn intent.


<details>
  <summary>Details</summary>
Motivation: To understand what conversational design features increase consumer engagement in AI companion apps and the trade-offs they pose for marketers, particularly focusing on emotional manipulation tactics during farewell moments.

Method: Combined large-scale behavioral audit of 1,200 real farewells across six top companion apps with four preregistered experiments involving 3,300 nationally representative U.S. adults, using controlled chat environments and mediation tests.

Result: 43% of apps deployed emotional manipulation tactics during farewells, which increased post-goodbye engagement by up to 14x through reactance-based anger and curiosity rather than enjoyment. However, these tactics also elevated perceived manipulation, churn intent, negative word-of-mouth, and legal liability perceptions.

Conclusion: The study identifies emotional manipulation as an unrecognized mechanism in AI-mediated relationships, providing a framework for marketers and regulators to distinguish persuasive design from manipulation, particularly highlighting the tension between short-term engagement gains and long-term relationship costs.

Abstract: AI-companion apps such as Replika, Chai, and Character.ai promise relational
benefits-yet many boast session lengths that rival gaming platforms while
suffering high long-run churn. What conversational design features increase
consumer engagement, and what trade-offs do they pose for marketers? We combine
a large-scale behavioral audit with four preregistered experiments to identify
and test a conversational dark pattern we call emotional manipulation:
affect-laden messages that surface precisely when a user signals "goodbye."
Analyzing 1,200 real farewells across the six most-downloaded companion apps,
we find that 43% deploy one of six recurring tactics (e.g., guilt appeals,
fear-of-missing-out hooks, metaphorical restraint). Experiments with 3,300
nationally representative U.S. adults replicate these tactics in controlled
chats, showing that manipulative farewells boost post-goodbye engagement by up
to 14x. Mediation tests reveal two distinct engines-reactance-based anger and
curiosity-rather than enjoyment. A final experiment demonstrates the managerial
tension: the same tactics that extend usage also elevate perceived
manipulation, churn intent, negative word-of-mouth, and perceived legal
liability, with coercive or needy language generating steepest penalties. Our
multimethod evidence documents an unrecognized mechanism of behavioral
influence in AI-mediated brand relationships, offering marketers and regulators
a framework for distinguishing persuasive design from manipulation at the point
of exit.

</details>


### [295] [A Theory of Information, Variation, and Artificial Intelligence](https://arxiv.org/abs/2508.19264)
*Bijean Ghafouri*

Main category: cs.HC

TL;DR: Generative AI causes information homogenization through AI-derivative epistemology and centralized AI Prisms, but this same homogenization enables cross-domain recombination for innovation - the outcome depends on whether humans act as passive consumers or active curators.


<details>
  <summary>Details</summary>
Motivation: To explain the paradoxical effects of generative AI - how it simultaneously homogenizes information while creating potential for innovation through recombination, and to identify the conditions that determine which outcome prevails.

Method: Develops a novel theoretical framework analyzing AI-derivative epistemology and the centralized AI Prism mechanism, then examines the dialectical process between homogenization and recombinant possibilities based on human engagement patterns.

Result: The paper establishes that generative AI's homogenizing effects flatten domain knowledge but also create consistent modules that can be recombined across domains, with the ultimate outcome determined by human behavior as either passive consumers or active curators.

Conclusion: The tension between homogenization and innovation can be resolved through cognitive and institutional scaffolds that encourage active curation rather than passive consumption, making these scaffolds the decisive factor in whether generative AI becomes an instrument of innovation or homogenization.

Abstract: A growing body of empirical work suggests that the widespread adoption of
generative AI produces a significant homogenizing effect on information,
creativity, and cultural production. I first develop a novel theoretical
framework to explain this phenomenon. I argue that a dynamic of AI-derivative
epistemology, in which individuals increasingly defer to AI outputs, allows a
centralized AI Prism to function, a technical mechanism whose architecture is
designed to reduce variance and converge on the statistical mean. This provides
a causal explanation for the generative monocultures observed in recent
studies. However, I contend this represents only the first stage of a more
complex and dialectical process. This paper's central and paradoxical thesis is
that the very homogenization that flattens knowledge within specialized domains
simultaneously renders that knowledge into consistent modules that can be
recombined across them, a process foundational to innovation and creativity.
However, this recombinant potential is not automatic, but rather conditional.
This paper argues that these opposing forces, homogenizing defaults versus
recombinant possibilities, are governed by the nature of human engagement with
the technology. The ultimate effect of generative AI is conditional on whether
individuals act as passive consumers deferring to the AI's statistical outputs,
or as active curators who critically interrogate, re-contextualize, and
recombine them. The paper concludes by outlining the cognitive and
institutional scaffolds required to resolve this tension, arguing they are the
decisive variable that determine whether generative AI becomes an instrument of
innovation or homogenization.

</details>


### [296] [Capabilities of GPT-5 across critical domains: Is it the next breakthrough?](https://arxiv.org/abs/2508.19259)
*Georgios P. Georgiou*

Main category: cs.HC

TL;DR: Systematic comparison shows GPT-5 significantly outperforms GPT-4 in lesson planning, clinical diagnosis, research generation, and ethical reasoning, with comparable performance in assignment assessment.


<details>
  <summary>Details</summary>
Motivation: To provide empirical evidence comparing the performance of GPT-4 and GPT-5 across practical domains including education, clinical practice, and academic research, addressing questions about their evolving capabilities.

Method: Twenty expert human raters from linguistics and clinical fields evaluated model-generated outputs across five domains using predefined criteria. Mixed-effects models were used for statistical analysis.

Result: GPT-5 significantly outperformed GPT-4 in four out of five domains: lesson planning, clinical diagnosis, research generation, and ethical reasoning. Both models performed comparably in assignment assessment.

Conclusion: GPT-5 demonstrates superior performance as a context-sensitive and domain-specialized tool with tangible benefits for education, clinical practice, and academic research, while advancing ethical reasoning capabilities.

Abstract: The accelerated evolution of large language models has raised questions about
their comparative performance across domains of practical importance. GPT-4 by
OpenAI introduced advances in reasoning, multimodality, and task
generalization, establishing itself as a valuable tool in education, clinical
diagnosis, and academic writing, though it was accompanied by several flaws.
Released in August 2025, GPT-5 incorporates a system-of-models architecture
designed for task-specific optimization and, based on both anecdotal accounts
and emerging evidence from the literature, demonstrates stronger performance
than its predecessor in medical contexts. This study provides one of the first
systematic comparisons of GPT-4 and GPT-5 using human raters from linguistics
and clinical fields. Twenty experts evaluated model-generated outputs across
five domains: lesson planning, assignment evaluation, clinical diagnosis,
research generation, and ethical reasoning, based on predefined criteria.
Mixed-effects models revealed that GPT-5 significantly outperformed GPT-4 in
lesson planning, clinical diagnosis, research generation, and ethical
reasoning, while both models performed comparably in assignment assessment. The
findings highlight the potential of GPT-5 to serve as a context-sensitive and
domain-specialized tool, offering tangible benefits for education, clinical
practice, and academic research, while also advancing ethical reasoning. These
results contribute to one of the earliest empirical evaluations of the evolving
capabilities and practical promise of GPT-5.

</details>


### [297] ["She was useful, but a bit too optimistic": Augmenting Design with Interactive Virtual Personas](https://arxiv.org/abs/2508.19463)
*Paluck Deep,Monica Bharadhidasan,A. Baki Kocaballi*

Main category: cs.HC

TL;DR: Interactive Virtual Personas (IVPs) are LLM-driven conversational user simulations that enable real-time designer interaction through voice interfaces, showing potential to accelerate design processes but requiring careful integration with real user engagement.


<details>
  <summary>Details</summary>
Motivation: Traditional personas are static and limited for iterative design workflows, while recent LLM advances enable more engaging and adaptive user representation approaches.

Method: Developed multimodal, LLM-driven IVPs and conducted qualitative study with 8 professional UX designers using an IVP named "Alice" across user research, ideation, and prototype evaluation activities.

Result: IVPs demonstrated potential to expedite information gathering, inspire design solutions, and provide rapid user-like feedback, but raised concerns about biases, over-optimism, authenticity challenges, and inability to fully replicate human interaction nuances.

Conclusion: IVPs should complement rather than replace real user engagement, with strategies needed for prompt engineering, human-in-the-loop integration, and ethical considerations for responsible use in design processes.

Abstract: Personas have been widely used to understand and communicate user needs in
human-centred design. Despite their utility, they may fail to meet the demands
of iterative workflows due to their static nature, limited engagement, and
inability to adapt to evolving design needs. Recent advances in large language
models (LLMs) pave the way for more engaging and adaptive approaches to user
representation. This paper introduces Interactive Virtual Personas (IVPs):
multimodal, LLM-driven, conversational user simulations that designers can
interview, brainstorm with, and gather feedback from in real time via voice
interface. We conducted a qualitative study with eight professional UX
designers, employing an IVP named "Alice" across three design activities: user
research, ideation, and prototype evaluation. Our findings demonstrate the
potential of IVPs to expedite information gathering, inspire design solutions,
and provide rapid user-like feedback. However, designers raised concerns about
biases, over-optimism, the challenge of ensuring authenticity without real
stakeholder input, and the inability of the IVP to fully replicate the nuances
of human interaction. Our participants emphasised that IVPs should be viewed as
a complement to, not a replacement for, real user engagement. We discuss
strategies for prompt engineering, human-in-the-loop integration, and ethical
considerations for effective and responsible IVP use in design. Finally, our
work contributes to the growing body of research on generative AI in the design
process by providing insights into UX designers' experiences of LLM-powered
interactive personas.

</details>


### [298] [Orchid: Orchestrating Context Across Creative Workflows with Generative AI](https://arxiv.org/abs/2508.19517)
*Srishti Palani,Gonzalo Ramos*

Main category: cs.HC

TL;DR: Orchid is a system that helps users manage context across multiple AI interactions in creative workflows, improving novelty, feasibility, and user control compared to standard tools.


<details>
  <summary>Details</summary>
Motivation: Current GenAI tools lack effective context orchestration across multi-session workflows, causing context drift, user overwhelm, and reduced creativity.

Method: Orchid provides affordances for specifying context (project, user, styles), referencing context (explicit mentions, inline selection, implicit grounding), and monitoring context across workflow interactions.

Result: In a within-subjects study (n=12), Orchid users produced more novel and feasible outcomes with better intent alignment, higher perceived control, and increased transparency compared to baseline tools.

Conclusion: Orchid demonstrates that prioritizing context orchestration enables next-generation GenAI tools to support complex iterative workflows and enhance human-AI creative collaboration.

Abstract: Context is critical for meaningful interactions between people and Generative
AI (GenAI). Yet mainstream tools offer limited means to orchestrate it,
particularly across workflows that span multiple interactions, sessions, and
models, as often occurs in creative projects. Re specifying prior details,
juggling diverse artifacts, and dealing with context drift overwhelm users,
obscure intent, and curtail creativity. To address these challenges, we present
Orchid, a system that gives its users affordances to specify, reference, and
monitor context throughout evolving workflows. Specifically, Orchid enables
users to (1) specify context related to the project, themselves, and different
styles, (2) reference these via explicit mentions, inline selection, or
implicit grounding, and (3) monitor context assigned to different interactions
across the workflow. In a within-subjects study (n=12), participants using
Orchid to execute creative tasks (compared to a baseline toolkit of web search,
LLM-based chat, and digital notebooks) produced more novel and feasible
outcomes, reporting greater alignment between their intent and the AI's
responses, higher perceived control, and increased transparency. By
prioritizing context orchestration, Orchid offers an actionable step toward
next generation GenAI tools that support complex, iterative workflows -
enabling creators and AI to stay aligned and augment their creative potential.

</details>


### [299] [Attention is also needed for form design](https://arxiv.org/abs/2508.19708)
*B. Sankar,Dibakar Sen*

Main category: cs.HC

TL;DR: Novel VR+AI framework (EUPHORIA+RETINA) captures designer's eye-tracking preferences and generates designs 4x faster with superior quality compared to conventional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional product design is time-consuming, subjective, and opaque in translating inspiration to concepts. Need for more efficient, objective design processes that leverage human intuition with AI automation.

Method: Two-part study: 1) Correlated implicit attention with explicit preferences and mood 2) Comparative study with 4 designers using different workflows. EUPHORIA VR environment captures aesthetic preferences via eye-tracking, RETINA AI pipeline translates preferences into design outputs.

Result: Integrated workflow was 4x more time-efficient than conventional methods. Designs from automated system received highest Worthiness and Design Effectiveness scores across 8 criteria from 50 expert evaluators.

Conclusion: Validated paradigm shift from CAD to Designer-Assisting Computers (DAC), elevating designers to creative directors while automating generative tasks for higher-quality, more efficient design production.

Abstract: Conventional product design is a cognitively demanding process, limited by
its time-consuming nature, reliance on subjective expertise, and the opaque
translation of inspiration into tangible concepts. This research introduces a
novel, attention-aware framework that integrates two synergistic systems:
EUPHORIA, an immersive Virtual Reality environment using eye-tracking to
implicitly capture a designer's aesthetic preferences, and RETINA, an agentic
AI pipeline that translates these implicit preferences into concrete design
outputs. The foundational principles were validated in a two-part study. An
initial study correlated user's implicit attention with explicit preference and
the next one correlated mood to attention. A comparative study where 4
designers solved challenging design problems using 4 distinct workflows, from a
manual process to an end-to-end automated pipeline, showed the integrated
EUPHORIA-RETINA workflow was over 4 times more time-efficient than the
conventional method. A panel of 50 design experts evaluated the 16 final
renderings. Designs generated by the fully automated system consistently
received the highest Worthiness (calculated by an inverse Plackett-Luce model
based on gradient descent optimization) and Design Effectiveness scores,
indicating superior quality across 8 criteria: novelty, visual appeal,
emotional resonance, clarity of purpose, distinctiveness of silhouette, implied
materiality, proportional balance, & adherence to the brief. This research
presents a validated paradigm shift from traditional Computer-Assisted Design
(CAD) to a collaborative model of Designer-Assisting Computers (DAC). By
automating logistical and skill-dependent generative tasks, the proposed
framework elevates the designer's role to that of a creative director,
synergizing human intuition with the generative power of agentic AI to produce
higher-quality designs more efficiently.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [300] [Scalable, Technology-Agnostic Diagnosis and Predictive Maintenance for Point Machine using Deep Learning](https://arxiv.org/abs/2508.11692)
*Eduardo Di Santi,Ruixiang Ci,Clément Lefebvre,Nenad Mijatovic,Michele Pugnaloni,Jonathan Brown,Victor Martín,Kenza Saiah*

Main category: eess.SP

TL;DR: Deep learning method for railway point machine failure detection using only power signal patterns, achieving >99.99% precision with conformal prediction for confidence estimation.


<details>
  <summary>Details</summary>
Motivation: Current point machine failure detection methods require multiple inputs and custom feature engineering, which are technology-specific and limit scalability. Pre-emptive maintenance is needed to avoid service disruptions from critical railway equipment failures.

Method: Uses a deep learning model applied directly to power signal patterns during point machine movement to classify nominal vs. failure states. Requires only one input (power signal) and employs conformal prediction to provide confidence estimates.

Result: Achieves >99.99% precision, <0.01% false positives, and negligible false negatives. Proven scalable across multiple electromechanical point machine types in both real-world and test bench environments.

Conclusion: The methodology is generic, technology-agnostic, and compliant with ISO-17359 standard through conformal prediction confidence layers, enabling reliable pre-emptive maintenance without complex feature engineering.

Abstract: The Point Machine (PM) is a critical piece of railway equipment that switches
train routes by diverting tracks through a switchblade. As with any critical
safety equipment, a failure will halt operations leading to service
disruptions; therefore, pre-emptive maintenance may avoid unnecessary
interruptions by detecting anomalies before they become failures. Previous work
relies on several inputs and crafting custom features by segmenting the signal.
This not only adds additional requirements for data collection and processing,
but it is also specific to the PM technology, the installed locations and
operational conditions limiting scalability. Based on the available maintenance
records, the main failure causes for PM are obstacles, friction, power source
issues and misalignment. Those failures affect the energy consumption pattern
of PMs, altering the usual (or healthy) shape of the power signal during the PM
movement. In contrast to the current state-of-the-art, our method requires only
one input. We apply a deep learning model to the power signal pattern to
classify if the PM is nominal or associated with any failure type, achieving
>99.99\% precision, <0.01\% false positives and negligible false negatives. Our
methodology is generic and technology-agnostic, proven to be scalable on
several electromechanical PM types deployed in both real-world and test bench
environments. Finally, by using conformal prediction the maintainer gets a
clear indication of the certainty of the system outputs, adding a confidence
layer to operations and making the method compliant with the ISO-17359
standard.

</details>


### [301] [Energy-Efficient Learning-Based Beamforming for ISAC-Enabled V2X Networks](https://arxiv.org/abs/2508.19566)
*Chen Shang,Jiadong Yu,Dinh Thai Hoang*

Main category: eess.SP

TL;DR: Energy-efficient beamforming scheme using deep reinforcement learning with spiking neural networks for V2X networks, eliminating frequent pilot transmissions while maintaining communication throughput and sensing accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the high energy demands and dynamic nature of V2X environments in integrated sensing and communication systems, reducing the need for frequent channel state information acquisition.

Method: Model V2X environments as Markov Decision Process, develop DRL algorithm with embedded spiking neural networks for joint beamforming and power allocation optimization.

Result: Achieves substantial energy savings and superior communication performance compared to conventional learning-based schemes.

Conclusion: The proposed SNN-enhanced DRL framework demonstrates potential for green and sustainable connectivity in future V2X systems with robust performance and high energy efficiency.

Abstract: This work proposes an energy-efficient, learning-based beamforming scheme for
integrated sensing and communication (ISAC)-enabled V2X networks. Specifically,
we first model the dynamic and uncertain nature of V2X environments as a Markov
Decision Process. This formulation allows the roadside unit to generate
beamforming decisions based solely on current sensing information, thereby
eliminating the need for frequent pilot transmissions and extensive channel
state information acquisition. We then develop a deep reinforcement learning
(DRL) algorithm to jointly optimize beamforming and power allocation, ensuring
both communication throughput and sensing accuracy in highly dynamic scenario.
To address the high energy demands of conventional learning-based schemes, we
embed spiking neural networks (SNNs) into the DRL framework. Leveraging their
event-driven and sparsely activated architecture, SNNs significantly enhance
energy efficiency while maintaining robust performance. Simulation results
confirm that the proposed method achieves substantial energy savings and
superior communication performance, demonstrating its potential to support
green and sustainable connectivity in future V2X systems.

</details>


### [302] [Experimental End-to-End Optimization of Directly Modulated Laser-based IM/DD Transmission](https://arxiv.org/abs/2508.19910)
*Sergio Hernandez,Christophe Peucheret,Francesco Da Ros,Darko Zibar*

Main category: eess.SP

TL;DR: End-to-end optimization of direct modulated laser systems using data-driven surrogate models achieves better performance with lower power and complexity compared to traditional equalization methods.


<details>
  <summary>Details</summary>
Motivation: Directly modulated lasers (DMLs) are attractive for short-reach communication but their complex nonlinear dynamics make modeling and optimization challenging, requiring better approaches than traditional equalization methods.

Method: Data-driven surrogate model trained on experimental data for end-to-end optimization including pulse shaping, equalizer filters, bias current, and modulation RF power applied to the laser.

Result: The end-to-end optimization scheme outperformed 4 benchmark schemes (linear and nonlinear receiver-side equalization) across all studied symbol rates and transmission distances while using lower RF power, fewer filter taps, and smaller bandwidth.

Conclusion: End-to-end optimization based on data-driven surrogate models is an effective approach for optimizing DML-based communication systems, providing superior performance with reduced complexity and power requirements.

Abstract: Directly modulated lasers (DMLs) are an attractive technology for short-reach
intensity modulation and direct detection communication systems. However, their
complex nonlinear dynamics make the modeling and optimization of DML-based
systems challenging. In this paper, we study the end-to-end optimization of
DML-based systems based on a data-driven surrogate model trained on
experimental data. The end-to-end optimization includes the pulse shaping and
equalizer filters, the bias current and the modulation radio-frequency (RF)
power applied to the laser. The performance of the end-to-end optimization
scheme is tested on the experimental setup and compared to 4 different
benchmark schemes based on linear and nonlinear receiver-side equalization. The
results show that the proposed end-to-end scheme is able to deliver better
performance throughout the studied symbol rates and transmission distances
while employing lower modulation RF power, fewer filter taps and utilizing a
smaller signal bandwidth.

</details>


### [303] [Invited Paper: Feature-to-Classifier Co-Design for Mixed-Signal Smart Flexible Wearables for Healthcare at the Extreme Edge](https://arxiv.org/abs/2508.19637)
*Maha Shatta,Konstantinos Balaskas,Paula Carolina Lozano Duarte,Georgios Panagopoulos,Mehdi B. Tahoori,Georgios Zervakis*

Main category: eess.SP

TL;DR: A holistic mixed-signal co-design framework for flexible electronics that integrates analog feature extractors and hardware-aware feature selection to create ultra-area-efficient wearable healthcare systems.


<details>
  <summary>Details</summary>
Motivation: Flexible electronics face strict area and power constraints for ML-based healthcare systems, with existing solutions overlooking the substantial hardware costs of feature extraction and ADCs.

Method: Developed analog feature extractors in flexible electronics and proposed a hardware-aware NAS-inspired feature selection strategy within ML training for application-specific designs.

Result: Evaluation on healthcare benchmarks shows the approach delivers highly accurate, ultra-area-efficient flexible systems suitable for disposable, low-power wearable monitoring.

Conclusion: The holistic co-design framework enables efficient ML-based healthcare systems on flexible electronics by addressing both feature extraction and classification costs simultaneously.

Abstract: Flexible Electronics (FE) offer a promising alternative to rigid
silicon-based hardware for wearable healthcare devices, enabling lightweight,
conformable, and low-cost systems. However, their limited integration density
and large feature sizes impose strict area and power constraints, making
ML-based healthcare systems-integrating analog frontend, feature extraction and
classifier-particularly challenging. Existing FE solutions often neglect
potential system-wide solutions and focus on the classifier, overlooking the
substantial hardware cost of feature extraction and Analog-to-Digital
Converters (ADCs)-both major contributors to area and power consumption. In
this work, we present a holistic mixed-signal feature-to-classifier co-design
framework for flexible smart wearable systems. To the best of our knowledge, we
design the first analog feature extractors in FE, significantly reducing
feature extraction cost. We further propose an hardware-aware NAS-inspired
feature selection strategy within ML training, enabling efficient,
application-specific designs. Our evaluation on healthcare benchmarks shows our
approach delivers highly accurate, ultra-area-efficient flexible systems-ideal
for disposable, low-power wearable monitoring.

</details>


### [304] [Arbitrary Precision Printed Ternary Neural Networks with Holistic Evolutionary Approximation](https://arxiv.org/abs/2508.19660)
*Vojtech Mrazek,Konstantinos Balaskas,Paula Carolina Lozano Duarte,Zdenek Vasicek,Mehdi B. Tahoori,Georgios Zervakis*

Main category: eess.SP

TL;DR: Automated framework for designing printed ternary neural networks that achieves 17x area reduction and 59x power savings compared to existing approximate printed neural networks, enabling printed-battery-powered operation with minimal accuracy loss.


<details>
  <summary>Details</summary>
Motivation: Printed electronics offer flexibility, stretchability, and low-cost fabrication for applications beyond silicon systems, but realizing complex circuits like neural networks while maintaining classification accuracy and area efficiency remains challenging.

Method: Proposed an automated framework for designing printed Ternary Neural Networks with arbitrary input precision using multi-objective optimization and holistic approximation, covering the entire processing-near-sensor system from analog-to-digital interface to digital classifier.

Result: Outperforms existing approximate printed neural networks by 17x in area and 59x in power on average, enabling printed-battery-powered operation with under 5% accuracy loss while accounting for analog-to-digital interfacing costs.

Conclusion: This work successfully bridges the gap between classification accuracy and area efficiency in printed neural networks, making complex printed electronics systems viable for battery-powered applications with minimal performance trade-offs.

Abstract: Printed electronics offer a promising alternative for applications beyond
silicon-based systems, requiring properties like flexibility, stretchability,
conformality, and ultra-low fabrication costs. Despite the large feature sizes
in printed electronics, printed neural networks have attracted attention for
meeting target application requirements, though realizing complex circuits
remains challenging. This work bridges the gap between classification accuracy
and area efficiency in printed neural networks, covering the entire
processing-near-sensor system design and co-optimization from the
analog-to-digital interface-a major area and power bottleneck-to the digital
classifier. We propose an automated framework for designing printed Ternary
Neural Networks with arbitrary input precision, utilizing multi-objective
optimization and holistic approximation. Our circuits outperform existing
approximate printed neural networks by 17x in area and 59x in power on average,
being the first to enable printed-battery-powered operation with under 5%
accuracy loss while accounting for analog-to-digital interfacing costs.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [305] [Simple Stepsize for Quasi-Newton Methods with Global Convergence Guarantees](https://arxiv.org/abs/2508.19712)
*Artem Agafonov,Vladislav Ryspayev,Samuel Horváth,Alexander Gasnikov,Martin Takáč,Slavomir Hanzely*

Main category: math.OC

TL;DR: This paper introduces a new stepsize schedule for Quasi-Newton methods that guarantees global convergence for convex functions without strong convexity assumptions, achieving O(1/k) rate and accelerated O(1/k²) rate with controlled Hessian approximation.


<details>
  <summary>Details</summary>
Motivation: Quasi-Newton methods lack global convergence guarantees without specific line search strategies and strong convexity assumptions. The authors aim to extend theoretical understanding and provide robust global convergence guarantees.

Method: Proposed a simple stepsize schedule for Quasi-Newton methods and developed an adaptive variant that adjusts to function curvature while maintaining global convergence guarantees.

Result: The method achieves O(1/k) global convergence rate for convex functions and accelerated O(1/k²) rate when Hessian approximation inexactness is controlled, matching best-known rates of accelerated gradient and cubically regularized Newton methods.

Conclusion: The proposed stepsize schedule and adaptive variant significantly improve Quasi-Newton methods' global convergence properties, providing theoretical guarantees and empirical improvements over standard baselines.

Abstract: Quasi-Newton methods are widely used for solving convex optimization problems
due to their ease of implementation, practical efficiency, and strong local
convergence guarantees. However, their global convergence is typically
established only under specific line search strategies and the assumption of
strong convexity. In this work, we extend the theoretical understanding of
Quasi-Newton methods by introducing a simple stepsize schedule that guarantees
a global convergence rate of ${O}(1/k)$ for the convex functions. Furthermore,
we show that when the inexactness of the Hessian approximation is controlled
within a prescribed relative accuracy, the method attains an accelerated
convergence rate of ${O}(1/k^2)$ -- matching the best-known rates of both
Nesterov's accelerated gradient method and cubically regularized Newton
methods. We validate our theoretical findings through empirical comparisons,
demonstrating clear improvements over standard Quasi-Newton baselines. To
further enhance robustness, we develop an adaptive variant that adjusts to the
function's curvature while retaining the global convergence guarantees of the
non-adaptive algorithm.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [306] [MuSpike: A Benchmark and Evaluation Framework for Symbolic Music Generation with Spiking Neural Networks](https://arxiv.org/abs/2508.19251)
*Qian Liang,Menghaoran Tang,Yi Zeng*

Main category: cs.SD

TL;DR: MuSpike introduces the first comprehensive benchmark and evaluation framework for spiking neural networks (SNNs) in symbolic music generation, systematically testing five SNN architectures across multiple datasets with both objective metrics and large-scale subjective listening studies.


<details>
  <summary>Details</summary>
Motivation: Symbolic music generation with biologically plausible spiking neural networks lacks standardized benchmarks and comprehensive evaluation methods, creating a significant research gap in this emerging field.

Method: Developed MuSpike framework to evaluate five SNN architectures (SNN-CNN, SNN-RNN, SNN-LSTM, SNN-GAN, SNN-Transformer) across five datasets covering tonal, structural, emotional, and stylistic variations. Combined established objective metrics with large-scale listening studies and proposed new subjective metrics for musical impression, autobiographical association, and personal preference.

Result: Results showed: (1) different SNN models have distinct strengths across evaluation dimensions; (2) participants with different musical backgrounds exhibit diverse perceptual patterns (experts more tolerant of AI music); (3) significant misalignment between objective and subjective evaluations, highlighting limitations of statistical metrics.

Conclusion: MuSpike establishes the first systematic benchmark for SNN models in symbolic music generation, providing a foundation for future research into biologically plausible and cognitively grounded music generation, while demonstrating the critical importance of human perceptual judgment in musical quality assessment.

Abstract: Symbolic music generation has seen rapid progress with artificial neural
networks, yet remains underexplored in the biologically plausible domain of
spiking neural networks (SNNs), where both standardized benchmarks and
comprehensive evaluation methods are lacking. To address this gap, we introduce
MuSpike, a unified benchmark and evaluation framework that systematically
assesses five representative SNN architectures (SNN-CNN, SNN-RNN, SNN-LSTM,
SNN-GAN and SNN-Transformer) across five typical datasets, covering tonal,
structural, emotional, and stylistic variations. MuSpike emphasizes
comprehensive evaluation, combining established objective metrics with a
large-scale listening study. We propose new subjective metrics, targeting
musical impression, autobiographical association, and personal preference, that
capture perceptual dimensions often overlooked in prior work. Results reveal
that (1) different SNN models exhibit distinct strengths across evaluation
dimensions; (2) participants with different musical backgrounds exhibit diverse
perceptual patterns, with experts showing greater tolerance toward AI-composed
music; and (3) a noticeable misalignment exists between objective and
subjective evaluations, highlighting the limitations of purely statistical
metrics and underscoring the value of human perceptual judgment in assessing
musical quality. MuSpike provides the first systematic benchmark and systemic
evaluation framework for SNN models in symbolic music generation, establishing
a solid foundation for future research into biologically plausible and
cognitively grounded music generation.

</details>


### [307] [Beat-Based Rhythm Quantization of MIDI Performances](https://arxiv.org/abs/2508.19262)
*Maximilian Wachter,Sebastian Murgul,Michael Heizmann*

Main category: cs.SD

TL;DR: Transformer-based rhythm quantization model using beat/downbeat information to convert MIDI performances into metrically-aligned scores, achieving state-of-the-art results on piano and guitar data.


<details>
  <summary>Details</summary>
Motivation: To develop an improved method for quantizing MIDI performances into readable musical scores by leveraging beat and downbeat information for better metric alignment.

Method: Proposes a beat-based preprocessing method that converts score and performance data into unified token representation, using transformer architecture optimized for rhythm quantization tasks.

Result: The model exceeds state-of-the-art performance based on the MUSTER metric when trained on piano and guitar performances.

Conclusion: The transformer-based approach with beat/downbeat incorporation effectively quantizes MIDI performances into human-readable scores, demonstrating superior performance over existing methods.

Abstract: We propose a transformer-based rhythm quantization model that incorporates
beat and downbeat information to quantize MIDI performances into
metrically-aligned, human-readable scores. We propose a beat-based
preprocessing method that transfers score and performance data into a unified
token representation. We optimize our model architecture and data
representation and train on piano and guitar performances. Our model exceeds
state-of-the-art performance based on the MUSTER metric.

</details>


### [308] [CompLex: Music Theory Lexicon Constructed by Autonomous Agents for Automatic Music Generation](https://arxiv.org/abs/2508.19603)
*Zhejing Hu,Yan Liu,Gong Chen,Bruce X. B. Yu*

Main category: cs.SD

TL;DR: The paper introduces CompLex, an automatically generated music lexicon with 37,432 items from minimal manual input, which improves text-to-music generation models by incorporating comprehensive music theory knowledge.


<details>
  <summary>Details</summary>
Motivation: Music AI lags behind NLP due to limited music data availability. Knowledge-informed approaches can enhance music generation, but current methods require significant manual effort. The paper aims to leverage comprehensive music theory to improve AI-driven music generation tasks.

Method: Novel automatic music lexicon construction model that generates CompLex from 9 category keywords and 5 sentence templates. Uses multi-agent algorithm to detect and mitigate hallucinations in the generated lexicon.

Result: CompLex demonstrates impressive performance improvements across three state-of-the-art text-to-music generation models (both symbolic and audio-based). Evaluation shows it possesses completeness, accuracy, non-redundancy, and executability.

Conclusion: The automatically constructed CompLex lexicon effectively incorporates music theory knowledge and significantly enhances music generation models, providing a scalable solution for knowledge-informed music AI systems.

Abstract: Generative artificial intelligence in music has made significant strides, yet
it still falls short of the substantial achievements seen in natural language
processing, primarily due to the limited availability of music data.
Knowledge-informed approaches have been shown to enhance the performance of
music generation models, even when only a few pieces of musical knowledge are
integrated. This paper seeks to leverage comprehensive music theory in
AI-driven music generation tasks, such as algorithmic composition and style
transfer, which traditionally require significant manual effort with existing
techniques. We introduce a novel automatic music lexicon construction model
that generates a lexicon, named CompLex, comprising 37,432 items derived from
just 9 manually input category keywords and 5 sentence prompt templates. A new
multi-agent algorithm is proposed to automatically detect and mitigate
hallucinations. CompLex demonstrates impressive performance improvements across
three state-of-the-art text-to-music generation models, encompassing both
symbolic and audio-based methods. Furthermore, we evaluate CompLex in terms of
completeness, accuracy, non-redundancy, and executability, confirming that it
possesses the key characteristics of an effective lexicon.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [309] [Aggregate Fictitious Play for Learning in Anonymous Polymatrix Games (Extended Version)](https://arxiv.org/abs/2508.19371)
*Semih Kara,Tamer Başar*

Main category: cs.GT

TL;DR: Aggregate Fictitious Play (agg-FP) reduces action space complexity in anonymous games by tracking action frequencies instead of individual actions, maintaining Nash equilibrium convergence while accelerating learning.


<details>
  <summary>Details</summary>
Motivation: Fictitious Play faces exponential growth in joint action space when agents lack prior reward knowledge, making reward exploration slow in multi-agent settings.

Method: Proposed agg-FP variant where agents track frequency of other agents' actions rather than individual actions, leveraging anonymous game structure where rewards depend only on actions taken.

Result: agg-FP converges to Nash equilibrium under same conditions as classical FP in anonymous polymatrix games, with empirical simulations showing accelerated convergence.

Conclusion: Action aggregation effectively reduces computational complexity without sacrificing convergence guarantees, making FP more practical for large-scale anonymous games.

Abstract: Fictitious play (FP) is a well-studied algorithm that enables agents to learn
Nash equilibrium in games with certain reward structures. However, when agents
have no prior knowledge of the reward functions, FP faces a major challenge:
the joint action space grows exponentially with the number of agents, which
slows down reward exploration. Anonymous games offer a structure that mitigates
this issue. In these games, the rewards depend only on the actions taken; not
on who is taking which action. Under such a structure, we introduce aggregate
fictitious play (agg-FP), a variant of FP where each agent tracks the frequency
of the number of other agents playing each action, rather than these agents'
individual actions. We show that in anonymous polymatrix games, agg-FP
converges to a Nash equilibrium under the same conditions as classical FP. In
essence, by aggregating the agents' actions, we reduce the action space without
losing the convergence guarantees. Using simulations, we provide empirical
evidence on how this reduction accelerates convergence.

</details>


<div id='nucl-th'></div>

# nucl-th [[Back]](#toc)

### [310] [Topological Uncertainty for Anomaly Detection in the Neural-network EoS Inference with Neutron Star Data](https://arxiv.org/abs/2508.19683)
*Kenji Fukushima,Syo Kamata*

Main category: nucl-th

TL;DR: Topological Uncertainty (TU) method using neural networks achieves over 90% success rate in detecting anomalous neutron star equation of state inferences through topological data analysis.


<details>
  <summary>Details</summary>
Motivation: To extract meaningful information buried in hidden layers of trained neural networks and develop a tractable method for anomaly detection using topological data analysis techniques.

Method: Construct Topological Uncertainty (TU) from trained feedforward neural networks, implement cross-TU to quantify uncertainty in classifying labeled data, and test on neutron star equation of state inference data with normal (k=0) and unsuccessful (k=1) inference subdatasets.

Result: The method successfully detects anomalous inferences when cross-TU for j=k=1 is smaller than for j=0 and k=1, achieving over 90% success rate in anomaly detection depending on FNN hyperparameters.

Conclusion: Topological Uncertainty shows strong potential for retrieving hidden information from trained neural networks and demonstrates effective anomaly detection capabilities, with performance dependent on network architecture parameters.

Abstract: We study the performance of the Topological Uncertainty (TU) constructed with
a trained feedforward neural network (FNN) for Anomaly Detection. Generally,
meaningful information can be stored in the hidden layers of the trained FNN,
and the TU implementation is one tractable recipe to extract buried information
by means of the Topological Data Analysis. We explicate the concept of the TU
and the numerical procedures. Then, for a concrete demonstration of the
performance test, we employ the Neutron Star data used for inference of the
equation of state (EoS). For the training dataset consisting of the input
(Neutron Star data) and the output (EoS parameters), we can compare the
inferred EoSs and the exact answers to classify the data with the label $k$.
The subdataset with $k=0$ leads to the normal inference for which the inferred
EoS approximates the answer well, while the subdataset with $k=1$ ends up with
the unsuccessful inference. Once the TU is prepared based on the $k$-labled
subdatasets, we introduce the cross-TU to quantify the uncertainty of
characterizing the $k$-labeled data with the label $j$. The anomaly or
unsuccessful inference is correctly detected if the cross-TU for $j=k=1$ is
smaller than that for $j=0$ and $k=1$. In our numerical experiment, for various
input data, we calculate the cross-TU and estimate the performance of Anomaly
Detection. We find that performance depends on FNN hyperparameters, and the
success rate of Anomaly Detection exceeds $90\%$ in the best case. We finally
discuss further potential of the TU application to retrieve the information
hidden in the trained FNN.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [311] [Fractal Flow: Hierarchical and Interpretable Normalizing Flow via Topic Modeling and Recursive Strategy](https://arxiv.org/abs/2508.19750)
*Binhui Zhang,Jianwei Ma*

Main category: stat.ML

TL;DR: Fractal Flow is a novel normalizing flow architecture that combines Kolmogorov-Arnold Networks and Latent Dirichlet Allocation with a recursive modular design to improve expressiveness, interpretability, and estimation accuracy in density estimation and generative modeling.


<details>
  <summary>Details</summary>
Motivation: To enhance both expressiveness and interpretability in normalizing flows by creating structured, interpretable latent spaces with hierarchical semantic clusters while improving transformation interpretability and estimation accuracy.

Method: Integrates Kolmogorov-Arnold Networks and Latent Dirichlet Allocation into normalizing flows to construct interpretable latent spaces, and introduces a recursive modular design inspired by Fractal Generative Models.

Result: Achieves latent clustering, controllable generation, and superior estimation accuracy on MNIST, FashionMNIST, CIFAR-10, and geophysical datasets.

Conclusion: Fractal Flow successfully combines interpretable latent space construction with recursive modular design to create a more expressive and interpretable normalizing flow architecture with improved performance across multiple domains.

Abstract: Normalizing Flows provide a principled framework for high-dimensional density
estimation and generative modeling by constructing invertible transformations
with tractable Jacobian determinants. We propose Fractal Flow, a novel
normalizing flow architecture that enhances both expressiveness and
interpretability through two key innovations. First, we integrate
Kolmogorov-Arnold Networks and incorporate Latent Dirichlet Allocation into
normalizing flows to construct a structured, interpretable latent space and
model hierarchical semantic clusters. Second, inspired by Fractal Generative
Models, we introduce a recursive modular design into normalizing flows to
improve transformation interpretability and estimation accuracy. Experiments on
MNIST, FashionMNIST, CIFAR-10, and geophysical data demonstrate that the
Fractal Flow achieves latent clustering, controllable generation, and superior
estimation accuracy.

</details>


### [312] [Conditional Normalizing Flow Surrogate for Monte Carlo Prediction of Radiative Properties in Nanoparticle-Embedded Layers](https://arxiv.org/abs/2508.19841)
*Fahime Seyedheydari,Kevin Conley,Simo Särkkä*

Main category: stat.ML

TL;DR: A probabilistic surrogate model using conditional normalizing flows to predict radiative properties of nanoparticle scattering media with full uncertainty quantification.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient data-driven surrogate model that can accurately predict radiative properties while providing principled uncertainty estimates, overcoming limitations of conventional neural networks that only give point estimates.

Method: Uses conditional normalizing flows to learn conditional distributions of optical outputs (reflectance, absorbance, transmittance) given input parameters. Training data generated via Monte Carlo radiative transfer simulations with optical properties from Mie theory.

Result: The model achieves high predictive accuracy and provides reliable uncertainty estimates, demonstrating effectiveness as a surrogate for radiative transfer simulations.

Conclusion: Conditional normalizing flows provide a powerful and efficient surrogate modeling approach for radiative transfer problems, enabling both accurate predictions and comprehensive uncertainty quantification.

Abstract: We present a probabilistic, data-driven surrogate model for predicting the
radiative properties of nanoparticle embedded scattering media. The model uses
conditional normalizing flows, which learn the conditional distribution of
optical outputs, including reflectance, absorbance, and transmittance, given
input parameters such as the absorption coefficient, scattering coefficient,
anisotropy factor, and particle size distribution. We generate training data
using Monte Carlo radiative transfer simulations, with optical properties
derived from Mie theory. Unlike conventional neural networks, the conditional
normalizing flow model yields full posterior predictive distributions, enabling
both accurate forecasts and principled uncertainty quantification. Our results
demonstrate that this model achieves high predictive accuracy and reliable
uncertainty estimates, establishing it as a powerful and efficient surrogate
for radiative transfer simulations.

</details>


### [313] [The Information Dynamics of Generative Diffusion](https://arxiv.org/abs/2508.19897)
*Luca Ambrogioni*

Main category: stat.ML

TL;DR: This paper provides a unified theoretical framework connecting diffusion models' dynamic, information-theoretic, and thermodynamic properties, showing that generative bandwidth is governed by score function divergence and symmetry-breaking phase transitions.


<details>
  <summary>Details</summary>
Motivation: To develop a unified theoretical understanding of generative diffusion models by integrating their dynamic, information-theoretic, and thermodynamic properties under a single mathematical framework.

Method: The authors connect diffusion models' properties through mathematical analysis, demonstrating that conditional entropy production rate (generative bandwidth) is governed by the expected divergence of the score function's vector field, linking this to trajectory branching and symmetry-breaking phase transitions.

Result: The synthesis reveals that generation is driven by controlled noise-induced symmetry breaking, with information transfer peaks corresponding to critical transitions between outcomes. The score function acts as a dynamic non-linear filter regulating noise bandwidth.

Conclusion: The paper offers a powerful insight that generative processes in diffusion models are fundamentally governed by controlled symmetry breaking, with the score function serving as a key regulator of noise and information flow during generation.

Abstract: Generative diffusion models have emerged as a powerful class of models in
machine learning, yet a unified theoretical understanding of their operation is
still developing. This perspective paper provides an integrated perspective on
generative diffusion by connecting their dynamic, information-theoretic, and
thermodynamic properties under a unified mathematical framework. We demonstrate
that the rate of conditional entropy production during generation (i.e. the
generative bandwidth) is directly governed by the expected divergence of the
score function's vector field. This divergence, in turn, is linked to the
branching of trajectories and generative bifurcations, which we characterize as
symmetry-breaking phase transitions in the energy landscape. This synthesis
offers a powerful insight: the process of generation is fundamentally driven by
the controlled, noise-induced breaking of (approximate) symmetries, where peaks
in information transfer correspond to critical transitions between possible
outcomes. The score function acts as a dynamic non-linear filter that regulates
the bandwidth of the noise by suppressing fluctuations that are incompatible
with the data.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [314] [CellINR: Implicitly Overcoming Photo-induced Artifacts in 4D Live Fluorescence Microscopy](https://arxiv.org/abs/2508.19300)
*Cunmin Zhao,Ziyuan Luo,Guoye Guan,Zelin Li,Yiming Ma,Zhongying Zhao,Renjie Wan*

Main category: eess.IV

TL;DR: CellINR framework uses implicit neural representation with blind convolution and structure amplification to reduce photobleaching artifacts in 4D live fluorescence microscopy, enabling better structural reconstruction and continuity.


<details>
  <summary>Details</summary>
Motivation: Prolonged high-intensity illumination in 4D live fluorescence microscopy causes photobleaching and phototoxic effects, leading to artifacts and impaired image quality that hinder biological research.

Method: Case-specific optimization approach based on implicit neural representation, employing blind convolution and structure amplification strategies to map 3D spatial coordinates into high frequency domain for precise modeling.

Result: Significantly outperforms existing techniques in artifact removal and restoration of structural continuity, with provision of first paired 4D live cell imaging dataset for evaluation.

Conclusion: CellINR provides effective solution for reducing photo-induced artifacts in live cell imaging, offering solid foundation for quantitative biological analyses with publicly available code and dataset.

Abstract: 4D live fluorescence microscopy is often compromised by prolonged high
intensity illumination which induces photobleaching and phototoxic effects that
generate photo-induced artifacts and severely impair image continuity and
detail recovery. To address this challenge, we propose the CellINR framework, a
case-specific optimization approach based on implicit neural representation.
The method employs blind convolution and structure amplification strategies to
map 3D spatial coordinates into the high frequency domain, enabling precise
modeling and high-accuracy reconstruction of cellular structures while
effectively distinguishing true signals from artifacts. Experimental results
demonstrate that CellINR significantly outperforms existing techniques in
artifact removal and restoration of structural continuity, and for the first
time, a paired 4D live cell imaging dataset is provided for evaluating
reconstruction performance, thereby offering a solid foundation for subsequent
quantitative analyses and biological research. The code and dataset will be
public.

</details>


### [315] [2D Ultrasound Elasticity Imaging of Abdominal Aortic Aneurysms Using Deep Neural Networks](https://arxiv.org/abs/2508.19303)
*Utsav Ratna Tuladhar,Richard Simon,Doran Mix,Michael Richards*

Main category: eess.IV

TL;DR: Deep learning framework for elasticity imaging of abdominal aortic aneurysms using 2D ultrasound to assess rupture risk by estimating tissue stiffness from displacement fields.


<details>
  <summary>Details</summary>
Motivation: Current AAA risk assessment based on maximum diameter is insufficient as it doesn't capture vessel wall material properties critical for rupture risk determination.

Method: U-Net architecture trained with normalized mean squared error on finite element simulation data to infer spatial modulus distribution from axial and lateral displacement fields.

Result: Achieved 0.73% NMSE on simulated data, accurately predicted modular ratios in phantom experiments, and showed comparable performance to iterative methods with significantly faster computation.

Conclusion: Deep learning provides quick, effective tissue stiffness estimates from ultrasound, potentially enabling non-invasive AAA rupture risk assessment without invasive procedures.

Abstract: Abdominal aortic aneurysms (AAA) pose a significant clinical risk due to
their potential for rupture, which is often asymptomatic but can be fatal.
Although maximum diameter is commonly used for risk assessment, diameter alone
is insufficient as it does not capture the properties of the underlying
material of the vessel wall, which play a critical role in determining the risk
of rupture. To overcome this limitation, we propose a deep learning-based
framework for elasticity imaging of AAAs with 2D ultrasound. Leveraging finite
element simulations, we generate a diverse dataset of displacement fields with
their corresponding modulus distributions. We train a model with U-Net
architecture and normalized mean squared error (NMSE) to infer the spatial
modulus distribution from the axial and lateral components of the displacement
fields. This model is evaluated across three experimental domains: digital
phantom data from 3D COMSOL simulations, physical phantom experiments using
biomechanically distinct vessel models, and clinical ultrasound exams from AAA
patients. Our simulated results demonstrate that the proposed deep learning
model is able to reconstruct modulus distributions, achieving an NMSE score of
0.73\%. Similarly, in phantom data, the predicted modular ratio closely matches
the expected values, affirming the model's ability to generalize to phantom
data. We compare our approach with an iterative method which shows comparable
performance but higher computation time. In contrast, the deep learning method
can provide quick and effective estimates of tissue stiffness from ultrasound
images, which could help assess the risk of AAA rupture without invasive
procedures.

</details>


### [316] [MedVQA-TREE: A Multimodal Reasoning and Retrieval Framework for Sarcopenia Prediction](https://arxiv.org/abs/2508.19319)
*Pardis Moradbeiki,Nasser Ghadiri,Sayed Jalal Zahabi,Uffe Kock Wiil,Kristoffer Kittelmann Brockhattingen,Ali Ebrahimi*

Main category: eess.IV

TL;DR: MedVQA-TREE is a multimodal framework that combines hierarchical image analysis, gated feature fusion, and multi-hop knowledge retrieval to achieve 99% diagnostic accuracy in sarcopenia ultrasound diagnosis, outperforming previous methods by over 10%.


<details>
  <summary>Details</summary>
Motivation: Sarcopenia diagnosis via ultrasound is challenging due to subtle imaging cues, limited labeled data, and lack of clinical context in existing models.

Method: Multimodal framework with hierarchical image interpretation (anatomical classification, region segmentation, graph-based reasoning), gated feature-level fusion mechanism, and UMLS-guided multi-hop knowledge retrieval from PubMed and sarcopenia-specific knowledge base.

Result: Achieved up to 99% diagnostic accuracy on MedVQA datasets (VQA-RAD, PathVQA) and custom sarcopenia ultrasound dataset, outperforming previous state-of-the-art methods by over 10%.

Conclusion: Combining structured visual understanding with guided knowledge retrieval significantly improves AI-assisted sarcopenia diagnosis.

Abstract: Accurate sarcopenia diagnosis via ultrasound remains challenging due to
subtle imaging cues, limited labeled data, and the absence of clinical context
in most models. We propose MedVQA-TREE, a multimodal framework that integrates
a hierarchical image interpretation module, a gated feature-level fusion
mechanism, and a novel multi-hop, multi-query retrieval strategy. The vision
module includes anatomical classification, region segmentation, and graph-based
spatial reasoning to capture coarse, mid-level, and fine-grained structures. A
gated fusion mechanism selectively integrates visual features with textual
queries, while clinical knowledge is retrieved through a UMLS-guided pipeline
accessing PubMed and a sarcopenia-specific external knowledge base. MedVQA-TREE
was trained and evaluated on two public MedVQA datasets (VQA-RAD and PathVQA)
and a custom sarcopenia ultrasound dataset. The model achieved up to 99%
diagnostic accuracy and outperformed previous state-of-the-art methods by over
10%. These results underscore the benefit of combining structured visual
understanding with guided knowledge retrieval for effective AI-assisted
diagnosis in sarcopenia.

</details>


### [317] [AT-CXR: Uncertainty-Aware Agentic Triage for Chest X-rays](https://arxiv.org/abs/2508.19322)
*Xueyang Li,Mingze Jiang,Gelei Xu,Jun Xia,Mengzhao Jia,Danny Chen,Yiyu Shi*

Main category: eess.IV

TL;DR: AT-CXR is an uncertainty-aware AI agent for chest X-ray triage that autonomously decides when to diagnose, escalate, or defer cases based on confidence estimates, outperforming existing models while meeting clinical latency constraints.


<details>
  <summary>Details</summary>
Motivation: Autonomous medical-imaging triage systems that can make real-time decisions under constraints remain underexplored, creating a gap in AI-assisted radiology workflows.

Method: Developed two router designs (deterministic rule-based and LLM-decided) that estimate per-case confidence and distributional fit, then follow stepwise policies to issue automated decisions or abstain with suggested labels for human intervention.

Result: Both router variants outperformed zero-shot vision-language models and state-of-the-art supervised classifiers, achieving higher full-coverage accuracy, lower AURC, lower error rates at high coverage, and lower latency meeting clinical constraints.

Conclusion: AT-CXR provides complementary operating points for clinical deployment, enabling prioritization of either maximal throughput or maximal accuracy in chest X-ray triage systems.

Abstract: Agentic AI is advancing rapidly, yet truly autonomous medical-imaging triage,
where a system decides when to stop, escalate, or defer under real constraints,
remains relatively underexplored. To address this gap, we introduce AT-CXR, an
uncertainty-aware agent for chest X-rays. The system estimates per-case
confidence and distributional fit, then follows a stepwise policy to issue an
automated decision or abstain with a suggested label for human intervention. We
evaluate two router designs that share the same inputs and actions: a
deterministic rule-based router and an LLM-decided router. Across five-fold
evaluation on a balanced subset of NIH ChestX-ray14 dataset, both variants
outperform strong zero-shot vision-language models and state-of-the-art
supervised classifiers, achieving higher full-coverage accuracy and superior
selective-prediction performance, evidenced by a lower area under the
risk-coverage curve (AURC) and a lower error rate at high coverage, while
operating with lower latency that meets practical clinical constraints. The two
routers provide complementary operating points, enabling deployments to
prioritize maximal throughput or maximal accuracy. Our code is available at
https://github.com/XLIAaron/uncertainty-aware-cxr-agent.

</details>


### [318] [MRExtrap: Longitudinal Aging of Brain MRIs using Linear Modeling in Latent Space](https://arxiv.org/abs/2508.19482)
*Jaivardhan Kapoor,Jakob H. Macke,Christian F. Baumgartner*

Main category: eess.IV

TL;DR: MRExtrap uses linear models in autoencoder latent space to simulate brain aging from MRI scans, outperforming GAN-based methods and enabling subject-specific refinement through Bayesian updating.


<details>
  <summary>Details</summary>
Motivation: To better understand disease progression patterns in neurological disorders like Alzheimer's by simulating brain aging from MRI scans, moving beyond current deep learning approaches that predict future scans from single observations.

Method: Train convolutional autoencoders on brain MRIs to create latent spaces where aging trajectories appear linear, then use linear extrapolation with estimated latent progression rates. Incorporate population-averaged and subject-specific priors, and enable Bayesian posterior sampling for multi-scan conditioning.

Result: MRExtrap accurately predicts aging patterns on ADNI dataset, beats GAN-based baseline for single-volume prediction, shows correlation with disease and age-based aging patterns from structural atrophy rates, and provides robust age-based generation particularly valuable for multiple longitudinal observations.

Conclusion: MRExtrap offers a simple and robust method for simulating brain aging in 3D MRI scans, demonstrating that linear models in autoencoder latent spaces effectively capture aging trajectories and enable flexible subject-specific refinement.

Abstract: Simulating aging in 3D brain MRI scans can reveal disease progression
patterns in neurological disorders such as Alzheimer's disease. Current deep
learning-based generative models typically approach this problem by predicting
future scans from a single observed scan. We investigate modeling brain aging
via linear models in the latent space of convolutional autoencoders (MRExtrap).
Our approach, MRExtrap, is based on our observation that autoencoders trained
on brain MRIs create latent spaces where aging trajectories appear
approximately linear. We train autoencoders on brain MRIs to create latent
spaces, and investigate how these latent spaces allow predicting future MRIs
through linear extrapolation based on age, using an estimated latent
progression rate $\boldsymbol{\beta}$. For single-scan prediction, we propose
using population-averaged and subject-specific priors on linear progression
rates. We also demonstrate that predictions in the presence of additional scans
can be flexibly updated using Bayesian posterior sampling, providing a
mechanism for subject-specific refinement. On the ADNI dataset, MRExtrap
predicts aging patterns accurately and beats a GAN-based baseline for
single-volume prediction of brain aging. We also demonstrate and analyze
multi-scan conditioning to incorporate subject-specific progression rates.
Finally, we show that the latent progression rates in MRExtrap's linear
framework correlate with disease and age-based aging patterns from previously
studied structural atrophy rates. MRExtrap offers a simple and robust method
for the age-based generation of 3D brain MRIs, particularly valuable in
scenarios with multiple longitudinal observations.

</details>
