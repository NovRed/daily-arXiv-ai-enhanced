<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 20]
- [cs.AI](#cs.AI) [Total: 16]
- [cs.CL](#cs.CL) [Total: 16]
- [cs.LG](#cs.LG) [Total: 16]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Large Language Models for Crash Detection in Video: A Survey of Methods, Datasets, and Challenges](https://arxiv.org/abs/2507.02074)
*Sanjeda Akter,Ibne Farabi Shihab,Anuj Sharma*

Main category: cs.CV

TL;DR: Survey of LLM-based methods for crash detection from video data, covering fusion strategies, datasets, architectures, benchmarks, and challenges.


<details>
  <summary>Details</summary>
Motivation: To address the critical problem of crash detection in intelligent transportation systems by leveraging advancements in LLMs and VLMs.

Method: Review and taxonomy of fusion strategies, analysis of model architectures, and comparison of performance benchmarks.

Result: Comprehensive survey highlighting key datasets, methods, and performance insights.

Conclusion: Provides a foundation for future research at the intersection of video understanding and foundation models.

Abstract: Crash detection from video feeds is a critical problem in intelligent
transportation systems. Recent developments in large language models (LLMs) and
vision-language models (VLMs) have transformed how we process, reason about,
and summarize multimodal information. This paper surveys recent methods
leveraging LLMs for crash detection from video data. We present a structured
taxonomy of fusion strategies, summarize key datasets, analyze model
architectures, compare performance benchmarks, and discuss ongoing challenges
and opportunities. Our review provides a foundation for future research in this
fast-growing intersection of video understanding and foundation models.

</details>


### [2] [Underwater Monocular Metric Depth Estimation: Real-World Benchmarks and Synthetic Fine-Tuning](https://arxiv.org/abs/2507.02148)
*Zijie Cai,Christopher Metzler*

Main category: cs.CV

TL;DR: A benchmark evaluates monocular depth estimation models in underwater environments, showing poor performance of terrestrial-trained models due to domain shifts. Fine-tuning on synthetic underwater data improves results.


<details>
  <summary>Details</summary>
Motivation: Underwater environments pose challenges like light attenuation, scattering, and lack of high-quality ground-truth data, limiting the reliability of monocular depth estimation.

Method: Evaluate zero-shot and fine-tuned models on underwater datasets (FLSea, SQUID). Fine-tune Depth Anything V2 with ViT-S backbone on synthetic underwater Hypersim data.

Result: Terrestrial-trained models perform poorly underwater; fine-tuned models show consistent improvement across benchmarks.

Conclusion: Domain adaptation and scale-aware supervision are crucial for robust underwater depth estimation, with synthetic data fine-tuning enhancing performance.

Abstract: Monocular depth estimation has recently advanced to provide not only relative
but also metric depth predictions. However, its reliability in underwater
environments remains limited due to light attenuation and scattering, color
distortion, turbidity, and the lack of high-quality metric ground-truth data.
In this paper, we present a comprehensive benchmark of zero-shot and fine-tuned
monocular metric depth estimation models on real-world underwater datasets with
metric depth annotations, such as FLSea and SQUID. We evaluate a diverse set of
state-of-the-art models across a range of underwater conditions with different
ranges. Our results show that large-scale models trained on terrestrial (real
or synthetic) data, while effective in in-air settings, perform poorly
underwater due to significant domain shifts. To address this, we fine-tune
Depth Anything V2 with a ViT-S backbone encoder on a synthetic underwater
variant of the Hypersim dataset, which we generated using a physically based
underwater image formation model. We demonstrate our fine-tuned model
consistently improves performance across all benchmarks and outperforms
baselines trained only on the clean in-air Hypersim dataset. Our study provides
a detailed evaluation and visualization for monocular metric depth estimation
in underwater scenes, highlighting the importance of domain adaptation and
scale-aware supervision for achieving robust and generalizable metric depth
predictions in challenging underwater environments for future research.

</details>


### [3] [ESTR-CoT: Towards Explainable and Accurate Event Stream based Scene Text Recognition with Chain-of-Thought Reasoning](https://arxiv.org/abs/2507.02200)
*Xiao Wang,Jingtao Jiang,Qiang Chen,Lan Chen,Lin Zhu,Yaowei Wang,Yonghong Tian,Jin Tang*

Main category: cs.CV

TL;DR: The paper introduces ESTR-CoT, a novel chain-of-thought reasoning framework for event stream scene text recognition, addressing interpretability and contextual reasoning challenges.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack interpretability and contextual reasoning, especially in challenging scenarios like low illumination and fast motion.

Method: The framework uses EVA-CLIP for vision encoding, Llama tokenizer for prompts, and Vicuna-7B for reasoning. It's trained on a new CoT dataset via three stages.

Result: Experiments on EventSTR, WordArt*, and IC15* datasets confirm the framework's effectiveness and interpretability.

Conclusion: ESTR-CoT advances event stream text recognition with improved reasoning and interpretability, supported by a new dataset.

Abstract: Event stream based scene text recognition is a newly arising research topic
in recent years which performs better than the widely used RGB cameras in
extremely challenging scenarios, especially the low illumination, fast motion.
Existing works either adopt end-to-end encoder-decoder framework or large
language models for enhanced recognition, however, they are still limited by
the challenges of insufficient interpretability and weak contextual logical
reasoning. In this work, we propose a novel chain-of-thought reasoning based
event stream scene text recognition framework, termed ESTR-CoT. Specifically,
we first adopt the vision encoder EVA-CLIP (ViT-G/14) to transform the input
event stream into tokens and utilize a Llama tokenizer to encode the given
generation prompt. A Q-former is used to align the vision token to the
pre-trained large language model Vicuna-7B and output both the answer and
chain-of-thought (CoT) reasoning process simultaneously. Our framework can be
optimized using supervised fine-tuning in an end-to-end manner. In addition, we
also propose a large-scale CoT dataset to train our framework via a three stage
processing (i.e., generation, polish, and expert verification). This dataset
provides a solid data foundation for the development of subsequent
reasoning-based large models. Extensive experiments on three event stream STR
benchmark datasets (i.e., EventSTR, WordArt*, IC15*) fully validated the
effectiveness and interpretability of our proposed framework. The source code
and pre-trained models will be released on
https://github.com/Event-AHU/ESTR-CoT.

</details>


### [4] [Team RAS in 9th ABAW Competition: Multimodal Compound Expression Recognition Approach](https://arxiv.org/abs/2507.02205)
*Elena Ryumina,Maxim Markitantov,Alexandr Axyonov,Dmitry Ryumin,Mikhail Dolgushin,Alexey Karpov*

Main category: cs.CV

TL;DR: A zero-shot multimodal approach for Compound Expression Recognition (CER) combines six modalities and achieves comparable results to supervised methods without domain adaptation.


<details>
  <summary>Details</summary>
Motivation: To detect complex emotional states by combining basic emotions without relying on task-specific training data.

Method: Uses zero-shot components like CLIP and Qwen-VL, introduces Multi-Head Probability Fusion (MHPF) and Compound Expressions (CE) transformation modules.

Result: Achieves F1 scores of 46.95% (AffWild2), 49.02% (AFEW), and 34.85% (C-EXPR-DB) via zero-shot testing.

Conclusion: The approach effectively captures compound emotions without domain adaptation, matching supervised methods.

Abstract: Compound Expression Recognition (CER), a subfield of affective computing,
aims to detect complex emotional states formed by combinations of basic
emotions. In this work, we present a novel zero-shot multimodal approach for
CER that combines six heterogeneous modalities into a single pipeline: static
and dynamic facial expressions, scene and label matching, scene context, audio,
and text. Unlike previous approaches relying on task-specific training data,
our approach uses zero-shot components, including Contrastive Language-Image
Pretraining (CLIP)-based label matching and Qwen-VL for semantic scene
understanding. We further introduce a Multi-Head Probability Fusion (MHPF)
module that dynamically weights modality-specific predictions, followed by a
Compound Expressions (CE) transformation module that uses Pair-Wise Probability
Aggregation (PPA) and Pair-Wise Feature Similarity Aggregation (PFSA) methods
to produce interpretable compound emotion outputs. Evaluated under multi-corpus
training, the proposed approach shows F1 scores of 46.95% on AffWild2, 49.02%
on Acted Facial Expressions in The Wild (AFEW), and 34.85% on C-EXPR-DB via
zero-shot testing, which is comparable to the results of supervised approaches
trained on target data. This demonstrates the effectiveness of the proposed
approach for capturing CE without domain adaptation. The source code is
publicly available.

</details>


### [5] [SciGA: A Comprehensive Dataset for Designing Graphical Abstracts in Academic Papers](https://arxiv.org/abs/2507.02212)
*Takuro Kawada,Shunsuke Kitada,Sota Nemoto,Hitoshi Iyatomi*

Main category: cs.CV

TL;DR: The paper introduces SciGA-145k, a dataset for Graphical Abstracts (GAs) research, proposing tasks and metrics to improve GA selection and generation.


<details>
  <summary>Details</summary>
Motivation: To address the unexplored potential of GAs in scientific communication and the barrier of advanced visualization skills for GA design.

Method: Introduces SciGA-145k dataset, defines intra-GA and inter-GA recommendation tasks, and proposes the CAR metric for evaluation.

Result: Provides baseline models for GA tasks and a novel metric (CAR) for fine-grained analysis.

Conclusion: SciGA-145k advances visual scientific communication and supports AI for Science development.

Abstract: Graphical Abstracts (GAs) play a crucial role in visually conveying the key
findings of scientific papers. While recent research has increasingly
incorporated visual materials such as Figure 1 as de facto GAs, their potential
to enhance scientific communication remains largely unexplored. Moreover,
designing effective GAs requires advanced visualization skills, creating a
barrier to their widespread adoption. To tackle these challenges, we introduce
SciGA-145k, a large-scale dataset comprising approximately 145,000 scientific
papers and 1.14 million figures, explicitly designed for supporting GA
selection and recommendation as well as facilitating research in automated GA
generation. As a preliminary step toward GA design support, we define two
tasks: 1) Intra-GA recommendation, which identifies figures within a given
paper that are well-suited to serve as GAs, and 2) Inter-GA recommendation,
which retrieves GAs from other papers to inspire the creation of new GAs. We
provide reasonable baseline models for these tasks. Furthermore, we propose
Confidence Adjusted top-1 ground truth Ratio (CAR), a novel recommendation
metric that offers a fine-grained analysis of model behavior. CAR addresses
limitations in traditional ranking-based metrics by considering cases where
multiple figures within a paper, beyond the explicitly labeled GA, may also
serve as GAs. By unifying these tasks and metrics, our SciGA-145k establishes a
foundation for advancing visual scientific communication while contributing to
the development of AI for Science.

</details>


### [6] [Understanding Trade offs When Conditioning Synthetic Data](https://arxiv.org/abs/2507.02217)
*Brandon Trabucco,Qasim Wani,Benjamin Pikus,Vasu Sharma*

Main category: cs.CV

TL;DR: The paper explores synthetic data generation for robust object detection in low-data regimes, comparing prompt-based and layout-based conditioning with diffusion models. Layout conditioning outperforms when diversity is high, significantly boosting detection performance.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of learning robust object detectors with limited real-world data, especially in industrial settings where data collection is time-consuming.

Method: Study of 80 visual concepts from object detection benchmarks, comparing prompt-based and layout-based conditioning in diffusion models for synthetic data generation.

Result: Layout conditioning excels with diverse cues, improving mean average precision by up to 177% compared to real data alone.

Conclusion: Layout-based conditioning in diffusion models is superior for diverse synthetic data, offering substantial performance gains in object detection.

Abstract: Learning robust object detectors from only a handful of images is a critical
challenge in industrial vision systems, where collecting high quality training
data can take months. Synthetic data has emerged as a key solution for data
efficient visual inspection and pick and place robotics. Current pipelines rely
on 3D engines such as Blender or Unreal, which offer fine control but still
require weeks to render a small dataset, and the resulting images often suffer
from a large gap between simulation and reality. Diffusion models promise a
step change because they can generate high quality images in minutes, yet
precise control, especially in low data regimes, remains difficult. Although
many adapters now extend diffusion beyond plain text prompts, the effect of
different conditioning schemes on synthetic data quality is poorly understood.
We study eighty diverse visual concepts drawn from four standard object
detection benchmarks and compare two conditioning strategies: prompt based and
layout based. When the set of conditioning cues is narrow, prompt conditioning
yields higher quality synthetic data; as diversity grows, layout conditioning
becomes superior. When layout cues match the full training distribution,
synthetic data raises mean average precision by an average of thirty four
percent and by as much as one hundred seventy seven percent compared with using
real data alone.

</details>


### [7] [High-Fidelity Differential-information Driven Binary Vision Transformer](https://arxiv.org/abs/2507.02222)
*Tian Gao,Zhiyuan Zhang,Kaijie Yin,Xu-Cheng Zhong,Hui Kong*

Main category: cs.CV

TL;DR: DIDB-ViT is a novel binary vision transformer that addresses performance degradation and reliance on full-precision modules in existing methods by incorporating differential information, frequency decomposition, and an improved activation function.


<details>
  <summary>Details</summary>
Motivation: To mitigate the trade-off between computational efficiency and performance in binary ViTs, which often suffer from severe degradation or depend on full-precision modules.

Method: Introduces an informative attention module with differential information, frequency decomposition via Haar wavelet, and an improved RPReLU activation function.

Result: Outperforms state-of-the-art quantization methods in ViTs, achieving superior classification and segmentation performance.

Conclusion: DIDB-ViT effectively balances efficiency and performance, making it suitable for edge-device deployment.

Abstract: The binarization of vision transformers (ViTs) offers a promising approach to
addressing the trade-off between high computational/storage demands and the
constraints of edge-device deployment. However, existing binary ViT methods
often suffer from severe performance degradation or rely heavily on
full-precision modules. To address these issues, we propose DIDB-ViT, a novel
binary ViT that is highly informative while maintaining the original ViT
architecture and computational efficiency. Specifically, we design an
informative attention module incorporating differential information to mitigate
information loss caused by binarization and enhance high-frequency retention.
To preserve the fidelity of the similarity calculations between binary Q and K
tensors, we apply frequency decomposition using the discrete Haar wavelet and
integrate similarities across different frequencies. Additionally, we introduce
an improved RPReLU activation function to restructure the activation
distribution, expanding the model's representational capacity. Experimental
results demonstrate that our DIDB-ViT significantly outperforms
state-of-the-art network quantization methods in multiple ViT architectures,
achieving superior image classification and segmentation performance.

</details>


### [8] [FMOcc: TPV-Driven Flow Matching for 3D Occupancy Prediction with Selective State Space Model](https://arxiv.org/abs/2507.02250)
*Jiangxia Chen,Tongyuan Huang,Ke Song*

Main category: cs.CV

TL;DR: FMOcc improves 3D semantic occupancy prediction for autonomous driving by refining features with flow matching and selective state space models, outperforming existing methods with fewer frames.


<details>
  <summary>Details</summary>
Motivation: Existing methods for 3D occupancy prediction rely on historical frame data, requiring extra resources and data. FMOcc addresses this by improving accuracy for occluded and distant scenes with fewer frames.

Method: FMOcc uses a Tri-perspective View (TPV) refinement network with flow matching (FMSSM) and selective state space models (PS3M) to refine features and filter air voxels. Mask Training (MT) enhances robustness.

Result: FMOcc achieves 43.1% RayIoU and 39.8% mIoU on Occ3D-nuScenes and 42.6% RayIoU on OpenOcc, with efficient memory (5.4G) and inference time (330ms).

Conclusion: FMOcc is an efficient and accurate solution for few-frame 3D occupancy prediction, outperforming state-of-the-art methods.

Abstract: 3D semantic occupancy prediction plays a pivotal role in autonomous driving.
However, inherent limitations of fewframe images and redundancy in 3D space
compromise prediction accuracy for occluded and distant scenes. Existing
methods enhance performance by fusing historical frame data, which need
additional data and significant computational resources. To address these
issues, this paper propose FMOcc, a Tri-perspective View (TPV) refinement
occupancy network with flow matching selective state space model for few-frame
3D occupancy prediction. Firstly, to generate missing features, we designed a
feature refinement module based on a flow matching model, which is called Flow
Matching SSM module (FMSSM). Furthermore, by designing the TPV SSM layer and
Plane Selective SSM (PS3M), we selectively filter TPV features to reduce the
impact of air voxels on non-air voxels, thereby enhancing the overall
efficiency of the model and prediction capability for distant scenes. Finally,
we design the Mask Training (MT) method to enhance the robustness of FMOcc and
address the issue of sensor data loss. Experimental results on the
Occ3D-nuScenes and OpenOcc datasets show that our FMOcc outperforms existing
state-of-theart methods. Our FMOcc with two frame input achieves notable scores
of 43.1% RayIoU and 39.8% mIoU on Occ3D-nuScenes validation, 42.6% RayIoU on
OpenOcc with 5.4 G inference memory and 330ms inference time.

</details>


### [9] [SurgVisAgent: Multimodal Agentic Model for Versatile Surgical Visual Enhancement](https://arxiv.org/abs/2507.02252)
*Zeyu Lei,Hongyuan Yu,Jinlin Wu,Zhen Chen*

Main category: cs.CV

TL;DR: SurgVisAgent, a multimodal surgical vision agent, enhances endoscopic images dynamically for various distortions, outperforming single-task models.


<details>
  <summary>Details</summary>
Motivation: Existing surgical enhancement algorithms are limited to single tasks, reducing effectiveness in complex scenarios.

Method: Built on multimodal large language models (MLLMs), SurgVisAgent uses a prior model, in-context few-shot learning, and chain-of-thought reasoning for customized enhancements.

Result: Extensive experiments show SurgVisAgent surpasses traditional single-task models in handling diverse surgical distortions.

Conclusion: SurgVisAgent offers a unified, effective solution for surgical image enhancement, addressing real-world complexity.

Abstract: Precise surgical interventions are vital to patient safety, and advanced
enhancement algorithms have been developed to assist surgeons in
decision-making. Despite significant progress, these algorithms are typically
designed for single tasks in specific scenarios, limiting their effectiveness
in complex real-world situations. To address this limitation, we propose
SurgVisAgent, an end-to-end intelligent surgical vision agent built on
multimodal large language models (MLLMs). SurgVisAgent dynamically identifies
distortion categories and severity levels in endoscopic images, enabling it to
perform a variety of enhancement tasks such as low-light enhancement,
overexposure correction, motion blur elimination, and smoke removal.
Specifically, to achieve superior surgical scenario understanding, we design a
prior model that provides domain-specific knowledge. Additionally, through
in-context few-shot learning and chain-of-thought (CoT) reasoning, SurgVisAgent
delivers customized image enhancements tailored to a wide range of distortion
types and severity levels, thereby addressing the diverse requirements of
surgeons. Furthermore, we construct a comprehensive benchmark simulating
real-world surgical distortions, on which extensive experiments demonstrate
that SurgVisAgent surpasses traditional single-task models, highlighting its
potential as a unified solution for surgical assistance.

</details>


### [10] [Multi-Label Classification Framework for Hurricane Damage Assessment](https://arxiv.org/abs/2507.02265)
*Zhangding Liu,Neda Mohammadi,John E. Taylor*

Main category: cs.CV

TL;DR: A novel multi-label classification framework for hurricane damage assessment using aerial imagery outperforms baselines with 90.23% mean average precision.


<details>
  <summary>Details</summary>
Motivation: Traditional single-label classification fails to capture the complexity of post-hurricane damage, necessitating a more accurate and timely assessment method.

Method: The framework combines ResNet-based feature extraction with a class-specific attention mechanism to identify multiple damage types in a single image.

Result: Achieves 90.23% mean average precision on the Rescuenet dataset from Hurricane Michael, surpassing baseline methods.

Conclusion: The framework improves damage assessment, aiding targeted disaster response and future mitigation strategies.

Abstract: Hurricanes cause widespread destruction, resulting in diverse damage types
and severities that require timely and accurate assessment for effective
disaster response. While traditional single-label classification methods fall
short of capturing the complexity of post-hurricane damage, this study
introduces a novel multi-label classification framework for assessing damage
using aerial imagery. The proposed approach integrates a feature extraction
module based on ResNet and a class-specific attention mechanism to identify
multiple damage types within a single image. Using the Rescuenet dataset from
Hurricane Michael, the proposed method achieves a mean average precision of
90.23%, outperforming existing baseline methods. This framework enhances
post-hurricane damage assessment, enabling more targeted and efficient disaster
response and contributing to future strategies for disaster mitigation and
resilience. This paper has been accepted at the ASCE International Conference
on Computing in Civil Engineering (i3CE 2025), and the camera-ready version
will appear in the official conference proceedings.

</details>


### [11] [Cross-domain Hyperspectral Image Classification based on Bi-directional Domain Adaptation](https://arxiv.org/abs/2507.02268)
*Yuxiang Zhang,Wei Li,Wen Jia,Mengmeng Zhang,Ran Tao,Shunlin Liang*

Main category: cs.CV

TL;DR: The paper proposes a Bi-directional Domain Adaptation (BiDA) framework for cross-domain hyperspectral image classification, leveraging domain-invariant and domain-specific features to improve adaptability and separability.


<details>
  <summary>Details</summary>
Motivation: Address spectral shifts in hyperspectral images from different regions or times, enhancing classification accuracy across domains.

Method: Introduces a triple-branch transformer with semantic tokenizer, coupled multi-head cross-attention, bi-directional distillation loss, and adaptive reinforcement strategy.

Result: Outperforms state-of-the-art methods by 3-5% in cross-temporal tree species classification.

Conclusion: BiDA effectively improves cross-domain HSI classification by integrating domain adaptation and feature interaction.

Abstract: Utilizing hyperspectral remote sensing technology enables the extraction of
fine-grained land cover classes. Typically, satellite or airborne images used
for training and testing are acquired from different regions or times, where
the same class has significant spectral shifts in different scenes. In this
paper, we propose a Bi-directional Domain Adaptation (BiDA) framework for
cross-domain hyperspectral image (HSI) classification, which focuses on
extracting both domain-invariant features and domain-specific information in
the independent adaptive space, thereby enhancing the adaptability and
separability to the target scene. In the proposed BiDA, a triple-branch
transformer architecture (the source branch, target branch, and coupled branch)
with semantic tokenizer is designed as the backbone. Specifically, the source
branch and target branch independently learn the adaptive space of source and
target domains, a Coupled Multi-head Cross-attention (CMCA) mechanism is
developed in coupled branch for feature interaction and inter-domain
correlation mining. Furthermore, a bi-directional distillation loss is designed
to guide adaptive space learning using inter-domain correlation. Finally, we
propose an Adaptive Reinforcement Strategy (ARS) to encourage the model to
focus on specific generalized feature extraction within both source and target
scenes in noise condition. Experimental results on cross-temporal/scene
airborne and satellite datasets demonstrate that the proposed BiDA performs
significantly better than some state-of-the-art domain adaptation approaches.
In the cross-temporal tree species classification task, the proposed BiDA is
more than 3\%$\sim$5\% higher than the most advanced method. The codes will be
available from the website:
https://github.com/YuxiangZhang-BIT/IEEE_TCSVT_BiDA.

</details>


### [12] [MAC-Lookup: Multi-Axis Conditional Lookup Model for Underwater Image Enhancement](https://arxiv.org/abs/2507.02270)
*Fanghai Yi,Zehong Zheng,Zexiao Liang,Yihang Dong,Xiyang Fang,Wangyu Wu,Xuhang Chen*

Main category: cs.CV

TL;DR: The paper introduces MAC-Lookup, a model for enhancing underwater images by improving color accuracy, sharpness, and contrast, outperforming traditional and deep learning methods.


<details>
  <summary>Details</summary>
Motivation: Underwater images suffer from visibility and color issues due to environmental factors, and existing methods (prior-based, pixel-based, or deep learning) are inadequate.

Method: MAC-Lookup combines Conditional 3D Lookup Table Color Correction (CLTCC) for initial correction and Multi-Axis Adaptive Enhancement (MAAE) for detail refinement, avoiding over-enhancement.

Result: Experiments show MAC-Lookup restores details and colors better than existing methods.

Conclusion: MAC-Lookup effectively addresses underwater image enhancement challenges, offering superior results.

Abstract: Enhancing underwater images is crucial for exploration. These images face
visibility and color issues due to light changes, water turbidity, and bubbles.
Traditional prior-based methods and pixel-based methods often fail, while deep
learning lacks sufficient high-quality datasets. We introduce the Multi-Axis
Conditional Lookup (MAC-Lookup) model, which enhances visual quality by
improving color accuracy, sharpness, and contrast. It includes Conditional 3D
Lookup Table Color Correction (CLTCC) for preliminary color and quality
correction and Multi-Axis Adaptive Enhancement (MAAE) for detail refinement.
This model prevents over-enhancement and saturation while handling underwater
challenges. Extensive experiments show that MAC-Lookup excels in enhancing
underwater images by restoring details and colors better than existing methods.
The code is https://github.com/onlycatdoraemon/MAC-Lookup.

</details>


### [13] [Spotlighting Partially Visible Cinematic Language for Video-to-Audio Generation via Self-distillation](https://arxiv.org/abs/2507.02271)
*Feizhen Huang,Yu Wu,Yutian Lin,Bo Du*

Main category: cs.CV

TL;DR: A self-distillation method improves Video-to-Audio (V2A) generation by incorporating cinematic language, enhancing performance in partial visibility scenarios and on the VGGSound dataset.


<details>
  <summary>Details</summary>
Motivation: Current V2A methods ignore cinematic language, leading to poor performance when Foley targets are partially visible.

Method: Proposes a self-distillation approach where a student model learns to align video features with audio-visual correspondences by simulating cinematic language variations.

Result: Achieves significant improvements in partial visibility scenarios and on the VGGSound dataset.

Conclusion: The method effectively addresses the limitations of current V2A models by integrating cinematic language, improving both partial visibility and general performance.

Abstract: Video-to-Audio (V2A) Generation achieves significant progress and plays a
crucial role in film and video post-production. However, current methods
overlook the cinematic language, a critical component of artistic expression in
filmmaking. As a result, their performance deteriorates in scenarios where
Foley targets are only partially visible. To address this challenge, we propose
a simple self-distillation approach to extend V2A models to cinematic language
scenarios. By simulating the cinematic language variations, the student model
learns to align the video features of training pairs with the same audio-visual
correspondences, enabling it to effectively capture the associations between
sounds and partial visual information. Our method not only achieves impressive
improvements under partial visibility across all evaluation metrics, but also
enhances performance on the large-scale V2A dataset, VGGSound.

</details>


### [14] [LaCo: Efficient Layer-wise Compression of Visual Tokens for Multimodal Large Language Models](https://arxiv.org/abs/2507.02279)
*Juntao Liu,Liqiang Niu,Wenchao Chen,Jie Zhou,Fandong Meng*

Main category: cs.CV

TL;DR: LaCo introduces layer-wise visual token compression within vision encoders, improving efficiency and performance over post-encoder methods.


<details>
  <summary>Details</summary>
Motivation: Existing token compression methods for MLLMs are limited as post-encoder modules, restricting efficiency gains.

Method: LaCo uses a layer-wise pixel-shuffle mechanism and residual learning with non-parametric shortcuts for compression.

Result: LaCo outperforms existing methods, improving training efficiency by 20% and inference throughput by 15%.

Conclusion: LaCo is a superior framework for visual token compression in MLLMs, balancing efficiency and performance.

Abstract: Existing visual token compression methods for Multimodal Large Language
Models (MLLMs) predominantly operate as post-encoder modules, limiting their
potential for efficiency gains. To address this limitation, we propose LaCo
(Layer-wise Visual Token Compression), a novel framework that enables effective
token compression within the intermediate layers of the vision encoder. LaCo
introduces two core components: 1) a layer-wise pixel-shuffle mechanism that
systematically merges adjacent tokens through space-to-channel transformations,
and 2) a residual learning architecture with non-parametric shortcuts that
preserves critical visual information during compression. Extensive experiments
indicate that our LaCo outperforms all existing methods when compressing tokens
in the intermediate layers of the vision encoder, demonstrating superior
effectiveness. In addition, compared to external compression, our method
improves training efficiency beyond 20% and inference throughput over 15% while
maintaining strong performance.

</details>


### [15] [Prompt Disentanglement via Language Guidance and Representation Alignment for Domain Generalization](https://arxiv.org/abs/2507.02288)
*De Cheng,Zhipeng Xu,Xinyang Jiang,Dongsheng Li,Nannan Wang,Xinbo Gao*

Main category: cs.CV

TL;DR: The paper proposes a novel framework for domain generalization (DG) using text feature-guided visual prompt tuning and introduces WERA to enhance visual feature disentanglement, outperforming state-of-the-art DG methods.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of designing prompts that disentangle invariant features across diverse domains in DG, leveraging the controllable language prompt of Visual Foundation Models (VFMs).

Method: A framework combining text feature-guided visual prompt tuning and WERA, which uses abstract prompts and stylized image augmentations to enhance diversity and alignment.

Result: Outperforms state-of-the-art DG methods on major datasets like PACS, VLCS, OfficeHome, DomainNet, and TerraInc.

Conclusion: The proposed method effectively leverages text and visual prompts to improve domain generalization, demonstrating superior performance.

Abstract: Domain Generalization (DG) seeks to develop a versatile model capable of
performing effectively on unseen target domains. Notably, recent advances in
pre-trained Visual Foundation Models (VFMs), such as CLIP, have demonstrated
considerable potential in enhancing the generalization capabilities of deep
learning models. Despite the increasing attention toward VFM-based domain
prompt tuning within DG, the effective design of prompts capable of
disentangling invariant features across diverse domains remains a critical
challenge. In this paper, we propose addressing this challenge by leveraging
the controllable and flexible language prompt of the VFM. Noting that the text
modality of VFMs is naturally easier to disentangle, we introduce a novel
framework for text feature-guided visual prompt tuning. This framework first
automatically disentangles the text prompt using a large language model (LLM)
and then learns domain-invariant visual representation guided by the
disentangled text feature. However, relying solely on language to guide visual
feature disentanglement has limitations, as visual features can sometimes be
too complex or nuanced to be fully captured by descriptive text. To address
this, we introduce Worst Explicit Representation Alignment (WERA), which
extends text-guided visual prompts by incorporating an additional set of
abstract prompts. These prompts enhance source domain diversity through
stylized image augmentations, while alignment constraints ensure that visual
representations remain consistent across both the original and augmented
distributions. Experiments conducted on major DG datasets, including PACS,
VLCS, OfficeHome, DomainNet, and TerraInc, demonstrate that our proposed method
outperforms state-of-the-art DG methods.

</details>


### [16] [ViRefSAM: Visual Reference-Guided Segment Anything Model for Remote Sensing Segmentation](https://arxiv.org/abs/2507.02294)
*Hanbo Bi,Yulong Xu,Ya Li,Yongqiang Mao,Boyuan Tong,Chongyang Li,Chunbo Lang,Wenhui Diao,Hongqi Wang,Yingchao Feng,Xian Sun*

Main category: cs.CV

TL;DR: ViRefSAM improves SAM's application in remote sensing by automating prompts and enhancing domain adaptability using few-shot learning, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: SAM struggles with manual prompt inefficiency and domain adaptability in remote sensing, prompting the need for an automated, domain-aware solution.

Method: ViRefSAM introduces a Visual Contextual Prompt Encoder for automatic prompts and a Dynamic Target Alignment Adapter to bridge domain gaps, leveraging few-shot learning.

Result: ViRefSAM achieves accurate segmentation of unseen classes with few reference images, outperforming benchmarks on iSAID-5$^i$, LoveDA-2$^i$, and COCO-20$^i$.

Conclusion: ViRefSAM effectively addresses SAM's limitations in remote sensing, enabling efficient and accurate segmentation without manual prompts.

Abstract: The Segment Anything Model (SAM), with its prompt-driven paradigm, exhibits
strong generalization in generic segmentation tasks. However, applying SAM to
remote sensing (RS) images still faces two major challenges. First, manually
constructing precise prompts for each image (e.g., points or boxes) is
labor-intensive and inefficient, especially in RS scenarios with dense small
objects or spatially fragmented distributions. Second, SAM lacks domain
adaptability, as it is pre-trained primarily on natural images and struggles to
capture RS-specific semantics and spatial characteristics, especially when
segmenting novel or unseen classes. To address these issues, inspired by
few-shot learning, we propose ViRefSAM, a novel framework that guides SAM
utilizing only a few annotated reference images that contain class-specific
objects. Without requiring manual prompts, ViRefSAM enables automatic
segmentation of class-consistent objects across RS images. Specifically,
ViRefSAM introduces two key components while keeping SAM's original
architecture intact: (1) a Visual Contextual Prompt Encoder that extracts
class-specific semantic clues from reference images and generates object-aware
prompts via contextual interaction with target images; and (2) a Dynamic Target
Alignment Adapter, integrated into SAM's image encoder, which mitigates the
domain gap by injecting class-specific semantics into target image features,
enabling SAM to dynamically focus on task-relevant regions. Extensive
experiments on three few-shot segmentation benchmarks, including iSAID-5$^i$,
LoveDA-2$^i$, and COCO-20$^i$, demonstrate that ViRefSAM enables accurate and
automatic segmentation of unseen classes by leveraging only a few reference
images and consistently outperforms existing few-shot segmentation methods
across diverse datasets.

</details>


### [17] [DreamComposer++: Empowering Diffusion Models with Multi-View Conditions for 3D Content Generation](https://arxiv.org/abs/2507.02299)
*Yunhan Yang,Shuo Chen,Yukun Huang,Xiaoyang Wu,Yuan-Chen Guo,Edmund Y. Lam,Hengshuang Zhao,Tong He,Xihui Liu*

Main category: cs.CV

TL;DR: DreamComposer++ improves novel view synthesis by integrating multi-view conditions into pre-trained diffusion models, enhancing controllability and quality.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with controllable novel view generation due to limited multi-view information.

Method: DreamComposer++ uses a view-aware 3D lifting module and multi-view feature fusion to enhance pre-trained diffusion models.

Result: The framework integrates with state-of-the-art models, improving controllable novel view synthesis and enabling 3D reconstruction.

Conclusion: DreamComposer++ advances controllable 3D object generation and expands application possibilities.

Abstract: Recent advancements in leveraging pre-trained 2D diffusion models achieve the
generation of high-quality novel views from a single in-the-wild image.
However, existing works face challenges in producing controllable novel views
due to the lack of information from multiple views. In this paper, we present
DreamComposer++, a flexible and scalable framework designed to improve current
view-aware diffusion models by incorporating multi-view conditions.
Specifically, DreamComposer++ utilizes a view-aware 3D lifting module to
extract 3D representations of an object from various views. These
representations are then aggregated and rendered into the latent features of
target view through the multi-view feature fusion module. Finally, the obtained
features of target view are integrated into pre-trained image or video
diffusion models for novel view synthesis. Experimental results demonstrate
that DreamComposer++ seamlessly integrates with cutting-edge view-aware
diffusion models and enhances their abilities to generate controllable novel
views from multi-view conditions. This advancement facilitates controllable 3D
object reconstruction and enables a wide range of applications.

</details>


### [18] [Flow-CDNet: A Novel Network for Detecting Both Slow and Fast Changes in Bitemporal Images](https://arxiv.org/abs/2507.02307)
*Haoxuan Li,Chenxu Wei,Haodong Wang,Xiaomeng Hu,Boyuan An,Lingyan Ran,Baosen Zhang,Jin Jin,Omirzhan Taukebayev,Amirkhan Temirbayev,Junrui Liu,Xiuwei Zhang*

Main category: cs.CV

TL;DR: Flow-CDNet is a novel change detection network with two branches—optical flow and binary change detection—to detect both slow and fast changes in bitemporal images. It outperforms existing methods on the Flow-Change dataset.


<details>
  <summary>Details</summary>
Motivation: Detecting both slow and fast changes in bitemporal images is crucial for real-life scenarios like hazard prediction, but existing methods often overlook slow changes.

Method: Flow-CDNet uses a pyramid structure for optical flow and a ResNet-based network for binary change detection, combined with a new loss function and metric (FEPE).

Result: Quantitative experiments show Flow-CDNet outperforms existing methods, and ablation studies confirm mutual enhancement between the two branches.

Conclusion: Flow-CDNet effectively addresses the challenge of detecting slow and fast changes, validated by superior performance on the Flow-Change dataset.

Abstract: Change detection typically involves identifying regions with changes between
bitemporal images taken at the same location. Besides significant changes, slow
changes in bitemporal images are also important in real-life scenarios. For
instance, weak changes often serve as precursors to major hazards in scenarios
like slopes, dams, and tailings ponds. Therefore, designing a change detection
network that simultaneously detects slow and fast changes presents a novel
challenge. In this paper, to address this challenge, we propose a change
detection network named Flow-CDNet, consisting of two branches: optical flow
branch and binary change detection branch. The first branch utilizes a pyramid
structure to extract displacement changes at multiple scales. The second one
combines a ResNet-based network with the optical flow branch's output to
generate fast change outputs. Subsequently, to supervise and evaluate this new
change detection framework, a self-built change detection dataset Flow-Change,
a loss function combining binary tversky loss and L2 norm loss, along with a
new evaluation metric called FEPE are designed. Quantitative experiments
conducted on Flow-Change dataset demonstrated that our approach outperforms the
existing methods. Furthermore, ablation experiments verified that the two
branches can promote each other to enhance the detection performance.

</details>


### [19] [LMPNet for Weakly-supervised Keypoint Discovery](https://arxiv.org/abs/2507.02308)
*Pei Guo,Ryan Farrell*

Main category: cs.CV

TL;DR: LMPNet discovers semantic keypoints using weakly-supervised learning with category labels, employing a leaky max pooling layer and clustering for robust, interpretable results.


<details>
  <summary>Details</summary>
Motivation: To discover semantic object keypoints without strong supervision, relying only on category labels for training.

Method: Uses leaky max pooling (LMP) to encourage filters to learn non-repeatable local patterns, a selection strategy for consistency, and a clustering layer for keypoint prediction.

Result: LMPNet discovers robust keypoints and achieves accuracy comparable to supervised models.

Conclusion: The method is effective for weakly-supervised keypoint discovery, offering interpretability and strong performance.

Abstract: In this work, we explore the task of semantic object keypoint discovery
weakly-supervised by only category labels. This is achieved by transforming
discriminatively-trained intermediate layer filters into keypoint detectors. We
begin by identifying three preferred characteristics of keypoint detectors: (i)
spatially sparse activations, (ii) consistency and (iii) diversity. Instead of
relying on hand-crafted loss terms, a novel computationally-efficient leaky max
pooling (LMP) layer is proposed to explicitly encourage final conv-layer
filters to learn "non-repeatable local patterns" that are well aligned with
object keypoints. Informed by visualizations, a simple yet effective selection
strategy is proposed to ensure consistent filter activations and attention
mask-out is then applied to force the network to distribute its attention to
the whole object instead of just the most discriminative region. For the final
keypoint prediction, a learnable clustering layer is proposed to group keypoint
proposals into keypoint predictions. The final model, named LMPNet, is highly
interpretable in that it directly manipulates network filters to detect
predefined concepts. Our experiments show that LMPNet can (i) automatically
discover semantic keypoints that are robust to object pose and (ii) achieves
strong prediction accuracy comparable to a supervised pose estimation model.

</details>


### [20] [Perception Activator: An intuitive and portable framework for brain cognitive exploration](https://arxiv.org/abs/2507.02311)
*Le Xu,Qi Zhang,Qixian Zhang,Hongyun Zhang,Duoqian Miao,Cairong Zhao*

Main category: cs.CV

TL;DR: The paper introduces a framework using fMRI representations to improve semantic alignment in brain-vision decoding, enhancing object detection and segmentation accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing brain-vision decoding methods lack fine-grained semantic alignment, leading to reconstruction distortions. The study aims to better understand visual perception and improve semantic integration.

Method: An experimental framework injects fMRI representations into multi-scale image features via cross-attention, evaluating downstream tasks like object detection and segmentation.

Result: Incorporating fMRI signals improves detection and segmentation accuracy, revealing rich multi-object semantic cues and spatial information.

Conclusion: fMRI contains valuable semantic and spatial data that current models underutilize, suggesting potential for enhanced brain-vision decoding.

Abstract: Recent advances in brain-vision decoding have driven significant progress,
reconstructing with high fidelity perceived visual stimuli from neural
activity, e.g., functional magnetic resonance imaging (fMRI), in the human
visual cortex. Most existing methods decode the brain signal using a two-level
strategy, i.e., pixel-level and semantic-level. However, these methods rely
heavily on low-level pixel alignment yet lack sufficient and fine-grained
semantic alignment, resulting in obvious reconstruction distortions of multiple
semantic objects. To better understand the brain's visual perception patterns
and how current decoding models process semantic objects, we have developed an
experimental framework that uses fMRI representations as intervention
conditions. By injecting these representations into multi-scale image features
via cross-attention, we compare both downstream performance and intermediate
feature changes on object detection and instance segmentation tasks with and
without fMRI information. Our results demonstrate that incorporating fMRI
signals enhances the accuracy of downstream detection and segmentation,
confirming that fMRI contains rich multi-object semantic cues and coarse
spatial localization information-elements that current models have yet to fully
exploit or integrate.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [21] [STELLA: Self-Evolving LLM Agent for Biomedical Research](https://arxiv.org/abs/2507.02004)
*Ruofan Jin,Zaixi Zhang,Mengdi Wang,Le Cong*

Main category: cs.AI

TL;DR: STELLA is a self-evolving AI agent for biomedical research, using a multi-agent architecture to autonomously improve its reasoning and tool integration, outperforming state-of-the-art models.


<details>
  <summary>Details</summary>
Motivation: The fragmented and rapidly growing biomedical research landscape exceeds human expertise, requiring adaptive AI solutions.

Method: STELLA employs a multi-agent architecture with an evolving Template Library and dynamic Tool Ocean, autonomously integrating new tools and learning from experience.

Result: STELLA achieves top accuracy on biomedical benchmarks (26% on Humanity's Last Exam, 54% on LAB-Bench: DBQA, 63% on LAB-Bench: LitQA), improving with experience.

Conclusion: STELLA advances AI systems by dynamically scaling expertise, accelerating biomedical discovery.

Abstract: The rapid growth of biomedical data, tools, and literature has created a
fragmented research landscape that outpaces human expertise. While AI agents
offer a solution, they typically rely on static, manually curated toolsets,
limiting their ability to adapt and scale. Here, we introduce STELLA, a
self-evolving AI agent designed to overcome these limitations. STELLA employs a
multi-agent architecture that autonomously improves its own capabilities
through two core mechanisms: an evolving Template Library for reasoning
strategies and a dynamic Tool Ocean that expands as a Tool Creation Agent
automatically discovers and integrates new bioinformatics tools. This allows
STELLA to learn from experience. We demonstrate that STELLA achieves
state-of-the-art accuracy on a suite of biomedical benchmarks, scoring
approximately 26\% on Humanity's Last Exam: Biomedicine, 54\% on LAB-Bench:
DBQA, and 63\% on LAB-Bench: LitQA, outperforming leading models by up to 6
percentage points. More importantly, we show that its performance
systematically improves with experience; for instance, its accuracy on the
Humanity's Last Exam benchmark almost doubles with increased trials. STELLA
represents a significant advance towards AI Agent systems that can learn and
grow, dynamically scaling their expertise to accelerate the pace of biomedical
discovery.

</details>


### [22] [HCVR: A Hybrid Approach with Correlation-aware Voting Rules for Feature Selection](https://arxiv.org/abs/2507.02073)
*Nikita Bhedasgaonkar,Rushikesh K. Joshi*

Main category: cs.AI

TL;DR: HCVR is a hybrid feature selection method combining P2P and P2T correlations, using voting rules to eliminate redundant features. It outperforms traditional methods on the SPAMBASE dataset.


<details>
  <summary>Details</summary>
Motivation: To improve feature selection by combining non-iterative and iterative approaches, leveraging correlations to retain relevant features efficiently.

Method: HCVR uses backward elimination with majority voting rules based on correlation thresholds (P2P and P2T). It's greedy and eliminates multiple features per step.

Result: HCVR showed better performance than traditional non-iterative (CFS, mRMR, MI) and iterative (RFE, SFS, Genetic Algorithm) methods on SPAMBASE.

Conclusion: HCVR is effective for dimensionality reduction, outperforming existing techniques in retaining relevant features and improving classifier performance.

Abstract: In this paper, we propose HCVR (Hybrid approach with Correlation-aware Voting
Rules), a lightweight rule-based feature selection method that combines
Parameter-to-Parameter (P2P) and Parameter-to-Target (P2T) correlations to
eliminate redundant features and retain relevant ones. This method is a hybrid
of non-iterative and iterative filtering approaches for dimensionality
reduction. It is a greedy method, which works by backward elimination,
eliminating possibly multiple features at every step. The rules contribute to
voting for features, and a decision to keep or discard is made by majority
voting. The rules make use of correlation thresholds between every pair of
features, and between features and the target. We provide the results from the
application of HCVR to the SPAMBASE dataset. The results showed improvement
performance as compared to traditional non-iterative (CFS, mRMR and MI) and
iterative (RFE, SFS and Genetic Algorithm) techniques. The effectiveness was
assessed based on the performance of different classifiers after applying
filtering.

</details>


### [23] [Reasoning on a Budget: A Survey of Adaptive and Controllable Test-Time Compute in LLMs](https://arxiv.org/abs/2507.02076)
*Mohammad Ali Alomrani,Yingxue Zhang,Derek Li,Qianyi Sun,Soumyasundar Pal,Zhanguang Zhang,Yaochen Hu,Rohan Deepak Ajwani,Antonios Valkanas,Raika Karimi,Peng Cheng,Yunzhou Wang,Pengyi Liao,Hanrui Huang,Bin Wang,Jianye Hao,Mark Coates*

Main category: cs.AI

TL;DR: The paper reviews efficient test-time compute (TTC) strategies for improving LLM reasoning, categorizing methods into fixed-budget (L1) and dynamic-scaling (L2) approaches. It benchmarks LLMs, highlighting performance-token trade-offs, and discusses future trends like hybrid models.


<details>
  <summary>Details</summary>
Motivation: Current LLMs inefficiently allocate compute for reasoning, overthinking simple tasks and underthinking hard ones, necessitating better TTC strategies.

Method: Introduces a taxonomy of TTC methods: L1 (fixed compute) and L2 (dynamic scaling). Benchmarks proprietary LLMs on diverse datasets.

Result: Identifies trade-offs between reasoning performance and token usage, emphasizing practical control and adaptability of TTC methods.

Conclusion: Future work should focus on hybrid models and challenges to enhance LLM efficiency, robustness, and user responsiveness.

Abstract: Large language models (LLMs) have rapidly progressed into general-purpose
agents capable of solving a broad spectrum of tasks. However, current models
remain inefficient at reasoning: they apply fixed inference-time compute
regardless of task complexity, often overthinking simple problems while
underthinking hard ones. This survey presents a comprehensive review of
efficient test-time compute (TTC) strategies, which aim to improve the
computational efficiency of LLM reasoning. We introduce a two-tiered taxonomy
that distinguishes between L1-controllability, methods that operate under fixed
compute budgets, and L2-adaptiveness, methods that dynamically scale inference
based on input difficulty or model confidence. We benchmark leading proprietary
LLMs across diverse datasets, highlighting critical trade-offs between
reasoning performance and token usage. Compared to prior surveys on efficient
reasoning, our review emphasizes the practical control, adaptability, and
scalability of TTC methods. Finally, we discuss emerging trends such as hybrid
thinking models and identify key challenges for future work towards making LLMs
more computationally efficient, robust, and responsive to user constraints.

</details>


### [24] [Measuring Scientific Capabilities of Language Models with a Systems Biology Dry Lab](https://arxiv.org/abs/2507.02083)
*Haonan Duan,Stephen Zhewen Lu,Caitlin Fiona Harrigan,Nishkrit Desai,Jiarui Lu,Michał Koziarski,Leonardo Cotta,Chris J. Maddison*

Main category: cs.AI

TL;DR: SciGym is a benchmark for evaluating LLMs' experiment design and analysis skills in biology, using simulated data to avoid wet-lab costs. Results show LLMs struggle with complex systems.


<details>
  <summary>Details</summary>
Motivation: To assess LLMs' scientific competencies in experiment design and analysis, which current benchmarks overlook due to wet-lab constraints.

Method: SciGym uses dry-lab biological models (encoded in Systems Biology Markup Language) to simulate data for evaluating LLMs on 137 small and 350 total systems.

Result: More capable LLMs performed better, but all declined significantly with increased system complexity.

Conclusion: LLMs have room for improvement in scientific capabilities, especially for complex tasks.

Abstract: Designing experiments and result interpretations are core scientific
competencies, particularly in biology, where researchers perturb complex
systems to uncover the underlying systems. Recent efforts to evaluate the
scientific capabilities of large language models (LLMs) fail to test these
competencies because wet-lab experimentation is prohibitively expensive: in
expertise, time and equipment. We introduce SciGym, a first-in-class benchmark
that assesses LLMs' iterative experiment design and analysis abilities in
open-ended scientific discovery tasks. SciGym overcomes the challenge of
wet-lab costs by running a dry lab of biological systems. These models, encoded
in Systems Biology Markup Language, are efficient for generating simulated
data, making them ideal testbeds for experimentation on realistically complex
systems. We evaluated six frontier LLMs on 137 small systems, and released a
total of 350 systems. Our evaluation shows that while more capable models
demonstrated superior performance, all models' performance declined
significantly as system complexity increased, suggesting substantial room for
improvement in the scientific capabilities of LLM agents.

</details>


### [25] [What Neuroscience Can Teach AI About Learning in Continuously Changing Environments](https://arxiv.org/abs/2507.02103)
*Daniel Durstewitz,Bruno Averbeck,Georgia Koppe*

Main category: cs.AI

TL;DR: The paper explores how neuroscience can inspire AI to achieve rapid, adaptive learning, similar to animals, and vice versa, contributing to NeuroAI.


<details>
  <summary>Details</summary>
Motivation: AI models lack the adaptive, continuous learning capabilities seen in animals, especially in dynamic social environments. Bridging this gap could enhance AI systems like robots or autonomous vehicles.

Method: Integrates AI literature on continual and in-context learning with neuroscience studies on behavioral shifts and reward changes.

Result: Proposes a framework for neuroscience to inform AI development and vice versa, fostering advancements in NeuroAI.

Conclusion: Collaboration between AI and neuroscience can drive adaptive learning in AI and deepen understanding of biological learning processes.

Abstract: Modern AI models, such as large language models, are usually trained once on
a huge corpus of data, potentially fine-tuned for a specific task, and then
deployed with fixed parameters. Their training is costly, slow, and gradual,
requiring billions of repetitions. In stark contrast, animals continuously
adapt to the ever-changing contingencies in their environments. This is
particularly important for social species, where behavioral policies and reward
outcomes may frequently change in interaction with peers. The underlying
computational processes are often marked by rapid shifts in an animal's
behaviour and rather sudden transitions in neuronal population activity. Such
computational capacities are of growing importance for AI systems operating in
the real world, like those guiding robots or autonomous vehicles, or for
agentic AI interacting with humans online. Can AI learn from neuroscience? This
Perspective explores this question, integrating the literature on continual and
in-context learning in AI with the neuroscience of learning on behavioral tasks
with shifting rules, reward probabilities, or outcomes. We will outline an
agenda for how specifically insights from neuroscience may inform current
developments in AI in this area, and - vice versa - what neuroscience may learn
from AI, contributing to the evolving field of NeuroAI.

</details>


### [26] [The Illusion of Fairness: Auditing Fairness Interventions with Audit Studies](https://arxiv.org/abs/2507.02152)
*Disa Sariola,Patrick Button,Aron Culotta,Nicholas Mattei*

Main category: cs.AI

TL;DR: The paper explores using audit study data to improve fairness in AI hiring algorithms, revealing flaws in traditional bias-mitigation methods and proposing new interventions.


<details>
  <summary>Details</summary>
Motivation: To address biases in AI hiring systems by leveraging high-quality audit study data, which provides rigorous estimates of discrimination compared to convenience samples.

Method: Utilizes audit study data to evaluate and train hiring algorithms, comparing traditional fairness interventions (like equalizing base rates) with new methods based on individual treatment effect estimation.

Result: Traditional fairness methods show 10% disparity when properly measured, while the proposed interventions further reduce algorithmic discrimination.

Conclusion: Audit study data enhances fairness in AI hiring systems, exposing limitations of common methods and offering more effective alternatives.

Abstract: Artificial intelligence systems, especially those using machine learning, are
being deployed in domains from hiring to loan issuance in order to automate
these complex decisions. Judging both the effectiveness and fairness of these
AI systems, and their human decision making counterpart, is a complex and
important topic studied across both computational and social sciences. Within
machine learning, a common way to address bias in downstream classifiers is to
resample the training data to offset disparities. For example, if hiring rates
vary by some protected class, then one may equalize the rate within the
training set to alleviate bias in the resulting classifier. While simple and
seemingly effective, these methods have typically only been evaluated using
data obtained through convenience samples, introducing selection bias and label
bias into metrics. Within the social sciences, psychology, public health, and
medicine, audit studies, in which fictitious ``testers'' (e.g., resumes,
emails, patient actors) are sent to subjects (e.g., job openings, businesses,
doctors) in randomized control trials, provide high quality data that support
rigorous estimates of discrimination. In this paper, we investigate how data
from audit studies can be used to improve our ability to both train and
evaluate automated hiring algorithms. We find that such data reveals cases
where the common fairness intervention method of equalizing base rates across
classes appears to achieve parity using traditional measures, but in fact has
roughly 10% disparity when measured appropriately. We additionally introduce
interventions based on individual treatment effect estimation methods that
further reduce algorithmic discrimination using this data.

</details>


### [27] [Data Diversification Methods In Alignment Enhance Math Performance In LLMs](https://arxiv.org/abs/2507.02173)
*Berkan Dokmeci,Qingyang Wu,Ben Athiwaratkun,Ce Zhang,Shuaiwen Leon Song,James Zou*

Main category: cs.AI

TL;DR: Diversified-Think-Solve (DTS) improves LLMs' mathematical reasoning by diversifying preference data, outperforming traditional methods with minimal computational cost.


<details>
  <summary>Details</summary>
Motivation: Mathematical reasoning remains a challenge for LLMs despite advances in preference learning. The study explores how diversified preference data can enhance this ability.

Method: Evaluated temperature sampling, Chain-of-Thought prompting, and MCTS, and introduced DTS, a structured approach to decompose problems into diverse reasoning paths.

Result: DTS improved performance by 7.1% on GSM8K and 4.2% on MATH, with only 1.03x computational overhead, while MCTS was 5x costlier with lower returns.

Conclusion: Structured exploration of diverse problem-solving methods (DTS) creates more effective preference data for mathematical alignment than traditional approaches.

Abstract: While recent advances in preference learning have enhanced alignment in human
feedback, mathematical reasoning remains a persistent challenge. We investigate
how data diversification strategies in preference optimization can improve the
mathematical reasoning abilities of large language models (LLMs). We evaluate
three common data generation methods: temperature sampling, Chain-of-Thought
prompting, and Monte Carlo Tree Search (MCTS), and introduce
Diversified-ThinkSolve (DTS), a novel structured approach that systematically
decomposes problems into diverse reasoning paths. Our results show that with
strategically diversified preference data, models can substantially improve
mathematical reasoning performance, with the best approach yielding gains of
7.1% on GSM8K and 4.2% on MATH over the base model. Despite its strong
performance, DTS incurs only a marginal computational overhead (1.03x) compared
to the baseline, while MCTS is nearly five times more costly with lower
returns. These findings demonstrate that structured exploration of diverse
problem-solving methods creates more effective preference data for mathematical
alignment than traditional approaches.

</details>


### [28] [Do Role-Playing Agents Practice What They Preach? Belief-Behavior Consistency in LLM-Based Simulations of Human Trust](https://arxiv.org/abs/2507.02197)
*Amogh Mannekote,Adam Davies,Guohao Li,Kristy Elizabeth Boyer,ChengXiang Zhai,Bonnie J Dorr,Francesco Pinto*

Main category: cs.AI

TL;DR: The paper examines the consistency between LLMs' stated beliefs and their behavior in role-playing simulations, introducing a metric to measure this and identifying factors affecting alignment.


<details>
  <summary>Details</summary>
Motivation: Ensuring LLM-based role-playing agents' outputs align with their assigned roles is critical for reliable synthetic data in human behavioral research.

Method: An evaluation framework using the GenAgents persona bank and the Trust Game measures belief-behavior consistency, exploring factors like belief types, information presentation, and forecasting depth.

Result: Systematic inconsistencies between LLMs' stated beliefs and simulation outcomes are found, even when beliefs seem plausible.

Conclusion: Researchers must identify when LLMs' beliefs align with behavior to use them appropriately in behavioral studies.

Abstract: As LLMs are increasingly studied as role-playing agents to generate synthetic
data for human behavioral research, ensuring that their outputs remain coherent
with their assigned roles has become a critical concern. In this paper, we
investigate how consistently LLM-based role-playing agents' stated beliefs
about the behavior of the people they are asked to role-play ("what they say")
correspond to their actual behavior during role-play ("how they act").
Specifically, we establish an evaluation framework to rigorously measure how
well beliefs obtained by prompting the model can predict simulation outcomes in
advance. Using an augmented version of the GenAgents persona bank and the Trust
Game (a standard economic game used to quantify players' trust and
reciprocity), we introduce a belief-behavior consistency metric to
systematically investigate how it is affected by factors such as: (1) the types
of beliefs we elicit from LLMs, like expected outcomes of simulations versus
task-relevant attributes of individual characters LLMs are asked to simulate;
(2) when and how we present LLMs with relevant information about Trust Game;
and (3) how far into the future we ask the model to forecast its actions. We
also explore how feasible it is to impose a researcher's own theoretical priors
in the event that the originally elicited beliefs are misaligned with research
objectives. Our results reveal systematic inconsistencies between LLMs' stated
(or imposed) beliefs and the outcomes of their role-playing simulation, at both
an individual- and population-level. Specifically, we find that, even when
models appear to encode plausible beliefs, they may fail to apply them in a
consistent way. These findings highlight the need to identify how and when
LLMs' stated beliefs align with their simulated behavior, allowing researchers
to use LLM-based agents appropriately in behavioral studies.

</details>


### [29] [Dilution, Diffusion and Symbiosis in Spatial Prisoner's Dilemma with Reinforcement Learning](https://arxiv.org/abs/2507.02211)
*Gustavo C. Mangold,Heitor C. M. Fernandes,Mendeli H. Vainstein*

Main category: cs.AI

TL;DR: The paper explores how dilution and mobility affect cooperation in spatial prisoner's dilemma games using multi-agent Q-learning, showing equivalence between fixed and learned update rules and symbiotic effects.


<details>
  <summary>Details</summary>
Motivation: To understand the impact of dilution and mobility on cooperation in spatial prisoner's dilemma games using reinforcement learning.

Method: Uses an independent multi-agent Q-learning algorithm to model different game-theoretical scenarios.

Result: Observes equivalence between fixed and learned update rules and emergence of symbiotic mutualistic effects.

Conclusion: Demonstrates the algorithm's versatility and benchmarking potential in modeling diverse game-theoretical scenarios.

Abstract: Recent studies in the spatial prisoner's dilemma games with reinforcement
learning have shown that static agents can learn to cooperate through a diverse
sort of mechanisms, including noise injection, different types of learning
algorithms and neighbours' payoff knowledge.In this work, using an independent
multi-agent Q-learning algorithm, we study the effects of dilution and mobility
in the spatial version of the prisoner's dilemma. Within this setting,
different possible actions for the algorithm are defined, connecting with
previous results on the classical, non-reinforcement learning spatial
prisoner's dilemma, showcasing the versatility of the algorithm in modeling
different game-theoretical scenarios and the benchmarking potential of this
approach.As a result, a range of effects is observed, including evidence that
games with fixed update rules can be qualitatively equivalent to those with
learned ones, as well as the emergence of a symbiotic mutualistic effect
between populations that forms when multiple actions are defined.

</details>


### [30] [Scaling LLM Planning: NL2FLOW for Parametric Problem Generation and Rigorous Evaluation](https://arxiv.org/abs/2507.02253)
*Jungkoo Kang*

Main category: cs.AI

TL;DR: NL2FLOW automates planning problem generation and evaluation for LLMs, showing 86% success in valid plans and 69% in optimal plans, with insights on task decomposition effects.


<details>
  <summary>Details</summary>
Motivation: Address the bottleneck of scalable, reliable data generation and evaluation for improving LLM planning and reasoning.

Method: Introduces NL2FLOW, a system for generating planning problems in natural language, structured representation, and PDDL, and evaluates LLMs on these.

Result: Highest models achieved 86% success in valid plans and 69% in optimal plans, with task decomposition degrading performance.

Conclusion: Dynamic understanding of LLM limitations and systematic tools are crucial for advancing their problem-solving potential.

Abstract: Progress in enhancing large language model (LLM) planning and reasoning
capabilities is significantly hampered by the bottleneck of scalable, reliable
data generation and evaluation. To overcome this, I introduce NL2FLOW, a fully
automated system for parametrically generating planning problems - expressed in
natural language, a structured intermediate representation, and formal PDDL -
and rigorously evaluating the quality of generated plans. I demonstrate
NL2FLOW's capabilities by generating a dataset of 2296 problems in the
automated workflow generation domain and evaluating multiple open-sourced,
instruct-tuned LLMs. My results reveal that the highest performing models
achieved 86% success in generating valid plans and 69% in generating optimal
plans, specifically for problems with feasible solutions. Regression analysis
shows that the influence of problem characteristics on plan generation is
contingent on both model and prompt design. Notably, I observed that the
highest success rate for translating natural language into a JSON
representation of a plan was lower than the highest rate of generating a valid
plan directly. This suggests that unnecessarily decomposing the reasoning task
- introducing intermediate translation steps - may actually degrade
performance, implying a benefit to models capable of reasoning directly from
natural language to action. As I scale LLM reasoning to increasingly complex
problems, the bottlenecks and sources of error within these systems will
inevitably shift. Therefore, a dynamic understanding of these limitations - and
the tools to systematically reveal them - will be crucial for unlocking the
full potential of LLMs as intelligent problem solvers.

</details>


### [31] [Iterated belief revision: from postulates to abilities](https://arxiv.org/abs/2507.02319)
*Paolo Liberatore*

Main category: cs.AI

TL;DR: The paper critiques belief revision approaches for focusing on postulates (constraints) rather than abilities (flexibility to reach diverse belief states). It highlights the need for mechanisms to achieve various doxastic states (e.g., plastic, dogmatic) and evaluates existing methods for these abilities.


<details>
  <summary>Details</summary>
Motivation: To address the lack of analysis of belief revision mechanisms' flexibility and their ability to reach diverse belief states, moving beyond syntactic postulates.

Method: Evaluates existing belief revision mechanisms (e.g., lexicographic, natural, radical) for their abilities to achieve specific doxastic states (e.g., plastic, dogmatic).

Result: Identifies which revision mechanisms possess certain abilities (e.g., plasticity, dogmatism) and lack others, providing a taxonomy of their capabilities.

Conclusion: Belief revision mechanisms should be assessed for their abilities to reach diverse belief states, not just syntactic constraints, to better suit practical applications.

Abstract: The belief revision field is opulent in new proposals and indigent in
analyses of existing approaches. Much work hinge on postulates, employed as
syntactic characterizations: some revision mechanism is equivalent to some
properties. Postulates constraint specific revision instances: certain
revisions update certain beliefs in a certain way. As an example, if the
revision is consistent with the current beliefs, it is incorporated with no
other change. A postulate like this tells what revisions must do and neglect
what they can do. Can they reach a certain state of beliefs? Can they reach all
possible states of beliefs? Can they reach all possible states of beliefs from
no previous belief? Can they reach a dogmatic state of beliefs, where
everything not believed is impossible? Can they make two conditions equally
believed? An application where every possible state of beliefs is sensible
requires each state of beliefs to be reachable. An application where conditions
may be equally believed requires such a belief state to be reachable. An
application where beliefs may become dogmatic requires a way to make them
dogmatic. Such doxastic states need to be reached in a way or another. Not in
specific way, as dictated by a typical belief revision postulate. This is an
ability, not a constraint: the ability of being plastic, equating, dogmatic.
Amnesic, correcting, believer, damascan, learnable are other abilities. Each
revision mechanism owns some of these abilities and lacks the others:
lexicographic, natural, restrained, very radical, full meet, radical, severe,
moderate severe, deep severe, plain severe and deep severe revisions, each of
these revisions is proved to possess certain abilities.

</details>


### [32] [OMS: On-the-fly, Multi-Objective, Self-Reflective Ad Keyword Generation via LLM Agent](https://arxiv.org/abs/2507.02353)
*Bowen Chen,Zhao Wang,Shingo Takamatsu*

Main category: cs.AI

TL;DR: OMS is a keyword generation framework for sponsored search ads, addressing LLM limitations by being on-the-fly, multi-objective, and self-reflective, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: LLM-based keyword generation lacks training data independence, multi-objective optimization, and quality control, hindering full automation.

Method: OMS is on-the-fly (no training data), multi-objective (optimizes multiple metrics), and self-reflective (evaluates keyword quality).

Result: OMS outperforms existing methods in benchmarks and real-world campaigns, with ablation and human evaluations confirming its effectiveness.

Conclusion: OMS successfully addresses LLM limitations, offering a robust solution for automated keyword generation in sponsored search advertising.

Abstract: Keyword decision in Sponsored Search Advertising is critical to the success
of ad campaigns. While LLM-based methods offer automated keyword generation,
they face three major limitations: reliance on large-scale query-keyword pair
data, lack of online multi-objective performance monitoring and optimization,
and weak quality control in keyword selection. These issues hinder the agentic
use of LLMs in fully automating keyword decisions by monitoring and reasoning
over key performance indicators such as impressions, clicks, conversions, and
CTA effectiveness. To overcome these challenges, we propose OMS, a keyword
generation framework that is On-the-fly (requires no training data, monitors
online performance, and adapts accordingly), Multi-objective (employs agentic
reasoning to optimize keywords based on multiple performance metrics), and
Self-reflective (agentically evaluates keyword quality). Experiments on
benchmarks and real-world ad campaigns show that OMS outperforms existing
methods; ablation and human evaluations confirm the effectiveness of each
component and the quality of generated keywords.

</details>


### [33] [An AI-native experimental laboratory for autonomous biomolecular engineering](https://arxiv.org/abs/2507.02379)
*Mingyu Wu,Zhaoguo Wang,Jiabin Wang,Zhiyuan Dong,Jingkai Yang,Qingting Li,Tianyu Huang,Lei Zhao,Mingqiang Li,Fei Wang,Chunhai Fan,Haibo Chen*

Main category: cs.AI

TL;DR: An AI-native autonomous laboratory is introduced, capable of managing complex, multi-objective experiments without human intervention, matching human scientist performance.


<details>
  <summary>Details</summary>
Motivation: To enable autonomous scientific research for non-specialists by overcoming limitations of current systems confined to simple workflows.

Method: Co-design of AI models, experiments, and instruments to create an end-to-end, multi-user platform for complex experiments.

Result: The system autonomously optimizes experiments, matches human performance, and improves efficiency in multi-user scenarios.

Conclusion: The platform advances biomaterials research, reduces expert dependency, and enables scalable science-as-a-service.

Abstract: Autonomous scientific research, capable of independently conducting complex
experiments and serving non-specialists, represents a long-held aspiration.
Achieving it requires a fundamental paradigm shift driven by artificial
intelligence (AI). While autonomous experimental systems are emerging, they
remain confined to areas featuring singular objectives and well-defined, simple
experimental workflows, such as chemical synthesis and catalysis. We present an
AI-native autonomous laboratory, targeting highly complex scientific
experiments for applications like autonomous biomolecular engineering. This
system autonomously manages instrumentation, formulates experiment-specific
procedures and optimization heuristics, and concurrently serves multiple user
requests. Founded on a co-design philosophy of models, experiments, and
instruments, the platform supports the co-evolution of AI models and the
automation system. This establishes an end-to-end, multi-user autonomous
laboratory that handles complex, multi-objective experiments across diverse
instrumentation. Our autonomous laboratory supports fundamental nucleic acid
functions-including synthesis, transcription, amplification, and sequencing. It
also enables applications in fields such as disease diagnostics, drug
development, and information storage. Without human intervention, it
autonomously optimizes experimental performance to match state-of-the-art
results achieved by human scientists. In multi-user scenarios, the platform
significantly improves instrument utilization and experimental efficiency. This
platform paves the way for advanced biomaterials research to overcome
dependencies on experts and resource barriers, establishing a blueprint for
science-as-a-service at scale.

</details>


### [34] [The Gauss-Markov Adjunction: Categorical Semantics of Residuals in Supervised Learning](https://arxiv.org/abs/2507.02442)
*Moto Kamiura*

Main category: cs.AI

TL;DR: The paper reformulates machine learning models using category theory to enhance AI explicability, focusing on multiple linear regression and introducing the Gauss-Markov Adjunction to describe parameter-residual interplay.


<details>
  <summary>Details</summary>
Motivation: To improve AI explicability and social implementation by providing a semantic framework for understanding AI systems through category theory.

Method: Reformulates supervised learning (using multiple linear regression) with category theory, defining categories for parameters/data and an adjoint functor pair (Gauss-Markov Adjunction).

Result: Shows how parameter-residual interplay is captured by the adjunction, linking ordinary least squares estimators and residuals via limit preservation.

Conclusion: Proposes this categorical framework as a foundation for AI explicability, extending denotational semantics from theoretical computer science.

Abstract: Enhancing the intelligibility and interpretability of machine learning is a
crucial task in responding to the demand for Explicability as an AI principle,
and in promoting the better social implementation of AI. The aim of our
research is to contribute to this improvement by reformulating machine learning
models through the lens of category theory, thereby developing a semantic
framework for structuring and understanding AI systems. Our categorical
modeling in this paper clarifies and formalizes the structural interplay
between residuals and parameters in supervised learning. The present paper
focuses on the multiple linear regression model, which represents the most
basic form of supervised learning. By defining two concrete categories
corresponding to parameters and data, along with an adjoint pair of functors
between them, we introduce our categorical formulation of supervised learning.
We show that the essential structure of this framework is captured by what we
call the Gauss-Markov Adjunction. Within this setting, the dual flow of
information can be explicitly described as a correspondence between variations
in parameters and residuals. The ordinary least squares estimator for the
parameters and the minimum residual are related via the preservation of limits
by the right adjoint functor. Furthermore, we position this formulation as an
instance of extended denotational semantics for supervised learning, and
propose applying a semantic perspective developed in theoretical computer
science as a formal foundation for Explicability in AI.

</details>


### [35] [Clarifying Before Reasoning: A Coq Prover with Structural Context](https://arxiv.org/abs/2507.02541)
*Yanzhen Lu,Hanbin Yang,Xiaodie Wang,Ge Zhang,Biao Li,Chenxu Fu,Chao Li,Yang Yuan,Andrew Chi-Chih Yao*

Main category: cs.AI

TL;DR: Improving task clarity with structured semantic context enhances reasoning in LLMs, achieving a 2.1x improvement in theorem proving success in Coq.


<details>
  <summary>Details</summary>
Motivation: To explore if better task clarity can boost reasoning in large language models, focusing on theorem proving in Coq.

Method: Introduces a concept-level metric for clarity, adds structured semantic context to inputs, and uses a Planner--Executor architecture with selective concept unfolding.

Result: Achieves 2.1x improvement in proof success (21.8% to 45.8%), outperforming Graph2Tac (33.2%). Fine-tuning smaller models yields 48.6% success.

Conclusion: Structured task representations bridge the gap between understanding and reasoning, proving effective for theorem proving in Coq.

Abstract: In this work, we investigate whether improving task clarity can enhance
reasoning ability of large language models, focusing on theorem proving in Coq.
We introduce a concept-level metric to evaluate task clarity and show that
adding structured semantic context to the standard input used by modern LLMs,
leads to a 1.85$\times$ improvement in clarity score
(44.5\%~$\rightarrow$~82.3\%). Using the general-purpose model
\texttt{DeepSeek-V3}, our approach leads to a 2.1$\times$ improvement in proof
success (21.8\%~$\rightarrow$~45.8\%) and outperforms the previous
state-of-the-art \texttt{Graph2Tac} (33.2\%). We evaluate this on 1,386
theorems randomly sampled from 15 standard Coq packages, following the same
evaluation protocol as \texttt{Graph2Tac}. Furthermore, fine-tuning smaller
models on our structured data can achieve even higher performance (48.6\%). Our
method uses selective concept unfolding to enrich task descriptions, and
employs a Planner--Executor architecture. These findings highlight the value of
structured task representations in bridging the gap between understanding and
reasoning.

</details>


### [36] [AI Research Agents for Machine Learning: Search, Exploration, and Generalization in MLE-bench](https://arxiv.org/abs/2507.02554)
*Edan Toledo,Karen Hambardzumyan,Martin Josifoski,Rishi Hazra,Nicolas Baldwin,Alexis Audran-Reiss,Michael Kuchnik,Despoina Magka,Minqi Jiang,Alisia Maria Lupidi,Andrei Lupu,Roberta Raileanu,Kelvin Niu,Tatiana Shavrina,Jean-Christophe Gagnon-Audet,Michael Shvartsman,Shagun Sodhani,Alexander H. Miller,Abhishek Charnalia,Derek Dunfield,Carole-Jean Wu,Pontus Stenetorp,Nicola Cancedda,Jakob Nicolaus Foerster,Yoram Bachrach*

Main category: cs.AI

TL;DR: AI research agents improve performance on MLE-bench by optimizing search policies and operator sets, achieving a 47.7% success rate in Kaggle competitions.


<details>
  <summary>Details</summary>
Motivation: To accelerate scientific progress by automating machine learning model design and training, focusing on improving AI research agents' performance in real-world challenges like Kaggle competitions.

Method: Formalized AI research agents as search policies navigating solution spaces, testing various operator sets and search strategies (Greedy, MCTS, Evolutionary).

Result: Best pairing of search strategy and operator set increased success rate from 39.6% to 47.7% on MLE-bench lite.

Conclusion: Joint consideration of search strategy, operator design, and evaluation methodology is crucial for advancing automated machine learning.

Abstract: AI research agents are demonstrating great potential to accelerate scientific
progress by automating the design, implementation, and training of machine
learning models. We focus on methods for improving agents' performance on
MLE-bench, a challenging benchmark where agents compete in Kaggle competitions
to solve real-world machine learning problems. We formalize AI research agents
as search policies that navigate a space of candidate solutions, iteratively
modifying them using operators. By designing and systematically varying
different operator sets and search policies (Greedy, MCTS, Evolutionary), we
show that their interplay is critical for achieving high performance. Our best
pairing of search strategy and operator set achieves a state-of-the-art result
on MLE-bench lite, increasing the success rate of achieving a Kaggle medal from
39.6% to 47.7%. Our investigation underscores the importance of jointly
considering the search strategy, operator design, and evaluation methodology in
advancing automated machine learning.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [37] [McBE: A Multi-task Chinese Bias Evaluation Benchmark for Large Language Models](https://arxiv.org/abs/2507.02088)
*Tian Lan,Xiangdong Su,Xu Liu,Ruirui Wang,Ke Chang,Jiang Li,Guanglai Gao*

Main category: cs.CL

TL;DR: The paper introduces McBE, a multi-task Chinese bias evaluation benchmark for LLMs, addressing gaps in existing datasets by covering diverse bias categories and tasks.


<details>
  <summary>Details</summary>
Motivation: Existing bias evaluation datasets are limited to English/North American culture and lack multi-task support, necessitating a culturally relevant and comprehensive benchmark for Chinese LLMs.

Method: Developed McBE with 4,077 instances, 12 bias categories, 82 subcategories, and 5 evaluation tasks to assess LLM biases.

Result: Popular LLMs exhibited varying bias levels, analyzed in-depth for novel insights.

Conclusion: McBE provides a robust tool for evaluating and mitigating biases in Chinese LLMs, filling a critical gap in bias assessment.

Abstract: As large language models (LLMs) are increasingly applied to various NLP
tasks, their inherent biases are gradually disclosed. Therefore, measuring
biases in LLMs is crucial to mitigate its ethical risks. However, most existing
bias evaluation datasets focus on English and North American culture, and their
bias categories are not fully applicable to other cultures. The datasets
grounded in the Chinese language and culture are scarce. More importantly,
these datasets usually only support single evaluation tasks and cannot evaluate
the bias from multiple aspects in LLMs. To address these issues, we present a
Multi-task Chinese Bias Evaluation Benchmark (McBE) that includes 4,077 bias
evaluation instances, covering 12 single bias categories, 82 subcategories and
introducing 5 evaluation tasks, providing extensive category coverage, content
diversity, and measuring comprehensiveness. Additionally, we evaluate several
popular LLMs from different series and with parameter sizes. In general, all
these LLMs demonstrated varying degrees of bias. We conduct an in-depth
analysis of results, offering novel insights into bias in LLMs.

</details>


### [38] [Reasoning or Not? A Comprehensive Evaluation of Reasoning LLMs for Dialogue Summarization](https://arxiv.org/abs/2507.02145)
*Keyan Jin,Yapeng Wang,Leonel Santos,Tao Fang,Xu Yang,Sio Kei Im,Hugo Gonçalo Oliveira*

Main category: cs.CL

TL;DR: The paper evaluates reasoning and non-reasoning LLMs for dialogue summarization, finding stepwise reasoning often leads to verbosity and inconsistencies, contrary to trends in other tasks.


<details>
  <summary>Details</summary>
Motivation: To assess the effectiveness of reasoning LLMs (like Long Chain-of-Thought) in dialogue summarization, given their unexplored potential in scenarios needing abstraction and conciseness.

Method: Comprehensive evaluation of reasoning and non-reasoning LLMs across three paradigms (generic, role-oriented, query-oriented) using benchmarks (SAMSum, DialogSum, CSDS, QMSum) and advanced metrics.

Result: Stepwise reasoning does not consistently improve summarization quality; reasoning LLMs often produce verbose, inconsistent, and less concise summaries.

Conclusion: Current reasoning LLMs have limitations in dialogue summarization, highlighting the need for targeted modeling and evaluation strategies.

Abstract: Dialogue summarization is a challenging task with significant practical value
in customer service, meeting analysis, and conversational AI. Although large
language models (LLMs) have achieved substantial progress in summarization
tasks, the performance of step-by-step reasoning architectures-specifically
Long Chain-of-Thought (CoT) implementations such as OpenAI-o1 and
DeepSeek-R1-remains unexplored for dialogue scenarios requiring concurrent
abstraction and conciseness. In this work, we present the first comprehensive
and systematic evaluation of state-of-the-art reasoning LLMs and non-reasoning
LLMs across three major paradigms-generic, role-oriented, and query-oriented
dialogue summarization. Our study spans diverse languages, domains, and summary
lengths, leveraging strong benchmarks (SAMSum, DialogSum, CSDS, and QMSum) and
advanced evaluation protocols that include both LLM-based automatic metrics and
human-inspired criteria. Contrary to trends in other reasoning-intensive tasks,
our findings show that explicit stepwise reasoning does not consistently
improve dialogue summarization quality. Instead, reasoning LLMs are often prone
to verbosity, factual inconsistencies, and less concise summaries compared to
their non-reasoning counterparts. Through scenario-specific analyses and
detailed case studies, we further identify when and why explicit reasoning may
fail to benefit-or even hinder-summarization in complex dialogue contexts. Our
work provides new insights into the limitations of current reasoning LLMs and
highlights the need for targeted modeling and evaluation strategies for
real-world dialogue summarization.

</details>


### [39] [Latent Chain-of-Thought? Decoding the Depth-Recurrent Transformer](https://arxiv.org/abs/2507.02199)
*Wenquan Lu,Yuechuan Yang,Kyle Lee,Yanshu Li,Enqi Liu*

Main category: cs.CL

TL;DR: The paper investigates latent Chain-of-Thought (CoT) reasoning in Huginn-3.5B, a depth-recurrent Transformer, finding limited interpretable evidence and probing inconsistencies.


<details>
  <summary>Details</summary>
Motivation: To explore whether latent reasoning structures emerge in Huginn-3.5B, aiming to internalize reasoning without externalizing steps.

Method: Probing techniques like Logit Lens and Coda Lens are used to analyze the model's internal behavior on arithmetic tasks.

Result: Limited evidence of interpretable latent CoT; probing inconsistencies across recurrent blocks; marginal gains from increased recurrence depth.

Conclusion: Latent CoT in Huginn-3.5B is less effective than models explicitly externalizing reasoning steps.

Abstract: Chain-of-thought (CoT) reasoning has enabled transformer-based language
models to excel at complex mathematics and multi-step planning. However, in
standard decoder-only architectures, these reasoning steps are externalized in
natural language, improving interpretability at the cost of efficiency. To
capture reasoning that is not easily represented in words, many works have
explored recurrent architectures that aim to internalize reasoning in latent
space, potentially supporting latent CoT. In this paper, we investigate whether
such reasoning structures emerge in Huginn-3.5B, a depth-recurrent Transformer
that reuses layers at inference time without increasing parameter count. We
examine the model's internal behavior on arithmetic tasks using a suite of
probing techniques including the Logit Lens and Coda Lens. Our findings reveal
limited evidence of interpretable latent CoT by tracking rank trajectories of
final and intermediate result tokens. Furthermore, we uncover significant
probing inconsistencies across recurrent blocks, where the interpretability of
hidden states depends heavily on both the layer index and the decoding method.
Finally, we empirically show that increasing recurrence depth yields only
marginal gains and falls well short of models that explicitly externalize
reasoning steps. The code is available at
https://github.com/wenquanlu/huginn-latent-cot.

</details>


### [40] [GDC Cohort Copilot: An AI Copilot for Curating Cohorts from the Genomic Data Commons](https://arxiv.org/abs/2507.02221)
*Steven Song,Anirudh Subramanyam,Zhenyu Zhang,Aarti Venkat,Robert L. Grossman*

Main category: cs.CL

TL;DR: GDC Cohort Copilot is an open-source tool that uses LLMs to convert natural language descriptions into GDC cohort filters, outperforming GPT-4o.


<details>
  <summary>Details</summary>
Motivation: Users struggle to navigate complex cohort creation in GDC; natural language descriptions could simplify this.

Method: Developed and evaluated multiple LLMs, including a locally-served open-source model, to generate cohort filters from natural language.

Result: The GDC Cohort LLM outperformed GPT-4o in generating accurate cohorts.

Conclusion: GDC Cohort Copilot effectively bridges the gap between natural language and complex cohort creation in GDC.

Abstract: Motivation: The Genomic Data Commons (GDC) provides access to high quality,
harmonized cancer genomics data through a unified curation and analysis
platform centered around patient cohorts. While GDC users can interactively
create complex cohorts through the graphical Cohort Builder, users (especially
new ones) may struggle to find specific cohort descriptors across hundreds of
possible fields and properties. However, users may be better able to describe
their desired cohort in free-text natural language.
  Results: We introduce GDC Cohort Copilot, an open-source copilot tool for
curating cohorts from the GDC. GDC Cohort Copilot automatically generates the
GDC cohort filter corresponding to a user-input natural language description of
their desired cohort, before exporting the cohort back to the GDC for further
analysis. An interactive user interface allows users to further refine the
generated cohort. We develop and evaluate multiple large language models (LLMs)
for GDC Cohort Copilot and demonstrate that our locally-served, open-source GDC
Cohort LLM achieves better results than GPT-4o prompting in generating GDC
cohorts.
  Availability and implementation: The standalone docker image for GDC Cohort
Copilot is available at https://quay.io/repository/cdis/gdc-cohort-copilot.
Source code is available at https://github.com/uc-cdis/gdc-cohort-copilot. GDC
Cohort LLM weights are available at https://huggingface.co/uc-ctds.

</details>


### [41] [MemAgent: Reshaping Long-Context LLM with Multi-Conv RL-based Memory Agent](https://arxiv.org/abs/2507.02259)
*Hongli Yu,Tinghong Chen,Jiangtao Feng,Jiangjie Chen,Weinan Dai,Qiying Yu,Ya-Qin Zhang,Wei-Ying Ma,Jingjing Liu,Mingxuan Wang,Hao Zhou*

Main category: cs.CL

TL;DR: MemAgent, a novel agent workflow, optimizes long-text tasks with linear complexity, achieving high performance on large-scale QA tasks.


<details>
  <summary>Details</summary>
Motivation: Handling infinitely long documents efficiently without performance degradation is a key challenge in long-text processing.

Method: Introduces MemAgent, which processes text in segments and updates memory with an overwrite strategy, extending the DAPO algorithm for training.

Result: Achieves <5% performance loss on a 3.5M QA task and 95%+ accuracy on a 512K RULER test.

Conclusion: MemAgent demonstrates strong long-context capabilities and scalability.

Abstract: Despite improvements by length extrapolation, efficient attention and memory
modules, handling infinitely long documents with linear complexity without
performance degradation during extrapolation remains the ultimate challenge in
long-text processing. We directly optimize for long-text tasks in an end-to-end
fashion and introduce a novel agent workflow, MemAgent, which reads text in
segments and updates the memory using an overwrite strategy. We extend the DAPO
algorithm to facilitate training via independent-context multi-conversation
generation. MemAgent has demonstrated superb long-context capabilities, being
able to extrapolate from an 8K context trained on 32K text to a 3.5M QA task
with performance loss < 5% and achieves 95%+ in 512K RULER test.

</details>


### [42] [DoMIX: An Efficient Framework for Exploiting Domain Knowledge in Fine-Tuning](https://arxiv.org/abs/2507.02302)
*Dohoon Kim,Donghun Kang,Taesup Moon*

Main category: cs.CL

TL;DR: DoMIX introduces a novel approach to continual Domain-Adaptive Pre-training (DAP) using LoRA modules, addressing computational costs, domain order sensitivity, and task-specific model generalization.


<details>
  <summary>Details</summary>
Motivation: Existing continual DAP methods are computationally expensive, sensitive to data order, and lack task-specific adaptability.

Method: Leverages LoRA modules for parameter-efficient fine-tuning, enabling efficient, parallel, and order-robust domain-adaptive pre-training.

Result: DoMIX provides tailored pre-trained models for specific tasks and extends to standard LLM fine-tuning.

Conclusion: DoMIX effectively addresses limitations of continual DAP, offering a scalable and adaptable solution.

Abstract: Domain-Adaptive Pre-training (DAP) has recently gained attention for its
effectiveness in fine-tuning pre-trained models. Building on this, continual
DAP has been explored to develop pre-trained models capable of incrementally
incorporating different domain datasets. However, existing continual DAP
methods face several limitations: (1) high computational cost and GPU memory
usage during training; (2) sensitivity to incremental data order; and (3)
providing a single, generalized model for all end tasks, which contradicts the
essence of DAP. In this paper, we propose DoMIX, a novel approach that
addresses these challenges by leveraging LoRA modules, a representative
parameter-efficient fine-tuning (PEFT) method. Our approach enables efficient
and parallel domain-adaptive pre-training that is robust to domain order and
effectively utilizes accumulated knowledge to provide tailored pre-trained
models for specific tasks. We also demonstrate that our method can be extended
beyond the DAP setting to standard LLM fine-tuning scenarios. Code is available
at https://github.com/dohoonkim-ai/DoMIX.

</details>


### [43] [Coling-UniA at SciVQA 2025: Few-Shot Example Retrieval and Confidence-Informed Ensembling for Multimodal Large Language Models](https://arxiv.org/abs/2507.02357)
*Christian Jaumann,Annemarie Friedrich,Rainer Lienhart*

Main category: cs.CL

TL;DR: The paper presents a system for SciVQA 2025, using an ensemble of Multimodal Large Language Models and few-shot retrieval, achieving third place with an F1 score of 85.12.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of Scientific Visual Question Answering by leveraging multimodal models and adaptive retrieval strategies.

Method: Uses an ensemble of two Multimodal Large Language Models, few-shot example retrieval, and model selection based on figure/question type and confidence levels.

Result: Ranked third out of seven with an average F1 score of 85.12 on blind test data.

Conclusion: The system demonstrates effectiveness in SciVQA, with publicly available code for further research.

Abstract: This paper describes our system for the SciVQA 2025 Shared Task on Scientific
Visual Question Answering. Our system employs an ensemble of two Multimodal
Large Language Models and various few-shot example retrieval strategies. The
model and few-shot setting are selected based on the figure and question type.
We also select answers based on the models' confidence levels. On the blind
test data, our system ranks third out of seven with an average F1 score of
85.12 across ROUGE-1, ROUGE-L, and BERTS. Our code is publicly available.

</details>


### [44] [QFFN-BERT: An Empirical Study of Depth, Performance, and Data Efficiency in Hybrid Quantum-Classical Transformers](https://arxiv.org/abs/2507.02364)
*Pilsung Kang*

Main category: cs.CL

TL;DR: QFFN-BERT replaces classical FFN layers in BERT with parameterized quantum circuits (PQCs), achieving higher accuracy and parameter efficiency while excelling in few-shot learning.


<details>
  <summary>Details</summary>
Motivation: FFNs dominate Transformer parameter counts, making them a target for efficiency gains. PQCs offer potential for enhanced expressibility and parameter reduction.

Method: Hybrid quantum-classical transformer with PQC-based FFN layers, using residual connections, RY/RZ rotations, and alternating entanglement for stable training.

Result: QFFN-BERT achieves 102% baseline accuracy, reduces FFN parameters by 99%, and performs well in few-shot learning.

Conclusion: PQCs, when co-designed with deep learning principles, can outperform classical FFNs in accuracy and efficiency.

Abstract: Parameterized quantum circuits (PQCs) have recently emerged as promising
components for enhancing the expressibility of neural architectures. In this
work, we introduce QFFN-BERT, a hybrid quantum-classical transformer where the
feedforward network (FFN) modules of a compact BERT variant are replaced by
PQC-based layers. This design is motivated by the dominant parameter
contribution of FFNs, which account for approximately two-thirds of the
parameters within standard Transformer encoder blocks. While prior studies have
primarily integrated PQCs into self-attention modules, our work focuses on the
FFN and systematically investigates the trade-offs between PQC depth,
expressibility, and trainability. Our final PQC architecture incorporates a
residual connection, both $R_Y$ and $R_Z$ rotations, and an alternating
entanglement strategy to ensure stable training and high expressibility. Our
experiments, conducted on a classical simulator, on the SST-2 and DBpedia
benchmarks demonstrate two key findings. First, a carefully configured
QFFN-BERT achieves up to 102.0% of the baseline accuracy, surpassing its
classical counterpart in a full-data setting while reducing FFN-specific
parameters by over 99%. Second, our model exhibits a consistent and competitive
edge in few-shot learning scenarios, confirming its potential for superior data
efficiency. These results, supported by an ablation study on a non-optimized
PQC that failed to learn, confirm that PQCs can serve as powerful and
parameter-efficient alternatives to classical FFNs when co-designed with
foundational deep learning principles.

</details>


### [45] [Efficient Code LLM Training via Distribution-Consistent and Diversity-Aware Data Selection](https://arxiv.org/abs/2507.02378)
*Weijie Lyu,Sheng-Jun Huang,Xuan Xia*

Main category: cs.CL

TL;DR: The paper introduces a parametric model for selecting high-quality code data, improving training efficiency and model performance with fewer samples.


<details>
  <summary>Details</summary>
Motivation: Current methods focus on data quantity over quality, reducing training efficiency. The paper aims to address this by prioritizing high-quality data selection.

Method: A parametric model is used to select code data, ensuring distribution consistency and diversity in the subset.

Result: Using only 10K samples, the method outperforms a 92K baseline by 2.4% (HumanEval) and 2.3% (MBPP).

Conclusion: The method effectively enhances model performance while reducing computational costs.

Abstract: Recent advancements in large language models (LLMs) have significantly
improved code generation and program comprehension, accelerating the evolution
of software engineering. Current methods primarily enhance model performance by
leveraging vast amounts of data, focusing on data quantity while often
overlooking data quality, thereby reducing training efficiency. To address
this, we introduce an approach that utilizes a parametric model for code data
selection, aimed at improving both training efficiency and model performance.
Our method optimizes the parametric model to ensure distribution consistency
and diversity within the selected subset, guaranteeing high-quality data.
Experimental results demonstrate that using only 10K samples, our method
achieves gains of 2.4% (HumanEval) and 2.3% (MBPP) over 92K full-sampled
baseline, outperforming other sampling approaches in both performance and
efficiency. This underscores that our method effectively boosts model
performance while significantly reducing computational costs.

</details>


### [46] [Benchmarking Akan ASR Models Across Domain-Specific Datasets: A Comparative Evaluation of Performance, Scalability, and Adaptability](https://arxiv.org/abs/2507.02407)
*Mark Atta Mensah,Isaac Wiafe,Akon Ekpezu,Justice Kwame Appati,Jamal-Deen Abdulai,Akosua Nyarkoa Wiafe-Akenten,Frank Ernest Yeboah,Gifty Odame*

Main category: cs.CL

TL;DR: The study benchmarks seven Akan ASR models (Whisper and Wav2Vec2) across diverse speech domains, revealing domain dependency and distinct error behaviors between architectures.


<details>
  <summary>Details</summary>
Motivation: To address the lack of evaluation on generalization across diverse speech contexts in ASR research.

Method: Benchmarked seven Akan ASR models using four diverse Akan speech corpora, analyzing word and character error rates.

Result: Models performed best within their training domains but degraded in mismatched scenarios. Whisper produced fluent but misleading errors, while Wav2Vec2 had obvious but less interpretable errors.

Conclusion: Highlights the need for domain adaptation, adaptive routing, and multilingual training for low-resource languages like Akan.

Abstract: Most existing automatic speech recognition (ASR) research evaluate models
using in-domain datasets. However, they seldom evaluate how they generalize
across diverse speech contexts. This study addresses this gap by benchmarking
seven Akan ASR models built on transformer architectures, such as Whisper and
Wav2Vec2, using four Akan speech corpora to determine their performance. These
datasets encompass various domains, including culturally relevant image
descriptions, informal conversations, biblical scripture readings, and
spontaneous financial dialogues. A comparison of the word error rate and
character error rate highlighted domain dependency, with models performing
optimally only within their training domains while showing marked accuracy
degradation in mismatched scenarios. This study also identified distinct error
behaviors between the Whisper and Wav2Vec2 architectures. Whereas fine-tuned
Whisper Akan models led to more fluent but potentially misleading transcription
errors, Wav2Vec2 produced more obvious yet less interpretable outputs when
encountering unfamiliar inputs. This trade-off between readability and
transparency in ASR errors should be considered when selecting architectures
for low-resource language (LRL) applications. These findings highlight the need
for targeted domain adaptation techniques, adaptive routing strategies, and
multilingual training frameworks for Akan and other LRLs.

</details>


### [47] [A Cookbook for Community-driven Data Collection of Impaired Speech in LowResource Languages](https://arxiv.org/abs/2507.02428)
*Sumaya Ahmed Salihs,Isaac Wiafe,Jamal-Deen Abdulai,Elikem Doe Atsakpo,Gifty Ayoka,Richard Cave,Akon Obu Ekpezu,Catherine Holloway,Katrin Tomanek,Fiifi Baffoe Payin Winful*

Main category: cs.CL

TL;DR: This study introduces a method for collecting impaired speech samples in low-resource languages, focusing on Akan, and provides tools and a cookbook for community-driven ASR model development.


<details>
  <summary>Details</summary>
Motivation: To democratize ASR technology for impaired speech in low-resource languages by enabling community-driven data collection and model building.

Method: Developed a cookbook of best practices, curated an open-source dataset of impaired speech in Akan, and fine-tuned ASR models.

Result: Produced the first open-source impaired speech dataset in Akan, along with tools and initial results of fine-tuned ASR models.

Conclusion: The study successfully created resources to support inclusive ASR technologies for speech-impaired individuals in low-resource languages.

Abstract: This study presents an approach for collecting speech samples to build
Automatic Speech Recognition (ASR) models for impaired speech, particularly,
low-resource languages. It aims to democratize ASR technology and data
collection by developing a "cookbook" of best practices and training for
community-driven data collection and ASR model building. As a proof-of-concept,
this study curated the first open-source dataset of impaired speech in Akan: a
widely spoken indigenous language in Ghana. The study involved participants
from diverse backgrounds with speech impairments. The resulting dataset, along
with the cookbook and open-source tools, are publicly available to enable
researchers and practitioners to create inclusive ASR technologies tailored to
the unique needs of speech impaired individuals. In addition, this study
presents the initial results of fine-tuning open-source ASR models to better
recognize impaired speech in Akan.

</details>


### [48] [IndianBailJudgments-1200: A Multi-Attribute Dataset for Legal NLP on Indian Bail Orders](https://arxiv.org/abs/2507.02506)
*Sneha Deshmukh,Prathmesh Kamble*

Main category: cs.CL

TL;DR: IndianBailJudgments-1200 is a new dataset of 1200 Indian bail judgments annotated for legal NLP tasks, addressing the lack of structured data in Indian legal NLP.


<details>
  <summary>Details</summary>
Motivation: The scarcity of structured legal datasets in regions like India hinders legal NLP development.

Method: A prompt-engineered GPT-4o pipeline was used to annotate the dataset, followed by consistency verification.

Result: The dataset includes 20+ attributes like bail outcome, IPC sections, and legal reasoning, supporting tasks like outcome prediction and fairness analysis.

Conclusion: IndianBailJudgments-1200 is the first public dataset for Indian bail jurisprudence, enabling diverse legal NLP applications.

Abstract: Legal NLP remains underdeveloped in regions like India due to the scarcity of
structured datasets. We introduce IndianBailJudgments-1200, a new benchmark
dataset comprising 1200 Indian court judgments on bail decisions, annotated
across 20+ attributes including bail outcome, IPC sections, crime type, and
legal reasoning. Annotations were generated using a prompt-engineered GPT-4o
pipeline and verified for consistency. This resource supports a wide range of
legal NLP tasks such as outcome prediction, summarization, and fairness
analysis, and is the first publicly available dataset focused specifically on
Indian bail jurisprudence.

</details>


### [49] [WebSailor: Navigating Super-human Reasoning for Web Agent](https://arxiv.org/abs/2507.02592)
*Kuan Li,Zhongwang Zhang,Huifeng Yin,Liwen Zhang,Litu Ou,Jialong Wu,Wenbiao Yin,Baixuan Li,Zhengwei Tao,Xinyu Wang,Weizhou Shen,Junkai Zhang,Dingchu Zhang,Xixi Wu,Yong Jiang,Ming Yan,Pengjun Xie,Fei Huang,Jingren Zhou*

Main category: cs.CL

TL;DR: WebSailor introduces a post-training method to enhance LLMs' ability to handle high-uncertainty tasks, matching proprietary agents' performance.


<details>
  <summary>Details</summary>
Motivation: Addressing the gap in open-source models' ability to systematically reduce uncertainty in complex information-seeking tasks.

Method: Uses structured sampling, information obfuscation, RFT cold start, and the DUPO algorithm for training.

Result: WebSailor outperforms open-source agents and matches proprietary agents in complex tasks.

Conclusion: The approach effectively bridges the capability gap between open-source and proprietary models.

Abstract: Transcending human cognitive limitations represents a critical frontier in
LLM training. Proprietary agentic systems like DeepResearch have demonstrated
superhuman capabilities on extremely complex information-seeking benchmarks
such as BrowseComp, a feat previously unattainable. We posit that their success
hinges on a sophisticated reasoning pattern absent in open-source models: the
ability to systematically reduce extreme uncertainty when navigating vast
information landscapes. Based on this insight, we introduce WebSailor, a
complete post-training methodology designed to instill this crucial capability.
Our approach involves generating novel, high-uncertainty tasks through
structured sampling and information obfuscation, RFT cold start, and an
efficient agentic RL training algorithm, Duplicating Sampling Policy
Optimization (DUPO). With this integrated pipeline, WebSailor significantly
outperforms all opensource agents in complex information-seeking tasks,
matching proprietary agents' performance and closing the capability gap.

</details>


### [50] [Revisiting Active Learning under (Human) Label Variation](https://arxiv.org/abs/2507.02593)
*Cornelia Gruber,Helen Alber,Bernd Bischl,Göran Kauermann,Barbara Plank,Matthias Aßenmacher*

Main category: cs.CL

TL;DR: The paper addresses the challenge of label variation in supervised learning, emphasizing human label variation (HLV) as an informative signal. It critiques assumptions in active learning (AL) and proposes a framework for HLV-aware AL, including LLM integration.


<details>
  <summary>Details</summary>
Motivation: Label variation is common but often treated as noise, ignoring HLV's informative value. AL assumptions rarely account for HLV, limiting practical applicability.

Method: The paper decomposes label variation into signal (HLV) and noise, surveys AL and LV literature, and proposes a conceptual framework for HLV-aware AL.

Result: A framework for integrating HLV into AL is introduced, addressing instance selection, annotator choice, and label representation, with LLMs as potential annotators.

Conclusion: The work provides a foundation for HLV-aware AL, better reflecting real-world annotation complexities and improving supervised learning with limited labeled data.

Abstract: Access to high-quality labeled data remains a limiting factor in applied
supervised learning. While label variation (LV), i.e., differing labels for the
same instance, is common, especially in natural language processing, annotation
frameworks often still rest on the assumption of a single ground truth. This
overlooks human label variation (HLV), the occurrence of plausible differences
in annotations, as an informative signal. Similarly, active learning (AL), a
popular approach to optimizing the use of limited annotation budgets in
training ML models, often relies on at least one of several simplifying
assumptions, which rarely hold in practice when acknowledging HLV. In this
paper, we examine foundational assumptions about truth and label nature,
highlighting the need to decompose observed LV into signal (e.g., HLV) and
noise (e.g., annotation error). We survey how the AL and (H)LV communities have
addressed -- or neglected -- these distinctions and propose a conceptual
framework for incorporating HLV throughout the AL loop, including instance
selection, annotator choice, and label representation. We further discuss the
integration of large language models (LLM) as annotators. Our work aims to lay
a conceptual foundation for HLV-aware active learning, better reflecting the
complexities of real-world annotation.

</details>


### [51] [MPF: Aligning and Debiasing Language Models post Deployment via Multi Perspective Fusion](https://arxiv.org/abs/2507.02595)
*Xin Guan,PeiHsin Lin,Zekun Wu,Ze Wang,Ruibo Zhang,Emre Kazim,Adriano Koshiyama*

Main category: cs.CL

TL;DR: MPF is a posttraining alignment framework for LLMs that mitigates bias by leveraging multiperspective generations and interpretable baselines, achieving alignment with minimal KL divergence and calibration error.


<details>
  <summary>Details</summary>
Motivation: Address the need for scalable and interpretable bias mitigation in LLMs without extensive prompt engineering or finetuning.

Method: Uses the SAGED pipeline to construct bias benchmarks and decompose baselines into interpretable components, guiding generation through weighted sampling.

Result: Aligns LLM outputs with nuanced baselines (e.g., HR professionals' sentiment), reducing KL divergence and calibration error while generalizing to unseen questions.

Conclusion: MPF provides a scalable, interpretable, and effective method for bias mitigation in deployed LLMs.

Abstract: Multiperspective Fusion (MPF) is a novel posttraining alignment framework for
large language models (LLMs) developed in response to the growing need for easy
bias mitigation. Built on top of the SAGED pipeline, an automated system for
constructing bias benchmarks and extracting interpretable baseline
distributions, MPF leverages multiperspective generations to expose and align
biases in LLM outputs with nuanced, humanlike baselines. By decomposing
baseline, such as sentiment distributions from HR professionals, into
interpretable perspective components, MPF guides generation through sampling
and balancing of responses, weighted by the probabilities obtained in the
decomposition. Empirically, we demonstrate its ability to align LLM sentiment
distributions with both counterfactual baselines (absolute equality) and the HR
baseline (biased for Top Univeristy), resulting in small KL divergence,
reduction of calibration error and generalization to unseen questions. This
shows that MPF offers a scalable and interpretable method for alignment and
bias mitigation, compatible with deployed LLMs and requiring no extensive
prompt engineering or finetuning.

</details>


### [52] [Exploring Gender Bias Beyond Occupational Titles](https://arxiv.org/abs/2507.02679)
*Ahmed Sabir,Rajesh Sharama*

Main category: cs.CL

TL;DR: The paper introduces GenderLexicon, a dataset and framework to quantify and explain gender biases in contextual elements like verbs, nouns, and occupations, validated across diverse datasets.


<details>
  <summary>Details</summary>
Motivation: To investigate and quantify gender biases in contextual language elements beyond occupational stereotypes.

Method: Developed GenderLexicon dataset and a framework to score contextual and gender biases, evaluated on five diverse datasets.

Result: Confirmed existence of broader gender biases and demonstrated the framework's effectiveness in quantifying and explaining these biases.

Conclusion: The proposed framework enhances the explainability of gender biases and is validated across multiple datasets, including non-English ones.

Abstract: In this work, we investigate the correlation between gender and contextual
biases, focusing on elements such as action verbs, object nouns, and
particularly on occupations. We introduce a novel dataset, GenderLexicon, and a
framework that can estimate contextual bias and its related gender bias. Our
model can interpret the bias with a score and thus improve the explainability
of gender bias. Also, our findings confirm the existence of gender biases
beyond occupational stereotypes. To validate our approach and demonstrate its
effectiveness, we conduct evaluations on five diverse datasets, including a
Japanese dataset.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [53] [Learnable-Differentiable Finite Volume Solver for Accelerated Simulation of Flows](https://arxiv.org/abs/2507.01975)
*Mengtao Yan,Qi Wang,Haining Wang,Ruizhi Chengze,Yi Zhang,Hongsheng Liu,Zidong Wang,Fan Yu,Qi Qi,Hao Sun*

Main category: cs.LG

TL;DR: LDSolver is a learnable, differentiable finite volume solver for efficient and accurate fluid flow simulation on coarse grids, outperforming baselines with limited data.


<details>
  <summary>Details</summary>
Motivation: Classical solvers are computationally expensive, while machine learning lacks interpretability and generalizability. LDSolver bridges this gap.

Method: Combines a differentiable finite volume solver with a learnable module for flux approximation and error correction on coarse grids.

Result: Achieves state-of-the-art performance on various flow systems with high accuracy and generalizability, even with limited training data.

Conclusion: LDSolver offers an efficient, accurate, and generalizable solution for fluid flow simulation, outperforming existing methods.

Abstract: Simulation of fluid flows is crucial for modeling physical phenomena like
meteorology, aerodynamics, and biomedicine. Classical numerical solvers often
require fine spatiotemporal grids to satisfy stability, consistency, and
convergence conditions, leading to substantial computational costs. Although
machine learning has demonstrated better efficiency, they typically suffer from
issues of interpretability, generalizability, and data dependency. Hence, we
propose a learnable and differentiable finite volume solver, called LDSolver,
designed for efficient and accurate simulation of fluid flows on spatiotemporal
coarse grids. LDSolver comprises two key components: (1) a differentiable
finite volume solver, and (2) an learnable module providing equivalent
approximation for fluxes (derivatives and interpolations), and temporal error
correction on coarse grids. Even with limited training data (e.g., only a few
trajectories), our model could accelerate the simulation while maintaining a
high accuracy with superior generalizability. Experiments on different flow
systems (e.g., Burgers, decaying, forced and shear flows) show that LDSolver
achieves state-of-the-art performance, surpassing baseline models with notable
margins.

</details>


### [54] [DKGCM: A Spatio-Temporal Prediction Model for Traffic Flow by Fusing Spatial Node Clustering Method and Fourier Bidirectional Mamba Mechanism](https://arxiv.org/abs/2507.01982)
*Siqing Long,Xiangzhi Huang,Jiemin Xie,Ming Cai*

Main category: cs.LG

TL;DR: Proposes DKGCM, a graph convolutional network for traffic demand forecasting, combining spatial clustering (DK-GCN) and temporal FFT with Mamba, enhanced by GRPO reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: Improving traffic demand forecasting accuracy by addressing complex spatiotemporal relationships in traffic systems.

Method: Uses DK-GCN for spatial dependencies (DTW and K-means clustering) and FFT with Mamba for temporal dependencies, optimized by GRPO reinforcement learning.

Result: Outperforms advanced methods on three public datasets.

Conclusion: DKGCM effectively captures spatiotemporal dependencies, enhancing traffic demand prediction accuracy.

Abstract: Accurate traffic demand forecasting enables transportation management
departments to allocate resources more effectively, thereby improving their
utilization efficiency. However, complex spatiotemporal relationships in
traffic systems continue to limit the performance of demand forecasting models.
To improve the accuracy of spatiotemporal traffic demand prediction, we propose
a new graph convolutional network structure called DKGCM. Specifically, we
first consider the spatial flow distribution of different traffic nodes and
propose a novel temporal similarity-based clustering graph convolution method,
DK-GCN. This method utilizes Dynamic Time Warping (DTW) and K-means clustering
to group traffic nodes and more effectively capture spatial dependencies. On
the temporal scale, we integrate the Fast Fourier Transform (FFT) within the
bidirectional Mamba deep learning framework to capture temporal dependencies in
traffic demand. To further optimize model training, we incorporate the GRPO
reinforcement learning strategy to enhance the loss function feedback
mechanism. Extensive experiments demonstrate that our model outperforms several
advanced methods and achieves strong results on three public datasets.

</details>


### [55] [Multimodal Misinformation Detection Using Early Fusion of Linguistic, Visual, and Social Features](https://arxiv.org/abs/2507.01984)
*Gautam Kishore Shahi*

Main category: cs.LG

TL;DR: This study explores multimodal feature combinations (text, images, social features) for misinformation detection, showing improved performance over unimodal and bimodal approaches.


<details>
  <summary>Details</summary>
Motivation: Address the gap in research on multimodal misinformation detection by integrating text, images, and social features.

Method: Analyzed 1,529 tweets using early fusion, extracting features via object detection, OCR, and social data enrichment.

Result: Combining unsupervised and supervised models improved classification by 15% over unimodal and 5% over bimodal models.

Conclusion: Multimodal feature integration enhances misinformation detection, with insights into propagation patterns.

Abstract: Amid a tidal wave of misinformation flooding social media during elections
and crises, extensive research has been conducted on misinformation detection,
primarily focusing on text-based or image-based approaches. However, only a few
studies have explored multimodal feature combinations, such as integrating text
and images for building a classification model to detect misinformation. This
study investigates the effectiveness of different multimodal feature
combinations, incorporating text, images, and social features using an early
fusion approach for the classification model. This study analyzed 1,529 tweets
containing both text and images during the COVID-19 pandemic and election
periods collected from Twitter (now X). A data enrichment process was applied
to extract additional social features, as well as visual features, through
techniques such as object detection and optical character recognition (OCR).
The results show that combining unsupervised and supervised machine learning
models improves classification performance by 15% compared to unimodal models
and by 5% compared to bimodal models. Additionally, the study analyzes the
propagation patterns of misinformation based on the characteristics of
misinformation tweets and the users who disseminate them.

</details>


### [56] [Positive region preserved random sampling: an efficient feature selection method for massive data](https://arxiv.org/abs/2507.01998)
*Hexiang Bai,Deyu Li,Jiye Liang,Yanhui Zhai*

Main category: cs.LG

TL;DR: A new feature selection method for massive data using sampling and rough set theory, ensuring high discriminatory ability and efficiency.


<details>
  <summary>Details</summary>
Motivation: Intelligent machines lack computing resources for feature selection in massive data, necessitating an efficient method.

Method: Uses sampling and rough set theory, measuring discriminatory ability via discernible object pairs ratio, and constructs positive region preserved samples.

Result: Validated on 11 datasets, the method finds approximate reducts quickly with discriminatory ability exceeding estimated bounds.

Conclusion: The method efficiently selects high-discriminatory feature subsets for massive data, even on personal computers.

Abstract: Selecting relevant features is an important and necessary step for
intelligent machines to maximize their chances of success. However, intelligent
machines generally have no enough computing resources when faced with huge
volume of data. This paper develops a new method based on sampling techniques
and rough set theory to address the challenge of feature selection for massive
data. To this end, this paper proposes using the ratio of discernible object
pairs to all object pairs that should be distinguished to measure the
discriminatory ability of a feature set. Based on this measure, a new feature
selection method is proposed. This method constructs positive region preserved
samples from massive data to find a feature subset with high discriminatory
ability. Compared with other methods, the proposed method has two advantages.
First, it is able to select a feature subset that can preserve the
discriminatory ability of all the features of the target massive data set
within an acceptable time on a personal computer. Second, the lower boundary of
the probability of the object pairs that can be discerned using the feature
subset selected in all object pairs that should be distinguished can be
estimated before finding reducts. Furthermore, 11 data sets of different sizes
were used to validate the proposed method. The results show that approximate
reducts can be found in a very short period of time, and the discriminatory
ability of the final reduct is larger than the estimated lower boundary.
Experiments on four large-scale data sets also showed that an approximate
reduct with high discriminatory ability can be obtained in reasonable time on a
personal computer.

</details>


### [57] [Continuous Wavelet Transform and Siamese Network-Based Anomaly Detection in Multi-variate Semiconductor Process Time Series](https://arxiv.org/abs/2507.01999)
*Bappaditya Dey,Daniel Sorensen,Minjin Hwang,Sandip Halder*

Main category: cs.LG

TL;DR: A novel machine learning approach for anomaly detection in semiconductor manufacturing using image-based MTS data and Siamese networks.


<details>
  <summary>Details</summary>
Motivation: Address challenges like high data dimensionality, class imbalance, noise, and non-stationary behavior in semiconductor fabrication anomaly detection.

Method: Convert MTS data to images via Continuous Wavelet Transform, fine-tune VGG-16 for classification, and use a Siamese network for comparison.

Result: High accuracy in anomaly detection on real FAB process data, applicable in supervised and semi-supervised settings.

Conclusion: The method is effective and flexible for offline anomaly detection in semiconductor manufacturing.

Abstract: Semiconductor manufacturing is an extremely complex process, characterized by
thousands of interdependent parameters collected across diverse tools and
process steps. Multi-variate time-series (MTS) analysis has emerged as a
critical methodology for enabling real-time monitoring, fault detection, and
predictive maintenance in such environments. However, anomaly prediction in
semiconductor fabrication presents several critical challenges, including high
data dimensionality, severe class imbalance due to the rarity of true faults,
noisy and missing measurements, and non-stationary behavior of production
systems. Furthermore, the complex interdependencies between variables and the
delayed emergence of faults across downstream stages complicate both anomaly
detection and root-cause-analysis. This paper presents a novel and generic
approach for anomaly detection in MTS data using machine learning. The proposed
methodology consists of three main steps: a) converting MTS data into
image-based representations using the Continuous Wavelet Transform, b)
developing a multi-class image classifier by fine-tuning a pretrained VGG-16
architecture on custom CWT image datasets, and c) constructing a Siamese
network composed of two identical sub-networks, each utilizing the fine-tuned
VGG-16 as a backbone. The network takes pairs of CWT images as input -one
serving as a reference or anchor (representing a known-good signal), and the
other as a query (representing an unknown signal). The model then compares the
embeddings of both inputs to determine whether they belong to the same class at
a given time step. Our approach demonstrates high accuracy in identifying
anomalies on a real FAB process time-series dataset, offering a promising
solution for offline anomaly detection in process and tool trace data.
Moreover, the approach is flexible and can be applied in both supervised and
semi-supervised settings.

</details>


### [58] [Temporal Chain of Thought: Long-Video Understanding by Thinking in Frames](https://arxiv.org/abs/2507.02001)
*Anurag Arnab,Ahmet Iscen,Mathilde Caron,Alireza Fathi,Cordelia Schmid*

Main category: cs.LG

TL;DR: Temporal Chain of Thought improves video question-answering by iteratively selecting relevant frames, outperforming standard methods on long videos.


<details>
  <summary>Details</summary>
Motivation: Long-video understanding is challenging for VLMs due to irrelevant distractors in large context windows.

Method: Uses the VLM to iteratively identify and extract the most relevant frames for answering questions.

Result: Achieves state-of-the-art results on 4 datasets, with significant improvements on long videos (e.g., 2.8 points better on LVBench).

Conclusion: Leveraging computation at inference-time for context selection enhances VLM performance, especially for long videos.

Abstract: Despite recent advances in Vision-Language Models (VLMs), long-video
understanding remains a challenging problem. Although state-of-the-art
long-context VLMs can process around 1000 input frames, they still struggle to
effectively leverage this sequence length, and succumb to irrelevant
distractors within the context window. We present Temporal Chain of Thought, an
inference strategy for video question-answering that curates the model's input
context. We use the VLM itself to iteratively identify and extract the most
relevant frames from the video, which are then used for answering. We
demonstrate how leveraging more computation at inference-time to select the
most relevant context leads to improvements in accuracy, in agreement with
recent work on inference-time scaling of LLMs. Moreover, we achieve
state-of-the-art results on 4 diverse video question-answering datasets,
showing consistent improvements with 3 different VLMs. In particular, our
method shines on longer videos which would not otherwise fit within the model's
context window: On longer videos of more than 1 hour on LVBench, our approach
using a context window of 32K outperforms the same VLM using standard inference
with a 700K context window by 2.8 points.

</details>


### [59] [AIRES: Accelerating Out-of-Core GCNs via Algorithm-System Co-Design](https://arxiv.org/abs/2507.02006)
*Shakya Jayakody,Youpeng Zhao,Jun Wang*

Main category: cs.LG

TL;DR: AIRES is a novel algorithm-system co-design solution that accelerates out-of-core SpGEMM computation for GCNs by addressing data alignment and memory bottlenecks, achieving up to 1.8x lower latency.


<details>
  <summary>Details</summary>
Motivation: Current systems for SpGEMM in GCNs suffer from high I/O latency and GPU under-utilization due to sparse format data alignment and memory allocation issues.

Method: AIRES proposes block-level data alignment and a tiling algorithm for sparse matrices, along with a three-phase dynamic scheduling system using GPU memory, GDS, and host memory.

Result: AIRES outperforms state-of-the-art methods, reducing latency by up to 1.8x in real-world benchmarks.

Conclusion: AIRES effectively addresses performance bottlenecks in out-of-core SpGEMM for GCNs, offering significant improvements in latency and throughput.

Abstract: Graph convolutional networks (GCNs) are fundamental in various scientific
applications, ranging from biomedical protein-protein interactions (PPI) to
large-scale recommendation systems. An essential component for modeling graph
structures in GCNs is sparse general matrix-matrix multiplication (SpGEMM). As
the size of graph data continues to scale up, SpGEMMs are often conducted in an
out-of-core fashion due to limited GPU memory space in resource-constrained
systems. Albeit recent efforts that aim to alleviate the memory constraints of
out-of-core SpGEMM through either GPU feature caching, hybrid CPU-GPU memory
layout, or performing the computation in sparse format, current systems suffer
from both high I/O latency and GPU under-utilization issues.
  In this paper, we first identify the problems of existing systems, where
sparse format data alignment and memory allocation are the main performance
bottlenecks, and propose AIRES, a novel algorithm-system co-design solution to
accelerate out-of-core SpGEMM computation for GCNs. Specifically, from the
algorithm angle, AIRES proposes to alleviate the data alignment issues on the
block level for matrices in sparse formats and develops a tiling algorithm to
facilitate row block-wise alignment. On the system level, AIRES employs a
three-phase dynamic scheduling that features a dual-way data transfer strategy
utilizing a tiered memory system: integrating GPU memory, GPU Direct Storage
(GDS), and host memory to reduce I/O latency and improve throughput.
Evaluations show that AIRES significantly outperforms the state-of-the-art
methods, achieving up to 1.8x lower latency in real-world graph processing
benchmarks.

</details>


### [60] [GeoAda: Efficiently Finetune Geometric Diffusion Models with Equivariant Adapters](https://arxiv.org/abs/2507.02085)
*Wanjia Zhao,Jiaqi Han,Siyi Gu,Mingjian Jiang,James Zou,Stefano Ermon*

Main category: cs.LG

TL;DR: GeoAda is an SE(3)-equivariant adapter framework for fine-tuning geometric diffusion models efficiently without altering the original architecture, preserving geometric consistency and avoiding overfitting.


<details>
  <summary>Details</summary>
Motivation: Efficiently fine-tuning geometric diffusion models for downstream tasks with varying geometric controls is underexplored.

Method: GeoAda uses a structured adapter design with control signal encoding, trainable copies of pretrained layers, and equivariant zero-initialized convolution.

Result: GeoAda achieves state-of-the-art fine-tuning performance, maintaining geometric consistency and original task accuracy.

Conclusion: GeoAda is widely applicable across diverse geometric control types and domains, outperforming baselines by mitigating overfitting and catastrophic forgetting.

Abstract: Geometric diffusion models have shown remarkable success in molecular
dynamics and structure generation. However, efficiently fine-tuning them for
downstream tasks with varying geometric controls remains underexplored. In this
work, we propose an SE(3)-equivariant adapter framework ( GeoAda) that enables
flexible and parameter-efficient fine-tuning for controlled generative tasks
without modifying the original model architecture. GeoAda introduces a
structured adapter design: control signals are first encoded through coupling
operators, then processed by a trainable copy of selected pretrained model
layers, and finally projected back via decoupling operators followed by an
equivariant zero-initialized convolution. By fine-tuning only these lightweight
adapter modules, GeoAda preserves the model's geometric consistency while
mitigating overfitting and catastrophic forgetting. We theoretically prove that
the proposed adapters maintain SE(3)-equivariance, ensuring that the geometric
inductive biases of the pretrained diffusion model remain intact during
adaptation. We demonstrate the wide applicability of GeoAda across diverse
geometric control types, including frame control, global control, subgraph
control, and a broad range of application domains such as particle dynamics,
molecular dynamics, human motion prediction, and molecule generation. Empirical
results show that GeoAda achieves state-of-the-art fine-tuning performance
while preserving original task accuracy, whereas other baselines experience
significant performance degradation due to overfitting and catastrophic
forgetting.

</details>


### [61] [Evaluating the Promise and Pitfalls of LLMs in Hiring Decisions](https://arxiv.org/abs/2507.02087)
*Eitan Anzenberg,Arunava Samajpati,Sivasankaran Chandrasekar,Varun Kacholia*

Main category: cs.LG

TL;DR: The paper benchmarks general-purpose LLMs against a proprietary hiring model (Match Score), showing superior accuracy and fairness in job candidate matching, and emphasizes the need for domain-specific models and bias auditing in high-stakes hiring tasks.


<details>
  <summary>Details</summary>
Motivation: To address concerns about accuracy and algorithmic bias in using LLMs for hiring, and to demonstrate the advantages of domain-specific models over general-purpose LLMs.

Method: Benchmarked state-of-the-art LLMs against Match Score using predictive accuracy metrics (ROC AUC, Precision-Recall AUC, F1-score) and fairness metrics (impact ratio across gender, race, and intersectional subgroups) on 10,000 real-world candidate-job pairs.

Result: Match Score outperformed general-purpose LLMs in accuracy (ROC AUC 0.85 vs 0.77) and fairness (minimum race-wise impact ratio of 0.957 vs 0.809 or lower for LLMs).

Conclusion: Domain-specific models like Match Score can achieve both accuracy and fairness in hiring, while general-purpose LLMs may propagate biases without safeguards. The study advocates for bias auditing and cautions against using off-the-shelf LLMs in high-stakes hiring without fairness measures.

Abstract: The use of large language models (LLMs) in hiring promises to streamline
candidate screening, but it also raises serious concerns regarding accuracy and
algorithmic bias where sufficient safeguards are not in place. In this work, we
benchmark several state-of-the-art foundational LLMs - including models from
OpenAI, Anthropic, Google, Meta, and Deepseek, and compare them with our
proprietary domain-specific hiring model (Match Score) for job candidate
matching. We evaluate each model's predictive accuracy (ROC AUC,
Precision-Recall AUC, F1-score) and fairness (impact ratio of cut-off analysis
across declared gender, race, and intersectional subgroups). Our experiments on
a dataset of roughly 10,000 real-world recent candidate-job pairs show that
Match Score outperforms the general-purpose LLMs on accuracy (ROC AUC 0.85 vs
0.77) and achieves significantly more equitable outcomes across demographic
groups. Notably, Match Score attains a minimum race-wise impact ratio of 0.957
(near-parity), versus 0.809 or lower for the best LLMs, (0.906 vs 0.773 for the
intersectionals, respectively). We discuss why pretraining biases may cause
LLMs with insufficient safeguards to propagate societal biases in hiring
scenarios, whereas a bespoke supervised model can more effectively mitigate
these biases. Our findings highlight the importance of domain-specific modeling
and bias auditing when deploying AI in high-stakes domains such as hiring, and
caution against relying on off-the-shelf LLMs for such tasks without extensive
fairness safeguards. Furthermore, we show with empirical evidence that there
shouldn't be a dichotomy between choosing accuracy and fairness in hiring: a
well-designed algorithm can achieve both accuracy in hiring and fairness in
outcomes.

</details>


### [62] [Sample Complexity Bounds for Linear Constrained MDPs with a Generative Model](https://arxiv.org/abs/2507.02089)
*Xingtu Liu,Lin F. Yang,Sharan Vaswani*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We consider infinite-horizon $\gamma$-discounted (linear) constrained Markov
decision processes (CMDPs) where the objective is to find a policy that
maximizes the expected cumulative reward subject to expected cumulative
constraints. Given access to a generative model, we propose to solve CMDPs with
a primal-dual framework that can leverage any black-box unconstrained MDP
solver. For linear CMDPs with feature dimension $d$, we instantiate the
framework by using mirror descent value iteration
(\texttt{MDVI})~\citep{kitamura2023regularization} an example MDP solver. We
provide sample complexity bounds for the resulting CMDP algorithm in two cases:
(i) relaxed feasibility, where small constraint violations are allowed, and
(ii) strict feasibility, where the output policy is required to exactly satisfy
the constraint. For (i), we prove that the algorithm can return an
$\epsilon$-optimal policy with high probability by using
$\tilde{O}\left(\frac{d^2}{(1-\gamma)^4\epsilon^2}\right)$ samples. We note
that these results exhibit a near-optimal dependence on both $d$ and
$\epsilon$. For (ii), we show that the algorithm requires
$\tilde{O}\left(\frac{d^2}{(1-\gamma)^6\epsilon^2\zeta^2}\right)$ samples,
where $\zeta$ is the problem-dependent Slater constant that characterizes the
size of the feasible region. Finally, we instantiate our framework for tabular
CMDPs and show that it can be used to recover near-optimal sample complexities
in this setting.

</details>


### [63] [Energy-Based Transformers are Scalable Learners and Thinkers](https://arxiv.org/abs/2507.02092)
*Alexi Gladstone,Ganesh Nanduru,Md Mofijul Islam,Peixuan Han,Hyeonjeong Ha,Aman Chadha,Yilun Du,Heng Ji,Jundong Li,Tariq Iqbal*

Main category: cs.LG

TL;DR: EBTs generalize System 2 Thinking via unsupervised learning, outperforming existing models in scaling and performance.


<details>
  <summary>Details</summary>
Motivation: To generalize System 2 Thinking approaches without modality or problem-specific constraints, and enable learning solely from unsupervised learning.

Method: Train Energy-Based Transformers (EBTs) to verify input-prediction compatibility, using gradient descent for energy minimization.

Result: EBTs scale faster (35% higher rate) and outperform Transformer++ (29% better) and Diffusion Transformers in tasks like image denoising.

Conclusion: EBTs are a promising paradigm for scaling model learning and thinking capabilities, showing better generalization.

Abstract: Inference-time computation techniques, analogous to human System 2 Thinking,
have recently become popular for improving model performances. However, most
existing approaches suffer from several limitations: they are modality-specific
(e.g., working only in text), problem-specific (e.g., verifiable domains like
math and coding), or require additional supervision/training on top of
unsupervised pretraining (e.g., verifiers or verifiable rewards). In this
paper, we ask the question "Is it possible to generalize these System 2
Thinking approaches, and develop models that learn to think solely from
unsupervised learning?" Interestingly, we find the answer is yes, by learning
to explicitly verify the compatibility between inputs and
candidate-predictions, and then re-framing prediction problems as optimization
with respect to this verifier. Specifically, we train Energy-Based Transformers
(EBTs) -- a new class of Energy-Based Models (EBMs) -- to assign an energy
value to every input and candidate-prediction pair, enabling predictions
through gradient descent-based energy minimization until convergence. Across
both discrete (text) and continuous (visual) modalities, we find EBTs scale
faster than the dominant Transformer++ approach during training, achieving an
up to 35% higher scaling rate with respect to data, batch size, parameters,
FLOPs, and depth. During inference, EBTs improve performance with System 2
Thinking by 29% more than the Transformer++ on language tasks, and EBTs
outperform Diffusion Transformers on image denoising while using fewer forward
passes. Further, we find that EBTs achieve better results than existing models
on most downstream tasks given the same or worse pretraining performance,
suggesting that EBTs generalize better than existing approaches. Consequently,
EBTs are a promising new paradigm for scaling both the learning and thinking
capabilities of models.

</details>


### [64] [Parametric Neural Amp Modeling with Active Learning](https://arxiv.org/abs/2507.02109)
*Florian Grötschla,Luca A. Lanzendörfer,Longxiang Jiao,Roger Wattenhofer*

Main category: cs.LG

TL;DR: PANAMA is an active learning framework for training parametric guitar amp models using WaveNet-like architecture, optimizing data sampling for minimal knob settings.


<details>
  <summary>Details</summary>
Motivation: To efficiently train virtual guitar amp models with minimal data by leveraging active learning.

Method: Uses gradient-based optimization to determine optimal datapoints (amp knob settings) for sampling.

Result: Demonstrates effectiveness under constrained sample counts.

Conclusion: PANAMA enables efficient virtual amp modeling with fewer samples through active learning.

Abstract: We introduce PANAMA, an active learning framework for the training of
end-to-end parametric guitar amp models using a WaveNet-like architecture. With
\model, one can create a virtual amp by recording samples that are determined
by an active learning strategy to use a minimum amount of datapoints (i.e., amp
knob settings). We show that gradient-based optimization algorithms can be used
to determine the optimal datapoints to sample, and that the approach helps
under a constrained number of samples.

</details>


### [65] [Scaling Collapse Reveals Universal Dynamics in Compute-Optimally Trained Neural Networks](https://arxiv.org/abs/2507.02119)
*Shikai Qiu,Lechao Xiao,Andrew Gordon Wilson,Jeffrey Pennington,Atish Agarwala*

Main category: cs.LG

TL;DR: Compute-optimally trained neural networks exhibit universal loss curve collapse when normalized, termed 'supercollapse,' indicating good scaling.


<details>
  <summary>Details</summary>
Motivation: To understand the scaling limits of neural network training dynamics as model size and training time grow together.

Method: Analyze loss curves of models with varying sizes, normalized by training compute and loss, and study the effects of learning rate decay, hyperparameters, and SGD noise dynamics.

Result: Supercollapse occurs across architectures, datasets, and schedules, breaking down with suboptimal hyperparameters. A simple SGD noise model explains the phenomenon.

Conclusion: Supercollapse provides a precise indicator of optimal scaling, linked to power-law neural scaling laws and SGD dynamics.

Abstract: What scaling limits govern neural network training dynamics when model size
and training time grow in tandem? We show that despite the complex interactions
between architecture, training algorithms, and data, compute-optimally trained
models exhibit a remarkably precise universality. Specifically, loss curves
from models of varying sizes collapse onto a single universal curve when
training compute and loss are normalized to unity at the end of training. With
learning rate decay, the collapse becomes so tight that differences in the
normalized curves across models fall below the noise floor of individual loss
curves across random seeds, a phenomenon we term supercollapse. We observe
supercollapse across learning rate schedules, datasets, and architectures,
including transformers trained on next-token prediction, and find it breaks
down when hyperparameters are scaled suboptimally, providing a precise and
practical indicator of good scaling. We explain these phenomena by connecting
collapse to the power-law structure in typical neural scaling laws, and
analyzing a simple yet surprisingly effective model of SGD noise dynamics that
accurately predicts loss curves across various learning rate schedules and
quantitatively explains the origin of supercollapse.

</details>


### [66] [CROP: Circuit Retrieval and Optimization with Parameter Guidance using LLMs](https://arxiv.org/abs/2507.02128)
*Jingyu Pan,Isaac Jacobson,Zheng Zhao,Tung-Chieh Chen,Guanglei Zhou,Chen-Chia Chang,Vineet Rashingkar,Yiran Chen*

Main category: cs.LG

TL;DR: CROP is an LLM-powered framework for automatic VLSI design tuning, improving efficiency and results over manual methods.


<details>
  <summary>Details</summary>
Motivation: Manual parameter selection in VLSI design is laborious and limited by expertise, necessitating automation.

Method: CROP uses dense vector representations of RTL code, embedding-based retrieval, and RAG-enhanced LLM-guided parameter search.

Result: CROP achieves better QoR with fewer iterations, e.g., 9.9% power reduction.

Conclusion: CROP demonstrates the potential of LLMs in automating and optimizing VLSI design.

Abstract: Modern very large-scale integration (VLSI) design requires the implementation
of integrated circuits using electronic design automation (EDA) tools. Due to
the complexity of EDA algorithms, the vast parameter space poses a huge
challenge to chip design optimization, as the combination of even moderate
numbers of parameters creates an enormous solution space to explore. Manual
parameter selection remains industrial practice despite being excessively
laborious and limited by expert experience. To address this issue, we present
CROP, the first large language model (LLM)-powered automatic VLSI design flow
tuning framework. Our approach includes: (1) a scalable methodology for
transforming RTL source code into dense vector representations, (2) an
embedding-based retrieval system for matching designs with semantically similar
circuits, and (3) a retrieval-augmented generation (RAG)-enhanced LLM-guided
parameter search system that constrains the search process with prior knowledge
from similar designs. Experiment results demonstrate CROP's ability to achieve
superior quality-of-results (QoR) with fewer iterations than existing
approaches on industrial designs, including a 9.9% reduction in power
consumption.

</details>


### [67] [Generative Latent Diffusion for Efficient Spatiotemporal Data Reduction](https://arxiv.org/abs/2507.02129)
*Xiao Li,Liangji Zhu,Anand Rangarajan,Sanjay Ranka*

Main category: cs.LG

TL;DR: A latent diffusion framework combining a variational autoencoder with a conditional diffusion model achieves high compression ratios and accurate reconstruction by compressing only keyframes.


<details>
  <summary>Details</summary>
Motivation: Generative models lack controllability and reconstruction accuracy for practical data compression.

Method: Proposes a latent diffusion framework using a variational autoencoder and conditional diffusion model to compress keyframes and reconstruct others via generative interpolation.

Result: Achieves up to 10x higher compression ratios than SZ3 and 63% better performance than learning-based methods under the same error.

Conclusion: The method enables efficient spatiotemporal reconstruction with reduced storage costs.

Abstract: Generative models have demonstrated strong performance in conditional
settings and can be viewed as a form of data compression, where the condition
serves as a compact representation. However, their limited controllability and
reconstruction accuracy restrict their practical application to data
compression. In this work, we propose an efficient latent diffusion framework
that bridges this gap by combining a variational autoencoder with a conditional
diffusion model. Our method compresses only a small number of keyframes into
latent space and uses them as conditioning inputs to reconstruct the remaining
frames via generative interpolation, eliminating the need to store latent
representations for every frame. This approach enables accurate spatiotemporal
reconstruction while significantly reducing storage costs. Experimental results
across multiple datasets show that our method achieves up to 10 times higher
compression ratios than rule-based state-of-the-art compressors such as SZ3,
and up to 63 percent better performance than leading learning-based methods
under the same reconstruction error.

</details>


### [68] [Non-exchangeable Conformal Prediction for Temporal Graph Neural Networks](https://arxiv.org/abs/2507.02151)
*Tuo Wang,Jian Kang,Yujun Yan,Adithya Kulkarni,Dawei Zhou*

Main category: cs.LG

TL;DR: NCPNET introduces a conformal prediction framework for temporal graphs, addressing limitations of static methods by incorporating temporal dependencies and improving efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing conformal prediction methods for GNNs focus on static graphs, ignoring the dynamic nature of real-world graphs, which violates exchangeability assumptions.

Method: NCPNET uses a diffusion-based non-conformity score to capture topological and temporal uncertainties and includes an efficiency-aware optimization algorithm.

Result: Experiments on real-world datasets (WIKI, REDDIT, DBLP, IBM) show NCPNET ensures coverage and reduces prediction set size by up to 31%.

Conclusion: NCPNET effectively extends conformal prediction to dynamic graphs, improving reliability and efficiency in high-stakes applications.

Abstract: Conformal prediction for graph neural networks (GNNs) offers a promising
framework for quantifying uncertainty, enhancing GNN reliability in high-stakes
applications. However, existing methods predominantly focus on static graphs,
neglecting the evolving nature of real-world graphs. Temporal dependencies in
graph structure, node attributes, and ground truth labels violate the
fundamental exchangeability assumption of standard conformal prediction
methods, limiting their applicability. To address these challenges, in this
paper, we introduce NCPNET, a novel end-to-end conformal prediction framework
tailored for temporal graphs. Our approach extends conformal prediction to
dynamic settings, mitigating statistical coverage violations induced by
temporal dependencies. To achieve this, we propose a diffusion-based
non-conformity score that captures both topological and temporal uncertainties
within evolving networks. Additionally, we develop an efficiency-aware
optimization algorithm that improves the conformal prediction process,
enhancing computational efficiency and reducing coverage violations. Extensive
experiments on diverse real-world temporal graphs, including WIKI, REDDIT,
DBLP, and IBM Anti-Money Laundering dataset, demonstrate NCPNET's capability to
ensure guaranteed coverage in temporal graphs, achieving up to a 31% reduction
in prediction set size on the WIKI dataset, significantly improving efficiency
compared to state-of-the-art methods. Our data and code are available at
https://github.com/ODYSSEYWT/NCPNET.

</details>
