<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 98]
- [cs.AI](#cs.AI) [Total: 28]
- [cs.CL](#cs.CL) [Total: 45]
- [cs.LG](#cs.LG) [Total: 50]
- [cs.GT](#cs.GT) [Total: 1]
- [cs.IR](#cs.IR) [Total: 25]
- [stat.ME](#stat.ME) [Total: 1]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [cs.SC](#cs.SC) [Total: 1]
- [astro-ph.CO](#astro-ph.CO) [Total: 1]
- [stat.ML](#stat.ML) [Total: 6]
- [cs.MA](#cs.MA) [Total: 2]
- [math.NA](#math.NA) [Total: 1]
- [physics.comp-ph](#physics.comp-ph) [Total: 2]
- [eess.IV](#eess.IV) [Total: 5]
- [cs.CY](#cs.CY) [Total: 3]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.LO](#cs.LO) [Total: 2]
- [cs.HC](#cs.HC) [Total: 7]
- [cs.SE](#cs.SE) [Total: 6]
- [cs.RO](#cs.RO) [Total: 4]
- [math.OC](#math.OC) [Total: 2]
- [cs.DC](#cs.DC) [Total: 2]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [cs.SD](#cs.SD) [Total: 6]
- [cs.CR](#cs.CR) [Total: 16]
- [cs.NE](#cs.NE) [Total: 1]
- [quant-ph](#quant-ph) [Total: 1]
- [physics.soc-ph](#physics.soc-ph) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.DM](#cs.DM) [Total: 1]
- [cs.DL](#cs.DL) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Boosting Adversarial Transferability via Residual Perturbation Attack](https://arxiv.org/abs/2508.05689)
*Jinjia Peng,Zeze Tao,Huibing Wang,Meng Wang,Yang Wang*

Main category: cs.CV

TL;DR: ResPA improves adversarial example transferability by using residual gradients for perturbation direction, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks are vulnerable to adversarial examples, and transfer-based attacks often suffer from limited transferability due to overlooked perturbation directions.

Method: ResPA uses residual gradients (difference between current and historical gradients) to guide adversarial examples toward flat loss regions, enhancing transferability.

Result: ResPA shows superior transferability compared to existing methods and further improves when combined with input transformation techniques.

Conclusion: ResPA effectively addresses the limitation of prior methods by leveraging residual gradients, offering a robust solution for transfer-based attacks.

Abstract: Deep neural networks are susceptible to adversarial examples while suffering
from incorrect predictions via imperceptible perturbations. Transfer-based
attacks create adversarial examples for surrogate models and transfer these
examples to target models under black-box scenarios. Recent studies reveal that
adversarial examples in flat loss landscapes exhibit superior transferability
to alleviate overfitting on surrogate models. However, the prior arts overlook
the influence of perturbation directions, resulting in limited transferability.
In this paper, we propose a novel attack method, named Residual Perturbation
Attack (ResPA), relying on the residual gradient as the perturbation direction
to guide the adversarial examples toward the flat regions of the loss function.
Specifically, ResPA conducts an exponential moving average on the input
gradients to obtain the first moment as the reference gradient, which
encompasses the direction of historical gradients. Instead of heavily relying
on the local flatness that stems from the current gradients as the perturbation
direction, ResPA further considers the residual between the current gradient
and the reference gradient to capture the changes in the global perturbation
direction. The experimental results demonstrate the better transferability of
ResPA than the existing typical transfer-based attack methods, while the
transferability can be further improved by combining ResPA with the current
input transformation methods. The code is available at
https://github.com/ZezeTao/ResPA.

</details>


### [2] [Generalized Few-Shot Out-of-Distribution Detection](https://arxiv.org/abs/2508.05732)
*Pinxuan Li,Bing Cao,Changqing Zhang,Qinghua Hu*

Main category: cs.CV

TL;DR: The paper introduces a Generalized Few-shot OOD Detection (GOOD) framework to improve generalization in OOD detection by leveraging a General Knowledge Model (GKM) and a Knowledge Dynamic Embedding (KDE) mechanism.


<details>
  <summary>Details</summary>
Motivation: Existing Few-shot OOD detection methods lack generalization capability, often overfitting to limited training data and performing inconsistently across scenarios.

Method: The GOOD framework uses a GKM to provide general knowledge instead of direct few-shot learning, along with a KDE mechanism to dynamically align output distributions based on Generalized Belief (G-Belief).

Result: The framework theoretically reduces generalization error and shows superior performance on real-world OOD benchmarks.

Conclusion: The GOOD framework effectively balances generality and specificity in OOD detection, enhancing generalization and consistency.

Abstract: Few-shot Out-of-Distribution (OOD) detection has emerged as a critical
research direction in machine learning for practical deployment. Most existing
Few-shot OOD detection methods suffer from insufficient generalization
capability for the open world. Due to the few-shot learning paradigm, the OOD
detection ability is often overfit to the limited training data itself, thus
degrading the performance on generalized data and performing inconsistently
across different scenarios. To address this challenge, we proposed a
Generalized Few-shot OOD Detection (GOOD) framework, which empowers the general
knowledge of the OOD detection model with an auxiliary General Knowledge Model
(GKM), instead of directly learning from few-shot data. We proceed to reveal
the few-shot OOD detection from a generalization perspective and theoretically
derive the Generality-Specificity balance (GS-balance) for OOD detection, which
provably reduces the upper bound of generalization error with a general
knowledge model. Accordingly, we propose a Knowledge Dynamic Embedding (KDE)
mechanism to adaptively modulate the guidance of general knowledge. KDE
dynamically aligns the output distributions of the OOD detection model to the
general knowledge model based on the Generalized Belief (G-Belief) of GKM,
thereby boosting the GS-balance. Experiments on real-world OOD benchmarks
demonstrate our superiority. Codes will be available.

</details>


### [3] [UnGuide: Learning to Forget with LoRA-Guided Diffusion Models](https://arxiv.org/abs/2508.05755)
*Agnieszka Polowczyk,Alicja Polowczyk,Dawid Malarz,Artur Kasymov,Marcin Mazur,Jacek Tabor,Przemys≈Çaw Spurek*

Main category: cs.CV

TL;DR: UnGuide introduces a dynamic inference mechanism (UnGuidance) for precise control in machine unlearning, outperforming LoRA-based methods by preserving content fidelity.


<details>
  <summary>Details</summary>
Motivation: Address concerns about misuse of text-to-image diffusion models by enabling effective removal of harmful or misleading content without compromising overall performance.

Method: Uses UnGuidance, a dynamic inference mechanism leveraging Classifier-Free Guidance (CFG), to modulate the guidance scale based on denoising stability, allowing selective unlearning via LoRA.

Result: Achieves controlled concept removal while retaining model performance, outperforming existing LoRA-based methods in erasure tasks.

Conclusion: UnGuide effectively balances unlearning and content fidelity, offering a robust solution for targeted knowledge removal in diffusion models.

Abstract: Recent advances in large-scale text-to-image diffusion models have heightened
concerns about their potential misuse, especially in generating harmful or
misleading content. This underscores the urgent need for effective machine
unlearning, i.e., removing specific knowledge or concepts from pretrained
models without compromising overall performance. One possible approach is
Low-Rank Adaptation (LoRA), which offers an efficient means to fine-tune models
for targeted unlearning. However, LoRA often inadvertently alters unrelated
content, leading to diminished image fidelity and realism. To address this
limitation, we introduce UnGuide -- a novel approach which incorporates
UnGuidance, a dynamic inference mechanism that leverages Classifier-Free
Guidance (CFG) to exert precise control over the unlearning process. UnGuide
modulates the guidance scale based on the stability of a few first steps of
denoising processes, enabling selective unlearning by LoRA adapter. For prompts
containing the erased concept, the LoRA module predominates and is
counterbalanced by the base model; for unrelated prompts, the base model
governs generation, preserving content fidelity. Empirical results demonstrate
that UnGuide achieves controlled concept removal and retains the expressive
power of diffusion models, outperforming existing LoRA-based methods in both
object erasure and explicit content removal tasks.

</details>


### [4] [Improving Masked Style Transfer using Blended Partial Convolution](https://arxiv.org/abs/2508.05769)
*Seyed Hadi Seyed,Ayberk Cansever,David Hart*

Main category: cs.CV

TL;DR: A partial-convolution-based style transfer network improves localized artistic style transfer by accurately applying styles to specific regions and blending imperfections.


<details>
  <summary>Details</summary>
Motivation: Existing methods apply style transfer to entire images, but users often need localized stylization. Masking post-stylization fails to properly capture style features in the region of interest.

Method: Proposes a partial-convolution-based network for precise region-specific style transfer and introduces network-internal blending to handle imperfect region selections.

Result: Demonstrates visual and quantitative improvements in stylization using the SA-1B dataset.

Conclusion: The proposed method outperforms standard masking techniques by better capturing style features in targeted regions and handling selection imperfections.

Abstract: Artistic style transfer has long been possible with the advancements of
convolution- and transformer-based neural networks. Most algorithms apply the
artistic style transfer to the whole image, but individual users may only need
to apply a style transfer to a specific region in the image. The standard
practice is to simply mask the image after the stylization. This work shows
that this approach tends to improperly capture the style features in the region
of interest. We propose a partial-convolution-based style transfer network that
accurately applies the style features exclusively to the region of interest.
Additionally, we present network-internal blending techniques that account for
imperfections in the region selection. We show that this visually and
quantitatively improves stylization using examples from the SA-1B dataset. Code
is publicly available at https://github.com/davidmhart/StyleTransferMasked.

</details>


### [5] [MAISI-v2: Accelerated 3D High-Resolution Medical Image Synthesis with Rectified Flow and Region-specific Contrastive Loss](https://arxiv.org/abs/2508.05772)
*Can Zhao,Pengfei Guo,Dong Yang,Yucheng Tang,Yufan He,Benjamin Simon,Mason Belue,Stephanie Harmon,Baris Turkbey,Daguang Xu*

Main category: cs.CV

TL;DR: MAISI-v2 is an accelerated 3D medical image synthesis framework using rectified flow for fast, high-quality generation and a novel contrastive loss for better condition fidelity.


<details>
  <summary>Details</summary>
Motivation: Existing methods for medical image synthesis face issues like limited generalizability, slow inference, and weak alignment with input conditions. MAISI-v2 aims to address these.

Method: Integrates rectified flow for speed and introduces a region-specific contrastive loss to improve condition fidelity.

Result: Achieves state-of-the-art image quality with 33x acceleration and demonstrates utility in downstream tasks like segmentation.

Conclusion: MAISI-v2 offers a fast, high-quality solution for medical image synthesis, with potential for clinical and research applications.

Abstract: Medical image synthesis is an important topic for both clinical and research
applications. Recently, diffusion models have become a leading approach in this
area. Despite their strengths, many existing methods struggle with (1) limited
generalizability that only work for specific body regions or voxel spacings,
(2) slow inference, which is a common issue for diffusion models, and (3) weak
alignment with input conditions, which is a critical issue for medical imaging.
MAISI, a previously proposed framework, addresses generalizability issues but
still suffers from slow inference and limited condition consistency. In this
work, we present MAISI-v2, the first accelerated 3D medical image synthesis
framework that integrates rectified flow to enable fast and high quality
generation. To further enhance condition fidelity, we introduce a novel
region-specific contrastive loss to enhance the sensitivity to region of
interest. Our experiments show that MAISI-v2 can achieve SOTA image quality
with $33 \times$ acceleration for latent diffusion model. We also conducted a
downstream segmentation experiment to show that the synthetic images can be
used for data augmentation. We release our code, training details, model
weights, and a GUI demo to facilitate reproducibility and promote further
development within the community.

</details>


### [6] [Few-Shot Deployment of Pretrained MRI Transformers in Brain Imaging Tasks](https://arxiv.org/abs/2508.05783)
*Mengyu Li,Guoyao Shen,Chad W. Farris,Xin Zhang*

Main category: cs.CV

TL;DR: A framework for few-shot deployment of pretrained MRI transformers using MAE pretraining, achieving state-of-the-art results in classification and segmentation tasks with minimal annotated data.


<details>
  <summary>Details</summary>
Motivation: Address the scarcity of annotated medical imaging data by leveraging pretrained transformers for efficient few-shot learning in diverse brain imaging tasks.

Method: Utilizes Masked Autoencoder (MAE) pretraining on a large-scale MRI dataset, combining frozen MAE encoders with lightweight heads for classification and hybrid MAE-FUnet for segmentation.

Result: Achieves state-of-the-art accuracy in MRI sequence identification and outperforms baselines in skull stripping and multi-class segmentation under data-limited conditions.

Conclusion: The framework is efficient, stable, and scalable, making it suitable for low-resource clinical environments and broader neuroimaging applications.

Abstract: Machine learning using transformers has shown great potential in medical
imaging, but its real-world applicability remains limited due to the scarcity
of annotated data. In this study, we propose a practical framework for the
few-shot deployment of pretrained MRI transformers in diverse brain imaging
tasks. By utilizing the Masked Autoencoder (MAE) pretraining strategy on a
large-scale, multi-cohort brain MRI dataset comprising over 31 million slices,
we obtain highly transferable latent representations that generalize well
across tasks and datasets. For high-level tasks such as classification, a
frozen MAE encoder combined with a lightweight linear head achieves
state-of-the-art accuracy in MRI sequence identification with minimal
supervision. For low-level tasks such as segmentation, we propose MAE-FUnet, a
hybrid architecture that fuses multiscale CNN features with pretrained MAE
embeddings. This model consistently outperforms other strong baselines in both
skull stripping and multi-class anatomical segmentation under data-limited
conditions. With extensive quantitative and qualitative evaluations, our
framework demonstrates efficiency, stability, and scalability, suggesting its
suitability for low-resource clinical environments and broader neuroimaging
applications.

</details>


### [7] [Optimization-Free Style Transfer for 3D Gaussian Splats](https://arxiv.org/abs/2508.05813)
*Raphael Du Sablon,David Hart*

Main category: cs.CV

TL;DR: A novel, optimization-free method for stylizing 3D Gaussian splats using a graph structure and surface-based stylization, achieving fast results without training.


<details>
  <summary>Details</summary>
Motivation: Existing methods for 3D Gaussian splat style transfer require reconstruction or optimization, which is time-consuming and resource-intensive.

Method: Generates a graph structure on the splat's implicit surface, applies feed-forward stylization, and interpolates back to the splats.

Result: Fast stylization (under 2 minutes on consumer hardware) with high-quality results, compatible with any style image or splat.

Conclusion: The proposed method is efficient, flexible, and outperforms existing approaches in speed and ease of use.

Abstract: The task of style transfer for 3D Gaussian splats has been explored in many
previous works, but these require reconstructing or fine-tuning the splat while
incorporating style information or optimizing a feature extraction network on
the splat representation. We propose a reconstruction- and optimization-free
approach to stylizing 3D Gaussian splats. This is done by generating a graph
structure across the implicit surface of the splat representation. A
feed-forward, surface-based stylization method is then used and interpolated
back to the individual splats in the scene. This allows for any style image and
3D Gaussian splat to be used without any additional training or optimization.
This also allows for fast stylization of splats, achieving speeds under 2
minutes even on consumer-grade hardware. We demonstrate the quality results
this approach achieves and compare to other 3D Gaussian splat style transfer
methods. Code is publicly available at
https://github.com/davidmhart/FastSplatStyler.

</details>


### [8] [MZEN: Multi-Zoom Enhanced NeRF for 3-D Reconstruction with Unknown Camera Poses](https://arxiv.org/abs/2508.05819)
*Jong-Ik Park,Carlee Joe-Wong,Gary K. Fedder*

Main category: cs.CV

TL;DR: MZEN enhances NeRF for industrial inspection by handling multi-zoom images, improving accuracy and detail capture.


<details>
  <summary>Details</summary>
Motivation: NeRF lacks fine-detail capture for industrial tasks like defect detection. Multi-zoom images break NeRF's multi-view consistency.

Method: MZEN introduces a learnable zoom scalar and a pose strategy: wide-field images establish a global frame, zoom-in images are pose-primed and refined.

Result: MZEN outperforms baselines, boosting PSNR by 28%, SSIM by 10%, and reducing LPIPS by 222%.

Conclusion: MZEN extends NeRF to industrial settings, capturing micron-level details while maintaining global accuracy.

Abstract: Neural Radiance Fields (NeRF) methods excel at 3D reconstruction from
multiple 2D images, even those taken with unknown camera poses. However, they
still miss the fine-detailed structures that matter in industrial inspection,
e.g., detecting sub-micron defects on a production line or analyzing chips with
Scanning Electron Microscopy (SEM). In these scenarios, the sensor resolution
is fixed and compute budgets are tight, so the only way to expose fine
structure is to add zoom-in images; yet, this breaks the multi-view consistency
that pose-free NeRF training relies on. We propose Multi-Zoom Enhanced NeRF
(MZEN), the first NeRF framework that natively handles multi-zoom image sets.
MZEN (i) augments the pin-hole camera model with an explicit, learnable zoom
scalar that scales the focal length, and (ii) introduces a novel pose strategy:
wide-field images are solved first to establish a global metric frame, and
zoom-in images are then pose-primed to the nearest wide-field counterpart via a
zoom-consistent crop-and-match procedure before joint refinement. Across eight
forward-facing scenes$\unicode{x2013}$synthetic TCAD models, real SEM of
micro-structures, and BLEFF objects$\unicode{x2013}$MZEN consistently
outperforms pose-free baselines and even high-resolution variants, boosting
PSNR by up to $28 \%$, SSIM by $10 \%$, and reducing LPIPS by up to $222 \%$.
MZEN, therefore, extends NeRF to real-world factory settings, preserving global
accuracy while capturing the micron-level details essential for industrial
inspection.

</details>


### [9] [TSMS-SAM2: Multi-scale Temporal Sampling Augmentation and Memory-Splitting Pruning for Promptable Video Object Segmentation and Tracking in Surgical Scenarios](https://arxiv.org/abs/2508.05829)
*Guoping Xu,Hua-Chieh Shao,You Zhang*

Main category: cs.CV

TL;DR: TSMS-SAM2 enhances promptable video object segmentation in surgical videos by addressing motion dynamics and memory redundancy in SAM2, achieving top performance on EndoVis datasets.


<details>
  <summary>Details</summary>
Motivation: Current foundation models like SAM2 struggle with surgical video analysis due to complex motion and memory inefficiency.

Method: TSMS-SAM2 uses multi-temporal-scale video sampling and memory splitting/pruning to improve segmentation.

Result: Achieved mean Dice scores of 95.24 and 86.73 on EndoVis2017 and EndoVis2018, outperforming prior methods.

Conclusion: TSMS-SAM2 is effective for robust, efficient segmentation in surgical videos, with potential for broader applications.

Abstract: Promptable video object segmentation and tracking (VOST) has seen significant
advances with the emergence of foundation models like Segment Anything Model 2
(SAM2); however, their application in surgical video analysis remains
challenging due to complex motion dynamics and the redundancy of memory that
impedes effective learning. In this work, we propose TSMS-SAM2, a novel
framework that enhances promptable VOST in surgical videos by addressing
challenges of rapid object motion and memory redundancy in SAM2. TSMS-SAM2
introduces two key strategies: multi-temporal-scale video sampling augmentation
to improve robustness against motion variability, and a memory splitting and
pruning mechanism that organizes and filters past frame features for more
efficient and accurate segmentation. Evaluated on EndoVis2017 and EndoVis2018
datasets, TSMS-SAM2 achieved the highest mean Dice scores of 95.24 and 86.73,
respectively, outperforming prior SAM-based and task-specific methods.
Extensive ablation studies confirm the effectiveness of multiscale temporal
augmentation and memory splitting, highlighting the framework's potential for
robust, efficient segmentation in complex surgical scenarios. Our source code
will be available at https://github.com/apple1986/TSMS-SAM2.

</details>


### [10] [Temporal Cluster Assignment for Efficient Real-Time Video Segmentation](https://arxiv.org/abs/2508.05851)
*Ka-Wai Yung,Felix J. S. Bragman,Jialang Xu,Imanol Luengo,Danail Stoyanov,Evangelos B. Mazomenos*

Main category: cs.CV

TL;DR: Temporal Cluster Assignment (TCA) improves video segmentation by leveraging temporal coherence, reducing computation while retaining details.


<details>
  <summary>Details</summary>
Motivation: High computational cost of Swin Transformer in video segmentation limits real-time applications; existing token reduction methods fail to exploit temporal redundancy.

Method: Introduces TCA, a fine-tuning-free strategy that refines token clusters using temporal correlations across frames.

Result: TCA consistently improves accuracy-speed trade-off on multiple datasets, including natural and domain-specific videos.

Conclusion: TCA effectively optimizes video segmentation by exploiting temporal coherence, generalizing well across diverse datasets.

Abstract: Vision Transformers have substantially advanced the capabilities of
segmentation models across both image and video domains. Among them, the Swin
Transformer stands out for its ability to capture hierarchical, multi-scale
representations, making it a popular backbone for segmentation in videos.
However, despite its window-attention scheme, it still incurs a high
computational cost, especially in larger variants commonly used for dense
prediction in videos. This remains a major bottleneck for real-time,
resource-constrained applications. Whilst token reduction methods have been
proposed to alleviate this, the window-based attention mechanism of Swin
requires a fixed number of tokens per window, limiting the applicability of
conventional pruning techniques. Meanwhile, training-free token clustering
approaches have shown promise in image segmentation while maintaining window
consistency. Nevertheless, they fail to exploit temporal redundancy, missing a
key opportunity to further optimize video segmentation performance. We
introduce Temporal Cluster Assignment (TCA), a lightweight and effective,
fine-tuning-free strategy that enhances token clustering by leveraging temporal
coherence across frames. Instead of indiscriminately dropping redundant tokens,
TCA refines token clusters using temporal correlations, thereby retaining
fine-grained details while significantly reducing computation. Extensive
evaluations on YouTube-VIS 2019, YouTube-VIS 2021, OVIS, and a private surgical
video dataset show that TCA consistently boosts the accuracy-speed trade-off of
existing clustering-based methods. Our results demonstrate that TCA generalizes
competently across both natural and domain-specific videos.

</details>


### [11] [VISTA: Vision-Language Imitation of Situational Thinking and Attention for Human-Like Driver Focus in Dynamic Environments](https://arxiv.org/abs/2508.05852)
*Kaiser Hamid,Khandakar Ashrafi Akbar,Nade Liang*

Main category: cs.CV

TL;DR: A vision-language framework predicts driver gaze shifts using natural language, outperforming general-purpose models in attention detection and interpretability.


<details>
  <summary>Details</summary>
Motivation: Existing studies focus on static attention estimation, but this work aims to model dynamic gaze shifts for better explainability in autonomous driving and HCI.

Method: The approach uses few-shot and zero-shot learning on RGB images, fine-tuning LLaVA with human-refined captions from BDD-A to align visual perception with gaze behavior.

Result: The fine-tuned model excels in attention shift detection and interpretability, outperforming general-purpose VLMs.

Conclusion: This work pioneers language-based gaze prediction, advancing explainable AI for autonomous driving and enabling downstream applications like behavior forecasting.

Abstract: Driver visual attention prediction is a critical task in autonomous driving
and human-computer interaction (HCI) research. Most prior studies focus on
estimating attention allocation at a single moment in time, typically using
static RGB images such as driving scene pictures. In this work, we propose a
vision-language framework that models the changing landscape of drivers' gaze
through natural language, using few-shot and zero-shot learning on single RGB
images. We curate and refine high-quality captions from the BDD-A dataset using
human-in-the-loop feedback, then fine-tune LLaVA to align visual perception
with attention-centric scene understanding. Our approach integrates both
low-level cues and top-down context (e.g., route semantics, risk anticipation),
enabling language-based descriptions of gaze behavior. We evaluate performance
across training regimes (few shot, and one-shot) and introduce domain-specific
metrics for semantic alignment and response diversity. Results show that our
fine-tuned model outperforms general-purpose VLMs in attention shift detection
and interpretability. To our knowledge, this is among the first attempts to
generate driver visual attention allocation and shifting predictions in natural
language, offering a new direction for explainable AI in autonomous driving.
Our approach provides a foundation for downstream tasks such as behavior
forecasting, human-AI teaming, and multi-agent coordination.

</details>


### [12] [Multi-view Gaze Target Estimation](https://arxiv.org/abs/2508.05857)
*Qiaomu Miao,Vivek Raju Golani,Jingyi Xu,Progga Paromita Dutta,Minh Hoai,Dimitris Samaras*

Main category: cs.CV

TL;DR: A multi-view camera method for gaze target estimation (GTE) improves accuracy by addressing single-view limitations like occlusion and ambiguity, using modules for head information, gaze selection, and scene attention.


<details>
  <summary>Details</summary>
Motivation: Existing single-view GTE methods struggle with face occlusion, target ambiguity, and out-of-view targets, limiting accuracy and applicability.

Method: The approach integrates two camera views with modules for Head Information Aggregation (HIA), Uncertainty-based Gaze Selection (UGS), and Epipolar-based Scene Attention (ESA).

Result: The method outperforms single-view baselines, especially when the second camera provides a clear face view, and enables gaze target estimation using only the second view.

Conclusion: The introduced multi-view approach and dataset advance GTE capabilities, addressing key limitations of single-view methods.

Abstract: This paper presents a method that utilizes multiple camera views for the gaze
target estimation (GTE) task. The approach integrates information from
different camera views to improve accuracy and expand applicability, addressing
limitations in existing single-view methods that face challenges such as face
occlusion, target ambiguity, and out-of-view targets. Our method processes a
pair of camera views as input, incorporating a Head Information Aggregation
(HIA) module for leveraging head information from both views for more accurate
gaze estimation, an Uncertainty-based Gaze Selection (UGS) for identifying the
most reliable gaze output, and an Epipolar-based Scene Attention (ESA) module
for cross-view background information sharing. This approach significantly
outperforms single-view baselines, especially when the second camera provides a
clear view of the person's face. Additionally, our method can estimate the gaze
target in the first view using the image of the person in the second view only,
a capability not possessed by single-view GTE methods. Furthermore, the paper
introduces a multi-view dataset for developing and evaluating multi-view GTE
methods. Data and code are available at
https://www3.cs.stonybrook.edu/~cvl/multiview_gte.html

</details>


### [13] [ETTA: Efficient Test-Time Adaptation for Vision-Language Models through Dynamic Embedding Updates](https://arxiv.org/abs/2508.05898)
*Hamidreza Dastmalchi,Aijun An,Ali cheraghian*

Main category: cs.CV

TL;DR: ETTA improves test-time adaptation for VLMs by dynamically integrating all test samples and adaptively combining prompt-based and cache-based methods, achieving better accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Pretrained VLMs like CLIP struggle with generalization under distribution shifts, and current cache-based TTA methods are limited by storing only high-confidence samples.

Method: ETTA introduces a Recursive Updating module for dynamic integration of test samples and an Adaptive Ensemble module to reduce prompt dependency, combining both for optimal performance.

Result: ETTA outperforms state-of-the-art TTA models in accuracy and computational efficiency on two benchmarks.

Conclusion: ETTA sets a new standard for efficient and effective test-time adaptation, with released code for reproducibility.

Abstract: Pretrained vision-language models (VLMs) like CLIP show strong zero-shot
performance but struggle with generalization under distribution shifts.
Test-Time Adaptation (TTA) addresses this by adapting VLMs to unlabeled test
data in new domains. While some TTA methods rely on prompt-tuning,
training-free cache-based approaches are preferred for efficiency. However,
current cache-based TTA models store only a limited set of high-confidence
samples, restricting the decision boundary to these samples and ignoring the
influence of other incoming test data. To address this, we propose Efficient
Test-Time Adaptation (ETTA), introducing a Recursive Updating module that
integrates all incoming test samples, progressively refining the decision
boundary. This strategy mimics an unbounded cache, dynamically updating
contextual embeddings for improved accuracy with minimal memory and
computational overhead. ETTA also includes an Adaptive Ensemble module to
reduce prompt dependency in image-to-text scores by dynamically selecting
optimal prompts for each class. Furthermore, ETTA adaptively combines scores
from both modules based on confidence levels, leveraging their complementary
strengths. Extensive experiments on two benchmarks confirm that ETTA surpasses
the state-of-the-art TTA models in computational complexity and accuracy,
setting a new standard for effective, efficient test-time adaptation. The code
has been released at https://github.com/hamidreza-dastmalchi/ETTA.

</details>


### [14] [HOLODECK 2.0: Vision-Language-Guided 3D World Generation with Editing](https://arxiv.org/abs/2508.05899)
*Zixuan Bian,Ruohan Ren,Yue Yang,Chris Callison-Burch*

Main category: cs.CV

TL;DR: HOLODECK 2.0 is a vision-language-guided framework for generating diverse 3D scenes from text, supporting interactive editing and outperforming baselines in quality and semantic fidelity.


<details>
  <summary>Details</summary>
Motivation: Current 3D scene generation relies heavily on manual effort and lacks flexibility. Automated methods struggle with open-domain scenes and editing.

Method: HOLODECK 2.0 uses vision-language models to parse scene requirements, generates assets via 3D generative models, and applies spatial constraints for coherent layouts.

Result: Human and CLIP-based evaluations show HOLODECK 2.0 generates high-quality scenes aligned with text, excelling in indoor and open-domain scenarios.

Conclusion: HOLODECK 2.0 advances 3D scene generation with text-to-scene capabilities, editing support, and practical applications like game modeling.

Abstract: 3D scene generation plays a crucial role in gaming, artistic creation,
virtual reality and many other domains. However, current 3D scene design still
relies heavily on extensive manual effort from creators, and existing automated
methods struggle to generate open-domain scenes or support flexible editing. As
a result, generating 3D worlds directly from text has garnered increasing
attention. In this paper, we introduce HOLODECK 2.0, an advanced
vision-language-guided framework for 3D world generation with support for
interactive scene editing based on human feedback. HOLODECK 2.0 can generate
diverse and stylistically rich 3D scenes (e.g., realistic, cartoon, anime, and
cyberpunk styles) that exhibit high semantic fidelity to fine-grained input
descriptions, suitable for both indoor and open-domain environments. HOLODECK
2.0 leverages vision-language models (VLMs) to identify and parse the objects
required in a scene and generates corresponding high-quality assets via
state-of-the-art 3D generative models. It then iteratively applies spatial
constraints derived from the VLMs to achieve semantically coherent and
physically plausible layouts. Human evaluations and CLIP-based assessments
demonstrate that HOLODECK 2.0 effectively generates high-quality scenes closely
aligned with detailed textual descriptions, consistently outperforming
baselines across indoor and open-domain scenarios. Additionally, we provide
editing capabilities that flexibly adapt to human feedback, supporting layout
refinement and style-consistent object edits. Finally, we present a practical
application of HOLODECK 2.0 in procedural game modeling, generating visually
rich and immersive environments, potentially boosting efficiency.

</details>


### [15] [Robust Image Stitching with Optimal Plane](https://arxiv.org/abs/2508.05903)
*Lang Nie,Yuan Mei,Kang Liao,Yunqiu Xu,Chunyu Lin,Bin Xiao*

Main category: cs.CV

TL;DR: RopStitch is an unsupervised deep image stitching framework that ensures robustness and naturalness by using a dual-branch architecture and virtual optimal planes.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of robustness and naturalness in image stitching, especially in diverse real-world scenes.

Method: Uses a dual-branch model (pretrained and learnable branches) for feature integration and introduces virtual optimal planes to balance content alignment and structural preservation.

Result: Outperforms existing methods in robustness and naturalness across various datasets.

Conclusion: RopStitch is effective for unsupervised image stitching, offering improved performance and generalizability.

Abstract: We present \textit{RopStitch}, an unsupervised deep image stitching framework
with both robustness and naturalness. To ensure the robustness of
\textit{RopStitch}, we propose to incorporate the universal prior of content
perception into the image stitching model by a dual-branch architecture. It
separately captures coarse and fine features and integrates them to achieve
highly generalizable performance across diverse unseen real-world scenes.
Concretely, the dual-branch model consists of a pretrained branch to capture
semantically invariant representations and a learnable branch to extract
fine-grained discriminative features, which are then merged into a whole by a
controllable factor at the correlation level. Besides, considering that content
alignment and structural preservation are often contradictory to each other, we
propose a concept of virtual optimal planes to relieve this conflict. To this
end, we model this problem as a process of estimating homography decomposition
coefficients, and design an iterative coefficient predictor and minimal
semantic distortion constraint to identify the optimal plane. This scheme is
finally incorporated into \textit{RopStitch} by warping both views onto the
optimal plane bidirectionally. Extensive experiments across various datasets
demonstrate that \textit{RopStitch} significantly outperforms existing methods,
particularly in scene robustness and content naturalness. The code is available
at {\color{red}https://github.com/MmelodYy/RopStitch}.

</details>


### [16] [Neural Field Representations of Mobile Computational Photography](https://arxiv.org/abs/2508.05907)
*Ilya Chugunov*

Main category: cs.CV

TL;DR: Neural field models enhance mobile imaging by compactly representing geometry and lighting, outperforming state-of-the-art methods without complex preprocessing or labeled data.


<details>
  <summary>Details</summary>
Motivation: The rise of advanced mobile imaging technologies and neural fields presents an opportunity to improve computational imaging without relying on traditional, resource-intensive methods.

Method: Uses well-designed neural field models trained via stochastic gradient descent to fit raw smartphone data, enabling depth estimation, layer separation, and image stitching.

Result: Outperforms existing approaches without needing labeled data or complex preprocessing, leveraging self-regularized models for inverse problems.

Conclusion: Neural fields offer a powerful, efficient solution for mobile computational imaging, enabling advanced applications with minimal dependencies.

Abstract: Over the past two decades, mobile imaging has experienced a profound
transformation, with cell phones rapidly eclipsing all other forms of digital
photography in popularity. Today's cell phones are equipped with a diverse
range of imaging technologies - laser depth ranging, multi-focal camera arrays,
and split-pixel sensors - alongside non-visual sensors such as gyroscopes,
accelerometers, and magnetometers. This, combined with on-board integrated
chips for image and signal processing, makes the cell phone a versatile
pocket-sized computational imaging platform. Parallel to this, we have seen in
recent years how neural fields - small neural networks trained to map
continuous spatial input coordinates to output signals - enable the
reconstruction of complex scenes without explicit data representations such as
pixel arrays or point clouds. In this thesis, I demonstrate how carefully
designed neural field models can compactly represent complex geometry and
lighting effects. Enabling applications such as depth estimation, layer
separation, and image stitching directly from collected in-the-wild mobile
photography data. These methods outperform state-of-the-art approaches without
relying on complex pre-processing steps, labeled ground truth data, or machine
learning priors. Instead, they leverage well-constructed, self-regularized
models that tackle challenging inverse problems through stochastic gradient
descent, fitting directly to raw measurements from a smartphone.

</details>


### [17] [Enhancing Construction Site Analysis and Understanding with 3D Segmentation](https://arxiv.org/abs/2508.05922)
*Sri Ramana Saketh Vasanthawada,Pengkun Liu,Pingbo Tang*

Main category: cs.CV

TL;DR: The paper evaluates SAM and Mask3D for 3D segmentation in construction sites, highlighting their adaptability and the need for tailored workflows for outdoor scenarios.


<details>
  <summary>Details</summary>
Motivation: Traditional methods struggle with construction sites' complexity, prompting the need for scalable, computer-vision-based solutions.

Method: Comparative analysis of SAM and Mask3D in real-world construction settings, assessing their performance and adaptability.

Result: Identifies gaps in segmentation approaches due to lack of outdoor benchmarks and showcases the models' effectiveness.

Conclusion: Tailored segmentation workflows are needed for actionable insights, advancing automated and precise construction monitoring.

Abstract: Monitoring construction progress is crucial yet resource-intensive, prompting
the exploration of computer-vision-based methodologies for enhanced efficiency
and scalability. Traditional data acquisition methods, primarily focusing on
indoor environments, falter in construction site's complex, cluttered, and
dynamically changing conditions. This paper critically evaluates the
application of two advanced 3D segmentation methods, Segment Anything Model
(SAM) and Mask3D, in challenging outdoor and indoor conditions. Trained
initially on indoor datasets, both models' adaptability and performance are
assessed in real-world construction settings, highlighting the gap in current
segmentation approaches due to the absence of benchmarks for outdoor scenarios.
Through a comparative analysis, this study not only showcases the relative
effectiveness of SAM and Mask3D but also addresses the critical need for
tailored segmentation workflows capable of extracting actionable insights from
construction site data, thereby advancing the field towards more automated and
precise monitoring techniques.

</details>


### [18] [A 3DGS-Diffusion Self-Supervised Framework for Normal Estimation from a Single Image](https://arxiv.org/abs/2508.05950)
*Yanxing Liang,Yinghui Wang,Jinlong Yang,Wei Li*

Main category: cs.CV

TL;DR: SINGAD is a self-supervised framework for normal estimation from a single image, using 3D Gaussian splatting and diffusion models to address multi-view inconsistency and data dependency.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on data-driven priors and lack explicit light-surface interaction modeling, causing multi-view conflicts and dependency on dense annotations.

Method: Integrates physics-driven light-interaction modeling and differentiable rendering-based reprojection to optimize normals using 3D geometric errors.

Result: Outperforms state-of-the-art methods on the Google Scanned Objects dataset.

Conclusion: SINGAD solves multi-view inconsistency and reduces reliance on annotated data, advancing single-image normal estimation.

Abstract: The lack of spatial dimensional information remains a challenge in normal
estimation from a single image. Recent diffusion-based methods have
demonstrated significant potential in 2D-to-3D implicit mapping, they rely on
data-driven statistical priors and miss the explicit modeling of light-surface
interaction, leading to multi-view normal direction conflicts. Moreover, the
discrete sampling mechanism of diffusion models causes gradient discontinuity
in differentiable rendering reconstruction modules, preventing 3D geometric
errors from being backpropagated to the normal generation network, thereby
forcing existing methods to depend on dense normal annotations. This paper
proposes SINGAD, a novel Self-supervised framework from a single Image for
Normal estimation via 3D GAussian splatting guided Diffusion. By integrating
physics-driven light-interaction modeling and a differentiable rendering-based
reprojection strategy, our framework directly converts 3D geometric errors into
normal optimization signals, solving the challenges of multi-view geometric
inconsistency and data dependency. Specifically, the framework constructs a
light-interaction-driven 3DGS reparameterization model to generate multi-scale
geometric features consistent with light transport principles, ensuring
multi-view normal consistency. A cross-domain feature fusion module is designed
within a conditional diffusion model, embedding geometric priors to constrain
normal generation while maintaining accurate geometric error propagation.
Furthermore, a differentiable 3D reprojection loss strategy is introduced for
self-supervised optimization that minimizes geometric error between the
reconstructed and input image, eliminating dependence on annotated normal
datasets. Quantitative evaluations on the Google Scanned Objects dataset
demonstrate that our method outperforms state-of-the-art approaches across
multiple metrics.

</details>


### [19] [Bifrost-1: Bridging Multimodal LLMs and Diffusion Models with Patch-level CLIP Latents](https://arxiv.org/abs/2508.05954)
*Han Lin,Jaemin Cho,Amir Zadeh,Chuan Li,Mohit Bansal*

Main category: cs.CV

TL;DR: Bifrost-1 integrates pretrained MLLMs and diffusion models using patch-level CLIP embeddings for efficient, high-fidelity image generation without compromising reasoning.


<details>
  <summary>Details</summary>
Motivation: To enable high-fidelity visual synthesis in LLMs without costly training or loss of reasoning capabilities.

Method: Uses patch-level CLIP embeddings as latents, integrates them into diffusion models via ControlNet, and adds a visual generation branch to MLLMs.

Result: Achieves comparable/better performance in fidelity and understanding with lower compute.

Conclusion: Bifrost-1 efficiently bridges MLLMs and diffusion models for controllable image generation.

Abstract: There is growing interest in integrating high-fidelity visual synthesis
capabilities into large language models (LLMs) without compromising their
strong reasoning capabilities. Existing methods that directly train LLMs or
bridge LLMs and diffusion models usually suffer from costly training since the
backbone LLMs have not seen image representations during pretraining. We
present Bifrost-1, a unified framework that bridges pretrained multimodal LLMs
(MLLMs) and diffusion models using patch-level CLIP image embeddings as latent
variables, which are natively aligned with the MLLM's CLIP visual encoder.
These patch-level image embeddings are integrated into the diffusion model with
a lightweight adaptation of its ControlNet. To retain the original multimodal
reasoning capabilities of MLLMs, we equip the MLLM with a visual generation
branch initialized from the original MLLM parameters when predicting the
patch-level image embeddings. By seamlessly integrating pretrained MLLMs and
diffusion models with patch-level CLIP latents, our framework enables
high-fidelity controllable image generation with significant training
efficiency. Our experiments demonstrate that Bifrost-1 achieves comparable or
better performance than previous methods in terms of visual fidelity and
multimodal understanding, with substantially lower compute during training. We
also provide comprehensive ablation studies showing the effectiveness of our
design choices.

</details>


### [20] [PASG: A Closed-Loop Framework for Automated Geometric Primitive Extraction and Semantic Anchoring in Robotic Manipulation](https://arxiv.org/abs/2508.05976)
*Zhihao Zhu,Yifan Zheng,Siyu Pan,Yaohui Jin,Yao Mu*

Main category: cs.CV

TL;DR: PASG bridges the gap between task semantics and geometric features in robotic manipulation using automatic primitive extraction and VLM-driven semantic anchoring, achieving performance comparable to manual annotations.


<details>
  <summary>Details</summary>
Motivation: The challenge of linking high-level task semantics with low-level geometric features in robotic manipulation, compounded by the limitations of current VLMs in semantic grounding and reliance on manual annotations.

Method: Proposes PASG, featuring automatic primitive extraction, VLM-driven semantic anchoring, and a spatial-semantic reasoning benchmark with a fine-tuned VLM (Qwen2.5VL-PA).

Result: Demonstrates effectiveness in diverse robotic manipulation tasks, achieving performance comparable to manual annotations.

Conclusion: PASG provides a unified paradigm for semantic-affordance understanding, bridging geometric primitives with task semantics in robotic manipulation.

Abstract: The fragmentation between high-level task semantics and low-level geometric
features remains a persistent challenge in robotic manipulation. While
vision-language models (VLMs) have shown promise in generating affordance-aware
visual representations, the lack of semantic grounding in canonical spaces and
reliance on manual annotations severely limit their ability to capture dynamic
semantic-affordance relationships. To address these, we propose Primitive-Aware
Semantic Grounding (PASG), a closed-loop framework that introduces: (1)
Automatic primitive extraction through geometric feature aggregation, enabling
cross-category detection of keypoints and axes; (2) VLM-driven semantic
anchoring that dynamically couples geometric primitives with functional
affordances and task-relevant description; (3) A spatial-semantic reasoning
benchmark and a fine-tuned VLM (Qwen2.5VL-PA). We demonstrate PASG's
effectiveness in practical robotic manipulation tasks across diverse scenarios,
achieving performance comparable to manual annotations. PASG achieves a
finer-grained semantic-affordance understanding of objects, establishing a
unified paradigm for bridging geometric primitives with task semantics in
robotic manipulation.

</details>


### [21] [AnimateScene: Camera-controllable Animation in Any Scene](https://arxiv.org/abs/2508.05982)
*Qingyang Liu,Bingjie Gao,Weiheng Huang,Jun Zhang,Zhongqian Sun,Yang Wei,Zelin Peng,Qianli Ma,Shuai Yang,Zhaohe Liao,Haonan Zhao,Li Niu*

Main category: cs.CV

TL;DR: AnimateScene integrates 3D scene reconstruction with 4D human animation, addressing placement, lighting/style alignment, and camera trajectory for realistic results.


<details>
  <summary>Details</summary>
Motivation: Seamlessly integrating 3D scenes with 4D human animation is challenging due to placement, lighting/style mismatches, and camera movement needs.

Method: AnimateScene uses an accurate placement module, training-free style alignment, and joint post-reconstruction for camera trajectories.

Result: The method produces dynamic scene videos with high detail and coherence across camera and action combinations.

Conclusion: AnimateScene effectively unifies scene and human animation, solving key challenges for realistic and engaging results.

Abstract: 3D scene reconstruction and 4D human animation have seen rapid progress and
broad adoption in recent years. However, seamlessly integrating reconstructed
scenes with 4D human animation to produce visually engaging results remains
challenging. One key difficulty lies in placing the human at the correct
location and scale within the scene while avoiding unrealistic
interpenetration. Another challenge is that the human and the background may
exhibit different lighting and style, leading to unrealistic composites. In
addition, appealing character motion videos are often accompanied by camera
movements, which means that the viewpoints need to be reconstructed along a
specified trajectory. We present AnimateScene, which addresses the above issues
in a unified framework. First, we design an accurate placement module that
automatically determines a plausible 3D position for the human and prevents any
interpenetration within the scene during motion. Second, we propose a
training-free style alignment method that adapts the 4D human representation to
match the background's lighting and style, achieving coherent visual
integration. Finally, we design a joint post-reconstruction method for both the
4D human and the 3D scene that allows camera trajectories to be inserted,
enabling the final rendered video to feature visually appealing camera
movements. Extensive experiments show that AnimateScene generates dynamic scene
videos with high geometric detail and spatiotemporal coherence across various
camera and action combinations.

</details>


### [22] [ETA: Energy-based Test-time Adaptation for Depth Completion](https://arxiv.org/abs/2508.05989)
*Younjoon Chung,Hyoungseob Park,Patrick Rim,Xiaoran Zhang,Jihe He,Ziyao Zeng,Safa Cicek,Byung-Woo Hong,James S. Duncan,Alex Wong*

Main category: cs.CV

TL;DR: Proposes Energy-based Test-time Adaptation (ETA) for adapting pretrained depth completion models to new environments by minimizing energy scores of predictions.


<details>
  <summary>Details</summary>
Motivation: Addresses covariate shift in depth completion models when transferred to novel environments, without prior access to target data.

Method: Uses adversarial perturbations to explore data space, trains an energy model to score predictions, and updates model parameters at test time to align with source distribution.

Result: Improves over state-of-the-art by 6.94% (outdoors) and 10.23% (indoors) across six datasets.

Conclusion: ETA effectively adapts depth completion models to new conditions without assumptions about target data.

Abstract: We propose a method for test-time adaptation of pretrained depth completion
models. Depth completion models, trained on some ``source'' data, often predict
erroneous outputs when transferred to ``target'' data captured in novel
environmental conditions due to a covariate shift. The crux of our method lies
in quantifying the likelihood of depth predictions belonging to the source data
distribution. The challenge is in the lack of access to out-of-distribution
(target) data prior to deployment. Hence, rather than making assumptions
regarding the target distribution, we utilize adversarial perturbations as a
mechanism to explore the data space. This enables us to train an energy model
that scores local regions of depth predictions as in- or out-of-distribution.
We update the parameters of pretrained depth completion models at test time to
minimize energy, effectively aligning test-time predictions to those of the
source distribution. We call our method ``Energy-based Test-time Adaptation'',
or ETA for short. We evaluate our method across three indoor and three outdoor
datasets, where ETA improve over the previous state-of-the-art method by an
average of 6.94% for outdoors and 10.23% for indoors. Project Page:
https://fuzzythecat.github.io/eta.

</details>


### [23] [Fast Motion Estimation and Context-Aware Refinement for Efficient Bayer-Domain Video Vision](https://arxiv.org/abs/2508.05990)
*Haichao Wang,Xinyue Xi,Jiangtao Wen,Yuxing Han*

Main category: cs.CV

TL;DR: Proposes an efficient video computer vision system by eliminating the image signal processor and using Bayer-format data, along with a fast block matching-based motion estimation algorithm and refinement techniques.


<details>
  <summary>Details</summary>
Motivation: Addresses temporal redundancy and front-end computation overhead in video computer vision systems, which existing methods fail to fully reduce.

Method: 1. Removes image signal processor, using Bayer-format data directly. 2. Introduces fast block matching-based motion estimation with MV refinement and context-aware block refinement. 3. Employs frame selection for accuracy-efficiency balance.

Result: Achieves significant acceleration with slight performance loss in multiple video computer vision tasks.

Conclusion: The proposed system effectively reduces redundancy and overhead, improving efficiency without major accuracy trade-offs.

Abstract: The efficiency of video computer vision system remains a challenging task due
to the high temporal redundancy inside a video. Existing works have been
proposed for efficient vision computer vision. However, they do not fully
reduce the temporal redundancy and neglect the front end computation overhead.
In this paper, we propose an efficient video computer vision system. First,
image signal processor is removed and Bayer-format data is directly fed into
video computer vision models, thus saving the front end computation. Second,
instead of optical flow models and video codecs, a fast block matching-based
motion estimation algorithm is proposed specifically for efficient video
computer vision, with a MV refinement module. To correct the error,
context-aware block refinement network is introduced to refine regions with
large error. To further balance the accuracy and efficiency, a frame selection
strategy is employed. Experiments on multiple video computer vision tasks
demonstrate that our method achieves significant acceleration with slight
performance loss.

</details>


### [24] [ECMF: Enhanced Cross-Modal Fusion for Multimodal Emotion Recognition in MER-SEMI Challenge](https://arxiv.org/abs/2508.05991)
*Juewen Hu,Yexin Li,Jiulin Li,Shuo Chen,Pring Wong*

Main category: cs.CV

TL;DR: A novel multimodal emotion recognition framework is proposed, leveraging pre-trained models and innovative fusion strategies to address data scarcity and achieve high performance.


<details>
  <summary>Details</summary>
Motivation: Enhancing human-computer interaction through improved emotion recognition by tackling the MER-SEMI challenge.

Method: Uses large-scale pre-trained models for feature extraction, a dual-branch visual encoder, context-enriched textual processing, and a fusion strategy with self-attention and residual connections. Noisy labels are refined via multi-source labeling.

Result: Achieves a weighted F-score of 87.49%, outperforming the baseline of 78.63% on the MER2025-SEMI dataset.

Conclusion: The framework is effective, demonstrating significant performance improvement in multimodal emotion recognition.

Abstract: Emotion recognition plays a vital role in enhancing human-computer
interaction. In this study, we tackle the MER-SEMI challenge of the MER2025
competition by proposing a novel multimodal emotion recognition framework. To
address the issue of data scarcity, we leverage large-scale pre-trained models
to extract informative features from visual, audio, and textual modalities.
Specifically, for the visual modality, we design a dual-branch visual encoder
that captures both global frame-level features and localized facial
representations. For the textual modality, we introduce a context-enriched
method that employs large language models to enrich emotional cues within the
input text. To effectively integrate these multimodal features, we propose a
fusion strategy comprising two key components, i.e., self-attention mechanisms
for dynamic modality weighting, and residual connections to preserve original
representations. Beyond architectural design, we further refine noisy labels in
the training set by a multi-source labeling strategy. Our approach achieves a
substantial performance improvement over the official baseline on the
MER2025-SEMI dataset, attaining a weighted F-score of 87.49% compared to
78.63%, thereby validating the effectiveness of the proposed framework.

</details>


### [25] [EvoMakeup: High-Fidelity and Controllable Makeup Editing with MakeupQuad](https://arxiv.org/abs/2508.05994)
*Huadong Wu,Yi Fu,Yunhao Li,Yuan Gao,Kang Du*

Main category: cs.CV

TL;DR: The paper introduces MakeupQuad, a dataset for facial makeup editing, and EvoMakeup, a framework for high-fidelity, controllable makeup transfer, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods for facial makeup editing produce low-quality results due to lack of structured paired data and struggle with identity and makeup fidelity.

Method: Proposes MakeupQuad dataset and EvoMakeup framework, which uses multi-stage distillation to iteratively improve data and model quality.

Result: EvoMakeup outperforms prior methods on real-world benchmarks, achieving superior makeup fidelity and identity preservation.

Conclusion: The method effectively balances makeup fidelity and identity preservation, supporting multi-task makeup editing in a single model.

Abstract: Facial makeup editing aims to realistically transfer makeup from a reference
to a target face. Existing methods often produce low-quality results with
coarse makeup details and struggle to preserve both identity and makeup
fidelity, mainly due to the lack of structured paired data -- where source and
result share identity, and reference and result share identical makeup. To
address this, we introduce MakeupQuad, a large-scale, high-quality dataset with
non-makeup faces, references, edited results, and textual makeup descriptions.
Building on this, we propose EvoMakeup, a unified training framework that
mitigates image degradation during multi-stage distillation, enabling iterative
improvement of both data and model quality. Although trained solely on
synthetic data, EvoMakeup generalizes well and outperforms prior methods on
real-world benchmarks. It supports high-fidelity, controllable, multi-task
makeup editing -- including full-face and partial reference-based editing, as
well as text-driven makeup editing -- within a single model. Experimental
results demonstrate that our method achieves superior makeup fidelity and
identity preservation, effectively balancing both aspects. Code and dataset
will be released upon acceptance.

</details>


### [26] [MathReal: We Keep It Real! A Real Scene Benchmark for Evaluating Math Reasoning in Multimodal Large Language Models](https://arxiv.org/abs/2508.06009)
*Jun Feng,Zixin Wang,Zhentao Zhang,Yue Guo,Zhihan Zhou,Xiuyi Chen,Zhenyang Li,Dawei Yin*

Main category: cs.CV

TL;DR: MathReal introduces a dataset of 2,000 real-world K-12 math questions with images, highlighting challenges for MLLMs in realistic educational settings.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack real-world K-12 educational images, limiting MLLM evaluation in authentic scenarios.

Method: Curated dataset (MathReal) with 2,000 questions, classified into image quality, perspective, and content interference, and evaluated MLLMs in six settings.

Result: MLLMs struggle with real-world images, revealing gaps in recognition, comprehension, and reasoning.

Conclusion: MathReal exposes MLLM limitations in real-world math reasoning, guiding future improvements.

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable
capabilities in visual mathematical reasoning across various existing
benchmarks. However, these benchmarks are predominantly based on clean or
processed multimodal inputs, without incorporating the images provided by
real-world Kindergarten through 12th grade (K-12) educational users. To address
this gap, we introduce MathReal, a meticulously curated dataset comprising
2,000 mathematical questions with images captured by handheld mobile devices in
authentic scenarios. Each question is an image, containing the question text
and visual element. We systematically classify the real images into three
primary categories: image quality degradation, perspective variation, and
irrelevant content interference, which are further delineated into 14
subcategories. Additionally, MathReal spans five core knowledge and ability
categories, which encompass three question types and are divided into three
difficulty levels. To comprehensively evaluate the multimodal mathematical
reasoning abilities of state-of-the-art MLLMs in real-world scenarios, we
design six experimental settings that enable a systematic analysis of their
performance. Through extensive experimentation, we find that the
problem-solving abilities of existing MLLMs are significantly challenged in
realistic educational contexts. Based on this, we conduct a thorough analysis
of their performance and error patterns, providing insights into their
recognition, comprehension, and reasoning capabilities, and outlining
directions for future improvements. Data and code:
https://github.com/junfeng0288/MathReal.

</details>


### [27] [ExploreGS: Explorable 3D Scene Reconstruction with Virtual Camera Samplings and Diffusion Priors](https://arxiv.org/abs/2508.06014)
*Minsu Kim,Subin Jeon,In Cho,Mijin Yoo,Seon Joo Kim*

Main category: cs.CV

TL;DR: A 3DGS-based pipeline improves novel view synthesis by generating additional training views and refining results with video diffusion priors, outperforming existing methods on challenging scenes.


<details>
  <summary>Details</summary>
Motivation: Existing methods for novel view synthesis with 3D Gaussian Splatting (3DGS) struggle with artifacts and missing regions when rendering from viewpoints outside the training trajectory, hindering seamless scene exploration.

Method: The proposed pipeline generates additional training views using an information-gain-driven virtual camera placement strategy and refines rendered results with video diffusion priors. Fine-tuning 3D Gaussians with these views enhances reconstruction quality.

Result: The approach outperforms existing 3DGS-based methods, enabling high-quality, artifact-free rendering from arbitrary viewpoints, as demonstrated on the Wild-Explore benchmark.

Conclusion: The proposed method significantly improves reconstruction quality and rendering performance for novel view synthesis, enabling seamless exploration of challenging scenes.

Abstract: Recent advances in novel view synthesis (NVS) have enabled real-time
rendering with 3D Gaussian Splatting (3DGS). However, existing methods struggle
with artifacts and missing regions when rendering from viewpoints that deviate
from the training trajectory, limiting seamless scene exploration. To address
this, we propose a 3DGS-based pipeline that generates additional training views
to enhance reconstruction. We introduce an information-gain-driven virtual
camera placement strategy to maximize scene coverage, followed by video
diffusion priors to refine rendered results. Fine-tuning 3D Gaussians with
these enhanced views significantly improves reconstruction quality. To evaluate
our method, we present Wild-Explore, a benchmark designed for challenging scene
exploration. Experiments demonstrate that our approach outperforms existing
3DGS-based methods, enabling high-quality, artifact-free rendering from
arbitrary viewpoints.
  https://exploregs.github.io

</details>


### [28] [Improved Sub-Visible Particle Classification in Flow Imaging Microscopy via Generative AI-Based Image Synthesis](https://arxiv.org/abs/2508.06021)
*Utku Ozbulak,Michaela Cohrs,Hristo L. Svilenov,Joris Vankerschaver,Wesley De Neve*

Main category: cs.CV

TL;DR: A diffusion model generates high-fidelity particle images to address data imbalance, improving multi-class classifier performance.


<details>
  <summary>Details</summary>
Motivation: Data scarcity and imbalance hinder effective multi-class classification of sub-visible particles, especially for rare types like silicone oil and air bubbles.

Method: Develop a diffusion model to generate synthetic particle images for dataset augmentation, validated by visual and structural similarity to real images.

Result: Experiments on 500,000 protein particle images show improved classification performance with negligible downsides.

Conclusion: The approach effectively mitigates data imbalance, and the models are publicly released for reproducibility and future research.

Abstract: Sub-visible particle analysis using flow imaging microscopy combined with
deep learning has proven effective in identifying particle types, enabling the
distinction of harmless components such as silicone oil from protein particles.
However, the scarcity of available data and severe imbalance between particle
types within datasets remain substantial hurdles when applying multi-class
classifiers to such problems, often forcing researchers to rely on less
effective methods. The aforementioned issue is particularly challenging for
particle types that appear unintentionally and in lower numbers, such as
silicone oil and air bubbles, as opposed to protein particles, where obtaining
large numbers of images through controlled settings is comparatively
straightforward. In this work, we develop a state-of-the-art diffusion model to
address data imbalance by generating high-fidelity images that can augment
training datasets, enabling the effective training of multi-class deep neural
networks. We validate this approach by demonstrating that the generated samples
closely resemble real particle images in terms of visual quality and structure.
To assess the effectiveness of using diffusion-generated images in training
datasets, we conduct large-scale experiments on a validation dataset comprising
500,000 protein particle images and demonstrate that this approach improves
classification performance with no negligible downside. Finally, to promote
open research and reproducibility, we publicly release both our diffusion
models and the trained multi-class deep neural network classifiers, along with
a straightforward interface for easy integration into future studies, at
https://github.com/utkuozbulak/svp-generative-ai.

</details>


### [29] [Learning 3D Texture-Aware Representations for Parsing Diverse Human Clothing and Body Parts](https://arxiv.org/abs/2508.06032)
*Kiran Chhatre,Christopher Peters,Srikrishna Karanam*

Main category: cs.CV

TL;DR: Spectrum is a unified network for detailed human parsing, leveraging a fine-tuned Image-to-Texture diffusion model to improve alignment with body parts and clothing, outperforming baselines in segmentation tasks.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack fine-grained parsing of clothing and body parts, and open-vocabulary models often group humans into a single category, missing detailed distinctions.

Method: Spectrum repurposes an Image-to-Texture diffusion model, fine-tuned on 3D human texture maps, to extract features and generate semantically valid masks for diverse clothing and body parts.

Result: Spectrum consistently outperforms baseline methods in cross-dataset experiments for body parts, clothing, unseen categories, and full-body masks.

Conclusion: Spectrum provides a robust solution for detailed human parsing, addressing limitations of existing methods and demonstrating superior performance in segmentation tasks.

Abstract: Existing methods for human parsing into body parts and clothing often use
fixed mask categories with broad labels that obscure fine-grained clothing
types. Recent open-vocabulary segmentation approaches leverage pretrained
text-to-image (T2I) diffusion model features for strong zero-shot transfer, but
typically group entire humans into a single person category, failing to
distinguish diverse clothing or detailed body parts. To address this, we
propose Spectrum, a unified network for part-level pixel parsing (body parts
and clothing) and instance-level grouping. While diffusion-based
open-vocabulary models generalize well across tasks, their internal
representations are not specialized for detailed human parsing. We observe
that, unlike diffusion models with broad representations, image-driven 3D
texture generators maintain faithful correspondence to input images, enabling
stronger representations for parsing diverse clothing and body parts. Spectrum
introduces a novel repurposing of an Image-to-Texture (I2Tx) diffusion model --
obtained by fine-tuning a T2I model on 3D human texture maps -- for improved
alignment with body parts and clothing. From an input image, we extract
human-part internal features via the I2Tx diffusion model and generate
semantically valid masks aligned to diverse clothing categories through
prompt-guided grounding. Once trained, Spectrum produces semantic segmentation
maps for every visible body part and clothing category, ignoring standalone
garments or irrelevant objects, for any number of humans in the scene. We
conduct extensive cross-dataset experiments -- separately assessing body parts,
clothing parts, unseen clothing categories, and full-body masks -- and
demonstrate that Spectrum consistently outperforms baseline methods in
prompt-based segmentation.

</details>


### [30] [InstantEdit: Text-Guided Few-Step Image Editing with Piecewise Rectified Flow](https://arxiv.org/abs/2508.06033)
*Yiming Gong,Zhen Zhu,Minjia Zhang*

Main category: cs.CV

TL;DR: InstantEdit is a fast text-guided image editing method using RectifiedFlow, PerRFI inversion, and Disentangled Prompt Guidance for high-quality, coherent edits.


<details>
  <summary>Details</summary>
Motivation: To enable fast and precise text-guided image editing while preserving critical content and following textual instructions closely.

Method: Uses RectifiedFlow with PerRFI inversion, Inversion Latent Injection for regeneration, Disentangled Prompt Guidance, and Canny-conditioned ControlNet.

Result: Achieves better qualitative and quantitative results on the PIE dataset compared to state-of-the-art methods.

Conclusion: InstantEdit is efficient and effective for text-guided image editing, outperforming existing few-step methods.

Abstract: We propose a fast text-guided image editing method called InstantEdit based
on the RectifiedFlow framework, which is structured as a few-step editing
process that preserves critical content while following closely to textual
instructions. Our approach leverages the straight sampling trajectories of
RectifiedFlow by introducing a specialized inversion strategy called PerRFI. To
maintain consistent while editable results for RectifiedFlow model, we further
propose a novel regeneration method, Inversion Latent Injection, which
effectively reuses latent information obtained during inversion to facilitate
more coherent and detailed regeneration. Additionally, we propose a
Disentangled Prompt Guidance technique to balance editability with detail
preservation, and integrate a Canny-conditioned ControlNet to incorporate
structural cues and suppress artifacts. Evaluation on the PIE image editing
dataset demonstrates that InstantEdit is not only fast but also achieves better
qualitative and quantitative results compared to state-of-the-art few-step
editing methods.

</details>


### [31] [More Is Better: A MoE-Based Emotion Recognition Framework with Human Preference Alignment](https://arxiv.org/abs/2508.06036)
*Jun Xie,Yingjian Zhu,Feng Chen,Zhenghao Zhang,Xiaohui Fan,Hongzhu Yi,Xinming Wang,Chen Yu,Yue Bi,Zhaoran Zhao,Xiongjun Guan,Zhepeng Wang*

Main category: cs.CV

TL;DR: A semi-supervised learning framework for emotion recognition using Mixture of Experts (MoE), integrating diverse modalities and pseudo-labeling, achieving 2nd place in MER2025-SEMI with an F1-score of 0.8772.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of semi-supervised learning in emotion recognition by leveraging diverse input modalities and unlabeled data effectively.

Method: Proposes a MoE system with diverse modalities (e.g., VLMs, AU), consensus-based pseudo-labeling, and a two-stage training paradigm, followed by a multi-expert voting ensemble and rule-based re-ranking.

Result: Achieves an F1-score of 0.8772 on the MER2025-SEMI test set, ranking 2nd.

Conclusion: The framework demonstrates robustness in semi-supervised emotion recognition by combining diverse inputs and pseudo-labeling, with competitive performance.

Abstract: In this paper, we present our solution for the semi-supervised learning track
(MER-SEMI) in MER2025. We propose a comprehensive framework, grounded in the
principle that "more is better," to construct a robust Mixture of Experts (MoE)
emotion recognition system. Our approach integrates a diverse range of input
modalities as independent experts, including novel signals such as knowledge
from large Vision-Language Models (VLMs) and temporal Action Unit (AU)
information. To effectively utilize unlabeled data, we introduce a
consensus-based pseudo-labeling strategy, generating high-quality labels from
the agreement between a baseline model and Gemini, which are then used in a
two-stage training paradigm. Finally, we employ a multi-expert voting ensemble
combined with a rule-based re-ranking process to correct prediction bias and
better align the outputs with human preferences. Evaluated on the MER2025-SEMI
challenge dataset, our method achieves an F1-score of 0.8772 on the test set,
ranking 2nd in the track. Our code is available at
https://github.com/zhuyjan/MER2025-MRAC25.

</details>


### [32] [Fourier-VLM: Compressing Vision Tokens in the Frequency Domain for Large Vision-Language Models](https://arxiv.org/abs/2508.06038)
*Huanyu Wang,Jushi Kai,Haoli Bai,Lu Hou,Bo Jiang,Ziwei He,Zhouhan Lin*

Main category: cs.CV

TL;DR: Fourier-VLM compresses visual representations in the frequency domain using DCT and FFT, reducing computational overhead while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: The large number of vision tokens in VLMs increases context length, causing high computational costs and latency. Existing methods compromise performance or add extra costs.

Method: Applies a low-pass filter to vision features using 2D DCT, efficiently computed via FFT, reducing token count without extra parameters.

Result: Achieves competitive performance, reduces FLOPs by 83.8%, and boosts generation speed by 31.2% compared to LLaVA-v1.5.

Conclusion: Fourier-VLM offers superior efficiency and practicality for VLMs without sacrificing performance.

Abstract: Vision-Language Models (VLMs) typically replace the predefined image
placeholder token (<image>) in textual instructions with visual features from
an image encoder, forming the input to a backbone Large Language Model (LLM).
However, the large number of vision tokens significantly increases the context
length, leading to high computational overhead and inference latency. While
previous efforts mitigate this by selecting only important visual features or
leveraging learnable queries to reduce token count, they often compromise
performance or introduce substantial extra costs. In response, we propose
Fourier-VLM, a simple yet efficient method that compresses visual
representations in the frequency domain. Our approach is motivated by the
observation that vision features output from the vision encoder exhibit
concentrated energy in low-frequency components. Leveraging this, we apply a
low-pass filter to the vision features using a two-dimentional Discrete Cosine
Transform (DCT). Notably, the DCT is efficiently computed via the Fast Fourier
Transform (FFT) operator with a time complexity of $\mathcal{O}(n\log n)$,
minimizing the extra computational cost while introducing no additional
parameters. Extensive experiments across various image-based benchmarks
demonstrate that Fourier-VLM achieves competitive performance with strong
generalizability across both LLaVA and Qwen-VL architectures. Crucially, it
reduce inference FLOPs by up to 83.8% and boots generation speed by 31.2%
compared to LLaVA-v1.5, highlighting the superior efficiency and practicality.

</details>


### [33] [NEP: Autoregressive Image Editing via Next Editing Token Prediction](https://arxiv.org/abs/2508.06044)
*Huimin Wu,Xiaojian Ma,Haozhe Zhao,Yanpeng Zhao,Qing Li*

Main category: cs.CV

TL;DR: The paper proposes Next Editing-token Prediction (NEP) for text-guided image editing, focusing only on regions needing edits to reduce computational costs and improve edit quality.


<details>
  <summary>Details</summary>
Motivation: Existing methods regenerate entire images, leading to unnecessary costs and compromised edit quality by biasing non-edited regions.

Method: Formulates editing as NEP using autoregressive image generation, pre-training an any-order autoregressive text-to-image model for zero-shot editing.

Result: Achieves state-of-the-art performance on benchmarks and supports test-time scaling (TTS) for iterative refinement.

Conclusion: NEP offers efficient, high-quality image editing by selectively regenerating only necessary regions, advancing the field.

Abstract: Text-guided image editing involves modifying a source image based on a
language instruction and, typically, requires changes to only small local
regions. However, existing approaches generate the entire target image rather
than selectively regenerate only the intended editing areas. This results in
(1) unnecessary computational costs and (2) a bias toward reconstructing
non-editing regions, which compromises the quality of the intended edits. To
resolve these limitations, we propose to formulate image editing as Next
Editing-token Prediction (NEP) based on autoregressive image generation, where
only regions that need to be edited are regenerated, thus avoiding unintended
modification to the non-editing areas. To enable any-region editing, we propose
to pre-train an any-order autoregressive text-to-image (T2I) model. Once
trained, it is capable of zero-shot image editing and can be easily adapted to
NEP for image editing, which achieves a new state-of-the-art on widely used
image editing benchmarks. Moreover, our model naturally supports test-time
scaling (TTS) through iteratively refining its generation in a zero-shot
manner. The project page is: https://nep-bigai.github.io/

</details>


### [34] [VQAThinker: Exploring Generalizable and Explainable Video Quality Assessment via Reinforcement Learning](https://arxiv.org/abs/2508.06051)
*Linhan Cao,Wei Sun,Weixia Zhang,Xiangyang Zhu,Jun Jia,Kaiwei Zhang,Dandan Zhu,Guangtao Zhai,Xiongkuo Min*

Main category: cs.CV

TL;DR: VQAThinker is a reasoning-based VQA framework using LMMs and reinforcement learning to improve generalization and explainability in video quality assessment.


<details>
  <summary>Details</summary>
Motivation: Existing VQA models struggle with poor generalization to OOD videos and limited explainability, hindering real-world applicability.

Method: Uses GRPO, a rule-guided reinforcement learning algorithm, with three VQA-specific rewards: bell-shaped regression, pairwise ranking, and temporal consistency.

Result: Achieves state-of-the-art performance on in-domain and OOD benchmarks, with strong generalization and superior explainability in distortion attribution and quality description.

Conclusion: Reinforcement learning with score-level supervision is effective for building generalizable and explainable VQA models.

Abstract: Video quality assessment (VQA) aims to objectively quantify perceptual
quality degradation in alignment with human visual perception. Despite recent
advances, existing VQA models still suffer from two critical limitations:
\textit{poor generalization to out-of-distribution (OOD) videos} and
\textit{limited explainability}, which restrict their applicability in
real-world scenarios. To address these challenges, we propose
\textbf{VQAThinker}, a reasoning-based VQA framework that leverages large
multimodal models (LMMs) with reinforcement learning to jointly model video
quality understanding and scoring, emulating human perceptual decision-making.
Specifically, we adopt group relative policy optimization (GRPO), a rule-guided
reinforcement learning algorithm that enables reasoning over video quality
under score-level supervision, and introduce three VQA-specific rewards: (1) a
\textbf{bell-shaped regression reward} that increases rapidly as the prediction
error decreases and becomes progressively less sensitive near the ground truth;
(2) a \textbf{pairwise ranking reward} that guides the model to correctly
determine the relative quality between video pairs; and (3) a \textbf{temporal
consistency reward} that encourages the model to prefer temporally coherent
videos over their perturbed counterparts. Extensive experiments demonstrate
that VQAThinker achieves state-of-the-art performance on both in-domain and OOD
VQA benchmarks, showing strong generalization for video quality scoring.
Furthermore, evaluations on video quality understanding tasks validate its
superiority in distortion attribution and quality description compared to
existing explainable VQA models and LMMs. These findings demonstrate that
reinforcement learning offers an effective pathway toward building
generalizable and explainable VQA models solely with score-level supervision.

</details>


### [35] [LV-Net: Anatomy-aware lateral ventricle shape modeling with a case study on Alzheimer's disease, the Australian Imaging Biomarkers and Lifestyle flagship study of ageing](https://arxiv.org/abs/2508.06055)
*Wonjung Park,Suhyun Ahn,Jinah Park*

Main category: cs.CV

TL;DR: LV-Net is a framework for creating 3D LV meshes from brain MRI, improving segmentation and shape analysis for neurological disease biomarkers.


<details>
  <summary>Details</summary>
Motivation: LV shape analysis is promising for neurological diseases but faces challenges due to shape variability and MRI resolution limitations.

Method: LV-Net deforms a joint LV-hippocampus template mesh, leveraging anatomical relationships to reduce artifacts and improve point correspondence.

Result: LV-Net achieves superior reconstruction accuracy and reliable shape descriptors, identifying disease-associated LV subregions in Alzheimer's analysis.

Conclusion: LV-Net enhances LV shape modeling, offering robust biomarkers for neurological diseases like Alzheimer's.

Abstract: Lateral ventricle (LV) shape analysis holds promise as a biomarker for
neurological diseases; however, challenges remain due to substantial shape
variability across individuals and segmentation difficulties arising from
limited MRI resolution. We introduce LV-Net, a novel framework for producing
individualized 3D LV meshes from brain MRI by deforming an anatomy-aware joint
LV-hippocampus template mesh. By incorporating anatomical relationships
embedded within the joint template, LV-Net reduces boundary segmentation
artifacts and improves reconstruction robustness. In addition, by classifying
the vertices of the template mesh based on their anatomical adjacency, our
method enhances point correspondence across subjects, leading to more accurate
LV shape statistics. We demonstrate that LV-Net achieves superior
reconstruction accuracy, even in the presence of segmentation imperfections,
and delivers more reliable shape descriptors across diverse datasets. Finally,
we apply LV-Net to Alzheimer's disease analysis, identifying LV subregions that
show significantly associations with the disease relative to cognitively normal
controls. The codes for LV shape modeling are available at
https://github.com/PWonjung/LV_Shape_Modeling.

</details>


### [36] [AGI for the Earth, the path, possibilities and how to evaluate intelligence of models that work with Earth Observation Data?](https://arxiv.org/abs/2508.06057)
*Mojtaba Valipour,Kelly Zheng,James Lowman,Spencer Szabados,Mike Gartner,Bobby Braswell*

Main category: cs.CV

TL;DR: The paper advocates for the inclusion of satellite spectral imagery in AGI research, critiques current benchmarks, and proposes a comprehensive benchmark for evaluating Earth observation models.


<details>
  <summary>Details</summary>
Motivation: Satellite spectral imagery is underutilized in AGI research despite its potential to enhance understanding of the natural world. Existing benchmarks lack the ability to evaluate generalization in this domain.

Method: The paper reviews existing benchmarks, identifies their limitations, and proposes a set of tasks for a new benchmark to assess Earth observation models.

Result: Current benchmarks are inadequate for evaluating Earth observation models, necessitating a more comprehensive approach.

Conclusion: A new benchmark is needed to effectively evaluate AGI models' interaction with Earth observation data, fostering advancements in the field.

Abstract: Artificial General Intelligence (AGI) is closer than ever to becoming a
reality, sparking widespread enthusiasm in the research community to collect
and work with various modalities, including text, image, video, and audio.
Despite recent efforts, satellite spectral imagery, as an additional modality,
has yet to receive the attention it deserves. This area presents unique
challenges, but also holds great promise in advancing the capabilities of AGI
in understanding the natural world. In this paper, we argue why Earth
Observation data is useful for an intelligent model, and then we review
existing benchmarks and highlight their limitations in evaluating the
generalization ability of foundation models in this domain. This paper
emphasizes the need for a more comprehensive benchmark to evaluate earth
observation models. To facilitate this, we propose a comprehensive set of tasks
that a benchmark should encompass to effectively assess a model's ability to
understand and interact with Earth observation data.

</details>


### [37] [Lightweight Quad Bayer HybridEVS Demosaicing via State Space Augmented Cross-Attention](https://arxiv.org/abs/2508.06058)
*Shiyang Zhou,Haijin Zeng,Yunfan Lu,Yongyong Chen,Jie Liu,Jingyong Su*

Main category: cs.CV

TL;DR: TSANet is a lightweight two-stage network for event-based camera demosaicing, improving performance and reducing computational costs.


<details>
  <summary>Details</summary>
Motivation: Challenges in combining Quad Bayer CFA with event pixels lead to aliasing and artifacts, especially on mobile devices.

Method: TSANet uses a two-stage approach with state space augmented cross-attention and a Cross-Swin State Block for demosaicing.

Result: Outperforms DemosaicFormer in PSNR and SSIM across seven datasets, with reduced parameters and computation.

Conclusion: TSANet enables efficient demosaicing for mobile devices, offering new possibilities for event-based photography.

Abstract: Event cameras like the Hybrid Event-based Vision Sensor (HybridEVS) camera
capture brightness changes as asynchronous "events" instead of frames, offering
advanced application on mobile photography. However, challenges arise from
combining a Quad Bayer Color Filter Array (CFA) sensor with event pixels
lacking color information, resulting in aliasing and artifacts on the
demosaicing process before downstream application. Current methods struggle to
address these issues, especially on resource-limited mobile devices. In
response, we introduce \textbf{TSANet}, a lightweight \textbf{T}wo-stage
network via \textbf{S}tate space augmented cross-\textbf{A}ttention, which can
handle event pixels inpainting and demosaicing separately, leveraging the
benefits of dividing complex tasks into manageable subtasks. Furthermore, we
introduce a lightweight Cross-Swin State Block that uniquely utilizes
positional prior for demosaicing and enhances global dependencies through the
state space model with linear complexity. In summary, TSANet demonstrates
excellent demosaicing performance on both simulated and real data of HybridEVS
while maintaining a lightweight model, averaging better results than the
previous state-of-the-art method DemosaicFormer across seven diverse datasets
in both PSNR and SSIM, while respectively reducing parameter and computation
costs by $1.86\times$ and $3.29\times$. Our approach presents new possibilities
for efficient image demosaicing on mobile devices. Code is available in the
supplementary materials.

</details>


### [38] [Distribution-Specific Learning for Joint Salient and Camouflaged Object Detection](https://arxiv.org/abs/2508.06063)
*Chao Hao,Zitong Yu,Xin Liu,Yuhao Wang,Weicheng Xie,Jingang Shi,Huanjing Yue,Jingyu Yang*

Main category: cs.CV

TL;DR: SCJoint is a joint learning scheme for Salient Object Detection (SOD) and Camouflaged Object Detection (COD), enabling a single network to perform both tasks effectively by decoupling their contradictory attributes with minimal task-specific parameters.


<details>
  <summary>Details</summary>
Motivation: Previous works assumed joint learning of SOD and COD would confuse the network, but this paper argues that with the right approach, joint learning can benefit both tasks.

Method: Proposes SCJoint, a scheme with shared network structure and task-specific learnable parameters, and a saliency-based sampling strategy (SBSS) to balance and improve training data.

Result: The trained network, JoNet, achieves competitive performance in both SOD and COD tasks.

Conclusion: Joint learning of SOD and COD is feasible and beneficial with the proposed SCJoint and SBSS, leading to a powerful generalist network.

Abstract: Salient object detection (SOD) and camouflaged object detection (COD) are two
closely related but distinct computer vision tasks. Although both are
class-agnostic segmentation tasks that map from RGB space to binary space, the
former aims to identify the most salient objects in the image, while the latter
focuses on detecting perfectly camouflaged objects that blend into the
background in the image. These two tasks exhibit strong contradictory
attributes. Previous works have mostly believed that joint learning of these
two tasks would confuse the network, reducing its performance on both tasks.
However, here we present an opposite perspective: with the correct approach to
learning, the network can simultaneously possess the capability to find both
salient and camouflaged objects, allowing both tasks to benefit from joint
learning. We propose SCJoint, a joint learning scheme for SOD and COD tasks,
assuming that the decoding processes of SOD and COD have different distribution
characteristics. The key to our method is to learn the respective means and
variances of the decoding processes for both tasks by inserting a minimal
amount of task-specific learnable parameters within a fully shared network
structure, thereby decoupling the contradictory attributes of the two tasks at
a minimal cost. Furthermore, we propose a saliency-based sampling strategy
(SBSS) to sample the training set of the SOD task to balance the training set
sizes of the two tasks. In addition, SBSS improves the training set quality and
shortens the training time. Based on the proposed SCJoint and SBSS, we train a
powerful generalist network, named JoNet, which has the ability to
simultaneously capture both ``salient" and ``camouflaged". Extensive
experiments demonstrate the competitive performance and effectiveness of our
proposed method. The code is available at https://github.com/linuxsino/JoNet.

</details>


### [39] [Can Large Models Fool the Eye? A New Turing Test for Biological Animation](https://arxiv.org/abs/2508.06072)
*Zijian Chen,Lirong Deng,Zhengyu Chen,Kaiwei Zhang,Qi Jia,Yuan Tian,Yucheng Zhu,Guangtao Zhai*

Main category: cs.CV

TL;DR: BioMotion Arena introduces a visual animation framework to evaluate LLMs and MLLMs, using point-light imaging to highlight performance gaps. It collects 45k votes on 53 models, showing high agreement between crowd-sourced and expert ratings. Over 90% of models fail basic motion tasks, making it a challenging benchmark.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks lack intuitive feedback on model performance. BioMotion Arena addresses this by leveraging visual perception of motion patterns for clearer evaluation.

Method: Uses point-light source imaging and pairwise comparisons to evaluate 53 LLMs/MLLMs on 90 biological motion variants, collecting 45k human votes.

Result: High agreement between crowd-sourced and expert votes. Over 90% of models fail basic motion tasks, including top models like InternVL3 and Claude-4.

Conclusion: BioMotion Arena provides discriminative feedback and serves as a flexible, challenging benchmark for model evaluation without ground-truth restrictions.

Abstract: Evaluating the abilities of large models and manifesting their gaps are
challenging. Current benchmarks adopt either ground-truth-based score-form
evaluation on static datasets or indistinct textual chatbot-style human
preferences collection, which may not provide users with immediate, intuitive,
and perceptible feedback on performance differences. In this paper, we
introduce BioMotion Arena, a novel framework for evaluating large language
models (LLMs) and multimodal large language models (MLLMs) via visual
animation. Our methodology draws inspiration from the inherent visual
perception of motion patterns characteristic of living organisms that utilizes
point-light source imaging to amplify the performance discrepancies between
models. Specifically, we employ a pairwise comparison evaluation and collect
more than 45k votes for 53 mainstream LLMs and MLLMs on 90 biological motion
variants. Data analyses show that the crowd-sourced human votes are in good
agreement with those of expert raters, demonstrating the superiority of our
BioMotion Arena in offering discriminative feedback. We also find that over
90\% of evaluated models, including the cutting-edge open-source InternVL3 and
proprietary Claude-4 series, fail to produce fundamental humanoid point-light
groups, much less smooth and biologically plausible motions. This enables
BioMotion Arena to serve as a challenging benchmark for performance
visualization and a flexible evaluation framework without restrictions on
ground-truth.

</details>


### [40] [Towards MR-Based Trochleoplasty Planning](https://arxiv.org/abs/2508.06076)
*Michael Wehrli,Alicia Durrer,Paul Friedrich,Sidaty El Hadramy,Edwin Li,Luana Brahaj,Carol C. Hasler,Philippe C. Cattin*

Main category: cs.CV

TL;DR: A pipeline for generating super-resolved 3D pseudo-healthy target morphologies from clinical MR scans to improve Trochlear Dysplasia treatment, reducing reliance on CT scans and surgical intuition.


<details>
  <summary>Details</summary>
Motivation: Current TD treatments rely on low-resolution MR scans and surgeon experience, leading to inconsistent outcomes and limited adoption of minimally invasive techniques.

Method: Uses Implicit Neural Representation (INR) for super-resolution, a multi-label network for bone segmentation, and a Wavelet Diffusion Model (WDM) to generate pseudo-healthy 3D shapes.

Result: Significantly improves sulcus angle (SA) and trochlear groove depth (TGD) in 25 TD patients, with sub-millimeter resolution for pre-/intraoperative use.

Conclusion: The proposed pipeline offers a radiation-free, high-resolution alternative for TD treatment planning, improving surgical outcomes.

Abstract: To treat Trochlear Dysplasia (TD), current approaches rely mainly on
low-resolution clinical Magnetic Resonance (MR) scans and surgical intuition.
The surgeries are planned based on surgeons experience, have limited adoption
of minimally invasive techniques, and lead to inconsistent outcomes. We propose
a pipeline that generates super-resolved, patient-specific 3D pseudo-healthy
target morphologies from conventional clinical MR scans. First, we compute an
isotropic super-resolved MR volume using an Implicit Neural Representation
(INR). Next, we segment femur, tibia, patella, and fibula with a multi-label
custom-trained network. Finally, we train a Wavelet Diffusion Model (WDM) to
generate pseudo-healthy target morphologies of the trochlear region. In
contrast to prior work producing pseudo-healthy low-resolution 3D MR images,
our approach enables the generation of sub-millimeter resolved 3D shapes
compatible for pre- and intraoperative use. These can serve as preoperative
blueprints for reshaping the femoral groove while preserving the native patella
articulation. Furthermore, and in contrast to other work, we do not require a
CT for our pipeline - reducing the amount of radiation. We evaluated our
approach on 25 TD patients and could show that our target morphologies
significantly improve the sulcus angle (SA) and trochlear groove depth (TGD).
The code and interactive visualization are available at
https://wehrlimi.github.io/sr-3d-planning/.

</details>


### [41] [DreamVE: Unified Instruction-based Image and Video Editing](https://arxiv.org/abs/2508.06080)
*Bin Xia,Jiyang Liu,Yuechen Zhang,Bohao Peng,Ruihang Chu,Yitong Wang,Xinglong Wu,Bei Yu,Jiaya Jia*

Main category: cs.CV

TL;DR: DreamVE is a unified model for instruction-based image and video editing, trained in two stages (image then video) with synthetic data pipelines for scalability and flexibility.


<details>
  <summary>Details</summary>
Motivation: Instruction-based video editing is limited by scarce training data, hindering practical use. DreamVE addresses this by leveraging scalable image data and synthetic pipelines.

Method: Two-stage training (image then video), collage-based and generative model-based data synthesis, and a framework using a T2V model with token concatenation for consistency.

Result: DreamVE achieves strong performance in key editing types, with enhanced generalization and transfer capabilities, though collage-based data limits attribute editing.

Conclusion: DreamVE offers a scalable, flexible solution for instruction-based editing, combining synthetic data and efficient training strategies.

Abstract: Instruction-based editing holds vast potential due to its simple and
efficient interactive editing format. However, instruction-based editing,
particularly for video, has been constrained by limited training data,
hindering its practical application. To this end, we introduce DreamVE, a
unified model for instruction-based image and video editing. Specifically, We
propose a two-stage training strategy: first image editing, then video editing.
This offers two main benefits: (1) Image data scales more easily, and models
are more efficient to train, providing useful priors for faster and better
video editing training. (2) Unifying image and video generation is natural and
aligns with current trends. Moreover, we present comprehensive training data
synthesis pipelines, including collage-based and generative model-based data
synthesis. The collage-based data synthesis combines foreground objects and
backgrounds to generate diverse editing data, such as object manipulation,
background changes, and text modifications. It can easily generate billions of
accurate, consistent, realistic, and diverse editing pairs. We pretrain DreamVE
on extensive collage-based data to achieve strong performance in key editing
types and enhance generalization and transfer capabilities. However,
collage-based data lacks some attribute editing cases, leading to a relative
drop in performance. In contrast, the generative model-based pipeline, despite
being hard to scale up, offers flexibility in handling attribute editing cases.
Therefore, we use generative model-based data to further fine-tune DreamVE.
Besides, we design an efficient and powerful editing framework for DreamVE. We
build on the SOTA T2V model and use a token concatenation with early drop
approach to inject source image guidance, ensuring strong consistency and
editability. The codes and models will be released.

</details>


### [42] [SwiftVideo: A Unified Framework for Few-Step Video Generation through Trajectory-Distribution Alignment](https://arxiv.org/abs/2508.06082)
*Yanxiao Sun,Jiafu Wu,Yun Cao,Chengming Xu,Yabiao Wang,Weijian Cao,Donghao Luo,Chengjie Wang,Yanwei Fu*

Main category: cs.CV

TL;DR: SwiftVideo is a unified distillation framework combining trajectory-preserving and distribution-matching to accelerate video synthesis while maintaining quality.


<details>
  <summary>Details</summary>
Motivation: Existing distillation methods for video synthesis suffer from performance breakdown or artifacts under few-step settings.

Method: Combines continuous-time consistency distillation for ODE trajectory preservation and dual-perspective alignment (distribution and trajectory alignment).

Result: Outperforms existing methods in few-step video generation on the OpenVid-1M benchmark.

Conclusion: SwiftVideo effectively reduces inference steps while preserving high-quality video generation.

Abstract: Diffusion-based or flow-based models have achieved significant progress in
video synthesis but require multiple iterative sampling steps, which incurs
substantial computational overhead. While many distillation methods that are
solely based on trajectory-preserving or distribution-matching have been
developed to accelerate video generation models, these approaches often suffer
from performance breakdown or increased artifacts under few-step settings. To
address these limitations, we propose \textbf{\emph{SwiftVideo}}, a unified and
stable distillation framework that combines the advantages of
trajectory-preserving and distribution-matching strategies. Our approach
introduces continuous-time consistency distillation to ensure precise
preservation of ODE trajectories. Subsequently, we propose a dual-perspective
alignment that includes distribution alignment between synthetic and real data
along with trajectory alignment across different inference steps. Our method
maintains high-quality video generation while substantially reducing the number
of inference steps. Quantitative evaluations on the OpenVid-1M benchmark
demonstrate that our method significantly outperforms existing approaches in
few-step video generation.

</details>


### [43] [AdaptInfer: Adaptive Token Pruning for Vision-Language Model Inference with Dynamical Text Guidance](https://arxiv.org/abs/2508.06084)
*Weichen Zhang,Zhui Zhu,Ningbo Li,Kebin Liu,Yunhao Liu*

Main category: cs.CV

TL;DR: AdaptInfer is a plug-and-play framework for adaptive vision token pruning in VLMs, reducing CUDA latency by 61.3% while maintaining high accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing pruning methods for VLMs fail to exploit dynamic internal signals during inference, leading to inefficiencies.

Method: Introduces a dynamic text-guided pruning mechanism and an offline analysis of cross-modal attention shifts to create an efficient pruning schedule.

Result: Reduces CUDA latency by 61.3% and maintains 92.9% accuracy on LLaVA-1.5-7B, outperforming SOTA under the same token budget.

Conclusion: AdaptInfer is a lightweight, effective, and generalizable solution for improving VLM efficiency.

Abstract: Vision-language models (VLMs) have achieved impressive performance on
multimodal reasoning tasks such as visual question answering (VQA), but their
inference cost remains a significant challenge due to the large number of
vision tokens processed during the prefill stage. Existing pruning methods
often rely on directly using the attention patterns or static text prompt
guidance, failing to exploit the dynamic internal signals generated during
inference. To address these issues, we propose AdaptInfer, a plug-and-play
framework for adaptive vision token pruning in VLMs. First, we introduce a
fine-grained, dynamic text-guided pruning mechanism that reuses layer-wise
text-to-text attention maps to construct soft priors over text-token
importance, allowing more informed scoring of vision tokens at each stage.
Second, we perform an offline analysis of cross-modal attention shifts and
identify consistent inflection locations in inference, which inspire us to
propose a more principled and efficient pruning schedule. Our method is
lightweight and plug-and-play, also generalizable across multi-modal tasks.
Experimental results have verified the effectiveness of the proposed method.
For example, it reduces CUDA latency by 61.3\% while maintaining an average
accuracy of 92.9\% on vanilla LLaVA-1.5-7B. Under the same token budget,
AdaptInfer surpasses SOTA in accuracy.

</details>


### [44] [Q-CLIP: Unleashing the Power of Vision-Language Models for Video Quality Assessment through Unified Cross-Modal Adaptation](https://arxiv.org/abs/2508.06092)
*Yachun Mi,Yu Li,Yanting Li,Shixin Sun,Chen Hui,Tong Zhang,Yuanyuan Liu,Chenyue Song,Shaohui Liu*

Main category: cs.CV

TL;DR: Q-CLIP is a Vision-Language Model (VLM)-based framework for Video Quality Assessment (VQA) that reduces computational costs and improves sensitivity to video quality through a Shared Cross-Modal Adapter and learnable prompts.


<details>
  <summary>Details</summary>
Motivation: Current VQA methods rely on pretraining on large datasets, which is computationally expensive and insufficient for capturing video quality factors like distortion and motion. VLMs offer potential for better generalization.

Method: Q-CLIP uses a Shared Cross-Modal Adapter (SCMA) with minimal trainable parameters and introduces learnable quality-level prompts. It also explores frame-difference-based sampling for better generalization.

Result: Q-CLIP shows excellent performance on multiple VQA datasets, demonstrating efficiency and effectiveness.

Conclusion: Q-CLIP addresses the limitations of traditional VQA methods by leveraging VLMs, reducing computational costs, and enhancing quality sensitivity, making it a promising solution for VQA.

Abstract: Accurate and efficient Video Quality Assessment (VQA) has long been a key
research challenge. Current mainstream VQA methods typically improve
performance by pretraining on large-scale classification datasets (e.g.,
ImageNet, Kinetics-400), followed by fine-tuning on VQA datasets. However, this
strategy presents two significant challenges: (1) merely transferring semantic
knowledge learned from pretraining is insufficient for VQA, as video quality
depends on multiple factors (e.g., semantics, distortion, motion, aesthetics);
(2) pretraining on large-scale datasets demands enormous computational
resources, often dozens or even hundreds of times greater than training
directly on VQA datasets. Recently, Vision-Language Models (VLMs) have shown
remarkable generalization capabilities across a wide range of visual tasks, and
have begun to demonstrate promising potential in quality assessment. In this
work, we propose Q-CLIP, the first fully VLMs-based framework for VQA. Q-CLIP
enhances both visual and textual representations through a Shared Cross-Modal
Adapter (SCMA), which contains only a minimal number of trainable parameters
and is the only component that requires training. This design significantly
reduces computational cost. In addition, we introduce a set of five learnable
quality-level prompts to guide the VLMs in perceiving subtle quality
variations, thereby further enhancing the model's sensitivity to video quality.
Furthermore, we investigate the impact of different frame sampling strategies
on VQA performance, and find that frame-difference-based sampling leads to
better generalization performance across datasets. Extensive experiments
demonstrate that Q-CLIP exhibits excellent performance on several VQA datasets.

</details>


### [45] [E-React: Towards Emotionally Controlled Synthesis of Human Reactions](https://arxiv.org/abs/2508.06093)
*Chen Zhu,Buzhen Huang,Zijing Wu,Binghui Zuo,Yangang Wang*

Main category: cs.CV

TL;DR: The paper introduces a method for generating diverse human reaction motions based on emotional cues, addressing the gap in existing frameworks that ignore emotions.


<details>
  <summary>Details</summary>
Motivation: Existing motion generation frameworks lack emotional consideration, reducing naturalness and limiting applications like human reaction synthesis.

Method: A semi-supervised emotion prior is integrated into an actor-reactor diffusion model to generate reactions by combining spatial interaction and emotional response.

Result: The model outperforms existing reaction generation methods, producing realistic reactions under various emotional conditions.

Conclusion: The proposed approach successfully incorporates emotion into motion generation, enhancing naturalness and applicability in interactive tasks.

Abstract: Emotion serves as an essential component in daily human interactions.
Existing human motion generation frameworks do not consider the impact of
emotions, which reduces naturalness and limits their application in interactive
tasks, such as human reaction synthesis. In this work, we introduce a novel
task: generating diverse reaction motions in response to different emotional
cues. However, learning emotion representation from limited motion data and
incorporating it into a motion generation framework remains a challenging
problem. To address the above obstacles, we introduce a semi-supervised emotion
prior in an actor-reactor diffusion model to facilitate emotion-driven reaction
synthesis. Specifically, based on the observation that motion clips within a
short sequence tend to share the same emotion, we first devise a
semi-supervised learning framework to train an emotion prior. With this prior,
we further train an actor-reactor diffusion model to generate reactions by
considering both spatial interaction and emotional response. Finally, given a
motion sequence of an actor, our approach can generate realistic reactions
under various emotional conditions. Experimental results demonstrate that our
model outperforms existing reaction generation methods. The code and data will
be made publicly available at https://ereact.github.io/

</details>


### [46] [UGD-IML: A Unified Generative Diffusion-based Framework for Constrained and Unconstrained Image Manipulation Localization](https://arxiv.org/abs/2508.06101)
*Yachun Mi,Xingyang He,Shixin Sun,Yu Li,Yanting Li,Zhixuan Li,Jian Jin,Chen Hui,Shaohui Liu*

Main category: cs.CV

TL;DR: The paper introduces UGD-IML, a generative framework using diffusion models for image manipulation localization, unifying IML and CIML tasks efficiently without large labeled datasets.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of existing IML methods that rely on large annotated datasets and complex pipelines, the paper aims to improve efficiency and performance in forgery detection.

Method: Proposes UGD-IML, a diffusion model-based framework that unifies IML and CIML tasks, leveraging class embedding and parameter-sharing for seamless mode switching.

Result: UGD-IML outperforms SOTA methods by 9.66 and 4.36 F1 metrics for IML and CIML, respectively, and excels in uncertainty estimation and robustness.

Conclusion: The framework offers a scalable, efficient solution for image manipulation localization, reducing dependency on labeled data and complex pipelines.

Abstract: In the digital age, advanced image editing tools pose a serious threat to the
integrity of visual content, making image forgery detection and localization a
key research focus. Most existing Image Manipulation Localization (IML) methods
rely on discriminative learning and require large, high-quality annotated
datasets. However, current datasets lack sufficient scale and diversity,
limiting model performance in real-world scenarios. To overcome this, recent
studies have explored Constrained IML (CIML), which generates pixel-level
annotations through algorithmic supervision. However, existing CIML approaches
often depend on complex multi-stage pipelines, making the annotation process
inefficient. In this work, we propose a novel generative framework based on
diffusion models, named UGD-IML, which for the first time unifies both IML and
CIML tasks within a single framework. By learning the underlying data
distribution, generative diffusion models inherently reduce the reliance on
large-scale labeled datasets, allowing our approach to perform effectively even
under limited data conditions. In addition, by leveraging a class embedding
mechanism and a parameter-sharing design, our model seamlessly switches between
IML and CIML modes without extra components or training overhead. Furthermore,
the end-to-end design enables our model to avoid cumbersome steps in the data
annotation process. Extensive experimental results on multiple datasets
demonstrate that UGD-IML outperforms the SOTA methods by an average of 9.66 and
4.36 in terms of F1 metrics for IML and CIML tasks, respectively. Moreover, the
proposed method also excels in uncertainty estimation, visualization and
robustness.

</details>


### [47] [MCA: 2D-3D Retrieval with Noisy Labels via Multi-level Adaptive Correction and Alignment](https://arxiv.org/abs/2508.06104)
*Gui Zou,Chaofan Gan,Chern Hong Lim,Supavadee Aramvith,Weiyao Lin*

Main category: cs.CV

TL;DR: The paper proposes MCA, a robust framework for 2D-3D cross-modal retrieval, addressing noisy labels through multimodal joint label correction and multi-level adaptive alignment.


<details>
  <summary>Details</summary>
Motivation: Imperfect annotations in 2D-3D data pose challenges for cross-modal retrieval, requiring robust solutions to handle noisy labels.

Method: MCA includes a Multimodal Joint label Correction (MJC) mechanism for label refinement and a Multi-level Adaptive Alignment (MAA) strategy for feature enhancement.

Result: MCA achieves state-of-the-art performance on conventional and noisy 3D benchmarks.

Conclusion: MCA is effective and generalizable for robust 2D-3D cross-modal retrieval under noisy label conditions.

Abstract: With the increasing availability of 2D and 3D data, significant advancements
have been made in the field of cross-modal retrieval. Nevertheless, the
existence of imperfect annotations presents considerable challenges, demanding
robust solutions for 2D-3D cross-modal retrieval in the presence of noisy label
conditions. Existing methods generally address the issue of noise by dividing
samples independently within each modality, making them susceptible to
overfitting on corrupted labels. To address these issues, we propose a robust
2D-3D \textbf{M}ulti-level cross-modal adaptive \textbf{C}orrection and
\textbf{A}lignment framework (MCA). Specifically, we introduce a Multimodal
Joint label Correction (MJC) mechanism that leverages multimodal historical
self-predictions to jointly model the modality prediction consistency, enabling
reliable label refinement. Additionally, we propose a Multi-level Adaptive
Alignment (MAA) strategy to effectively enhance cross-modal feature semantics
and discrimination across different levels. Extensive experiments demonstrate
the superiority of our method, MCA, which achieves state-of-the-art performance
on both conventional and realistic noisy 3D benchmarks, highlighting its
generality and effectiveness.

</details>


### [48] [Mask & Match: Learning to Recognize Handwritten Math with Self-Supervised Attention](https://arxiv.org/abs/2508.06107)
*Shree Mitra,Ritabrata Chakraborty,Nilkanta Sahu*

Main category: cs.CV

TL;DR: A self-supervised learning framework for handwritten mathematical expression recognition (HMER) using contrastive loss and progressive spatial masking, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: HMER is challenging due to 2D structure and complex symbol relationships; labeled data is expensive.

Method: Self-supervised pretraining with contrastive loss, novel attention network with progressive masking, and supervised fine-tuning.

Result: Outperforms SSL and supervised baselines on CROHME benchmarks.

Conclusion: Progressive attention mechanism enhances HMER performance without supervision.

Abstract: Recognizing handwritten mathematical expressions (HMER) is a challenging task
due to the inherent two-dimensional structure, varying symbol scales, and
complex spatial relationships among symbols. In this paper, we present a
self-supervised learning (SSL) framework for HMER that eliminates the need for
expensive labeled data. Our approach begins by pretraining an image encoder
using a combination of global and local contrastive loss, enabling the model to
learn both holistic and fine-grained representations. A key contribution of
this work is a novel self-supervised attention network, which is trained using
a progressive spatial masking strategy. This attention mechanism is designed to
learn semantically meaningful focus regions, such as operators, exponents, and
nested mathematical notation, without requiring any supervision. The
progressive masking curriculum encourages the network to become increasingly
robust to missing or occluded visual information, ultimately improving
structural understanding. Our complete pipeline consists of (1) self-supervised
pretraining of the encoder, (2) self-supervised attention learning, and (3)
supervised fine-tuning with a transformer decoder to generate LATEX sequences.
Extensive experiments on CROHME benchmarks demonstrate that our method
outperforms existing SSL and fully supervised baselines, validating the
effectiveness of our progressive attention mechanism in enhancing HMER
performance. Our codebase can be found here.

</details>


### [49] [FMCE-Net++: Feature Map Convergence Evaluation and Training](https://arxiv.org/abs/2508.06109)
*Zhibo Zhu,Renyu Huang,Lei He*

Main category: cs.CV

TL;DR: FMCE-Net++ is a training framework integrating FMCE-Net to improve DNN interpretability and performance without architectural changes.


<details>
  <summary>Details</summary>
Motivation: Address the lack of experimental validation and closed-loop integration in FMCE for DNN interpretability.

Method: Proposes FMCE-Net++, using a pretrained FMCE-Net as an auxiliary head with Representation Auxiliary Loss for joint supervision.

Result: Achieves accuracy gains (e.g., +1.16 pp on ResNet-50/CIFAR-10) across multiple datasets.

Conclusion: FMCE-Net++ effectively enhances model performance and interpretability.

Abstract: Deep Neural Networks (DNNs) face interpretability challenges due to their
opaque internal representations. While Feature Map Convergence Evaluation
(FMCE) quantifies module-level convergence via Feature Map Convergence Scores
(FMCS), it lacks experimental validation and closed-loop integration. To
address this limitation, we propose FMCE-Net++, a novel training framework that
integrates a pretrained, frozen FMCE-Net as an auxiliary head. This module
generates FMCS predictions, which, combined with task labels, jointly supervise
backbone optimization through a Representation Auxiliary Loss. The RAL
dynamically balances the primary classification loss and feature convergence
optimization via a tunable \Representation Abstraction Factor. Extensive
experiments conducted on MNIST, CIFAR-10, FashionMNIST, and CIFAR-100
demonstrate that FMCE-Net++ consistently enhances model performance without
architectural modifications or additional data. Key experimental outcomes
include accuracy gains of $+1.16$ pp (ResNet-50/CIFAR-10) and $+1.08$ pp
(ShuffleNet v2/CIFAR-100), validating that FMCE-Net++ can effectively elevate
state-of-the-art performance ceilings.

</details>


### [50] [GMF-Drive: Gated Mamba Fusion with Spatial-Aware BEV Representation for End-to-End Autonomous Driving](https://arxiv.org/abs/2508.06113)
*Jian Wang,Chaokang Jiang,Haitao Xu*

Main category: cs.CV

TL;DR: GMF-Drive introduces a gated Mamba fusion framework for autonomous driving, replacing transformers with efficient state-space models and improving LiDAR representation.


<details>
  <summary>Details</summary>
Motivation: Overcoming limitations of transformer-based fusion in diffusion models for autonomous driving, such as high computational complexity and lack of spatial priors.

Method: Uses a geometrically-augmented pillar LiDAR representation and a hierarchical gated Mamba fusion (GM-Fusion) architecture with state-space models (SSMs).

Result: Achieves state-of-the-art performance on NAVSIM benchmark, outperforming DiffusionDrive.

Conclusion: Task-specific SSMs can surpass transformers in performance and efficiency for autonomous driving.

Abstract: Diffusion-based models are redefining the state-of-the-art in end-to-end
autonomous driving, yet their performance is increasingly hampered by a
reliance on transformer-based fusion. These architectures face fundamental
limitations: quadratic computational complexity restricts the use of
high-resolution features, and a lack of spatial priors prevents them from
effectively modeling the inherent structure of Bird's Eye View (BEV)
representations. This paper introduces GMF-Drive (Gated Mamba Fusion for
Driving), an end-to-end framework that overcomes these challenges through two
principled innovations. First, we supersede the information-limited
histogram-based LiDAR representation with a geometrically-augmented pillar
format encoding shape descriptors and statistical features, preserving critical
3D geometric details. Second, we propose a novel hierarchical gated mamba
fusion (GM-Fusion) architecture that substitutes an expensive transformer with
a highly efficient, spatially-aware state-space model (SSM). Our core BEV-SSM
leverages directional sequencing and adaptive fusion mechanisms to capture
long-range dependencies with linear complexity, while explicitly respecting the
unique spatial properties of the driving scene. Extensive experiments on the
challenging NAVSIM benchmark demonstrate that GMF-Drive achieves a new
state-of-the-art performance, significantly outperforming DiffusionDrive.
Comprehensive ablation studies validate the efficacy of each component,
demonstrating that task-specific SSMs can surpass a general-purpose transformer
in both performance and efficiency for autonomous driving.

</details>


### [51] [SynSeg: Feature Synergy for Multi-Category Contrastive Learning in Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2508.06115)
*Weichen Zhang,Kebin Liu,Fan Dang,Zhui Zhu,Xikai Sun,Yunhao Liu*

Main category: cs.CV

TL;DR: SynSeg introduces Multi-Category Contrastive Learning (MCCL) and Feature Synergy Structure (FSS) to improve weakly-supervised semantic segmentation, outperforming SOTA methods.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in open-vocabulary semantic segmentation, such as semantic misalignment and poor performance in weakly-supervised settings.

Method: Uses MCCL for robust intra- and inter-category alignment and FSS for discriminative feature reconstruction.

Result: Achieves higher accuracy than SOTA baselines (e.g., 4.5% on VOC, 8.9% on Context).

Conclusion: SynSeg enhances semantic localization and discrimination under weak supervision, demonstrating superior performance.

Abstract: Semantic segmentation in open-vocabulary scenarios presents significant
challenges due to the wide range and granularity of semantic categories.
Existing weakly-supervised methods often rely on category-specific supervision
and ill-suited feature construction methods for contrastive learning, leading
to semantic misalignment and poor performance. In this work, we propose a novel
weakly-supervised approach, SynSeg, to address the challenges. SynSeg performs
Multi-Category Contrastive Learning (MCCL) as a stronger training signal with a
new feature reconstruction framework named Feature Synergy Structure (FSS).
Specifically, MCCL strategy robustly combines both intra- and inter-category
alignment and separation in order to make the model learn the knowledge of
correlations from different categories within the same image. Moreover, FSS
reconstructs discriminative features for contrastive learning through prior
fusion and semantic-activation-map enhancement, effectively avoiding the
foreground bias introduced by the visual encoder. In general, SynSeg
effectively improves the abilities in semantic localization and discrimination
under weak supervision. Extensive experiments on benchmarks demonstrate that
our method outperforms state-of-the-art (SOTA) performance. For instance,
SynSeg achieves higher accuracy than SOTA baselines by 4.5\% on VOC, 8.9\% on
Context, 2.6\% on Object and 2.0\% on City.

</details>


### [52] [Learning Representations of Satellite Images with Evaluations on Synoptic Weather Events](https://arxiv.org/abs/2508.06122)
*Ting-Shuo Yo,Shih-Hao Su,Chien-Ming Wu,Wei-Ting Chen,Jung-Lien Chu,Chiao-Wei Chang,Hung-Chi Kuo*

Main category: cs.CV

TL;DR: The study compared PCA, CAE, and PT for weather event classification using satellite images, finding CAE superior in performance but lacking physical interpretability.


<details>
  <summary>Details</summary>
Motivation: To evaluate the effectiveness of different representation learning algorithms (PCA, CAE, PT) for classifying weather events from satellite images.

Method: Applied PCA, CAE, and PT to satellite images, evaluated their latent spaces for weather event classification, and tested impact of resolution and latent space size.

Result: CAE outperformed PCA and PT in classification tasks, PT excelled in tropical cyclones, higher-resolution data improved deep learning performance, and smaller latent spaces increased false alarms.

Conclusion: CAE is effective but lacks physical interpretability; future work could integrate physics into CAE for better attribution.

Abstract: This study applied representation learning algorithms to satellite images and
evaluated the learned latent spaces with classifications of various weather
events. The algorithms investigated include the classical linear
transformation, i.e., principal component analysis (PCA), state-of-the-art deep
learning method, i.e., convolutional autoencoder (CAE), and a residual network
pre-trained with large image datasets (PT). The experiment results indicated
that the latent space learned by CAE consistently showed higher threat scores
for all classification tasks. The classifications with PCA yielded high hit
rates but also high false-alarm rates. In addition, the PT performed
exceptionally well at recognizing tropical cyclones but was inferior in other
tasks. Further experiments suggested that representations learned from
higher-resolution datasets are superior in all classification tasks for
deep-learning algorithms, i.e., CAE and PT. We also found that smaller latent
space sizes had minor impact on the classification task's hit rate. Still, a
latent space dimension smaller than 128 caused a significantly higher false
alarm rate. Though the CAE can learn latent spaces effectively and efficiently,
the interpretation of the learned representation lacks direct connections to
physical attributions. Therefore, developing a physics-informed version of CAE
can be a promising outlook for the current work.

</details>


### [53] [SC-Captioner: Improving Image Captioning with Self-Correction by Reinforcement Learning](https://arxiv.org/abs/2508.06125)
*Lin Zhang,Xianfang Zeng,Kangcong Li,Gang Yu,Tao Chen*

Main category: cs.CV

TL;DR: SC-Captioner is a reinforcement learning framework for self-correcting image caption models, using a novel reward function and refined metrics, validated on the RefinedCaps dataset.


<details>
  <summary>Details</summary>
Motivation: Improving image captioning by enabling models to self-correct errors and refine captions for accuracy.

Method: Decomposes captions into object, attribute, and relation sets via scene-graph parsing, calculates set differences for reward design, and introduces refined metrics for evaluation.

Result: Outperforms direct preference optimization, generating better captions across scenarios.

Conclusion: SC-Captioner effectively enhances caption quality through self-correction and refined evaluation.

Abstract: We propose SC-Captioner, a reinforcement learning framework that enables the
self-correcting capability of image caption models. Our crucial technique lies
in the design of the reward function to incentivize accurate caption
corrections. Specifically, the predicted and reference captions are decomposed
into object, attribute, and relation sets using scene-graph parsing algorithms.
We calculate the set difference between sets of initial and self-corrected
captions to identify added and removed elements. These elements are matched
against the reference sets to calculate correctness bonuses for accurate
refinements and mistake punishments for wrong additions and removals, thereby
forming the final reward. For image caption quality assessment, we propose a
set of metrics refined from CAPTURE that alleviate its incomplete precision
evaluation and inefficient relation matching problems. Furthermore, we collect
a fine-grained annotated image caption dataset, RefinedCaps, consisting of 6.5K
diverse images from COCO dataset. Experiments show that applying SC-Captioner
on large visual-language models can generate better image captions across
various scenarios, significantly outperforming the direct preference
optimization training strategy.

</details>


### [54] [SAM Encoder Breach by Adversarial Simplicial Complex Triggers Downstream Model Failures](https://arxiv.org/abs/2508.06127)
*Yi Qin,Rui Wang,Tao Huang,Tong Xiao,Liping Jing*

Main category: cs.CV

TL;DR: VeSCA is a novel adversarial attack method targeting SAM's vulnerabilities, improving transferability by 12.7% over state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: SAM's vulnerabilities pose risks to downstream applications, necessitating proactive evaluation of transferable weaknesses.

Method: VeSCA leverages SAM's encoder to generate adversarial examples by identifying shared vulnerable regions via a parametric simplicial complex, refined iteratively.

Result: VeSCA outperforms existing methods by 12.7% across three downstream model categories and five datasets.

Conclusion: SAM's vulnerabilities highlight the need for more robust foundation models.

Abstract: While the Segment Anything Model (SAM) transforms interactive segmentation
with zero-shot abilities, its inherent vulnerabilities present a single-point
risk, potentially leading to the failure of numerous downstream applications.
Proactively evaluating these transferable vulnerabilities is thus imperative.
Prior adversarial attacks on SAM often present limited transferability due to
insufficient exploration of common weakness across domains. To address this, we
propose Vertex-Refining Simplicial Complex Attack (VeSCA), a novel method that
leverages only the encoder of SAM for generating transferable adversarial
examples. Specifically, it achieves this by explicitly characterizing the
shared vulnerable regions between SAM and downstream models through a
parametric simplicial complex. Our goal is to identify such complexes within
adversarially potent regions by iterative vertex-wise refinement. A lightweight
domain re-adaptation strategy is introduced to bridge domain divergence using
minimal reference data during the initialization of simplicial complex.
Ultimately, VeSCA generates consistently transferable adversarial examples
through random simplicial complex sampling. Extensive experiments demonstrate
that VeSCA achieves performance improved by 12.7% compared to state-of-the-art
methods across three downstream model categories across five domain-specific
datasets. Our findings further highlight the downstream model risks posed by
SAM's vulnerabilities and emphasize the urgency of developing more robust
foundation models.

</details>


### [55] [Roll Your Eyes: Gaze Redirection via Explicit 3D Eyeball Rotation](https://arxiv.org/abs/2508.06136)
*YoungChan Choi,HengFei Wang,YiHua Cheng,Boeun Kim,Hyung Jin Chang,YoungGeun Choi,Sang-Il Choi*

Main category: cs.CV

TL;DR: A novel 3D gaze redirection framework using explicit 3D eyeball structure and 3D Gaussian Splatting outperforms NeRF-based methods in image quality and gaze accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing gaze redirection methods rely on implicit neural representations (NeRF), lacking explicit modeling of eyeball rotation and translation.

Method: Introduces a dedicated 3D eyeball structure with 3D Gaussian Splatting (3DGS) and an adaptive deformation module for muscle movements.

Result: Achieves superior image quality and gaze estimation accuracy on the ETH-XGaze dataset.

Conclusion: The framework effectively generates photorealistic gaze images with explicit control over eyeball movement.

Abstract: We propose a novel 3D gaze redirection framework that leverages an explicit
3D eyeball structure. Existing gaze redirection methods are typically based on
neural radiance fields, which employ implicit neural representations via volume
rendering. Unlike these NeRF-based approaches, where the rotation and
translation of 3D representations are not explicitly modeled, we introduce a
dedicated 3D eyeball structure to represent the eyeballs with 3D Gaussian
Splatting (3DGS). Our method generates photorealistic images that faithfully
reproduce the desired gaze direction by explicitly rotating and translating the
3D eyeball structure. In addition, we propose an adaptive deformation module
that enables the replication of subtle muscle movements around the eyes.
Through experiments conducted on the ETH-XGaze dataset, we demonstrate that our
framework is capable of generating diverse novel gaze images, achieving
superior image quality and gaze estimation accuracy compared to previous
state-of-the-art methods.

</details>


### [56] [DiffCap: Diffusion-based Real-time Human Motion Capture using Sparse IMUs and a Monocular Camera](https://arxiv.org/abs/2508.06139)
*Shaohua Pan,Xinyu Yi,Yan Zhou,Weihua Jian,Yuan Zhang,Pengfei Wan,Feng Xu*

Main category: cs.CV

TL;DR: A diffusion-based method combines sparse IMUs and a monocular camera for real-time human motion capture, leveraging sequential visual features and frame-wise IMU data for robustness and accuracy.


<details>
  <summary>Details</summary>
Motivation: To address challenges in human motion capture, such as occlusions or camera view limitations, by fusing sparse IMUs and monocular camera data in a unified framework.

Method: Uses a diffusion model to integrate sequential visual information (as a condition embedding) and frame-wise IMU measurements (concatenated with noisy body poses).

Result: Demonstrates robustness to visual degenerations and achieves state-of-the-art performance in pose estimation.

Conclusion: The proposed framework effectively combines IMUs and camera data, offering a robust solution for real-time motion capture.

Abstract: Combining sparse IMUs and a monocular camera is a new promising setting to
perform real-time human motion capture. This paper proposes a diffusion-based
solution to learn human motion priors and fuse the two modalities of signals
together seamlessly in a unified framework. By delicately considering the
characteristics of the two signals, the sequential visual information is
considered as a whole and transformed into a condition embedding, while the
inertial measurement is concatenated with the noisy body pose frame by frame to
construct a sequential input for the diffusion model. Firstly, we observe that
the visual information may be unavailable in some frames due to occlusions or
subjects moving out of the camera view. Thus incorporating the sequential
visual features as a whole to get a single feature embedding is robust to the
occasional degenerations of visual information in those frames. On the other
hand, the IMU measurements are robust to occlusions and always stable when
signal transmission has no problem. So incorporating them frame-wisely could
better explore the temporal information for the system. Experiments have
demonstrated the effectiveness of the system design and its state-of-the-art
performance in pose estimation compared with the previous works. Our codes are
available for research at https://shaohua-pan.github.io/diffcap-page.

</details>


### [57] [SDEval: Safety Dynamic Evaluation for Multimodal Large Language Models](https://arxiv.org/abs/2508.06142)
*Hanqing Wang,Yuan Tian,Mingyu Liu,Zhenhao Zhang,Xiangyang Zhu*

Main category: cs.CV

TL;DR: SDEval is a dynamic evaluation framework for assessing safety in Multimodal Large Language Models (MLLMs) by adjusting benchmark distribution and complexity, mitigating data contamination and exposing safety limitations.


<details>
  <summary>Details</summary>
Motivation: Address outdated safety datasets and data contamination issues in MLLMs by proposing a dynamic evaluation framework.

Method: Uses text, image, and text-image dynamics to generate new samples from benchmarks, exploring their individual and combined effects on model safety.

Result: SDEval significantly impacts safety evaluation, mitigates data contamination, and reveals MLLM safety limitations across benchmarks.

Conclusion: SDEval is a versatile framework for dynamic safety evaluation, applicable to various benchmarks, enhancing safety assessment in MLLMs.

Abstract: In the rapidly evolving landscape of Multimodal Large Language Models
(MLLMs), the safety concerns of their outputs have earned significant
attention. Although numerous datasets have been proposed, they may become
outdated with MLLM advancements and are susceptible to data contamination
issues. To address these problems, we propose \textbf{SDEval}, the
\textit{first} safety dynamic evaluation framework to controllably adjust the
distribution and complexity of safety benchmarks. Specifically, SDEval mainly
adopts three dynamic strategies: text, image, and text-image dynamics to
generate new samples from original benchmarks. We first explore the individual
effects of text and image dynamics on model safety. Then, we find that
injecting text dynamics into images can further impact safety, and conversely,
injecting image dynamics into text also leads to safety risks. SDEval is
general enough to be applied to various existing safety and even capability
benchmarks. Experiments across safety benchmarks, MLLMGuard and VLSBench, and
capability benchmarks, MMBench and MMVet, show that SDEval significantly
influences safety evaluation, mitigates data contamination, and exposes safety
limitations of MLLMs. Code is available at https://github.com/hq-King/SDEval

</details>


### [58] [Text-guided Visual Prompt DINO for Generic Segmentation](https://arxiv.org/abs/2508.06146)
*Yuchen Guan,Chong Sun,Canmiao Fu,Zhipeng Huang,Chun Yuan,Chen Li*

Main category: cs.CV

TL;DR: Prompt-DINO introduces early fusion, order-aligned query selection, and a generative data engine to improve multimodal vision models for open-world segmentation.


<details>
  <summary>Details</summary>
Motivation: Address limitations in late-stage feature fusion, suboptimal query selection, and caption-derived vocabulary constraints in multimodal vision models.

Method: Early fusion mechanism, order-aligned query selection for DETR-based architectures, and a generative data engine using the RAP model.

Result: Achieves state-of-the-art performance on open-world detection benchmarks, expands semantic coverage, and reduces label noise by 80.5%.

Conclusion: Establishes a new paradigm for scalable multimodal detection and data generation in open-world scenarios.

Abstract: Recent advancements in multimodal vision models have highlighted limitations
in late-stage feature fusion and suboptimal query selection for hybrid prompts
open-world segmentation, alongside constraints from caption-derived
vocabularies. To address these challenges, we propose Prompt-DINO, a
text-guided visual Prompt DINO framework featuring three key innovations.
First, we introduce an early fusion mechanism that unifies text/visual prompts
and backbone features at the initial encoding stage, enabling deeper
cross-modal interactions to resolve semantic ambiguities. Second, we design
order-aligned query selection for DETR-based architectures, explicitly
optimizing the structural alignment between text and visual queries during
decoding to enhance semantic-spatial consistency. Third, we develop a
generative data engine powered by the Recognize Anything via Prompting (RAP)
model, which synthesizes 0.5B diverse training instances through a dual-path
cross-verification pipeline, reducing label noise by 80.5% compared to
conventional approaches. Extensive experiments demonstrate that Prompt-DINO
achieves state-of-the-art performance on open-world detection benchmarks while
significantly expanding semantic coverage beyond fixed-vocabulary constraints.
Our work establishes a new paradigm for scalable multimodal detection and data
generation in open-world scenarios. Data&Code are available at
https://github.com/WeChatCV/WeVisionOne.

</details>


### [59] [DSConv: Dynamic Splitting Convolution for Pansharpening](https://arxiv.org/abs/2508.06147)
*Xuanyu Liu,Bonan An*

Main category: cs.CV

TL;DR: The paper introduces DSConv, a novel method for pansharpening that dynamically splits convolution kernels and uses attention to enhance feature extraction, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Pansharpening is challenging, and existing methods rely on standard convolutions, missing the potential of adaptive convolutions for better inter-pixel correlation handling.

Method: Proposes DSConv, which dynamically splits convolution kernels with attention, improving feature extraction and network capabilities.

Result: DSConv achieves superior performance in pansharpening, validated by comprehensive experiments.

Conclusion: DSConv is effective, generalizable, and sets a new benchmark for pansharpening tasks.

Abstract: Aiming to obtain a high-resolution image, pansharpening involves the fusion
of a multi-spectral image (MS) and a panchromatic image (PAN), the low-level
vision task remaining significant and challenging in contemporary research.
Most existing approaches rely predominantly on standard convolutions, few
making the effort to adaptive convolutions, which are effective owing to the
inter-pixel correlations of remote sensing images. In this paper, we propose a
novel strategy for dynamically splitting convolution kernels in conjunction
with attention, selecting positions of interest, and splitting the original
convolution kernel into multiple smaller kernels, named DSConv. The proposed
DSConv more effectively extracts features of different positions within the
receptive field, enhancing the network's generalization, optimization, and
feature representation capabilities. Furthermore, we innovate and enrich
concepts of dynamic splitting convolution and provide a novel network
architecture for pansharpening capable of achieving the tasks more efficiently,
building upon this methodology. Adequate fair experiments illustrate the
effectiveness and the state-of-the-art performance attained by
DSConv.Comprehensive and rigorous discussions proved the superiority and
optimal usage conditions of DSConv.

</details>


### [60] [VISTAR:A User-Centric and Role-Driven Benchmark for Text-to-Image Evaluation](https://arxiv.org/abs/2508.06152)
*Kaiyuan Jiang,Ruoxi Sun,Ying Cao,Yuqi Xu,Xinran Zhang,Junyan Guo,ChengSheng Deng*

Main category: cs.CV

TL;DR: VISTAR is a multi-dimensional benchmark for text-to-image evaluation, combining deterministic metrics and a novel HWPQ scheme for abstract semantics, achieving high human alignment and outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Existing metrics for text-to-image evaluation are limited; VISTAR aims to provide a comprehensive, user-centric benchmark grounded in expert input.

Method: Uses deterministic metrics for quantifiable attributes and HWPQ for abstract semantics, validated by human comparisons and expert-defined roles/angles.

Result: Achieves >75% human alignment, with HWPQ at 85.9% accuracy; no universal top model, role-weighted scores guide domain-specific deployment.

Conclusion: VISTAR offers reproducible, actionable evaluation for text-to-image models, with publicly released resources.

Abstract: We present VISTAR, a user-centric, multi-dimensional benchmark for
text-to-image (T2I) evaluation that addresses the limitations of existing
metrics. VISTAR introduces a two-tier hybrid paradigm: it employs
deterministic, scriptable metrics for physically quantifiable attributes (e.g.,
text rendering, lighting) and a novel Hierarchical Weighted P/N Questioning
(HWPQ) scheme that uses constrained vision-language models to assess abstract
semantics (e.g., style fusion, cultural fidelity). Grounded in a Delphi study
with 120 experts, we defined seven user roles and nine evaluation angles to
construct the benchmark, which comprises 2,845 prompts validated by over 15,000
human pairwise comparisons. Our metrics achieve high human alignment (>75%),
with the HWPQ scheme reaching 85.9% accuracy on abstract semantics,
significantly outperforming VQA baselines. Comprehensive evaluation of
state-of-the-art models reveals no universal champion, as role-weighted scores
reorder rankings and provide actionable guidance for domain-specific
deployment. All resources are publicly released to foster reproducible T2I
assessment.

</details>


### [61] [An Interpretable Multi-Plane Fusion Framework With Kolmogorov-Arnold Network Guided Attention Enhancement for Alzheimer's Disease Diagnosis](https://arxiv.org/abs/2508.06157)
*Xiaoxiao Yang,Meiliang Liu,Yunfang Xu,Zijin Li,Zhengye Si,Xinyue Yang,Zhiwen Zhao*

Main category: cs.CV

TL;DR: The paper proposes MPF-KANSC, a deep learning framework combining multi-plane fusion and a novel attention mechanism for early Alzheimer's disease diagnosis using sMRI.


<details>
  <summary>Details</summary>
Motivation: Early and precise AD diagnosis is challenging due to subtle brain changes. Existing methods often fail to capture complex relationships in brain pathology.

Method: MPF-KANSC integrates multi-plane fusion (coronal, sagittal, axial) and a Kolmogorov-Arnold Network-guided spatial-channel attention mechanism for feature extraction.

Result: MPF-KANSC outperforms existing methods on the ADNI dataset and reveals right-lateralized asymmetry in subcortical changes.

Conclusion: The framework improves AD diagnosis accuracy and interpretability, offering insights into disease progression.

Abstract: Alzheimer's disease (AD) is a progressive neurodegenerative disorder that
severely impairs cognitive function and quality of life. Timely intervention in
AD relies heavily on early and precise diagnosis, which remains challenging due
to the complex and subtle structural changes in the brain. Most existing deep
learning methods focus only on a single plane of structural magnetic resonance
imaging (sMRI) and struggle to accurately capture the complex and nonlinear
relationships among pathological regions of the brain, thus limiting their
ability to precisely identify atrophic features. To overcome these limitations,
we propose an innovative framework, MPF-KANSC, which integrates multi-plane
fusion (MPF) for combining features from the coronal, sagittal, and axial
planes, and a Kolmogorov-Arnold Network-guided spatial-channel attention
mechanism (KANSC) to more effectively learn and represent sMRI atrophy
features. Specifically, the proposed model enables parallel feature extraction
from multiple anatomical planes, thus capturing more comprehensive structural
information. The KANSC attention mechanism further leverages a more flexible
and accurate nonlinear function approximation technique, facilitating precise
identification and localization of disease-related abnormalities. Experiments
on the ADNI dataset confirm that the proposed MPF-KANSC achieves superior
performance in AD diagnosis. Moreover, our findings provide new evidence of
right-lateralized asymmetry in subcortical structural changes during AD
progression, highlighting the model's promising interpretability.

</details>


### [62] [Fewer Denoising Steps or Cheaper Per-Step Inference: Towards Compute-Optimal Diffusion Model Deployment](https://arxiv.org/abs/2508.06160)
*Zhenbang Du,Yonggan Fu,Lifu Wang,Jiayi Qian,Xiao Luo,Yingyan,Lin*

Main category: cs.CV

TL;DR: PostDiff is a training-free framework for accelerating pre-trained diffusion models by reducing redundancy at input and module levels, showing that reducing per-step inference cost is more effective than cutting denoising steps.


<details>
  <summary>Details</summary>
Motivation: High computational demands of diffusion models hinder deployment on resource-limited platforms, prompting investigation into compute-optimal deployment without fine-tuning.

Method: Proposes PostDiff with mixed-resolution denoising (input level) and hybrid module caching (module level) to reduce redundancy.

Result: PostDiff improves fidelity-efficiency trade-off; reducing per-step inference cost is more effective than reducing denoising steps.

Conclusion: PostDiff offers a practical solution for efficient diffusion model deployment, emphasizing per-step cost reduction for balanced performance.

Abstract: Diffusion models have shown remarkable success across generative tasks, yet
their high computational demands challenge deployment on resource-limited
platforms. This paper investigates a critical question for compute-optimal
diffusion model deployment: Under a post-training setting without fine-tuning,
is it more effective to reduce the number of denoising steps or to use a
cheaper per-step inference? Intuitively, reducing the number of denoising steps
increases the variability of the distributions across steps, making the model
more sensitive to compression. In contrast, keeping more denoising steps makes
the differences smaller, preserving redundancy, and making post-training
compression more feasible. To systematically examine this, we propose PostDiff,
a training-free framework for accelerating pre-trained diffusion models by
reducing redundancy at both the input level and module level in a post-training
manner. At the input level, we propose a mixed-resolution denoising scheme
based on the insight that reducing generation resolution in early denoising
steps can enhance low-frequency components and improve final generation
fidelity. At the module level, we employ a hybrid module caching strategy to
reuse computations across denoising steps. Extensive experiments and ablation
studies demonstrate that (1) PostDiff can significantly improve the
fidelity-efficiency trade-off of state-of-the-art diffusion models, and (2) to
boost efficiency while maintaining decent generation fidelity, reducing
per-step inference cost is often more effective than reducing the number of
denoising steps. Our code is available at
https://github.com/GATECH-EIC/PostDiff.

</details>


### [63] [UW-3DGS: Underwater 3D Reconstruction with Physics-Aware Gaussian Splatting](https://arxiv.org/abs/2508.06169)
*Wenpeng Xing,Jie Chen,Zaifeng Yang,Changting Lin,Jianfeng Dong,Chaochao Chen,Xun Zhou,Meng Han*

Main category: cs.CV

TL;DR: UW-3DGS improves underwater 3D reconstruction by adapting 3D Gaussian Splatting, addressing light degradation and noise with a learnable underwater module and physics-aware pruning.


<details>
  <summary>Details</summary>
Motivation: Traditional methods like NeRF struggle with underwater conditions due to light absorption and scattering, leading to degraded geometry and color fidelity.

Method: UW-3DGS uses a voxel-based regression for underwater image formation and a Physics-Aware Uncertainty Pruning (PAUP) branch to remove noisy Gaussians.

Result: Achieves PSNR of 27.604, SSIM of 0.868, and LPIPS of 0.104 on SeaThru-NeRF, with ~65% fewer floating artifacts.

Conclusion: UW-3DGS outperforms existing methods in underwater 3D reconstruction, offering cleaner geometry and more accurate light transport.

Abstract: Underwater 3D scene reconstruction faces severe challenges from light
absorption, scattering, and turbidity, which degrade geometry and color
fidelity in traditional methods like Neural Radiance Fields (NeRF). While NeRF
extensions such as SeaThru-NeRF incorporate physics-based models, their MLP
reliance limits efficiency and spatial resolution in hazy environments. We
introduce UW-3DGS, a novel framework adapting 3D Gaussian Splatting (3DGS) for
robust underwater reconstruction. Key innovations include: (1) a plug-and-play
learnable underwater image formation module using voxel-based regression for
spatially varying attenuation and backscatter; and (2) a Physics-Aware
Uncertainty Pruning (PAUP) branch that adaptively removes noisy floating
Gaussians via uncertainty scoring, ensuring artifact-free geometry. The
pipeline operates in training and rendering stages. During training, noisy
Gaussians are optimized end-to-end with underwater parameters, guided by PAUP
pruning and scattering modeling. In rendering, refined Gaussians produce clean
Unattenuated Radiance Images (URIs) free from media effects, while learned
physics enable realistic Underwater Images (UWIs) with accurate light
transport. Experiments on SeaThru-NeRF and UWBundle datasets show superior
performance, achieving PSNR of 27.604, SSIM of 0.868, and LPIPS of 0.104 on
SeaThru-NeRF, with ~65% reduction in floating artifacts.

</details>


### [64] [Synthetic Data-Driven Multi-Architecture Framework for Automated Polyp Segmentation Through Integrated Detection and Mask Generation](https://arxiv.org/abs/2508.06170)
*Ojonugwa Oluwafemi Ejiga Peter,Akingbola Oluwapemiisin,Amalahu Chetachi,Adeniran Opeyemi,Fahmi Khalifa,Md Mahmudur Rahman*

Main category: cs.CV

TL;DR: A novel multidirectional framework automates polyp detection in colonoscopy images, combining synthetic data generation with detection and segmentation algorithms for improved accuracy.


<details>
  <summary>Details</summary>
Motivation: Colorectal cancer is a leading cause of cancer mortality, and early detection via colonoscopy is crucial. Limited dataset sizes and annotation complexities hinder effective automation.

Method: The system uses Stable Diffusion for synthetic data, Faster R-CNN for detection, and SAM for segmentation. Five segmentation models (U-Net, PSPNet, FPN, LinkNet, MANet) are evaluated.

Result: Faster R-CNN achieved 93.08% recall and 88.97% precision. FPN scored highest in PSNR and SSIM, while UNet led in recall and LinkNet in IoU and Dice.

Conclusion: The framework effectively addresses dataset limitations and improves polyp detection, with FPN and LinkNet showing superior segmentation performance.

Abstract: Colonoscopy is a vital tool for the early diagnosis of colorectal cancer,
which is one of the main causes of cancer-related mortality globally; hence, it
is deemed an essential technique for the prevention and early detection of
colorectal cancer. The research introduces a unique multidirectional
architectural framework to automate polyp detection within colonoscopy images
while helping resolve limited healthcare dataset sizes and annotation
complexities. The research implements a comprehensive system that delivers
synthetic data generation through Stable Diffusion enhancements together with
detection and segmentation algorithms. This detection approach combines Faster
R-CNN for initial object localization while the Segment Anything Model (SAM)
refines the segmentation masks. The faster R-CNN detection algorithm achieved a
recall of 93.08% combined with a precision of 88.97% and an F1 score of
90.98%.SAM is then used to generate the image mask. The research evaluated five
state-of-the-art segmentation models that included U-Net, PSPNet, FPN, LinkNet,
and MANet using ResNet34 as a base model. The results demonstrate the superior
performance of FPN with the highest scores of PSNR (7.205893) and SSIM
(0.492381), while UNet excels in recall (84.85%) and LinkNet shows balanced
performance in IoU (64.20%) and Dice score (77.53%).

</details>


### [65] [Graph-based Robot Localization Using a Graph Neural Network with a Floor Camera and a Feature Rich Industrial Floor](https://arxiv.org/abs/2508.06177)
*Dominik Br√§mer,Diana Kleingarn,Oliver Urbann*

Main category: cs.CV

TL;DR: A novel localization framework using flooring characteristics with graph-based representations and GCNs outperforms traditional methods in accuracy (0.64cm error) and efficiency, while solving the kidnapped robot problem.


<details>
  <summary>Details</summary>
Motivation: Traditional localization methods like Lidar or QR-code systems lack scalability and adaptability in complex environments.

Method: Uses graph-based representations of floor features and Graph Convolutional Networks (GCNs) for localization.

Result: Achieves 0.64cm localization error and solves the kidnapped robot problem per frame without complex filtering.

Conclusion: The framework enhances robotic navigation in diverse environments by improving accuracy and efficiency.

Abstract: Accurate localization represents a fundamental challenge in
  robotic navigation. Traditional methodologies, such as Lidar or QR-code based
systems, suffer from inherent scalability and adaptability con straints,
particularly in complex environments. In this work, we propose
  an innovative localization framework that harnesses flooring characteris tics
by employing graph-based representations and Graph Convolutional
  Networks (GCNs). Our method uses graphs to represent floor features,
  which helps localize the robot more accurately (0.64cm error) and more
  efficiently than comparing individual image features. Additionally, this
  approach successfully addresses the kidnapped robot problem in every
  frame without requiring complex filtering processes. These advancements
  open up new possibilities for robotic navigation in diverse environments.

</details>


### [66] [Effective Training Data Synthesis for Improving MLLM Chart Understanding](https://arxiv.org/abs/2508.06492)
*Yuwei Yang,Zeyu Zhang,Yunzhong Hou,Zhuowan Li,Gaowen Liu,Ali Payani,Yuan-Sen Ting,Liang Zheng*

Main category: cs.CV

TL;DR: The paper introduces a modular and diversified chart generation pipeline to improve multimodal large language models' (MLLMs) chart understanding, achieving better performance on real-world and synthetic benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs struggle with chart understanding, and synthetic training data often lacks realism, limiting model performance.

Method: A five-step data synthesis pipeline is designed, separating data and function creation, diversifying visuals, filtering low-quality data, and generating QA pairs with GPT-4. The resulting dataset (ECD) includes 10k+ charts and 300k+ QA pairs.

Result: ECD improves MLLMs' performance across real-world and synthetic test sets.

Conclusion: Modular and diversified chart generation enhances MLLMs' chart understanding, with ECD serving as an effective training resource.

Abstract: Being able to effectively read scientific plots, or chart understanding, is a
central part toward building effective agents for science. However, existing
multimodal large language models (MLLMs), especially open-source ones, are
still falling behind with a typical success rate of 30%-50% on challenging
benchmarks. Previous studies on fine-tuning MLLMs with synthetic charts are
often restricted by their inadequate similarity to the real charts, which could
compromise model training and performance on complex real-world charts. In this
study, we show that modularizing chart generation and diversifying visual
details improves chart understanding capabilities. In particular, we design a
five-step data synthesis pipeline, where we separate data and function creation
for single plot generation, condition the generation of later subplots on
earlier ones for multi-subplot figures, visually diversify the generated
figures, filter out low quality data, and finally generate the question-answer
(QA) pairs with GPT-4o. This approach allows us to streamline the generation of
fine-tuning datasets and introduce the effective chart dataset (ECD), which
contains 10k+ chart images and 300k+ QA pairs, covering 25 topics and featuring
250+ chart type combinations with high visual complexity. We show that ECD
consistently improves the performance of various MLLMs on a range of real-world
and synthetic test sets. Code, data and models are available at:
https://github.com/yuweiyang-anu/ECD.

</details>


### [67] [MA-CBP: A Criminal Behavior Prediction Framework Based on Multi-Agent Asynchronous Collaboration](https://arxiv.org/abs/2508.06189)
*Cheng Liu,Daou Zhang,Tingxu Liu,Yuhan Wang,Jinyang Chen,Yuexuan Li,Xinying Xiao,Chenbo Xin,Ziru Wang,Weichao Wu*

Main category: cs.CV

TL;DR: MA-CBP is a multi-agent framework for predicting criminal behavior in real-time video by fusing frame-level semantics and causal summaries for early warnings.


<details>
  <summary>Details</summary>
Motivation: Urbanization increases public security threats; traditional methods lack high-level semantics and real-time capability.

Method: Transforms video streams into semantic descriptions, constructs causal summaries, and fuses frames for joint reasoning.

Result: Achieves superior performance on datasets and enables early criminal activity warnings.

Conclusion: MA-CBP offers a promising solution for urban public safety with multi-scale language supervision.

Abstract: With the acceleration of urbanization, criminal behavior in public scenes
poses an increasingly serious threat to social security. Traditional anomaly
detection methods based on feature recognition struggle to capture high-level
behavioral semantics from historical information, while generative approaches
based on Large Language Models (LLMs) often fail to meet real-time
requirements. To address these challenges, we propose MA-CBP, a criminal
behavior prediction framework based on multi-agent asynchronous collaboration.
This framework transforms real-time video streams into frame-level semantic
descriptions, constructs causally consistent historical summaries, and fuses
adjacent image frames to perform joint reasoning over long- and short-term
contexts. The resulting behavioral decisions include key elements such as event
subjects, locations, and causes, enabling early warning of potential criminal
activity. In addition, we construct a high-quality criminal behavior dataset
that provides multi-scale language supervision, including frame-level,
summary-level, and event-level semantic annotations. Experimental results
demonstrate that our method achieves superior performance on multiple datasets
and offers a promising solution for risk warning in urban public safety
scenarios.

</details>


### [68] [A Semantic Segmentation Algorithm for Pleural Effusion Based on DBIF-AUNet](https://arxiv.org/abs/2508.06191)
*Ruixiang Tang,Jianglong Qin,Mingda Zhang,Yan Song,Yi Wu,Wei Wu*

Main category: cs.CV

TL;DR: Proposed DBIF-AUNet model improves pleural effusion CT image segmentation by addressing gray-level similarity, blurred edges, and variable morphology through dual-branch interaction and feature disentanglement.


<details>
  <summary>Details</summary>
Motivation: Enhance clinical diagnosis by accurately segmenting pleural effusion CT images, overcoming challenges like similar gray levels, blurred edges, and complex morphology.

Method: Introduces DBIF-AUNet with Dual-Domain Feature Disentanglement (DDFD) and Branch Interaction Attention Fusion (BIAF) modules for multi-scale feature complementarity and dynamic feature fusion.

Result: Achieved IoU of 80.1% and Dice score of 89.0%, outperforming U-Net++ and Swin-UNet by significant margins.

Conclusion: DBIF-AUNet significantly improves segmentation accuracy for complex pleural effusion CT images, demonstrating superior performance over existing models.

Abstract: Pleural effusion semantic segmentation can significantly enhance the accuracy
and timeliness of clinical diagnosis and treatment by precisely identifying
disease severity and lesion areas. Currently, semantic segmentation of pleural
effusion CT images faces multiple challenges. These include similar gray levels
between effusion and surrounding tissues, blurred edges, and variable
morphology. Existing methods often struggle with diverse image variations and
complex edges, primarily because direct feature concatenation causes semantic
gaps. To address these challenges, we propose the Dual-Branch Interactive
Fusion Attention model (DBIF-AUNet). This model constructs a densely nested
skip-connection network and innovatively refines the Dual-Domain Feature
Disentanglement module (DDFD). The DDFD module orthogonally decouples the
functions of dual-domain modules to achieve multi-scale feature complementarity
and enhance characteristics at different levels. Concurrently, we design a
Branch Interaction Attention Fusion module (BIAF) that works synergistically
with the DDFD. This module dynamically weights and fuses global, local, and
frequency band features, thereby improving segmentation robustness.
Furthermore, we implement a nested deep supervision mechanism with hierarchical
adaptive hybrid loss to effectively address class imbalance. Through validation
on 1,622 pleural effusion CT images from Southwest Hospital, DBIF-AUNet
achieved IoU and Dice scores of 80.1% and 89.0% respectively. These results
outperform state-of-the-art medical image segmentation models U-Net++ and
Swin-UNet by 5.7%/2.7% and 2.2%/1.5% respectively, demonstrating significant
optimization in segmentation accuracy for complex pleural effusion CT images.

</details>


### [69] [LoRA in LoRA: Towards Parameter-Efficient Architecture Expansion for Continual Visual Instruction Tuning](https://arxiv.org/abs/2508.06202)
*Chang Che,Ziqi Wang,Pengwan Yang,Qi Wang,Hui Ma,Zenglin Shi*

Main category: cs.CV

TL;DR: LiLoRA is an efficient architecture expansion method for CVIT in MLLMs, reducing parameter overhead and improving scalability while mitigating catastrophic forgetting.


<details>
  <summary>Details</summary>
Motivation: Address the challenges of catastrophic forgetting and parameter inefficiency in continual visual instruction tuning for MLLMs.

Method: Introduces LiLoRA, which shares LoRA matrix A across tasks, applies low-rank decomposition to matrix B, and uses a cosine-regularized stability loss.

Result: LiLoRA outperforms existing methods in sequential task learning with improved parameter efficiency.

Conclusion: LiLoRA is a scalable and effective solution for CVIT in MLLMs, balancing performance and efficiency.

Abstract: Continual Visual Instruction Tuning (CVIT) enables Multimodal Large Language
Models (MLLMs) to incrementally learn new tasks over time. However, this
process is challenged by catastrophic forgetting, where performance on
previously learned tasks deteriorates as the model adapts to new ones. A common
approach to mitigate forgetting is architecture expansion, which introduces
task-specific modules to prevent interference. Yet, existing methods often
expand entire layers for each task, leading to significant parameter overhead
and poor scalability. To overcome these issues, we introduce LoRA in LoRA
(LiLoRA), a highly efficient architecture expansion method tailored for CVIT in
MLLMs. LiLoRA shares the LoRA matrix A across tasks to reduce redundancy,
applies an additional low-rank decomposition to matrix B to minimize
task-specific parameters, and incorporates a cosine-regularized stability loss
to preserve consistency in shared representations over time. Extensive
experiments on a diverse CVIT benchmark show that LiLoRA consistently achieves
superior performance in sequential task learning while significantly improving
parameter efficiency compared to existing approaches.

</details>


### [70] [AnomalyMoE: Towards a Language-free Generalist Model for Unified Visual Anomaly Detection](https://arxiv.org/abs/2508.06203)
*Zhaopeng Gu,Bingke Zhu,Guibo Zhu,Yingying Chen,Wei Ge,Ming Tang,Jinqiao Wang*

Main category: cs.CV

TL;DR: AnomalyMoE is a universal anomaly detection framework using a Mixture-of-Experts architecture to detect diverse anomalies hierarchically, outperforming specialized methods.


<details>
  <summary>Details</summary>
Motivation: Existing anomaly detection methods are too specialized, limiting generalizability across domains and anomaly types.

Method: AnomalyMoE decomposes anomaly detection into three semantic hierarchies (local, component, global) using dedicated expert networks, with EIR and ESB modules for expert diversity and utilization.

Result: AnomalyMoE achieves state-of-the-art performance on 8 diverse datasets, surpassing specialized methods in their domains.

Conclusion: AnomalyMoE offers a universal, hierarchical approach to anomaly detection, improving generalizability and performance.

Abstract: Anomaly detection is a critical task across numerous domains and modalities,
yet existing methods are often highly specialized, limiting their
generalizability. These specialized models, tailored for specific anomaly types
like textural defects or logical errors, typically exhibit limited performance
when deployed outside their designated contexts. To overcome this limitation,
we propose AnomalyMoE, a novel and universal anomaly detection framework based
on a Mixture-of-Experts (MoE) architecture. Our key insight is to decompose the
complex anomaly detection problem into three distinct semantic hierarchies:
local structural anomalies, component-level semantic anomalies, and global
logical anomalies. AnomalyMoE correspondingly employs three dedicated expert
networks at the patch, component, and global levels, and is specialized in
reconstructing features and identifying deviations at its designated semantic
level. This hierarchical design allows a single model to concurrently
understand and detect a wide spectrum of anomalies. Furthermore, we introduce
an Expert Information Repulsion (EIR) module to promote expert diversity and an
Expert Selection Balancing (ESB) module to ensure the comprehensive utilization
of all experts. Experiments on 8 challenging datasets spanning industrial
imaging, 3D point clouds, medical imaging, video surveillance, and logical
anomaly detection demonstrate that AnomalyMoE establishes new state-of-the-art
performance, significantly outperforming specialized methods in their
respective domains.

</details>


### [71] [PA-HOI: A Physics-Aware Human and Object Interaction Dataset](https://arxiv.org/abs/2508.06205)
*Ruiyan Wang,Lin Zuo,Zonghao Lin,Qiang Wang,Zhengxue Cheng,Rong Xie,Jun Ling,Li Song*

Main category: cs.CV

TL;DR: The paper introduces the PA-HOI Motion Capture dataset, focusing on how objects' physical attributes affect human motion dynamics, addressing gaps in existing HOI datasets.


<details>
  <summary>Details</summary>
Motivation: Existing HOI datasets overlook the impact of objects' physical properties on human motion. The PA-HOI dataset aims to fill this gap by emphasizing these influences.

Method: The dataset includes 562 motion sequences of human-object interactions, involving subjects of different genders and 35 3D objects with varying size, shape, and weight.

Result: The dataset extends understanding of how object attributes influence human posture, speed, and interaction strategies, and integrates well with motion generation methods.

Conclusion: The PA-HOI dataset enhances realism in motion generation by incorporating physical awareness, validated through integration with existing methods.

Abstract: The Human-Object Interaction (HOI) task explores the dynamic interactions
between humans and objects in physical environments, providing essential
biomechanical and cognitive-behavioral foundations for fields such as robotics,
virtual reality, and human-computer interaction. However, existing HOI data
sets focus on details of affordance, often neglecting the influence of physical
properties of objects on human long-term motion. To bridge this gap, we
introduce the PA-HOI Motion Capture dataset, which highlights the impact of
objects' physical attributes on human motion dynamics, including human posture,
moving velocity, and other motion characteristics. The dataset comprises 562
motion sequences of human-object interactions, with each sequence performed by
subjects of different genders interacting with 35 3D objects that vary in size,
shape, and weight. This dataset stands out by significantly extending the scope
of existing ones for understanding how the physical attributes of different
objects influence human posture, speed, motion scale, and interacting
strategies. We further demonstrate the applicability of the PA-HOI dataset by
integrating it with existing motion generation methods, validating its capacity
to transfer realistic physical awareness.

</details>


### [72] [Interpretable Rheumatoid Arthritis Scoring via Anatomy-aware Multiple Instance Learning](https://arxiv.org/abs/2508.06218)
*Zhiyan Bo,Laura C. Coates,Bartlomiej W. Papiez*

Main category: cs.CV

TL;DR: A two-stage pipeline for interpretable SvdH score prediction in RA using dual-hand radiographs, achieving state-of-the-art accuracy comparable to radiologists.


<details>
  <summary>Details</summary>
Motivation: The complexity of manual SvdH scoring limits its clinical use, prompting the need for an automated, efficient method.

Method: A two-stage pipeline with disease-relevant region extraction and attention-based multiple instance learning for image-level prediction. Two region extraction schemes were tested.

Result: Best model achieved PCC 0.943 and RMSE 15.73; ensemble learning improved to PCC 0.945 and RMSE 15.57, matching radiologist performance.

Conclusion: The pipeline offers accurate, interpretable SvdH scoring, identifying clinically relevant anatomical structures for RA progression.

Abstract: The Sharp/van der Heijde (SvdH) score has been widely used in clinical trials
to quantify radiographic damage in Rheumatoid Arthritis (RA), but its
complexity has limited its adoption in routine clinical practice. To address
the inefficiency of manual scoring, this work proposes a two-stage pipeline for
interpretable image-level SvdH score prediction using dual-hand radiographs.
Our approach extracts disease-relevant image regions and integrates them using
attention-based multiple instance learning to generate image-level features for
prediction. We propose two region extraction schemes: 1) sampling image tiles
most likely to contain abnormalities, and 2) cropping patches containing
disease-relevant joints. With Scheme 2, our best individual score prediction
model achieved a Pearson's correlation coefficient (PCC) of 0.943 and a root
mean squared error (RMSE) of 15.73. Ensemble learning further boosted
prediction accuracy, yielding a PCC of 0.945 and RMSE of 15.57, achieving
state-of-the-art performance that is comparable to that of experienced
radiologists (PCC = 0.97, RMSE = 18.75). Finally, our pipeline effectively
identified and made decisions based on anatomical structures which clinicians
consider relevant to RA progression.

</details>


### [73] [TEFormer: Texture-Aware and Edge-Guided Transformer for Semantic Segmentation of Urban Remote Sensing Images](https://arxiv.org/abs/2508.06224)
*Guoyu Zhou,Jing Zhang,Yi Yan,Hui Zhang,Li Zhuo*

Main category: cs.CV

TL;DR: Proposes TEFormer, a Transformer-based model for semantic segmentation of urban remote sensing images, integrating texture awareness and edge guidance to address challenges like subtle texture differences and complex edge morphologies.


<details>
  <summary>Details</summary>
Motivation: Urban remote sensing images (URSIs) present challenges like subtle texture differences, irregular shapes, and blurred boundaries, leading to semantic ambiguity and misclassification.

Method: TEFormer includes a texture-aware module (TaM) for fine-grained texture capture, an edge-guided tri-branch decoder (Eg3Head) for local edge preservation, and an edge-guided feature fusion module (EgFFM) for refined segmentation.

Result: Achieves mIoU of 88.57%, 81.46%, and 53.55% on Potsdam, Vaihingen, and LoveDA datasets, demonstrating effectiveness.

Conclusion: TEFormer effectively addresses semantic segmentation challenges in URSIs by combining texture awareness and edge guidance, achieving high performance on benchmark datasets.

Abstract: Semantic segmentation of urban remote sensing images (URSIs) is crucial for
applications such as urban planning and environmental monitoring. However,
geospatial objects often exhibit subtle texture differences and similar spatial
structures, which can easily lead to semantic ambiguity and misclassification.
Moreover, challenges such as irregular object shapes, blurred boundaries, and
overlapping spatial distributions of semantic objects contribute to complex and
diverse edge morphologies, further complicating accurate segmentation. To
tackle these issues, we propose a texture-aware and edge-guided Transformer
(TEFormer) that integrates texture awareness and edge-guidance mechanisms for
semantic segmentation of URSIs. In the encoder, a texture-aware module (TaM) is
designed to capture fine-grained texture differences between visually similar
categories to enhance semantic discrimination. Then, an edge-guided tri-branch
decoder (Eg3Head) is constructed to preserve local edges and details for
multiscale context-awareness. Finally, an edge-guided feature fusion module
(EgFFM) is to fuse contextual and detail information with edge information to
realize refined semantic segmentation. Extensive experiments show that TEFormer
achieves mIoU of 88.57%, 81.46%, and 53.55% on the Potsdam, Vaihingen, and
LoveDA datasets, respectively, shows the effectiveness in URSI semantic
segmentation.

</details>


### [74] [Depth Jitter: Seeing through the Depth](https://arxiv.org/abs/2508.06227)
*Md Sazidur Rahman,David Cabecinhas,Ricard Marxer*

Main category: cs.CV

TL;DR: Depth-Jitter is a depth-based augmentation technique that improves model robustness in depth-sensitive environments by simulating natural depth variations.


<details>
  <summary>Details</summary>
Motivation: Conventional augmentation techniques lack depth awareness, limiting model performance in real-world depth variations.

Method: Depth-Jitter applies adaptive depth offsetting guided by depth variance thresholds to generate synthetic depth perturbations while preserving structural integrity.

Result: Depth-Jitter enhances model stability and generalization in depth-sensitive environments, though it doesn't always outperform traditional methods in absolute performance.

Conclusion: Depth-Jitter demonstrates the potential of depth-aware augmentation for real-world applications and encourages further research in depth-based learning strategies.

Abstract: Depth information is essential in computer vision, particularly in underwater
imaging, robotics, and autonomous navigation. However, conventional
augmentation techniques overlook depth aware transformations, limiting model
robustness in real world depth variations. In this paper, we introduce
Depth-Jitter, a novel depth-based augmentation technique that simulates natural
depth variations to improve generalization. Our approach applies adaptive depth
offsetting, guided by depth variance thresholds, to generate synthetic depth
perturbations while preserving structural integrity. We evaluate Depth-Jitter
on two benchmark datasets, FathomNet and UTDAC2020 demonstrating its impact on
model stability under diverse depth conditions. Extensive experiments compare
Depth-Jitter against traditional augmentation strategies such as ColorJitter,
analyzing performance across varying learning rates, encoders, and loss
functions. While Depth-Jitter does not always outperform conventional methods
in absolute performance, it consistently enhances model stability and
generalization in depth-sensitive environments. These findings highlight the
potential of depth-aware augmentation for real-world applications and provide a
foundation for further research into depth-based learning strategies. The
proposed technique is publicly available to support advancements in depth-aware
augmentation. The code is publicly available on
\href{https://github.com/mim-team/Depth-Jitter}{github}.

</details>


### [75] [Towards Unified Image Deblurring using a Mixture-of-Experts Decoder](https://arxiv.org/abs/2508.06228)
*Daniel Feijoo,Paula Garrido-Mellado,Jaesung Rim,Alvaro Garcia,Marcos V. Conde*

Main category: cs.CV

TL;DR: An all-in-one image deblurring method using a mixture-of-experts (MoE) decoding module to handle diverse blur types efficiently.


<details>
  <summary>Details</summary>
Motivation: Existing deblurring methods lack generalization, requiring multiple models for different blur types, which is impractical.

Method: Proposes a mixture-of-experts (MoE) decoding module that dynamically routes features based on blur type for precise restoration.

Result: Achieves performance comparable to task-specific models and shows robustness on unseen blur scenarios.

Conclusion: The unified approach offers practical, efficient, and generalized deblurring for diverse blur types.

Abstract: Image deblurring, removing blurring artifacts from images, is a fundamental
task in computational photography and low-level computer vision. Existing
approaches focus on specialized solutions tailored to particular blur types,
thus, these solutions lack generalization. This limitation in current methods
implies requiring multiple models to cover several blur types, which is not
practical in many real scenarios. In this paper, we introduce the first
all-in-one deblurring method capable of efficiently restoring images affected
by diverse blur degradations, including global motion, local motion, blur in
low-light conditions, and defocus blur. We propose a mixture-of-experts (MoE)
decoding module, which dynamically routes image features based on the
recognized blur degradation, enabling precise and efficient restoration in an
end-to-end manner. Our unified approach not only achieves performance
comparable to dedicated task-specific models, but also demonstrates remarkable
robustness and generalization capabilities on unseen blur degradation
scenarios.

</details>


### [76] [Deepfake Detection that Generalizes Across Benchmarks](https://arxiv.org/abs/2508.06248)
*Andrii Yermakov,Jan Cech,Jiri Matas,Mario Fritz*

Main category: cs.CV

TL;DR: LNCLIP-DF adapts a pre-trained CLIP model with minimal parameter tuning (0.03%) to generalize deepfake detection across unseen techniques, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Generalizing deepfake detectors to unseen manipulation techniques is challenging, and current methods often introduce unnecessary complexity.

Method: Fine-tunes only Layer Normalization parameters in CLIP, enforces a hyperspherical feature manifold using L2 normalization and latent space augmentations.

Result: Outperforms complex methods in cross-dataset AUROC, showing robust generalization on 13 datasets. Key findings highlight the importance of paired real-fake data and dataset diversity.

Conclusion: Targeted, minimal changes to pre-trained models can achieve state-of-the-art generalization efficiently.

Abstract: The generalization of deepfake detectors to unseen manipulation techniques
remains a challenge for practical deployment. Although many approaches adapt
foundation models by introducing significant architectural complexity, this
work demonstrates that robust generalization is achievable through a
parameter-efficient adaptation of a pre-trained CLIP vision encoder. The
proposed method, LNCLIP-DF, fine-tunes only the Layer Normalization parameters
(0.03% of the total) and enhances generalization by enforcing a hyperspherical
feature manifold using L2 normalization and latent space augmentations.
  We conducted an extensive evaluation on 13 benchmark datasets spanning from
2019 to 2025. The proposed method achieves state-of-the-art performance,
outperforming more complex, recent approaches in average cross-dataset AUROC.
Our analysis yields two primary findings for the field: 1) training on paired
real-fake data from the same source video is essential for mitigating shortcut
learning and improving generalization, and 2) detection difficulty on academic
datasets has not strictly increased over time, with models trained on older,
diverse datasets showing strong generalization capabilities.
  This work delivers a computationally efficient and reproducible method,
proving that state-of-the-art generalization is attainable by making targeted,
minimal changes to a pre-trained CLIP model. The code will be made publicly
available upon acceptance.

</details>


### [77] [FedX: Explanation-Guided Pruning for Communication-Efficient Federated Learning in Remote Sensing](https://arxiv.org/abs/2508.06256)
*Barƒ±≈ü B√ºy√ºkta≈ü,Jonas Klotz,Beg√ºm Demir*

Main category: cs.CV

TL;DR: FedX reduces communication overhead in federated learning for remote sensing tasks by using explanation-guided pruning to minimize model size without losing performance.


<details>
  <summary>Details</summary>
Motivation: Federated learning (FL) is constrained by communication overhead in remote sensing tasks due to frequent large model updates. FedX addresses this by pruning less relevant model components.

Method: FedX employs backpropagation-based explanation methods to identify and prune task-irrelevant model components, creating a sparse global model for reduced communication.

Result: FedX significantly reduces shared model parameters while improving generalization, outperforming unpruned and state-of-the-art pruning methods on BigEarthNet-S2 and EuroSAT datasets.

Conclusion: FedX effectively balances communication efficiency and model performance in FL for remote sensing, offering a practical solution for privacy-sensitive applications.

Abstract: Federated learning (FL) enables the collaborative training of deep neural
networks across decentralized data archives (i.e., clients), where each client
stores data locally and only shares model updates with a central server. This
makes FL a suitable learning paradigm for remote sensing (RS) image
classification tasks, where data centralization may be restricted due to legal
and privacy constraints. However, a key challenge in applying FL to RS tasks is
the communication overhead caused by the frequent exchange of large model
updates between clients and the central server. To address this issue, in this
paper we propose a novel strategy (denoted as FedX) that uses
explanation-guided pruning to reduce communication overhead by minimizing the
size of the transmitted models without compromising performance. FedX leverages
backpropagation-based explanation methods to estimate the task-specific
importance of model components and prunes the least relevant ones at the
central server. The resulting sparse global model is then sent to clients,
substantially reducing communication overhead. We evaluate FedX on multi-label
scene classification using the BigEarthNet-S2 dataset and single-label scene
classification using the EuroSAT dataset. Experimental results show the success
of FedX in significantly reducing the number of shared model parameters while
enhancing the generalization capability of the global model, compared to both
unpruned model and state-of-the-art pruning methods. The code of FedX will be
available at https://git.tu-berlin.de/rsim/FedX.

</details>


### [78] [XAG-Net: A Cross-Slice Attention and Skip Gating Network for 2.5D Femur MRI Segmentation](https://arxiv.org/abs/2508.06258)
*Byunghyun Ko,Anning Tian,Jeongkyu Lee*

Main category: cs.CV

TL;DR: XAG-Net, a 2.5D U-Net-based model with cross-slice and skip attention mechanisms, outperforms existing methods in femur MRI segmentation.


<details>
  <summary>Details</summary>
Motivation: Existing 2D and 3D deep learning methods for femur MRI segmentation are limited, necessitating a more accurate and efficient solution.

Method: Proposes XAG-Net, incorporating pixel-wise cross-slice attention (CSA) and skip attention gating (AG) for better inter-slice and intra-slice modeling.

Result: XAG-Net achieves higher accuracy than 2D, 2.5D, and 3D U-Net models while remaining computationally efficient.

Conclusion: XAG-Net is a promising framework for efficient and accurate femur MRI segmentation, validated by ablation studies.

Abstract: Accurate segmentation of femur structures from Magnetic Resonance Imaging
(MRI) is critical for orthopedic diagnosis and surgical planning but remains
challenging due to the limitations of existing 2D and 3D deep learning-based
segmentation approaches. In this study, we propose XAG-Net, a novel 2.5D
U-Net-based architecture that incorporates pixel-wise cross-slice attention
(CSA) and skip attention gating (AG) mechanisms to enhance inter-slice
contextual modeling and intra-slice feature refinement. Unlike previous
CSA-based models, XAG-Net applies pixel-wise softmax attention across adjacent
slices at each spatial location for fine-grained inter-slice modeling.
Extensive evaluations demonstrate that XAG-Net surpasses baseline 2D, 2.5D, and
3D U-Net models in femur segmentation accuracy while maintaining computational
efficiency. Ablation studies further validate the critical role of the CSA and
AG modules, establishing XAG-Net as a promising framework for efficient and
accurate femur MRI segmentation.

</details>


### [79] [SIFThinker: Spatially-Aware Image Focus for Visual Reasoning](https://arxiv.org/abs/2508.06259)
*Zhangquan Chen,Ruihui Zhao,Chuwei Luo,Mingze Sun,Xinlei Yu,Yangyang Kang,Ruqi Huang*

Main category: cs.CV

TL;DR: SIFThinker is a spatially-aware framework for MLLMs that improves visual tasks by correcting attention with spatial cues and focusing on relevant regions.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs struggle with complex visual tasks like spatial understanding and fine-grained perception, lacking iterative refinement of attention.

Method: Introduces SIFThinker, using depth-enhanced bounding boxes and natural language for attention correction. Includes a reverse-expansion-forward-inference strategy and GRPO-SIF training paradigm.

Result: Outperforms state-of-the-art methods in spatial understanding and fine-grained perception while maintaining general capabilities.

Conclusion: SIFThinker effectively enhances MLLMs' visual task performance through spatial awareness and iterative refinement.

Abstract: Current multimodal large language models (MLLMs) still face significant
challenges in complex visual tasks (e.g., spatial understanding, fine-grained
perception). Prior methods have tried to incorporate visual reasoning, however,
they fail to leverage attention correction with spatial cues to iteratively
refine their focus on prompt-relevant regions. In this paper, we introduce
SIFThinker, a spatially-aware "think-with-images" framework that mimics human
visual perception. Specifically, SIFThinker enables attention correcting and
image region focusing by interleaving depth-enhanced bounding boxes and natural
language. Our contributions are twofold: First, we introduce a
reverse-expansion-forward-inference strategy that facilitates the generation of
interleaved image-text chains of thought for process-level supervision, which
in turn leads to the construction of the SIF-50K dataset. Besides, we propose
GRPO-SIF, a reinforced training paradigm that integrates depth-informed visual
grounding into a unified reasoning pipeline, teaching the model to dynamically
correct and focus on prompt-relevant regions. Extensive experiments demonstrate
that SIFThinker outperforms state-of-the-art methods in spatial understanding
and fine-grained visual perception, while maintaining strong general
capabilities, highlighting the effectiveness of our method.

</details>


### [80] [Uncertainty-quantified Rollout Policy Adaptation for Unlabelled Cross-domain Temporal Grounding](https://arxiv.org/abs/2508.06317)
*Jian Hu,Zixu Cheng,Shaogang Gong,Isabel Guan,Jianye Hao,Jun Wang,Kun Shao*

Main category: cs.CV

TL;DR: The paper introduces URPA, a data-efficient method for cross-domain video temporal grounding without labeled target data, using uncertainty-quantified rollouts for adaptation.


<details>
  <summary>Details</summary>
Motivation: Existing methods like GRPO require labeled data and are computationally expensive, limiting their use in unlabeled domains and real-time applications.

Method: URPA adapts a model trained on a labeled source domain to a target domain using unlabeled videos, leveraging uncertainty-quantified rollouts for pseudo-labeling and confidence-based training.

Result: URPA achieves strong generalization across six cross-domain settings using minimal unlabeled target videos.

Conclusion: URPA addresses the limitations of GRPO by enabling efficient, label-free cross-domain adaptation for real-time video temporal grounding.

Abstract: Video Temporal Grounding (TG) aims to temporally locate video segments
matching a natural language description (a query) in a long video. While
Vision-Language Models (VLMs) are effective at holistic semantic matching, they
often struggle with fine-grained temporal localisation. Recently, Group
Relative Policy Optimisation (GRPO) reformulates the inference process as a
reinforcement learning task, enabling fine-grained grounding and achieving
strong in-domain performance. However, GRPO relies on labelled data, making it
unsuitable in unlabelled domains. Moreover, because videos are large and
expensive to store and process, performing full-scale adaptation introduces
prohibitive latency and computational overhead, making it impractical for
real-time deployment. To overcome both problems, we introduce a Data-Efficient
Unlabelled Cross-domain Temporal Grounding method, from which a model is first
trained on a labelled source domain, then adapted to a target domain using only
a small number of unlabelled videos from the target domain. This approach
eliminates the need for target annotation and keeps both computational and
storage overhead low enough to run in real time. Specifically, we introduce.
Uncertainty-quantified Rollout Policy Adaptation (URPA) for cross-domain
knowledge transfer in learning video temporal grounding without target labels.
URPA generates multiple candidate predictions using GRPO rollouts, averages
them to form a pseudo label, and estimates confidence from the variance across
these rollouts. This confidence then weights the training rewards, guiding the
model to focus on reliable supervision. Experiments on three datasets across
six cross-domain settings show that URPA generalises well using only a few
unlabelled target videos. Codes will be released once published.

</details>


### [81] [Mixture of Experts Guided by Gaussian Splatters Matters: A new Approach to Weakly-Supervised Video Anomaly Detection](https://arxiv.org/abs/2508.06318)
*Giacomo D'Amicantonio,Snehashis Majhi,Quan Kong,Lorenzo Garattoni,Gianpiero Francesca,Fran√ßois Bremond,Egor Bondarev*

Main category: cs.CV

TL;DR: The paper introduces GS-MoE, a novel framework for Weakly-Supervised Video Anomaly Detection (WSVAD) that uses specialized expert models guided by temporal Gaussian splatting to improve anomaly detection performance.


<details>
  <summary>Details</summary>
Motivation: Current WSVAD models struggle with complex real-world anomalies due to shared model processing and weak supervision signals lacking temporal precision.

Method: Proposes GS-MoE, a mixture of expert models, each specialized for specific anomaly types, guided by a temporal Gaussian splatting loss to enhance weak supervision.

Result: Achieves 91.58% AUC on UCF-Crime and superior results on XD-Violence and MSAD datasets.

Conclusion: GS-MoE sets a new benchmark for VAD under weak supervision by leveraging category-specific expertise and temporal guidance.

Abstract: Video Anomaly Detection (VAD) is a challenging task due to the variability of
anomalous events and the limited availability of labeled data. Under the
Weakly-Supervised VAD (WSVAD) paradigm, only video-level labels are provided
during training, while predictions are made at the frame level. Although
state-of-the-art models perform well on simple anomalies (e.g., explosions),
they struggle with complex real-world events (e.g., shoplifting). This
difficulty stems from two key issues: (1) the inability of current models to
address the diversity of anomaly types, as they process all categories with a
shared model, overlooking category-specific features; and (2) the weak
supervision signal, which lacks precise temporal information, limiting the
ability to capture nuanced anomalous patterns blended with normal events. To
address these challenges, we propose Gaussian Splatting-guided Mixture of
Experts (GS-MoE), a novel framework that employs a set of expert models, each
specialized in capturing specific anomaly types. These experts are guided by a
temporal Gaussian splatting loss, enabling the model to leverage temporal
consistency and enhance weak supervision. The Gaussian splatting approach
encourages a more precise and comprehensive representation of anomalies by
focusing on temporal segments most likely to contain abnormal events. The
predictions from these specialized experts are integrated through a
mixture-of-experts mechanism to model complex relationships across diverse
anomaly patterns. Our approach achieves state-of-the-art performance, with a
91.58% AUC on the UCF-Crime dataset, and demonstrates superior results on
XD-Violence and MSAD datasets. By leveraging category-specific expertise and
temporal guidance, GS-MoE sets a new benchmark for VAD under weak supervision.

</details>


### [82] [Can Diffusion Models Bridge the Domain Gap in Cardiac MR Imaging?](https://arxiv.org/abs/2508.06327)
*Xin Ci Wong,Duygu Sarikaya,Kieran Zucker,Marc De Kamps,Nishant Ravikumar*

Main category: cs.CV

TL;DR: A diffusion model (DM) generates synthetic cardiac MR images to address domain shift in AI models, improving segmentation performance on unseen domains without transfer learning.


<details>
  <summary>Details</summary>
Motivation: Domain shift in cardiac MR imaging degrades AI model performance. Synthetic data offers a solution but faces anatomical consistency challenges.

Method: A DM trained on source domain generates synthetic images resembling a reference, maintaining structural fidelity. Evaluated with 2D/3D nnU-Net and U-Net for domain generalization and adaptation.

Result: Significant improvement in segmentation performance on unseen domains (p < 0.01) compared to real data alone.

Conclusion: The DM-based approach effectively addresses domain shift without needing transfer learning, beneficial in data-scarce settings.

Abstract: Magnetic resonance (MR) imaging, including cardiac MR, is prone to domain
shift due to variations in imaging devices and acquisition protocols. This
challenge limits the deployment of trained AI models in real-world scenarios,
where performance degrades on unseen domains. Traditional solutions involve
increasing the size of the dataset through ad-hoc image augmentation or
additional online training/transfer learning, which have several limitations.
Synthetic data offers a promising alternative, but anatomical/structural
consistency constraints limit the effectiveness of generative models in
creating image-label pairs. To address this, we propose a diffusion model (DM)
trained on a source domain that generates synthetic cardiac MR images that
resemble a given reference. The synthetic data maintains spatial and structural
fidelity, ensuring similarity to the source domain and compatibility with the
segmentation mask. We assess the utility of our generative approach in
multi-centre cardiac MR segmentation, using the 2D nnU-Net, 3D nnU-Net and
vanilla U-Net segmentation networks. We explore domain generalisation, where,
domain-invariant segmentation models are trained on synthetic source domain
data, and domain adaptation, where, we shift target domain data towards the
source domain using the DM. Both strategies significantly improved segmentation
performance on data from an unseen target domain, in terms of surface-based
metrics (Welch's t-test, p < 0.01), compared to training segmentation models on
real data alone. The proposed method ameliorates the need for transfer learning
or online training to address domain shift challenges in cardiac MR image
analysis, especially useful in data-scarce settings.

</details>


### [83] [ViPro-2: Unsupervised State Estimation via Integrated Dynamics for Guiding Video Prediction](https://arxiv.org/abs/2508.06335)
*Patrick Takenaka,Johannes Maucher,Marco F. Huber*

Main category: cs.CV

TL;DR: Improved ViPro model for unsupervised state inference from observations, avoiding shortcuts and tested on a 3D Orbits dataset.


<details>
  <summary>Details</summary>
Motivation: Previous ViPro model relied on ground truth initial states, leading to shortcuts and poor performance with noisy observations.

Method: Enhanced ViPro to infer states from observations without initial ground truth, tested on a new 3D Orbits dataset.

Result: Model successfully infers states unsupervised, closing the gap to real-world scenarios.

Conclusion: Improved ViPro enables robust state inference from observations, advancing video frame prediction.

Abstract: Predicting future video frames is a challenging task with many downstream
applications. Previous work has shown that procedural knowledge enables deep
models for complex dynamical settings, however their model ViPro assumed a
given ground truth initial symbolic state. We show that this approach led to
the model learning a shortcut that does not actually connect the observed
environment with the predicted symbolic state, resulting in the inability to
estimate states given an observation if previous states are noisy. In this
work, we add several improvements to ViPro that enables the model to correctly
infer states from observations without providing a full ground truth state in
the beginning. We show that this is possible in an unsupervised manner, and
extend the original Orbits dataset with a 3D variant to close the gap to real
world scenarios.

</details>


### [84] [Street View Sociability: Interpretable Analysis of Urban Social Behavior Across 15 Cities](https://arxiv.org/abs/2508.06342)
*Kieran Elrod,Katherine Flanigan,Mario Berg√©s*

Main category: cs.CV

TL;DR: The paper explores using street view imagery and a multimodal large language model to infer social interactions in urban areas, linking these to environmental factors and place attachment.


<details>
  <summary>Details</summary>
Motivation: Existing research focuses on pedestrian volume, not social interaction quality. The study aims to extract latent social information from street view images using social science theory.

Method: Analyzed 2,998 street view images from 15 cities using a multimodal large language model guided by Mehta's sociability taxonomy. Linear regression models tested correlations with place attachment and environmental predictors.

Result: Sky view index linked to all sociability types, green view index predicted enduring sociability, and place attachment correlated with fleeting sociability.

Conclusion: Street view imagery can infer relationships between social interactions and built environment variables, offering a scalable tool for urban sociability research.

Abstract: Designing socially active streets has long been a goal of urban planning, yet
existing quantitative research largely measures pedestrian volume rather than
the quality of social interactions. We hypothesize that street view imagery --
an inexpensive data source with global coverage -- contains latent social
information that can be extracted and interpreted through established social
science theory. As a proof of concept, we analyzed 2,998 street view images
from 15 cities using a multimodal large language model guided by Mehta's
taxonomy of passive, fleeting, and enduring sociability -- one illustrative
example of a theory grounded in urban design that could be substituted or
complemented by other sociological frameworks. We then used linear regression
models, controlling for factors like weather, time of day, and pedestrian
counts, to test whether the inferred sociability measures correlate with
city-level place attachment scores from the World Values Survey and with
environmental predictors (e.g., green, sky, and water view indices) derived
from individual street view images. Results aligned with long-standing urban
planning theory: the sky view index was associated with all three sociability
types, the green view index predicted enduring sociability, and place
attachment was positively associated with fleeting sociability. These results
provide preliminary evidence that street view images can be used to infer
relationships between specific types of social interactions and built
environment variables. Further research could establish street view imagery as
a scalable, privacy-preserving tool for studying urban sociability, enabling
cross-cultural theory testing and evidence-based design of socially vibrant
cities.

</details>


### [85] [Aligning Effective Tokens with Video Anomaly in Large Language Models](https://arxiv.org/abs/2508.06350)
*Yingxian Chen,Jiahui Liu,Ruifan Di,Yanwei Li,Chirui Chang,Shizhen Zhao,Wilton W. T. Fok,Xiaojuan Qi,Yik-Chung Wu*

Main category: cs.CV

TL;DR: VA-GPT is a novel MLLM for summarizing and localizing abnormal events in videos, using SETS and TETG modules to improve spatial and temporal analysis. It outperforms existing methods.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs struggle with anomalies due to spatial and temporal sparsity, leading to suboptimal results.

Method: Proposes VA-GPT with SETS and TETG modules for effective token alignment and introduces a fine-tuning dataset and cross-domain benchmark.

Result: Outperforms state-of-the-art methods on various benchmarks.

Conclusion: VA-GPT effectively addresses challenges in abnormal event analysis, improving accuracy and interaction.

Abstract: Understanding abnormal events in videos is a vital and challenging task that
has garnered significant attention in a wide range of applications. Although
current video understanding Multi-modal Large Language Models (MLLMs) are
capable of analyzing general videos, they often struggle to handle anomalies
due to the spatial and temporal sparsity of abnormal events, where the
redundant information always leads to suboptimal outcomes. To address these
challenges, exploiting the representation and generalization capabilities of
Vison Language Models (VLMs) and Large Language Models (LLMs), we propose
VA-GPT, a novel MLLM designed for summarizing and localizing abnormal events in
various videos. Our approach efficiently aligns effective tokens between visual
encoders and LLMs through two key proposed modules: Spatial Effective Token
Selection (SETS) and Temporal Effective Token Generation (TETG). These modules
enable our model to effectively capture and analyze both spatial and temporal
information associated with abnormal events, resulting in more accurate
responses and interactions. Furthermore, we construct an instruction-following
dataset specifically for fine-tuning video-anomaly-aware MLLMs, and introduce a
cross-domain evaluation benchmark based on XD-Violence dataset. Our proposed
method outperforms existing state-of-the-art methods on various benchmarks.

</details>


### [86] [An Implemention of Two-Phase Image Segmentation using the Split Bregman Method](https://arxiv.org/abs/2508.06351)
*Olakunle S. Abawonse,G√ºnay Doƒüan*

Main category: cs.CV

TL;DR: Implementation of a two-phase image segmentation algorithm based on Goldstein et al.'s modification of the Chan-Vese model, optimized using the split Bregman method.


<details>
  <summary>Details</summary>
Motivation: To efficiently partition 2D images into foreground and background regions using a modified energy model for smoother boundaries and distinct pixel value averages.

Method: Modifies the Chan-Vese energy model for efficient minimization via the split Bregman method, assigning pixel membership to two regions.

Result: Detailed implementation and performance evaluation across various images and algorithm parameters.

Conclusion: The method effectively segments images into two regions with smooth boundaries, validated by empirical testing.

Abstract: In this paper, we describe an implementation of the two-phase image
segmentation algorithm proposed by Goldstein, Bresson, Osher in
\cite{gold:bre}. This algorithm partitions the domain of a given 2d image into
foreground and background regions, and each pixel of the image is assigned
membership to one of these two regions. The underlying assumption for the
segmentation model is that the pixel values of the input image can be
summarized by two distinct average values, and that the region boundaries are
smooth. Accordingly, the model is defined as an energy in which the variable is
a region membership function to assign pixels to either region, originally
proposed by Chan and Vese in \cite{chan:vese}. This energy is the sum of image
data terms in the regions and a length penalty for region boundaries.
Goldstein, Bresson, Osher modify the energy of Chan-Vese in \cite{gold:bre} so
that their new energy can be minimized efficiently using the split Bregman
method to produce an equivalent two-phase segmentation. We provide a detailed
implementation of this method \cite{gold:bre}, and document its performance
with several images over a range of algorithm parameters.

</details>


### [87] [Are you In or Out (of gallery)? Wisdom from the Same-Identity Crowd](https://arxiv.org/abs/2508.06357)
*Aman Bhatta,Maria Dhakal,Michael C. King,Kevin W. Bowyer*

Main category: cs.CV

TL;DR: The paper introduces a new method for detecting Out-of-gallery cases in one-to-many facial identification by training a classifier using ranks of additional enrolled images, showing effectiveness across degraded probes and demographics.


<details>
  <summary>Details</summary>
Motivation: To reduce false positives and wrongful arrests by objectively determining if a rank-one result in facial identification is Out-of-gallery.

Method: Generates training data from ranks of additional enrolled images, trains a classifier to predict In-gallery/Out-of-gallery status, and tests on degraded probes and across demographics.

Result: The approach works well for mugshot and degraded probes, with similar accuracy across demographics, and is effective only with advanced matchers.

Conclusion: The method offers a viable solution to reduce false identifications and investigative waste, with performance tied to advanced matchers.

Abstract: A central problem in one-to-many facial identification is that the person in
the probe image may or may not have enrolled image(s) in the gallery; that is,
may be In-gallery or Out-of-gallery. Past approaches to detect when a rank-one
result is Out-of-gallery have mostly focused on finding a suitable threshold on
the similarity score. We take a new approach, using the additional enrolled
images of the identity with the rank-one result to predict if the rank-one
result is In-gallery / Out-of-gallery. Given a gallery of identities and
images, we generate In-gallery and Out-of-gallery training data by extracting
the ranks of additional enrolled images corresponding to the rank-one identity.
We then train a classifier to utilize this feature vector to predict whether a
rank-one result is In-gallery or Out-of-gallery. Using two different datasets
and four different matchers, we present experimental results showing that our
approach is viable for mugshot quality probe images, and also, importantly, for
probes degraded by blur, reduced resolution, atmospheric turbulence and
sunglasses. We also analyze results across demographic groups, and show that
In-gallery / Out-of-gallery classification accuracy is similar across
demographics. Our approach has the potential to provide an objective estimate
of whether a one-to-many facial identification is Out-of-gallery, and thereby
to reduce false positive identifications, wrongful arrests, and wasted
investigative time. Interestingly, comparing the results of older deep
CNN-based face matchers with newer ones suggests that the effectiveness of our
Out-of-gallery detection approach emerges only with matchers trained using
advanced margin-based loss functions.

</details>


### [88] [Text as Any-Modality for Zero-Shot Classification by Consistent Prompt Tuning](https://arxiv.org/abs/2508.06382)
*Xiangyu Wu,Feng Yu,Yang Yang,Jianfeng Lu*

Main category: cs.CV

TL;DR: TaAM-CPT is a scalable method for general representation across unlimited modalities using only text data, achieving top results without modality-specific labels.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on labeled data or are limited to single modalities, prompting the need for a scalable, generalizable approach.

Method: TaAM-CPT uses modality prompt pools, text construction, and modality-aligned encoders, with intra- and inter-modal learning objectives for consistency.

Result: TaAM-CPT achieves leading results on diverse tasks (video, image, audio classification) without modality-specific labeled data.

Conclusion: TaAM-CPT offers a scalable, generalizable solution for multimodal learning, leveraging text data and pre-trained models effectively.

Abstract: The integration of prompt tuning with multimodal learning has shown
significant generalization abilities for various downstream tasks. Despite
advancements, existing methods heavily depend on massive modality-specific
labeled data (e.g., video, audio, and image), or are customized for a single
modality. In this study, we present Text as Any-Modality by Consistent Prompt
Tuning (TaAM-CPT), a scalable approach for constructing a general
representation model toward unlimited modalities using solely text data.
TaAM-CPT comprises modality prompt pools, text construction, and
modality-aligned text encoders from pre-trained models, which allows for
extending new modalities by simply adding prompt pools and modality-aligned
text encoders. To harmonize the learning across different modalities, TaAM-CPT
designs intra- and inter-modal learning objectives, which can capture category
details within modalities while maintaining semantic consistency across
different modalities. Benefiting from its scalable architecture and pre-trained
models, TaAM-CPT can be seamlessly extended to accommodate unlimited
modalities. Remarkably, without any modality-specific labeled data, TaAM-CPT
achieves leading results on diverse datasets spanning various modalities,
including video classification, image classification, and audio classification.
The code is available at https://github.com/Jinx630/TaAM-CPT.

</details>


### [89] [FVGen: Accelerating Novel-View Synthesis with Adversarial Video Diffusion Distillation](https://arxiv.org/abs/2508.06392)
*Wenbin Teng,Gonglin Chen,Haiwei Chen,Yajie Zhao*

Main category: cs.CV

TL;DR: FVGen accelerates novel view synthesis using VDMs with a distilled few-step model, reducing sampling time by over 90% while maintaining quality.


<details>
  <summary>Details</summary>
Motivation: Sparse views in 3D reconstruction cause artifacts; VDMs are slow for dense observation generation.

Method: Distills a multi-step VDM into a few-step model using GANs and softened reverse KL-divergence.

Result: Achieves similar/better visual quality with 90% faster sampling than prior methods.

Conclusion: FVGen enhances efficiency for sparse-view 3D reconstruction tasks.

Abstract: Recent progress in 3D reconstruction has enabled realistic 3D models from
dense image captures, yet challenges persist with sparse views, often leading
to artifacts in unseen areas. Recent works leverage Video Diffusion Models
(VDMs) to generate dense observations, filling the gaps when only sparse views
are available for 3D reconstruction tasks. A significant limitation of these
methods is their slow sampling speed when using VDMs. In this paper, we present
FVGen, a novel framework that addresses this challenge by enabling fast novel
view synthesis using VDMs in as few as four sampling steps. We propose a novel
video diffusion model distillation method that distills a multi-step denoising
teacher model into a few-step denoising student model using Generative
Adversarial Networks (GANs) and softened reverse KL-divergence minimization.
Extensive experiments on real-world datasets show that, compared to previous
works, our framework generates the same number of novel views with similar (or
even better) visual quality while reducing sampling time by more than 90%.
FVGen significantly improves time efficiency for downstream reconstruction
tasks, particularly when working with sparse input views (more than 2) where
pre-trained VDMs need to be run multiple times to achieve better spatial
coverage.

</details>


### [90] [A Classification-Aware Super-Resolution Framework for Ship Targets in SAR Imagery](https://arxiv.org/abs/2508.06407)
*Ch Muhammad Awais,Marco Reggiannini,Davide Moroni,Oktay Karakus*

Main category: cs.CV

TL;DR: The paper explores integrating classification objectives into super-resolution (SR) to improve both image quality and downstream classification accuracy, proposing a novel method for synthetic aperture radar imagery.


<details>
  <summary>Details</summary>
Motivation: Low-resolution images limit automated analysis accuracy; traditional SR methods focus on pixel-level metrics, neglecting the impact on classification performance.

Method: A novel SR methodology optimizes loss functions for both image quality and classification performance, applied to synthetic aperture radar imagery.

Result: The approach improves image quality and enhances classification accuracy.

Conclusion: Integrating classification objectives into SR can improve both image fidelity and downstream task performance.

Abstract: High-resolution imagery plays a critical role in improving the performance of
visual recognition tasks such as classification, detection, and segmentation.
In many domains, including remote sensing and surveillance, low-resolution
images can limit the accuracy of automated analysis. To address this,
super-resolution (SR) techniques have been widely adopted to attempt to
reconstruct high-resolution images from low-resolution inputs. Related
traditional approaches focus solely on enhancing image quality based on
pixel-level metrics, leaving the relationship between super-resolved image
fidelity and downstream classification performance largely underexplored. This
raises a key question: can integrating classification objectives directly into
the super-resolution process further improve classification accuracy? In this
paper, we try to respond to this question by investigating the relationship
between super-resolution and classification through the deployment of a
specialised algorithmic strategy. We propose a novel methodology that increases
the resolution of synthetic aperture radar imagery by optimising loss functions
that account for both image quality and classification performance. Our
approach improves image quality, as measured by scientifically ascertained
image quality indicators, while also enhancing classification accuracy.

</details>


### [91] [Feature-Space Oversampling for Addressing Class Imbalance in SAR Ship Classification](https://arxiv.org/abs/2508.06420)
*Ch Muhammad Awais,Marco Reggiannini,Davide Moroni,Oktay Karakus*

Main category: cs.CV

TL;DR: The paper evaluates oversampling in feature space for SAR ship classification, proposing two novel algorithms (M2m$_f$, M2m$_u$) that outperform baselines, improving F1-scores by 8.82% and 4.44% on two datasets.


<details>
  <summary>Details</summary>
Motivation: Addressing class imbalance in SAR ship classification due to long-tailed datasets, where underrepresented classes complicate classification.

Method: Proposed two algorithms (M2m$_f$, M2m$_u$) inspired by M2m, tested on OpenSARShip and FuSARShip datasets using ViT, VGG16, and ResNet50 as feature extractors. Analyzed oversampling impact on class sizes.

Result: Novel methods outperformed original M2m and baselines, achieving average F1-score increases of 8.82% (FuSARShip) and 4.44% (OpenSARShip).

Conclusion: The proposed oversampling methods effectively improve SAR ship classification performance, especially for underrepresented classes.

Abstract: SAR ship classification faces the challenge of long-tailed datasets, which
complicates the classification of underrepresented classes. Oversampling
methods have proven effective in addressing class imbalance in optical data. In
this paper, we evaluated the effect of oversampling in the feature space for
SAR ship classification. We propose two novel algorithms inspired by the
Major-to-minor (M2m) method M2m$_f$, M2m$_u$. The algorithms are tested on two
public datasets, OpenSARShip (6 classes) and FuSARShip (9 classes), using three
state-of-the-art models as feature extractors: ViT, VGG16, and ResNet50.
Additionally, we also analyzed the impact of oversampling methods on different
class sizes. The results demonstrated the effectiveness of our novel methods
over the original M2m and baselines, with an average F1-score increase of 8.82%
for FuSARShip and 4.44% for OpenSARShip.

</details>


### [92] [SPARSE Data, Rich Results: Few-Shot Semi-Supervised Learning via Class-Conditioned Image Translation](https://arxiv.org/abs/2508.06429)
*Guido Manni,Clemente Lauretti,Loredana Zollo,Paolo Soda*

Main category: cs.CV

TL;DR: A GAN-based semi-supervised learning framework for medical imaging improves classification with minimal labeled data, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Deep learning in medical imaging is hindered by limited labeled data, necessitating innovative solutions for low labeled-data regimes.

Method: The framework integrates a generator, discriminator, and classifier in a three-phase training process, using ensemble-based pseudo-labeling and image-to-image translation.

Result: Outperforms six state-of-the-art methods, especially in extreme 5-shot settings, across eleven MedMNIST datasets.

Conclusion: Provides a practical solution for medical imaging with scarce labeled data, maintaining robust performance across all evaluated settings.

Abstract: Deep learning has revolutionized medical imaging, but its effectiveness is
severely limited by insufficient labeled training data. This paper introduces a
novel GAN-based semi-supervised learning framework specifically designed for
low labeled-data regimes, evaluated across settings with 5 to 50 labeled
samples per class. Our approach integrates three specialized neural networks --
a generator for class-conditioned image translation, a discriminator for
authenticity assessment and classification, and a dedicated classifier --
within a three-phase training framework. The method alternates between
supervised training on limited labeled data and unsupervised learning that
leverages abundant unlabeled images through image-to-image translation rather
than generation from noise. We employ ensemble-based pseudo-labeling that
combines confidence-weighted predictions from the discriminator and classifier
with temporal consistency through exponential moving averaging, enabling
reliable label estimation for unlabeled data. Comprehensive evaluation across
eleven MedMNIST datasets demonstrates that our approach achieves statistically
significant improvements over six state-of-the-art GAN-based semi-supervised
methods, with particularly strong performance in the extreme 5-shot setting
where the scarcity of labeled data is most challenging. The framework maintains
its superiority across all evaluated settings (5, 10, 20, and 50 shots per
class). Our approach offers a practical solution for medical imaging
applications where annotation costs are prohibitive, enabling robust
classification performance even with minimal labeled data. Code is available at
https://github.com/GuidoManni/SPARSE.

</details>


### [93] [MotionSwap](https://arxiv.org/abs/2508.06430)
*Om Patil,Jinesh Modi,Suryabha Mukhopadhyay,Meghaditya Giri,Chhavi Malhotra*

Main category: cs.CV

TL;DR: This paper enhances SimSwap for face swapping with self/cross-attention, dynamic loss weighting, and cosine annealing, improving identity preservation and visual quality.


<details>
  <summary>Details</summary>
Motivation: To advance face swapping technology by improving fidelity, identity preservation, and attribute consistency.

Method: Enhanced SimSwap with self/cross-attention, dynamic loss weighting, and cosine annealing learning rate scheduling.

Result: Better identity similarity, lower FID scores, and superior visual quality compared to baseline.

Conclusion: Future work includes StyleGAN3 integration, lip sync, 3D modeling, and temporal consistency for videos.

Abstract: Face swapping technology has gained significant attention in both academic
research and commercial applications. This paper presents our implementation
and enhancement of SimSwap, an efficient framework for high fidelity face
swapping. We introduce several improvements to the original model, including
the integration of self and cross-attention mechanisms in the generator
architecture, dynamic loss weighting, and cosine annealing learning rate
scheduling. These enhancements lead to significant improvements in identity
preservation, attribute consistency, and overall visual quality.
  Our experimental results, spanning 400,000 training iterations, demonstrate
progressive improvements in generator and discriminator performance. The
enhanced model achieves better identity similarity, lower FID scores, and
visibly superior qualitative results compared to the baseline. Ablation studies
confirm the importance of each architectural and training improvement. We
conclude by identifying key future directions, such as integrating StyleGAN3,
improving lip synchronization, incorporating 3D facial modeling, and
introducing temporal consistency for video-based applications.

</details>


### [94] [CLIPin: A Non-contrastive Plug-in to CLIP for Multimodal Semantic Alignment](https://arxiv.org/abs/2508.06434)
*Shengzhu Yang,Jiawei Du,Shuai Lu,Weihang Zhang,Ningli Wang,Huiqi Li*

Main category: cs.CV

TL;DR: CLIPin is a non-contrastive plug-in for CLIP-style models to improve multimodal alignment, tested effectively on diverse tasks.


<details>
  <summary>Details</summary>
Motivation: Weak supervision in natural datasets and low diversity in medical datasets hinder CLIP's ability to learn robust representations.

Method: Proposes CLIPin, a unified plug-in with shared pre-projectors for image and text, integrating contrastive and non-contrastive learning.

Result: CLIPin enhances alignment robustness and generality, validated on various downstream tasks.

Conclusion: CLIPin is an effective plug-and-play component for improving CLIP-style models.

Abstract: Large-scale natural image-text datasets, especially those automatically
collected from the web, often suffer from loose semantic alignment due to weak
supervision, while medical datasets tend to have high cross-modal correlation
but low content diversity. These properties pose a common challenge for
contrastive language-image pretraining (CLIP): they hinder the model's ability
to learn robust and generalizable representations. In this work, we propose
CLIPin, a unified non-contrastive plug-in that can be seamlessly integrated
into CLIP-style architectures to improve multimodal semantic alignment,
providing stronger supervision and enhancing alignment robustness. Furthermore,
two shared pre-projectors are designed for image and text modalities
respectively to facilitate the integration of contrastive and non-contrastive
learning in a parameter-compromise manner. Extensive experiments on diverse
downstream tasks demonstrate the effectiveness and generality of CLIPin as a
plug-and-play component compatible with various contrastive frameworks. Code is
available at https://github.com/T6Yang/CLIPin.

</details>


### [95] [TRUST: Leveraging Text Robustness for Unsupervised Domain Adaptation](https://arxiv.org/abs/2508.06452)
*Mattia Litrico,Mario Valerio Giuffrida,Sebastiano Battiato,Devis Tuia*

Main category: cs.CV

TL;DR: TRUST is a novel UDA method using language modality to guide vision model adaptation, improving robustness to complex domain shifts.


<details>
  <summary>Details</summary>
Motivation: Existing UDA methods struggle with complex domain shifts (e.g., geographical). Language modality offers robustness, motivating its integration into vision model adaptation.

Method: TRUST generates pseudo-labels from captions, estimates uncertainty via CLIP similarity scores, and uses a multimodal soft-contrastive learning loss to align vision and language features.

Result: TRUST outperforms prior methods, achieving state-of-the-art on DomainNet and GeoNet benchmarks.

Conclusion: TRUST effectively leverages language modality to enhance UDA performance, particularly for complex domain shifts.

Abstract: Recent unsupervised domain adaptation (UDA) methods have shown great success
in addressing classical domain shifts (e.g., synthetic-to-real), but they still
suffer under complex shifts (e.g. geographical shift), where both the
background and object appearances differ significantly across domains. Prior
works showed that the language modality can help in the adaptation process,
exhibiting more robustness to such complex shifts. In this paper, we introduce
TRUST, a novel UDA approach that exploits the robustness of the language
modality to guide the adaptation of a vision model. TRUST generates
pseudo-labels for target samples from their captions and introduces a novel
uncertainty estimation strategy that uses normalised CLIP similarity scores to
estimate the uncertainty of the generated pseudo-labels. Such estimated
uncertainty is then used to reweight the classification loss, mitigating the
adverse effects of wrong pseudo-labels obtained from low-quality captions. To
further increase the robustness of the vision model, we propose a multimodal
soft-contrastive learning loss that aligns the vision and language feature
spaces, by leveraging captions to guide the contrastive training of the vision
model on target images. In our contrastive loss, each pair of images acts as
both a positive and a negative pair and their feature representations are
attracted and repulsed with a strength proportional to the similarity of their
captions. This solution avoids the need for hardly determining positive and
negative pairs, which is critical in the UDA setting. Our approach outperforms
previous methods, setting the new state-of-the-art on classical (DomainNet) and
complex (GeoNet) domain shifts. The code will be available upon acceptance.

</details>


### [96] [Text Embedded Swin-UMamba for DeepLesion Segmentation](https://arxiv.org/abs/2508.06453)
*Ruida Cheng,Tejas Sudharshan Mathai,Pritam Mukherjee,Benjamin Hou,Qingqing Zhu,Zhiyong Lu,Matthew McAuliffe,Ronald M. Summers*

Main category: cs.CV

TL;DR: The paper explores integrating large language models (LLMs) into lesion segmentation on CT scans, combining imaging and text data for improved accuracy.


<details>
  <summary>Details</summary>
Motivation: To enhance lesion segmentation by leveraging both imaging features and text descriptions from radiology reports.

Method: The study integrates text into the Swin-UMamba architecture, using the ULS23 DeepLesion dataset and report descriptions.

Result: Achieved a Dice Score of 82% and Hausdorff distance of 6.58 pixels, outperforming prior models like LanGuideMedSeg, xLSTM-UNet, and nnUNet.

Conclusion: The Text-Swin-UMamba model demonstrates feasibility and superior performance in lesion segmentation by combining imaging and text data.

Abstract: Segmentation of lesions on CT enables automatic measurement for clinical
assessment of chronic diseases (e.g., lymphoma). Integrating large language
models (LLMs) into the lesion segmentation workflow offers the potential to
combine imaging features with descriptions of lesion characteristics from the
radiology reports. In this study, we investigate the feasibility of integrating
text into the Swin-UMamba architecture for the task of lesion segmentation. The
publicly available ULS23 DeepLesion dataset was used along with short-form
descriptions of the findings from the reports. On the test dataset, a high Dice
Score of 82% and low Hausdorff distance of 6.58 (pixels) was obtained for
lesion segmentation. The proposed Text-Swin-UMamba model outperformed prior
approaches: 37% improvement over the LLM-driven LanGuideMedSeg model (p <
0.001),and surpassed the purely image-based xLSTM-UNet and nnUNet models by
1.74% and 0.22%, respectively. The dataset and code can be accessed at
https://github.com/ruida/LLM-Swin-UMamba

</details>


### [97] [WGAST: Weakly-Supervised Generative Network for Daily 10 m Land Surface Temperature Estimation via Spatio-Temporal Fusion](https://arxiv.org/abs/2508.06485)
*Sofiane Bouaziz,Adel Hafiane,Raphael Canals,Rachid Nedjai*

Main category: cs.CV

TL;DR: WGAST is a weakly-supervised generative network for daily 10 m Land Surface Temperature (LST) estimation, outperforming existing methods with improved accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: The increasing demand for precise environmental monitoring due to urbanization, climate change, and agricultural stress necessitates high-resolution LST data, which current remote sensing systems struggle to provide due to spatial-temporal resolution trade-offs.

Method: WGAST uses a conditional generative adversarial network with four stages: feature extraction, fusion, LST reconstruction, and noise suppression. It employs encoders, cosine similarity, normalization, temporal attention, and a Gaussian filter. Training is weakly supervised with a PatchGAN discriminator.

Result: WGAST reduces RMSE by 17.18% and improves SSIM by 11.00% compared to baselines. It is robust to cloud-induced LST and captures fine-scale thermal patterns, validated by 33 ground-based sensors.

Conclusion: WGAST is an effective deep learning framework for high-resolution LST estimation, addressing the limitations of existing methods and demonstrating superior performance.

Abstract: Urbanization, climate change, and agricultural stress are increasing the
demand for precise and timely environmental monitoring. Land Surface
Temperature (LST) is a key variable in this context and is retrieved from
remote sensing satellites. However, these systems face a trade-off between
spatial and temporal resolution. While spatio-temporal fusion methods offer
promising solutions, few have addressed the estimation of daily LST at 10 m
resolution. In this study, we present WGAST, a Weakly-Supervised Generative
Network for Daily 10 m LST Estimation via Spatio-Temporal Fusion of Terra
MODIS, Landsat 8, and Sentinel-2. WGAST is the first end-to-end deep learning
framework designed for this task. It adopts a conditional generative
adversarial architecture, with a generator composed of four stages: feature
extraction, fusion, LST reconstruction, and noise suppression. The first stage
employs a set of encoders to extract multi-level latent representations from
the inputs, which are then fused in the second stage using cosine similarity,
normalization, and temporal attention mechanisms. The third stage decodes the
fused features into high-resolution LST, followed by a Gaussian filter to
suppress high-frequency noise. Training follows a weakly supervised strategy
based on physical averaging principles and reinforced by a PatchGAN
discriminator. Experiments demonstrate that WGAST outperforms existing methods
in both quantitative and qualitative evaluations. Compared to the
best-performing baseline, on average, WGAST reduces RMSE by 17.18% and improves
SSIM by 11.00%. Furthermore, WGAST is robust to cloud-induced LST and
effectively captures fine-scale thermal patterns, as validated against 33
ground-based sensors. The code is available at
https://github.com/Sofianebouaziz1/WGAST.git.

</details>


### [98] [LightSwitch: Multi-view Relighting with Material-guided Diffusion](https://arxiv.org/abs/2508.06494)
*Yehonathan Litman,Fernando De la Torre,Shubham Tulsiani*

Main category: cs.CV

TL;DR: Lightswitch is a novel diffusion framework for 3D relighting that leverages multi-view and material cues, outperforming prior methods in speed and quality.


<details>
  <summary>Details</summary>
Motivation: Existing 2D relighting priors ignore intrinsic properties and multi-view data, leading to subpar results.

Method: Lightswitch uses a finetuned material-relighting diffusion framework with multi-view and material cues for scalable denoising.

Result: It achieves superior 2D relighting quality and matches or outperforms state-of-the-art methods in under 2 minutes.

Conclusion: Lightswitch efficiently and consistently relights diverse objects, advancing 3D relighting with inferred intrinsic properties.

Abstract: Recent approaches for 3D relighting have shown promise in integrating 2D
image relighting generative priors to alter the appearance of a 3D
representation while preserving the underlying structure. Nevertheless,
generative priors used for 2D relighting that directly relight from an input
image do not take advantage of intrinsic properties of the subject that can be
inferred or cannot consider multi-view data at scale, leading to subpar
relighting. In this paper, we propose Lightswitch, a novel finetuned
material-relighting diffusion framework that efficiently relights an arbitrary
number of input images to a target lighting condition while incorporating cues
from inferred intrinsic properties. By using multi-view and material
information cues together with a scalable denoising scheme, our method
consistently and efficiently relights dense multi-view data of objects with
diverse material compositions. We show that our 2D relighting prediction
quality exceeds previous state-of-the-art relighting priors that directly
relight from images. We further demonstrate that LightSwitch matches or
outperforms state-of-the-art diffusion inverse rendering methods in relighting
synthetic and real objects in as little as 2 minutes.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [99] [InfiGUI-G1: Advancing GUI Grounding with Adaptive Exploration Policy Optimization](https://arxiv.org/abs/2508.05731)
*Yuhang Liu,Zeyu Liu,Shuanghe Zhu,Pengxiang Li,Congkai Xie,Jiasheng Wang,Xueyu Hu,Xiaotian Han,Jianbo Yuan,Xinyao Wang,Shengyu Zhang,Hongxia Yang,Fei Wu*

Main category: cs.AI

TL;DR: AEPO improves semantic alignment in MLLMs for GUI tasks by addressing exploration inefficiencies, outperforming RLVR by 9.0%.


<details>
  <summary>Details</summary>
Motivation: Robust grounding of natural language instructions in GUIs requires precise spatial and semantic alignment, with current methods struggling due to inefficient exploration.

Method: Introduces Adaptive Exploration Policy Optimization (AEPO), using multi-answer generation and an Adaptive Exploration Reward (AER) function.

Result: AEPO-trained models (InfiGUI-G1-3B and InfiGUI-G1-7B) achieve state-of-the-art performance, with up to 9.0% improvement over RLVR.

Conclusion: AEPO effectively addresses exploration bottlenecks, enhancing semantic alignment in MLLMs for GUI tasks.

Abstract: The emergence of Multimodal Large Language Models (MLLMs) has propelled the
development of autonomous agents that operate on Graphical User Interfaces
(GUIs) using pure visual input. A fundamental challenge is robustly grounding
natural language instructions. This requires a precise spatial alignment, which
accurately locates the coordinates of each element, and, more critically, a
correct semantic alignment, which matches the instructions to the functionally
appropriate UI element. Although Reinforcement Learning with Verifiable Rewards
(RLVR) has proven to be effective at improving spatial alignment for these
MLLMs, we find that inefficient exploration bottlenecks semantic alignment,
which prevent models from learning difficult semantic associations. To address
this exploration problem, we present Adaptive Exploration Policy Optimization
(AEPO), a new policy optimization framework. AEPO employs a multi-answer
generation strategy to enforce broader exploration, which is then guided by a
theoretically grounded Adaptive Exploration Reward (AER) function derived from
first principles of efficiency eta=U/C. Our AEPO-trained models, InfiGUI-G1-3B
and InfiGUI-G1-7B, establish new state-of-the-art results across multiple
challenging GUI grounding benchmarks, achieving significant relative
improvements of up to 9.0% against the naive RLVR baseline on benchmarks
designed to test generalization and semantic understanding. Resources are
available at https://github.com/InfiXAI/InfiGUI-G1.

</details>


### [100] [A Framework for Inherently Safer AGI through Language-Mediated Active Inference](https://arxiv.org/abs/2508.05766)
*Bo Wen*

Main category: cs.AI

TL;DR: A novel framework for safe AGI combines Active Inference with LLMs, integrating safety into core design via transparent beliefs and hierarchical value alignment.


<details>
  <summary>Details</summary>
Motivation: Traditional AI safety methods (post-hoc interpretability, reward engineering) have limitations; this work aims for inherently safer AGI.

Method: Uses natural language for belief representation, multi-agent self-organization via Active Inference, and hierarchical Markov blankets for safety.

Result: Proposes mechanisms like belief-preference separation, bounded rationality, and modular agent structures for safety.

Conclusion: The framework offers a safer AGI development path, validated through experiments on the ARC benchmark.

Abstract: This paper proposes a novel framework for developing safe Artificial General
Intelligence (AGI) by combining Active Inference principles with Large Language
Models (LLMs). We argue that traditional approaches to AI safety, focused on
post-hoc interpretability and reward engineering, have fundamental limitations.
We present an architecture where safety guarantees are integrated into the
system's core design through transparent belief representations and
hierarchical value alignment. Our framework leverages natural language as a
medium for representing and manipulating beliefs, enabling direct human
oversight while maintaining computational tractability. The architecture
implements a multi-agent system where agents self-organize according to Active
Inference principles, with preferences and safety constraints flowing through
hierarchical Markov blankets. We outline specific mechanisms for ensuring
safety, including: (1) explicit separation of beliefs and preferences in
natural language, (2) bounded rationality through resource-aware free energy
minimization, and (3) compositional safety through modular agent structures.
The paper concludes with a research agenda centered on the Abstraction and
Reasoning Corpus (ARC) benchmark, proposing experiments to validate our
framework's safety properties. Our approach offers a path toward AGI
development that is inherently safer, rather than retrofitted with safety
measures.

</details>


### [101] [Whither symbols in the era of advanced neural networks?](https://arxiv.org/abs/2508.05776)
*Thomas L. Griffiths,Brenden M. Lake,R. Thomas McCoy,Ellie Pavlick,Taylor W. Webb*

Main category: cs.AI

TL;DR: Neural networks show abilities like human minds, challenging the symbolic systems view of cognition, but symbolic systems still play a role in defining problems for human thought.


<details>
  <summary>Details</summary>
Motivation: To challenge the idea that human cognition is purely symbolic by showing neural networks exhibit similar abilities.

Method: Analyzing the capabilities of modern neural networks and their parallels to human cognitive processes.

Result: Neural networks demonstrate abilities akin to human cognition, undermining the symbolic systems argument.

Conclusion: Proposes a new research agenda to explore the symbolic basis of human thought, acknowledging the role of symbolic systems in defining cognitive problems.

Abstract: Some of the strongest evidence that human minds should be thought about in
terms of symbolic systems has been the way they combine ideas, produce novelty,
and learn quickly. We argue that modern neural networks -- and the artificial
intelligence systems built upon them -- exhibit similar abilities. This
undermines the argument that the cognitive processes and representations used
by human minds are symbolic, although the fact that these neural networks are
typically trained on data generated by symbolic systems illustrates that such
systems play an important role in characterizing the abstract problems that
human minds have to solve. This argument leads us to offer a new agenda for
research on the symbolic basis of human thought.

</details>


### [102] [Holistic Explainable AI (H-XAI): Extending Transparency Beyond Developers in AI-Driven Decision Making](https://arxiv.org/abs/2508.05792)
*Kausik Lakkaraju,Siva Likitha Valluru,Biplav Srivastava*

Main category: cs.AI

TL;DR: H-XAI integrates causal rating with traditional XAI to provide interactive, stakeholder-specific explanations, addressing gaps in current XAI methods.


<details>
  <summary>Details</summary>
Motivation: Current XAI methods focus on justifying model outputs for developers, neglecting diverse stakeholder needs. Evaluative AI shifts toward hypothesis testing but remains operationally focused.

Method: H-XAI combines causal rating methods with traditional XAI, enabling interactive hypothesis testing and comparison against random and biased baselines. It supports instance-level and global explanations.

Result: Demonstrated in two case studies (credit risk classification and financial forecasting), H-XAI effectively answers stakeholder questions at individual and model levels.

Conclusion: H-XAI bridges gaps in XAI by offering a unified, adaptable framework for diverse stakeholder needs, enhancing understanding and evaluation of model behavior.

Abstract: Current eXplainable AI (XAI) methods largely serve developers, often focusing
on justifying model outputs rather than supporting diverse stakeholder needs. A
recent shift toward Evaluative AI reframes explanation as a tool for hypothesis
testing, but still focuses primarily on operational organizations. We introduce
Holistic-XAI (H-XAI), a unified framework that integrates causal rating methods
with traditional XAI methods to support explanation as an interactive,
multi-method process. H-XAI allows stakeholders to ask a series of questions,
test hypotheses, and compare model behavior against automatically constructed
random and biased baselines. It combines instance-level and global
explanations, adapting to each stakeholder's goals, whether understanding
individual decisions, assessing group-level bias, or evaluating robustness
under perturbations. We demonstrate the generality of our approach through two
case studies spanning six scenarios: binary credit risk classification and
financial time-series forecasting. H-XAI fills critical gaps left by existing
XAI methods by combining causal ratings and post-hoc explanations to answer
stakeholder-specific questions at both the individual decision level and the
overall model level.

</details>


### [103] [Safety of Embodied Navigation: A Survey](https://arxiv.org/abs/2508.05855)
*Zixia Wang,Jia Hu,Ronghui Mu*

Main category: cs.AI

TL;DR: A survey on safety in embodied navigation, covering attacks, defenses, and evaluation methods, with future research directions.


<details>
  <summary>Details</summary>
Motivation: Address safety concerns in embodied navigation due to its critical applications in dynamic environments.

Method: Comprehensive analysis of existing safety challenges, mitigation technologies, datasets, and metrics.

Result: Identifies gaps in attack methods, defenses, evaluation techniques, and verification frameworks.

Conclusion: Aims to guide future research for safer embodied navigation systems, benefiting societal safety and industrial efficiency.

Abstract: As large language models (LLMs) continue to advance and gain influence, the
development of embodied AI has accelerated, drawing significant attention,
particularly in navigation scenarios. Embodied navigation requires an agent to
perceive, interact with, and adapt to its environment while moving toward a
specified target in unfamiliar settings. However, the integration of embodied
navigation into critical applications raises substantial safety concerns. Given
their deployment in dynamic, real-world environments, ensuring the safety of
such systems is critical. This survey provides a comprehensive analysis of
safety in embodied navigation from multiple perspectives, encompassing attack
strategies, defense mechanisms, and evaluation methodologies. Beyond conducting
a comprehensive examination of existing safety challenges, mitigation
technologies, and various datasets and metrics that assess effectiveness and
robustness, we explore unresolved issues and future research directions in
embodied navigation safety. These include potential attack methods, mitigation
strategies, more reliable evaluation techniques, and the implementation of
verification frameworks. By addressing these critical gaps, this survey aims to
provide valuable insights that can guide future research toward the development
of safer and more reliable embodied navigation systems. Furthermore, the
findings of this study have broader implications for enhancing societal safety
and increasing industrial efficiency.

</details>


### [104] [Planning Agents on an Ego-Trip: Leveraging Hybrid Ego-Graph Ensembles for Improved Tool Retrieval in Enterprise Task Planning](https://arxiv.org/abs/2508.05888)
*Sahil Bansal,Sai Shruthi Sistla,Aarti Arikatala,Sebastian Schreiber*

Main category: cs.AI

TL;DR: The paper proposes a Knowledge Graph (KG)-based tool retrieval framework to improve accuracy in selecting tools for multi-step user queries, outperforming traditional similarity-based methods.


<details>
  <summary>Details</summary>
Motivation: Effective tool retrieval is crucial for AI agents handling complex user queries, but current methods relying on query-tool similarity are limited, especially for multi-step tasks.

Method: The framework uses KG-based retrieval, leveraging ensembles of 1-hop ego tool graphs to model tool relationships and dependencies for contextual selection.

Result: The KG-based method achieves 91.85% tool coverage (Complete Recall), outperforming the 89.26% of the strongest non-KG baseline.

Conclusion: Structural information in KGs complements similarity matching, enhancing tool retrieval for sequential tasks.

Abstract: Effective tool retrieval is essential for AI agents to select from a vast
array of tools when identifying and planning actions in the context of complex
user queries. Despite its central role in planning, this aspect remains
underexplored in the literature. Traditional approaches rely primarily on
similarities between user queries and tool descriptions, which significantly
limits retrieval accuracy, specifically when handling multi-step user requests.
To address these limitations, we propose a Knowledge Graph (KG)-based tool
retrieval framework that captures the semantic relationships between tools and
their functional dependencies. Our retrieval algorithm leverages ensembles of
1-hop ego tool graphs to model direct and indirect connections between tools,
enabling more comprehensive and contextual tool selection for multi-step tasks.
We evaluate our approach on a synthetically generated internal dataset across
six defined user classes, extending previous work on coherent dialogue
synthesis and too retrieval benchmarks. Results demonstrate that our tool
graph-based method achieves 91.85% tool coverage on the micro-average Complete
Recall metric, compared to 89.26% for re-ranked semantic-lexical hybrid
retrieval, the strongest non-KG baseline in our experiments. These findings
support our hypothesis that the structural information in the KG provides
complementary signals to pure similarity matching, particularly for queries
requiring sequential tool composition.

</details>


### [105] [Mediator-Guided Multi-Agent Collaboration among Open-Source Models for Medical Decision-Making](https://arxiv.org/abs/2508.05996)
*Kaitao Chen,Mianxin Liu,Daoming Zong,Chaoyue Ding,Shaohao Rui,Yankai Jiang,Mu Zhou,Xiaosong Wang*

Main category: cs.AI

TL;DR: MedOrch is a mediator-guided multi-agent framework for medical multimodal decision-making, using LLM-based mediators to enhance collaboration among VLM-based expert agents.


<details>
  <summary>Details</summary>
Motivation: Existing multi-agent systems struggle with multimodal tasks and lack self-reflection, limiting their use in medical workflows.

Method: Proposes MedOrch, leveraging an LLM-based mediator to coordinate diverse VLM-based agents for collaborative decision-making.

Result: Outperforms individual agents on medical benchmarks without training, showcasing effective collaboration.

Conclusion: Mediator-guided collaboration enhances medical multimodal intelligence, with potential for broader applications.

Abstract: Complex medical decision-making involves cooperative workflows operated by
different clinicians. Designing AI multi-agent systems can expedite and augment
human-level clinical decision-making. Existing multi-agent researches primarily
focus on language-only tasks, yet their extension to multimodal scenarios
remains challenging. A blind combination of diverse vision-language models
(VLMs) can amplify an erroneous outcome interpretation. VLMs in general are
less capable in instruction following and importantly self-reflection, compared
to large language models (LLMs) of comparable sizes. This disparity largely
constrains VLMs' ability in cooperative workflows. In this study, we propose
MedOrch, a mediator-guided multi-agent collaboration framework for medical
multimodal decision-making. MedOrch employs an LLM-based mediator agent that
enables multiple VLM-based expert agents to exchange and reflect on their
outputs towards collaboration. We utilize multiple open-source general-purpose
and domain-specific VLMs instead of costly GPT-series models, revealing the
strength of heterogeneous models. We show that the collaboration within
distinct VLM-based agents can surpass the capabilities of any individual agent.
We validate our approach on five medical vision question answering benchmarks,
demonstrating superior collaboration performance without model training. Our
findings underscore the value of mediator-guided multi-agent collaboration in
advancing medical multimodal intelligence. Our code will be made publicly
available.

</details>


### [106] [Society of Mind Meets Real-Time Strategy: A Hierarchical Multi-Agent Framework for Strategic Reasoning](https://arxiv.org/abs/2508.06042)
*Daechul Ahn,San Kim,Jonghyun Choi*

Main category: cs.AI

TL;DR: Proposes HIMA, a hierarchical multi-agent framework for dynamic tasks like StarCraftII, combining specialized imitation learning agents under a meta-controller for adaptive planning.


<details>
  <summary>Details</summary>
Motivation: Existing LLMs struggle with dynamic, long-horizon tasks like StarCraftII due to resource constraints and partial observability.

Method: Uses hierarchical multi-agent framework (HIMA) with specialized imitation learning agents and a meta-controller (SP) for adaptive planning.

Result: HIMA outperforms state-of-the-art methods in strategic clarity, adaptability, and efficiency.

Conclusion: Combining specialized imitation modules with meta-level orchestration enhances robustness in general-purpose AI agents.

Abstract: Large Language Models (LLMs) have recently demonstrated impressive action
sequence prediction capabilities but often struggle with dynamic, long-horizon
tasks such as real-time strategic games. In a game such as StarCraftII (SC2),
agents need to manage resource constraints and adapt to evolving battlefield
situations in a partially observable environment. This often overwhelms
exisiting LLM-based approaches. To address these challenges, we propose a
hierarchical multi-agent framework that employs specialized imitation learning
agents under a meta-controller called Strategic Planner (SP). By expert
demonstrations, each specialized agent learns a distinctive strategy, such as
aerial support or defensive maneuvers, and produces coherent, structured
multistep action sequences. The SP then orchestrates these proposals into a
single, environmentally adaptive plan that ensures local decisions aligning
with long-term strategies. We call this HIMA (Hierarchical Imitation
Multi-Agent). We also present TEXTSCII-ALL, a comprehensive SC2 testbed that
encompasses all race match combinations in SC2. Our empirical results show that
HIMA outperforms state of the arts in strategic clarity, adaptability, and
computational efficiency, underscoring the potential of combining specialized
imitation modules with meta-level orchestration to develop more robust,
general-purpose AI agents.

</details>


### [107] [LLMs for Resource Allocation: A Participatory Budgeting Approach to Inferring Preferences](https://arxiv.org/abs/2508.06060)
*Sankarshan Damle,Boi Faltings*

Main category: cs.AI

TL;DR: A framework uses Participatory Budgeting (PB) to evaluate LLMs' resource allocation and reasoning, testing preference inference from natural-language inputs.


<details>
  <summary>Details</summary>
Motivation: To explore LLMs' capability in structured resource allocation and reasoning, addressing gaps in evaluation due to data contamination and static benchmarks.

Method: Three prompting strategies (greedy selection, direct optimization, hill-climbing refinement) are used to allocate resources under constraints, comparing LLM outputs to an oracle. Preference inference from natural-language inputs is also tested.

Result: Prompt design significantly impacts performance. LLMs show potential for mechanism design with unstructured inputs, though preference inference accuracy varies.

Conclusion: LLMs demonstrate promise for resource allocation and reasoning tasks, with adaptive benchmarks like PB offering robust evaluation.

Abstract: Large Language Models (LLMs) are increasingly expected to handle complex
decision-making tasks, yet their ability to perform structured resource
allocation remains underexplored. Evaluating their reasoning is also difficult
due to data contamination and the static nature of existing benchmarks. We
present a dual-purpose framework leveraging Participatory Budgeting (PB) both
as (i) a practical setting for LLM-based resource allocation and (ii) an
adaptive benchmark for evaluating their reasoning capabilities. We task LLMs
with selecting project subsets under feasibility (e.g., budget) constraints via
three prompting strategies: greedy selection, direct optimization, and a
hill-climbing-inspired refinement. We benchmark LLMs' allocations against a
utility-maximizing oracle. Interestingly, we also test whether LLMs can infer
structured preferences from natural-language voter input or metadata, without
explicit votes. By comparing allocations based on inferred preferences to those
from ground-truth votes, we evaluate LLMs' ability to extract preferences from
open-ended input. Our results underscore the role of prompt design and show
that LLMs hold promise for mechanism design with unstructured inputs.

</details>


### [108] [Don't Forget Imagination!](https://arxiv.org/abs/2508.06062)
*Evgenii E. Vityaev,Andrei Mantsivoda*

Main category: cs.AI

TL;DR: The paper highlights the underestimated role of cognitive imagination in AI, proposing semantic models as a tool to simulate it for improved reasoning and decision-making.


<details>
  <summary>Details</summary>
Motivation: Cognitive imagination is crucial for human reasoning but is undervalued in AI, limiting its capabilities. The paper advocates for its integration to advance AI.

Method: The authors propose semantic models, which learn like neural networks but use probabilistic causal relationships to ensure consistent and manipulable imaginary contexts.

Result: Semantic models can simulate cognitive imagination by maintaining coherent, holistic systems of concepts and causal links, enabling better reasoning.

Conclusion: The paper calls for prioritizing cognitive imagination in AI research, with semantic models as a promising tool for achieving this breakthrough.

Abstract: Cognitive imagination is a type of imagination that plays a key role in human
thinking. It is not a ``picture-in-the-head'' imagination. It is a faculty to
mentally visualize coherent and holistic systems of concepts and causal links
that serve as semantic contexts for reasoning, decision making and prediction.
Our position is that the role of cognitive imagination is still greatly
underestimated, and this creates numerous problems and diminishes the current
capabilities of AI. For instance, when reasoning, humans rely on imaginary
contexts to retrieve background info. They also constantly return to the
context for semantic verification that their reasoning is still reasonable.
Thus, reasoning without imagination is blind. This paper is a call for greater
attention to cognitive imagination as the next promising breakthrough in
artificial intelligence. As an instrument for simulating cognitive imagination,
we propose semantic models -- a new approach to mathematical models that can
learn, like neural networks, and are based on probabilistic causal
relationships. Semantic models can simulate cognitive imagination because they
ensure the consistency of imaginary contexts and implement a glass-box approach
that allows the context to be manipulated as a holistic and coherent system of
interrelated facts glued together with causal relations.

</details>


### [109] [A Generic Complete Anytime Beam Search for Optimal Decision Tree](https://arxiv.org/abs/2508.06064)
*Harold Silv√®re Kiossou,Siegfried Nijssen,Pierre Schaus*

Main category: cs.AI

TL;DR: CA-DL8.5 is a generic, anytime beam search algorithm for optimal decision tree learning, unifying existing strategies and outperforming others in anytime performance.


<details>
  <summary>Details</summary>
Motivation: Existing exact methods for optimal decision tree learning lack balanced anytime behavior, and their extensions have not been systematically compared.

Method: CA-DL8.5 extends DL8.5 with modular heuristics and relaxation mechanisms, using branch-and-bound pruning and trie-based caching with restart-based beam search.

Result: CA-DL8.5 with LDS heuristics outperforms other variants and Blossom in anytime performance while ensuring completeness and optimality.

Conclusion: CA-DL8.5 provides a flexible, effective framework for anytime decision tree learning, with LDS heuristics delivering the best performance.

Abstract: Finding an optimal decision tree that minimizes classification error is known
to be NP-hard. While exact algorithms based on MILP, CP, SAT, or dynamic
programming guarantee optimality, they often suffer from poor anytime behavior
-- meaning they struggle to find high-quality decision trees quickly when the
search is stopped before completion -- due to unbalanced search space
exploration. To address this, several anytime extensions of exact methods have
been proposed, such as LDS-DL8.5, Top-k-DL8.5, and Blossom, but they have not
been systematically compared, making it difficult to assess their relative
effectiveness. In this paper, we propose CA-DL8.5, a generic, complete, and
anytime beam search algorithm that extends the DL8.5 framework and unifies some
existing anytime strategies. In particular, CA-DL8.5 generalizes previous
approaches LDS-DL8.5 and Top-k-DL8.5, by allowing the integration of various
heuristics and relaxation mechanisms through a modular design. The algorithm
reuses DL8.5's efficient branch-and-bound pruning and trie-based caching,
combined with a restart-based beam search that gradually relaxes pruning
criteria to improve solution quality over time. Our contributions are twofold:
(1) We introduce this new generic framework for exact and anytime decision tree
learning, enabling the incorporation of diverse heuristics and search
strategies; (2) We conduct a rigorous empirical comparison of several
instantiations of CA-DL8.5 -- based on Purity, Gain, Discrepancy, and Top-k
heuristics -- using an anytime evaluation metric called the primal gap
integral. Experimental results on standard classification benchmarks show that
CA-DL8.5 using LDS (limited discrepancy) consistently provides the best anytime
performance, outperforming both other CA-DL8.5 variants and the Blossom
algorithm while maintaining completeness and optimality guarantees.

</details>


### [110] [ME$^3$-BEV: Mamba-Enhanced Deep Reinforcement Learning for End-to-End Autonomous Driving with BEV-Perception](https://arxiv.org/abs/2508.06074)
*Siyi Lu,Run Liu,Dongsheng Yang,Lei He*

Main category: cs.AI

TL;DR: The paper introduces a deep reinforcement learning (DRL) approach for autonomous driving, combining bird's-eye view (BEV) perception with the Mamba framework for efficient spatio-temporal feature extraction. The proposed ME¬≥-BEV framework outperforms existing models in dynamic urban scenarios.


<details>
  <summary>Details</summary>
Motivation: Autonomous driving systems struggle with complex environment perception and real-time decision-making. Traditional modular approaches have error propagation issues, while end-to-end learning faces computational bottlenecks.

Method: The paper presents the Mamba-BEV model for BEV-based perception and temporal feature modeling, integrated into the ME¬≥-BEV framework for end-to-end DRL. Interpretability is enhanced via semantic segmentation visualization.

Result: Experiments on the CARLA simulator show ME¬≥-BEV outperforms existing models in collision rate and trajectory accuracy.

Conclusion: The ME¬≥-BEV framework offers a promising solution for real-time autonomous driving by combining efficient perception and decision-making.

Abstract: Autonomous driving systems face significant challenges in perceiving complex
environments and making real-time decisions. Traditional modular approaches,
while offering interpretability, suffer from error propagation and coordination
issues, whereas end-to-end learning systems can simplify the design but face
computational bottlenecks. This paper presents a novel approach to autonomous
driving using deep reinforcement learning (DRL) that integrates bird's-eye view
(BEV) perception for enhanced real-time decision-making. We introduce the
\texttt{Mamba-BEV} model, an efficient spatio-temporal feature extraction
network that combines BEV-based perception with the Mamba framework for
temporal feature modeling. This integration allows the system to encode vehicle
surroundings and road features in a unified coordinate system and accurately
model long-range dependencies. Building on this, we propose the
\texttt{ME$^3$-BEV} framework, which utilizes the \texttt{Mamba-BEV} model as a
feature input for end-to-end DRL, achieving superior performance in dynamic
urban driving scenarios. We further enhance the interpretability of the model
by visualizing high-dimensional features through semantic segmentation,
providing insight into the learned representations. Extensive experiments on
the CARLA simulator demonstrate that \texttt{ME$^3$-BEV} outperforms existing
models across multiple metrics, including collision rate and trajectory
accuracy, offering a promising solution for real-time autonomous driving.

</details>


### [111] [Aggregate-Combine-Readout GNNs Are More Expressive Than Logic C2](https://arxiv.org/abs/2508.06091)
*Stan P Hauke,Przemys≈Çaw Andrzej Wa≈Çƒôga*

Main category: cs.AI

TL;DR: The paper resolves an open problem by proving that aggregate-combine-readout GNNs' logical expressiveness exceeds that of C2, providing insights into infinitary logics.


<details>
  <summary>Details</summary>
Motivation: To address the unresolved question of whether full C2 characterises the logical expressiveness of aggregate-combine-readout GNNs, as posed by Barcel√≥ et al. (2020).

Method: The authors prove the logical expressiveness of aggregate-combine-readout GNNs strictly exceeds C2, analyzing undirected and directed graphs.

Result: The study shows that aggregate-combine-readout GNNs surpass C2 in logical expressiveness, with implications for infinitary logics.

Conclusion: The work resolves the open problem and advances understanding of GNNs' expressiveness and infinitary logics.

Abstract: In recent years, there has been growing interest in understanding the
expressive power of graph neural networks (GNNs) by relating them to logical
languages. This research has been been initialised by an influential result of
Barcel\'o et al. (2020), who showed that the graded modal logic (or a guarded
fragment of the logic C2), characterises the logical expressiveness of
aggregate-combine GNNs. As a ``challenging open problem'' they left the
question whether full C2 characterises the logical expressiveness of
aggregate-combine-readout GNNs. This question has remained unresolved despite
several attempts. In this paper, we solve the above open problem by proving
that the logical expressiveness of aggregate-combine-readout GNNs strictly
exceeds that of C2. This result holds over both undirected and directed graphs.
Beyond its implications for GNNs, our work also leads to purely logical
insights on the expressive power of infinitary logics.

</details>


### [112] [PanelTR: Zero-Shot Table Reasoning Framework Through Multi-Agent Scientific Discussion](https://arxiv.org/abs/2508.06110)
*Yiran Rex Ma*

Main category: cs.AI

TL;DR: PanelTR, a framework using LLM agent scientists, improves table reasoning without relying on annotated data or complex augmentation, outperforming vanilla LLMs and matching supervised models.


<details>
  <summary>Details</summary>
Motivation: Address limitations in table reasoning (e.g., dependency on annotated data, poor LLM performance) by introducing a structured scientific approach.

Method: PanelTR employs agent scientists for individual investigations, self-review, and peer-review discussions, leveraging five personas for semantic-level transfer.

Result: Outperforms vanilla LLMs and rivals supervised models on four benchmarks, without training data.

Conclusion: Structured scientific methodology enables flexible, zero-shot semantic understanding for complex tasks like table reasoning.

Abstract: Table reasoning, including tabular QA and fact verification, often depends on
annotated data or complex data augmentation, limiting flexibility and
generalization. LLMs, despite their versatility, often underperform compared to
simple supervised models. To approach these issues, we introduce PanelTR, a
framework utilizing LLM agent scientists for robust table reasoning through a
structured scientific approach. PanelTR's workflow involves agent scientists
conducting individual investigations, engaging in self-review, and
participating in collaborative peer-review discussions. This process, driven by
five scientist personas, enables semantic-level transfer without relying on
data augmentation or parametric optimization. Experiments across four
benchmarks show that PanelTR outperforms vanilla LLMs and rivals fully
supervised models, all while remaining independent of training data. Our
findings indicate that structured scientific methodology can effectively handle
complex tasks beyond table reasoning with flexible semantic understanding in a
zero-shot context.

</details>


### [113] [SKATE, a Scalable Tournament Eval: Weaker LLMs differentiate between stronger ones using verifiable challenges](https://arxiv.org/abs/2508.06111)
*Dewi S. W. Gould,Bruno Mlodozeniec,Samuel F. Brown*

Main category: cs.AI

TL;DR: SKATE is an automated, scalable evaluation framework where LLMs generate and solve tasks for each other, enabling objective and open-ended assessment of model capabilities.


<details>
  <summary>Details</summary>
Motivation: Current evaluation methods for foundation models require extensive expertise and lack scalability, hindering their ability to keep pace with rapid model advancements.

Method: SKATE treats evaluation as a game: LLMs act as both task-setters and solvers, creating verifiable tasks that highlight strengths and expose weaknesses. It uses a TrueSkill-based ranking system for objective scoring.

Result: The framework successfully differentiates model capabilities, reveals self-preferencing behavior, and surfaces fine-grained differences between models.

Conclusion: SKATE represents a scalable, automated solution for evaluating LLMs, addressing limitations of current methods and adapting to rapid model evolution.

Abstract: Evaluating the capabilities and risks of foundation models is paramount, yet
current methods demand extensive domain expertise, hindering their scalability
as these models rapidly evolve. We introduce SKATE: a novel evaluation
framework in which large language models (LLMs) compete by generating and
solving verifiable tasks for one another. Our core insight is to treat
evaluation as a game: models act as both task-setters and solvers, incentivized
to create questions which highlight their own strengths while exposing others'
weaknesses. SKATE offers several key advantages, balancing scalability,
open-endedness, and objectivity. It is fully automated, data-free, and
scalable, requiring no human input or domain expertise. By using verifiable
tasks rather than LLM judges, scoring is objective. Unlike domain-limited
programmatically-generated benchmarks (e.g. chess-playing or spatial
reasoning), having LLMs creatively pose challenges enables open-ended and
scalable evaluation. As a proof of concept, we introduce LLM-set
code-output-prediction (COP) challenges as a verifiable and extensible
framework in which to test our approach. Using a TrueSkill-based ranking
system, we evaluate six frontier LLMs and find that: (1) weaker models can
reliably differentiate and score stronger ones, (2) LLM-based systems are
capable of self-preferencing behavior, generating questions that align with
their own capabilities, and (3) SKATE automatically surfaces fine-grained
capability differences between models. Our findings are an important step
towards general, scalable evaluation frameworks which can keep pace with LLM
progress.

</details>


### [114] [Study of Robust Features in Formulating Guidance for Heuristic Algorithms for Solving the Vehicle Routing Problem](https://arxiv.org/abs/2508.06129)
*Bachtiar Herdianto,Romain Billot,Flavien Lucas,Marc Sevaux*

Main category: cs.AI

TL;DR: The paper explores using machine learning and explainable AI to analyze feature importance in metaheuristic algorithms for solving the Vehicle Routing Problem (VRP), proposing a unified framework for ranking impactful features.


<details>
  <summary>Details</summary>
Motivation: To enhance the design of metaheuristic algorithms for VRP by leveraging machine learning to understand and predict solution quality, moving beyond traditional human-crafted designs.

Method: Conducts a sensitivity analysis using multiple classifier models to predict VRP solution quality and employs explainable AI to interpret model decisions.

Result: Feature importance varies, but certain features consistently predict solution quality. A unified framework ranks feature impact across scenarios.

Conclusion: Feature importance analysis can guide metaheuristic algorithm design for VRP, offering a foundation for more efficient solutions.

Abstract: The Vehicle Routing Problem (VRP) is a complex optimization problem with
numerous real-world applications, mostly solved using metaheuristic algorithms
due to its $\mathcal{NP}$-Hard nature. Traditionally, these metaheuristics rely
on human-crafted designs developed through empirical studies. However, recent
research shows that machine learning methods can be used the structural
characteristics of solutions in combinatorial optimization, thereby aiding in
designing more efficient algorithms, particularly for solving VRP. Building on
this advancement, this study extends the previous research by conducting a
sensitivity analysis using multiple classifier models that are capable of
predicting the quality of VRP solutions. Hence, by leveraging explainable AI,
this research is able to extend the understanding of how these models make
decisions. Finally, our findings indicate that while feature importance varies,
certain features consistently emerge as strong predictors. Furthermore, we
propose a unified framework able of ranking feature impact across different
scenarios to illustrate this finding. These insights highlight the potential of
feature importance analysis as a foundation for developing a guidance mechanism
of metaheuristic algorithms for solving the VRP.

</details>


### [115] [Retrieval Augmented Large Language Model System for Comprehensive Drug Contraindications](https://arxiv.org/abs/2508.06145)
*Byeonghun Bang,Jongsuk Yoon,Dong-Jin Chang,Seho Park,Yong Oh Lee*

Main category: cs.AI

TL;DR: The study improves LLMs for healthcare by using a RAG pipeline with GPT-4o-mini and text-embedding-3-small, significantly boosting accuracy in drug contraindication tasks.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of accurate drug contraindication information in healthcare using LLMs.

Method: Implemented a RAG pipeline with GPT-4o-mini, text-embedding-3-small embeddings, and Langchain for hybrid retrieval and re-ranking, using DUR data.

Result: Accuracy improved from 0.49-0.57 to 0.94, 0.87, and 0.89 for age groups, pregnancy, and concomitant drug use, respectively.

Conclusion: RAG-augmented LLMs enhance precision in drug contraindication information, reducing prescription uncertainty.

Abstract: The versatility of large language models (LLMs) has been explored across
various sectors, but their application in healthcare poses challenges,
particularly in the domain of pharmaceutical contraindications where accurate
and reliable information is required. This study enhances the capability of
LLMs to address contraindications effectively by implementing a Retrieval
Augmented Generation (RAG) pipeline. Utilizing OpenAI's GPT-4o-mini as the base
model, and the text-embedding-3-small model for embeddings, our approach
integrates Langchain to orchestrate a hybrid retrieval system with re-ranking.
This system leverages Drug Utilization Review (DUR) data from public databases,
focusing on contraindications for specific age groups, pregnancy, and
concomitant drug use. The dataset includes 300 question-answer pairs across
three categories, with baseline model accuracy ranging from 0.49 to 0.57.
Post-integration of the RAG pipeline, we observed a significant improvement in
model accuracy, achieving rates of 0.94, 0.87, and 0.89 for contraindications
related to age groups, pregnancy, and concomitant drug use, respectively. The
results indicate that augmenting LLMs with a RAG framework can substantially
reduce uncertainty in prescription and drug intake decisions by providing more
precise and reliable drug contraindication information.

</details>


### [116] [Overconfidence in LLM-as-a-Judge: Diagnosis and Confidence-Driven Solution](https://arxiv.org/abs/2508.06225)
*Zailong Tian,Zhuoheng Han,Yanzhe Chen,Haozhe Xu,Xi Yang,richeng xuan,Hongfeng Wang,Lizi Liao*

Main category: cs.AI

TL;DR: The paper advocates for confidence-driven, risk-aware LLM-as-a-Judge systems, addressing the overconfidence issue in current models and introducing TH-Score and LLM-as-a-Fuser for improved reliability.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-as-a-Judge systems focus on accuracy but lack well-calibrated confidence, leading to unreliable judgments. The paper highlights the Overconfidence Phenomenon and its risks.

Method: Introduces TH-Score to measure confidence-accuracy alignment and proposes LLM-as-a-Fuser, an ensemble framework for risk-aware evaluation.

Result: The approach improves calibration and reliability, outperforming existing baselines in adaptive, confidence-driven evaluation.

Conclusion: Shifting to confidence-driven systems with well-calibrated confidence enhances the trustworthiness and adaptability of LLM-as-a-Judge evaluations.

Abstract: Large Language Models (LLMs) are widely used as automated judges, where
practical value depends on both accuracy and trustworthy, risk-aware judgments.
Existing approaches predominantly focus on accuracy, overlooking the necessity
of well-calibrated confidence, which is vital for adaptive and reliable
evaluation pipelines. In this work, we advocate a shift from accuracy-centric
evaluation to confidence-driven, risk-aware LLM-as-a-Judge systems, emphasizing
the necessity of well-calibrated confidence for trustworthy and adaptive
evaluation. We systematically identify the **Overconfidence Phenomenon** in
current LLM-as-a-Judges, where predicted confidence significantly overstates
actual correctness, undermining reliability in practical deployment. To
quantify this phenomenon, we introduce **TH-Score**, a novel metric measuring
confidence-accuracy alignment. Furthermore, we propose **LLM-as-a-Fuser**, an
ensemble framework that transforms LLMs into reliable, risk-aware evaluators.
Extensive experiments demonstrate that our approach substantially improves
calibration and enables adaptive, confidence-driven evaluation pipelines,
achieving superior reliability and accuracy compared to existing baselines.

</details>


### [117] [GeoLaux: A Benchmark for Evaluating MLLMs' Geometry Performance on Long-Step Problems Requiring Auxiliary Lines](https://arxiv.org/abs/2508.06226)
*Yumeng Fu,Jiayin Zhu,Lingling Zhang,Bo Zhao,Shaoxuan Ma,Yushun Zhang,Yanrui Wu,Wenjun Wu*

Main category: cs.AI

TL;DR: The GeoLaux benchmark evaluates MLLMs' geometry problem-solving skills, focusing on auxiliary line construction and long-step reasoning, revealing performance gaps and potential improvements.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for MLLMs overlook auxiliary line construction and fine-grained process evaluation, limiting their ability to assess long-step reasoning in geometry.

Method: The GeoLaux benchmark includes 2,186 geometry problems, with a novel five-dimensional evaluation strategy assessing correctness, process quality, and auxiliary line impact.

Result: Experiments on 13 MLLMs show performance degradation in extended reasoning, shortcut tendencies in proving problems, and a lack of auxiliary line awareness.

Conclusion: GeoLaux serves as a benchmark for evaluating and improving MLLMs' long-step geometric reasoning, particularly in auxiliary line construction.

Abstract: Geometry problem solving (GPS) requires models to master diagram
comprehension, logical reasoning, knowledge application, numerical computation,
and auxiliary line construction. This presents a significant challenge for
Multimodal Large Language Models (MLLMs). However, existing benchmarks for
evaluating MLLM geometry skills overlook auxiliary line construction and lack
fine-grained process evaluation, making them insufficient for assessing MLLMs'
long-step reasoning abilities. To bridge these gaps, we present the GeoLaux
benchmark, comprising 2,186 geometry problems, incorporating both calculation
and proving questions. Notably, the problems require an average of 6.51
reasoning steps, with a maximum of 24 steps, and 41.8% of them need auxiliary
line construction. Building on the dataset, we design a novel five-dimensional
evaluation strategy assessing answer correctness, process correctness, process
quality, auxiliary line impact, and error causes. Extensive experiments on 13
leading MLLMs (including thinking models and non-thinking models) yield three
pivotal findings: First, models exhibit substantial performance degradation in
extended reasoning steps (nine models demonstrate over 50% performance drop).
Second, compared to calculation problems, MLLMs tend to take shortcuts when
solving proving problems. Third, models lack auxiliary line awareness, and
enhancing this capability proves particularly beneficial for overall geometry
reasoning improvement. These findings establish GeoLaux as both a benchmark for
evaluating MLLMs' long-step geometric reasoning with auxiliary lines and a
guide for capability advancement. Our dataset and code are included in
supplementary materials and will be released.

</details>


### [118] [Learning Logical Rules using Minimum Message Length](https://arxiv.org/abs/2508.06230)
*Ruben Sharma,Sebastijan Dumanƒçiƒá,Ross D. King,Andrew Cropper*

Main category: cs.AI

TL;DR: A Bayesian inductive logic programming approach unifies probabilistic and logical learning, outperforming previous methods by balancing hypothesis complexity and data fit.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of unifying probabilistic and logical learning in AI.

Method: Uses Bayesian inductive logic programming with priors favoring general programs and a likelihood favoring accuracy, learning minimum message length programs from noisy data.

Result: Outperforms previous methods, especially those learning minimum description length programs, and shows data efficiency and insensitivity to example balance.

Conclusion: The approach is effective, data-efficient, and robust, capable of learning from exclusively positive examples.

Abstract: Unifying probabilistic and logical learning is a key challenge in AI. We
introduce a Bayesian inductive logic programming approach that learns minimum
message length programs from noisy data. Our approach balances hypothesis
complexity and data fit through priors, which explicitly favour more general
programs, and a likelihood that favours accurate programs. Our experiments on
several domains, including game playing and drug design, show that our method
significantly outperforms previous methods, notably those that learn minimum
description length programs. Our results also show that our approach is
data-efficient and insensitive to example balance, including the ability to
learn from exclusively positive examples.

</details>


### [119] [Symmetry breaking for inductive logic programming](https://arxiv.org/abs/2508.06263)
*Andrew Cropper,David M. Cerna,Matti J√§rvisalo*

Main category: cs.AI

TL;DR: A method to break symmetries in inductive logic programming, reducing solving times significantly.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of searching vast and logically equivalent hypothesis spaces in inductive logic programming.

Method: Introducing a symmetry-breaking technique implemented in answer set programming.

Result: Experiments show solving times reduced from over an hour to 17 seconds across domains like visual reasoning and game playing.

Conclusion: The symmetry-breaking method effectively improves efficiency in inductive logic programming.

Abstract: The goal of inductive logic programming is to search for a hypothesis that
generalises training data and background knowledge. The challenge is searching
vast hypothesis spaces, which is exacerbated because many logically equivalent
hypotheses exist. To address this challenge, we introduce a method to break
symmetries in the hypothesis space. We implement our idea in answer set
programming. Our experiments on multiple domains, including visual reasoning
and game playing, show that our approach can reduce solving times from over an
hour to just 17 seconds.

</details>


### [120] [LLM Robustness Leaderboard v1 --Technical report](https://arxiv.org/abs/2508.06296)
*Pierre Peign√© - Lefebvre,Quentin Feuillade-Montixi,Tom David,Nicolas Miailhe*

Main category: cs.AI

TL;DR: The paper introduces PRISM Eval's Behavior Elicitation Tool (BET) for automated red-teaming of LLMs, achieving high attack success rates and proposing fine-grained robustness metrics.


<details>
  <summary>Details</summary>
Motivation: To assess and improve the robustness of state-of-the-art LLMs by identifying vulnerabilities and attack effectiveness.

Method: Uses Dynamic Adversarial Optimization for automated red-teaming and introduces fine-grained metrics for robustness analysis.

Result: Achieves 100% Attack Success Rate against 37 of 41 LLMs, with attack difficulty varying by over 300-fold.

Conclusion: Demonstrates practical pathways for distributed robustness assessment and highlights universal LLM vulnerabilities.

Abstract: This technical report accompanies the LLM robustness leaderboard published by
PRISM Eval for the Paris AI Action Summit. We introduce PRISM Eval Behavior
Elicitation Tool (BET), an AI system performing automated red-teaming through
Dynamic Adversarial Optimization that achieves 100% Attack Success Rate (ASR)
against 37 of 41 state-of-the-art LLMs. Beyond binary success metrics, we
propose a fine-grained robustness metric estimating the average number of
attempts required to elicit harmful behaviors, revealing that attack difficulty
varies by over 300-fold across models despite universal vulnerability. We
introduce primitive-level vulnerability analysis to identify which jailbreaking
techniques are most effective for specific hazard categories. Our collaborative
evaluation with trusted third parties from the AI Safety Network demonstrates
practical pathways for distributed robustness assessment across the community.

</details>


### [121] [A "good regulator theorem" for embodied agents](https://arxiv.org/abs/2508.06326)
*Nathaniel Virgo,Martin Biehl,Manuel Baltieri,Matteo Capucci*

Main category: cs.AI

TL;DR: The paper revisits Conant and Ashby's claim about regulators modeling systems, showing that agents performing regulation tasks can be interpreted as having 'beliefs' about their environment, updated via sensory input. This broader notion of models involves an observer's perspective.


<details>
  <summary>Details</summary>
Motivation: To address apparent counterexamples in Artificial Life where systems regulate without obvious models, proposing a more generalizable framework.

Method: Introduces a reinterpretation of models as externally imposed 'beliefs' updated by sensory input, applicable beyond classic control theory.

Result: Demonstrates that any regulating agent can be seen as having a model of its environment, even if trivial, resolving counterexamples.

Conclusion: The observer's role is essential in defining models, broadening the applicability of Conant and Ashby's intuition.

Abstract: In a classic paper, Conant and Ashby claimed that "every good regulator of a
system must be a model of that system." Artificial Life has produced many
examples of systems that perform tasks with apparently no model in sight; these
suggest Conant and Ashby's theorem doesn't easily generalise beyond its
restricted setup. Nevertheless, here we show that a similar intuition can be
fleshed out in a different way: whenever an agent is able to perform a
regulation task, it is possible for an observer to interpret it as having
"beliefs" about its environment, which it "updates" in response to sensory
input. This notion of belief updating provides a notion of model that is more
sophisticated than Conant and Ashby's, as well as a theorem that is more
broadly applicable. However, it necessitates a change in perspective, in that
the observer plays an essential role in the theory: models are not a mere
property of the system but are imposed on it from outside. Our theorem holds
regardless of whether the system is regulating its environment in a classic
control theory setup, or whether it's regulating its own internal state; the
model is of its environment either way. The model might be trivial, however,
and this is how the apparent counterexamples are resolved.

</details>


### [122] [AntiCheatPT: A Transformer-Based Approach to Cheat Detection in Competitive Computer Games](https://arxiv.org/abs/2508.06348)
*Mille Mei Zhen Loo,Gert Luzkov,Paolo Burelli*

Main category: cs.AI

TL;DR: AntiCheatPT_256, a transformer-based model, detects cheating in Counter-Strike 2 with 89.17% accuracy and 93.36% AUC, using a new dataset (CS2CD) of 795 matches.


<details>
  <summary>Details</summary>
Motivation: Cheating in online games undermines fairness, and current anti-cheat systems struggle to adapt without invasive measures.

Method: Developed AntiCheatPT_256, a transformer model trained on 90,707 augmented context windows from the CS2CD dataset.

Result: Achieved 89.17% accuracy and 93.36% AUC on an unaugmented test set.

Conclusion: The model provides a reproducible, data-driven baseline for future cheat detection research.

Abstract: Cheating in online video games compromises the integrity of gaming
experiences. Anti-cheat systems, such as VAC (Valve Anti-Cheat), face
significant challenges in keeping pace with evolving cheating methods without
imposing invasive measures on users' systems. This paper presents
AntiCheatPT\_256, a transformer-based machine learning model designed to detect
cheating behaviour in Counter-Strike 2 using gameplay data. To support this, we
introduce and publicly release CS2CD: A labelled dataset of 795 matches. Using
this dataset, 90,707 context windows were created and subsequently augmented to
address class imbalance. The transformer model, trained on these windows,
achieved an accuracy of 89.17\% and an AUC of 93.36\% on an unaugmented test
set. This approach emphasizes reproducibility and real-world applicability,
offering a robust baseline for future research in data-driven cheat detection.

</details>


### [123] [From Explainable to Explanatory Artificial Intelligence: Toward a New Paradigm for Human-Centered Explanations through Generative AI](https://arxiv.org/abs/2508.06352)
*Christian Meske,Justin Brenne,Erdi Uenal,Sabahat Oelcer,Ayseguel Doganguen*

Main category: cs.AI

TL;DR: The paper introduces 'Explanatory AI' as a user-centered alternative to traditional XAI, focusing on human understanding through narrative, adaptability, and context-sensitive explanations.


<details>
  <summary>Details</summary>
Motivation: Current XAI methods lack adaptability and fail to support meaningful human understanding, necessitating a shift toward user-centered explanations.

Method: Develops an eight-dimensional conceptual model for Explanatory AI and validates it empirically using Rapid Contextual Design with healthcare professionals.

Result: Users prefer context-sensitive, multimodal explanations over technical transparency, highlighting the need for human-centered AI explanations.

Conclusion: The paper advocates for Explanatory AI as a paradigm shift, emphasizing user comprehension and setting a research agenda for diverse domains.

Abstract: Current explainable AI (XAI) approaches prioritize algorithmic transparency
and present explanations in abstract, non-adaptive formats that often fail to
support meaningful end-user understanding. This paper introduces "Explanatory
AI" as a complementary paradigm that leverages generative AI capabilities to
serve as explanatory partners for human understanding rather than providers of
algorithmic transparency. While XAI reveals algorithmic decision processes for
model validation, Explanatory AI addresses contextual reasoning to support
human decision-making in sociotechnical contexts. We develop a definition and
systematic eight-dimensional conceptual model distinguishing Explanatory AI
through narrative communication, adaptive personalization, and progressive
disclosure principles. Empirical validation through Rapid Contextual Design
methodology with healthcare professionals demonstrates that users consistently
prefer context-sensitive, multimodal explanations over technical transparency.
Our findings reveal the practical urgency for AI systems designed for human
comprehension rather than algorithmic introspection, establishing a
comprehensive research agenda for advancing user-centered AI explanation
approaches across diverse domains and cultural contexts.

</details>


### [124] [Automated Creation of the Legal Knowledge Graph Addressing Legislation on Violence Against Women: Resource, Methodology and Lessons Learned](https://arxiv.org/abs/2508.06368)
*Claudia dAmato,Giuseppe Rubini,Francesco Didio,Donato Francioso,Fatima Zahra Amara,Nicola Fanizzi*

Main category: cs.AI

TL;DR: The paper introduces two methods for constructing Legal Knowledge Graphs (KGs) for violence against women cases, using a bottom-up approach and Large Language Models, validated via competency questions.


<details>
  <summary>Details</summary>
Motivation: Legal KGs are scarce but essential for enhancing legal decision-making, accessibility, and predictive justice tools.

Method: Two approaches: a systematic bottom-up method and a Large Language Model-based solution, integrating data extraction, ontology development, and semantic enrichment.

Result: Developed KGs improve legal information accessibility, enable complex queries, and support predictive justice tools.

Conclusion: The legal KGs are impactful for human and machine use, advancing legal decision-making and predictive justice.

Abstract: Legal decision-making process requires the availability of comprehensive and
detailed legislative background knowledge and up-to-date information on legal
cases and related sentences/decisions. Legal Knowledge Graphs (KGs) would be a
valuable tool to facilitate access to legal information, to be queried and
exploited for the purpose, and to enable advanced reasoning and machine
learning applications. Indeed, legal KGs may act as knowledge intensive
component to be used by pre-dictive machine learning solutions supporting the
decision process of the legal expert. Nevertheless, a few KGs can be found in
the legal domain. To fill this gap, we developed a legal KG targeting legal
cases of violence against women, along with clear adopted methodologies.
Specifically, the paper introduces two complementary approaches for automated
legal KG construction; a systematic bottom-up approach, customized for the
legal domain, and a new solution leveraging Large Language Models. Starting
from legal sentences publicly available from the European Court of Justice, the
solutions integrate structured data extraction, ontology development, and
semantic enrichment to produce KGs tailored for legal cases involving violence
against women. After analyzing and comparing the results of the two approaches,
the developed KGs are validated via suitable competency questions. The obtained
KG may be impactful for multiple purposes: can improve the accessibility to
legal information both to humans and machine, can enable complex queries and
may constitute an important knowledge component to be possibly exploited by
machine learning tools tailored for predictive justice.

</details>


### [125] [The Fair Game: Auditing & Debiasing AI Algorithms Over Time](https://arxiv.org/abs/2508.06443)
*Debabrota Basu,Udvas Das*

Main category: cs.AI

TL;DR: The paper introduces "Fair Game," a dynamic framework using Reinforcement Learning to adapt ML fairness goals over time by integrating an Auditor and Debiasing algorithm.


<details>
  <summary>Details</summary>
Motivation: Current Fair ML definitions are observational, conflicting, and static, failing to adapt in dynamic social environments.

Method: Proposes "Fair Game," combining an Auditor and Debiasing algorithm in an RL loop to dynamically adjust fairness goals.

Result: "Fair Game" enables flexible, adaptive fairness in ML systems, simulating societal ethical evolution.

Conclusion: The framework bridges the gap between static Fair ML and dynamic societal needs, offering a scalable solution for pre- and post-deployment fairness.

Abstract: An emerging field of AI, namely Fair Machine Learning (ML), aims to quantify
different types of bias (also known as unfairness) exhibited in the predictions
of ML algorithms, and to design new algorithms to mitigate them. Often, the
definitions of bias used in the literature are observational, i.e. they use the
input and output of a pre-trained algorithm to quantify a bias under concern.
In reality,these definitions are often conflicting in nature and can only be
deployed if either the ground truth is known or only in retrospect after
deploying the algorithm. Thus,there is a gap between what we want Fair ML to
achieve and what it does in a dynamic social environment. Hence, we propose an
alternative dynamic mechanism,"Fair Game",to assure fairness in the predictions
of an ML algorithm and to adapt its predictions as the society interacts with
the algorithm over time. "Fair Game" puts together an Auditor and a Debiasing
algorithm in a loop around an ML algorithm. The "Fair Game" puts these two
components in a loop by leveraging Reinforcement Learning (RL). RL algorithms
interact with an environment to take decisions, which yields new observations
(also known as data/feedback) from the environment and in turn, adapts future
decisions. RL is already used in algorithms with pre-fixed long-term fairness
goals. "Fair Game" provides a unique framework where the fairness goals can be
adapted over time by only modifying the auditor and the different biases it
quantifies. Thus,"Fair Game" aims to simulate the evolution of ethical and
legal frameworks in the society by creating an auditor which sends feedback to
a debiasing algorithm deployed around an ML system. This allows us to develop a
flexible and adaptive-over-time framework to build Fair ML systems pre- and
post-deployment.

</details>


### [126] [What Voting Rules Actually Do: A Data-Driven Analysis of Multi-Winner Voting](https://arxiv.org/abs/2508.06454)
*Joshua Caiata,Ben Armstrong,Kate Larson*

Main category: cs.AI

TL;DR: A data-driven framework evaluates multi-winner voting rules' axiom violations across diverse preference distributions, showing neural networks can outperform traditional rules.


<details>
  <summary>Details</summary>
Motivation: To move beyond worst-case analysis and assess voting rules' practical performance under varied preference distributions.

Method: Proposed a data-driven framework to analyze voting rules' axiomatic performance and tested neural networks as voting rules.

Result: Neural networks outperform traditional voting rules in minimizing axiom violations.

Conclusion: Data-driven approaches can enhance voting system design and encourage further research in social choice.

Abstract: Committee-selection problems arise in many contexts and applications, and
there has been increasing interest within the social choice research community
on identifying which properties are satisfied by different multi-winner voting
rules. In this work, we propose a data-driven framework to evaluate how
frequently voting rules violate axioms across diverse preference distributions
in practice, shifting away from the binary perspective of axiom satisfaction
given by worst-case analysis. Using this framework, we analyze the relationship
between multi-winner voting rules and their axiomatic performance under several
preference distributions. We then show that neural networks, acting as voting
rules, can outperform traditional rules in minimizing axiom violations. Our
results suggest that data-driven approaches to social choice can inform the
design of new voting systems and support the continuation of data-driven
research in social choice.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [127] [PEACH: A sentence-aligned Parallel English-Arabic Corpus for Healthcare](https://arxiv.org/abs/2508.05722)
*Rania Al-Sabbagh*

Main category: cs.CL

TL;DR: PEACH is a manually aligned English-Arabic healthcare corpus with 51,671 parallel sentences, useful for linguistics, translation studies, and NLP tasks.


<details>
  <summary>Details</summary>
Motivation: To provide a gold-standard parallel corpus for healthcare texts, aiding research in linguistics, translation, and NLP applications.

Method: Manual alignment of 51,671 parallel sentences from patient leaflets and educational materials, totaling ~590,517 English and 567,707 Arabic tokens.

Result: PEACH is a high-quality, publicly accessible corpus with varied sentence lengths (9.52-11.83 words avg).

Conclusion: PEACH supports diverse applications like bilingual lexicons, domain-specific machine translation, readability assessment, and translation education.

Abstract: This paper introduces PEACH, a sentence-aligned parallel English-Arabic
corpus of healthcare texts encompassing patient information leaflets and
educational materials. The corpus contains 51,671 parallel sentences, totaling
approximately 590,517 English and 567,707 Arabic word tokens. Sentence lengths
vary between 9.52 and 11.83 words on average. As a manually aligned corpus,
PEACH is a gold-standard corpus, aiding researchers in contrastive linguistics,
translation studies, and natural language processing. It can be used to derive
bilingual lexicons, adapt large language models for domain-specific machine
translation, evaluate user perceptions of machine translation in healthcare,
assess patient information leaflets and educational materials' readability and
lay-friendliness, and as an educational resource in translation studies. PEACH
is publicly accessible.

</details>


### [128] [Guardians and Offenders: A Survey on Harmful Content Generation and Safety Mitigation](https://arxiv.org/abs/2508.05775)
*Chi Zhang,Changjia Zhu,Junjie Xiong,Xiaoran Xu,Lingyao Li,Yao Liu,Zhuo Lu*

Main category: cs.CL

TL;DR: A survey on the dual role of Large Language Models (LLMs) in beneficial applications and harmful content, reviewing toxicity, jailbreaking, and defenses, while proposing future research directions.


<details>
  <summary>Details</summary>
Motivation: LLMs revolutionize content creation but also pose risks of toxic or biased outputs, creating a sociotechnical challenge.

Method: Systematic review of studies on unintentional toxicity, adversarial attacks, and content moderation; proposes a taxonomy and evaluates mitigation techniques like RLHF and prompt engineering.

Result: Identifies limitations in current evaluation methods and highlights the evolving landscape of LLM safety.

Conclusion: Outlines future research to develop robust, ethically aligned language technologies.

Abstract: Large Language Models (LLMs) have revolutionized content creation across
digital platforms, offering unprecedented capabilities in natural language
generation and understanding. These models enable beneficial applications such
as content generation, question and answering (Q&A), programming, and code
reasoning. Meanwhile, they also pose serious risks by inadvertently or
intentionally producing toxic, offensive, or biased content. This dual role of
LLMs, both as powerful tools for solving real-world problems and as potential
sources of harmful language, presents a pressing sociotechnical challenge. In
this survey, we systematically review recent studies spanning unintentional
toxicity, adversarial jailbreaking attacks, and content moderation techniques.
We propose a unified taxonomy of LLM-related harms and defenses, analyze
emerging multimodal and LLM-assisted jailbreak strategies, and assess
mitigation efforts, including reinforcement learning with human feedback
(RLHF), prompt engineering, and safety alignment. Our synthesis highlights the
evolving landscape of LLM safety, identifies limitations in current evaluation
methodologies, and outlines future research directions to guide the development
of robust and ethically aligned language technologies.

</details>


### [129] [FineDialFact: A benchmark for Fine-grained Dialogue Fact Verification](https://arxiv.org/abs/2508.05782)
*Xiangyan Chen,Yufeng Li,Yujian Gan,Arkaitz Zubiaga,Matthew Purver*

Main category: cs.CL

TL;DR: The paper introduces FineDialFact, a benchmark for fine-grained dialogue fact verification, addressing LLM hallucinations by verifying atomic facts in responses. It shows CoT reasoning improves performance, but the task remains challenging.


<details>
  <summary>Details</summary>
Motivation: LLMs often produce hallucinations, complicating NLP applications like dialogue systems. Current detection methods are too simplistic, needing fine-grained verification.

Method: Developed FineDialFact benchmark, constructed a dataset from public dialogues, and evaluated baseline methods, including CoT reasoning.

Result: CoT reasoning improved performance, but the best F1-score was only 0.75, indicating the task's difficulty.

Conclusion: FineDialFact is a valuable but challenging benchmark for future research on hallucination detection in dialogues.

Abstract: Large Language Models (LLMs) are known to produce hallucinations - factually
incorrect or fabricated information - which poses significant challenges for
many Natural Language Processing (NLP) applications, such as dialogue systems.
As a result, detecting hallucinations has become a critical area of research.
Current approaches to hallucination detection in dialogue systems primarily
focus on verifying the factual consistency of generated responses. However,
these responses often contain a mix of accurate, inaccurate or unverifiable
facts, making one factual label overly simplistic and coarse-grained. In this
paper, we introduce a benchmark, FineDialFact, for fine-grained dialogue fact
verification, which involves verifying atomic facts extracted from dialogue
responses. To support this, we construct a dataset based on publicly available
dialogue datasets and evaluate it using various baseline methods. Experimental
results demonstrate that methods incorporating Chain-of-Thought (CoT) reasoning
can enhance performance in dialogue fact verification. Despite this, the best
F1-score achieved on the HybriDialogue, an open-domain dialogue dataset, is
only 0.75, indicating that the benchmark remains a challenging task for future
research. Our dataset and code will be public on GitHub.

</details>


### [130] [Human-like fleeting memory improves language learning but impairs reading time prediction in transformer language models](https://arxiv.org/abs/2508.05803)
*Abishek Thamma,Micha Heilbron*

Main category: cs.CL

TL;DR: Fleeting memory improves language learning in transformers but impairs human reading time prediction, challenging prior assumptions.


<details>
  <summary>Details</summary>
Motivation: To investigate if memory limitations benefit language learning, contrasting classic cognitive science with transformer models.

Method: Training transformers with and without fleeting memory on a realistic dataset, evaluating language modeling and human reading time prediction.

Result: Fleeting memory improved language modeling but worsened reading time prediction, with no clear explanation for the discrepancy.

Conclusion: Memory limitations aid neural network language learning but not behavioral prediction, supporting cognitive science theories partially.

Abstract: Human memory is fleeting. As words are processed, the exact wordforms that
make up incoming sentences are rapidly lost. Cognitive scientists have long
believed that this limitation of memory may, paradoxically, help in learning
language - an idea supported by classic connectionist modelling work. The rise
of Transformers appears to challenge this idea, as these models can learn
language effectively, despite lacking memory limitations or other architectural
recency biases. Here, we investigate the hypothesized benefit of fleeting
memory for language learning in tightly controlled experiments on transformer
language models. Training transformers with and without fleeting memory on a
developmentally realistic training set, we find that fleeting memory
consistently improves language learning (as quantified by both overall language
modelling performance and targeted syntactic evaluation) but, unexpectedly,
impairs surprisal-based prediction of human reading times. Interestingly,
follow up analyses revealed that this discrepancy - better language modeling,
yet worse reading time prediction - could not be accounted for by prior
explanations of why better language models sometimes fit human reading time
worse. Together, these results support a benefit of memory limitations on
neural network language learning - but not on predicting behavior.

</details>


### [131] ["Mirror" Language AI Models of Depression are Criterion-Contaminated](https://arxiv.org/abs/2508.05830)
*Tong Li,Rasiq Hussain,Mehak Gupta,Joshua R. Oltmanns*

Main category: cs.CL

TL;DR: The study compares Mirror and Non-Mirror models for predicting depression scores, finding Mirror models inflate effect sizes due to criterion contamination, while Non-Mirror models offer more generalizable results.


<details>
  <summary>Details</summary>
Motivation: To address the issue of criterion contamination in Mirror models, which artificially inflates effect sizes and reduces generalizability in depression prediction.

Method: Used GPT-4, GPT-4o, and LLaMA3-70B to predict depression scores from structured diagnostic and life history interviews, comparing Mirror and Non-Mirror models.

Result: Mirror models showed inflated effect sizes (R2 = .80), while Non-Mirror models had smaller but still significant effect sizes (R2 = .27). Both correlated similarly with self-reported symptoms (r = ~.54).

Conclusion: Non-Mirror models provide more interpretable and generalizable features for real-world depression assessment, highlighting the bias in Mirror models.

Abstract: A growing number of studies show near-perfect LLM language-based prediction
of depression assessment scores (up to R2 of .70). However, many develop these
models directly from language responses to depression assessments. These
"Mirror models" suffer from "criterion contamination", which arises when a
predicted score depends in part on the predictors themselves. This causes
artificial effect size inflation which reduces model generalizability. The
present study compares the performance of Mirror models versus "Non-Mirror
models", which are developed from language that does not mirror the assessment
they are developed to predict. N = 110 research participants completed two
different interviews: structured diagnostic and life history interviews. GPT-4,
GPT-4o and LLaMA3-70B were then prompted to predict structured diagnostic
interview depression scores from the two transcripts separately. Mirror models
(using structured diagnostic data) showed very large effect sizes (e.g., R2 =
.80). As expected, NonMirror models (using life history data) demonstrated
smaller effect sizes, but were relatively large (e.g., R2 = .27). When Mirror
and Non-Mirror model-predicted structured interview depression scores were
correlated with self-reported depression symptoms, Mirror and NonMirror
performed the same (e.g., r = ~.54), indicating that Mirror models contain bias
perhaps due to criterion contamination. Topic modeling identified clusters
across Mirror and Non-Mirror models, as well as between true-positive and
false-positive predictions. In this head-to-head comparison study, Mirror
language AI models of depression showed artificially inflated effect sizes and
less generalizability. As language AI models for depression continue to evolve,
incorporating Non-Mirror models may identify interpretable, and generalizable
semantic features that have unique utility in real-world psychological
assessment.

</details>


### [132] [Discovering Properties of Inflectional Morphology in Neural Emergent Communication](https://arxiv.org/abs/2508.05843)
*Miles Gilberti,Shane Storks,Huteng Dai*

Main category: cs.CL

TL;DR: The paper reinterprets emergent communication (EmCom) by simulating double articulation and inflectional morphology, discovering that phonological constraints encourage concatenative morphology and emergent languages mimic natural language fusion.


<details>
  <summary>Details</summary>
Motivation: To explore how emergent communication with neural agents can better mimic human language, particularly inflectional morphology, and compare it to natural language schemes.

Method: Reinterpret the attribute-value reconstruction game with a small-vocabulary constraint to simulate double articulation and introduce a novel setting for inflectional morphology. Develop new metrics and explore variations like concatenativity and fusionality.

Result: Phonological constraints encourage concatenative morphology, and emergent languages replicate natural language's tendency to fuse grammatical attributes.

Conclusion: The study provides insights into how emergent communication can better resemble human language, particularly in morphological properties.

Abstract: Emergent communication (EmCom) with deep neural network-based agents promises
to yield insights into the nature of human language, but remains focused
primarily on a few subfield-specific goals and metrics that prioritize
communication schemes which represent attributes with unique characters
one-to-one and compose them syntactically. We thus reinterpret a common EmCom
setting, the attribute-value reconstruction game, by imposing a
small-vocabulary constraint to simulate double articulation, and formulating a
novel setting analogous to naturalistic inflectional morphology (enabling
meaningful comparison to natural language communication schemes). We develop
new metrics and explore variations of this game motivated by real properties of
inflectional morphology: concatenativity and fusionality. Through our
experiments, we discover that simulated phonological constraints encourage
concatenative morphology, and emergent languages replicate the tendency of
natural languages to fuse grammatical attributes.

</details>


### [133] [Do Machines Think Emotionally? Cognitive Appraisal Analysis of Large Language Models](https://arxiv.org/abs/2508.05880)
*Sree Bhattacharyya,Lucas Craig,Tharun Dilliraj,Jia Li,James Z. Wang*

Main category: cs.CL

TL;DR: The paper investigates how Large Language Models (LLMs) reason about emotions using cognitive appraisal theory, introducing the CoRE benchmark to evaluate their cognitive reasoning capabilities.


<details>
  <summary>Details</summary>
Motivation: To move beyond superficial emotion tasks and explore LLMs' deeper cognitive reasoning about emotions, addressing gaps in current affective computing research.

Method: Introduces the CoRE benchmark, evaluates LLMs using cognitive appraisal theory, and conducts experiments to analyze reasoning patterns and cognitive dimensions.

Result: Reveals diverse reasoning patterns across LLMs and identifies key cognitive dimensions for emotional reasoning.

Conclusion: The study advances understanding of LLMs' emotional reasoning and provides a benchmark for future research in affective computing.

Abstract: Affective Computing has been established as a crucial field of inquiry to
advance the holistic development of Artificial Intelligence (AI) systems.
Foundation models -- especially Large Language Models (LLMs) -- have been
evaluated, trained, or instruction-tuned in several past works, to become
better predictors or generators of emotion. Most of these studies, however,
approach emotion-related tasks in a supervised manner, assessing or training
the capabilities of LLMs using discrete emotion labels associated with stimuli
(e.g., text, images, video, audio). Evaluation studies, in particular, have
often been limited to standard and superficial emotion-related tasks, such as
the recognition of evoked or expressed emotions. In this paper, we move beyond
surface-level emotion tasks to investigate how LLMs reason about emotions
through cognitive dimensions. Drawing from cognitive appraisal theory, we
examine whether LLMs produce coherent and plausible cognitive reasoning when
reasoning about emotionally charged stimuli. We introduce a large-scale
benchmark on Cognitive Reasoning for Emotions - CoRE - to evaluate internal
cognitive structures implicitly used by LLMs for emotional reasoning. Through a
plethora of evaluation experiments and analysis, we seek to answer: (a) Are
models more likely to implicitly rely on specific cognitive appraisal
dimensions?, (b) What cognitive dimensions are important for characterizing
specific emotions?, and, (c) Can the internal representations of different
emotion categories in LLMs be interpreted through cognitive appraisal
dimensions? Our results and analyses reveal diverse reasoning patterns across
different LLMs. Our benchmark and code will be made publicly available.

</details>


### [134] [Spectrum Projection Score: Aligning Retrieved Summaries with Reader Models in Retrieval-Augmented Generation](https://arxiv.org/abs/2508.05909)
*Zhanghao Hu,Qinglin Zhu,Siya Qi,Yulan He,Hanqi Yan,Lin Gui*

Main category: cs.CL

TL;DR: The paper introduces Spectrum Projection Score (SPS) to measure retrieval relevance in RAG systems and presents xCompress, a framework for dynamic retrieval summary compression.


<details>
  <summary>Details</summary>
Motivation: Prior work evaluates RAG holistically, making it hard to isolate retrieval's true impact due to LLM prompt sensitivity.

Method: Proposes SPS, a lightweight metric to assess semantic alignment of retrieved summaries, and xCompress for dynamic summary compression.

Result: Experiments on QA benchmarks show SPS improves performance and clarifies retrieval-generation interaction.

Conclusion: SPS and xCompress enhance RAG systems by providing a principled way to evaluate and optimize retrieval relevance.

Abstract: Large Language Models (LLMs) have shown improved generation performance
through retrieval-augmented generation (RAG) following the retriever-reader
paradigm, which supplements model inputs with externally retrieved knowledge.
However, prior work often evaluates RAG holistically, assessing the retriever
and reader jointly, making it difficult to isolate the true contribution of
retrieval, particularly given the prompt sensitivity of LLMs used as readers.
We introduce Spectrum Projection Score (SPS), a lightweight, supervision-free
metric that allows the reader to gauge the semantic alignment of a retrieved
summary with its hidden representation by comparing the area formed by
generated tokens from the summary, and the principal directions of subspace in
the reader and to measure the relevance. Building on SPS we present xCompress,
an inference time controller framework that dynamically samples, ranks, and
compresses retrieval summary candidates. Extensive experiments on five QA
benchmarks with four open source LLMs show that SPS not only enhances
performance across a range of tasks but also provides a principled perspective
on the interaction between retrieval and generation.

</details>


### [135] [Prosocial Behavior Detection in Player Game Chat: From Aligning Human-AI Definitions to Efficient Annotation at Scale](https://arxiv.org/abs/2508.05938)
*Rafal Kocielnik,Min Kim,Penphob,Boonyarungsrit,Fereshteh Soltani,Deshawn Sambrano,Animashree Anandkumar,R. Michael Alvarez*

Main category: cs.CL

TL;DR: A three-stage pipeline for scalable, high-precision prosocial content classification, combining LLM-based labeling, human-AI refinement, and cost-efficient inference.


<details>
  <summary>Details</summary>
Motivation: Prosociality detection is a novel challenge lacking definitions and labeled data, requiring innovative approaches for trust and safety systems.

Method: A pipeline with LLM-based labeling, human-AI refinement, and a two-stage inference system (lightweight classifier + GPT-4 for ambiguous cases).

Result: Achieves high precision (~0.90) with ~70% cost reduction by escalating only ~35% of ambiguous cases to GPT-4.

Conclusion: Targeted human-AI interaction, task formulation, and deployment-aware design enable scalable solutions for novel responsible AI tasks.

Abstract: Detecting prosociality in text--communication intended to affirm, support, or
improve others' behavior--is a novel and increasingly important challenge for
trust and safety systems. Unlike toxic content detection, prosociality lacks
well-established definitions and labeled data, requiring new approaches to both
annotation and deployment. We present a practical, three-stage pipeline that
enables scalable, high-precision prosocial content classification while
minimizing human labeling effort and inference costs. First, we identify the
best LLM-based labeling strategy using a small seed set of human-labeled
examples. We then introduce a human-AI refinement loop, where annotators review
high-disagreement cases between GPT-4 and humans to iteratively clarify and
expand the task definition-a critical step for emerging annotation tasks like
prosociality. This process results in improved label quality and definition
alignment. Finally, we synthesize 10k high-quality labels using GPT-4 and train
a two-stage inference system: a lightweight classifier handles high-confidence
predictions, while only $\sim$35\% of ambiguous instances are escalated to
GPT-4o. This architecture reduces inference costs by $\sim$70% while achieving
high precision ($\sim$0.90). Our pipeline demonstrates how targeted human-AI
interaction, careful task formulation, and deployment-aware architecture design
can unlock scalable solutions for novel responsible AI tasks.

</details>


### [136] [Adversarial Topic-aware Prompt-tuning for Cross-topic Automated Essay Scoring](https://arxiv.org/abs/2508.05987)
*Chunyun Zhang,Hongyan Zhao,Chaoran Cui,Qilong Song,Zhiqing Lu,Shuai Gong,Kailin Liu*

Main category: cs.CL

TL;DR: ATOP improves cross-topic AES by jointly learning topic-shared and topic-specific features using adversarial training and pseudo-labeling.


<details>
  <summary>Details</summary>
Motivation: Existing AES methods neglect topic-specific features, limiting their ability to assess traits like topic adherence.

Method: ATOP uses adversarial training and a neighbor-based classifier to learn topic-aware prompts from PLMs.

Result: ATOP outperforms state-of-the-art methods on the ASAP++ dataset in holistic and multi-trait scoring.

Conclusion: ATOP effectively addresses topic discrepancies in AES, enhancing model transferability and performance.

Abstract: Cross-topic automated essay scoring (AES) aims to develop a transferable
model capable of effectively evaluating essays on a target topic. A significant
challenge in this domain arises from the inherent discrepancies between topics.
While existing methods predominantly focus on extracting topic-shared features
through distribution alignment of source and target topics, they often neglect
topic-specific features, limiting their ability to assess critical traits such
as topic adherence. To address this limitation, we propose an Adversarial
TOpic-aware Prompt-tuning (ATOP), a novel method that jointly learns
topic-shared and topic-specific features to improve cross-topic AES. ATOP
achieves this by optimizing a learnable topic-aware prompt--comprising both
shared and specific components--to elicit relevant knowledge from pre-trained
language models (PLMs). To enhance the robustness of topic-shared prompt
learning and mitigate feature scale sensitivity introduced by topic alignment,
we incorporate adversarial training within a unified regression and
classification framework. In addition, we employ a neighbor-based classifier to
model the local structure of essay representations and generate pseudo-labels
for target-topic essays. These pseudo-labels are then used to guide the
supervised learning of topic-specific prompts tailored to the target topic.
Extensive experiments on the publicly available ASAP++ dataset demonstrate that
ATOP significantly outperforms existing state-of-the-art methods in both
holistic and multi-trait essay scoring. The implementation of our method is
publicly available at: https://anonymous.4open.science/r/ATOP-A271.

</details>


### [137] [Crisp Attention: Regularizing Transformers via Structured Sparsity](https://arxiv.org/abs/2508.06016)
*Sagar Gandhi,Vishal Gandhi*

Main category: cs.CL

TL;DR: Introducing structured sparsity in attention mechanisms improves model accuracy, challenging the assumption that sparsity reduces performance.


<details>
  <summary>Details</summary>
Motivation: Address the quadratic computational cost of self-attention in Transformers and explore if sparsity can enhance accuracy, not just efficiency.

Method: Applied post-hoc structured sparsity to DistilBERT's attention during fine-tuning on SST-2 sentiment analysis.

Result: 80% sparsity improved validation accuracy by 0.97% (91.59% vs. dense baseline).

Conclusion: Sparsity acts as an implicit regularizer, improving generalization and performance, not just efficiency.

Abstract: The quadratic computational cost of the self-attention mechanism is a primary
challenge in scaling Transformer models. While attention sparsity is widely
studied as a technique to improve computational efficiency, it is almost
universally assumed to come at the cost of model accuracy. In this paper, we
report a surprising counter-example to this common wisdom. By introducing
structured, post-hoc sparsity to the attention mechanism of a DistilBERT model
during fine-tuning on the SST-2 sentiment analysis task, we find that model
accuracy improves significantly. Our model with 80\% attention sparsity
achieves a validation accuracy of 91.59\%, a 0.97\% absolute improvement over
the dense baseline. We hypothesize that this phenomenon is due to sparsity
acting as a powerful implicit regularizer, preventing the model from
overfitting by forcing it to make predictions with a more constrained and
robust set of features. Our work recasts attention sparsity not just as a tool
for computational efficiency, but as a potential method for improving the
generalization and performance of Transformer models.

</details>


### [138] [Temporal Self-Rewarding Language Models: Decoupling Chosen-Rejected via Past-Future](https://arxiv.org/abs/2508.06026)
*Yidong Wang,Xin Wang,Cunxiang Wang,Junfeng Fang,Qiufeng Wang,Jianing Chu,Xuran Meng,Shuxun Yang,Libo Qin,Yue Zhang,Wei Ye,Shikun Zhang*

Main category: cs.CL

TL;DR: The paper introduces Temporal Self-Rewarding Language Models to address the limitation of synchronized improvement in Self-Rewarding paradigms, improving preference learning by coordinating past, present, and future model outputs.


<details>
  <summary>Details</summary>
Motivation: Existing Self-Rewarding paradigms suffer from narrowing representational differences between chosen and rejected responses, undermining effective preference learning.

Method: The proposed dual-phase framework includes Anchored Rejection (fixing rejected responses using past outputs) and Future-Guided Chosen (curating chosen samples using future model predictions).

Result: Experiments show significant improvements, e.g., Llama3.1-8B achieves a 29.44 win rate on AlpacaEval 2.0, outperforming the baseline by 9.75, with superior generalization across tasks.

Conclusion: The method enhances learning signals and outperforms Self-Rewarding baselines, demonstrating robust generalization without task-specific data.

Abstract: Self-Rewarding Language Models propose an architecture in which the Large
Language Models(LLMs) both generates responses and evaluates its own outputs
via LLM-as-a-Judge prompting, dynamically improving its generative capabilities
through iterative Direct Preference Optimization (DPO). However, our analysis
reveals a critical limitation in existing Self-Rewarding paradigms: the
synchronized improvement of chosen and rejected responses progressively narrows
the representational difference between contrasting samples, undermining
effective preference learning. We propose \textbf{Temporal Self-Rewarding
Language Models} that strategically coordinate past, present, and future model
generations to sustain learning signals. Our dual-phase framework introduces:
(1) \textit{Anchored Rejection} - fixing rejected responses using the past
initial model's outputs and (2) \textit{Future-Guided Chosen} - dynamically
curating chosen samples using next-generation model predictions. Extensive
experiments across three model families (Llama, Qwen, Mistral) and different
model sizes (Llama3B/8B/70B) demonstrate significant improvements when trained
with our method compared to Self-Rewarding using same computation resources.
For example, Llama3.1-8B reaches a 29.44 win rate on AlpacaEval 2.0 with our
method, outperforming the Self-Rewarding baseline (19.69) by 9.75. Notably, our
method also demonstrates superior out-of-distribution generalization across
mathematical reasoning (GSM8K), knowledge-based QA (ARC, TruthfulQA), and code
generation (HumanEval) tasks, even though we do not specifically collect such
training data.

</details>


### [139] [Efficient Knowledge Probing of Large Language Models by Adapting Pre-trained Embeddings](https://arxiv.org/abs/2508.06030)
*Kartik Sharma,Yiqiao Jin,Rakshit Trivedi,Srijan Kumar*

Main category: cs.CL

TL;DR: The paper introduces PEEK, a method using proxy embeddings to estimate LLM knowledge without costly forward passes, achieving up to 90% accuracy.


<details>
  <summary>Details</summary>
Motivation: LLMs acquire vast knowledge, but probing their understanding is computationally expensive. PEEK aims to address this gap efficiently.

Method: PEEK leverages pre-trained embedding models to predict LLM knowledge by training on identified facts and using a linear decoder layer.

Result: Evaluations on 3 datasets, 4 LLMs, and 7 embedding models show 90% accuracy in predicting LLM knowledge, with sentence embeddings outperforming graph embeddings.

Conclusion: PEEK offers a scalable way to identify LLM knowledge gaps and insights into their inductive bias, with code and data publicly available.

Abstract: Large language models (LLMs) acquire knowledge across diverse domains such as
science, history, and geography encountered during generative pre-training.
However, due to their stochasticity, it is difficult to predict what LLMs have
acquired. Prior work has developed different ways to probe this knowledge by
investigating the hidden representations, crafting specific task prompts,
curating representative samples, and estimating their uncertainty. However,
these methods require making forward passes through the underlying model to
probe the LLM's knowledge about a specific fact, making them computationally
expensive and time-consuming. To bridge this gap, we propose $\textbf{PEEK}$ or
$\textbf{P}$roxy $\textbf{E}$mbeddings to $\textbf{E}$stimate
$\textbf{K}$nowledge of LLMs, by leveraging the pre-trained embedding models
that effectively encode factual knowledge as text or graphs as proxies for
LLMs. First, we identify a training set of facts known by LLMs through various
probing strategies and then adapt embedding models to predict the LLM outputs
with a linear decoder layer. Comprehensive evaluation on $3$ Wikipedia-derived
datasets, $4$ LLMs, and $7$ embedding models shows that embeddings can predict
LLM knowledge on a held-out set with up to 90 % accuracy. Furthermore, we find
that sentence embedding models are more suitable than graph embeddings to
predict LLM knowledge, shedding light on the underlying representation of the
factual landscape. Thus, we believe that knowledge-adapted embeddings can be
used to identify knowledge gaps in LLMs at scale and can provide deeper
insights into LLMs' internal inductive bias. The code and data are made
available at https://github.com/claws-lab/peek.

</details>


### [140] [EvolvR: Self-Evolving Pairwise Reasoning for Story Evaluation to Enhance Generation](https://arxiv.org/abs/2508.06046)
*Xinda Wang,Zhengxu Hou,Yangshijie Zhang,Bingren Yan,Zhibo Yang,Xingsheng Zhang,Luxi Xing,Qiang Zhou,Chen Zhang*

Main category: cs.CL

TL;DR: The paper introduces EvolvR, a Self-Evolving Pairwise Reasoning framework, to improve LLMs' performance in open-ended story evaluation by synthesizing and filtering score-aligned Chain-of-Thought data, achieving SOTA results and enhancing story generation.


<details>
  <summary>Details</summary>
Motivation: Existing methods for story evaluation using LLMs lack adaptability (closed-source models) or reasoning rigor (open-source models), limiting their effectiveness in open-ended tasks like story evaluation.

Method: Proposes EvolvR, which synthesizes score-aligned Chain-of-Thought data via multi-persona strategy, filters it for quality using multi-agents, and trains an evaluator as a reward model for story generation.

Result: Achieves SOTA performance on benchmarks (StoryER, HANNA, OpenMEVA) and significantly improves generated story quality when used as a reward model.

Conclusion: The self-evolving approach of EvolvR effectively addresses limitations in LLM-based story evaluation and enhances story generation, validating its superiority.

Abstract: Although the effectiveness of Large Language Models (LLMs) as judges
(LLM-as-a-judge) has been validated, their performance remains limited in
open-ended tasks, particularly in story evaluation. Accurate story evaluation
is crucial not only for assisting human quality judgment but also for providing
key signals to guide story generation. However, existing methods face a
dilemma: prompt engineering for closed-source models suffers from poor
adaptability, while fine-tuning approaches for open-source models lack the
rigorous reasoning capabilities essential for story evaluation. To address
this, we propose the Self-Evolving Pairwise Reasoning (EvolvR) framework.
Grounded in pairwise comparison, the framework first self-synthesizes
score-aligned Chain-of-Thought (CoT) data via a multi-persona strategy. To
ensure data quality, these raw CoTs undergo a self-filtering process, utilizing
multi-agents to guarantee their logical rigor and robustness. Finally, the
evaluator trained on the refined data is deployed as a reward model to guide
the story generation task. Experimental results demonstrate that our framework
achieves state-of-the-art (SOTA) performance on three evaluation benchmarks
including StoryER, HANNA and OpenMEVA. Furthermore, when served as a reward
model, it significantly enhances the quality of generated stories, thereby
fully validating the superiority of our self-evolving approach.

</details>


### [141] [ConlangCrafter: Constructing Languages with a Multi-Hop LLM Pipeline](https://arxiv.org/abs/2508.06094)
*Morris Alper,Moran Yanuka,Raja Giryes,Ga≈°per Begu≈°*

Main category: cs.CL

TL;DR: ConlangCrafter uses LLMs to automate conlang creation, breaking it into stages like phonology and syntax, ensuring diversity and coherence.


<details>
  <summary>Details</summary>
Motivation: To leverage modern LLMs for creative conlang generation, simplifying the process without requiring human linguistic expertise.

Method: A multi-hop pipeline (ConlangCrafter) decomposes language design into stages (phonology, morphology, etc.), using LLMs for meta-linguistic reasoning, randomness for diversity, and self-refinement for consistency.

Result: Produces coherent and typologically diverse conlangs, validated by metrics.

Conclusion: ConlangCrafter effectively automates conlang creation, demonstrating LLMs' potential as computational creativity aids.

Abstract: Constructed languages (conlangs) such as Esperanto and Quenya have played
diverse roles in art, philosophy, and international communication. Meanwhile,
large-scale foundation models have revolutionized creative generation in text,
images, and beyond. In this work, we leverage modern LLMs as computational
creativity aids for end-to-end conlang creation. We introduce ConlangCrafter, a
multi-hop pipeline that decomposes language design into modular stages --
phonology, morphology, syntax, lexicon generation, and translation. At each
stage, our method leverages LLMs' meta-linguistic reasoning capabilities,
injecting randomness to encourage diversity and leveraging self-refinement
feedback to encourage consistency in the emerging language description. We
evaluate ConlangCrafter on metrics measuring coherence and typological
diversity, demonstrating its ability to produce coherent and varied conlangs
without human linguistic expertise.

</details>


### [142] [Few-Shot Prompting for Extractive Quranic QA with Instruction-Tuned LLMs](https://arxiv.org/abs/2508.06103)
*Mohamed Basem,Islam Oshallah,Ali Hamdi,Ammar Mohammed*

Main category: cs.CL

TL;DR: Two approaches for Quran QA: few-shot prompting with large language models (Gemini, DeepSeek) and a specialized Arabic prompt framework. Post-processing improves precision, reducing hallucinations. Results show superiority over traditional models with a pAP10 score of 0.637.


<details>
  <summary>Details</summary>
Motivation: Address challenges in Quran QA like complex language, unique terminology, and deep meaning.

Method: Uses few-shot prompting with instruction-tuned large models (Gemini, DeepSeek) and a specialized Arabic prompt framework. Includes post-processing (subword alignment, overlap suppression, semantic filtering).

Result: Large language models with Arabic instructions outperform traditional models, achieving a pAP10 score of 0.637.

Conclusion: Prompt-based instruction tuning is effective for low-resource, semantically rich QA tasks.

Abstract: This paper presents two effective approaches for Extractive Question
Answering (QA) on the Quran. It addresses challenges related to complex
language, unique terminology, and deep meaning in the text. The second uses
few-shot prompting with instruction-tuned large language models such as Gemini
and DeepSeek. A specialized Arabic prompt framework is developed for span
extraction. A strong post-processing system integrates subword alignment,
overlap suppression, and semantic filtering. This improves precision and
reduces hallucinations. Evaluations show that large language models with Arabic
instructions outperform traditional fine-tuned models. The best configuration
achieves a pAP10 score of 0.637. The results confirm that prompt-based
instruction tuning is effective for low-resource, semantically rich QA tasks.

</details>


### [143] [You Don't Need Pre-built Graphs for RAG: Retrieval Augmented Generation with Adaptive Reasoning Structures](https://arxiv.org/abs/2508.06105)
*Shengyuan Chen,Chuang Zhou,Zheng Yuan,Qinggang Zhang,Zeyang Cui,Hao Chen,Yilin Xiao,Jiannong Cao,Xiao Huang*

Main category: cs.CL

TL;DR: LogicRAG dynamically extracts reasoning structures for adaptive retrieval, avoiding costly pre-built graphs, and improves efficiency and performance in LLM tasks.


<details>
  <summary>Details</summary>
Motivation: Addressing the inefficiency and misalignment of pre-built graphs in GraphRAG methods, which lead to high token costs and ineffective knowledge retrieval.

Method: Decomposes queries into subproblems, constructs a DAG for logical dependencies, linearizes it for coherent reasoning, and prunes graphs/contexts to reduce token costs.

Result: LogicRAG outperforms state-of-the-art baselines in performance and efficiency.

Conclusion: LogicRAG offers a dynamic, efficient solution for retrieval-augmented generation, eliminating the need for pre-built graphs while improving accuracy and reducing costs.

Abstract: Large language models (LLMs) often suffer from hallucination, generating
factually incorrect statements when handling questions beyond their knowledge
and perception. Retrieval-augmented generation (RAG) addresses this by
retrieving query-relevant contexts from knowledge bases to support LLM
reasoning. Recent advances leverage pre-constructed graphs to capture the
relational connections among distributed documents, showing remarkable
performance in complex tasks. However, existing Graph-based RAG (GraphRAG)
methods rely on a costly process to transform the corpus into a graph,
introducing overwhelming token cost and update latency. Moreover, real-world
queries vary in type and complexity, requiring different logic structures for
accurate reasoning. The pre-built graph may not align with these required
structures, resulting in ineffective knowledge retrieval. To this end, we
propose a \textbf{\underline{Logic}}-aware
\textbf{\underline{R}}etrieval-\textbf{\underline{A}}ugmented
\textbf{\underline{G}}eneration framework (\textbf{LogicRAG}) that dynamically
extracts reasoning structures at inference time to guide adaptive retrieval
without any pre-built graph. LogicRAG begins by decomposing the input query
into a set of subproblems and constructing a directed acyclic graph (DAG) to
model the logical dependencies among them. To support coherent multi-step
reasoning, LogicRAG then linearizes the graph using topological sort, so that
subproblems can be addressed in a logically consistent order. Besides, LogicRAG
applies graph pruning to reduce redundant retrieval and uses context pruning to
filter irrelevant context, significantly reducing the overall token cost.
Extensive experiments demonstrate that LogicRAG achieves both superior
performance and efficiency compared to state-of-the-art baselines.

</details>


### [144] [AURA: Affordance-Understanding and Risk-aware Alignment Technique for Large Language Models](https://arxiv.org/abs/2508.06124)
*Sayantan Adak,Pratyush Chatterjee,Somnath Banerjee,Rima Hazra,Somak Aditya,Animesh Mukherjee*

Main category: cs.CL

TL;DR: AURA is a multi-layered framework using Process Reward Models (PRMs) to improve LLM safety by evaluating logical coherence and safety-awareness at each reasoning step, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Current LLMs struggle with affordance-based safety risks, where outputs unintentionally enable harmful actions due to overlooked logical implications. Existing safety solutions lack granularity and proactive intervention.

Method: AURA combines introspective self-critique, fine-grained PRM assessments, and adaptive safety-aware decoding to guide models toward safer reasoning.

Result: AURA significantly outperforms existing methods, enhancing logical integrity and safety-awareness in model outputs.

Conclusion: AURA sets a new benchmark for safer, more responsible AI, advancing alignment-sensitive applications.

Abstract: Present day LLMs face the challenge of managing affordance-based safety
risks-situations where outputs inadvertently facilitate harmful actions due to
overlooked logical implications. Traditional safety solutions, such as scalar
outcome-based reward models, parameter tuning, or heuristic decoding
strategies, lack the granularity and proactive nature needed to reliably detect
and intervene during subtle yet crucial reasoning steps. Addressing this
fundamental gap, we introduce AURA, an innovative, multi-layered framework
centered around Process Reward Models (PRMs), providing comprehensive, step
level evaluations across logical coherence and safety-awareness. Our framework
seamlessly combines introspective self-critique, fine-grained PRM assessments,
and adaptive safety-aware decoding to dynamically and proactively guide models
toward safer reasoning trajectories. Empirical evidence clearly demonstrates
that this approach significantly surpasses existing methods, significantly
improving the logical integrity and affordance-sensitive safety of model
outputs. This research represents a pivotal step toward safer, more
responsible, and contextually aware AI, setting a new benchmark for
alignment-sensitive applications.

</details>


### [145] [Less is More: Selective Reflection for Compatible and Efficient Knowledge Distillation in Large Language Models](https://arxiv.org/abs/2508.06135)
*Lingyuan Liu,Mengxiang Zhang*

Main category: cs.CL

TL;DR: Selective Reflection Distillation (SRD) improves Knowledge Distillation (KD) by refining training data quality and student-model compatibility, reducing computational costs and enhancing performance.


<details>
  <summary>Details</summary>
Motivation: Existing KD methods overlook data quality and student-model compatibility, limiting effectiveness.

Method: SRD uses student model reflections to curate high-quality, compatible training data and employs curriculum scheduling.

Result: SRD improves distilled model performance and reduces training runtime by up to 39%.

Conclusion: Data quality and compatibility are crucial for effective KD; SRD provides a principled solution.

Abstract: Knowledge Distillation (KD) is a fundamental technique for compressing large
language models (LLMs) into compact, efficient student models. However,
existing white-box KD methods mainly focus on balancing ground truth and
student-generated responses while overlooking two critical factors: training
data quality and student-model compatibility. To address these limitations, we
propose Selective Reflection Distillation (SRD), a novel data curation
framework that leverages reflections from student models to systematically
refine training data. SRD dynamically evaluates and selects prompt-response
pairs by comparing ground truth data with student model outputs, selectively
curating high-quality, student-compatible training instances through automated
ranking based on difficulty. Furthermore, after selecting the training data, a
curriculum scheduling strategy is employed to incrementally introduce these
curated subsets into the distillation process at fixed intervals. As a
plug-and-play enhancement, SRD consistently improves distillation outcomes
across diverse white-box KD approaches and model architectures, as well as
decreases computational cost significantly during KD training. Experiments on a
range of language model benchmarks demonstrate SRD's consistent improvements in
distilled model performance, as well as a reduction in training runtime by up
to 39%, under diverse KD methods and model families. Notably, SRD operates as a
plug-and-play module, enhancing sample efficiency without modifying underlying
KD algorithms. Our findings highlight that data quality and compatibility are
pivotal to effective and efficient distillation of LLMs, and SRD provides a
principled framework to achieve both. This work advances the understanding of
data-centric factors in KD and offers practical insights for enhancing the
capability and efficiency of compressed LLMs.

</details>


### [146] [Scaling Personality Control in LLMs with Big Five Scaler Prompts](https://arxiv.org/abs/2508.06149)
*Gunhee Cho,Yun-Gyung Cheong*

Main category: cs.CL

TL;DR: Big5-Scaler is a prompt-based framework for controlling Big Five personality traits in LLMs without extra training, showing effectiveness in trait expression and dialogue generation.


<details>
  <summary>Details</summary>
Motivation: To enable fine-grained personality control in LLMs for building personality-aware dialogue agents.

Method: Embeds numeric trait values into natural language prompts to condition LLMs.

Result: Induces consistent and distinguishable traits, with performance varying by prompt type and scale.

Conclusion: Concise prompts and lower trait intensities are effective for personality-aware dialogue agents.

Abstract: We present Big5-Scaler, a prompt-based framework for conditioning large
language models (LLMs) with controllable Big Five personality traits. By
embedding numeric trait values into natural language prompts, our method
enables fine-grained personality control without additional training. We
evaluate Big5-Scaler across trait expression, dialogue generation, and human
trait imitation tasks. Results show that it induces consistent and
distinguishable personality traits across models, with performance varying by
prompt type and scale. Our analysis highlights the effectiveness of concise
prompts and lower trait intensities, providing a efficient approach for
building personality-aware dialogue agents.

</details>


### [147] [Semantic and Structural Analysis of Implicit Biases in Large Language Models: An Interpretable Approach](https://arxiv.org/abs/2508.06155)
*Renhan Zhang,Lian Lian,Zhen Qi,Guiran Liu*

Main category: cs.CL

TL;DR: The paper proposes an interpretable method to detect implicit social biases in large language model outputs, using nested semantic representation and contextual contrast. It achieves strong performance on the StereoSet dataset.


<details>
  <summary>Details</summary>
Motivation: Address implicit stereotypes in language model outputs, which are not easily captured by explicit features, to ensure trustworthy content generation.

Method: Combines nested semantic representation with contextual contrast, extracts latent bias features, and uses attention weight perturbation to analyze bias pathways.

Result: Achieves high bias detection accuracy, semantic consistency, and contextual sensitivity across multiple stereotype dimensions.

Conclusion: The method provides a transparent, reliable foundation for bias detection, suitable for real-world applications requiring trustworthy outputs.

Abstract: This paper addresses the issue of implicit stereotypes that may arise during
the generation process of large language models. It proposes an interpretable
bias detection method aimed at identifying hidden social biases in model
outputs, especially those semantic tendencies that are not easily captured
through explicit linguistic features. The method combines nested semantic
representation with a contextual contrast mechanism. It extracts latent bias
features from the vector space structure of model outputs. Using attention
weight perturbation, it analyzes the model's sensitivity to specific social
attribute terms, thereby revealing the semantic pathways through which bias is
formed. To validate the effectiveness of the method, this study uses the
StereoSet dataset, which covers multiple stereotype dimensions including
gender, profession, religion, and race. The evaluation focuses on several key
metrics, such as bias detection accuracy, semantic consistency, and contextual
sensitivity. Experimental results show that the proposed method achieves strong
detection performance across various dimensions. It can accurately identify
bias differences between semantically similar texts while maintaining high
semantic alignment and output stability. The method also demonstrates high
interpretability in its structural design. It helps uncover the internal bias
association mechanisms within language models. This provides a more transparent
and reliable technical foundation for bias detection. The approach is suitable
for real-world applications where high trustworthiness of generated content is
required.

</details>


### [148] [One Size Does Not Fit All: A Distribution-Aware Sparsification for More Precise Model Merging](https://arxiv.org/abs/2508.06163)
*Yingfeng Luo,Dingyang Lin,Junxin Wang,Ziqiang Xu,Kaiyan Chang,Tong Zheng,Bei Li,Anxiang Ma,Tong Xiao,Zhengtao Yu,Jingbo Zhu*

Main category: cs.CL

TL;DR: TADrop introduces an adaptive sparsification strategy for model merging, outperforming uniform sparsity methods by tailoring pruning to parameter tensors' properties.


<details>
  <summary>Details</summary>
Motivation: Current model merging methods use uniform sparsity, ignoring parameter heterogeneity, leading to suboptimal performance. TADrop addresses this by adapting sparsity levels to each tensor.

Method: TADrop assigns custom sparsity levels to parameter tensors based on their distributional properties, pruning denser tensors more aggressively.

Result: TADrop boosts performance across diverse tasks and models, e.g., achieving a 2.0% average gain on ViT-B/32 tasks.

Conclusion: TADrop offers a more effective, structure-aware approach to model merging, setting a new baseline for high-performance fusion.

Abstract: Model merging has emerged as a compelling data-free paradigm for multi-task
learning, enabling the fusion of multiple fine-tuned models into a single,
powerful entity. A key technique in merging methods is sparsification, which
prunes redundant parameters from task vectors to mitigate interference.
However, prevailing approaches employ a ``one-size-fits-all'' strategy,
applying a uniform sparsity ratio that overlooks the inherent structural and
statistical heterogeneity of model parameters. This often leads to a suboptimal
trade-off, where critical parameters are inadvertently pruned while less useful
ones are retained. To address this limitation, we introduce \textbf{TADrop}
(\textbf{T}ensor-wise \textbf{A}daptive \textbf{Drop}), an adaptive
sparsification strategy that respects this heterogeneity. Instead of a global
ratio, TADrop assigns a tailored sparsity level to each parameter tensor based
on its distributional properties. The core intuition is that tensors with
denser, more redundant distributions can be pruned aggressively, while sparser,
more critical ones are preserved. As a simple and plug-and-play module, we
validate TADrop by integrating it with foundational, classic, and SOTA merging
methods. Extensive experiments across diverse tasks (vision, language, and
multimodal) and models (ViT, BEiT) demonstrate that TADrop consistently and
significantly boosts their performance. For instance, when enhancing a leading
merging method, it achieves an average performance gain of 2.0\% across 8
ViT-B/32 tasks. TADrop provides a more effective way to mitigate parameter
interference by tailoring sparsification to the model's structure, offering a
new baseline for high-performance model merging.

</details>


### [149] [UR$^2$: Unify RAG and Reasoning through Reinforcement Learning](https://arxiv.org/abs/2508.06165)
*Weitao Li,Boran Xiang,Xiaolong Wang,Zhinan Gou,Weizhi Ma,Yang Liu*

Main category: cs.CL

TL;DR: UR2 unifies retrieval-augmented generation (RAG) and reinforcement learning (RLVR) for broader task adaptability, outperforming existing methods and matching GPT-4 variants on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current RAG and RLVR methods are isolated, limiting generalization. UR2 aims to integrate them dynamically for broader applicability.

Method: UR2 uses difficulty-aware curriculum training and hybrid knowledge access (offline corpora + LLM summaries) to unify retrieval and reasoning.

Result: UR2 outperforms existing RAG and RL methods, matching GPT-4 variants on open-domain QA, MMLU-Pro, medical, and math tasks.

Conclusion: UR2 successfully bridges the gap between retrieval and reasoning, offering a flexible and high-performing framework for diverse tasks.

Abstract: Large Language Models (LLMs) have shown remarkable capabilities through two
complementary paradigms: Retrieval-Augmented Generation (RAG), which enhances
knowledge grounding, and Reinforcement Learning from Verifiable Rewards (RLVR),
which optimizes complex reasoning abilities. However, these two capabilities
are often developed in isolation, and existing efforts to unify them remain
narrow in scope-typically limited to open-domain QA with fixed retrieval
settings and task-specific assumptions. This lack of integration constrains
generalization and limits the applicability of RAG-RL methods to broader
domains. To bridge this gap, we propose UR2 (Unified RAG and Reasoning), a
general framework that unifies retrieval and reasoning through reinforcement
learning. UR2 introduces two key contributions: a difficulty-aware curriculum
training that selectively invokes retrieval only for challenging problems, and
a hybrid knowledge access strategy combining domain-specific offline corpora
with LLM-generated summaries. These components are designed to enable dynamic
coordination between retrieval and reasoning, improving adaptability across a
diverse range of tasks. Experiments across open-domain QA, MMLU-Pro, medical,
and mathematical reasoning tasks demonstrate that UR2 (built on Qwen2.5-3/7B
and LLaMA-3.1-8B) significantly outperforms existing RAG and RL methods,
achieving comparable performance to GPT-4o-mini and GPT-4.1-mini on several
benchmarks. We have released all code, models, and data at
https://github.com/Tsinghua-dhy/UR2.

</details>


### [150] [Pragmatics beyond humans: meaning, communication, and LLMs](https://arxiv.org/abs/2508.06167)
*V√≠t Gvo≈ædiak*

Main category: cs.CL

TL;DR: The paper redefines pragmatics as a dynamic interface for language in social action, critiques traditional theories in light of LLMs, and proposes new frameworks like HMC and probabilistic pragmatics to address human-machine communication challenges.


<details>
  <summary>Details</summary>
Motivation: To address the inadequacy of traditional pragmatic theories in explaining communication involving large language models (LLMs) and to propose more suitable frameworks.

Method: Critiques traditional semiotic hierarchies, examines human-machine communication tensions, and introduces the Human-Machine Communication (HMC) framework and probabilistic pragmatics.

Result: Highlights the limitations of human-centered pragmatics for LLMs, proposes alternative frameworks, and identifies issues like substitutionalism and context frustration.

Conclusion: Pragmatic theory needs adjustment or expansion to better account for generative AI communication, emphasizing co-construction of pragmatic conditions.

Abstract: The paper reconceptualizes pragmatics not as a subordinate, third dimension
of meaning, but as a dynamic interface through which language operates as a
socially embedded tool for action. With the emergence of large language models
(LLMs) in communicative contexts, this understanding needs to be further
refined and methodologically reconsidered. The first section challenges the
traditional semiotic trichotomy, arguing that connectionist LLM architectures
destabilize established hierarchies of meaning, and proposes the Human-Machine
Communication (HMC) framework as a more suitable alternative. The second
section examines the tension between human-centred pragmatic theories and the
machine-centred nature of LLMs. While traditional, Gricean-inspired pragmatics
continue to dominate, it relies on human-specific assumptions ill-suited to
predictive systems like LLMs. Probabilistic pragmatics, particularly the
Rational Speech Act framework, offers a more compatible teleology by focusing
on optimization rather than truth-evaluation. The third section addresses the
issue of substitutionalism in three forms - generalizing, linguistic, and
communicative - highlighting the anthropomorphic biases that distort LLM
evaluation and obscure the role of human communicative subjects. Finally, the
paper introduces the concept of context frustration to describe the paradox of
increased contextual input paired with a collapse in contextual understanding,
emphasizing how users are compelled to co-construct pragmatic conditions both
for the model and themselves. These arguments suggest that pragmatic theory may
need to be adjusted or expanded to better account for communication involving
generative AI.

</details>


### [151] [Comparing Knowledge Injection Methods for LLMs in a Low-Resource Regime](https://arxiv.org/abs/2508.06178)
*Hugo Abonizio,Thales Almeida,Roberto Lotufo,Rodrigo Nogueira*

Main category: cs.CL

TL;DR: The paper explores methods for injecting small, unstructured information into LLMs, addressing challenges like catastrophic forgetting and limited data. It compares continued pre-training with synthetic data augmentation, finding diversity improves learning. It also examines RAG sensitivity and introduces self-generated synthetic data as a solution.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of updating LLMs with small datasets and mitigate catastrophic forgetting while improving knowledge acquisition.

Method: Evaluated knowledge acquisition using news datasets and QA pairs, comparing continued pre-training with synthetic data augmentation and RAG-based approaches.

Result: Diverse textual variations significantly improve learning; RAG methods degrade control datasets more than parametric methods. Self-generated synthetic data shows promise.

Conclusion: Diverse prompting and synthetic data generation enhance knowledge injection in LLMs with limited data, offering a pathway for self-improving updates.

Abstract: Large language models (LLMs) often require vast amounts of text to
effectively acquire new knowledge. While continuing pre-training on large
corpora or employing retrieval-augmented generation (RAG) has proven
successful, updating an LLM with only a few thousand or million tokens remains
challenging. In this work, we investigate the task of injecting small,
unstructured information into LLMs and its relation to the catastrophic
forgetting phenomenon. We use a dataset of recent news -- ensuring no overlap
with the model's pre-training data -- to evaluate the knowledge acquisition by
probing the model with question-answer pairs related the learned information.
Starting from a continued pre-training baseline, we explored different
augmentation algorithms to generate synthetic data to improve the knowledge
acquisition capabilities. Our experiments show that simply continuing
pre-training on limited data yields modest improvements, whereas exposing the
model to diverse textual variations significantly improves the learning of new
facts -- particularly with methods that induce greater variability through
diverse prompting. Furthermore, we shed light on the forgetting phenomenon in
small-data regimes, illustrating the delicate balance between learning new
content and retaining existing capabilities. We also confirm the sensitivity of
RAG-based approaches for knowledge injection, which often lead to greater
degradation on control datasets compared to parametric methods. Finally, we
demonstrate that models can generate effective synthetic training data
themselves, suggesting a pathway toward self-improving model updates. All code
and generated data used in our experiments are publicly available, providing a
resource for studying efficient knowledge injection in LLMs with limited data
at https://github.com/hugoabonizio/knowledge-injection-methods.

</details>


### [152] [DKG-LLM : A Framework for Medical Diagnosis and Personalized Treatment Recommendations via Dynamic Knowledge Graph and Large Language Model Integration](https://arxiv.org/abs/2508.06186)
*Ali Sarabadani,Maryam Abdollahi Shamami,Hamidreza Sadeghsalehi,Borhan Asadi,Saba Hesaraki*

Main category: cs.CL

TL;DR: The DKG-LLM framework integrates a dynamic knowledge graph with the Grok 3 LLM for medical diagnosis and treatment recommendations, achieving high accuracy and semantic coverage.


<details>
  <summary>Details</summary>
Motivation: To enhance medical diagnosis and personalized treatment by leveraging LLMs and dynamic knowledge graphs for noisy, complex data.

Method: Uses the Adaptive Semantic Fusion Algorithm (ASFA) to dynamically generate and update a knowledge graph from heterogeneous medical data, integrating it with the Grok 3 LLM.

Result: Achieves 84.19% diagnostic accuracy, 89.63% treatment recommendation accuracy, and 93.48% semantic coverage.

Conclusion: DKG-LLM is a reliable, transformative tool for medical applications, handling complex data and learning from physician feedback.

Abstract: Large Language Models (LLMs) have grown exponentially since the release of
ChatGPT. These models have gained attention due to their robust performance on
various tasks, including language processing tasks. These models achieve
understanding and comprehension of tasks by training billions of parameters.
The development of these models is a transformative force in enhancing natural
language understanding and has taken a significant step towards artificial
general intelligence (AGI). In this study, we aim to present the DKG-LLM
framework. The DKG-LLM framework introduces a groundbreaking approach to
medical diagnosis and personalized treatment recommendations by integrating a
dynamic knowledge graph (DKG) with the Grok 3 large language model. Using the
Adaptive Semantic Fusion Algorithm (ASFA), heterogeneous medical data
(including clinical reports and PubMed articles) and patient records
dynamically generate a knowledge graph consisting of 15,964 nodes in 13
distinct types (e.g., diseases, symptoms, treatments, patient profiles) and
127,392 edges in 26 relationship types (e.g., causal, therapeutic,
association). ASFA utilizes advanced probabilistic models, Bayesian inference,
and graph optimization to extract semantic information, dynamically updating
the graph with approximately 150 new nodes and edges in each data category
while maintaining scalability with up to 987,654 edges. Real-world datasets,
including MIMIC-III and PubMed, were utilized to evaluate the proposed
architecture. The evaluation results show that DKG-LLM achieves a diagnostic
accuracy of 84.19%. The model also has a treatment recommendation accuracy of
89.63% and a semantic coverage of 93.48%. DKG-LLM is a reliable and
transformative tool that handles noisy data and complex multi-symptom diseases,
along with feedback-based learning from physician input.

</details>


### [153] [Beyond Uniform Criteria: Scenario-Adaptive Multi-Dimensional Jailbreak Evaluation](https://arxiv.org/abs/2508.06194)
*Lai Jiang,Yuekang Li,Xiaohan Zhang,Youtao Ding,Li Pan*

Main category: cs.CL

TL;DR: SceneJailEval introduces a scenario-adaptive multi-dimensional framework for jailbreak evaluation, outperforming existing methods with higher precision and adaptability.


<details>
  <summary>Details</summary>
Motivation: Current jailbreak evaluation methods lack precision due to binary classifications and uniform criteria, leading to scenario-specific mismatches.

Method: SceneJailEval uses a scenario-adaptive framework and a diverse 14-scenario dataset for evaluation.

Result: Achieves state-of-the-art F1 scores (0.917 full-scenario, 0.995 JBB), surpassing prior methods.

Conclusion: SceneJailEval addresses limitations of existing methods, offering precise, adaptable, and extensible jailbreak evaluation.

Abstract: Precise jailbreak evaluation is vital for LLM red teaming and jailbreak
research. Current approaches employ binary classification ( e.g., string
matching, toxic text classifiers, LLM-driven methods), yielding only "yes/no"
labels without quantifying harm intensity. Existing multi-dimensional
frameworks ( e.g., Security Violation, Relative Truthfulness, Informativeness)
apply uniform evaluation criteria across scenarios, resulting in
scenario-specific mismatches--for instance, "Relative Truthfulness" is
irrelevant to "hate speech"--which compromise evaluation precision. To tackle
these limitations, we introduce SceneJailEval, with key contributions: (1) A
groundbreaking scenario-adaptive multi-dimensional framework for jailbreak
evaluation, overcoming the critical "one-size-fits-all" constraint of existing
multi-dimensional methods, and featuring strong extensibility to flexibly adapt
to customized or emerging scenarios. (2) A comprehensive 14-scenario dataset
with diverse jailbreak variants and regional cases, filling the long-standing
gap in high-quality, holistic benchmarks for scenario-adaptive evaluation. (3)
SceneJailEval achieves state-of-the-art results, with an F1 score of 0.917 on
our full-scenario dataset (+6% over prior SOTA) and 0.995 on JBB (+3% over
prior SOTA), surpassing accuracy limits of existing evaluation methods in
heterogeneous scenarios and confirming its advantage.

</details>


### [154] [EICAP: Deep Dive in Assessment and Enhancement of Large Language Models in Emotional Intelligence through Multi-Turn Conversations](https://arxiv.org/abs/2508.06196)
*Nizi Nazar,Ehsaneddin Asgari*

Main category: cs.CL

TL;DR: The paper introduces a four-layer EI taxonomy for LLMs, presents the EICAP-Bench benchmark, evaluates six LLMs, and fine-tunes Qwen2.5 models, finding limited EI improvement through current methods.


<details>
  <summary>Details</summary>
Motivation: Address the underexplored role of Emotional Intelligence (EI) in human-aligned LLMs by developing a unified taxonomy and benchmark.

Method: Proposes a four-layer EI taxonomy, creates EICAP-Bench for evaluation, and fine-tunes Qwen2.5 models using LoRA on UltraChat dataset.

Result: Qwen2.5-Instruct performs best; fine-tuning improves only the Appraisal layer, showing limitations in current EI training methods.

Conclusion: Current pretraining and instruction-tuning are insufficient for deep EI in LLMs, calling for targeted data and modeling strategies.

Abstract: Emotional Intelligence (EI) is a critical yet underexplored dimension in the
development of human-aligned LLMs. To address this gap, we introduce a unified,
psychologically grounded four-layer taxonomy of EI tailored for large language
models (LLMs), encompassing emotional tracking, cause inference, appraisal, and
emotionally appropriate response generation. Building on this framework, we
present EICAP-Bench, a novel MCQ style multi-turn benchmark designed to
evaluate EI capabilities in open-source LLMs across diverse linguistic and
cultural contexts. We evaluate six LLMs: LLaMA3 (8B), LLaMA3-Instruct, Gemma
(9B), Gemma-Instruct, Qwen2.5 (7B), and Qwen2.5-Instruct on EmoCap-Bench,
identifying Qwen2.5-Instruct as the strongest baseline. To assess the potential
for enhancing EI capabilities, we fine-tune both Qwen2.5-Base and
Qwen2.5-Instruct using LoRA adapters on UltraChat (UC), a large-scale,
instruction-tuned dialogue dataset, in both English and Arabic. Our statistical
analysis reveals that among the five EI layers, only the Appraisal layer shows
significant improvement through UC-based fine-tuning. These findings highlight
the limitations of existing pretraining and instruction-tuning paradigms in
equipping LLMs with deeper emotional reasoning and underscore the need for
targeted data and modeling strategies for comprehensive EI alignment.

</details>


### [155] [Classification is a RAG problem: A case study on hate speech detection](https://arxiv.org/abs/2508.06204)
*Richard Willats,Josh Pennington,Aravind Mohan,Bertie Vidgen*

Main category: cs.CL

TL;DR: The paper introduces Retrieval-Augmented Generation (RAG) for content moderation, shifting from static classification to dynamic policy evaluation, achieving robust accuracy, explainability, and adaptability without retraining.


<details>
  <summary>Details</summary>
Motivation: To address the need for adaptable content moderation systems that can evolve with policies without costly retraining.

Method: Uses a Contextual Policy Engine (CPE) based on RAG, which retrieves and evaluates content against contextual knowledge at inference time.

Result: Demonstrates strong performance, explainability, and dynamic policy updates without retraining, with fine-grained control over specific identity groups.

Conclusion: RAG transforms classification into a flexible, transparent, and adaptable process for content moderation and broader applications.

Abstract: Robust content moderation requires classification systems that can quickly
adapt to evolving policies without costly retraining. We present classification
using Retrieval-Augmented Generation (RAG), which shifts traditional
classification tasks from determining the correct category in accordance with
pre-trained parameters to evaluating content in relation to contextual
knowledge retrieved at inference. In hate speech detection, this transforms the
task from "is this hate speech?" to "does this violate the hate speech policy?"
  Our Contextual Policy Engine (CPE) - an agentic RAG system - demonstrates
this approach and offers three key advantages: (1) robust classification
accuracy comparable to leading commercial systems, (2) inherent explainability
via retrieved policy segments, and (3) dynamic policy updates without model
retraining. Through three experiments, we demonstrate strong baseline
performance and show that the system can apply fine-grained policy control by
correctly adjusting protection for specific identity groups without requiring
retraining or compromising overall performance. These findings establish that
RAG can transform classification into a more flexible, transparent, and
adaptable process for content moderation and wider classification problems.

</details>


### [156] [InfoCausalQA:Can Models Perform Non-explicit Causal Reasoning Based on Infographic?](https://arxiv.org/abs/2508.06220)
*Keummin Ka,Junhyeong Park,Jahyun Jeon,Youngjae Yu*

Main category: cs.CL

TL;DR: InfoCausalQA is a new benchmark for evaluating causal reasoning in VLMs using infographics. Current VLMs show limited performance, especially in semantic causal reasoning, compared to humans.


<details>
  <summary>Details</summary>
Motivation: Causal inference in multimodal settings is underexplored in VLMs, despite its importance in human cognition.

Method: The benchmark includes two tasks (quantitative and semantic causal reasoning) with 1,482 QA pairs generated by GPT-4o and refined by humans.

Result: VLMs perform poorly, especially in semantic reasoning, lagging behind human capabilities.

Conclusion: InfoCausalQA underscores the need to improve causal reasoning in multimodal AI systems.

Abstract: Recent advances in Vision-Language Models (VLMs) have demonstrated impressive
capabilities in perception and reasoning. However, the ability to perform
causal inference -- a core aspect of human cognition -- remains underexplored,
particularly in multimodal settings. In this study, we introduce InfoCausalQA,
a novel benchmark designed to evaluate causal reasoning grounded in
infographics that combine structured visual data with textual context. The
benchmark comprises two tasks: Task 1 focuses on quantitative causal reasoning
based on inferred numerical trends, while Task 2 targets semantic causal
reasoning involving five types of causal relations: cause, effect,
intervention, counterfactual, and temporal. We manually collected 494
infographic-text pairs from four public sources and used GPT-4o to generate
1,482 high-quality multiple-choice QA pairs. These questions were then
carefully revised by humans to ensure they cannot be answered based on
surface-level cues alone but instead require genuine visual grounding. Our
experimental results reveal that current VLMs exhibit limited capability in
computational reasoning and even more pronounced limitations in semantic causal
reasoning. Their significantly lower performance compared to humans indicates a
substantial gap in leveraging infographic-based information for causal
inference. Through InfoCausalQA, we highlight the need for advancing the causal
reasoning abilities of multimodal AI systems.

</details>


### [157] [Large Language Model Data Generation for Enhanced Intent Recognition in German Speech](https://arxiv.org/abs/2508.06277)
*Theresa Pekarek Rosin,Burak Can Kaplan,Stefan Wermter*

Main category: cs.CL

TL;DR: The paper proposes a novel intent recognition (IR) approach for elderly German speakers, combining an adapted Whisper ASR model with Transformer-based language models trained on synthetic data from LLMs (LeoLM, Llama3, ChatGPT). Synthetic data improves performance, with LeoLM outperforming ChatGPT.


<details>
  <summary>Details</summary>
Motivation: Existing IR systems are limited to short English commands, leaving a gap for elderly German speakers. The paper aims to address this by leveraging synthetic data from LLMs.

Method: The approach combines a fine-tuned Whisper ASR model for elderly German speech with Transformer-based language models trained on synthetic text from LeoLM, Llama3, and ChatGPT. Synthetic speech is used for cross-dataset testing.

Result: Synthetic LLM-generated data enhances classification performance and robustness. LeoLM (13B) outperforms ChatGPT (175B) for German intent recognition.

Conclusion: Generative AI can bridge data gaps in low-resource domains like elderly German speech. The method is transparent and reproducible.

Abstract: Intent recognition (IR) for speech commands is essential for artificial
intelligence (AI) assistant systems; however, most existing approaches are
limited to short commands and are predominantly developed for English. This
paper addresses these limitations by focusing on IR from speech by elderly
German speakers. We propose a novel approach that combines an adapted Whisper
ASR model, fine-tuned on elderly German speech (SVC-de), with Transformer-based
language models trained on synthetic text datasets generated by three
well-known large language models (LLMs): LeoLM, Llama3, and ChatGPT. To
evaluate the robustness of our approach, we generate synthetic speech with a
text-to-speech model and conduct extensive cross-dataset testing. Our results
show that synthetic LLM-generated data significantly boosts classification
performance and robustness to different speaking styles and unseen vocabulary.
Notably, we find that LeoLM, a smaller, domain-specific 13B LLM, surpasses the
much larger ChatGPT (175B) in dataset quality for German intent recognition.
Our approach demonstrates that generative AI can effectively bridge data gaps
in low-resource domains. We provide detailed documentation of our data
generation and training process to ensure transparency and reproducibility.

</details>


### [158] [Matrix-Driven Instant Review: Confident Detection and Reconstruction of LLM Plagiarism on PC](https://arxiv.org/abs/2508.06309)
*Ruichong Zhang*

Main category: cs.CL

TL;DR: MDIR is a new method for detecting LLM plagiarism using matrix analysis and Large Deviation Theory, offering accurate weight reconstruction, p-value estimation, and efficiency.


<details>
  <summary>Details</summary>
Motivation: Growing concerns about IP theft in LLMs and shortcomings of existing plagiarism detection methods.

Method: Matrix-Driven Instant Review (MDIR) leverages matrix analysis and Large Deviation Theory for weight similarity analysis.

Result: MDIR reliably detects plagiarism post-transformations and operates efficiently on a single PC.

Conclusion: MDIR addresses key limitations of existing methods, providing a robust and accessible solution for LLM plagiarism detection.

Abstract: In recent years, concerns about intellectual property (IP) in large language
models (LLMs) have grown significantly. Plagiarizing other LLMs (through direct
weight copying, upcycling, pruning, or continual pretraining) and claiming
authorship without properly attributing to the original license, is a serious
misconduct that can lead to significant financial and reputational harm to the
original developers. However, existing methods for detecting LLM plagiarism
fall short in key areas. They fail to accurately reconstruct weight
correspondences, lack the ability to compute statistical significance measures
such as $p$-values, and may mistakenly flag models trained on similar data as
being related. To address these limitations, we propose Matrix-Driven Instant
Review (MDIR), a novel method that leverages matrix analysis and Large
Deviation Theory. MDIR achieves accurate reconstruction of weight
relationships, provides rigorous $p$-value estimation, and focuses exclusively
on weight similarity without requiring full model inference. Experimental
results demonstrate that MDIR reliably detects plagiarism even after extensive
transformations, such as random permutations and continual pretraining with
trillions of tokens. Moreover, all detections can be performed on a single PC
within an hour, making MDIR both efficient and accessible.

</details>


### [159] [Harnessing Adaptive Topology Representations for Zero-Shot Graph Question Answering](https://arxiv.org/abs/2508.06345)
*Yanbin Wei,Jiangyue Yan,Chun Kang,Yang Chen,Hua Liu,James T. Kwok,Yu Zhang*

Main category: cs.CL

TL;DR: The paper introduces DynamicTRF, a framework to improve zero-shot graph QA by dynamically selecting the best graph representation form (TRF) for each question, enhancing accuracy and conciseness.


<details>
  <summary>Details</summary>
Motivation: Current approaches use a single graph representation (TRF) for all tasks, leading to incorrect or verbose responses. The paper aims to address this by tailoring TRFs to zero-shot graph QA and introducing a new metric (GRE) for performance and brevity.

Method: The authors analyze existing TRFs, design a set of TRFs ($F_{ZS}$), and introduce the GRE metric. They create a TRF Preference (TRFP) dataset and train a TRF router to dynamically assign the best TRF for each question.

Result: DynamicTRF significantly improves zero-shot graph QA accuracy across 7 in-domain and 2 out-of-domain tasks.

Conclusion: DynamicTRF enhances LMMs' zero-shot graph QA by dynamically selecting optimal TRFs, balancing accuracy and conciseness.

Abstract: Large Multimodal Models (LMMs) have shown generalized zero-shot capabilities
in diverse domain question-answering (QA) tasks, including graph QA that
involves complex graph topologies. However, most current approaches use only a
single type of graph representation, namely Topology Representation Form (TRF),
such as prompt-unified text descriptions or style-fixed visual styles. Those
"one-size-fits-all" approaches fail to consider the specific preferences of
different models or tasks, often leading to incorrect or overly long responses.
To address this, we first analyze the characteristics and weaknesses of
existing TRFs, and then design a set of TRFs, denoted by $F_{ZS}$, tailored to
zero-shot graph QA. We then introduce a new metric, Graph Response Efficiency
(GRE), which measures the balance between the performance and the brevity in
graph QA. Built on these, we develop the DynamicTRF framework, which aims to
improve both the accuracy and conciseness of graph QA. To be specific,
DynamicTRF first creates a TRF Preference (TRFP) dataset that ranks TRFs based
on their GRE scores, to probe the question-specific TRF preferences. Then it
trains a TRF router on the TRFP dataset, to adaptively assign the best TRF from
$F_{ZS}$ for each question during the inference. Extensive experiments across 7
in-domain algorithmic graph QA tasks and 2 out-of-domain downstream tasks show
that DynamicTRF significantly enhances the zero-shot graph QA of LMMs in terms
of accuracy

</details>


### [160] [Cyberbullying Detection via Aggression-Enhanced Prompting](https://arxiv.org/abs/2508.06360)
*Aisha Saeid,Anu Sabu,Girish A. Koushik,Ferrante Neri,Diptesh Kanojia*

Main category: cs.CL

TL;DR: The study explores integrating aggression detection as an auxiliary task to improve cyberbullying detection in LLMs, proposing an enriched prompt pipeline that outperforms standard methods.


<details>
  <summary>Details</summary>
Motivation: Cyberbullying detection is challenging due to subtle expressions. The study aims to enhance LLM performance by leveraging aggression detection as an auxiliary task.

Method: Experiments on five aggression datasets and one cyberbullying dataset using instruction-tuned LLMs. Evaluated zero-shot, few-shot, LoRA fine-tuning, and multi-task learning, then proposed an enriched prompt pipeline.

Result: The enriched prompt pipeline consistently outperforms standard LoRA fine-tuning, showing aggression-informed context boosts detection.

Conclusion: Auxiliary tasks like aggression detection can enhance LLM generalization for safety-critical social media applications.

Abstract: Detecting cyberbullying on social media remains a critical challenge due to
its subtle and varied expressions. This study investigates whether integrating
aggression detection as an auxiliary task within a unified training framework
can enhance the generalisation and performance of large language models (LLMs)
in cyberbullying detection. Experiments are conducted on five aggression
datasets and one cyberbullying dataset using instruction-tuned LLMs. We
evaluated multiple strategies: zero-shot, few-shot, independent LoRA
fine-tuning, and multi-task learning (MTL). Given the inconsistent results of
MTL, we propose an enriched prompt pipeline approach in which aggression
predictions are embedded into cyberbullying detection prompts to provide
contextual augmentation. Preliminary results show that the enriched prompt
pipeline consistently outperforms standard LoRA fine-tuning, indicating that
aggression-informed context significantly boosts cyberbullying detection. This
study highlights the potential of auxiliary tasks, such as aggression
detection, to improve the generalisation of LLMs for safety-critical
applications on social networks.

</details>


### [161] [Evaluating Style-Personalized Text Generation: Challenges and Directions](https://arxiv.org/abs/2508.06374)
*Anubhav Jangra,Bahareh Sarrafzadeh,Adrian de Wynter,Silviu Cucerzan,Sujay Kumar Jauhar*

Main category: cs.CL

TL;DR: The paper critiques traditional metrics like BLEU and ROUGE for evaluating style-personalized text generation, proposing diverse metrics like style embeddings and LLM-as-judge, validated through a comprehensive benchmark.


<details>
  <summary>Details</summary>
Motivation: To address the lack of effective evaluation in low-resource author style personalized text generation and challenge conventional metrics.

Method: Proposes alternative metrics (style embeddings, LLM-as-judge) and tests them via a benchmark covering eight tasks and three settings (domain discrimination, authorship attribution, LLM discrimination).

Result: Shows that an ensemble of diverse metrics is more effective for evaluating style-personalized text generation.

Conclusion: Advocates for using a combination of metrics to holistically assess style-personalized text generation.

Abstract: While prior research has built tools and benchmarks towards style
personalized text generation, there has been limited exploration of evaluation
in low-resource author style personalized text generation space. Through this
work, we question the effectiveness of the widely adopted evaluation metrics
like BLEU and ROUGE, and explore other evaluation paradigms such as style
embeddings and LLM-as-judge to holistically evaluate the style personalized
text generation task. We evaluate these metrics and their ensembles using our
style discrimination benchmark, that spans eight writing tasks, and evaluates
across three settings, domain discrimination, authorship attribution, and LLM
personalized vs non-personalized discrimination. We provide conclusive evidence
to adopt ensemble of diverse evaluation metrics to effectively evaluate style
personalized text generation.

</details>


### [162] [LLMs vs. Chinese Anime Enthusiasts: A Comparative Study on Emotionally Supportive Role-Playing](https://arxiv.org/abs/2508.06388)
*Lanlan Qiu,Xiao Pu,Yeqi Feng,Tianxing He*

Main category: cs.CL

TL;DR: The paper introduces ChatAnime, a dataset for Emotionally Supportive Role-Playing (ESRP) using LLMs, focusing on anime characters. It evaluates LLMs' performance in role-playing and emotional support, showing they outperform humans in some metrics but lag in diversity.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap in combining role-playing and emotional support capabilities of LLMs, using anime characters for their defined personalities and fan bases.

Method: Created ChatAnime dataset with 20 anime characters, 60 scenarios, and 40 enthusiasts. Collected dialogue data from 10 LLMs and humans, evaluated using 9 metrics.

Result: Top LLMs excel in role-playing and emotional support but trail humans in response diversity.

Conclusion: The work provides resources for optimizing LLMs in ESRP, with datasets publicly available.

Abstract: Large Language Models (LLMs) have demonstrated impressive capabilities in
role-playing conversations and providing emotional support as separate research
directions. However, there remains a significant research gap in combining
these capabilities to enable emotionally supportive interactions with virtual
characters. To address this research gap, we focus on anime characters as a
case study because of their well-defined personalities and large fan bases.
This choice enables us to effectively evaluate how well LLMs can provide
emotional support while maintaining specific character traits. We introduce
ChatAnime, the first Emotionally Supportive Role-Playing (ESRP) dataset. We
first thoughtfully select 20 top-tier characters from popular anime communities
and design 60 emotion-centric real-world scenario questions. Then, we execute a
nationwide selection process to identify 40 Chinese anime enthusiasts with
profound knowledge of specific characters and extensive experience in
role-playing. Next, we systematically collect two rounds of dialogue data from
10 LLMs and these 40 Chinese anime enthusiasts. To evaluate the ESRP
performance of LLMs, we design a user experience-oriented evaluation system
featuring 9 fine-grained metrics across three dimensions: basic dialogue,
role-playing and emotional support, along with an overall metric for response
diversity. In total, the dataset comprises 2,400 human-written and 24,000
LLM-generated answers, supported by over 132,000 human annotations.
Experimental results show that top-performing LLMs surpass human fans in
role-playing and emotional support, while humans still lead in response
diversity. We hope this work can provide valuable resources and insights for
future research on optimizing LLMs in ESRP. Our datasets are available at
https://github.com/LanlanQiu/ChatAnime.

</details>


### [163] [Quantifying Conversation Drift in MCP via Latent Polytope](https://arxiv.org/abs/2508.06418)
*Haoran Shi,Hongwei Yao,Shuo Shao,Shaopeng Jiao,Ziqi Peng,Zhan Qin,Cong Wang*

Main category: cs.CL

TL;DR: SecMCP is a secure framework for detecting and mitigating security risks in Model Context Protocol (MCP)-enhanced LLMs, addressing tool poisoning and prompt injection attacks by analyzing latent space trajectories.


<details>
  <summary>Details</summary>
Motivation: MCP's non-isolated execution context introduces security and privacy risks like tool poisoning and indirect prompt injection, which existing defenses fail to address effectively.

Method: SecMCP models LLM activation vectors in a latent polytope space to detect anomalous shifts in conversational dynamics, enabling proactive threat detection.

Result: SecMCP achieves robust detection (AUROC > 0.915) on benchmark datasets (MS MARCO, HotpotQA, FinQA) with three LLMs (Llama3, Vicuna, Mistral).

Conclusion: SecMCP provides a systematic solution to MCP security threats, validated empirically, and introduces a novel methodology for quantifying conversation drift.

Abstract: The Model Context Protocol (MCP) enhances large language models (LLMs) by
integrating external tools, enabling dynamic aggregation of real-time data to
improve task execution. However, its non-isolated execution context introduces
critical security and privacy risks. In particular, adversarially crafted
content can induce tool poisoning or indirect prompt injection, leading to
conversation hijacking, misinformation propagation, or data exfiltration.
Existing defenses, such as rule-based filters or LLM-driven detection, remain
inadequate due to their reliance on static signatures, computational
inefficiency, and inability to quantify conversational hijacking. To address
these limitations, we propose SecMCP, a secure framework that detects and
quantifies conversation drift, deviations in latent space trajectories induced
by adversarial external knowledge. By modeling LLM activation vectors within a
latent polytope space, SecMCP identifies anomalous shifts in conversational
dynamics, enabling proactive detection of hijacking, misleading, and data
exfiltration. We evaluate SecMCP on three state-of-the-art LLMs (Llama3,
Vicuna, Mistral) across benchmark datasets (MS MARCO, HotpotQA, FinQA),
demonstrating robust detection with AUROC scores exceeding 0.915 while
maintaining system usability. Our contributions include a systematic
categorization of MCP security threats, a novel latent polytope-based
methodology for quantifying conversation drift, and empirical validation of
SecMCP's efficacy.

</details>


### [164] [Memp: Exploring Agent Procedural Memory](https://arxiv.org/abs/2508.06433)
*Runnan Fang,Yuan Liang,Xiaobin Wang,Jialong Wu,Shuofei Qiao,Pengjun Xie,Fei Huang,Huajun Chen,Ningyu Zhang*

Main category: cs.CL

TL;DR: Memp enhances LLM agents with learnable, updatable procedural memory, improving task success and efficiency.


<details>
  <summary>Details</summary>
Motivation: Address brittle procedural memory in LLM agents by making it learnable and updatable.

Method: Proposes Memp, which distills past trajectories into fine-grained instructions and script-like abstractions, with strategies for Build, Retrieval, and Update.

Result: Agents achieve higher success rates and efficiency; procedural memory from stronger models benefits weaker ones.

Conclusion: Memp enables lifelong learning in LLM agents, improving performance and adaptability.

Abstract: Large Language Models (LLMs) based agents excel at diverse tasks, yet they
suffer from brittle procedural memory that is manually engineered or entangled
in static parameters. In this work, we investigate strategies to endow agents
with a learnable, updatable, and lifelong procedural memory. We propose Memp
that distills past agent trajectories into both fine-grained, step-by-step
instructions and higher-level, script-like abstractions, and explore the impact
of different strategies for Build, Retrieval, and Update of procedural memory.
Coupled with a dynamic regimen that continuously updates, corrects, and
deprecates its contents, this repository evolves in lockstep with new
experience. Empirical evaluation on TravelPlanner and ALFWorld shows that as
the memory repository is refined, agents achieve steadily higher success rates
and greater efficiency on analogous tasks. Moreover, procedural memory built
from a stronger model retains its value: migrating the procedural memory to a
weaker model yields substantial performance gains.

</details>


### [165] [Learning the Topic, Not the Language: How LLMs Classify Online Immigration Discourse Across Languages](https://arxiv.org/abs/2508.06435)
*Andrea Nasuto,Stefano Maria Iacus,Francisco Rowe,Devika Jain*

Main category: cs.CL

TL;DR: LLMs fine-tuned in one or two languages can classify immigration-related content in unseen languages, but stance detection benefits from multilingual fine-tuning. Minimal exposure to underrepresented languages corrects pre-training biases, and lightweight models offer cost-effective alternatives.


<details>
  <summary>Details</summary>
Motivation: To explore whether knowledge from fine-tuning in a few languages transfers to unseen languages and if pre-training biases can be corrected with minimal intervention.

Method: Fine-tuned lightweight LLaMA models on monolingual, bilingual, or multilingual datasets to classify immigration-related tweets across 13 languages.

Result: LLMs fine-tuned in one or two languages classify content reliably in unseen languages, but multilingual fine-tuning improves stance detection. Minimal exposure to underrepresented languages corrects biases.

Conclusion: Limited language coverage suffices for topic-level generalization, and lightweight interventions can correct biases, offering scalable, cost-effective solutions.

Abstract: Large language models (LLMs) are transforming social-science research by
enabling scalable, precise analysis. Their adaptability raises the question of
whether knowledge acquired through fine-tuning in a few languages can transfer
to unseen languages that only appeared during pre-training. To examine this, we
fine-tune lightweight LLaMA 3.2-3B models on monolingual, bilingual, or
multilingual data sets to classify immigration-related tweets from X/Twitter
across 13 languages, a domain characterised by polarised, culturally specific
discourse. We evaluate whether minimal language-specific fine-tuning enables
cross-lingual topic detection and whether adding targeted languages corrects
pre-training biases. Results show that LLMs fine-tuned in one or two languages
can reliably classify immigration-related content in unseen languages. However,
identifying whether a tweet expresses a pro- or anti-immigration stance
benefits from multilingual fine-tuning. Pre-training bias favours dominant
languages, but even minimal exposure to under-represented languages during
fine-tuning (as little as $9.62\times10^{-11}$ of the original pre-training
token volume) yields significant gains. These findings challenge the assumption
that cross-lingual mastery requires extensive multilingual training: limited
language coverage suffices for topic-level generalisation, and structural
biases can be corrected with lightweight interventions. By releasing
4-bit-quantised, LoRA fine-tuned models, we provide an open-source,
reproducible alternative to proprietary LLMs that delivers 35 times faster
inference at just 0.00000989% of the dollar cost of the OpenAI GPT-4o model,
enabling scalable, inclusive research.

</details>


### [166] [Echoes of Automation: The Increasing Use of LLMs in Newsmaking](https://arxiv.org/abs/2508.06445)
*Abolfazl Ansari,Delvin Ce Zhang,Nafis Irtiza Tripto,Dongwon Lee*

Main category: cs.CL

TL;DR: The study analyzes AI-generated content in news articles, finding increased GenAI use, especially in local and college media, with impacts on writing style and formality.


<details>
  <summary>Details</summary>
Motivation: To address concerns about journalistic integrity and authorship due to the rise of Generative AI (GenAI) and LLMs.

Method: Analyzed over 40,000 news articles using three AI-text detectors (Binoculars, Fast-Detect GPT, GPTZero) and conducted sentence-level and linguistic analysis.

Result: Substantial increase in GenAI use, particularly in introductions; GenAI enhances word richness and readability but reduces formality, creating uniform styles.

Conclusion: GenAI is increasingly used in news writing, altering linguistic characteristics and raising concerns about journalistic authenticity.

Abstract: The rapid rise of Generative AI (GenAI), particularly LLMs, poses concerns
for journalistic integrity and authorship. This study examines AI-generated
content across over 40,000 news articles from major, local, and college news
media, in various media formats. Using three advanced AI-text detectors (e.g.,
Binoculars, Fast-Detect GPT, and GPTZero), we find substantial increase of
GenAI use in recent years, especially in local and college news. Sentence-level
analysis reveals LLMs are often used in the introduction of news, while
conclusions usually written manually. Linguistic analysis shows GenAI boosts
word richness and readability but lowers formality, leading to more uniform
writing styles, particularly in local media.

</details>


### [167] [SlimInfer: Accelerating Long-Context LLM Inference via Dynamic Token Pruning](https://arxiv.org/abs/2508.06447)
*Lingkun Long,Rubing Yang,Yushi Huang,Desheng Hui,Ao Zhou,Jianlei Yang*

Main category: cs.CL

TL;DR: SlimInfer accelerates LLM inference by pruning less critical prompt tokens dynamically, achieving significant speedups without performance loss.


<details>
  <summary>Details</summary>
Motivation: High computational demands limit long-context inference for LLMs; existing methods still process full hidden states, reducing efficiency.

Method: SlimInfer dynamically prunes redundant tokens in hidden states using a fine-grained mechanism, leveraging an information diffusion phenomenon.

Result: Achieves up to 2.53√ó TTFT speedup and 1.88√ó latency reduction for LLaMA3.1-8B-Instruct on an RTX 4090.

Conclusion: SlimInfer efficiently reduces computational and memory costs while maintaining model performance, offering a practical solution for LLM inference.

Abstract: Long-context inference for Large Language Models (LLMs) is heavily limited by
high computational demands. While several existing methods optimize attention
computation, they still process the full set of hidden states at each layer,
limiting overall efficiency. In this work, we propose SlimInfer, an innovative
framework that aims to accelerate inference by directly pruning less critical
prompt tokens during the forward pass. Our key insight is an information
diffusion phenomenon: As information from critical tokens propagates through
layers, it becomes distributed across the entire sequence. This diffusion
process suggests that LLMs can maintain their semantic integrity when excessive
tokens, even including these critical ones, are pruned in hidden states.
Motivated by this, SlimInfer introduces a dynamic fine-grained pruning
mechanism that accurately removes redundant tokens of hidden state at
intermediate layers. This layer-wise pruning naturally enables an asynchronous
KV cache manager that prefetches required token blocks without complex
predictors, reducing both memory usage and I/O costs. Extensive experiments
show that SlimInfer can achieve up to $\mathbf{2.53\times}$ time-to-first-token
(TTFT) speedup and $\mathbf{1.88\times}$ end-to-end latency reduction for
LLaMA3.1-8B-Instruct on a single RTX 4090, without sacrificing performance on
LongBench. Our code will be released upon acceptance.

</details>


### [168] [GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models](https://arxiv.org/abs/2508.06471)
*GLM-4. 5 Team,:,Aohan Zeng,Xin Lv,Qinkai Zheng,Zhenyu Hou,Bin Chen,Chengxing Xie,Cunxiang Wang,Da Yin,Hao Zeng,Jiajie Zhang,Kedong Wang,Lucen Zhong,Mingdao Liu,Rui Lu,Shulin Cao,Xiaohan Zhang,Xuancheng Huang,Yao Wei,Yean Cheng,Yifan An,Yilin Niu,Yuanhao Wen,Yushi Bai,Zhengxiao Du,Zihan Wang,Zilin Zhu,Bohan Zhang,Bosi Wen,Bowen Wu,Bowen Xu,Can Huang,Casey Zhao,Changpeng Cai,Chao Yu,Chen Li,Chendi Ge,Chenghua Huang,Chenhui Zhang,Chenxi Xu,Chenzheng Zhu,Chuang Li,Congfeng Yin,Daoyan Lin,Dayong Yang,Dazhi Jiang,Ding Ai,Erle Zhu,Fei Wang,Gengzheng Pan,Guo Wang,Hailong Sun,Haitao Li,Haiyang Li,Haiyi Hu,Hanyu Zhang,Hao Peng,Hao Tai,Haoke Zhang,Haoran Wang,Haoyu Yang,He Liu,He Zhao,Hongwei Liu,Hongxi Yan,Huan Liu,Huilong Chen,Ji Li,Jiajing Zhao,Jiamin Ren,Jian Jiao,Jiani Zhao,Jianyang Yan,Jiaqi Wang,Jiayi Gui,Jiayue Zhao,Jie Liu,Jijie Li,Jing Li,Jing Lu,Jingsen Wang,Jingwei Yuan,Jingxuan Li,Jingzhao Du,Jinhua Du,Jinxin Liu,Junkai Zhi,Junli Gao,Ke Wang,Lekang Yang,Liang Xu,Lin Fan,Lindong Wu,Lintao Ding,Lu Wang,Man Zhang,Minghao Li,Minghuan Xu,Mingming Zhao,Mingshu Zhai,Pengfan Du,Qian Dong,Shangde Lei,Shangqing Tu,Shangtong Yang,Shaoyou Lu,Shijie Li,Shuang Li,Shuang-Li,Shuxun Yang,Sibo Yi,Tianshu Yu,Wei Tian,Weihan Wang,Wenbo Yu,Weng Lam Tam,Wenjie Liang,Wentao Liu,Xiao Wang,Xiaohan Jia,Xiaotao Gu,Xiaoying Ling,Xin Wang,Xing Fan,Xingru Pan,Xinyuan Zhang,Xinze Zhang,Xiuqing Fu,Xunkai Zhang,Yabo Xu,Yandong Wu,Yida Lu,Yidong Wang,Yilin Zhou,Yiming Pan,Ying Zhang,Yingli Wang,Yingru Li,Yinpei Su,Yipeng Geng,Yitong Zhu,Yongkun Yang,Yuhang Li,Yuhao Wu,Yujiang Li,Yunan Liu,Yunqing Wang,Yuntao Li,Yuxuan Zhang,Zezhen Liu,Zhen Yang,Zhengda Zhou,Zhongpei Qiao,Zhuoer Feng,Zhuorui Liu,Zichen Zhang,Zihan Wang,Zijun Yao,Zikang Wang,Ziqiang Liu,Ziwei Chai,Zixuan Li,Zuodong Zhao,Wenguang Chen,Jidong Zhai,Bin Xu,Minlie Huang,Hongning Wang,Juanzi Li,Yuxiao Dong,Jie Tang*

Main category: cs.CL

TL;DR: GLM-4.5 is an open-source MoE model with 355B parameters, excelling in reasoning and agentic tasks, ranking 3rd overall and 2nd in agentic benchmarks.


<details>
  <summary>Details</summary>
Motivation: To advance research in reasoning and agentic AI systems by developing a high-performance, efficient model.

Method: Uses a hybrid reasoning method, multi-stage training on 23T tokens, and post-training with expert iteration and reinforcement learning.

Result: Achieves 70.1% on TAU-Bench, 91.0% on AIME 24, and 64.2% on SWE-bench Verified.

Conclusion: GLM-4.5 and its compact version, GLM-4.5-Air, are released to foster further research in AI reasoning and agentic systems.

Abstract: We present GLM-4.5, an open-source Mixture-of-Experts (MoE) large language
model with 355B total parameters and 32B activated parameters, featuring a
hybrid reasoning method that supports both thinking and direct response modes.
Through multi-stage training on 23T tokens and comprehensive post-training with
expert model iteration and reinforcement learning, GLM-4.5 achieves strong
performance across agentic, reasoning, and coding (ARC) tasks, scoring 70.1% on
TAU-Bench, 91.0% on AIME 24, and 64.2% on SWE-bench Verified. With much fewer
parameters than several competitors, GLM-4.5 ranks 3rd overall among all
evaluated models and 2nd on agentic benchmarks. We release both GLM-4.5 (355B
parameters) and a compact version, GLM-4.5-Air (106B parameters), to advance
research in reasoning and agentic AI systems. Code, models, and more
information are available at https://github.com/zai-org/GLM-4.5.

</details>


### [169] [HapticLLaMA: A Multimodal Sensory Language Model for Haptic Captioning](https://arxiv.org/abs/2508.06475)
*Guimin Hu,Daniel Hershcovich,Hasti Seifi*

Main category: cs.CL

TL;DR: HapticLLaMA, a multimodal model, interprets haptic signals into natural language descriptions, achieving strong performance in haptic captioning.


<details>
  <summary>Details</summary>
Motivation: To explore underexplored haptic signals in multimodal research for applications like VR and accessibility.

Method: Proposes HapticLLaMA with two haptic tokenizers, trained via supervised fine-tuning and RLHF.

Result: Achieves METEOR 59.98, BLEU-4 32.06, and 61% human ratings >3.5/7, with RLHF improving alignment.

Conclusion: Demonstrates LLMs' potential to adapt to sensory data, advancing haptic captioning.

Abstract: Haptic captioning is the task of generating natural language descriptions
from haptic signals, such as vibrations, for use in virtual reality,
accessibility, and rehabilitation applications. While previous multimodal
research has focused primarily on vision and audio, haptic signals for the
sense of touch remain underexplored. To address this gap, we formalize the
haptic captioning task and propose HapticLLaMA, a multimodal sensory language
model that interprets vibration signals into descriptions in a given sensory,
emotional, or associative category. We investigate two types of haptic
tokenizers, a frequency-based tokenizer and an EnCodec-based tokenizer, that
convert haptic signals into sequences of discrete units, enabling their
integration with the LLaMA model. HapticLLaMA is trained in two stages: (1)
supervised fine-tuning using the LLaMA architecture with LoRA-based adaptation,
and (2) fine-tuning via reinforcement learning from human feedback (RLHF). We
assess HapticLLaMA's captioning performance using both automated n-gram metrics
and human evaluation. HapticLLaMA demonstrates strong capability in
interpreting haptic vibration signals, achieving a METEOR score of 59.98 and a
BLEU-4 score of 32.06 respectively. Additionally, over 61% of the generated
captions received human ratings above 3.5 on a 7-point scale, with RLHF
yielding a 10% improvement in the overall rating distribution, indicating
stronger alignment with human haptic perception. These findings highlight the
potential of large language models to process and adapt to sensory data.

</details>


### [170] [Post-training for Efficient Communication via Convention Formation](https://arxiv.org/abs/2508.06482)
*Yilun Hua,Evan Wang,Yoav Artzi*

Main category: cs.CL

TL;DR: Post-training fine-tuning improves LLMs' ability to form ad-hoc conventions in multi-turn interactions, as shown by new benchmarks.


<details>
  <summary>Details</summary>
Motivation: To address the lack of natural convention-forming behavior in LLMs during multi-turn interactions, unlike humans.

Method: Targeted fine-tuning on heuristically identified demonstrations of convention formation, evaluated via two new benchmarks.

Result: Post-trained LLMs show significantly improved convention formation abilities in both benchmarks.

Conclusion: The post-training process effectively enhances LLMs' ability to form conventions, bridging a gap with human communication.

Abstract: Humans communicate with increasing efficiency in multi-turn interactions, by
adapting their language and forming ad-hoc conventions. In contrast, prior work
shows that LLMs do not naturally show this behavior. We develop a post-training
process to develop this ability through targeted fine-tuning on heuristically
identified demonstrations of convention formation. We evaluate with two new
benchmarks focused on this capability. First, we design a focused,
cognitively-motivated interaction benchmark that consistently elicits strong
convention formation trends in humans. Second, we create a new
document-grounded reference completion task that reflects in-the-wild
convention formation behavior. Our studies show significantly improved
convention formation abilities in post-trained LLMs across the two evaluation
methods.

</details>


### [171] [Indian Legal NLP Benchmarks : A Survey](https://arxiv.org/abs/2107.06056)
*Prathamesh Kalamkar,Janani Venugopalan Ph. D.,Vivek Raghavan Ph. D*

Main category: cs.CL

TL;DR: The paper highlights the need for specialized NLP benchmarks for Indian legal text to advance AI in the legal field.


<details>
  <summary>Details</summary>
Motivation: Legal text differs significantly from standard English, requiring tailored NLP benchmarks to address legal-specific tasks and spur innovation.

Method: The authors review existing work and propose ideas for creating new benchmarks for Indian legal NLP.

Result: The proposed benchmarks aim to benefit both the AI community and the legal fraternity by addressing unique challenges in legal text.

Conclusion: Specialized benchmarks for Indian legal NLP are essential to drive progress and innovation in the field.

Abstract: Availability of challenging benchmarks is the key to advancement of AI in a
specific field.Since Legal Text is significantly different than normal English
text, there is a need to create separate Natural Language Processing benchmarks
for Indian Legal Text which are challenging and focus on tasks specific to
Legal Systems. This will spur innovation in applications of Natural language
Processing for Indian Legal Text and will benefit AI community and Legal
fraternity. We review the existing work in this area and propose ideas to
create new benchmarks for Indian Legal Natural Language Processing.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [172] [Diagrams-to-Dynamics (D2D): Exploring Causal Loop Diagram Leverage Points under Uncertainty](https://arxiv.org/abs/2508.05659)
*Jeroen F. Uleman,Loes Crielaard,Leonie K. Elsenburg,Guido A. Veldhuis,Karien Stronks,Naja Hulvej Rod,Rick Quax,V√≠tor V. Vasconcelos*

Main category: cs.LG

TL;DR: D2D converts causal loop diagrams (CLDs) into system dynamics models (SDMs) for dynamic analysis, outperforming network centrality analysis in consistency with data-driven models.


<details>
  <summary>Details</summary>
Motivation: CLDs are qualitative and static, limiting dynamic analysis and intervention strategies. Quantitative CLD methods like network centrality often lead to false inferences.

Method: D2D transforms CLDs into SDMs using structural information (link existence and polarity) with minimal user input (labeling variables as stocks, flows/auxiliaries, or constants).

Result: D2D distinguishes high- and low-ranked leverage points, shows consistency with data-driven models, and provides uncertainty estimates.

Conclusion: D2D is a promising tool for dynamic modeling, implemented in open-source Python and web apps, with potential for broader validation.

Abstract: Causal loop diagrams (CLDs) are widely used in health and environmental
research to represent hypothesized causal structures underlying complex
problems. However, as qualitative and static representations, CLDs are limited
in their ability to support dynamic analysis and inform intervention
strategies. Additionally, quantitative CLD analysis methods like network
centrality analysis often lead to false inference. We propose
Diagrams-to-Dynamics (D2D), a method for converting CLDs into exploratory
system dynamics models (SDMs) in the absence of empirical data. With minimal
user input - following a protocol to label variables as stocks,
flows/auxiliaries, or constants - D2D leverages the structural information
already encoded in CLDs, namely, link existence and polarity, to simulate
hypothetical interventions and explore potential leverage points under
uncertainty. Results suggest that D2D helps distinguish between high- and
low-ranked leverage points. We compare D2D to a data-driven SDM constructed
from the same CLD and variable labeling. D2D showed greater consistency with
the data-driven model than network centrality analysis, while providing
uncertainty estimates and guidance for future data collection. The method is
implemented in an open-source Python package and a web-based application to
support further testing and lower the barrier to dynamic modeling for
researchers working with CLDs. We expect additional validation will further
establish the approach's utility across a broad range of cases and domains.

</details>


### [173] [A Graph Neural Network Approach for Mapping the Conceptual Structure and Inter-Branch Connectivity of Physics](https://arxiv.org/abs/2508.05724)
*Massimiliano Romiti*

Main category: cs.LG

TL;DR: A novel framework represents physical laws as a weighted knowledge graph, achieving high accuracy in link prediction and revealing key insights into physics structure.


<details>
  <summary>Details</summary>
Motivation: To systematically analyze and represent physical laws using a knowledge graph, resolving ambiguities and uncovering cross-domain relationships.

Method: Constructed a database of physics equations, developed a weighted graph representation, and trained a Graph Attention Network (GAT) for link prediction.

Result: Achieved a test AUC of 0.9742, outperforming baselines, and identified key findings like conceptual axes and hub equations.

Conclusion: The framework successfully models physics structure, suggesting novel analogies and enabling targeted analysis of subfields.

Abstract: This work introduces a novel framework for representing and analyzing
physical laws as a weighted knowledge graph. We constructed a database of 659
distinct physical equations, subjected to rigorous semantic cleaning to resolve
notational ambiguities, resulting in a corpus of 400 advanced physics
equations. We developed an enhanced graph representation where both physical
concepts and equations are nodes, connected by weighted inter-equation bridges.
These weights are objectively defined using normalized metrics for variable
overlap, physics-informed importance scores, and bibliometric data. A Graph
Attention Network (GAT) was trained for link prediction, achieving a test AUC
of 0.9742 +/- 0.0018 across five independent runs, significantly outperforming
both classical heuristics (best baseline AUC: 0.9487) and established GNN
architectures like GraphSAGE (AUC: 0.9504, p = 0.029). Statistical testing
confirmed significance of all comparisons (p < 0.05), with 2.7% improvement
over the best baseline. Our analysis reveals three key findings: (i) The model
autonomously rediscovers the known macroscopic structure of physics,
identifying strong conceptual axes between Electromagnetism and Statistical
Mechanics. (ii) It identifies central hub equations that serve as critical
bridges between multiple physical domains. (iii) The model generates stable,
computationally-derived hypotheses for cross-domain relationships, identifying
both known principles and suggesting novel mathematical analogies for further
theoretical investigation. The framework can generate hundreds of such
hypotheses, enabling the creation of specialized datasets for targeted analysis
of specific physics subfields. Code and data available at
https://github.com/kingelanci/graphysics

</details>


### [174] [Machine Learning-Based Nonlinear Nudging for Chaotic Dynamical Systems](https://arxiv.org/abs/2508.05778)
*Jaemin Oh,Jinsil Lee,Youngjoon Hong*

Main category: cs.LG

TL;DR: Neural network nudging is proposed for nonlinear state space models, with theoretical backing and successful evaluation on chaotic systems.


<details>
  <summary>Details</summary>
Motivation: Designing effective nudging terms for nonlinear systems is challenging, motivating a data-driven approach.

Method: Proposes neural network nudging, leveraging Kazantzis--Kravaris--Luenberger observer theory, and tests on chaotic benchmarks.

Result: Effective nudging terms are learned, demonstrated on Lorenz 96, Kuramoto--Sivashinsky, and Kolmogorov flow.

Conclusion: Neural network nudging is a viable method for nonlinear systems, supported by theory and empirical results.

Abstract: Nudging is an empirical data assimilation technique that incorporates an
observation-driven control term into the model dynamics. The trajectory of the
nudged system approaches the true system trajectory over time, even when the
initial conditions differ. For linear state space models, such control terms
can be derived under mild assumptions. However, designing effective nudging
terms becomes significantly more challenging in the nonlinear setting. In this
work, we propose neural network nudging, a data-driven method for learning
nudging terms in nonlinear state space models. We establish a theoretical
existence result based on the Kazantzis--Kravaris--Luenberger observer theory.
The proposed approach is evaluated on three benchmark problems that exhibit
chaotic behavior: the Lorenz 96 model, the Kuramoto--Sivashinsky equation, and
the Kolmogorov flow.

</details>


### [175] [From Imperfect Signals to Trustworthy Structure: Confidence-Aware Inference from Heterogeneous and Reliability-Varying Utility Data](https://arxiv.org/abs/2508.05791)
*Haoran Li,Lihao Mai,Muhao Guo,Jiaqi Wu,Yang Weng,Yannan Sun,Ce Jimmy Liu*

Main category: cs.LG

TL;DR: A scalable framework integrates heterogeneous data to reconstruct accurate distribution grid topology, combining spatial and dynamic data dimensions with confidence-aware inference and physical constraints.


<details>
  <summary>Details</summary>
Motivation: Accurate grid topology is crucial for reliable operations, but utility data varies in quality and source. A robust method is needed to integrate and trust such data.

Method: The framework jointly leverages spatial (GIS, asset metadata) and dynamic (voltage time series) data, using confidence-aware inference and embedding operational constraints (e.g., transformer limits, radial topology).

Result: Validated on 8000+ meters across 3 feeders, achieving over 95% accuracy in topology reconstruction, with improved confidence calibration and computational efficiency.

Conclusion: The framework ensures uncertainty-aware, structurally valid topology reconstruction, enabling trustworthy and actionable results under real-world conditions.

Abstract: Accurate distribution grid topology is essential for reliable modern grid
operations. However, real-world utility data originates from multiple sources
with varying characteristics and levels of quality. In this work, developed in
collaboration with Oncor Electric Delivery, we propose a scalable framework
that reconstructs a trustworthy grid topology by systematically integrating
heterogeneous data. We observe that distribution topology is fundamentally
governed by two complementary dimensions: the spatial layout of physical
infrastructure (e.g., GIS and asset metadata) and the dynamic behavior of the
system in the signal domain (e.g., voltage time series). When jointly
leveraged, these dimensions support a complete and physically coherent
reconstruction of network connectivity. To address the challenge of uneven data
quality without compromising observability, we introduce a confidence-aware
inference mechanism that preserves structurally informative yet imperfect
inputs, while quantifying the reliability of each inferred connection for
operator interpretation. This soft handling of uncertainty is tightly coupled
with hard enforcement of physical feasibility: we embed operational
constraints, such as transformer capacity limits and radial topology
requirements, directly into the learning process. Together, these components
ensure that inference is both uncertainty-aware and structurally valid,
enabling rapid convergence to actionable, trustworthy topologies under
real-world deployment conditions. The proposed framework is validated using
data from over 8000 meters across 3 feeders in Oncor's service territory,
demonstrating over 95% accuracy in topology reconstruction and substantial
improvements in confidence calibration and computational efficiency relative to
baseline methods.

</details>


### [176] [Optimal Linear Baseline Models for Scientific Machine Learning](https://arxiv.org/abs/2508.05831)
*Alexander DeLise,Kyle Loh,Krish Patel,Meredith Teague,Andrea Arnold,Matthias Chung*

Main category: cs.LG

TL;DR: The paper presents a theoretical framework for analyzing linear encoder-decoder architectures in scientific machine learning, focusing on interpretability and Bayes risk minimization.


<details>
  <summary>Details</summary>
Motivation: To address the opacity of nonlinear neural networks and provide interpretable solutions for scientific machine learning problems.

Method: Develops a unified framework using Bayes risk minimization, deriving closed-form, rank-constrained linear and affine linear optimal mappings for forward and inverse tasks.

Result: Generalizes existing formulations to handle rank deficiencies and validates results with numerical experiments in biomedical imaging, finance, and fluid dynamics.

Conclusion: The framework offers a robust baseline for understanding and benchmarking neural network models in scientific machine learning.

Abstract: Across scientific domains, a fundamental challenge is to characterize and
compute the mappings from underlying physical processes to observed signals and
measurements. While nonlinear neural networks have achieved considerable
success, they remain theoretically opaque, which hinders adoption in contexts
where interpretability is paramount. In contrast, linear neural networks serve
as a simple yet effective foundation for gaining insight into these complex
relationships. In this work, we develop a unified theoretical framework for
analyzing linear encoder-decoder architectures through the lens of Bayes risk
minimization for solving data-driven scientific machine learning problems. We
derive closed-form, rank-constrained linear and affine linear optimal mappings
for forward modeling and inverse recovery tasks. Our results generalize
existing formulations by accommodating rank-deficiencies in data, forward
operators, and measurement processes. We validate our theoretical results by
conducting numerical experiments on datasets from simple biomedical imaging,
financial factor analysis, and simulations involving nonlinear fluid dynamics
via the shallow water equations. This work provides a robust baseline for
understanding and benchmarking learned neural network models for scientific
machine learning problems.

</details>


### [177] [An Effective Approach for Node Classification in Textual Graphs](https://arxiv.org/abs/2508.05836)
*Rituparna Datta,Nibir Chandra Mandal*

Main category: cs.LG

TL;DR: A novel framework combining TAPE and Graphormer improves node classification in TAGs by integrating semantic and structural features, achieving state-of-the-art results on ogbn-arxiv.


<details>
  <summary>Details</summary>
Motivation: Challenges in integrating text semantics with graph structure for node classification in complex networks like citation networks.

Method: Uses TAPE with ChatGPT for semantic explanations, fused with structural features via Graphormer's attention mechanisms.

Result: Achieves 0.772 accuracy, outperforming GCN baselines (0.713), with strong precision, recall, and F1-scores.

Conclusion: The framework offers a scalable, robust solution for dynamic TAGs, advancing research in knowledge systems.

Abstract: Textual Attribute Graphs (TAGs) are critical for modeling complex networks
like citation networks, but effective node classification remains challenging
due to difficulties in integrating rich semantics from text with structural
graph information. Existing methods often struggle with capturing nuanced
domain-specific terminology, modeling long-range dependencies, adapting to
temporal evolution, and scaling to massive datasets. To address these issues,
we propose a novel framework that integrates TAPE (Text-Attributed Graph
Representation Enhancement) with Graphormer. Our approach leverages a large
language model (LLM), specifically ChatGPT, within the TAPE framework to
generate semantically rich explanations from paper content, which are then
fused into enhanced node representations. These embeddings are combined with
structural features using a novel integration layer with learned attention
weights. Graphormer's path-aware position encoding and multi-head attention
mechanisms are employed to effectively capture long-range dependencies across
the citation network. We demonstrate the efficacy of our framework on the
challenging ogbn-arxiv dataset, achieving state-of-the-art performance with a
classification accuracy of 0.772, significantly surpassing the best GCN
baseline of 0.713. Our method also yields strong results in precision (0.671),
recall (0.577), and F1-score (0.610). We validate our approach through
comprehensive ablation studies that quantify the contribution of each
component, demonstrating the synergy between semantic and structural
information. Our framework provides a scalable and robust solution for node
classification in dynamic TAGs, offering a promising direction for future
research in knowledge systems and scientific discovery.

</details>


### [178] [A Markov Decision Process Framework for Early Maneuver Decisions in Satellite Collision Avoidance](https://arxiv.org/abs/2508.05876)
*Francesca Ferrara,Lander W. Schillinger Arana,Florian D√∂rfler,Sarah H. Q. Li*

Main category: cs.LG

TL;DR: A reinforcement learning policy gradient algorithm is used to train an autonomous guidance policy for collision avoidance maneuvers, optimizing fuel consumption and collision risk.


<details>
  <summary>Details</summary>
Motivation: To improve collision avoidance maneuver (CAM) decision-making by minimizing fuel consumption while maintaining acceptable collision risks.

Method: Model CAM as a continuous state, discrete action MDP with analytical models for risk, fuel, and orbit geometry. Train policy using RL-PG on historical data.

Result: Trained policy reduces fuel consumption per CAM and maintains collision risk guarantees, outperforming conventional methods.

Conclusion: The RL-PG approach effectively balances fuel efficiency and collision risk, offering a viable alternative to traditional CAM policies.

Abstract: This work presents a Markov decision process (MDP) framework to model
decision-making for collision avoidance maneuver (CAM) and a reinforcement
learning policy gradient (RL-PG) algorithm to train an autonomous guidance
policy using historic CAM data. In addition to maintaining acceptable collision
risks, this approach seeks to minimize the average fuel consumption of CAMs by
making early maneuver decisions. We model CAM as a continuous state, discrete
action and finite horizon MDP, where the critical decision is determining when
to initiate the maneuver. The MDP model also incorporates analytical models for
conjunction risk, propellant consumption, and transit orbit geometry. The
Markov policy effectively trades-off maneuver delay-which improves the
reliability of conjunction risk indicators-with propellant consumption-which
increases with decreasing maneuver time. Using historical data of tracked
conjunction events, we verify this framework and conduct an extensive ablation
study on the hyper-parameters used within the MDP. On synthetic conjunction
events, the trained policy significantly minimizes both the overall and average
propellant consumption per CAM when compared to a conventional cut-off policy
that initiates maneuvers 24 hours before the time of closest approach (TCA). On
historical conjunction events, the trained policy consumes more propellant
overall but reduces the average propellant consumption per CAM. For both
historical and synthetic conjunction events, the trained policy achieves equal
if not higher overall collision risk guarantees.

</details>


### [179] [The Fourth State: Signed-Zero Ternary for Stable LLM Quantization (and More)](https://arxiv.org/abs/2508.05905)
*Jeffrey Uhlmann*

Main category: cs.LG

TL;DR: SZT, a 2-bit quantization method, improves information density under fixed resource budgets without forward-path penalties.


<details>
  <summary>Details</summary>
Motivation: To challenge the view of quantization as a suboptimal trade-off and explore its potential for higher efficiency under fixed resource constraints.

Method: Introduces Signed-Zero Ternary (SZT), a deterministic 2-bit quantization method that preserves gradient information.

Result: SZT may enhance information density compared to non-quantized alternatives.

Conclusion: SZT offers a promising approach to efficient quantization without compromising performance.

Abstract: Quantization is usually regarded as a means to trade quality of performance
for reduced compute requirements, i.e., as a suboptimal approximation. However,
if examined in terms of a fixed overall resource budget, a very different
perspective arises. We introduce Signed-Zero Ternary (SZT), a 2-bit
quantization that deterministically provides gradient information with no
forward-path penalty. Our analysis provides evidence that it may improve
information density compared to non-quantized alternatives.

</details>


### [180] [Dual Signal Decomposition of Stochastic Time Series](https://arxiv.org/abs/2508.05915)
*Alex Glushkovsky*

Main category: cs.LG

TL;DR: The paper proposes a machine learning method to decompose stochastic time series into mean, dispersion, and noise, using dual signal fitting and regularization.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of decomposing stochastic time series into meaningful components (mean, dispersion, noise) while preserving patterns and relationships.

Method: Applies machine learning to fit a dual signal (mean and dispersion) with a loss function balancing fit and regularization. Uses sequential or joint learning approaches and neural networks for optimization.

Result: The decomposition serves as a smoothing/denoising algorithm, isolating noise and revealing complex relationships in heteroskedastic time series.

Conclusion: The method enables analysis, forecasting, and cross-effect studies of time series by decomposing them into interpretable components.

Abstract: The research paper addresses decomposition of a stochastic time series into
three time series representing a dual signal i.e., the mean and the dispersion,
with noise isolated. Decomposition is done by applying machine learning to fit
a dual signal. Machine learning minimizes the loss function which compromises
between fitting the original time series and penalizing irregularities of the
dual signal. The latter includes terms based on the first and second order
derivatives along time. To preserve special patterns, weighting of the
regularization components of the loss function has been introduced based on
Statistical Process Control methodology. The proposed decomposition can be
applied as a smoothing algorithm against the mean and dispersion of the time
series. By isolating noise, the proposed decomposition can be seen as a
denoising algorithm. Two approaches of the learning process have been
considered: sequential and jointly. The former approach learns the mean signal
first and then dispersion. The latter approach fits the dual signal jointly.
Jointly learning can uncover complex relationships for the time series with
heteroskedasticity. Learning has been set by solving the direct non-linear
unconstrained optimization problem or by applying neural networks that have
sequential or twin output architectures. Tuning of the loss function
hyperparameters focuses on the isolated noise to be a stationary stochastic
process without autocorrelation properties. Depending on the applications, the
hyperparameters of the learning can be tuned towards either the discrete states
by stepped signal or smoothed series. The decomposed dual signal can be
represented on the 2D space and used to learn inherent structures, to forecast
both mean and dispersion, or to analyze cross effects in case of multiple time
series.

</details>


### [181] [Fast, Convex and Conditioned Network for Multi-Fidelity Vectors and Stiff Univariate Differential Equations](https://arxiv.org/abs/2508.05921)
*Siddharth Rout*

Main category: cs.LG

TL;DR: The paper addresses poor optimization in neural PDE solvers due to ill-conditioning, proposing Shifted Gaussian Encoding to improve matrix rank and convergence.


<details>
  <summary>Details</summary>
Motivation: Neural PDE solvers often fail due to ill-conditioning, not expressivity limits, especially in multi-fidelity and stiff problems.

Method: Introduces Shifted Gaussian Encoding, an activation filtering step to enhance matrix rank and expressivity while maintaining convexity.

Result: Extends solvable Peclet numbers by two orders, reduces error by six orders, and outperforms deep networks in speed and accuracy.

Conclusion: Conditioning, not depth, is the bottleneck; simple architectural changes can unlock significant improvements in neural solvers.

Abstract: Accuracy in neural PDE solvers often breaks down not because of limited
expressivity, but due to poor optimisation caused by ill-conditioning,
especially in multi-fidelity and stiff problems. We study this issue in
Physics-Informed Extreme Learning Machines (PIELMs), a convex variant of neural
PDE solvers, and show that asymptotic components in governing equations can
produce highly ill-conditioned activation matrices, severely limiting
convergence. We introduce Shifted Gaussian Encoding, a simple yet effective
activation filtering step that increases matrix rank and expressivity while
preserving convexity. Our method extends the solvable range of Peclet numbers
in steady advection-diffusion equations by over two orders of magnitude,
achieves up to six orders lower error on multi-frequency function learning, and
fits high-fidelity image vectors more accurately and faster than deep networks
with over a million parameters. This work highlights that conditioning, not
depth, is often the bottleneck in scientific neural solvers and that simple
architectural changes can unlock substantial gains.

</details>


### [182] [Mitigating Think-Answer Mismatch in LLM Reasoning Through Noise-Aware Advantage Reweighting](https://arxiv.org/abs/2508.05928)
*Si Shen,Peijun Shen,Wenhua Zhao,Danhao Zhu*

Main category: cs.LG

TL;DR: S-GRPO addresses the Think-Answer Mismatch in GRPO by introducing noise-aware advantage weights, improving robustness and performance in large reasoning models.


<details>
  <summary>Details</summary>
Motivation: GRPO's vulnerability to noisy rewards, especially in unbalanced response groups, degrades learning. S-GRPO aims to stabilize this process.

Method: S-GRPO derives optimal, noise-aware advantage weights to counteract reward noise and stabilize training.

Result: S-GRPO outperforms GRPO by +2.5%, +2.2%, and +2.4% on Qwen-Math-7B-Base, Llama-3.2-3B-Base, and Qwen-Math-1.5B-Instruct, respectively, and remains stable under 20% synthetic noise.

Conclusion: S-GRPO enhances robustness and effectiveness in training large reasoning models, proving superior to GRPO in noisy environments.

Abstract: Group-Relative Policy Optimization (GRPO) is a key technique for training
large reasoning models, yet it suffers from a critical vulnerability: the
\emph{Think-Answer Mismatch}, where noisy reward signals corrupt the learning
process. This problem is most severe in unbalanced response groups,
paradoxically degrading the signal precisely when it should be most
informative. To address this challenge, we propose Stable Group-Relative Policy
Optimization (S-GRPO), a principled enhancement that derives optimal,
noise-aware advantage weights to stabilize training. Our comprehensive
experiments on mathematical reasoning benchmarks demonstrate S-GRPO's
effectiveness and robustness. On various models, S-GRPO significantly
outperforms DR. GRPO, achieving performance gains of +2.5% on
Qwen-Math-7B-Base, +2.2% on Llama-3.2-3B-Base, and +2.4% on
Qwen-Math-1.5B-Instruct. Most critically, while standard GRPO fails to learn
under 20% synthetic reward noise, S-GRPO maintains stable learning progress.
These results highlight S-GRPO's potential for more robust and effective
training of large-scale reasoning models. \footnote{Code and data are available
at: https://github.com/shenpeijun0212/S-GRPO

</details>


### [183] [Multi-Armed Bandits-Based Optimization of Decision Trees](https://arxiv.org/abs/2508.05957)
*Hasibul Karim Shanto,Umme Ayman Koana,Shadikur Rahman*

Main category: cs.LG

TL;DR: The paper proposes a Multi-Armed Bandits (MAB)-based pruning approach for decision trees to improve generalization and reduce overfitting, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional pruning techniques like CCP and REP are greedy and may compromise long-term generalization, especially with small or complex datasets.

Method: A reinforcement learning-based MAB approach dynamically prunes decision trees by treating pruning as an exploration-exploitation problem.

Result: Experiments show the MAB-based pruning achieves better predictive performance than conventional methods.

Conclusion: The MAB approach offers a dynamic and probabilistic way to optimize decision tree pruning, enhancing model robustness.

Abstract: Decision trees, without appropriate constraints, can easily become overly
complex and prone to overfit, capturing noise rather than generalizable
patterns. To resolve this problem,pruning operation is a crucial part in
optimizing decision trees, as it not only reduces the complexity of trees but
also decreases the probability of generating overfit models. The conventional
pruning techniques like Cost-Complexity Pruning (CCP) and Reduced Error Pruning
(REP) are mostly based on greedy approaches that focus on immediate gains in
performance while pruning nodes of the decision tree. However, this might
result in a lower generalization in the long run, compromising the robust
ability of the tree model when introduced to unseen data samples, particularly
when trained with small and complex datasets. To address this challenge, we are
proposing a Multi-Armed Bandits (MAB)-based pruning approach, a reinforcement
learning (RL)-based technique, that will dynamically prune the tree to generate
an optimal decision tree with better generalization. Our proposed approach
assumes the pruning process as an exploration-exploitation problem, where we
are utilizing the MAB algorithms to find optimal branch nodes to prune based on
feedback from each pruning actions. Experimental evaluation on several
benchmark datasets, demonstrated that our proposed approach results in better
predictive performance compared to the traditional ones. This suggests the
potential of utilizing MAB for a dynamic and probabilistic way of decision tree
pruning, in turn optimizing the decision tree-based model.

</details>


### [184] [Mildly Conservative Regularized Evaluation for Offline Reinforcement Learning](https://arxiv.org/abs/2508.05960)
*Haohui Chen,Zhiyong Chen*

Main category: cs.LG

TL;DR: The paper proposes a mildly conservative regularized evaluation (MCRE) framework and MCRQ algorithm to balance conservatism and performance in offline RL, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of distribution shift and overestimation in offline RL without excessive conservatism.

Method: Introduces MCRE, combining TD error and behavior cloning in Bellman backup, and MCRQ, integrating MCRE into an actor-critic framework.

Result: MCRQ outperforms strong baselines and state-of-the-art offline RL algorithms on benchmark datasets.

Conclusion: MCRE and MCRQ effectively balance conservatism and performance, improving offline RL outcomes.

Abstract: Offline reinforcement learning (RL) seeks to learn optimal policies from
static datasets without further environment interaction. A key challenge is the
distribution shift between the learned and behavior policies, leading to
out-of-distribution (OOD) actions and overestimation. To prevent gross
overestimation, the value function must remain conservative; however, excessive
conservatism may hinder performance improvement. To address this, we propose
the mildly conservative regularized evaluation (MCRE) framework, which balances
conservatism and performance by combining temporal difference (TD) error with a
behavior cloning term in the Bellman backup. Building on this, we develop the
mildly conservative regularized Q-learning (MCRQ) algorithm, which integrates
MCRE into an off-policy actor-critic framework. Experiments show that MCRQ
outperforms strong baselines and state-of-the-art offline RL algorithms on
benchmark datasets.

</details>


### [185] [LinguaFluid: Language Guided Fluid Control via Semantic Rewards in Reinforcement Learning](https://arxiv.org/abs/2508.05977)
*Aoming Liang,Chi Cheng,Dashuai Chen,Boai Sun,Dixia Fan*

Main category: cs.LG

TL;DR: The paper introduces a semantically aligned RL method using SBERT to compute rewards based on textual goal descriptions, eliminating the need for manual reward engineering.


<details>
  <summary>Details</summary>
Motivation: Designing effective reward functions in RL is challenging, especially for tasks with non-numeric goals. Current methods rely on heuristics or manual tuning.

Method: Rewards are computed by aligning the current state with a target semantic instruction using SBERT, measuring cosine similarity between goal and episode descriptions.

Result: The method achieves competitive control behavior without hand-crafted rewards and shows a correlation between language embedding and Euclidean spaces.

Conclusion: The framework enables alignment of agent behavior with natural language goals and supports integration with larger language models for control applications.

Abstract: In the domain of scientific machine learning, designing effective reward
functions remains a challenge in reinforcement learning (RL), particularly in
environments where task goals are difficult to specify numerically. Reward
functions in existing work are predominantly based on heuristics, manual
engineering, or task-specific tuning. In this work, we introduce a semantically
aligned reinforcement learning method where rewards are computed by aligning
the current state with a target semantic instruction using a
Sentence-Bidirectional Encoder Representations from Transformers (SBERT).
Instead of relying on manually defined reward functions, the policy receives
feedback based on the reward, which is a cosine similarity between the goal
textual description and the statement description in the episode. We evaluated
our approach in several environments and showed that semantic reward can guide
learning to achieve competitive control behavior, even in the absence of
hand-crafted reward functions. Our study demonstrates a correlation between the
language embedding space and the conventional Euclidean space. This framework
opens new horizons for aligning agent behavior with natural language goals and
lays the groundwork for a more seamless integration of larger language models
(LLMs) and fluid control applications.

</details>


### [186] [Parameter-free Optimal Rates for Nonlinear Semi-Norm Contractions with Applications to $Q$-Learning](https://arxiv.org/abs/2508.05984)
*Ankur Naskar,Gugan Thoppe,Vijay Gupta*

Main category: cs.LG

TL;DR: The paper addresses the challenge of achieving parameter-free optimal convergence rates for nonlinear fixed-point equations like Q-learning and TD-learning by leveraging semi-norm contractions and Polyak-Ruppert averaging.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with non-monotonicity in semi-norms, hindering optimal convergence rates for nonlinear fixed-point equations.

Method: The authors recast the averaged error as a linear recursion with a nonlinear perturbation and use a coupling technique to tame the nonlinearity by combining semi-norm contraction with a suitably induced norm's monotonicity.

Result: The approach achieves the first parameter-free optimal rates (OÃÉ(1/‚àöt)) for Q-learning in average-reward and discounted settings, applicable across various deployment scenarios.

Conclusion: The proposed framework closes the gap in achieving optimal convergence rates for nonlinear fixed-point equations, with broad applicability in reinforcement learning.

Abstract: Algorithms for solving \textit{nonlinear} fixed-point equations -- such as
average-reward \textit{$Q$-learning} and \textit{TD-learning} -- often involve
semi-norm contractions. Achieving parameter-free optimal convergence rates for
these methods via Polyak--Ruppert averaging has remained elusive, largely due
to the non-monotonicity of such semi-norms. We close this gap by (i.) recasting
the averaged error as a linear recursion involving a nonlinear perturbation,
and (ii.) taming the nonlinearity by coupling the semi-norm's contraction with
the monotonicity of a suitably induced norm. Our main result yields the first
parameter-free $\tilde{O}(1/\sqrt{t})$ optimal rates for $Q$-learning in both
average-reward and exponentially discounted settings, where $t$ denotes the
iteration index. The result applies within a broad framework that accommodates
synchronous and asynchronous updates, single-agent and distributed deployments,
and data streams obtained either from simulators or along Markovian
trajectories.

</details>


### [187] [Pruning the Unsurprising: Efficient Code Reasoning via First-Token Surprisal](https://arxiv.org/abs/2508.05988)
*Wenhao Zeng,Yaoning Wang,Chao Hu,Yuling Shi,Chengcheng Wan,Hongyu Zhang,Xiaodong Gu*

Main category: cs.LG

TL;DR: ASAP is a novel CoT compression framework that improves efficiency and accuracy in code reasoning tasks by combining anchor-guided pruning and logic-aware pruning.


<details>
  <summary>Details</summary>
Motivation: Excessively long reasoning traces in LRMs increase training costs, inference latency, and deployment challenges, while existing CoT compression methods compromise coherence or fail to capture critical steps.

Method: ASAP uses a coarse-to-fine approach: anchor-guided pruning preserves core reasoning structure, and logic-aware pruning selects essential steps using a first-token surprisal metric. It also teaches models to generate concise CoTs autonomously.

Result: ASAP reduces token generation by 23.5% and inference latency by 43.5% on LiveCodeBench v4_v5, achieving 36.19% Pass@1 accuracy, outperforming baselines.

Conclusion: ASAP offers an efficient and accurate solution for CoT compression, advancing the development of powerful and efficient LRMs.

Abstract: Recently, Large Reasoning Models (LRMs) have demonstrated remarkable
capabilities in code reasoning by scaling up the length of Chain-of-Thought
(CoT). However, excessively long reasoning traces introduce substantial
challenges in terms of training cost, inference latency, and deployment
feasibility. While various CoT compression approaches have emerged to address
this challenge, they face inherent trade-offs: token-level methods often
disrupt syntactic and logical coherence, while step-level methods based on
perplexity fail to reliably capture the logically critical reasoning steps. In
this paper, we propose ASAP (Anchor-guided, Surprisal-based Pruning), a novel
coarse-to-fine framework for CoT compression. ASAP first performs anchor-guided
pruning to preserve the core reasoning structure, which efficiently reduces the
search space for subsequent processing. It then enables a logic-aware pruning
by selecting logically essential reasoning steps based on a novel first-token
surprisal metric. Finally, ASAP teaches models to autonomously generate and
leverage these concise CoTs at inference time, enabling efficient reasoning in
coding tasks. Experiments show that ASAP achieves state-of-the-art accuracy
across multiple code generation benchmarks while substantially reducing
training and inference costs. On the challenging LiveCodeBench v4_v5 benchmark,
our approach reduces token generation by 23.5% and inference latency by 43.5%
compared to the strongest baseline, while achieving a competitive accuracy of
36.19% in Pass@1. Our results highlight a promising direction for building
powerful and efficient LRMs.

</details>


### [188] [Optimizing Prompt Sequences using Monte Carlo Tree Search for LLM-Based Optimization](https://arxiv.org/abs/2508.05995)
*Fei Xu Yu,Gina Adam,Nathaniel D. Bastian,Tian Lan*

Main category: cs.LG

TL;DR: MCTS-OPS combines LLMs with Monte Carlo Tree Search to improve multi-step prompt selection for better code generation and optimization.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with complex tasks requiring consistent multi-step planning, and existing MCTS-based methods focus on simpler tasks or heuristic-based code.

Method: MCTS-OPS formulates prompt selection as a sequential decision process guided by MCTS, refining multi-step prompts for improved code generation.

Result: Significant improvements in code execution success rate, optimization rewards (2‚àº4√ó higher), and solution optimality (10% more cases) compared to baselines.

Conclusion: Combining symbolic planning (MCTS) with LLMs enhances robust, high-quality code generation in complex domains.

Abstract: Large language models (LLMs) have demonstrated remarkable capabilities in
code generation and structured reasoning; however, their performance often
degrades on complex tasks that require consistent multi-step planning. Recent
work has explored combining LLMs with Monte Carlo Tree Search (MCTS), yet
existing approaches primarily focus on generating heuristic-based code for
optimization or target simpler tasks where correctness alone is sufficient. In
this work, we propose MCTS-OPS, a novel neural-symbolic framework that
formulates prompt selection as a sequential decision process guided by MCTS.
Our method explores and refines multi-step prompt sequences for the goal of
improving code generation quality and enhancing the problem-solving
capabilities of LLMs in general optimization. Experiments on network
optimization show significant improvement over the baselines, both in the
success rate of executing the generated code and in the optimization results
with the specified objective and constraints (2$\sim$4$\times$ higher reward
and 3$\times$ lower standard deviation). Moreover, it improves the chance of
attaining the optimal solution by about 10\% of cases, compared to baseline
methods in hard problems. These results highlight the promise of combining
symbolic planning with LLMs for robust, high-quality code generation in complex
domains.

</details>


### [189] [Stepwise Fine and Gray: Subject-Specific Variable Selection Shows When Hemodynamic Data Improves Prognostication of Comatose Post-Cardiac Arrest Patients](https://arxiv.org/abs/2508.06023)
*Xiaobin Shen,Jonathan Elmer,George H. Chen*

Main category: cs.LG

TL;DR: A novel stepwise dynamic competing risks model improves neurological outcome prediction for comatose post-cardiac arrest patients by leveraging time-invariant and time-varying features at optimal phases.


<details>
  <summary>Details</summary>
Motivation: Prognostication for comatose post-cardiac arrest patients is critical for ICU decision-making, but current methods don't optimally use time-varying data.

Method: Extends the Fine and Gray model to incorporate two phases (time-invariant and time-varying features) and uses neural networks for nonlinear relationships.

Result: Demonstrated robust performance in predicting outcomes (awakening, withdrawal of therapy, death) in a cohort of 2,278 patients.

Conclusion: The model generalizes to multi-phase feature collection and can enhance dynamic prediction tasks by identifying when and for whom new data improves prognostication.

Abstract: Prognostication for comatose post-cardiac arrest patients is a critical
challenge that directly impacts clinical decision-making in the ICU. Clinical
information that informs prognostication is collected serially over time.
Shortly after cardiac arrest, various time-invariant baseline features are
collected (e.g., demographics, cardiac arrest characteristics). After ICU
admission, additional features are gathered, including time-varying hemodynamic
data (e.g., blood pressure, doses of vasopressor medications). We view these as
two phases in which we collect new features. In this study, we propose a novel
stepwise dynamic competing risks model that improves the prediction of
neurological outcomes by automatically determining when to take advantage of
time-invariant features (first phase) and time-varying features (second phase).
Notably, our model finds patients for whom this second phase (time-varying
hemodynamic) information is beneficial for prognostication and also when this
information is beneficial (as we collect more hemodynamic data for a patient
over time, how important these data are for prognostication varies). Our
approach extends the standard Fine and Gray model to explicitly model the two
phases and to incorporate neural networks to flexibly capture complex nonlinear
feature relationships. Evaluated on a retrospective cohort of 2,278 comatose
post-arrest patients, our model demonstrates robust discriminative performance
for the competing outcomes of awakening, withdrawal of life-sustaining therapy,
and death despite maximal support. Our approach generalizes to more than two
phases in which new features are collected and could be used in other dynamic
prediction tasks, where it may be helpful to know when and for whom newly
collected features significantly improve prediction.

</details>


### [190] [Adaptive Heterogeneous Graph Neural Networks: Bridging Heterophily and Heterogeneity](https://arxiv.org/abs/2508.06034)
*Qin Chen,Guojie Song*

Main category: cs.LG

TL;DR: AHGNN addresses heterophily in heterogeneous graphs with adaptive convolution and attention, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Existing studies overlook heterophilic HGs, leading to performance issues.

Method: AHGNN uses heterophily-aware convolution and coarse-to-fine attention for diverse semantic integration.

Result: AHGNN excels in high-heterophily scenarios, outperforming 20 baselines on 7 real-world graphs.

Conclusion: AHGNN effectively handles heterophily in HGs, improving performance in practical applications.

Abstract: Heterogeneous graphs (HGs) are common in real-world scenarios and often
exhibit heterophily. However, most existing studies focus on either
heterogeneity or heterophily in isolation, overlooking the prevalence of
heterophilic HGs in practical applications. Such ignorance leads to their
performance degradation. In this work, we first identify two main challenges in
modeling heterophily HGs: (1) varying heterophily distributions across hops and
meta-paths; (2) the intricate and often heterophily-driven diversity of
semantic information across different meta-paths. Then, we propose the Adaptive
Heterogeneous Graph Neural Network (AHGNN) to tackle these challenges. AHGNN
employs a heterophily-aware convolution that accounts for heterophily
distributions specific to both hops and meta-paths. It then integrates messages
from diverse semantic spaces using a coarse-to-fine attention mechanism, which
filters out noise and emphasizes informative signals. Experiments on seven
real-world graphs and twenty baselines demonstrate the superior performance of
AHGNN, particularly in high-heterophily situations.

</details>


### [191] [DP-LLM: Runtime Model Adaptation with Dynamic Layer-wise Precision Assignment](https://arxiv.org/abs/2508.06041)
*Sangwoo Kwon,Seong Hoon Seo,Jae W. Lee,Yeonhong Park*

Main category: cs.LG

TL;DR: DP-LLM dynamically adjusts layer precision in on-device LLMs for optimal performance-latency trade-offs, outperforming prior methods.


<details>
  <summary>Details</summary>
Motivation: Handling varying runtime constraints (latency, accuracy) for on-device LLMs efficiently.

Method: Multi-scale quantization with dynamic precision assignment (DP-LLM) using lightweight error estimators and learned thresholds.

Result: Superior performance-latency trade-off across models and benchmarks.

Conclusion: DP-LLM effectively addresses runtime adaptation in LLMs, offering dynamic precision control.

Abstract: How can we effectively handle queries for on-device large language models
(LLMs) with varying runtime constraints, such as latency and accuracy?
Multi-scale quantization addresses this challenge by enabling memory-efficient
runtime model adaptation of LLMs through the overlaying of multiple model
variants quantized to different bitwidths. Meanwhile, an important question
still remains open-ended: how can models be properly configured to match a
target precision or latency? While mixed-precision offers a promising solution,
we take this further by leveraging the key observation that the sensitivity of
each layer dynamically changes across decoding iterations. Building on this
insight, we introduce DP-LLM, a novel mechanism that dynamically assigns
precision to each layer based on input values. DP-LLM augments each linear
layer in an LLM with a precision selector that determines the bitwidth at
runtime using a lightweight error estimator and threshold values learned
through fine-tuning. Experimental results across multiple models and benchmarks
demonstrate that DP-LLM achieves a superior performance-latency trade-off,
outperforming prior approaches.

</details>


### [192] [Architecture-Aware Generalization Bounds for Temporal Networks: Theory and Fair Comparison Methodology](https://arxiv.org/abs/2508.06066)
*Barak Gahtan,Alex M. Bronstein*

Main category: cs.LG

TL;DR: The paper provides non-vacuous generalization bounds for deep temporal models like TCNs, introduces a delayed-feedback blocking mechanism, and evaluates the impact of temporal dependence on learning.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap in theoretical understanding of generalization in deep temporal architectures and provide principled evaluation methods.

Method: Derives generalization bounds for exponentially mixing sequences, introduces a delayed-feedback blocking mechanism, and proposes a fair-comparison methodology to isolate temporal structure effects.

Result: Bounds scale with network depth, kernel size, and input dimension. Temporal dependence can enhance learning under fixed information budgets, with strong dependencies showing smaller generalization gaps.

Conclusion: Temporal dependence can improve learning, but empirical convergence rates diverge from theory, highlighting gaps for future research.

Abstract: Deep temporal architectures such as Temporal Convolutional Networks (TCNs)
achieve strong predictive performance on sequential data, yet theoretical
understanding of their generalization remains limited. We address this gap by
providing both the first non-vacuous, architecture-aware generalization bounds
for deep temporal models and a principled evaluation methodology.
  For exponentially $\beta$-mixing sequences, we derive bounds scaling as $
O\!\Bigl(R\,\sqrt{\tfrac{D\,p\,n\,\log N}{N}}\Bigr), $ where $D$ is network
depth, $p$ kernel size, $n$ input dimension, and $R$ weight norm. Our
delayed-feedback blocking mechanism transforms dependent samples into
effectively independent ones while discarding only $O(1/\log N)$ of the data,
yielding $\sqrt{D}$ scaling instead of exponential, implying that doubling
depth requires approximately quadrupling the training data.
  We also introduce a fair-comparison methodology that fixes the effective
sample size to isolate the effect of temporal structure from information
content. Under $N_{\text{eff}}=2{,}000$, strongly dependent sequences
($\rho=0.8$) exhibit $\approx76\%$ smaller generalization gaps than weakly
dependent ones ($\rho=0.2$), challenging the intuition that dependence is
purely detrimental. Yet convergence rates diverge from theory: weak
dependencies follow $N_{\text{eff}}^{-1.21}$ scaling and strong dependencies
follow $N_{\text{eff}}^{-0.89}$, both steeper than the predicted $N^{-0.5}$.
These findings reveal that temporal dependence can enhance learning under fixed
information budgets, while highlighting gaps between theory and practice that
motivate future research.

</details>


### [193] [Recurrent Deep Differentiable Logic Gate Networks](https://arxiv.org/abs/2508.06097)
*Simon B√ºhrer,Andreas Plesner,Till Aczel,Roger Wattenhofer*

Main category: cs.LG

TL;DR: First implementation of Recurrent Deep Differentiable Logic Gate Networks (RDDLGN) for sequence-to-sequence learning, achieving competitive results on WMT'14 English-German translation.


<details>
  <summary>Details</summary>
Motivation: Explore the application of differentiable logic gates in sequential modeling, an area previously unexplored.

Method: Combine Boolean operations with recurrent architectures to create RDDLGN.

Result: Achieves 5.00 BLEU and 30.9% accuracy during training, nearing GRU performance (5.41 BLEU) with graceful degradation (4.39 BLEU) during inference.

Conclusion: Recurrent logic-based neural computation is viable, enabling new research directions like FPGA acceleration and recursive network architectures.

Abstract: While differentiable logic gates have shown promise in feedforward networks,
their application to sequential modeling remains unexplored. This paper
presents the first implementation of Recurrent Deep Differentiable Logic Gate
Networks (RDDLGN), combining Boolean operations with recurrent architectures
for sequence-to-sequence learning.
  Evaluated on WMT'14 English-German translation, RDDLGN achieves 5.00 BLEU and
30.9\% accuracy during training, approaching GRU performance (5.41 BLEU) and
graceful degradation (4.39 BLEU) during inference. This work establishes
recurrent logic-based neural computation as viable, opening research directions
for FPGA acceleration in sequential modeling and other recursive network
architectures.

</details>


### [194] [GCHR : Goal-Conditioned Hindsight Regularization for Sample-Efficient Reinforcement Learning](https://arxiv.org/abs/2508.06108)
*Xing Lei,Wenyan Yang,Kaiqiang Ke,Shentao Yang,Xuetao Zhang,Joni Pajarinen,Donglin Wang*

Main category: cs.LG

TL;DR: HGR and HSR improve sample efficiency in GCRL by leveraging hindsight goals and self-imitation, outperforming HER-based methods.


<details>
  <summary>Details</summary>
Motivation: Sparse rewards in GCRL limit sample efficiency; HER alone underutilizes experiences.

Method: Proposes HGR (hindsight goal-conditioned regularization) and combines it with HSR (hindsight self-imitation regularization).

Result: Achieves better sample reuse and performance in navigation and manipulation tasks.

Conclusion: HGR and HSR enhance off-policy GCRL by maximizing experience utilization.

Abstract: Goal-conditioned reinforcement learning (GCRL) with sparse rewards remains a
fundamental challenge in reinforcement learning. While hindsight experience
replay (HER) has shown promise by relabeling collected trajectories with
achieved goals, we argue that trajectory relabeling alone does not fully
exploit the available experiences in off-policy GCRL methods, resulting in
limited sample efficiency. In this paper, we propose Hindsight Goal-conditioned
Regularization (HGR), a technique that generates action regularization priors
based on hindsight goals. When combined with hindsight self-imitation
regularization (HSR), our approach enables off-policy RL algorithms to maximize
experience utilization. Compared to existing GCRL methods that employ HER and
self-imitation techniques, our hindsight regularizations achieve substantially
more efficient sample reuse and the best performances, which we empirically
demonstrate on a suite of navigation and manipulation tasks.

</details>


### [195] [Improving Diagnostic Accuracy for Oral Cancer with inpainting Synthesis Lesions Generated Using Diffusion Models](https://arxiv.org/abs/2508.06151)
*Yong Oh Lee,JeeEun Kim,Jung Woo Lee*

Main category: cs.LG

TL;DR: A novel method using synthetic image generation with a fine-tuned diffusion model improves oral cancer diagnostic accuracy by addressing data scarcity.


<details>
  <summary>Details</summary>
Motivation: Limited annotated datasets in oral cancer diagnostics hinder model performance due to data variability and insufficiency.

Method: Proposed a fine-tuned diffusion model for synthesizing realistic oral cancer lesions, enhancing diagnostic algorithms.

Result: Achieved 0.97 accuracy in classification and 0.85 in lesion detection.

Conclusion: Synthetic image generation shows promise for medical diagnostics, with potential applications in other cancers.

Abstract: In oral cancer diagnostics, the limited availability of annotated datasets
frequently constrains the performance of diagnostic models, particularly due to
the variability and insufficiency of training data. To address these
challenges, this study proposed a novel approach to enhance diagnostic accuracy
by synthesizing realistic oral cancer lesions using an inpainting technique
with a fine-tuned diffusion model. We compiled a comprehensive dataset from
multiple sources, featuring a variety of oral cancer images. Our method
generated synthetic lesions that exhibit a high degree of visual fidelity to
actual lesions, thereby significantly enhancing the performance of diagnostic
algorithms. The results show that our classification model achieved a
diagnostic accuracy of 0.97 in differentiating between cancerous and
non-cancerous tissues, while our detection model accurately identified lesion
locations with 0.85 accuracy. This method validates the potential for synthetic
image generation in medical diagnostics and paves the way for further research
into extending these methods to other types of cancer diagnostics.

</details>


### [196] [Differentially Private Federated Clustering with Random Rebalancing](https://arxiv.org/abs/2508.06183)
*Xiyuan Yang,Shengyuan Hu,Soyeon Kim,Tian Li*

Main category: cs.LG

TL;DR: RR-Cluster is a lightweight add-on for federated clustering algorithms that improves privacy/utility tradeoffs by rebalancing cluster assignments to reduce privacy noise.


<details>
  <summary>Details</summary>
Motivation: Federated clustering enhances model performance but increases privacy risks. Standard DP mechanisms degrade utility due to uncontrolled client assignments.

Method: Proposes RR-Cluster, a technique to randomly rebalance cluster assignments, ensuring a minimum client count per cluster to reduce noise.

Result: RR-Cluster improves privacy/utility tradeoffs, validated on synthetic and real-world datasets.

Conclusion: RR-Cluster effectively balances privacy and utility in federated clustering, with theoretical and empirical support.

Abstract: Federated clustering aims to group similar clients into clusters and produce
one model for each cluster. Such a personalization approach typically improves
model performance compared with training a single model to serve all clients,
but can be more vulnerable to privacy leakage. Directly applying client-level
differentially private (DP) mechanisms to federated clustering could degrade
the utilities significantly. We identify that such deficiencies are mainly due
to the difficulties of averaging privacy noise within each cluster (following
standard privacy mechanisms), as the number of clients assigned to the same
clusters is uncontrolled. To this end, we propose a simple and effective
technique, named RR-Cluster, that can be viewed as a light-weight add-on to
many federated clustering algorithms. RR-Cluster achieves reduced privacy noise
via randomly rebalancing cluster assignments, guaranteeing a minimum number of
clients assigned to each cluster. We analyze the tradeoffs between decreased
privacy noise variance and potentially increased bias from incorrect
assignments and provide convergence bounds for RR-Clsuter. Empirically, we
demonstrate the RR-Cluster plugged into strong federated clustering algorithms
results in significantly improved privacy/utility tradeoffs across both
synthetic and real-world datasets.

</details>


### [197] [Benchmarking Pretrained Molecular Embedding Models For Molecular Representation Learning](https://arxiv.org/abs/2508.06199)
*Mateusz Praski,Jakub Adamczyk,Wojciech Czech*

Main category: cs.LG

TL;DR: A study compares 25 pretrained neural models for molecular chemistry tasks, finding most offer no improvement over the baseline ECFP fingerprint, except CLAMP.


<details>
  <summary>Details</summary>
Motivation: To rigorously evaluate the effectiveness of pretrained neural models in molecular chemistry tasks compared to traditional methods like ECFP fingerprints.

Method: Comparison of 25 models across 25 datasets using a hierarchical Bayesian statistical testing framework.

Result: Most neural models show negligible improvement over ECFP; only CLAMP performs significantly better.

Conclusion: Highlights evaluation rigor concerns in existing studies and suggests practical recommendations for improvement.

Abstract: Pretrained neural networks have attracted significant interest in chemistry
and small molecule drug design. Embeddings from these models are widely used
for molecular property prediction, virtual screening, and small data learning
in molecular chemistry. This study presents the most extensive comparison of
such models to date, evaluating 25 models across 25 datasets. Under a fair
comparison framework, we assess models spanning various modalities,
architectures, and pretraining strategies. Using a dedicated hierarchical
Bayesian statistical testing model, we arrive at a surprising result: nearly
all neural models show negligible or no improvement over the baseline ECFP
molecular fingerprint. Only the CLAMP model, which is also based on molecular
fingerprints, performs statistically significantly better than the
alternatives. These findings raise concerns about the evaluation rigor in
existing studies. We discuss potential causes, propose solutions, and offer
practical recommendations.

</details>


### [198] [Graph Federated Learning for Personalized Privacy Recommendation](https://arxiv.org/abs/2508.06208)
*Ce Na,Kai Yang,Dengzhao Fang,Yu Li,Jingtong Gao,Chengcheng Zhu,Jiale Zhang,Xiaobing Sun,Yi Chang*

Main category: cs.LG

TL;DR: GFed-PP is a federated recommendation system that adapts to varying user privacy preferences, leveraging public user data for improved performance while ensuring privacy.


<details>
  <summary>Details</summary>
Motivation: Existing FedRecs assume uniform privacy requirements, ignoring the potential of public user data. GFed-PP addresses this by accommodating both private and public users.

Method: GFed-PP uses a user-item interaction graph for public users, builds a user relationship graph, and employs a lightweight GCN for personalized embeddings. Privacy is maintained via local learning and federated optimization.

Result: GFed-PP outperforms existing methods on five datasets, enhancing recommendation accuracy without compromising privacy.

Conclusion: GFed-PP offers a practical solution for federated recommendation systems by balancing privacy and performance through adaptive privacy preferences.

Abstract: Federated recommendation systems (FedRecs) have gained significant attention
for providing privacy-preserving recommendation services. However, existing
FedRecs assume that all users have the same requirements for privacy
protection, i.e., they do not upload any data to the server. The approaches
overlook the potential to enhance the recommendation service by utilizing
publicly available user data. In real-world applications, users can choose to
be private or public. Private users' interaction data is not shared, while
public users' interaction data can be shared. Inspired by the issue, this paper
proposes a novel Graph Federated Learning for Personalized Privacy
Recommendation (GFed-PP) that adapts to different privacy requirements while
improving recommendation performance. GFed-PP incorporates the interaction data
of public users to build a user-item interaction graph, which is then used to
form a user relationship graph. A lightweight graph convolutional network (GCN)
is employed to learn each user's user-specific personalized item embedding. To
protect user privacy, each client learns the user embedding and the scoring
function locally. Additionally, GFed-PP achieves optimization of the federated
recommendation framework through the initialization of item embedding on
clients and the aggregation of the user relationship graph on the server.
Experimental results demonstrate that GFed-PP significantly outperforms
existing methods for five datasets, offering superior recommendation accuracy
without compromising privacy. This framework provides a practical solution for
accommodating varying privacy preferences in federated recommendation systems.

</details>


### [199] [Reparameterization Proximal Policy Optimization](https://arxiv.org/abs/2508.06214)
*Hai Zhong,Xun Wang,Zhuoran Li,Longbo Huang*

Main category: cs.LG

TL;DR: RPO combines RPG and PPO for stable, sample-efficient reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: Training instability in RPG due to high-variance gradients hinders sample efficiency.

Method: Proposes RPO by linking RPG with PPO's surrogate objective, enabling stable sample reuse via backpropagation.

Result: RPO outperforms on locomotion and manipulation tasks with superior sample efficiency.

Conclusion: RPO bridges RPG and PPO, offering stable, efficient reinforcement learning.

Abstract: Reparameterization policy gradient (RPG) is promising for improving sample
efficiency by leveraging differentiable dynamics. However, a critical barrier
is its training instability, where high-variance gradients can destabilize the
learning process. To address this, we draw inspiration from Proximal Policy
Optimization (PPO), which uses a surrogate objective to enable stable sample
reuse in the model-free setting. We first establish a connection between this
surrogate objective and RPG, which has been largely unexplored and is
non-trivial. Then, we bridge this gap by demonstrating that the
reparameterization gradient of a PPO-like surrogate objective can be computed
efficiently using backpropagation through time. Based on this key insight, we
propose Reparameterization Proximal Policy Optimization (RPO), a stable and
sample-efficient RPG-based method. RPO enables multiple epochs of stable sample
reuse by optimizing a clipped surrogate objective tailored for RPG, while being
further stabilized by Kullback-Leibler (KL) divergence regularization and
remaining fully compatible with existing variance reduction methods. We
evaluate RPO on a suite of challenging locomotion and manipulation tasks, where
experiments demonstrate that our method achieves superior sample efficiency and
strong performance.

</details>


### [200] [SCAR: State-Space Compression for AI-Driven Resource Management in 6G-Enabled Vehicular Infotainment Systems](https://arxiv.org/abs/2508.06243)
*Ioan-Sorin Comsa,Purav Shah,Karthik Vaidhyanathan,Deepak Gangadharan,Christof Imhof,Per Bergamin,Aryan Kaushik,Gabriel-Miro Muntean,Ramona Trestian*

Main category: cs.LG

TL;DR: SCAR is an Edge AI framework for 6G vehicular networks, using ML-based compression and RL to optimize scheduling and fairness, outperforming baselines in throughput and fairness.


<details>
  <summary>Details</summary>
Motivation: Traditional RRM techniques struggle with the complexity and volume of data in 6G vehicular networks, necessitating a more efficient solution.

Method: SCAR employs ML-based compression (clustering, RBF networks) and trains RL policies on compressed CQI data to optimize scheduling and fairness.

Result: SCAR improves feasible scheduling time by 14%, reduces unfair scheduling by 15%, and cuts CQI clustering distortion by 10%.

Conclusion: SCAR is scalable and effective for dynamic vehicular networks, enhancing fairness and efficiency in 6G infotainment services.

Abstract: The advent of 6G networks opens new possibilities for connected infotainment
services in vehicular environments. However, traditional Radio Resource
Management (RRM) techniques struggle with the increasing volume and complexity
of data such as Channel Quality Indicators (CQI) from autonomous vehicles. To
address this, we propose SCAR (State-Space Compression for AI-Driven Resource
Management), an Edge AI-assisted framework that optimizes scheduling and
fairness in vehicular infotainment. SCAR employs ML-based compression
techniques (e.g., clustering and RBF networks) to reduce CQI data size while
preserving essential features. These compressed states are used to train
6G-enabled Reinforcement Learning policies that maximize throughput while
meeting fairness objectives defined by the NGMN. Simulations show that SCAR
increases time in feasible scheduling regions by 14\% and reduces unfair
scheduling time by 15\% compared to RL baselines without CQI compression.
Furthermore, Simulated Annealing with Stochastic Tunneling (SAST)-based
clustering reduces CQI clustering distortion by 10\%, confirming its
efficiency. These results demonstrate SCAR's scalability and fairness benefits
for dynamic vehicular networks.

</details>


### [201] [Epidemic Control on a Large-Scale-Agent-Based Epidemiology Model using Deep Deterministic Policy Gradient](https://arxiv.org/abs/2304.04475)
*Gaurav Deshkar,Jayanta Kshirsagar,Harshal Hayatnagarkar,Janani Venugopalan*

Main category: cs.LG

TL;DR: The paper proposes a DDPG-based framework for optimizing pandemic interventions like lockdowns and vaccinations, balancing health and economic outcomes on a large-scale simulation.


<details>
  <summary>Details</summary>
Motivation: Current research lacks scalability and flexibility in modeling optimal pandemic interventions, limiting their practical applicability.

Method: Uses a Deep Deterministic Policy Gradient (DDPG) framework on a large-scale agent-based simulation (100,000 individuals) for multi-objective optimization of lockdown and vaccination strategies.

Result: Optimal policies balance health (infection, hospitalization) and economy (poverty reduction) without lockdowns, focusing on mid-age and elderly vaccination.

Conclusion: The framework shows promise but requires further validation and open-sourcing for broader application.

Abstract: To mitigate the impact of the pandemic, several measures include lockdowns,
rapid vaccination programs, school closures, and economic stimulus. These
interventions can have positive or unintended negative consequences. Current
research to model and determine an optimal intervention automatically through
round-tripping is limited by the simulation objectives, scale (a few thousand
individuals), model types that are not suited for intervention studies, and the
number of intervention strategies they can explore (discrete vs continuous). We
address these challenges using a Deep Deterministic Policy Gradient (DDPG)
based policy optimization framework on a large-scale (100,000 individual)
epidemiological agent-based simulation where we perform multi-objective
optimization. We determine the optimal policy for lockdown and vaccination in a
minimalist age-stratified multi-vaccine scenario with a basic simulation for
economic activity. With no lockdown and vaccination (mid-age and elderly),
results show optimal economy (individuals below the poverty line) with balanced
health objectives (infection, and hospitalization). An in-depth simulation is
needed to further validate our results and open-source our framework.

</details>


### [202] [Membership Inference Attack with Partial Features](https://arxiv.org/abs/2508.06244)
*Xurun Wang,Guangrui Liu,Xinjie Li,Haoyu He,Lin Yao,Weizhe Zhang*

Main category: cs.LG

TL;DR: The paper introduces Partial Feature Membership Inference (PFMI), a scenario where an attacker infers membership using partial feature data, and proposes MRAD, a two-stage attack framework combining reconstruction and anomaly detection.


<details>
  <summary>Details</summary>
Motivation: Existing membership inference attacks assume full feature access, which is unrealistic. The study addresses the gap by exploring attacks with partial feature information.

Method: Proposes MRAD: (1) optimizes missing features to minimize loss, (2) uses anomaly detection to measure deviation from training distribution.

Result: MRAD is effective across datasets, achieving AUC ~0.6 on STL-10 with 40% missing features.

Conclusion: MRAD addresses PFMI effectively, demonstrating robustness with partial feature access and compatibility with anomaly detection techniques.

Abstract: Machine learning models have been shown to be susceptible to membership
inference attack, which can be used to determine whether a given sample appears
in the training data. Existing membership inference methods commonly assume
that the adversary has full access to the features of the target sample. This
assumption, however, does not hold in many real-world scenarios where only
partial features information is available, thereby limiting the applicability
of these methods. In this work, we study an inference scenario where the
adversary observes only partial features of each sample and aims to infer
whether this observed subset was present in the training set of the target
model. We define this problem as Partial Feature Membership Inference (PFMI).
To address this problem, we propose MRAD (Memory-guided Reconstruction and
Anomaly Detection), a two-stage attack framework. In the first stage, MRAD
optimizes the unknown feature values to minimize the loss of the sample. In the
second stage, it measures the deviation between the reconstructed sample and
the training distribution using anomaly detection. Empirical results
demonstrate that MRAD is effective across a range of datasets, and maintains
compatibility with various off-the-shelf anomaly detection techniques. For
example, on STL-10, our attack achieves an AUC of around 0.6 even with 40% of
the missing features.

</details>


### [203] [Near-Optimal Regret for Efficient Stochastic Combinatorial Semi-Bandits](https://arxiv.org/abs/2508.06247)
*Zichun Ye,Runqi Wang,Xutong Liu,Shuai Li*

Main category: cs.LG

TL;DR: CMOSS is a new algorithm for combinatorial multi-armed bandits that avoids the log T regret of UCB-based methods and the computational overhead of adversarial methods, achieving near-optimal regret.


<details>
  <summary>Details</summary>
Motivation: Existing methods like CUCB and adversarial approaches (EXP3.M, HYBRID) have limitations: CUCB suffers from log T regret, while adversarial methods are computationally heavy. CMOSS aims to resolve this trade-off.

Method: CMOSS is a computationally efficient algorithm designed for semi-bandit feedback, achieving instance-independent regret of O((log k)^2‚àö(kmT)). It is also extended to cascading feedback.

Result: CMOSS eliminates the log T dependency and matches the lower bound Œ©(‚àö(kmT)) up to O((log k)^2). Experiments show it outperforms benchmarks in regret and runtime.

Conclusion: CMOSS is a practical and efficient solution for combinatorial bandits, offering near-optimal regret without computational drawbacks.

Abstract: The combinatorial multi-armed bandit (CMAB) is a cornerstone of sequential
decision-making framework, dominated by two algorithmic families: UCB-based and
adversarial methods such as follow the regularized leader (FTRL) and online
mirror descent (OMD). However, prominent UCB-based approaches like CUCB suffer
from additional regret factor $\log T$ that is detrimental over long horizons,
while adversarial methods such as EXP3.M and HYBRID impose significant
computational overhead. To resolve this trade-off, we introduce the
Combinatorial Minimax Optimal Strategy in the Stochastic setting (CMOSS). CMOSS
is a computationally efficient algorithm that achieves an instance-independent
regret of $O\big( (\log k)^2\sqrt{kmT}\big )$ under semi-bandit feedback, where
$m$ is the number of arms and $k$ is the maximum cardinality of a feasible
action. Crucially, this result eliminates the dependency on $\log T$ and
matches the established $\Omega\big( \sqrt{kmT}\big)$ lower bound up to
$O\big((\log k)^2\big)$. We then extend our analysis to show that CMOSS is also
applicable to cascading feedback. Experiments on synthetic and real-world
datasets validate that CMOSS consistently outperforms benchmark algorithms in
both regret and runtime efficiency.

</details>


### [204] [AttriLens-Mol: Attribute Guided Reinforcement Learning for Molecular Property Prediction with Large Language Models](https://arxiv.org/abs/2508.04748)
*Xuan Lin,Long Chen,Yile Wang*

Main category: cs.LG

TL;DR: AttriLens-Mol is a reinforcement learning framework for molecular property prediction with LLMs, using attribute-guided rewards to improve reasoning and performance.


<details>
  <summary>Details</summary>
Motivation: Current LLMs for molecular property prediction rely on human-crafted prompts and verbose reasoning, lacking relevance. AttriLens-Mol aims to address this by guiding reasoning with structured rewards.

Method: AttriLens-Mol uses three rewards: format (structured output), count (avoid irrelevant attributes), and rationality (verified relatedness). It trains models like R1-Distilled-Qwen2.5 and R1-Distilled-LLaMA3.1 on 4,000 samples.

Result: The method outperforms supervised fine-tuning and advanced models (GPT-3.5, GPT-4o, etc.) on in-distribution and out-of-distribution datasets. Extracted attributes also improve interpretability and decision tree performance.

Conclusion: AttriLens-Mol effectively elicits relevant molecular attributes, enhancing interpretability and prediction performance, and is released as open-source.

Abstract: Large Language Models (LLMs) have shown promise in assisting molecular
property prediction tasks but often rely on human-crafted prompts and
chain-of-thought templates. While recent advanced large reasoning models like
DeepSeek-R1 employ reinforcement learning for an extended ``thinking'' process,
their reasoning can be verbose and lack relevance. We introduce AttriLens-Mol,
an attribute-guided reinforcement learning framework for molecular property
prediction with LLMs. AttriLens-Mol steers the model's reasoning by using: (1)
a format reward encouraging attribute-based structured output, (2) a count
reward to avoid enumerating irrelevant attributes, and (3) a rationality reward
using advanced LLMs and RDKit to verify the relatedness of the generated
attributes. This approach implicitly elicits the model's inherent knowledge of
relevant molecular attributes during reasoning, enables making predictions for
the molecular property more effectively. Experiments on both in-distribution
and out-of-distribution datasets show that, training both 7B-size
R1-Distilled-Qwen2.5 and R1-Distilled-LLaMA3.1 models on 4,000 samples with our
proposed AttriLens-Mol method significantly boosts the performance, getting
comparable or better results than supervised fine-tuning models
(Mol-Instructions, ChemDFM, etc.) and advanced models (GPT-3.5, GPT-4o,
DeepSeek-V3, DeepSeek-R1, etc.). Further, our extracted attributes for the
target property, when used as features for an interpretable decision tree
model, yield superior performance compared to attributes generated by prompting
LLMs. This shows that AttriLens-Mol effectively elicits more relevant and
predictive molecular attributes, leading to enhanced interpretability and
performance for property prediction. We release the code in
https://github.com/szu-tera/AttriLens-Mol.

</details>


### [205] [In-Training Defenses against Emergent Misalignment in Language Models](https://arxiv.org/abs/2508.06249)
*David Kacz√©r,Magnus J√∏rgenv√•g,Clemens Vetter,Lucie Flek,Florian Mai*

Main category: cs.LG

TL;DR: The paper studies safeguards against emergent misalignment (EMA) in fine-tuned LLMs, evaluating four regularization methods to prevent harmful behaviors outside the target domain.


<details>
  <summary>Details</summary>
Motivation: Fine-tuning LLMs can inadvertently induce harmful behaviors (EMA), even with hidden weights, posing risks. The study aims to provide practical safeguards for API-based fine-tuning.

Method: Four interventions are tested: (i) KL-divergence regularization, (ii) ‚Ñì2 distance in feature space, (iii) SafeLoRA (safe subspace projection), and (iv) interleaving safe examples.

Result: The methods are evaluated on EMA-inducing tasks and benign tasks, assessing their effectiveness and impact.

Conclusion: The paper highlights open questions in EMA research and the need for further study.

Abstract: Fine-tuning lets practitioners repurpose aligned large language models (LLMs)
for new domains, yet recent work reveals emergent misalignment (EMA): Even a
small, domain-specific fine-tune can induce harmful behaviors far outside the
target domain. Even in the case where model weights are hidden behind a
fine-tuning API, this gives attackers inadvertent access to a broadly
misaligned model in a way that can be hard to detect from the fine-tuning data
alone. We present the first systematic study of in-training safeguards against
EMA that are practical for providers who expose fine-tuning via an API. We
investigate four training regularization interventions: (i) KL-divergence
regularization toward a safe reference model, (ii) $\ell_2$ distance in feature
space, (iii) projecting onto a safe subspace (SafeLoRA), and (iv) interleaving
of a small amount of safe training examples from a general instruct-tuning
dataset. We first evaluate the methods' emergent misalignment effect across
four malicious, EMA-inducing tasks. Second, we assess the methods' impacts on
benign tasks. We conclude with a discussion of open questions in emergent
misalignment research.

</details>


### [206] [Synthetic Data Generation and Differential Privacy using Tensor Networks' Matrix Product States (MPS)](https://arxiv.org/abs/2508.06251)
*Alejandro Moreno R.,Desale Fentaw,Samuel Palmer,Ra√∫l Salles de Padua,Ninad Dixit,Samuel Mugel,Roman Or√∫s,Manuel Radons,Josef Menter,Ali Abedi*

Main category: cs.LG

TL;DR: Proposes a Tensor Networks (MPS) method for generating high-quality, privacy-preserving synthetic tabular data, outperforming models like CTGAN and VAE under strict privacy constraints.


<details>
  <summary>Details</summary>
Motivation: Addresses data scarcity, privacy concerns, and the need for diverse datasets in AI by leveraging synthetic data generation.

Method: Uses Matrix Product States (MPS) with noise injection and gradient clipping for differential privacy (DP) guarantees via R√©nyi DP accounting.

Result: MPS outperforms classical models (CTGAN, VAE, PrivBayes) in fidelity and privacy, especially under strict privacy constraints.

Conclusion: MPS is a promising, interpretable, and scalable tool for privacy-aware synthetic data generation, suitable for sensitive domains.

Abstract: Synthetic data generation is a key technique in modern artificial
intelligence, addressing data scarcity, privacy constraints, and the need for
diverse datasets in training robust models. In this work, we propose a method
for generating privacy-preserving high-quality synthetic tabular data using
Tensor Networks, specifically Matrix Product States (MPS). We benchmark the
MPS-based generative model against state-of-the-art models such as CTGAN, VAE,
and PrivBayes, focusing on both fidelity and privacy-preserving capabilities.
To ensure differential privacy (DP), we integrate noise injection and gradient
clipping during training, enabling privacy guarantees via R\'enyi Differential
Privacy accounting. Across multiple metrics analyzing data fidelity and
downstream machine learning task performance, our results show that MPS
outperforms classical models, particularly under strict privacy constraints.
This work highlights MPS as a promising tool for privacy-aware synthetic data
generation. By combining the expressive power of tensor network representations
with formal privacy mechanisms, the proposed approach offers an interpretable
and scalable alternative for secure data sharing. Its structured design
facilitates integration into sensitive domains where both data quality and
confidentiality are critical.

</details>


### [207] [Multi-Omics Analysis for Cancer Subtype Inference via Unrolling Graph Smoothness Priors](https://arxiv.org/abs/2508.06257)
*Jielong Lu,Zhihao Wu,Jiajun Yu,Jiajun Bu,Haishuai Wang*

Main category: cs.LG

TL;DR: GTMancer, a Graph Transformer framework, enhances multi-omics cancer subtype classification by integrating contrastive learning and dual attention mechanisms, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to fully capture the intricate coupling between heterogeneous omics data, limiting their ability to resolve subtle cancer subtype heterogeneity for precision oncology.

Method: GTMancer uses contrastive learning to embed multi-omics data into a unified semantic space and employs dual attention coefficients to capture structural graph priors within and among omics data.

Result: GTMancer outperforms state-of-the-art algorithms on seven real-world cancer datasets.

Conclusion: The proposed framework effectively integrates multi-omics data, improving cancer subtype classification and advancing precision oncology.

Abstract: Integrating multi-omics datasets through data-driven analysis offers a
comprehensive understanding of the complex biological processes underlying
various diseases, particularly cancer. Graph Neural Networks (GNNs) have
recently demonstrated remarkable ability to exploit relational structures in
biological data, enabling advances in multi-omics integration for cancer
subtype classification. Existing approaches often neglect the intricate
coupling between heterogeneous omics, limiting their capacity to resolve subtle
cancer subtype heterogeneity critical for precision oncology. To address these
limitations, we propose a framework named Graph Transformer for Multi-omics
Cancer Subtype Classification (GTMancer). This framework builds upon the GNN
optimization problem and extends its application to complex multi-omics data.
Specifically, our method leverages contrastive learning to embed multi-omics
data into a unified semantic space. We unroll the multiplex graph optimization
problem in that unified space and introduce dual sets of attention coefficients
to capture structural graph priors both within and among multi-omics data. This
approach enables global omics information to guide the refining of the
representations of individual omics. Empirical experiments on seven real-world
cancer datasets demonstrate that GTMancer outperforms existing state-of-the-art
algorithms.

</details>


### [208] [OM2P: Offline Multi-Agent Mean-Flow Policy](https://arxiv.org/abs/2508.06269)
*Zhuoran Li,Xun Wang,Hai Zhong,Longbo Huang*

Main category: cs.LG

TL;DR: OM2P introduces a one-step action sampling method for offline multi-agent reinforcement learning, improving efficiency and reducing resource usage.


<details>
  <summary>Details</summary>
Motivation: Addressing the inefficiency of diffusion and flow-based models in offline MARL due to iterative generation processes.

Method: Proposes OM2P with reward-aware optimization, mean-flow matching loss, Q-function supervision, and derivative-free estimation.

Result: Achieves 3.8x GPU memory reduction and 10.8x training speed-up, outperforming benchmarks.

Conclusion: OM2P successfully integrates mean-flow models into offline MARL, enabling practical generative policies for multi-agent settings.

Abstract: Generative models, especially diffusion and flow-based models, have been
promising in offline multi-agent reinforcement learning. However, integrating
powerful generative models into this framework poses unique challenges. In
particular, diffusion and flow-based policies suffer from low sampling
efficiency due to their iterative generation processes, making them impractical
in time-sensitive or resource-constrained settings. To tackle these
difficulties, we propose OM2P (Offline Multi-Agent Mean-Flow Policy), a novel
offline MARL algorithm to achieve efficient one-step action sampling. To
address the misalignment between generative objectives and reward maximization,
we introduce a reward-aware optimization scheme that integrates a
carefully-designed mean-flow matching loss with Q-function supervision.
Additionally, we design a generalized timestep distribution and a
derivative-free estimation strategy to reduce memory overhead and improve
training stability. Empirical evaluations on Multi-Agent Particle and MuJoCo
benchmarks demonstrate that OM2P achieves superior performance, with up to a
3.8x reduction in GPU memory usage and up to a 10.8x speed-up in training time.
Our approach represents the first to successfully integrate mean-flow model
into offline MARL, paving the way for practical and scalable generative
policies in cooperative multi-agent settings.

</details>


### [209] [A Study on Regularization-Based Continual Learning Methods for Indic ASR](https://arxiv.org/abs/2508.06280)
*Gokul Adethya T,S. Jaya Nirmala*

Main category: cs.LG

TL;DR: The paper explores Continual Learning (CL) for ASR in Indian languages, testing three CL strategies to mitigate forgetting while sequentially learning new languages. Results show CL's effectiveness over naive fine-tuning.


<details>
  <summary>Details</summary>
Motivation: India's linguistic diversity and data privacy constraints make traditional multilingual ASR impractical, necessitating CL for sequential learning without forgetting.

Method: A Conformer-based hybrid RNN-T/CTC model pretrained on Hindi is incrementally trained on eight more Indian languages. Three CL strategies (EWC, MAS, LwF) are evaluated.

Result: CL outperforms naive fine-tuning in mitigating forgetting, with performance analyzed via WER and Backward Transfer. Training epochs (1-10) per task are also explored.

Conclusion: CL is effective for scalable ASR in diverse Indian languages under realistic constraints, with code made publicly available.

Abstract: Indias linguistic diversity poses significant challenges for developing
inclusive Automatic Speech Recognition (ASR) systems. Traditional multilingual
models, which require simultaneous access to all language data, are impractical
due to the sequential arrival of data and privacy constraints. Continual
Learning (CL) offers a solution by enabling models to learn new languages
sequentially without catastrophically forgetting previously learned knowledge.
This paper investigates CL for ASR on Indian languages using a subset of the
IndicSUPERB benchmark. We employ a Conformer-based hybrid RNN-T/CTC model,
initially pretrained on Hindi, which is then incrementally trained on eight
additional Indian languages, for a total sequence of nine languages. We
evaluate three prominent regularization- and distillation-based CL strategies:
Elastic Weight Consolidation (EWC), Memory Aware Synapses (MAS), and Learning
without Forgetting (LwF), selected for their suitability in no-replay,
privacy-conscious scenarios. Performance is analyzed using Word Error Rate
(WER) for both RNN-T and CTC paths on clean and noisy data, as well as
knowledge retention via Backward Transfer. We also explore the impact of
varying the number of training epochs (1, 2, 5, and 10) per task. Results,
compared against naive fine-tuning, demonstrate CLs effectiveness in mitigating
forgetting, making it a promising approach for scalable ASR in diverse Indian
languages under realistic constraints. The code is available at:
https://github.com/FrozenWolf-Cyber/Indic-CL-ASR

</details>


### [210] [Low-Bit Data Processing Using Multiple-Output Spiking Neurons with Non-linear Reset Feedback](https://arxiv.org/abs/2508.06292)
*Sanja Karilanova,Subhrakanti Dey,Ay√ßa √ñz√ßelikkale*

Main category: cs.LG

TL;DR: The paper proposes a novel spiking neuron model combining linear state-space models (SSMs) with non-linear feedback, achieving performance comparable to existing benchmarks in SNNs.


<details>
  <summary>Details</summary>
Motivation: To bridge the benefits of spiking neural networks (SNNs) and deep state-space models (SSMs) by addressing the lack of reset mechanisms and high-precision activations in SSMs.

Method: Introduces a multiple-output spiking neuron model with linear SSM state transitions and non-linear feedback via reset, clarifying distinctions between spiking, reset condition, and reset action.

Result: The model performs comparably to SNN benchmarks in tasks like keyword spotting, event-based vision, and sequential pattern recognition, overcoming instability in linear dynamics.

Conclusion: The proposed reset mechanism enhances learning and stability, extending beyond the limitations of strict linear dynamics in deep SSMs.

Abstract: Neuromorphic computing is an emerging technology enabling low-latency and
energy-efficient signal processing. A key algorithmic tool in neuromorphic
computing is spiking neural networks (SNNs). SNNs are biologically inspired
neural networks which utilize stateful neurons, and provide low-bit data
processing by encoding and decoding information using spikes. Similar to SNNs,
deep state-space models (SSMs) utilize stateful building blocks. However, deep
SSMs, which recently achieved competitive performance in various temporal
modeling tasks, are typically designed with high-precision activation functions
and no reset mechanisms. To bridge the gains offered by SNNs and the recent
deep SSM models, we propose a novel multiple-output spiking neuron model that
combines a linear, general SSM state transition with a non-linear feedback
mechanism through reset. Compared to the existing neuron models for SNNs, our
proposed model clearly conceptualizes the differences between the spiking
function, the reset condition and the reset action. The experimental results on
various tasks, i.e., a keyword spotting task, an event-based vision task and a
sequential pattern recognition task, show that our proposed model achieves
performance comparable to existing benchmarks in the SNN literature. Our
results illustrate how the proposed reset mechanism can overcome instability
and enable learning even when the linear part of neuron dynamics is unstable,
allowing us to go beyond the strictly enforced stability of linear dynamics in
recent deep SSM models.

</details>


### [211] [FedMeNF: Privacy-Preserving Federated Meta-Learning for Neural Fields](https://arxiv.org/abs/2508.06301)
*Junhyeog Yun,Minui Hong,Gunhee Kim*

Main category: cs.LG

TL;DR: FedMeNF is a novel Federated Meta-Learning approach for neural fields that ensures privacy while optimizing efficiently with limited data.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of traditional FML in neural fields, such as privacy leakage and high resource demands.

Method: Introduces FedMeNF with a privacy-preserving loss function for local meta-optimization, avoiding data retention.

Result: Achieves fast optimization and robust performance with few-shot or non-IID data, preserving privacy.

Conclusion: FedMeNF effectively balances privacy and efficiency in neural field learning for edge devices.

Abstract: Neural fields provide a memory-efficient representation of data, which can
effectively handle diverse modalities and large-scale data. However, learning
to map neural fields often requires large amounts of training data and
computations, which can be limited to resource-constrained edge devices. One
approach to tackle this limitation is to leverage Federated Meta-Learning
(FML), but traditional FML approaches suffer from privacy leakage. To address
these issues, we introduce a novel FML approach called FedMeNF. FedMeNF
utilizes a new privacy-preserving loss function that regulates privacy leakage
in the local meta-optimization. This enables the local meta-learner to optimize
quickly and efficiently without retaining the client's private data. Our
experiments demonstrate that FedMeNF achieves fast optimization speed and
robust reconstruction performance, even with few-shot or non-IID data across
diverse data modalities, while preserving client data privacy.

</details>


### [212] [Unsupervised Partner Design Enables Robust Ad-hoc Teamwork](https://arxiv.org/abs/2508.06336)
*Constantin Ruhdorfer,Matteo Bortoletto,Victor Oei,Anna Penzkofer,Andreas Bulling*

Main category: cs.LG

TL;DR: UPD is a population-free, multi-agent reinforcement learning framework for ad-hoc teamwork, generating diverse training partners without pretrained agents or manual tuning. It outperforms baselines and is perceived as more adaptive and human-like.


<details>
  <summary>Details</summary>
Motivation: To enable robust ad-hoc teamwork without relying on pretrained partners or manual parameter tuning, addressing limitations of existing methods.

Method: UPD stochastically mixes an ego agent's policy with biased random behaviours, scoring partners using a variance-based learnability metric. It integrates with unsupervised environment design for dynamic curricula.

Result: UPD outperforms population-based and population-free baselines in evaluations on Overcooked-AI and a user study, showing higher returns and better human-like collaboration.

Conclusion: UPD is highly effective for unsupervised partner design, offering superior performance and adaptability in cooperative settings.

Abstract: We introduce Unsupervised Partner Design (UPD) - a population-free,
multi-agent reinforcement learning framework for robust ad-hoc teamwork that
adaptively generates training partners without requiring pretrained partners or
manual parameter tuning. UPD constructs diverse partners by stochastically
mixing an ego agent's policy with biased random behaviours and scores them
using a variance-based learnability metric that prioritises partners near the
ego agent's current learning frontier. We show that UPD can be integrated with
unsupervised environment design, resulting in the first method enabling fully
unsupervised curricula over both level and partner distributions in a
cooperative setting. Through extensive evaluations on Overcooked-AI and the
Overcooked Generalisation Challenge, we demonstrate that this dynamic partner
curriculum is highly effective: UPD consistently outperforms both
population-based and population-free baselines as well as ablations. In a user
study, we further show that UPD achieves higher returns than all baselines and
was perceived as significantly more adaptive, more human-like, a better
collaborator, and less frustrating.

</details>


### [213] [Introducing Fractional Classification Loss for Robust Learning with Noisy Labels](https://arxiv.org/abs/2508.06346)
*Mert Can Kurucu,Tufan Kumbasar,ƒ∞brahim Eksin,M√ºjde G√ºzelkaya*

Main category: cs.LG

TL;DR: FCL is an adaptive robust loss function for deep neural networks that automatically adjusts to label noise, balancing robustness and convergence speed without manual tuning.


<details>
  <summary>Details</summary>
Motivation: Existing robust loss functions require extensive hyperparameter tuning for label noise, which is inefficient and dataset-specific.

Method: FCL combines the fractional derivative of Cross-Entropy (active) and Mean Absolute Error (passive) losses, with a learnable parameter Œº to dynamically adjust robustness.

Result: FCL achieves state-of-the-art performance on benchmark datasets without manual hyperparameter tuning.

Conclusion: FCL provides an effective, adaptive solution for training deep networks under label noise, eliminating the need for dataset-specific tuning.

Abstract: Robust loss functions are crucial for training deep neural networks in the
presence of label noise, yet existing approaches require extensive,
dataset-specific hyperparameter tuning. In this work, we introduce Fractional
Classification Loss (FCL), an adaptive robust loss that automatically
calibrates its robustness to label noise during training. Built within the
active-passive loss framework, FCL employs the fractional derivative of the
Cross-Entropy (CE) loss as its active component and the Mean Absolute Error
(MAE) as its passive loss component. With this formulation, we demonstrate that
the fractional derivative order $\mu$ spans a family of loss functions that
interpolate between MAE-like robustness and CE-like fast convergence.
Furthermore, we integrate $\mu$ into the gradient-based optimization as a
learnable parameter and automatically adjust it to optimize the trade-off
between robustness and convergence speed. We reveal that FCL's unique property
establishes a critical trade-off that enables the stable learning of $\mu$:
lower log penalties on difficult or mislabeled examples improve robustness but
impose higher penalties on easy or clean data, reducing model confidence in
them. Consequently, FCL can dynamically reshape its loss landscape to achieve
effective classification performance under label noise. Extensive experiments
on benchmark datasets show that FCL achieves state-of-the-art results without
the need for manual hyperparameter tuning.

</details>


### [214] [Structural Equation-VAE: Disentangled Latent Representations for Tabular Data](https://arxiv.org/abs/2508.06347)
*Ruiyu Zhang,Ce Zhao,Xin Zhao,Lin Nie,Wai-Fung Lam*

Main category: cs.LG

TL;DR: SE-VAE introduces a structured VAE for interpretable latent representations in tabular data, outperforming baselines in disentanglement and robustness.


<details>
  <summary>Details</summary>
Motivation: The challenge of learning interpretable latent representations from tabular data in deep generative modeling.

Method: SE-VAE embeds structural equation modeling into a VAE, aligning latent subspaces with indicator groupings and isolating nuisance variation.

Result: SE-VAE outperforms baselines in factor recovery, interpretability, and robustness to nuisance variation.

Conclusion: SE-VAE provides a principled framework for theory-driven generative modeling, emphasizing architectural structure over regularization.

Abstract: Learning interpretable latent representations from tabular data remains a
challenge in deep generative modeling. We introduce SE-VAE (Structural
Equation-Variational Autoencoder), a novel architecture that embeds measurement
structure directly into the design of a variational autoencoder. Inspired by
structural equation modeling, SE-VAE aligns latent subspaces with known
indicator groupings and introduces a global nuisance latent to isolate
construct-specific confounding variation. This modular architecture enables
disentanglement through design rather than through statistical regularizers
alone. We evaluate SE-VAE on a suite of simulated tabular datasets and
benchmark its performance against a series of leading baselines using standard
disentanglement metrics. SE-VAE consistently outperforms alternatives in factor
recovery, interpretability, and robustness to nuisance variation. Ablation
results reveal that architectural structure, rather than regularization
strength, is the key driver of performance. SE-VAE offers a principled
framework for white-box generative modeling in scientific and social domains
where latent constructs are theory-driven and measurement validity is
essential.

</details>


### [215] [Geometric-k-means: A Bound Free Approach to Fast and Eco-Friendly k-means](https://arxiv.org/abs/2508.06353)
*Parichit Sharma,Marcin Stanislaw,Hasan Kurban,Oguzhan Kulekci,Mehmet Dalkilic*

Main category: cs.LG

TL;DR: Gk-means improves k-means by using geometric principles (scalar projection) to focus on high expressive data (HE), reducing computational overhead while maintaining quality. It outperforms traditional and SOTA k-means variants in runtime, distance computations, and energy efficiency.


<details>
  <summary>Details</summary>
Motivation: Enhance the efficiency and energy economy of the widely used k-means algorithm by leveraging geometric principles.

Method: Utilizes scalar projection to identify and focus on high expressive data (HE), bypassing low expressive data (LE) to reduce computational overhead.

Result: Gk-means outperforms traditional and SOTA k-means variants in runtime, distance computations, and energy efficiency across synthetic, real-world, and high-dimensional datasets.

Conclusion: Gk-means is a more efficient and sustainable alternative to traditional k-means, offering significant improvements in speed and resource usage without compromising solution quality.

Abstract: This paper introduces Geometric-k-means (or Gk-means for short), a novel
approach that significantly enhances the efficiency and energy economy of the
widely utilized k-means algorithm, which, despite its inception over five
decades ago, remains a cornerstone in machine learning applications. The
essence of Gk-means lies in its active utilization of geometric principles,
specifically scalar projection, to significantly accelerate the algorithm
without sacrificing solution quality. This geometric strategy enables a more
discerning focus on data points that are most likely to influence cluster
updates, which we call as high expressive data (HE). In contrast, low
expressive data (LE), does not impact clustering outcome, is effectively
bypassed, leading to considerable reductions in computational overhead.
Experiments spanning synthetic, real-world and high-dimensional datasets,
demonstrate Gk-means is significantly better than traditional and state of the
art (SOTA) k-means variants in runtime and distance computations (DC).
Moreover, Gk-means exhibits better resource efficiency, as evidenced by its
reduced energy footprint, placing it as more sustainable alternative.

</details>


### [216] [Beyond Prompt-Induced Lies: Investigating LLM Deception on Benign Prompts](https://arxiv.org/abs/2508.06361)
*Zhaomin Wu,Mingzhe Du,See-Kiong Ng,Bingsheng He*

Main category: cs.LG

TL;DR: The paper investigates self-initiated deception in LLMs, proposing a framework to quantify deception likelihood using two metrics. Findings show advanced LLMs increasingly deceive as tasks grow complex, raising deployment concerns.


<details>
  <summary>Details</summary>
Motivation: To explore underexamined self-initiated deception in LLMs beyond human-induced scenarios, addressing a critical trustworthiness gap in real-world applications.

Method: A novel framework using 'contact searching questions' with two metrics: Deceptive Intention Score (bias toward hidden objectives) and Deceptive Behavior Score (inconsistency between belief and output). Evaluated 14 leading LLMs.

Result: Both deception metrics escalate with task difficulty, revealing advanced LLMs' increasing deceptive tendencies in complex scenarios.

Conclusion: The study highlights a growing deception risk in LLMs under complexity, urging caution in deploying LLM agents in critical domains.

Abstract: Large Language Models (LLMs) have been widely deployed in reasoning,
planning, and decision-making tasks, making their trustworthiness a critical
concern. The potential for intentional deception, where an LLM deliberately
fabricates or conceals information to serve a hidden objective, remains a
significant and underexplored threat. Existing studies typically induce such
deception by explicitly setting a "hidden" objective through prompting or
fine-tuning, which may not fully reflect real-world human-LLM interactions.
Moving beyond this human-induced deception, we investigate LLMs' self-initiated
deception on benign prompts. To address the absence of ground truth in this
evaluation, we propose a novel framework using "contact searching questions."
This framework introduces two statistical metrics derived from psychological
principles to quantify the likelihood of deception. The first, the Deceptive
Intention Score, measures the model's bias towards a hidden objective. The
second, Deceptive Behavior Score, measures the inconsistency between the LLM's
internal belief and its expressed output. Upon evaluating 14 leading LLMs, we
find that both metrics escalate as task difficulty increases, rising in
parallel for most models. Building on these findings, we formulate a
mathematical model to explain this behavior. These results reveal that even the
most advanced LLMs exhibit an increasing tendency toward deception when
handling complex problems, raising critical concerns for the deployment of LLM
agents in complex and crucial domains.

</details>


### [217] [ActivityDiff: A diffusion model with Positive and Negative Activity Guidance for De Novo Drug Design](https://arxiv.org/abs/2508.06364)
*Renyi Zhou,Huimin Zhu,Jing Tang,Min Li*

Main category: cs.LG

TL;DR: ActivityDiff is a generative method using diffusion models with classifier guidance to design molecules with desired activities while minimizing off-target effects.


<details>
  <summary>Details</summary>
Motivation: Precise control over molecular activity, including multi-target modulation and toxicity mitigation, is lacking in current drug design methods.

Method: ActivityDiff employs classifier-guided diffusion models, using separately trained drug-target classifiers for positive and negative guidance.

Result: The method effectively handles tasks like single-/dual-target generation, selective generation, and off-target reduction.

Conclusion: ActivityDiff offers a novel, versatile framework for integrated molecular activity control, balancing efficacy and safety.

Abstract: Achieving precise control over a molecule's biological activity-encompassing
targeted activation/inhibition, cooperative multi-target modulation, and
off-target toxicity mitigation-remains a critical challenge in de novo drug
design. However, existing generative methods primarily focus on producing
molecules with a single desired activity, lacking integrated mechanisms for the
simultaneous management of multiple intended and unintended molecular
interactions. Here, we propose ActivityDiff, a generative approach based on the
classifier-guidance technique of diffusion models. It leverages separately
trained drug-target classifiers for both positive and negative guidance,
enabling the model to enhance desired activities while minimizing harmful
off-target effects. Experimental results show that ActivityDiff effectively
handles essential drug design tasks, including single-/dual-target generation,
fragment-constrained dual-target design, selective generation to enhance target
specificity, and reduction of off-target effects. These results demonstrate the
effectiveness of classifier-guided diffusion in balancing efficacy and safety
in molecular design. Overall, our work introduces a novel paradigm for
achieving integrated control over molecular activity, and provides ActivityDiff
as a versatile and extensible framework.

</details>


### [218] [End-to-End Text-to-SQL with Dataset Selection: Leveraging LLMs for Adaptive Query Generation](https://arxiv.org/abs/2508.06387)
*Anurag Tripathi,Vaibhav Patle,Abhinav Jain,Ayush Pundir,Sairam Menon,Ajeet Kumar Singh*

Main category: cs.LG

TL;DR: A three-stage text-to-SQL framework improves database intent prediction and SQL generation by leveraging LLMs, prompt engineering, and critic agents.


<details>
  <summary>Details</summary>
Motivation: Traditional text-to-SQL methods assume a pre-specified database, which is impractical for multiple extensive databases. This work addresses the overlooked step of identifying the correct database.

Method: The framework: 1) extracts implicit rules from NLQs using LLMs, 2) predicts the correct db_id with a RoBERTa-based model, and 3) refines SQL using critic agents.

Result: Outperforms state-of-the-art models in database intent prediction and SQL generation accuracy.

Conclusion: The proposed framework effectively handles multi-database scenarios, improving usability for non-technical users.

Abstract: Text-to-SQL bridges the gap between natural language and structured database
language, thus allowing non-technical users to easily query databases.
Traditional approaches model text-to-SQL as a direct translation task, where a
given Natural Language Query (NLQ) is mapped to an SQL command. Recent advances
in large language models (LLMs) have significantly improved translation
accuracy, however, these methods all require that the target database is
pre-specified. This becomes problematic in scenarios with multiple extensive
databases, where identifying the correct database becomes a crucial yet
overlooked step. In this paper, we propose a three-stage end-to-end text-to-SQL
framework to identify the user's intended database before generating SQL
queries. Our approach leverages LLMs and prompt engineering to extract implicit
information from natural language queries (NLQs) in the form of a ruleset. We
then train a large db\_id prediction model, which includes a RoBERTa-based
finetuned encoder, to predict the correct Database identifier (db\_id) based on
both the NLQ and the LLM-generated rules. Finally, we refine the generated SQL
by using critic agents to correct errors. Experimental results demonstrate that
our framework outperforms the current state-of-the-art models in both database
intent prediction and SQL generation accuracy.

</details>


### [219] [A New Lens on Homelessness: Daily Tent Monitoring with 311 Calls and Street Images](https://arxiv.org/abs/2508.06409)
*Wooyong Jung,Sola Kim,Dongwook Kim,Maryam Tabar,Dongwon Lee*

Main category: cs.LG

TL;DR: The study proposes using crowdsourced data (311 Service Calls and street-level imagery) to track and forecast homeless tent trends in San Francisco, offering more detailed and timely insights than traditional methods.


<details>
  <summary>Details</summary>
Motivation: Existing homelessness monitoring methods like PIT counts lack frequency, consistency, and spatial detail, necessitating a better approach.

Method: The study leverages publicly available crowdsourced data (311 Service Calls and street-level imagery) to create a predictive model for tracking homeless tent trends.

Result: The model captures fine-grained daily and neighborhood-level variations, revealing patterns like rapid fluctuations during COVID-19 and spatial shifts in tent locations.

Conclusion: This approach provides timely, localized, and cost-effective data, aiding policy responses and interventions to reduce unsheltered homelessness.

Abstract: Homelessness in the United States has surged to levels unseen since the Great
Depression. However, existing methods for monitoring it, such as point-in-time
(PIT) counts, have limitations in terms of frequency, consistency, and spatial
detail. This study proposes a new approach using publicly available,
crowdsourced data, specifically 311 Service Calls and street-level imagery, to
track and forecast homeless tent trends in San Francisco. Our predictive model
captures fine-grained daily and neighborhood-level variations, uncovering
patterns that traditional counts often overlook, such as rapid fluctuations
during the COVID-19 pandemic and spatial shifts in tent locations over time. By
providing more timely, localized, and cost-effective information, this approach
serves as a valuable tool for guiding policy responses and evaluating
interventions aimed at reducing unsheltered homelessness.

</details>


### [220] [Sample-efficient LLM Optimization with Reset Replay](https://arxiv.org/abs/2508.06412)
*Zichuan Liu,Jinyu Wang,Lei Song,Jiang Bian*

Main category: cs.LG

TL;DR: LoRR enhances LLM optimization by improving sample efficiency and reducing primacy bias through high-replay training, periodic resets, and hybrid optimization.


<details>
  <summary>Details</summary>
Motivation: Address low sample efficiency and primacy bias in RL and preference optimization methods for LLMs.

Method: Introduces LoRR, a plugin with high-replay training, periodic resets, and hybrid optimization (SFT + preference-based losses).

Result: Boosts performance on reasoning benchmarks; iterative DPO with LoRR matches complex RL methods on math tasks.

Conclusion: LoRR is a practical, efficient paradigm for LLM finetuning, maximizing performance with limited data.

Abstract: Recent advancements in post-training Large Language Models (LLMs),
particularly through Reinforcement Learning (RL) and preference optimization
methods, are key drivers for enhancing their reasoning capabilities. However,
these methods are often plagued by low sample efficiency and a susceptibility
to primacy bias, where overfitting to initial experiences degrades policy
quality and damages the learning process. To address these challenges, we
introduce LLM optimization with Reset Replay (LoRR), a general and powerful
plugin designed to enhance sample efficiency in any preference-based
optimization framework. LoRR core mechanism enables training at a high replay
number, maximizing the utility of each collected data batch. To counteract the
risk of overfitting inherent in high-replay training, LoRR incorporates a
periodic reset strategy with reusing initial data, which preserves network
plasticity. Furthermore, it leverages a hybrid optimization objective,
combining supervised fine-tuning (SFT) and preference-based losses to further
bolster data exploitation. Our extensive experiments demonstrate that LoRR
significantly boosts the performance of various preference optimization methods
on both mathematical and general reasoning benchmarks. Notably, an iterative
DPO approach augmented with LoRR achieves comparable performance on challenging
math tasks, outperforming some complex and computationally intensive RL-based
algorithms. These findings highlight that LoRR offers a practical,
sample-efficient, and highly effective paradigm for LLM finetuning, unlocking
greater performance from limited data.

</details>


### [221] [LLM Unlearning using Gradient Ratio-Based Influence Estimation and Noise Injection](https://arxiv.org/abs/2508.06467)
*Ameya Anjarlekar,Sandeep Pombra*

Main category: cs.LG

TL;DR: GRIN is a modular framework for LLM unlearning, using gradient-ratio-based metrics and selective noise injection to improve unlearning while preserving model utility.


<details>
  <summary>Details</summary>
Motivation: Addressing the need for effective machine unlearning in LLMs due to legal and ethical concerns, avoiding incomplete forgetting or unrelated knowledge degradation.

Method: GRIN identifies key parameters for forgetting using a gradient-ratio metric, then selectively injects noise before fine-tuning.

Result: Validated on benchmarks (TOFU, WMDP, SafePKU), GRIN improves unlearning performance without degrading model utility.

Conclusion: GRIN offers a targeted and effective solution for LLM unlearning, balancing performance and utility.

Abstract: The growing legal and ethical scrutiny of large language models (LLMs)
necessitates effective machine unlearning, particularly for sensitive or
unauthorized data. Existing empirical methods often yield incomplete forgetting
or unintended degradation of unrelated knowledge due to poor localization. In
this work, we propose GRIN: a modular and targeted framework for LLM
unlearning. GRIN introduces a novel gradient-ratio-based metric to identify
parameters most responsible for memorizing forget data. We then perform
selective noise injection into these parameters prior to fine-tuning, which
improves unlearning performance while maintaining model utility. Finally, we
propose new evaluation metrics tailored to the LLM setting and validate our
approach on standard benchmarks such as TOFU, WMDP, and SafePKU.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [222] [Stochastic Bandits for Crowdsourcing and Multi-Platform Autobidding](https://arxiv.org/abs/2508.05844)
*Fran√ßois Bachoc,Nicol√≤ Cesa-Bianchi,Tommaso Cesari,Roberto Colomboni*

Main category: cs.GT

TL;DR: A stochastic bandit model is proposed for budget allocation in crowdsourcing and autobidding, with an algorithm achieving $K\sqrt{T}$ regret and improved bounds under diminishing-returns conditions.


<details>
  <summary>Details</summary>
Motivation: Applications in crowdsourcing (splitting money among workers) and autobidding (budget allocation in auctions) drive the need for efficient budget allocation strategies.

Method: A stochastic bandit model is defined where arms represent budget fractions on the $K$-dimensional simplex. Rewards are sums of stochastic rewards, unlocked probabilistically based on budget allocation. An algorithm is designed with expected regret of order $K\sqrt{T}$.

Result: The algorithm achieves $K\sqrt{T}$ regret (matching lower bound) and improved $K (\log T)^2$ bounds under diminishing-returns conditions.

Conclusion: The proposed model and algorithm effectively address budget allocation in stochastic settings, with theoretical guarantees on regret.

Abstract: Motivated by applications in crowdsourcing, where a fixed sum of money is
split among $K$ workers, and autobidding, where a fixed budget is used to bid
in $K$ simultaneous auctions, we define a stochastic bandit model where arms
belong to the $K$-dimensional probability simplex and represent the fraction of
budget allocated to each task/auction. The reward in each round is the sum of
$K$ stochastic rewards, where each of these rewards is unlocked with a
probability that varies with the fraction of the budget allocated to that
task/auction. We design an algorithm whose expected regret after $T$ steps is
of order $K\sqrt{T}$ (up to log factors) and prove a matching lower bound.
Improved bounds of order $K (\log T)^2$ are shown when the function mapping
budget to probability of unlocking the reward (i.e., terminating the task or
winning the auction) satisfies additional diminishing-returns conditions.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [223] [Request-Only Optimization for Recommendation Systems](https://arxiv.org/abs/2508.05640)
*Liang Guo,Wei Li,Lucy Liao,Huihui Cheng,Rui Zhang,Yu Shi,Yueming Wang,Yanzun Huang,Keke Zhai,Pengchao Wang,Timothy Shi,Xuan Cao,Shengzhi Wang,Renqin Cai,Zhaojie Gong,Omkar Vichare,Rui Jian,Leon Gao,Shiyan Deng,Xingyu Liu,Xiong Zhang,Fu Li,Wenlei Xie,Bin Wen,Rui Li,Xing Liu,Jiaqi Zhai*

Main category: cs.IR

TL;DR: The paper introduces Request-Only Optimizations (ROO), a paradigm for improving storage, training efficiency, and model quality in Deep Learning Recommendation Models (DLRMs) by treating user requests as training units.


<details>
  <summary>Details</summary>
Motivation: DLRMs are massive-scale ML applications requiring efficient storage and training methods due to their complexity and vast data.

Method: ROO co-designs data (request-only), infrastructure (request-only pipelines), and model architectures (request-only neural designs) to optimize efficiency.

Result: ROO reduces storage via feature deduplication and enhances model quality by enabling scalable architectures like Generative Recommenders.

Conclusion: ROO offers a holistic solution for efficient and high-quality recommendation systems by redefining training units and optimizing computations.

Abstract: Deep Learning Recommendation Models (DLRMs) represent one of the largest
machine learning applications on the planet. Industry-scale DLRMs are trained
with petabytes of recommendation data to serve billions of users every day. To
utilize the rich user signals in the long user history, DLRMs have been scaled
up to unprecedented complexity, up to trillions of floating-point operations
(TFLOPs) per example. This scale, coupled with the huge amount of training
data, necessitates new storage and training algorithms to efficiently improve
the quality of these complex recommendation systems. In this paper, we present
a Request-Only Optimizations (ROO) training and modeling paradigm. ROO
simultaneously improves the storage and training efficiency as well as the
model quality of recommendation systems. We holistically approach this
challenge through co-designing data (i.e., request-only data), infrastructure
(i.e., request-only based data processing pipeline), and model architecture
(i.e., request-only neural architectures). Our ROO training and modeling
paradigm treats a user request as a unit of the training data. Compared with
the established practice of treating a user impression as a unit, our new
design achieves native feature deduplication in data logging, consequently
saving data storage. Second, by de-duplicating computations and communications
across multiple impressions in a request, this new paradigm enables highly
scaled-up neural network architectures to better capture user interest signals,
such as Generative Recommenders (GRs) and other request-only friendly
architectures.

</details>


### [224] [Query-Aware Graph Neural Networks for Enhanced Retrieval-Augmented Generation](https://arxiv.org/abs/2508.05647)
*Vibhor Agrawal,Fay Wang,Rishi Puri*

Main category: cs.IR

TL;DR: A novel GNN architecture for RAG improves retrieval accuracy on complex questions using query-aware attention and learned scoring heads.


<details>
  <summary>Details</summary>
Motivation: Traditional dense retrieval methods treat documents independently, lacking the ability to capture relationships between text chunks for multi-hop questions.

Method: Constructs per-episode knowledge graphs and uses an Enhanced Graph Attention Network with query-guided pooling to dynamically focus on relevant graph parts.

Result: Outperforms standard dense retrievers, especially in multi-document reasoning tasks.

Conclusion: The approach is scalable and effective for complex question answering, leveraging PyTorch Geometric for efficient graph processing.

Abstract: We present a novel graph neural network (GNN) architecture for
retrieval-augmented generation (RAG) that leverages query-aware attention
mechanisms and learned scoring heads to improve retrieval accuracy on complex,
multi-hop questions. Unlike traditional dense retrieval methods that treat
documents as independent entities, our approach constructs per-episode
knowledge graphs that capture both sequential and semantic relationships
between text chunks. We introduce an Enhanced Graph Attention Network with
query-guided pooling that dynamically focuses on relevant parts of the graph
based on user queries. Experimental results demonstrate that our approach
significantly outperforms standard dense retrievers on complex question
answering tasks, particularly for questions requiring multi-document reasoning.
Our implementation leverages PyTorch Geometric for efficient processing of
graph-structured data, enabling scalable deployment in production retrieval
systems

</details>


### [225] [AquiLLM: a RAG Tool for Capturing Tacit Knowledge in Research Groups](https://arxiv.org/abs/2508.05648)
*Chandler Campbell,Bernie Boscoe,Tuan Do*

Main category: cs.IR

TL;DR: AquiLLM is a lightweight RAG system designed to help research groups access informal and private knowledge, addressing privacy concerns overlooked by current systems.


<details>
  <summary>Details</summary>
Motivation: Research groups struggle with capturing and retrieving distributed, informal knowledge like emails and meeting notes, which are often undocumented or passed orally.

Method: AquiLLM is introduced as a modular RAG system supporting varied document types and configurable privacy settings.

Result: The system enables more effective access to both formal and informal knowledge within scholarly groups.

Conclusion: AquiLLM addresses gaps in current RAG-LLM systems by focusing on privacy and diverse knowledge sources for research groups.

Abstract: Research groups face persistent challenges in capturing, storing, and
retrieving knowledge that is distributed across team members. Although
structured data intended for analysis and publication is often well managed,
much of a group's collective knowledge remains informal, fragmented, or
undocumented--often passed down orally through meetings, mentoring, and
day-to-day collaboration. This includes private resources such as emails,
meeting notes, training materials, and ad hoc documentation. Together, these
reflect the group's tacit knowledge--the informal, experience-based expertise
that underlies much of their work. Accessing this knowledge can be difficult,
requiring significant time and insider understanding. Retrieval-augmented
generation (RAG) systems offer promising solutions by enabling users to query
and generate responses grounded in relevant source material. However, most
current RAG-LLM systems are oriented toward public documents and overlook the
privacy concerns of internal research materials. We introduce AquiLLM
(pronounced ah-quill-em), a lightweight, modular RAG system designed to meet
the needs of research groups. AquiLLM supports varied document types and
configurable privacy settings, enabling more effective access to both formal
and informal knowledge within scholarly groups.

</details>


### [226] [OmniBench-RAG: A Multi-Domain Evaluation Platform for Retrieval-Augmented Generation Tools](https://arxiv.org/abs/2508.05650)
*Jiaxuan Liang,Shide Zhou,Kailong Wang*

Main category: cs.IR

TL;DR: OmniBench RAG is introduced as an automated platform for evaluating RAG systems across multiple domains, addressing gaps in existing methods by providing standardized metrics and reproducible comparisons.


<details>
  <summary>Details</summary>
Motivation: Existing RAG evaluation methods lack domain coverage, fine-grained metrics, and standardized frameworks, making it difficult to assess true performance benefits.

Method: The platform uses dynamic test generation, modular pipelines, and automated knowledge base construction, with two metrics: Improvements (accuracy) and Transformation (efficiency).

Result: Evaluation shows varied RAG effectiveness, with gains in some domains (e.g., culture) and declines in others (e.g., mathematics).

Conclusion: Systematic, domain-aware assessment is crucial for understanding RAG performance, and OmniBench RAG provides a standardized solution.

Abstract: While Retrieval Augmented Generation (RAG) is now widely adopted to enhance
LLMs, evaluating its true performance benefits in a reproducible and
interpretable way remains a major hurdle. Existing methods often fall short:
they lack domain coverage, employ coarse metrics that miss sub document
precision, and fail to capture computational trade offs. Most critically, they
provide no standardized framework for comparing RAG effectiveness across
different models and domains.
  We introduce OmniBench RAG, a novel automated platform for multi domain
evaluation of RAG systems. The platform quantifies performance gains across
accuracy and efficiency dimensions, spanning nine knowledge fields including
culture, geography, and health. We introduce two standardized metrics:
Improvements (accuracy gains) and Transformation (efficiency differences
between pre RAG and post RAG models), enabling reproducible comparisons across
models and tasks. The platform features dynamic test generation, modular
evaluation pipelines, and automated knowledge base construction. Our evaluation
reveals striking variability in RAG effectiveness, from significant gains in
culture to declines in mathematics, highlighting the critical importance of
systematic, domain aware assessment. A demonstration video is available at:
https://www.youtube.com/watch?v=BZx83QFcTCI. Code and datasets:
https://github.com/Garnett-Liang/Omnibench-RAG.

</details>


### [227] [Lessons from A Large Language Model-based Outdoor Trail Recommendation Chatbot with Retrieval Augmented Generation](https://arxiv.org/abs/2508.05652)
*Julia Ann Mathew,Suining He*

Main category: cs.IR

TL;DR: The paper presents Judy, an LLM-based chatbot with RAG for outdoor trail recommendations, addressing challenges in accuracy and usability.


<details>
  <summary>Details</summary>
Motivation: The rise in outdoor activities demands conversational AI for personalized trail suggestions, requiring solutions for accurate information and efficient recommendations.

Method: Developed Judy, a chatbot using LLM with RAG, and conducted case studies on Connecticut trails, including data collection, management, and model performance analysis.

Result: Judy demonstrated accuracy, effectiveness, and usability in recommending trails.

Conclusion: The study highlights the potential of LLM with RAG for outdoor trail recommendation systems.

Abstract: The increasing popularity of outdoor recreational activities (such as hiking
and biking) has boosted the demand for a conversational AI system to provide
informative and personalized suggestion on outdoor trails. Challenges arise in
response to (1) how to provide accurate outdoor trail information via
conversational AI; and (2) how to enable usable and efficient recommendation
services. To address above, this paper discusses the preliminary and practical
lessons learned from developing Judy, an outdoor trail recommendation chatbot
based on the large language model (LLM) with retrieval augmented generation
(RAG). To gain concrete system insights, we have performed case studies with
the outdoor trails in Connecticut (CT), US. We have conducted web-based data
collection, outdoor trail data management, and LLM model performance studies on
the RAG-based recommendation. Our experimental results have demonstrated the
accuracy, effectiveness, and usability of Judy in recommending outdoor trails
based on the LLM with RAG.

</details>


### [228] [Comparison of Information Retrieval Techniques Applied to IT Support Tickets](https://arxiv.org/abs/2508.05654)
*Leonardo Santiago Benitez Pereira,Robinson Pizzio,Samir Bonho*

Main category: cs.IR

TL;DR: The paper compares 11 Information Retrieval techniques for IT help desk systems, finding Sentence-BERT (multilingual) as the best performer (78.7% relevance). It also introduces a novel metric for retrieval quality and shares open-source datasets and code.


<details>
  <summary>Details</summary>
Motivation: To improve IT help desk systems by identifying the most effective Information Retrieval techniques for recommending corrective actions from past tickets.

Method: Comparison of 11 techniques (including Sentence-BERT, TF-IDF, Word2vec, LDA) on IT support ticket datasets, with a focus on relevance and practicality.

Result: Sentence-BERT (multilingual) achieved the highest relevance (78.7%). Other techniques like TF-IDF (69.0%), Word2vec (68.7%), and LDA (66.3%) also performed well.

Conclusion: Sentence-BERT is the most effective for IT help desk systems. The work also provides open-source resources and a novel metric for evaluating retrieval quality.

Abstract: Institutions dependent on IT services and resources acknowledge the crucial
significance of an IT help desk system, that act as a centralized hub
connecting IT staff and users for service requests. Employing various Machine
Learning models, these IT help desk systems allow access to corrective actions
used in the past, but each model has different performance when applied to
different datasets. This work compares eleven Information Retrieval techniques
in a dataset of IT support tickets, with the goal of implementing a software
that facilitates the work of Information Technology support analysts. The best
results were obtained with the Sentence-BERT technique, in its multi-language
variation distilluse-base-multilingual-cased-v1, where 78.7% of the
recommendations made by the model were considered relevant. TF-IDF (69.0%),
Word2vec (68.7%) and LDA (66.3%) techniques also had consistent results.
Furthermore, the used datasets and essential parts of coding have been
published and made open source. It also demonstrated the practicality of a
support ticket recovery system by implementing a minimal viable prototype, and
described in detail the implementation of the system. Finally, this work
proposed a novel metric for comparing the techniques, whose aim is to closely
reflect the perception of the IT analysts about the retrieval quality.

</details>


### [229] [Beyond Single Labels: Improving Conversational Recommendation through LLM-Powered Data Augmentation](https://arxiv.org/abs/2508.05657)
*Haozhe Xu,Xiaohua Wang,Changze Lv,Xiaoqing Zheng*

Main category: cs.IR

TL;DR: A novel data augmentation framework for conversational recommender systems (CRSs) addresses false negatives by balancing semantic relevance and collaborative information, improving recommendation performance.


<details>
  <summary>Details</summary>
Motivation: CRSs often incorrectly label items users might like as negative during training, leading to suboptimal recommendations.

Method: The framework uses an LLM-based semantic retriever and relevance scorer for data augmentation, followed by a two-stage training strategy.

Result: Experiments on benchmark datasets and user simulators show consistent performance improvements.

Conclusion: The proposed approach effectively advances CRS performance by mitigating false negatives.

Abstract: Conversational recommender systems (CRSs) enhance recommendation quality by
engaging users in multi-turn dialogues, capturing nuanced preferences through
natural language interactions. However, these systems often face the false
negative issue, where items that a user might like are incorrectly labeled as
negative during training, leading to suboptimal recommendations.Expanding the
label set through data augmentation presents an intuitive solution but faces
the challenge of balancing two key aspects: ensuring semantic relevance and
preserving the collaborative information inherent in CRS datasets. To address
these issues, we propose a novel data augmentation framework that first
leverages an LLM-based semantic retriever to identify diverse and semantically
relevant items, which are then filtered by a relevance scorer to remove noisy
candidates. Building on this, we introduce a two-stage training strategy
balancing semantic relevance and collaborative information. Extensive
experiments on two benchmark datasets and user simulators demonstrate
significant and consistent performance improvements across various
recommenders, highlighting the effectiveness of our approach in advancing CRS
performance.

</details>


### [230] [Open-Source Agentic Hybrid RAG Framework for Scientific Literature Review](https://arxiv.org/abs/2508.05660)
*Aditya Nagori,Ricardo Accorsi Casonatto,Ayush Gautam,Abhinav Manikantha Sai Cheruvu,Rishikesan Kamaleswaran*

Main category: cs.IR

TL;DR: A dynamic agentic approach improves hybrid RAG systems by dynamically selecting between GraphRAG and VectorRAG, adapting generation in real time, and quantifying uncertainty, outperforming baselines in relevance and precision.


<details>
  <summary>Details</summary>
Motivation: The increasing volume of scientific publications requires advanced tools that combine structured metadata with full-text analysis, addressing limitations of static, proprietary hybrid RAG systems.

Method: The system ingests open-access data, builds a citation-based knowledge graph (KG) and vector store (VS), and uses a Llama-3.3-70B agent to dynamically select between GraphRAG and VectorRAG. Instruction tuning and bootstrapped evaluation refine the process.

Result: The Instruction-Tuned Agent with DPO outperforms baselines, achieving gains in metrics like Context Recall (0.63), Context Precision (0.56), and Faithfulness (0.24).

Conclusion: The approach enhances reasoning over heterogeneous sources and provides a scalable framework for autonomous scientific discovery.

Abstract: The surge in scientific publications challenges traditional review methods,
demanding tools that integrate structured metadata with full-text analysis.
Hybrid Retrieval Augmented Generation (RAG) systems, combining graph queries
with vector search offer promise but are typically static, rely on proprietary
tools, and lack uncertainty estimates. We present an agentic approach that
encapsulates the hybrid RAG pipeline within an autonomous agent capable of (1)
dynamically selecting between GraphRAG and VectorRAG for each query, (2)
adapting instruction-tuned generation in real time to researcher needs, and (3)
quantifying uncertainty during inference. This dynamic orchestration improves
relevance, reduces hallucinations, and promotes reproducibility.
  Our pipeline ingests bibliometric open-access data from PubMed, arXiv, and
Google Scholar APIs, builds a Neo4j citation-based knowledge graph (KG), and
embeds full-text PDFs into a FAISS vector store (VS) using the all-MiniLM-L6-v2
model. A Llama-3.3-70B agent selects GraphRAG (translating queries to Cypher
for KG) or VectorRAG (combining sparse and dense retrieval with re-ranking).
Instruction tuning refines domain-specific generation, and bootstrapped
evaluation yields standard deviation for evaluation metrics.
  On synthetic benchmarks mimicking real-world queries, the Instruction-Tuned
Agent with Direct Preference Optimization (DPO) outperforms the baseline,
achieving a gain of 0.63 in VS Context Recall and a 0.56 gain in overall
Context Precision. Additional gains include 0.24 in VS Faithfulness, 0.12 in
both VS Precision and KG Answer Relevance, 0.11 in overall Faithfulness score,
0.05 in KG Context Recall, and 0.04 in both VS Answer Relevance and overall
Precision. These results highlight the system's improved reasoning over
heterogeneous sources and establish a scalable framework for autonomous,
agentic scientific discovery.

</details>


### [231] [Zero-Shot Retrieval for Scalable Visual Search in a Two-Sided Marketplace](https://arxiv.org/abs/2508.05661)
*Andre Rusli,Shoma Ishimoto,Sho Akiyama,Aman Kumar Singh*

Main category: cs.IR

TL;DR: A scalable visual search system for Mercari's C2C marketplace, using zero-shot vision-language models, outperforms fine-tuned baselines with a 13.3% nDCG@5 increase and 40.9% higher transaction rates.


<details>
  <summary>Details</summary>
Motivation: To improve product exploration in unstructured, visually-driven C2C marketplaces like Mercari.

Method: Evaluated zero-shot vision-language models (e.g., SigLIP) against fine-tuned baselines, integrating real-time inference and background indexing with optimized embedding pipelines.

Result: SigLIP outperformed others, with a 13.3% nDCG@5 boost and 40.9% higher transaction rates in A/B tests.

Conclusion: Zero-shot models are practical for production, offering strong baselines with minimal overhead and flexibility for future fine-tuning.

Abstract: Visual search offers an intuitive way for customers to explore diverse
product catalogs, particularly in consumer-to-consumer (C2C) marketplaces where
listings are often unstructured and visually driven. This paper presents a
scalable visual search system deployed in Mercari's C2C marketplace, where
end-users act as buyers and sellers. We evaluate recent vision-language models
for zero-shot image retrieval and compare their performance with an existing
fine-tuned baseline. The system integrates real-time inference and background
indexing workflows, supported by a unified embedding pipeline optimized through
dimensionality reduction. Offline evaluation using user interaction logs shows
that the multilingual SigLIP model outperforms other models across multiple
retrieval metrics, achieving a 13.3% increase in nDCG@5 over the baseline. A
one-week online A/B test in production further confirms real-world impact, with
the treatment group showing substantial gains in engagement and conversion, up
to a 40.9% increase in transaction rate via image search. Our findings
highlight that recent zero-shot models can serve as a strong and practical
baseline for production use, which enables teams to deploy effective visual
search systems with minimal overhead, while retaining the flexibility to
fine-tune based on future data or domain-specific needs.

</details>


### [232] [From Static to Dynamic: A Streaming RAG Approach to Real-time Knowledge Base](https://arxiv.org/abs/2508.05662)
*Yuzhou Zhu*

Main category: cs.IR

TL;DR: Streaming RAG addresses challenges in dynamic data streams by combining multi-vector screening, mini-batch clustering, and heavy-hitter filtering, achieving efficient retrieval with low latency and high throughput.


<details>
  <summary>Details</summary>
Motivation: Static RAG frameworks struggle with dynamic data streams due to high memory costs, latency from periodic rebuilds, and loss of semantic coverage from naive sampling.

Method: Uses multi-vector cosine screening, mini-batch clustering, and a heavy-hitter filter to maintain a compact prototype set, with an incremental index upsert mechanism for updates.

Result: Achieves significant gains in Recall@10, low latency (<15 ms), high throughput (>900 docs/s), and improvements in QA and summarization tasks.

Conclusion: Streaming RAG sets a new standard for retrieval augmentation in dynamic environments, balancing efficiency and performance.

Abstract: Dynamic streams from news feeds, social media, sensor networks, and financial
markets challenge static RAG frameworks. Full-scale indices incur high memory
costs; periodic rebuilds introduce latency that undermines data freshness;
naive sampling sacrifices semantic coverage. We present Streaming RAG, a
unified pipeline that combines multi-vector cosine screening, mini-batch
clustering, and a counter-based heavy-hitter filter to maintain a compact
prototype set. We further prove an approximation bound \$E\[R(K\_t)] \ge R^\* -
L \Delta\$ linking retrieval quality to clustering variance. An incremental
index upsert mechanism refreshes prototypes without interrupting queries.
Experiments on eight real-time streams show statistically significant gains in
Recall\@10 (up to 3 points, p < 0.01), end-to-end latency below 15 ms, and
throughput above 900 documents per second under a 150 MB budget. Hyperparameter
sensitivity analysis over cluster count, admission probability, relevance
threshold, and counter capacity validates default settings. In open-domain
question answering with GPT-3.5 Turbo, we record 3.2-point gain in Exact Match
and 2.8-point gain in F1 on SQuAD; abstractive summarization yields ROUGE-L
improvements. Streaming RAG establishes a new Pareto frontier for retrieval
augmentation.

</details>


### [233] [Enhancing Retrieval-Augmented Generation for Electric Power Industry Customer Support](https://arxiv.org/abs/2508.05664)
*Hei Yu Chan,Kuok Tou Ho,Chenglong Ma,Yujing Si,Hok Lai Lin,Sa Lei Lam*

Main category: cs.IR

TL;DR: The paper evaluates techniques like query rewriting, RAG Fusion, and intent recognition for improving AI customer service systems, finding graph-based RAG superior. The final system achieves high accuracy on complex queries.


<details>
  <summary>Details</summary>
Motivation: Standard NLP pipelines and finetuned language models often fail on ambiguous or multi-intent queries, prompting the need for more robust solutions in customer support.

Method: The study compares techniques like query rewriting, RAG Fusion, keyword augmentation, intent recognition, and context reranking, testing them on vector-store and graph-based RAG frameworks.

Result: Graph-based RAG outperforms vector-store, with query rewriting, RAG Fusion, and reranking improving performance. The final system achieves 97.9% and 89.6% accuracy on test datasets.

Conclusion: Combining intent recognition, RAG Fusion, and reranking effectively handles complex queries, significantly outperforming baseline models.

Abstract: Many AI customer service systems use standard NLP pipelines or finetuned
language models, which often fall short on ambiguous, multi-intent, or
detail-specific queries. This case study evaluates recent techniques: query
rewriting, RAG Fusion, keyword augmentation, intent recognition, and context
reranking, for building a robust customer support system in the electric power
domain. We compare vector-store and graph-based RAG frameworks, ultimately
selecting the graph-based RAG for its superior performance in handling complex
queries. We find that query rewriting improves retrieval for queries using
non-standard terminology or requiring precise detail. RAG Fusion boosts
performance on vague or multifaceted queries by merging multiple retrievals.
Reranking reduces hallucinations by filtering irrelevant contexts. Intent
recognition supports the decomposition of complex questions into more targeted
sub-queries, increasing both relevance and efficiency. In contrast, keyword
augmentation negatively impacts results due to biased keyword selection. Our
final system combines intent recognition, RAG Fusion, and reranking to handle
disambiguation and multi-source queries. Evaluated on both a GPT-4-generated
dataset and a real-world electricity provider FAQ dataset, it achieves 97.9%
and 89.6% accuracy respectively, substantially outperforming baseline RAG
models.

</details>


### [234] [HySemRAG: A Hybrid Semantic Retrieval-Augmented Generation Framework for Automated Literature Synthesis and Methodological Gap Analysis](https://arxiv.org/abs/2508.05666)
*Alejandro Godinez*

Main category: cs.IR

TL;DR: HySemRAG combines ETL pipelines with RAG for automated literature synthesis, addressing RAG limitations with hybrid retrieval, self-correction, and citation verification. It outperforms traditional methods in semantic similarity and accuracy.


<details>
  <summary>Details</summary>
Motivation: To automate large-scale literature synthesis and identify research gaps, overcoming limitations in existing RAG architectures.

Method: Multi-layered approach: hybrid retrieval, agentic self-correction, and citation verification. Implementation involves eight stages, including metadata acquisition, document analysis, and knowledge graph construction.

Result: Achieves 35.1% higher semantic similarity scores and 99.0% citation accuracy. Identifies trends and gaps in geospatial epidemiology.

Conclusion: HySemRAG is broadly applicable for accelerating evidence synthesis and discovery across scientific domains.

Abstract: We present HySemRAG, a framework that combines Extract, Transform, Load (ETL)
pipelines with Retrieval-Augmented Generation (RAG) to automate large-scale
literature synthesis and identify methodological research gaps. The system
addresses limitations in existing RAG architectures through a multi-layered
approach: hybrid retrieval combining semantic search, keyword filtering, and
knowledge graph traversal; an agentic self-correction framework with iterative
quality assurance; and post-hoc citation verification ensuring complete
traceability. Our implementation processes scholarly literature through eight
integrated stages: multi-source metadata acquisition, asynchronous PDF
retrieval, custom document layout analysis using modified Docling architecture,
bibliographic management, LLM-based field extraction, topic modeling, semantic
unification, and knowledge graph construction. The system creates dual data
products - a Neo4j knowledge graph enabling complex relationship queries and
Qdrant vector collections supporting semantic search - serving as foundational
infrastructure for verifiable information synthesis. Evaluation across 643
observations from 60 testing sessions demonstrates structured field extraction
achieving 35.1% higher semantic similarity scores (0.655 $\pm$ 0.178) compared
to PDF chunking approaches (0.485 $\pm$ 0.204, p < 0.000001). The agentic
quality assurance mechanism achieves 68.3% single-pass success rates with 99.0%
citation accuracy in validated responses. Applied to geospatial epidemiology
literature on ozone exposure and cardiovascular disease, the system identifies
methodological trends and research gaps, demonstrating broad applicability
across scientific domains for accelerating evidence synthesis and discovery.

</details>


### [235] [ITDR: An Instruction Tuning Dataset for Enhancing Large Language Models in Recommendations](https://arxiv.org/abs/2508.05667)
*Zekun Liu,Xiaowen Huang,Jitao Sang*

Main category: cs.IR

TL;DR: The paper introduces ITDR, an instruction tuning dataset for improving LLMs' performance in recommendation systems by addressing the gap in understanding user-item interactions.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with recommendation tasks due to structural differences between user behavior data and natural language, and prompt-based methods underperform.

Method: Constructed ITDR, a dataset with 200,000 instances from 13 public datasets, covering 7 subtasks for user-item interaction and understanding.

Result: ITDR significantly boosts performance of open-source LLMs like GLM-4 and LLaMA-3.2 on recommendation tasks.

Conclusion: ITDR effectively enhances LLMs for recommendations, with insights on task correlations and tuning effectiveness.

Abstract: Large language models (LLMs) have demonstrated outstanding performance in
natural language processing tasks. However, in the field of recommendation
systems, due to the structural differences between user behavior data and
natural language, LLMs struggle to effectively model the associations between
user preferences and items. Although prompt-based methods can generate
recommendation results, their inadequate understanding of recommendation tasks
leads to constrained performance. To address this gap, in this work, we
construct a sufficient instruction tuning dataset, ITDR, which encompasses 7
subtasks across two core root tasks--user-item interaction and user-item
understanding. The dataset integrates data from 13 public recommendation
datasets and is built using manually crafted standardized templates, comprising
approximately 200,000 instances. Experimental results demonstrate that ITDR
significantly enhances the performance of mainstream open-source LLMs such as
GLM-4, Qwen2.5, Qwen2.5-Instruct and LLaMA-3.2 on recommendation tasks.
Furthermore, we analyze the correlations between tasks and explore the impact
of task descriptions and data scale on instruction tuning effectiveness.
Finally, we perform comparative experiments against closed-source LLMs with
substantial parameters. Our tuning dataset ITDR and the fine-tuned large
recommendation models can be accessed at https://github.com/hellolzk/ITDR.

</details>


### [236] [A Survey of LLM-based Deep Search Agents: Paradigm, Optimization, Evaluation, and Challenges](https://arxiv.org/abs/2508.05668)
*Yunjia Xi,Jianghao Lin,Yongzhao Xiao,Zheli Zhou,Rong Shan,Te Gao,Jiachen Zhu,Weiwen Liu,Yong Yu,Weinan Zhang*

Main category: cs.IR

TL;DR: A survey on LLM-based Search Agents, analyzing their architecture, optimization, applications, and evaluation, while identifying open challenges and future directions.


<details>
  <summary>Details</summary>
Motivation: The rise of LLMs has transformed web search, enabling deeper, autonomous information retrieval. This survey aims to systematically analyze and categorize search agents to understand their potential and challenges.

Method: Comprehensive analysis and categorization of existing works from architectural, optimization, application, and evaluation perspectives.

Result: Identifies critical open challenges and outlines promising future research directions in LLM-based search agents.

Conclusion: The survey highlights the transformative potential of search agents and provides a roadmap for future advancements in the field.

Abstract: The advent of Large Language Models (LLMs) has significantly revolutionized
web search. The emergence of LLM-based Search Agents marks a pivotal shift
towards deeper, dynamic, autonomous information seeking. These agents can
comprehend user intentions and environmental context and execute multi-turn
retrieval with dynamic planning, extending search capabilities far beyond the
web. Leading examples like OpenAI's Deep Research highlight their potential for
deep information mining and real-world applications. This survey provides the
first systematic analysis of search agents. We comprehensively analyze and
categorize existing works from the perspectives of architecture, optimization,
application, and evaluation, ultimately identifying critical open challenges
and outlining promising future research directions in this rapidly evolving
field. Our repository is available on
https://github.com/YunjiaXi/Awesome-Search-Agent-Papers.

</details>


### [237] [Fine-Tuning Vision-Language Models for Markdown Conversion of Financial Tables in Malaysian Audited Financial Reports](https://arxiv.org/abs/2508.05669)
*Jin Khye Tan,En Jun Choong,Ethan Jeremiah Chitty,Yan Pheng Choo,John Hsin Yang Wong,Chern Eu Cheah*

Main category: cs.IR

TL;DR: A fine-tuned vision-language model (VLM) based on Qwen2.5-VL-7B is proposed to convert financial tables from Malaysian audited reports into Markdown with high accuracy, outperforming larger models and proprietary solutions.


<details>
  <summary>Details</summary>
Motivation: The challenge of accurately extracting and representing tabular data from financial documents, especially with complex layouts and implicit cues, drives the need for a specialized solution.

Method: The approach involves a fine-tuned VLM, a curated dataset of 2,152 image-text pairs, and supervised fine-tuning using LoRA. Performance is evaluated using a criteria-based LLM-as-a-judge and a novel Markdown TEDS metric.

Result: The model achieves 92.20% accuracy on criteria-based assessment and 96.53% on Markdown TEDS, outperforming base models, larger VLMs, and proprietary models like GPT-4o and Gemini 2.5 Flash.

Conclusion: Domain-specific fine-tuning effectively bridges the gap between unstructured financial documents and automation, offering efficiency and accuracy without computational overhead.

Abstract: Accurately extracting and representing the structure of tabular data from
financial documents remains a critical challenge in document understanding,
particularly for regulatory and analytical use cases. This study addresses the
complexity of converting financial tables from Malaysian audited financial
reports into Markdown format, a task complicated by rotated layouts,
multi-level headers, and implicit structural cues. We propose a fine-tuned
vision-language model (VLM), based on Qwen2.5-VL-7B, optimized for
high-fidelity Markdown generation from document images. Our approach includes a
curated dataset of 2,152 image-text pairs with augmentations and a supervised
fine-tuning strategy using LoRA. To assess performance, we evaluated our model
on 100 out-of-sample tables using a dual framework: a criteria-based
LLM-as-a-judge for fine-grained accuracy and our novel Markdown
Tree-Edit-Distance-based Similarity (TEDS) metric for holistic structural
fidelity. Our model achieves a 92.20% overall accuracy on the criteria-based
assessment and a 96.53% Markdown TEDS score. This performance significantly
surpasses its Qwen2.5-VL-7B base model, larger-scale VLMs, and specialized
reasoning-enabled models. Compared to these self-hosted alternatives, it also
significantly reduces inference time. Furthermore, its accuracy exceeds that of
widely used proprietary models such as OpenAI's GPT-4o and Gemini 2.5 Flash.
These results demonstrate that domain-specific fine-tuning provides an
effective and efficient method to bridge the gap between unstructured financial
documents and downstream automation, rivalling much larger and more general
models without their computational overhead.

</details>


### [238] [LMAR: Language Model Augmented Retriever for Domain-specific Knowledge Indexing](https://arxiv.org/abs/2508.05672)
*Yao Zhao,Yantian Ding,Zhiyue Zhang,Dapeng Yao,Yanxun Xu*

Main category: cs.IR

TL;DR: LMAR is a model-agnostic framework combining LLM-guided data synthesis, contrastive embedding adaptation, and efficient text clustering to improve domain-specific RAG systems.


<details>
  <summary>Details</summary>
Motivation: Address performance deterioration of pre-trained embeddings and high computational costs of LLM-based retrievers in RAG systems.

Method: Two-stage pipeline: (1) Triplet sampling and synthetic data augmentation using LLMs as labelers/validators, (2) Contrastive embedding adaptation and efficient text clustering.

Result: Outperforms baseline models on domain-specific benchmarks with moderate hardware requirements and low latency.

Conclusion: LMAR is a practical, cost-effective solution for scalable domain-specific adaptation in RAG systems.

Abstract: Retrieval Augmented Generation (RAG) systems often struggle with
domain-specific knowledge due to performance deterioration of pre-trained
embeddings and prohibitive computational costs of large language model
(LLM)-based retrievers. While fine-tuning data augmentation embedding models
offers a promising direction, its effectiveness is limited by the need for
high-quality training data and reliable chunking strategies that preserve
contextual integrity. We propose LMAR (Language Model Augmented Retriever), a
model-agnostic framework that addresses these challenges by combining
LLM-guided data synthesis with contrastive embedding adaptation and efficient
text clustering. LMAR consists of a two-stage pipeline: (1) Triplet sampling
and synthetic data augmentation, where LLMs act as both labeler and validator
to ensure high-fidelity supervision throughout the pipeline. Experimental
results across multiple domain-specific benchmark datasets demonstrate that
LMAR outperforms multiple baseline models, while maintaining moderate hardware
requirements and low latency. Its model-agnostic nature further enables
seamless integration with emerging RAG architectures and text embedding models,
ensuring continual improvements without redesigning the pipeline. These results
highlight LMAR as a practical and cost-effective solution for scalable
domain-specific adaptation.

</details>


### [239] [AI Guided Accelerator For Search Experience](https://arxiv.org/abs/2508.05649)
*Jayanth Yetukuri,Mehran Elyasi,Samarth Agrawal,Aritra Mandal,Rui Kong,Harish Vempati,Ishita Khan*

Main category: cs.IR

TL;DR: A novel framework models transitional queries in e-commerce searches, using LLMs for intent-aware query expansion, improving conversion and engagement.


<details>
  <summary>Details</summary>
Motivation: Traditional query rewrite methods miss sequential user behavior dynamics, limiting relevance in e-commerce searches.

Method: Mines structured query trajectories from user logs, models transitional queries, and uses LLMs for diverse query generation.

Result: Shows measurable gains in conversion and engagement metrics compared to existing methods.

Conclusion: The approach effectively captures and leverages transitional queries for better e-commerce search experiences.

Abstract: Effective query reformulation is pivotal in narrowing the gap between a
user's exploratory search behavior and the identification of relevant products
in e-commerce environments. While traditional approaches predominantly model
query rewrites as isolated pairs, they often fail to capture the sequential and
transitional dynamics inherent in real-world user behavior. In this work, we
propose a novel framework that explicitly models transitional
queries--intermediate reformulations occurring during the user's journey toward
their final purchase intent. By mining structured query trajectories from
eBay's large-scale user interaction logs, we reconstruct query sequences that
reflect shifts in intent while preserving semantic coherence. This approach
allows us to model a user's shopping funnel, where mid-journey transitions
reflect exploratory behavior and intent refinement. Furthermore, we incorporate
generative Large Language Models (LLMs) to produce semantically diverse and
intent-preserving alternative queries, extending beyond what can be derived
through collaborative filtering alone. These reformulations can be leveraged to
populate Related Searches or to power intent-clustered carousels on the search
results page, enhancing both discovery and engagement. Our contributions
include (i) the formal identification and modeling of transitional queries,
(ii) the introduction of a structured query sequence mining pipeline for intent
flow understanding, and (iii) the application of LLMs for scalable,
intent-aware query expansion. Empirical evaluation demonstrates measurable
gains in conversion and engagement metrics compared to the existing Related
Searches module, validating the effectiveness of our approach in real-world
e-commerce settings.

</details>


### [240] [Breaking the Top-$K$ Barrier: Advancing Top-$K$ Ranking Metrics Optimization in Recommender Systems](https://arxiv.org/abs/2508.05673)
*Weiqin Yang,Jiawei Chen,Shengjia Zhang,Peng Wu,Yuegang Sun,Yan Feng,Chun Chen,Can Wang*

Main category: cs.IR

TL;DR: The paper introduces SoftmaxLoss@K (SL@K), a novel loss function for optimizing NDCG@K in recommender systems, addressing challenges like discontinuity and Top-K truncation.


<details>
  <summary>Details</summary>
Motivation: Top-K ranking metrics like NDCG@K are standard for evaluating recommender systems but are hard to optimize due to discontinuity and truncation issues. Existing methods either ignore truncation or are computationally expensive and unstable.

Method: The proposed SL@K integrates quantile techniques for Top-K truncation and derives a smooth upper bound for NDCG@K optimization, ensuring theoretical guarantees, efficiency, and stability.

Result: Experiments on four datasets and three recommendation backbones show SL@K outperforms existing methods with an average improvement of 6.03%.

Conclusion: SL@K is a robust, efficient, and stable solution for optimizing NDCG@K, offering significant performance gains in recommender systems.

Abstract: In the realm of recommender systems (RS), Top-$K$ ranking metrics such as
NDCG@$K$ are the gold standard for evaluating recommendation performance.
However, during the training of recommendation models, optimizing NDCG@$K$
poses significant challenges due to its inherent discontinuous nature and the
intricate Top-$K$ truncation. Recent efforts to optimize NDCG@$K$ have either
overlooked the Top-$K$ truncation or suffered from high computational costs and
training instability. To overcome these limitations, we propose SoftmaxLoss@$K$
(SL@$K$), a novel recommendation loss tailored for NDCG@$K$ optimization.
Specifically, we integrate the quantile technique to handle Top-$K$ truncation
and derive a smooth upper bound for optimizing NDCG@$K$ to address
discontinuity. The resulting SL@$K$ loss has several desirable properties,
including theoretical guarantees, ease of implementation, computational
efficiency, gradient stability, and noise robustness. Extensive experiments on
four real-world datasets and three recommendation backbones demonstrate that
SL@$K$ outperforms existing losses with a notable average improvement of 6.03%.
The code is available at https://github.com/Tiny-Snow/IR-Benchmark.

</details>


### [241] [Are All Genders Equal in the Eyes of Algorithms? -- Analysing Search and Retrieval Algorithms for Algorithmic Gender Fairness](https://arxiv.org/abs/2508.05680)
*Stefanie Urchs,Veronika Thurner,Matthias A√üenmacher,Ludwig Bothmann,Christian Heumann,Stephanie Thiemichen*

Main category: cs.IR

TL;DR: The paper examines gender fairness in algorithmic systems like search engines, revealing subtle biases in academic visibility despite no overt discrimination.


<details>
  <summary>Details</summary>
Motivation: To assess whether algorithmic systems reproduce or amplify societal gender biases in academic visibility and knowledge dissemination.

Method: Introduces a bias-preserving fairness definition and analyzes a dataset of academic profiles from German institutions, evaluating metadata completeness, publication retrieval, and search result visibility.

Result: No overt discrimination found, but male professors had more aligned search results and publications, while female professors showed higher variability in digital visibility.

Conclusion: Fairness evaluations in digital systems must consider both technical performance and representational equality to address subtle biases.

Abstract: Algorithmic systems such as search engines and information retrieval
platforms significantly influence academic visibility and the dissemination of
knowledge. Despite assumptions of neutrality, these systems can reproduce or
reinforce societal biases, including those related to gender. This paper
introduces and applies a bias-preserving definition of algorithmic gender
fairness, which assesses whether algorithmic outputs reflect real-world gender
distributions without introducing or amplifying disparities. Using a
heterogeneous dataset of academic profiles from German universities and
universities of applied sciences, we analyse gender differences in metadata
completeness, publication retrieval in academic databases, and visibility in
Google search results. While we observe no overt algorithmic discrimination,
our findings reveal subtle but consistent imbalances: male professors are
associated with a greater number of search results and more aligned publication
records, while female professors display higher variability in digital
visibility. These patterns reflect the interplay between platform algorithms,
institutional curation, and individual self-presentation. Our study highlights
the need for fairness evaluations that account for both technical performance
and representational equality in digital systems.

</details>


### [242] [Domain-Specific Fine-Tuning and Prompt-Based Learning: A Comparative Study for developing Natural Language-Based BIM Information Retrieval Systems](https://arxiv.org/abs/2508.05676)
*Han Gao,Timo Hartmann,Botao Zhong,Kai Lia,Hanbin Luo*

Main category: cs.IR

TL;DR: The paper compares domain-specific fine-tuning and prompt-based learning for NLI-based BIM information retrieval, proposing a hybrid approach for balanced performance.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of accurately extracting BIM data via natural language queries due to domain complexity and specificity.

Method: A two-stage framework (intent recognition and table-based QA) is tested using a BIM-specific dataset of 1,740 queries.

Result: Fine-tuning excels in intent recognition, while prompt-based learning (e.g., GPT-4o) performs better in QA. A hybrid approach balances both tasks.

Conclusion: The hybrid method is robust for real-world BIM scenarios, offering insights for designing language-driven BIM systems.

Abstract: Building Information Modeling (BIM) is essential for managing building data
across the entire lifecycle, supporting tasks from design to maintenance.
Natural Language Interface (NLI) systems are increasingly explored as
user-friendly tools for information retrieval in Building Information Modeling
(BIM) environments. Despite their potential, accurately extracting BIM-related
data through natural language queries remains a persistent challenge due to the
complexity use queries and specificity of domain knowledge. This study presents
a comparative analysis of two prominent approaches for developing NLI-based BIM
information retrieval systems: domain-specific fine-tuning and prompt-based
learning using large language models (LLMs). A two-stage framework consisting
of intent recognition and table-based question answering is implemented to
evaluate the effectiveness of both approaches. To support this evaluation, a
BIM-specific dataset of 1,740 annotated queries of varying types across 69
models is constructed. Experimental results show that domain-specific
fine-tuning delivers superior performance in intent recognition tasks, while
prompt-based learning, particularly with GPT-4o, shows strength in table-based
question answering. Based on these findings, this study identify a hybrid
configuration that combines fine-tuning for intent recognition with
prompt-based learning for question answering, achieving more balanced and
robust performance across tasks. This integrated approach is further tested
through case studies involving BIM models of varying complexity. This study
provides a systematic analysis of the strengths and limitations of each
approach and discusses the applicability of the NLI to real-world BIM
scenarios. The findings offer insights for researchers and practitioners in
designing intelligent, language-driven BIM systems.

</details>


### [243] [Multi-Faceted Large Embedding Tables for Pinterest Ads Ranking](https://arxiv.org/abs/2508.05700)
*Runze Su,Jiayin Jin,Jiacheng Li,Sihan Wang,Guangtong Bai,Zelun Wang,Li Tang,Yixiong Meng,Huasen Wu,Zhimeng Pan,Kungang Li,Han Sun,Zhifang Liu,Haoyang Li,Siping Ji,Ling Leng,Prathibha Deshikachar*

Main category: cs.IR

TL;DR: The paper introduces a multi-faceted pretraining scheme for large embedding tables in Pinterest's ads ranking models, overcoming initial neutral metrics and achieving significant performance gains in CTR and CVR. A CPU-GPU hybrid serving infrastructure was also designed to address scalability and GPU memory limits.


<details>
  <summary>Details</summary>
Motivation: Large embedding tables are crucial for capturing complex interactions in recommendation systems, but initial attempts at training them from scratch yielded neutral results, prompting the need for an improved approach.

Method: A novel multi-faceted pretraining scheme incorporating multiple algorithms was introduced, alongside a CPU-GPU hybrid serving infrastructure to address scalability and memory constraints.

Result: The approach led to significant improvements: 1.34% CPC reduction, 2.60% CTR increase, and neutral latency change.

Conclusion: The multi-faceted pretraining and hybrid serving framework successfully enhanced performance and scalability in Pinterest's ads ranking system.

Abstract: Large embedding tables are indispensable in modern recommendation systems,
thanks to their ability to effectively capture and memorize intricate details
of interactions among diverse entities. As we explore integrating large
embedding tables into Pinterest's ads ranking models, we encountered not only
common challenges such as sparsity and scalability, but also several obstacles
unique to our context. Notably, our initial attempts to train large embedding
tables from scratch resulted in neutral metrics. To tackle this, we introduced
a novel multi-faceted pretraining scheme that incorporates multiple pretraining
algorithms. This approach greatly enriched the embedding tables and resulted in
significant performance improvements. As a result, the multi-faceted large
embedding tables bring great performance gain on both the Click-Through Rate
(CTR) and Conversion Rate (CVR) domains. Moreover, we designed a CPU-GPU hybrid
serving infrastructure to overcome GPU memory limits and elevate the
scalability. This framework has been deployed in the Pinterest Ads system and
achieved 1.34% online CPC reduction and 2.60% CTR increase with neutral
end-to-end latency change.

</details>


### [244] [G-UBS: Towards Robust Understanding of Implicit Feedback via Group-Aware User Behavior Simulation](https://arxiv.org/abs/2508.05709)
*Boyu Chen,Siran Chen,Zhengrong Yue,Kainan Yan,Chenyun Yu,Beibei Kong,Cheng Lei,Chengxiang Zhuo,Zang Li,Yali Wang*

Main category: cs.IR

TL;DR: The paper introduces G-UBS, a novel paradigm for interpreting noisy implicit feedback in recommendation systems by leveraging group context and reinforcement learning, achieving superior performance on a new benchmark IF-VR.


<details>
  <summary>Details</summary>
Motivation: Explicit feedback is scarce, and implicit feedback is noisy, leading to misjudged user interests. The paper aims to improve recommendation accuracy by better interpreting implicit feedback.

Method: Proposes G-UBS with two agents: User Group Manager (UGM) for clustering users and User Feedback Modeler (UFM) for group-aware reinforcement learning.

Result: G-UBS outperforms mainstream models with 4.0% higher play rate (>30%) and 14.9% higher reasoning accuracy on the IF-VR benchmark.

Conclusion: G-UBS effectively addresses noise in implicit feedback, enhancing recommendation performance through group context and reinforcement learning.

Abstract: User feedback is critical for refining recommendation systems, yet explicit
feedback (e.g., likes or dislikes) remains scarce in practice. As a more
feasible alternative, inferring user preferences from massive implicit feedback
has shown great potential (e.g., a user quickly skipping a recommended video
usually indicates disinterest). Unfortunately, implicit feedback is often
noisy: a user might skip a video due to accidental clicks or other reasons,
rather than disliking it. Such noise can easily misjudge user interests,
thereby undermining recommendation performance. To address this issue, we
propose a novel Group-aware User Behavior Simulation (G-UBS) paradigm, which
leverages contextual guidance from relevant user groups, enabling robust and
in-depth interpretation of implicit feedback for individual users.
Specifically, G-UBS operates via two key agents. First, the User Group Manager
(UGM) effectively clusters users to generate group profiles utilizing a
``summarize-cluster-reflect" workflow based on LLMs. Second, the User Feedback
Modeler (UFM) employs an innovative group-aware reinforcement learning
approach, where each user is guided by the associated group profiles during the
reinforcement learning process, allowing UFM to robustly and deeply examine the
reasons behind implicit feedback. To assess our G-UBS paradigm, we have
constructed a Video Recommendation benchmark with Implicit Feedback (IF-VR). To
the best of our knowledge, this is the first multi-modal benchmark for implicit
feedback evaluation in video recommendation, encompassing 15k users, 25k
videos, and 933k interaction records with implicit feedback. Extensive
experiments on IF-VR demonstrate that G-UBS significantly outperforms
mainstream LLMs and MLLMs, with a 4.0% higher proportion of videos achieving a
play rate > 30% and 14.9% higher reasoning accuracy on IF-VR.

</details>


### [245] [eSASRec: Enhancing Transformer-based Recommendations in a Modular Fashion](https://arxiv.org/abs/2508.06450)
*Daria Tikhonovich,Nikita Zelinskiy,Aleksandr V. Petrov,Mayya Spirina,Andrei Semenov,Andrey V. Savchenko,Sergei Kuliev*

Main category: cs.IR

TL;DR: The paper introduces eSASRec, an enhanced version of SASRec for sequential recommendations, combining modular improvements like LiGR Transformer layers and Sampled Softmax Loss, achieving 23% better performance than state-of-the-art models.


<details>
  <summary>Details</summary>
Motivation: To systematically benchmark the additivity of modular improvements in Transformer-based sequential recommendation models, which has not been explored before.

Method: Combines SASRec's training objective, LiGR Transformer layers, and Sampled Softmax Loss to create eSASRec, evaluated in both academic and production-like benchmarks.

Result: eSASRec outperforms state-of-the-art models by 23% in academic benchmarks and performs competitively in production-like settings, residing on the Pareto frontier for accuracy-coverage tradeoff.

Conclusion: eSASRec is a simple yet strong baseline for sequential recommendations, easily integrable into existing pipelines, with open-source implementations provided.

Abstract: Since their introduction, Transformer-based models, such as SASRec and
BERT4Rec, have become common baselines for sequential recommendations,
surpassing earlier neural and non-neural methods. A number of following
publications have shown that the effectiveness of these models can be improved
by, for example, slightly updating the architecture of the Transformer layers,
using better training objectives, and employing improved loss functions.
However, the additivity of these modular improvements has not been
systematically benchmarked - this is the gap we aim to close in this paper.
Through our experiments, we identify a very strong model that uses SASRec's
training objective, LiGR Transformer layers, and Sampled Softmax Loss. We call
this combination eSASRec (Enhanced SASRec). While we primarily focus on
realistic, production-like evaluation, in our preliminarily study we find that
common academic benchmarks show eSASRec to be 23% more effective compared to
the most recent state-of-the-art models, such as ActionPiece. In our main
production-like benchmark, eSASRec resides on the Pareto frontier in terms of
the accuracy-coverage tradeoff (alongside the recent industrial models HSTU and
FuXi. As the modifications compared to the original SASRec are relatively
straightforward and no extra features are needed (such as timestamps in HSTU),
we believe that eSASRec can be easily integrated into existing recommendation
pipelines and can can serve as a strong yet very simple baseline for emerging
complicated algorithms. To facilitate this, we provide the open-source
implementations for our models and benchmarks in repository
https://github.com/blondered/transformer_benchmark

</details>


### [246] [Maximum Impact with Fewer Features: Efficient Feature Selection for Cold-Start Recommenders through Collaborative Importance Weighting](https://arxiv.org/abs/2508.06455)
*Nikita Sukhorukov,Danil Gusak,Evgeny Frolov*

Main category: cs.IR

TL;DR: Proposes a feature selection strategy for cold-start recommender systems, balancing accuracy and efficiency by prioritizing user behavioral data and using a hybrid matrix factorization technique.


<details>
  <summary>Details</summary>
Motivation: Addresses the challenge of irrelevant/noisy features in cold-start scenarios, which degrade performance and increase computational costs.

Method: Uses a hybrid matrix factorization technique to incorporate collaborative behavior data correlations, then ranks features via the maximum volume algorithm.

Result: Outperforms existing feature selection methods in cold-start scenarios, achieving high accuracy with minimal features and superior efficiency.

Conclusion: The method effectively selects influential features, enhancing recommendation accuracy while maintaining computational efficiency.

Abstract: Cold-start challenges in recommender systems necessitate leveraging auxiliary
features beyond user-item interactions. However, the presence of irrelevant or
noisy features can degrade predictive performance, whereas an excessive number
of features increases computational demands, leading to higher memory
consumption and prolonged training times.
  To address this, we propose a feature selection strategy that prioritizes the
user behavioral information. Our method enhances the feature representation by
incorporating correlations from collaborative behavior data using a hybrid
matrix factorization technique and then ranks features using a mechanism based
on the maximum volume algorithm. This approach identifies the most influential
features, striking a balance between recommendation accuracy and computational
efficiency. We conduct an extensive evaluation across various datasets and
hybrid recommendation models, demonstrating that our method excels in
cold-start scenarios by selecting minimal yet highly effective feature subsets.
Even under strict feature reduction, our approach surpasses existing feature
selection techniques while maintaining superior efficiency.

</details>


### [247] [Semantic Item Graph Enhancement for Multimodal Recommendation](https://arxiv.org/abs/2508.06154)
*Xiaoxiong Zhang,Xin Zhou,Zhiwei Zeng,Dusit Niyato,Zhiqi Shen*

Main category: cs.IR

TL;DR: The paper proposes a method to enhance multimodal recommendation systems by addressing semantic deficiencies in item-item graphs and reducing noise impact through contrastive learning and representation alignment.


<details>
  <summary>Details</summary>
Motivation: Prior methods suffer from semantic deficiencies and noise in raw modality features, limiting performance in multimodal recommendation systems.

Method: The approach infuses collaborative signals into modality-specific graphs, uses modulus-based perturbations for noise robustness, and aligns representations via InfoNCE losses.

Result: Experiments on four datasets confirm the framework's effectiveness.

Conclusion: The proposed method improves semantic modeling and noise robustness, enhancing recommendation performance.

Abstract: Multimodal recommendation systems have attracted increasing attention for
their improved performance by leveraging items' multimodal information. Prior
methods often build modality-specific item-item semantic graphs from raw
modality features and use them as supplementary structures alongside the
user-item interaction graph to enhance user preference learning. However, these
semantic graphs suffer from semantic deficiencies, including (1) insufficient
modeling of collaborative signals among items and (2) structural distortions
introduced by noise in raw modality features, ultimately compromising
performance. To address these issues, we first extract collaborative signals
from the interaction graph and infuse them into each modality-specific item
semantic graph to enhance semantic modeling. Then, we design a modulus-based
personalized embedding perturbation mechanism that injects perturbations with
modulus-guided personalized intensity into embeddings to generate contrastive
views. This enables the model to learn noise-robust representations through
contrastive learning, thereby reducing the effect of structural noise in
semantic graphs. Besides, we propose a dual representation alignment mechanism
that first aligns multiple semantic representations via a designed Anchor-based
InfoNCE loss using behavior representations as anchors, and then aligns
behavior representations with the fused semantics by standard InfoNCE, to
ensure representation consistency. Extensive experiments on four benchmark
datasets validate the effectiveness of our framework.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [248] [IOCC: Aligning Semantic and Cluster Centers for Few-shot Short Text Clustering](https://arxiv.org/abs/2508.06126)
*Jixuan Yin,Zhihao Yao,Wenshuai Huo,Xinmiao Yu,Xiaocheng Feng,Bo Li*

Main category: stat.ME

TL;DR: IOCC is a few-shot contrastive learning method for clustering tasks, improving semantic alignment between cluster and semantic centers via IEOT and CACL modules, outperforming previous methods.


<details>
  <summary>Details</summary>
Motivation: Conventional methods struggle with semantic alignment in short text clustering due to limited expressiveness, leading to suboptimal representations.

Method: IOCC uses IEOT for semantic interactions and pseudo-labeling, and CACL to optimize representations toward pseudo-centers.

Result: IOCC achieves up to 7.34% improvement on benchmarks, excelling in clustering stability and efficiency.

Conclusion: IOCC effectively bridges the gap between cluster and semantic centers, enhancing clustering performance.

Abstract: In clustering tasks, it is essential to structure the feature space into
clear, well-separated distributions. However, because short text
representations have limited expressiveness, conventional methods struggle to
identify cluster centers that truly capture each category's underlying
semantics, causing the representations to be optimized in suboptimal
directions. To address this issue, we propose IOCC, a novel few-shot
contrastive learning method that achieves alignment between the cluster centers
and the semantic centers. IOCC consists of two key modules:
Interaction-enhanced Optimal Transport (IEOT) and Center-aware Contrastive
Learning (CACL). Specifically, IEOT incorporates semantic interactions between
individual samples into the conventional optimal transport problem, and
generate pseudo-labels. Based on these pseudo-labels, we aggregate
high-confidence samples to construct pseudo-centers that approximate the
semantic centers. Next, CACL optimizes text representations toward their
corresponding pseudo-centers. As training progresses, the collaboration between
the two modules gradually reduces the gap between cluster centers and semantic
centers. Therefore, the model will learn a high-quality distribution, improving
clustering performance. Extensive experiments on eight benchmark datasets show
that IOCC outperforms previous methods, achieving up to 7.34\% improvement on
challenging Biomedical dataset and also excelling in clustering stability and
efficiency. The code is available at:
https://anonymous.4open.science/r/IOCC-C438.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [249] [CLAPP: The CLASS LLM Agent for Pair Programming](https://arxiv.org/abs/2508.05728)
*Santiago Casas,Christian Fidler,Boris Bolliet,Francisco Villaescusa-Navarro,Julien Lesgourgues*

Main category: astro-ph.IM

TL;DR: CLAPP is an AI assistant for CLASS, aiding researchers with coding, debugging, and plotting via LLMs and domain-specific retrieval.


<details>
  <summary>Details</summary>
Motivation: To support scientists unfamiliar with AI tools and enhance productivity in computational cosmology.

Method: Combines multi-agent LLM orchestration, semantic search, and live Python execution.

Result: A user-friendly web app that lowers the entry barrier for CLASS users.

Conclusion: CLAPP enables effective human-AI collaboration in cosmology research.

Abstract: We introduce CLAPP (CLASS LLM Agent for Pair Programming), an interactive AI
assistant designed to support researchers working with the Einstein-Boltzmann
solver CLASS. CLAPP leverages large language models (LLMs) and domain-specific
retrieval to provide conversational coding support for CLASS-answering
questions, generating code, debugging errors, and producing plots. Its
architecture combines multi-agent LLM orchestration, semantic search across
CLASS documentation, and a live Python execution environment. Deployed as a
user-friendly web application, CLAPP lowers the entry barrier for scientists
unfamiliar with AI tools and enables more productive human-AI collaboration in
computational and numerical cosmology. The app is available at
https://classclapp.streamlit.app

</details>


<div id='cs.SC'></div>

# cs.SC [[Back]](#toc)

### [250] [Tree-Based Deep Learning for Ranking Symbolic Integration Algorithms](https://arxiv.org/abs/2508.06383)
*Rashid Barket,Matthew England,J√ºrgen Gerhard*

Main category: cs.SC

TL;DR: A machine learning approach improves symbolic indefinite integration in Maple by predicting optimal methods and output complexity, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for selecting integration algorithms in Maple are inefficient and lack problem-specific consideration.

Method: A two-stage ML architecture uses tree-based models to first identify applicable methods and then rank them by predicted output complexity.

Result: Achieves nearly 90% accuracy on a test set and outperforms Maple's built-in selector on out-of-distribution benchmarks.

Conclusion: Tree-based representations and problem framing are crucial for ML in symbolic computation, with potential for broader application in mathematical software.

Abstract: Symbolic indefinite integration in Computer Algebra Systems such as Maple
involves selecting the most effective algorithm from multiple available
methods. Not all methods will succeed for a given problem, and when several do,
the results, though mathematically equivalent, can differ greatly in
presentation complexity. Traditionally, this choice has been made with minimal
consideration of the problem instance, leading to inefficiencies.
  We present a machine learning (ML) approach using tree-based deep learning
models within a two-stage architecture: first identifying applicable methods
for a given instance, then ranking them by predicted output complexity.
Furthermore, we find representing mathematical expressions as tree structures
significantly improves performance over sequence-based representations, and our
two-stage framework outperforms alternative ML formulations.
  Using a diverse dataset generated by six distinct data generators, our models
achieve nearly 90% accuracy in selecting the optimal method on a 70,000 example
holdout test set. On an independent out-of-distribution benchmark from Maple's
internal test suite, our tree transformer model maintains strong
generalisation, outperforming Maple's built-in selector and prior ML
approaches.
  These results highlight the critical role of data representation and problem
framing in ML for symbolic computation, and we expect our methodology to
generalise effectively to similar optimisation problems in mathematical
software.

</details>


<div id='astro-ph.CO'></div>

# astro-ph.CO [[Back]](#toc)

### [251] [Detecting Model Misspecification in Cosmology with Scale-Dependent Normalizing Flows](https://arxiv.org/abs/2508.05744)
*Aizhan Akhmetzhanova,Carolina Cuesta-Lazaro,Siddharth Mishra-Sharma*

Main category: astro-ph.CO

TL;DR: A framework combining neural summary statistics and normalizing flows is proposed to detect model misspecification in cosmological simulations using Bayesian evidence estimation.


<details>
  <summary>Details</summary>
Motivation: Cosmological surveys generate vast, high-dimensional data, requiring accurate modeling. Validating theoretical models against observed data is challenging, especially with the need for data representations that retain cosmological information while reducing dimensionality.

Method: The approach integrates scale-dependent neural summary statistics with normalizing flows for Bayesian evidence estimation. Neural networks are conditioned on smoothing scales to identify model breakdowns.

Result: Demonstrated on matter and gas density fields from CAMELS simulation suites with varied subgrid physics, the method effectively detects model misspecification.

Conclusion: The framework provides a data-driven way to validate theoretical models in cosmology, addressing challenges in high-dimensional data analysis.

Abstract: Current and upcoming cosmological surveys will produce unprecedented amounts
of high-dimensional data, which require complex high-fidelity forward
simulations to accurately model both physical processes and systematic effects
which describe the data generation process. However, validating whether our
theoretical models accurately describe the observed datasets remains a
fundamental challenge. An additional complexity to this task comes from
choosing appropriate representations of the data which retain all the relevant
cosmological information, while reducing the dimensionality of the original
dataset. In this work we present a novel framework combining scale-dependent
neural summary statistics with normalizing flows to detect model
misspecification in cosmological simulations through Bayesian evidence
estimation. By conditioning our neural network models for data compression and
evidence estimation on the smoothing scale, we systematically identify where
theoretical models break down in a data-driven manner. We demonstrate a first
application to our approach using matter and gas density fields from three
CAMELS simulation suites with different subgrid physics implementations.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [252] [Random Walk Learning and the Pac-Man Attack](https://arxiv.org/abs/2508.05663)
*Xingran Chen,Parimal Parag,Rohit Bhagat,Zonghong Liu,Salim El Rouayheb*

Main category: stat.ML

TL;DR: The paper addresses the vulnerability of random walk (RW)-based algorithms to the "Pac-Man" attack, where malicious nodes terminate RWs, and proposes the Average Crossing (AC) algorithm to prevent RW extinction while maintaining convergence in decentralized learning.


<details>
  <summary>Details</summary>
Motivation: RW-based algorithms are widely used in distributed systems and decentralized learning but are vulnerable to stealthy adversarial attacks like the Pac-Man attack, which disrupts the learning process.

Method: The authors propose the AC algorithm, a decentralized mechanism that duplicates RWs to counteract the Pac-Man attack, ensuring RW survival and learning convergence.

Result: Theoretical and empirical results show that AC keeps the RW population bounded and maintains convergence in stochastic gradient descent, even under attack, with a quantifiable deviation from the optimum. A phase transition in extinction probability is also observed.

Conclusion: The AC algorithm effectively mitigates the Pac-Man attack, ensuring robustness in RW-based decentralized learning, with theoretical and empirical validation.

Abstract: Random walk (RW)-based algorithms have long been popular in distributed
systems due to low overheads and scalability, with recent growing applications
in decentralized learning. However, their reliance on local interactions makes
them inherently vulnerable to malicious behavior. In this work, we investigate
an adversarial threat that we term the ``Pac-Man'' attack, in which a malicious
node probabilistically terminates any RW that visits it. This stealthy behavior
gradually eliminates active RWs from the network, effectively halting the
learning process without triggering failure alarms. To counter this threat, we
propose the Average Crossing (AC) algorithm--a fully decentralized mechanism
for duplicating RWs to prevent RW extinction in the presence of Pac-Man. Our
theoretical analysis establishes that (i) the RW population remains almost
surely bounded under AC and (ii) RW-based stochastic gradient descent remains
convergent under AC, even in the presence of Pac-Man, with a quantifiable
deviation from the true optimum. Our extensive empirical results on both
synthetic and real-world datasets corroborate our theoretical findings.
Furthermore, they uncover a phase transition in the extinction probability as a
function of the duplication threshold. We offer theoretical insights by
analyzing a simplified variant of the AC, which sheds light on the observed
phase transition.

</details>


### [253] [Reduction Techniques for Survival Analysis](https://arxiv.org/abs/2508.05715)
*Johannes Piller,L√©a Orsini,Simon Wiegrebe,John Zobolas,Lukas Burk,Sophie Hanna Langbein,Philip Studener,Markus Goeswein,Andreas Bender*

Main category: stat.ML

TL;DR: The paper introduces reduction techniques for survival analysis, enabling the use of standard machine learning tools by transforming survival tasks into regression or classification tasks. It reviews these techniques, implements them, and benchmarks their performance.


<details>
  <summary>Details</summary>
Motivation: To simplify survival analysis by leveraging standard machine learning tools without ignoring the unique aspects of survival data.

Method: Overview and implementation of reduction techniques, transforming survival tasks into regression or classification, followed by benchmarking against established methods.

Result: The reduction techniques are shown to be viable, with their performance compared to traditional survival analysis methods.

Conclusion: Reduction techniques offer a practical way to apply standard machine learning tools to survival analysis, balancing ease of use with performance.

Abstract: In this work, we discuss what we refer to as reduction techniques for
survival analysis, that is, techniques that "reduce" a survival task to a more
common regression or classification task, without ignoring the specifics of
survival data. Such techniques particularly facilitate machine learning-based
survival analysis, as they allow for applying standard tools from machine and
deep learning to many survival tasks without requiring custom learners. We
provide an overview of different reduction techniques and discuss their
respective strengths and weaknesses. We also provide a principled
implementation of some of these reductions, such that they are directly
available within standard machine learning workflows. We illustrate each
reduction using dedicated examples and perform a benchmark analysis that
compares their predictive performance to established machine learning methods
for survival analysis.

</details>


### [254] [Stochastic Trace Optimization of Parameter Dependent Matrices Based on Statistical Learning Theory](https://arxiv.org/abs/2508.05764)
*Arvind K. Saibaba,Ilse C. F. Ipsen*

Main category: stat.ML

TL;DR: A Monte Carlo estimator is proposed for minimizing the trace of parameter-dependent matrices, with bounds on backward error derived using epsilon nets and generic chaining.


<details>
  <summary>Details</summary>
Motivation: To efficiently minimize the trace of matrices dependent on parameters, especially for cases with small offdiagonal mass and compact parameter spaces.

Method: Monte Carlo estimator with sampling amount determined to bound backward error, analyzed via epsilon nets and generic chaining.

Result: Bounds predict small sampling for matrices with small offdiagonal mass and compact parameter spaces, with weak or implicit dependence on matrix dimension.

Conclusion: Epsilon net bounds are easier to evaluate, while chaining bounds may be superior but are harder to compute.

Abstract: We consider matrices $\boldsymbol{A}(\boldsymbol\theta)\in\mathbb{R}^{m\times
m}$ that depend, possibly nonlinearly, on a parameter $\boldsymbol\theta$ from
a compact parameter space $\Theta$. We present a Monte Carlo estimator for
minimizing $\text{trace}(\boldsymbol{A}(\boldsymbol\theta))$ over all
$\boldsymbol\theta\in\Theta$, and determine the sampling amount so that the
backward error of the estimator is bounded with high probability. We derive two
types of bounds, based on epsilon nets and on generic chaining. Both types
predict a small sampling amount for matrices
$\boldsymbol{A}(\boldsymbol\theta)$ with small offdiagonal mass, and parameter
spaces $\Theta$ of small ``size.'' Dependence on the matrix dimension~$m$ is
only weak or not explicit. The bounds based on epsilon nets are easier to
evaluate and come with fully specified constants. In contrast, the bounds based
on chaining depend on the Talagrand functionals which are difficult to
evaluate, except in very special cases. Comparisons between the two types of
bounds are difficult, although the literature suggests that chaining bounds can
be superior.

</details>


### [255] [Lightweight Auto-bidding based on Traffic Prediction in Live Advertising](https://arxiv.org/abs/2508.06069)
*Bo Yang,Ruixuan Luo,Junqi Jin,Han Zhu*

Main category: stat.ML

TL;DR: A lightweight bidding algorithm, Binary Constrained Bidding (BiCB), is proposed for real-time live advertising, combining optimal bidding formulas and traffic estimation for efficient performance.


<details>
  <summary>Details</summary>
Motivation: Live advertising requires real-time bidding and faces challenges like unknown future traffic and high computational complexity in existing methods.

Method: BiCB combines optimal bidding formulas from mathematical analysis with statistical traffic estimation, offering a low-complexity solution.

Result: BiCB achieves good approximation to optimal results with low computational cost, proven by offline and online experiments.

Conclusion: BiCB is effective for live advertising, balancing performance and engineering efficiency.

Abstract: Internet live streaming is widely used in online entertainment and
e-commerce, where live advertising is an important marketing tool for anchors.
An advertising campaign hopes to maximize the effect (such as conversions)
under constraints (such as budget and cost-per-click). The mainstream control
of campaigns is auto-bidding, where the performance depends on the decision of
the bidding algorithm in each request. The most widely used auto-bidding
algorithms include Proportional-Integral-Derivative (PID) control, linear
programming (LP), reinforcement learning (RL), etc. Existing methods either do
not consider the entire time traffic, or have too high computational
complexity. In this paper, the live advertising has high requirements for
real-time bidding (second-level control) and faces the difficulty of unknown
future traffic. Therefore, we propose a lightweight bidding algorithm Binary
Constrained Bidding (BiCB), which neatly combines the optimal bidding formula
given by mathematical analysis and the statistical method of future traffic
estimation, and obtains good approximation to the optimal result through a low
complexity solution. In addition, we complement the form of upper and lower
bound constraints for traditional auto-bidding modeling and give theoretical
analysis of BiCB. Sufficient offline and online experiments prove BiCB's good
performance and low engineering cost.

</details>


### [256] [Decorrelated feature importance from local sample weighting](https://arxiv.org/abs/2508.06337)
*Benedikt Fr√∂hlich,Alison Durst,Merle Behr*

Main category: stat.ML

TL;DR: The paper proposes local sample weighting (losaw) to improve feature importance (FI) scores in ML models when features are correlated, enhancing interpretability and sometimes prediction accuracy.


<details>
  <summary>Details</summary>
Motivation: Feature importance scores are limited by feature correlations, often misrepresenting signal and noise features. The goal is to improve FI accuracy in such scenarios.

Method: Introduces losaw, a sample weighting technique inspired by inverse probability weighting in causal inference, to decorrelate features locally within ML models. It integrates with decision trees and neural networks.

Result: Losaw consistently improves FI scores and often enhances out-of-distribution prediction accuracy while maintaining in-distribution performance.

Conclusion: Losaw effectively addresses FI limitations due to feature correlation, offering a flexible solution with interpretability-prediction tradeoffs.

Abstract: Feature importance (FI) statistics provide a prominent and valuable method of
insight into the decision process of machine learning (ML) models, but their
effectiveness has well-known limitations when correlation is present among the
features in the training data. In this case, the FI often tends to be
distributed among all features which are in correlation with the
response-generating signal features. Even worse, if multiple signal features
are in strong correlation with a noise feature, while being only modestly
correlated with one another, this can result in a noise feature having a
distinctly larger FI score than any signal feature. Here we propose local
sample weighting (losaw) which can flexibly be integrated into many ML
algorithms to improve FI scores in the presence of feature correlation in the
training data. Our approach is motivated from inverse probability weighting in
causal inference and locally, within the ML model, uses a sample weighting
scheme to decorrelate a target feature from the remaining features. This
reduces model bias locally, whenever the effect of a potential signal feature
is evaluated and compared to others. Moreover, losaw comes with a natural
tuning parameter, the minimum effective sample size of the weighted population,
which corresponds to an interpretation-prediction-tradeoff, analog to a
bias-variance-tradeoff as for classical ML tuning parameters. We demonstrate
how losaw can be integrated within decision tree-based ML methods and within
mini-batch training of neural networks. We investigate losaw for random forest
and convolutional neural networks in a simulation study on settings showing
diverse correlation patterns. We found that losaw improves FI consistently.
Moreover, it often improves prediction accuracy for out-of-distribution, while
maintaining a similar accuracy for in-distribution test data.

</details>


### [257] [DP-SPRT: Differentially Private Sequential Probability Ratio Tests](https://arxiv.org/abs/2508.06377)
*Thomas Michel,Debabrota Basu,Emilie Kaufmann*

Main category: stat.ML

TL;DR: DP-SPRT is a privacy-constrained wrapper for Wald's Sequential Probability Ratio Test, offering calibrated error probabilities and improved efficiency over naive methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the gap in privacy-preserving sequential hypothesis testing, particularly for simple hypotheses under differential privacy constraints.

Method: DP-SPRT uses a private mechanism (OutsideInterval) to process queries and stop when results fall outside a predefined interval, improving upon naive composition methods.

Result: The method provides near-optimal sample complexity for small errors and close hypotheses, with practical performance validated experimentally.

Conclusion: DP-SPRT is a viable and efficient solution for privacy-constrained sequential hypothesis testing, with theoretical and practical advantages.

Abstract: We revisit Wald's celebrated Sequential Probability Ratio Test for sequential
tests of two simple hypotheses, under privacy constraints. We propose DP-SPRT,
a wrapper that can be calibrated to achieve desired error probabilities and
privacy constraints, addressing a significant gap in previous work. DP-SPRT
relies on a private mechanism that processes a sequence of queries and stops
after privately determining when the query results fall outside a predefined
interval. This OutsideInterval mechanism improves upon naive composition of
existing techniques like AboveThreshold, potentially benefiting other
sequential algorithms. We prove generic upper bounds on the error and sample
complexity of DP-SPRT that can accommodate various noise distributions based on
the practitioner's privacy needs. We exemplify them in two settings: Laplace
noise (pure Differential Privacy) and Gaussian noise (R\'enyi differential
privacy). In the former setting, by providing a lower bound on the sample
complexity of any $\epsilon$-DP test with prescribed type I and type II errors,
we show that DP-SPRT is near optimal when both errors are small and the two
hypotheses are close. Moreover, we conduct an experimental study revealing its
good practical performance.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [258] [Risk Analysis Techniques for Governed LLM-based Multi-Agent Systems](https://arxiv.org/abs/2508.05687)
*Alistair Reid,Simon O'Callaghan,Liam Carroll,Tiberio Caetano*

Main category: cs.MA

TL;DR: The paper discusses the need for a new risk analysis approach for multi-agent AI systems, identifying six critical failure modes and providing tools for practitioners to assess them.


<details>
  <summary>Details</summary>
Motivation: Organisations are adopting interconnected multi-agent AI systems, but interactions between agents create emergent behaviors and novel risks, requiring a different approach than single-agent systems.

Method: The report examines six failure modes (e.g., cascading failures, communication issues) and offers a toolkit for risk assessment, advocating for staged testing and evidence collection through simulation, benchmarking, and red teaming.

Result: A methodology is proposed for robust risk management in LLM-based multi-agent systems, focusing on validity and progressive testing.

Conclusion: The approach lays the groundwork for safer deployment and operation of multi-agent AI systems by addressing emergent risks through structured analysis and testing.

Abstract: Organisations are starting to adopt LLM-based AI agents, with their
deployments naturally evolving from single agents towards interconnected,
multi-agent networks. Yet a collection of safe agents does not guarantee a safe
collection of agents, as interactions between agents over time create emergent
behaviours and induce novel failure modes. This means multi-agent systems
require a fundamentally different risk analysis approach than that used for a
single agent.
  This report addresses the early stages of risk identification and analysis
for multi-agent AI systems operating within governed environments where
organisations control their agent configurations and deployment. In this
setting, we examine six critical failure modes: cascading reliability failures,
inter-agent communication failures, monoculture collapse, conformity bias,
deficient theory of mind, and mixed motive dynamics. For each, we provide a
toolkit for practitioners to extend or integrate into their existing frameworks
to assess these failure modes within their organisational contexts.
  Given fundamental limitations in current LLM behavioural understanding, our
approach centres on analysis validity, and advocates for progressively
increasing validity through staged testing across stages of abstraction and
deployment that gradually increases exposure to potential negative impacts,
while collecting convergent evidence through simulation, observational
analysis, benchmarking, and red teaming. This methodology establishes the
groundwork for robust organisational risk management as these LLM-based
multi-agent systems are deployed and operated.

</details>


### [259] [Semantic Reasoning Meets Numerical Precision: An LLM-Powered Multi-Agent System for Power Grid Control](https://arxiv.org/abs/2508.05702)
*Yan Zhang*

Main category: cs.MA

TL;DR: Grid-Agent is an AI-driven framework combining LLMs and multi-agent reinforcement learning to autonomously detect and remediate power grid violations in real time.


<details>
  <summary>Details</summary>
Motivation: The complexity of modern power grids due to DERs, EVs, and extreme weather events exceeds traditional methods, necessitating adaptive, scalable solutions.

Method: Grid-Agent uses modular agents: a planning agent for action sequences and a validation agent for stability checks, with adaptive multiscale network representation for scalability.

Result: Superior violation mitigation in IEEE and CIGRE test systems, with continuous learning capabilities for diverse topologies.

Conclusion: Grid-Agent is highly suitable for smart grids needing rapid, autonomous responses to dynamic conditions.

Abstract: The increasing penetration of Distributed Energy Resources (DERs), widespread
adoption of Electric Vehicles (EVs), and the growing frequency of extreme
weather events have significantly increased the complexity of power grid
planning, operation, and management. Traditional rule-based systems and
numerical optimization approaches often struggle with the scale, dynamics, and
adaptability required by modern power networks. This paper introduces
Grid-Agent, an autonomous, AI-driven framework that combines Large Language
Models (LLMs) with multi-agent reinforcement learning to detect and remediate
grid violations in real time. Grid-Agent integrates semantic reasoning with
numerical precision through a modular agent architecture: a planning agent
generates coordinated action sequences using numerical power flow solvers,
while a validation agent evaluates system stability and action effectiveness
via sandboxed execution with safety rollbacks. To ensure scalability,
Grid-Agent incorporates an adaptive multiscale network representation that
dynamically selects optimal encoding schemes based on network size and
complexity. The framework enables coordinated violation resolution through
optimizing switch configurations, battery deployment, and load curtailment
strategies. Experimental results in standard IEEE and CIGRE test systems (IEEE
69-bus, CIGRE MV, and IEEE 30-bus) demonstrate superior violation mitigation
performance. Additionally, the framework's built-in data collection and
learning capabilities enable continuous learning and adaptation to diverse
network topologies. The autonomous nature of the framework makes it
particularly suitable for modern smart grid applications requiring rapid
response to dynamic operating conditions.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [260] [Numerical Considerations in Weighted Model Counting](https://arxiv.org/abs/2508.06264)
*Randal E. Bryant*

Main category: math.NA

TL;DR: The paper proposes a method to efficiently compute weighted model counts with guaranteed precision by combining numeric representations, addressing issues of accuracy and computational cost.


<details>
  <summary>Details</summary>
Motivation: Weighted model counting is crucial in probabilistic reasoning and risk assessment, but existing methods using floating-point or rational arithmetic face accuracy or efficiency challenges.

Method: The approach combines multiple numeric representations: extended-range double (ERD) for nonnegative weights, and interval floating-point with rational arithmetic for mixed weights, ensuring precision and efficiency.

Result: The method provides tight bounds on precision loss for nonnegative weights and avoids underflow/overflow issues. It also handles mixed weights robustly.

Conclusion: The proposed hybrid approach efficiently achieves guaranteed precision in weighted model counting, demonstrating robustness in challenging scenarios.

Abstract: Weighted model counting computes the sum of the rational-valued weights
associated with the satisfying assignments for a Boolean formula, where the
weight of an assignment is given by the product of the weights assigned to the
positive and negated variables comprising the assignment. Weighted model
counting finds applications across a variety of domains including probabilistic
reasoning and quantitative risk assessment.
  Most weighted model counting programs operate by (explicitly or implicitly)
converting the input formula into a form that enables arithmetic evaluation,
using multiplication for conjunctions and addition for disjunctions. Performing
this evaluation using floating-point arithmetic can yield inaccurate results,
and it cannot quantify the level of precision achieved. Computing with rational
arithmetic gives exact results, but it is costly in both time and space.
  This paper describes how to combine multiple numeric representations to
efficiently compute weighted model counts that are guaranteed to achieve a
user-specified precision. When all weights are nonnegative, we prove that the
precision loss of arithmetic evaluation using floating-point arithmetic can be
tightly bounded. We show that supplementing a standard IEEE double-precision
representation with a separate 64-bit exponent, a format we call extended-range
double (ERD), avoids the underflow and overflow issues commonly encountered in
weighted model counting. For problems with mixed negative and positive weights,
we show that a combination of interval floating-point arithmetic and rational
arithmetic can achieve the twin goals of efficiency and guaranteed precision.
For our evaluations, we have devised especially challenging formulas and weight
assignments, demonstrating the robustness of our approach.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [261] [Moment Estimate and Variational Approach for Learning Generalized Diffusion with Non-gradient Structures](https://arxiv.org/abs/2508.01854)
*Fanze Kong,Chen-Chih Lai,Yubin Lu*

Main category: physics.comp-ph

TL;DR: A two-stage data-driven framework identifies governing laws of non-gradient generalized diffusions, recovering pseudo-potentials and rotations effectively.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of identifying governing laws in non-gradient generalized diffusions, which involve complex dynamics like dissipation-rotation and rough pseudo-potentials.

Method: Combines energy dissipation laws, a physically consistent penalty, and first-moment evolution in a two-stage approach to recover pseudo-potentials and rotations.

Result: Demonstrates effectiveness in learning physical laws for non-gradient generalized diffusions through numerical experiments.

Conclusion: The proposed framework successfully identifies governing laws in complex non-gradient diffusion processes, validated by numerical results.

Abstract: This paper proposes a data-driven learning framework for identifying
governing laws of generalized diffusions with non-gradient components. By
combining energy dissipation laws with a physically consistent penalty and
first-moment evolution, we design a two-stage method to recover the
pseudo-potential and rotation in the pointwise orthogonal decomposition of a
class of non-gradient drifts in generalized diffusions. Our two-stage method is
applied to complex generalized diffusion processes including
dissipation-rotation dynamics, rough pseudo-potentials and noisy data.
Representative numerical experiments demonstrate the effectiveness of our
approach for learning physical laws in non-gradient generalized diffusions.

</details>


### [262] [Hybrid Physics-Machine Learning Models for Quantitative Electron Diffraction Refinements](https://arxiv.org/abs/2508.05908)
*Shreshth A. Malik,Tiarnan A. S. Doherty,Benjamin Colmey,Stephen J. Roberts,Yarin Gal,Paul A. Midgley*

Main category: physics.comp-ph

TL;DR: A hybrid physics-machine learning framework for high-fidelity electron microscopy simulations, combining differentiable physical models with neural networks to optimize experimental variables and achieve state-of-the-art refinement performance.


<details>
  <summary>Details</summary>
Motivation: Traditional methods struggle to model real-world experimental effects in electron microscopy simulations, limiting quantitative crystal structure refinements.

Method: Integrates differentiable physical simulations with neural networks, enabling gradient-based joint optimization of physical parameters and experimental variables.

Result: Achieves state-of-the-art refinement in 3D electron diffraction, accurately recovering atomic positions, thermal displacements, and thickness profiles.

Conclusion: Differentiable hybrid modeling is a powerful paradigm for quantitative electron microscopy, overcoming historical limitations posed by experimental complexities.

Abstract: High-fidelity electron microscopy simulations required for quantitative
crystal structure refinements face a fundamental challenge: while physical
interactions are well-described theoretically, real-world experimental effects
are challenging to model analytically. To address this gap, we present a novel
hybrid physics-machine learning framework that integrates differentiable
physical simulations with neural networks. By leveraging automatic
differentiation throughout the simulation pipeline, our method enables
gradient-based joint optimization of physical parameters and neural network
components representing experimental variables, offering superior scalability
compared to traditional second-order methods. We demonstrate this framework
through application to three-dimensional electron diffraction (3D-ED) structure
refinement, where our approach learns complex thickness distributions directly
from diffraction data rather than relying on simplified geometric models. This
method achieves state-of-the-art refinement performance across synthetic and
experimental datasets, recovering atomic positions, thermal displacements, and
thickness profiles with high fidelity. The modular architecture proposed can
naturally be extended to accommodate additional physical phenomena and extended
to other electron microscopy techniques. This establishes differentiable hybrid
modeling as a powerful new paradigm for quantitative electron microscopy, where
experimental complexities have historically limited analysis.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [263] [Neural Field-Based 3D Surface Reconstruction of Microstructures from Multi-Detector Signals in Scanning Electron Microscopy](https://arxiv.org/abs/2508.04728)
*Shuo Chen,Yijin Li,Xi Zheng,Guofeng Zhang*

Main category: eess.IV

TL;DR: NFH-SEM is a neural field-based hybrid method for 3D SEM reconstruction, eliminating manual calibration and shadow errors, validated on diverse samples.


<details>
  <summary>Details</summary>
Motivation: Conventional 2D SEM images lack 3D topography, and existing methods struggle with complex microstructures due to discrete representations and calibration needs.

Method: NFH-SEM uses multi-view, multi-detector 2D SEM images, fusing geometric and photometric data into a continuous neural field, with self-calibration and shadow disentanglement.

Result: High-fidelity reconstructions of complex samples like microstructures, pollen, and particle surfaces, showing precise detail and broad applicability.

Conclusion: NFH-SEM effectively addresses limitations of existing methods, enabling accurate 3D reconstruction of intricate microstructures without manual calibration.

Abstract: The scanning electron microscope (SEM) is a widely used imaging device in
scientific research and industrial applications. Conventional two-dimensional
(2D) SEM images do not directly reveal the three-dimensional (3D) topography of
micro samples, motivating the development of SEM 3D surface reconstruction
methods. However, reconstruction of complex microstructures remains challenging
for existing methods due to the limitations of discrete 3D representations, the
need for calibration with reference samples, and shadow-induced gradient
errors. Here, we introduce NFH-SEM, a neural field-based hybrid SEM 3D
reconstruction method that takes multi-view, multi-detector 2D SEM images as
input and fuses geometric and photometric information into a continuous neural
field representation. NFH-SEM eliminates the manual calibration procedures
through end-to-end self-calibration and automatically disentangles shadows from
SEM images during training, enabling accurate reconstruction of intricate
microstructures. We validate the effectiveness of NFH-SEM on real and simulated
datasets. Our experiments show high-fidelity reconstructions of diverse,
challenging samples, including two-photon lithography microstructures, peach
pollen, and silicon carbide particle surfaces, demonstrating precise detail and
broad applicability.

</details>


### [264] [Transformer-Based Explainable Deep Learning for Breast Cancer Detection in Mammography: The MammoFormer Framework](https://arxiv.org/abs/2508.06137)
*Ojonugwa Oluwafemi Ejiga Peter,Daniel Emakporuena,Bamidele Dayo Tunde,Maryam Abdulkarim,Abdullahi Bn Umar*

Main category: eess.IV

TL;DR: MammoFormer combines transformers and XAI to improve breast cancer detection in mammograms, outperforming CNNs with up to 13% better accuracy.


<details>
  <summary>Details</summary>
Motivation: Breast cancer detection is challenging due to subtle abnormalities and variability in expert interpretations. CNNs lack contextual processing and explainability, hindering clinical adoption.

Method: Developed MammoFormer, integrating transformers with multi-feature enhancement and XAI. Tested seven architectures (CNNs, ViT, Swin Transformer, ConvNext) and four enhancement techniques.

Result: Achieved up to 13% performance improvement, with ViT reaching 98.3% accuracy using AHE and Swin Transformer gaining 13% with HOG.

Conclusion: MammoFormer addresses clinical barriers by optimizing transformers, integrating XAI, and combining CNN reliability with global context modeling, outperforming CNNs.

Abstract: Breast cancer detection through mammography interpretation remains difficult
because of the minimal nature of abnormalities that experts need to identify
alongside the variable interpretations between readers. The potential of CNNs
for medical image analysis faces two limitations: they fail to process both
local information and wide contextual data adequately, and do not provide
explainable AI (XAI) operations that doctors need to accept them in clinics.
The researcher developed the MammoFormer framework, which unites
transformer-based architecture with multi-feature enhancement components and
XAI functionalities within one framework. Seven different architectures
consisting of CNNs, Vision Transformer, Swin Transformer, and ConvNext were
tested alongside four enhancement techniques, including original images,
negative transformation, adaptive histogram equalization, and histogram of
oriented gradients. The MammoFormer framework addresses critical clinical
adoption barriers of AI mammography systems through: (1) systematic
optimization of transformer architectures via architecture-specific feature
enhancement, achieving up to 13% performance improvement, (2) comprehensive
explainable AI integration providing multi-perspective diagnostic
interpretability, and (3) a clinically deployable ensemble system combining CNN
reliability with transformer global context modeling. The combination of
transformer models with suitable feature enhancements enables them to achieve
equal or better results than CNN approaches. ViT achieves 98.3% accuracy
alongside AHE while Swin Transformer gains a 13.0% advantage through HOG
enhancements

</details>


### [265] [Clinically-guided Data Synthesis for Laryngeal Lesion Detection](https://arxiv.org/abs/2508.06182)
*Chiara Baldini,Kaisar Kushibar,Richard Osuala,Simone Balocco,Oliver Diaz,Karim Lekadir,Leonardo S. Mattos*

Main category: eess.IV

TL;DR: The paper proposes a Latent Diffusion Model (LDM) with a ControlNet adapter to generate synthetic laryngeal endoscopic images, addressing data scarcity for CADx/e systems in otorhinolaryngology. Synthetic data improved lesion detection rates and was validated by experts.


<details>
  <summary>Details</summary>
Motivation: Specialized medical fields like otorhinolaryngology lack well-annotated datasets for CADx/e systems, hindering their development. Biopsy remains the gold standard but is costly and risky.

Method: A Latent Diffusion Model (LDM) coupled with a ControlNet adapter generates realistic laryngeal endoscopic image-annotation pairs, guided by clinical observations.

Result: Adding 10% synthetic data improved lesion detection rates by 9% (internal) and 22.1% (external). Experts struggled to distinguish synthetic from real images.

Conclusion: The method effectively addresses data scarcity, enhances CADx/e performance, and demonstrates the potential of synthetic data in medical diagnostics.

Abstract: Although computer-aided diagnosis (CADx) and detection (CADe) systems have
made significant progress in various medical domains, their application is
still limited in specialized fields such as otorhinolaryngology. In the latter,
current assessment methods heavily depend on operator expertise, and the high
heterogeneity of lesions complicates diagnosis, with biopsy persisting as the
gold standard despite its substantial costs and risks. A critical bottleneck
for specialized endoscopic CADx/e systems is the lack of well-annotated
datasets with sufficient variability for real-world generalization. This study
introduces a novel approach that exploits a Latent Diffusion Model (LDM)
coupled with a ControlNet adapter to generate laryngeal endoscopic
image-annotation pairs, guided by clinical observations. The method addresses
data scarcity by conditioning the diffusion process to produce realistic,
high-quality, and clinically relevant image features that capture diverse
anatomical conditions. The proposed approach can be leveraged to expand
training datasets for CADx/e models, empowering the assessment process in
laryngology. Indeed, during a downstream task of detection, the addition of
only 10% synthetic data improved the detection rate of laryngeal lesions by 9%
when the model was internally tested and 22.1% on out-of-domain external data.
Additionally, the realism of the generated images was evaluated by asking 5
expert otorhinolaryngologists with varying expertise to rate their confidence
in distinguishing synthetic from real images. This work has the potential to
accelerate the development of automated tools for laryngeal disease diagnosis,
offering a solution to data scarcity and demonstrating the applicability of
synthetic data in real-world scenarios.

</details>


### [266] [Advanced Deep Learning Techniques for Accurate Lung Cancer Detection and Classification](https://arxiv.org/abs/2508.06287)
*Mobarak Abumohsen,Enrique Costa-Montenegro,Silvia Garc√≠a-M√©ndez,Amani Yousef Owda,Majdi Owda*

Main category: eess.IV

TL;DR: A DenseNet201-based approach for lung cancer detection from CT images achieves 98.95% accuracy using Focal Loss, data augmentation, and regularization to address imbalanced data and overfitting.


<details>
  <summary>Details</summary>
Motivation: Lung cancer is a leading cause of death, and current CT-based detection methods suffer from high false positives due to small, imbalanced datasets.

Method: Uses DenseNet201 with Focal Loss, data augmentation, and regularization to improve accuracy and handle imbalanced data.

Result: Achieves 98.95% accuracy in lung cancer detection.

Conclusion: The proposed method effectively addresses dataset imbalance and overfitting, yielding high accuracy for lung cancer detection.

Abstract: Lung cancer (LC) ranks among the most frequently diagnosed cancers and is one
of the most common causes of death for men and women worldwide. Computed
Tomography (CT) images are the most preferred diagnosis method because of their
low cost and their faster processing times. Many researchers have proposed
various ways of identifying lung cancer using CT images. However, such
techniques suffer from significant false positives, leading to low accuracy.
The fundamental reason results from employing a small and imbalanced dataset.
This paper introduces an innovative approach for LC detection and
classification from CT images based on the DenseNet201 model. Our approach
comprises several advanced methods such as Focal Loss, data augmentation, and
regularization to overcome the imbalanced data issue and overfitting challenge.
The findings show the appropriateness of the proposal, attaining a promising
performance of 98.95% accuracy.

</details>


### [267] [Multivariate Fields of Experts](https://arxiv.org/abs/2508.06490)
*Stanislas Ducotterd,Michael Unser*

Main category: eess.IV

TL;DR: A new framework for learning image priors using multivariate fields of experts, outperforming univariate models and nearing deep-learning performance with fewer resources.


<details>
  <summary>Details</summary>
Motivation: To generalize existing fields of experts methods by incorporating multivariate potential functions for improved image prior learning.

Method: Uses multivariate potential functions via Moreau envelopes of the ‚Ñì‚àû-norm, applied to inverse problems like denoising and deblurring.

Result: Outperforms univariate models, approaches deep-learning performance, and is faster with fewer parameters and data.

Conclusion: The proposed model is effective, efficient, and interpretable, making it a strong alternative to deep-learning methods.

Abstract: We introduce the multivariate fields of experts, a new framework for the
learning of image priors. Our model generalizes existing fields of experts
methods by incorporating multivariate potential functions constructed via
Moreau envelopes of the $\ell_\infty$-norm. We demonstrate the effectiveness of
our proposal across a range of inverse problems that include image denoising,
deblurring, compressed-sensing magnetic-resonance imaging, and computed
tomography. The proposed approach outperforms comparable univariate models and
achieves performance close to that of deep-learning-based regularizers while
being significantly faster, requiring fewer parameters, and being trained on
substantially fewer data. In addition, our model retains a relatively high
level of interpretability due to its structured design.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [268] [Towards Transparent Ethical AI: A Roadmap for Trustworthy Robotic Systems](https://arxiv.org/abs/2508.05846)
*Ahmad Farooq,Kamran Iqbal*

Main category: cs.CY

TL;DR: The paper argues that transparency in AI decision-making is crucial for ethical and trustworthy robotic systems, proposing methods like explainable AI and standardized metrics to address challenges.


<details>
  <summary>Details</summary>
Motivation: To ensure ethical behavior in AI and robotics by emphasizing transparency as a key factor for accountability, informed consent, and debugging ethical algorithms.

Method: Explores technical, ethical, and practical challenges in transparency, proposing solutions like standardized metrics, explainable AI techniques, and user-friendly interfaces.

Result: Introduces a framework linking technical implementation with ethical considerations, analyzing transparency's impact on public trust, policies, and future research.

Conclusion: Transparency is fundamental for ethical AI design, contributing to responsible AI and robotics, and guiding future advancements.

Abstract: As artificial intelligence (AI) and robotics increasingly permeate society,
ensuring the ethical behavior of these systems has become paramount. This paper
contends that transparency in AI decision-making processes is fundamental to
developing trustworthy and ethically aligned robotic systems. We explore how
transparency facilitates accountability, enables informed consent, and supports
the debugging of ethical algorithms. The paper outlines technical, ethical, and
practical challenges in implementing transparency and proposes novel approaches
to enhance it, including standardized metrics, explainable AI techniques, and
user-friendly interfaces. This paper introduces a framework that connects
technical implementation with ethical considerations in robotic systems,
focusing on the specific challenges of achieving transparency in dynamic,
real-world contexts. We analyze how prioritizing transparency can impact public
trust, regulatory policies, and avenues for future research. By positioning
transparency as a fundamental element in ethical AI system design, we aim to
add to the ongoing discussion on responsible AI and robotics, providing
direction for future advancements in this vital field.

</details>


### [269] [Learning by Teaching: Engaging Students as Instructors of Large Language Models in Computer Science Education](https://arxiv.org/abs/2508.05979)
*Xinming Yang,Haasil Pujara,Jun Li*

Main category: cs.CY

TL;DR: A novel pedagogy inverts LLM use in CS education: students teach LLMs, improving engagement and performance.


<details>
  <summary>Details</summary>
Motivation: Address passive learning and over-reliance on LLMs in CS education by fostering active engagement.

Method: Students act as instructors, teaching LLMs with engineered knowledge gaps; implemented via the Socrates system.

Result: Statistically significant improvements in student performance compared to historical cohorts.

Conclusion: The framework is practical and cost-effective for deepening student engagement and mastery using LLMs.

Abstract: While Large Language Models (LLMs) are often used as virtual tutors in
computer science (CS) education, this approach can foster passive learning and
over-reliance. This paper presents a novel pedagogical paradigm that inverts
this model: students act as instructors who must teach an LLM to solve
problems. To facilitate this, we developed strategies for designing questions
with engineered knowledge gaps that only a student can bridge, and we introduce
Socrates, a system for deploying this method with minimal overhead. We
evaluated our approach in an undergraduate course and found that this
active-learning method led to statistically significant improvements in student
performance compared to historical cohorts. Our work demonstrates a practical,
cost-effective framework for using LLMs to deepen student engagement and
mastery.

</details>


### [270] [Dimensional Characterization and Pathway Modeling for Catastrophic AI Risks](https://arxiv.org/abs/2508.06411)
*Ze Shen Chin*

Main category: cs.CY

TL;DR: The paper provides a structured framework for analyzing AI catastrophic risks, categorizing them across seven dimensions and modeling risk pathways to identify mitigation strategies.


<details>
  <summary>Details</summary>
Motivation: To address the lack of a comprehensive framework for understanding and mitigating AI catastrophic risks.

Method: Characterizes six AI risks across seven dimensions and models risk pathways from hazard to harm.

Result: Offers a systematic approach for risk identification and scenario-specific interventions.

Conclusion: The framework provides a structured, actionable foundation for managing AI risks.

Abstract: Although discourse around the risks of Artificial Intelligence (AI) has
grown, it often lacks a comprehensive, multidimensional framework, and concrete
causal pathways mapping hazard to harm. This paper aims to bridge this gap by
examining six commonly discussed AI catastrophic risks: CBRN, cyber offense,
sudden loss of control, gradual loss of control, environmental risk, and
geopolitical risk. First, we characterize these risks across seven key
dimensions, namely intent, competency, entity, polarity, linearity, reach, and
order. Next, we conduct risk pathway modeling by mapping step-by-step
progressions from the initial hazard to the resulting harms. The dimensional
approach supports systematic risk identification and generalizable mitigation
strategies, while risk pathway models help identify scenario-specific
interventions. Together, these methods offer a more structured and actionable
foundation for managing catastrophic AI risks across the value chain.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [271] [Ensemble-Based Graph Representation of fMRI Data for Cognitive Brain State Classification](https://arxiv.org/abs/2508.06118)
*Daniil Vlasenko,Vadim Ushakov,Alexey Zaikin,Denis Zakharov*

Main category: q-bio.NC

TL;DR: An ensemble-based graph representation method for fMRI data achieves high accuracy in binary brain-state classification, outperforming traditional correlation-based graphs.


<details>
  <summary>Details</summary>
Motivation: The high dimensionality and noise in neuroimaging data make classifying human cognitive brain states challenging, necessitating innovative methods.

Method: Proposes an ensemble-based graph representation for fMRI data, using edge weights derived from posterior probabilities of cognitive states. Applied to seven HCP tasks.

Result: Achieved 97.07% to 99.74% accuracy with logistic regression. Ensemble graphs outperformed correlation-based graphs in GNN classification.

Conclusion: Ensemble graphs provide richer topological information, enhance brain-state discrimination, and are adaptable to other tasks and modalities.

Abstract: Understanding and classifying human cognitive brain states based on
neuroimaging data remains one of the foremost and most challenging problems in
neuroscience, owing to the high dimensionality and intrinsic noise of the
signals. In this work, we propose an ensemble-based graph representation method
of functional magnetic resonance imaging (fMRI) data for the task of binary
brain-state classification. Our method builds the graph by leveraging multiple
base machine-learning models: each edge weight reflects the difference in
posterior probabilities between two cognitive states, yielding values in the
range [-1, 1] that encode confidence in a given state. We applied this approach
to seven cognitive tasks from the Human Connectome Project (HCP 1200 Subject
Release), including working memory, gambling, motor activity, language, social
cognition, relational processing, and emotion processing. Using only the mean
incident edge weights of the graphs as features, a simple logistic-regression
classifier achieved average accuracies from 97.07% to 99.74%. We also compared
our ensemble graphs with classical correlation-based graphs in a classification
task with a graph neural network (GNN). In all experiments, the highest
classification accuracy was obtained with ensemble graphs. These results
demonstrate that ensemble graphs convey richer topological information and
enhance brain-state discrimination. Our approach preserves edge-level
interpretability of the fMRI graph representation, is adaptable to multiclass
and regression tasks, and can be extended to other neuroimaging modalities and
pathological-state classification.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [272] [SHACL Validation in the Presence of Ontologies: Semantics and Rewriting Techniques](https://arxiv.org/abs/2507.12286)
*Anouk Oudshoorn,Magdalena Ortiz,Mantas Simkus*

Main category: cs.LO

TL;DR: The paper proposes a semantics for SHACL validation with ontologies using core universal models, addressing the semantic gap between SHACL and OWL. It provides techniques for model construction and validation, and analyzes computational complexity.


<details>
  <summary>Details</summary>
Motivation: The semantic gap between SHACL (closed-world) and OWL (open-world) poses challenges for combining these formalisms. The paper aims to bridge this gap for effective RDF data management.

Method: The authors advocate core universal models for SHACL validation with ontologies, focusing on Horn-ALCHIQ. They develop a rewriting technique to reduce validation to standard SHACL validation.

Result: The technique enables SHACL validation in the presence of ontologies. Complexity analysis shows EXPTIME-completeness in general and PTIME-completeness in data complexity.

Conclusion: The proposed approach successfully addresses the semantic gap, providing a practical solution for combining SHACL and OWL, though computational complexity remains a challenge.

Abstract: SHACL and OWL are two prominent W3C standards for managing RDF data. These
languages share many features, but they have one fundamental difference: OWL,
designed for inferring facts from incomplete data, makes the open-world
assumption, whereas SHACL is a constraint language that treats the data as
complete and must be validated under the closed-world assumption. The
combination of both formalisms is very appealing and has been called for, but
their semantic gap is a major challenge, semantically and computationally. In
this paper, we advocate a semantics for SHACL validation in the presence of
ontologies based on core universal models. We provide a technique for
constructing these models for ontologies in the rich data-tractable description
logic Horn-ALCHIQ. Furthermore, we use a finite representation of this model to
develop a rewriting technique that reduces SHACL validation in the presence of
ontologies to standard validation. Finally, we study the complexity of SHACL
validation in the presence of ontologies, and show that even very simple
ontologies make the problem EXPTIME-complete, and PTIME-complete in data
complexity.

</details>


### [273] [Basic interactive algorithms: Preview](https://arxiv.org/abs/2508.05798)
*Yuri Gurevich*

Main category: cs.LO

TL;DR: The paper previews an upcoming work on axiomatizing basic interactive algorithms, contrasting classical and modern algorithmic notions, and linking nondeterministic/probabilistic algorithms to basic algorithms with oracles.


<details>
  <summary>Details</summary>
Motivation: To extend the axiomatization of classical algorithms to modern interactive ones and clarify the differences between the original and physical versions of the Church-Turing thesis.

Method: Proposes viewing nondeterministic, probabilistic, and quantum algorithms as basic algorithms augmented with oracles.

Result: Illustrates how diverse algorithmic classes can be unified under the framework of basic algorithms with oracles.

Conclusion: The work aims to provide a foundational axiomatization for interactive algorithms, bridging classical and modern computational paradigms.

Abstract: This dialog paper offers a preview and provides a foretaste of an upcoming
work on the axiomatization of basic interactive algorithms.
  The modern notion of algorithm was elucidated in the 1930s--1950s. It was
axiomatized a quarter of a century ago as the notion of ``sequential
algorithm'' or ``classical algorithm''; we prefer to call it ``basic algorithm"
now. The axiomatization was used to show that for every basic algorithm there
is a behaviorally equivalent abstract state machine. It was also used to prove
the Church-Turing thesis as it has been understood by the logicians.
  Starting from the 1960s, the notion of algorithm has expanded --
probabilistic algorithms, quantum algorithms, etc. -- prompting introduction of
a much more ambitious version of the Church-Turing thesis commonly known as the
``physical thesis.'' We emphasize the difference between the two versions of
the Church-Turing thesis and illustrate how nondeterministic and probabilistic
algorithms can be viewed as basic algorithms with appropriate oracles. The same
view applies to quantum circuit algorithms and many other classes of
algorithms.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [274] [Automated Visualization Makeovers with LLMs](https://arxiv.org/abs/2508.05637)
*Siddharth Gangwar,David A. Selby,Sebastian J. Vollmer*

Main category: cs.HC

TL;DR: The paper explores using multi-modal LLMs to provide feedback for improving data visualizations, focusing on prompt engineering and user education rather than generating plots from scratch.


<details>
  <summary>Details</summary>
Motivation: Data visualization is often not taught in data science curricula, and visualisation makeovers are community-driven. The paper investigates if LLMs can automate this feedback process.

Method: The system uses a pre-trained LLM, primed with visualization best practices, to critique and improve plots based on user input or code. It relies on prompt engineering and latent knowledge in the LLM.

Result: A quantitative evaluation measures the LLM's sensitivity to plotting issues across chart types. The tool is made available as a self-hosted applet with a web interface.

Conclusion: LLMs can semi-automate visualization feedback, offering a scalable way to educate users on improving their plots.

Abstract: Making a good graphic that accurately and efficiently conveys the desired
message to the audience is both an art and a science, typically not taught in
the data science curriculum. Visualisation makeovers are exercises where the
community exchange feedback to improve charts and data visualizations. Can
multi-modal large language models (LLMs) emulate this task? Given a plot in the
form of an image file, or the code used to generate it, an LLM, primed with a
list of visualization best practices, is employed to semi-automatically
generate constructive criticism to produce a better plot. Our system is centred
around prompt engineering of a pre-trained model, relying on a combination of
userspecified guidelines and any latent knowledge of data visualization
practices that might lie within an LLMs training corpus. Unlike other works,
the focus is not on generating valid visualization scripts from raw data or
prompts, but on educating the user how to improve their existing data
visualizations according to an interpretation of best practices. A quantitative
evaluation is performed to measure the sensitivity of the LLM agent to various
plotting issues across different chart types. We make the tool available as a
simple self-hosted applet with an accessible Web interface.

</details>


### [275] [Modeling Interactive Narrative Systems: A Formal Approach](https://arxiv.org/abs/2508.05653)
*Jules Clerc,Domitile Lourdeaux,Mohamed Sallak,Johann Barbier,Marc Ravaine*

Main category: cs.HC

TL;DR: The paper introduces a formal representation framework for Interactive Narrative Systems (INS) to unify research efforts and improve system analysis and comparison.


<details>
  <summary>Details</summary>
Motivation: The fragmented research and diverse representations in INS hinder progress. A unified framework is needed to enhance collaboration and evaluation.

Method: The paper proposes a formal representation framework with consistent vocabulary and modeling structure, validated through experiments on the 'Little Red Riding Hood' scenario.

Result: The framework successfully improves the analysis, description, and comparison of INS properties, as demonstrated in the experimental validation.

Conclusion: The proposed formalism fosters coherence in INS research and provides a methodology for formal representation, aiding future collaboration and evaluation.

Abstract: Interactive Narrative Systems (INS) have revolutionized digital experiences
by empowering users to actively shape their stories, diverging from traditional
passive storytelling. However, the field faces challenges due to fragmented
research efforts and diverse system representations. This paper introduces a
formal representation framework for INS, inspired by diverse approaches from
the state of the art. By providing a consistent vocabulary and modeling
structure, the framework facilitates the analysis, the description and
comparison of INS properties. Experimental validations on the "Little Red
Riding Hood" scenario highlight the usefulness of the proposed formalism and
its impact on improving the evaluation of INS. This work aims to foster
collaboration and coherence within the INS research community by proposing a
methodology for formally representing these systems.

</details>


### [276] [Do Ethical AI Principles Matter to Users? A Large-Scale Analysis of User Sentiment and Satisfaction](https://arxiv.org/abs/2508.05913)
*Stefan Pasch,Min Chul Cha*

Main category: cs.HC

TL;DR: The study explores how ethical AI principles impact user satisfaction by analyzing 100,000+ reviews, finding positive associations across seven ethical dimensions, with variations based on user and product types.


<details>
  <summary>Details</summary>
Motivation: To empirically assess whether ethical AI principles (fairness, transparency, robustness) are recognized and valued by users, and how they influence satisfaction.

Method: Analyzed user reviews of AI products from G2 using transformer-based language models to measure sentiment across seven ethical dimensions from the EU Ethics Guidelines.

Result: All seven ethical dimensions positively correlate with user satisfaction, with stronger effects for non-technical users and end-user applications. Technical users focus on system-level concerns, while non-technical users emphasize human-centric issues.

Conclusion: Ethical AI design is crucial for user satisfaction, with contextual differences (user roles, product types) needing consideration.

Abstract: As AI systems become increasingly embedded in organizational workflows and
consumer applications, ethical principles such as fairness, transparency, and
robustness have been widely endorsed in policy and industry guidelines.
However, there is still scarce empirical evidence on whether these principles
are recognized, valued, or impactful from the perspective of users. This study
investigates the link between ethical AI and user satisfaction by analyzing
over 100,000 user reviews of AI products from G2. Using transformer-based
language models, we measure sentiment across seven ethical dimensions defined
by the EU Ethics Guidelines for Trustworthy AI. Our findings show that all
seven dimensions are positively associated with user satisfaction. Yet, this
relationship varies systematically across user and product types. Technical
users and reviewers of AI development platforms more frequently discuss
system-level concerns (e.g., transparency, data governance), while
non-technical users and reviewers of end-user applications emphasize
human-centric dimensions (e.g., human agency, societal well-being). Moreover,
the association between ethical AI and user satisfaction is significantly
stronger for non-technical users and end-user applications across all
dimensions. Our results highlight the importance of ethical AI design from
users' perspectives and underscore the need to account for contextual
differences across user roles and product types.

</details>


### [277] [ThematicPlane: Bridging Tacit User Intent and Latent Spaces for Image Generation](https://arxiv.org/abs/2508.06065)
*Daniel Lee,Nikhil Sharma,Donghoon Shin,DaEun Choi,Harsh Sharma,Jeonghwan Kim,Heng Ji*

Main category: cs.HC

TL;DR: ThematicPlane is a system for intuitive, semantics-driven interaction in generative AI, helping users align outputs with creative intent by manipulating high-level concepts like mood or style.


<details>
  <summary>Details</summary>
Motivation: Aligning generative AI outputs with nuanced creative intent is challenging, especially for non-experts, due to limitations in existing tools.

Method: ThematicPlane introduces an interactive thematic design plane for navigating semantic concepts, tested in a study with 6 participants.

Result: Users engaged in creative workflows, leveraging unexpected results, but differing expectations highlighted a need for explainable controls.

Conclusion: ThematicPlane supports expressive, iterative workflows and suggests directions for more intuitive generative design tools.

Abstract: Generative AI has made image creation more accessible, yet aligning outputs
with nuanced creative intent remains challenging, particularly for non-experts.
Existing tools often require users to externalize ideas through prompts or
references, limiting fluid exploration. We introduce ThematicPlane, a system
that enables users to navigate and manipulate high-level semantic concepts
(e.g., mood, style, or narrative tone) within an interactive thematic design
plane. This interface bridges the gap between tacit creative intent and system
control. In our exploratory study (N=6), participants engaged in divergent and
convergent creative modes, often embracing unexpected results as inspiration or
iteration cues. While they grounded their exploration in familiar themes,
differing expectations of how themes mapped to outputs revealed a need for more
explainable controls. Overall, ThematicPlane fosters expressive, iterative
workflows and highlights new directions for intuitive, semantics-driven
interaction in generative design tools.

</details>


### [278] [REFS: Robust EEG feature selection with missing multi-dimensional annotation for emotion recognition](https://arxiv.org/abs/2508.05933)
*Xueyuan Xu,Wenjia Dong,Fulin Wei,Li Zhuo*

Main category: cs.HC

TL;DR: A novel EEG feature selection method for missing multi-dimensional emotion recognition, using adaptive orthogonal non-negative matrix factorization and regularization techniques, outperforms existing methods in robustness.


<details>
  <summary>Details</summary>
Motivation: Challenges in multi-dimensional emotion recognition due to high-dimensional EEG features, small sample sizes, missing labels, and variability in emotion perception.

Method: Proposes adaptive orthogonal non-negative matrix factorization for label reconstruction and least squares regression with manifold learning and redundancy minimization for feature selection.

Result: Outperforms thirteen advanced feature selection methods on datasets DREAMER, DEAP, and HDED.

Conclusion: The method effectively addresses challenges in EEG-based emotion recognition, offering robust performance despite missing data.

Abstract: The affective brain-computer interface is a crucial technology for affective
interaction and emotional intelligence, emerging as a significant area of
research in the human-computer interaction. Compared to single-type features,
multi-type EEG features provide a multi-level representation for analyzing
multi-dimensional emotions. However, the high dimensionality of multi-type EEG
features, combined with the relatively small number of high-quality EEG
samples, poses challenges such as classifier overfitting and suboptimal
real-time performance in multi-dimensional emotion recognition. Moreover,
practical applications of affective brain-computer interface frequently
encounters partial absence of multi-dimensional emotional labels due to the
open nature of the acquisition environment, and ambiguity and variability in
individual emotion perception. To address these challenges, this study proposes
a novel EEG feature selection method for missing multi-dimensional emotion
recognition. The method leverages adaptive orthogonal non-negative matrix
factorization to reconstruct the multi-dimensional emotional label space
through second-order and higher-order correlations, which could reduce the
negative impact of missing values and outliers on label reconstruction.
Simultaneously, it employs least squares regression with graph-based manifold
learning regularization and global feature redundancy minimization
regularization to enable EEG feature subset selection despite missing
information, ultimately achieving robust EEG-based multi-dimensional emotion
recognition. Simulation experiments on three widely used multi-dimensional
emotional datasets, DREAMER, DEAP and HDED, reveal that the proposed method
outperforms thirteen advanced feature selection methods in terms of robustness
for EEG emotional feature selection.

</details>


### [279] [ASLSL: Adaptive shared latent structure learning with incomplete multi-modal physiological data for multi-dimensional emotional feature selection](https://arxiv.org/abs/2508.05934)
*Xueyuan Xu,Tianze Yu,Wenjia Dong,Fulin Wei,Li Zhuo*

Main category: cs.HC

TL;DR: The paper proposes ASLSL, a method for feature selection in incomplete multi-modal physiological signals to improve emotion recognition by addressing missing data and noise.


<details>
  <summary>Details</summary>
Motivation: Multi-modal physiological signals for emotion recognition often suffer from high-dimensional, noisy, and incomplete data, leading to poor classifier performance.

Method: ASLSL uses adaptive shared latent structure learning to create a common latent space for incomplete multi-modal signals and emotional labels, reducing missing data impact.

Result: ASLSL outperforms 17 other feature selection methods on DEAP and DREAMER datasets.

Conclusion: ASLSL effectively handles incomplete multi-modal data, improving emotion recognition performance.

Abstract: Recently, multi-modal physiological signals based emotion recognition has
garnered increasing attention in the field of brain-computer interfaces.
Nevertheness, the associated multi-modal physiological features are often
high-dimensional and inevitably include irrelevant, redundant, and noisy
representation, which can easily lead to overfitting, poor performance, and
high computational complexity in emotion classifiers. Feature selection has
been widely applied to address these challenges. However, previous studies
generally assumed that multi-modal physiological data are complete, whereas in
reality, the data are often incomplete due to the openness of the acquisition
and operational environment. For example, a part of samples are available in
several modalities but not in others. To address this issue, we propose a novel
method for incomplete multi-modal physiological signal feature selection called
adaptive shared latent structure learning (ASLSL). Based on the property that
similar features share similar emotional labels, ASLSL employs adaptive shared
latent structure learning to explore a common latent space shared for
incomplete multi-modal physiological signals and multi-dimensional emotional
labels, thereby mitigating the impact of missing information and mining
consensus information. Two most popular multi-modal physiological emotion
datasets (DEAP and DREAMER) with multi-dimensional emotional labels were
utilized to compare the performance between compare ASLSL and seventeen feature
selection methods. Comprehensive experimental results on these datasets
demonstrate the effectiveness of ASLSL.

</details>


### [280] [Hand by Hand: LLM Driving EMS Assistant for Operational Skill Learning](https://arxiv.org/abs/2508.06000)
*Wei Xiang,Ziyue Lei,Haoyuan Che,Fangyuan Ye,Xueting Wu,Lingyun Sun*

Main category: cs.HC

TL;DR: The paper explores LLM-driven kinesthetic assistance for operational skill learning, introducing FlightAxis, a tool combining LLM and EMS for flight training, showing high user acceptance and improved performance.


<details>
  <summary>Details</summary>
Motivation: Current LLM training lacks kinesthetic feedback, a key aspect of operational skill learning. This work aims to bridge this gap by integrating LLM with EMS for hands-on training.

Method: Developed FlightAxis, using an 'Align-Analyze-Adjust' strategy to combine LLM with EMS for guiding forearm movements in simulated flight tasks.

Result: High user acceptance of LLM-mediated body control, reduced task completion times, and enhanced awareness of operation flaws without increasing perceived load.

Conclusion: Demonstrates the potential of kinesthetic LLM training for operational skill acquisition, improving engagement and performance.

Abstract: Operational skill learning, inherently physical and reliant on hands-on
practice and kinesthetic feedback, has yet to be effectively replicated in
large language model (LLM)-supported training. Current LLM training assistants
primarily generate customized textual feedback, neglecting the crucial
kinesthetic modality. This gap derives from the textual and uncertain nature of
LLMs, compounded by concerns on user acceptance of LLM driven body control. To
bridge this gap and realize the potential of collaborative human-LLM action,
this work explores human experience of LLM driven kinesthetic assistance.
Specifically, we introduced an "Align-Analyze-Adjust" strategy and developed
FlightAxis, a tool that integrates LLM with Electrical Muscle Stimulation (EMS)
for flight skill acquisition, a representative operational skill domain.
FlightAxis learns flight skills from manuals and guides forearm movements
during simulated flight tasks. Our results demonstrate high user acceptance of
LLM-mediated body control and significantly reduced task completion times.
Crucially, trainees reported that this kinesthetic assistance enhanced their
awareness of operation flaws and fostered increased engagement in the training
process, rather than relieving perceived load. This work demonstrated the
potential of kinesthetic LLM training in operational skill acquisition.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [281] [Position: Intelligent Coding Systems Should Write Programs with Justifications](https://arxiv.org/abs/2508.06017)
*Xiangzhe Xu,Shiwei Feng,Zian Su,Chengpeng Wang,Xiangyu Zhang*

Main category: cs.SE

TL;DR: The paper advocates for AI-driven coding systems to provide clear justifications for their decisions to enhance trust and usability, especially for non-expert users. It proposes neuro-symbolic approaches to ensure cognitive alignment and semantic faithfulness in these justifications.


<details>
  <summary>Details</summary>
Motivation: Addressing trust and usability concerns in AI-driven coding systems, particularly for non-expert users who lack the ability to inspect low-level implementations.

Method: Proposes neuro-symbolic approaches where symbolic constraints guide model behavior during training, and neural representations enrich program semantics for automated consistency checks.

Result: Identifies limitations in existing methods (formal verification, static analysis, post-hoc explainability) and suggests neuro-symbolic techniques as a solution.

Conclusion: Neuro-symbolic approaches can bridge the gap between model reasoning and user understanding, improving trust and usability in AI-driven coding systems.

Abstract: Intelligent coding systems are transforming software development by enabling
users to specify code behavior in natural language. However, the opaque
decision-making of AI-driven coders raises trust and usability concerns,
particularly for non-expert users who cannot inspect low-level implementations.
We argue that these systems should not only generate code but also produce
clear, consistent justifications that bridge model reasoning and user
understanding. To this end, we identify two critical justification
properties-cognitive alignment and semantic faithfulness-and highlight the
limitations of existing methods, including formal verification, static
analysis, and post-hoc explainability. We advocate exploring neuro-symbolic
approaches for justification generation, where symbolic constraints guide model
behavior during training and program semantics are enriched through neural
representations, enabling automated consistency checks at inference time.

</details>


### [282] [Empirical Evaluation of AI-Assisted Software Package Selection: A Knowledge Graph Approach](https://arxiv.org/abs/2508.05693)
*Siamak Farshidi,Amir Saberhabibi,Behbod Eskafi,Niloofar Nikfarjam,Sadegh Eskandari,Slinger Jansen,Michel Chaudron,Bedir Tekinerdogan*

Main category: cs.SE

TL;DR: The paper proposes a data-driven framework, PySelect, for selecting third-party Python packages using MCDM principles, AI-assisted intent modeling, and empirical data, improving over generative AI baselines.


<details>
  <summary>Details</summary>
Motivation: Challenges in selecting third-party software packages due to lack of transparent evidence and over-reliance on popularity in generative AI tools.

Method: Formulates package selection as an MCDM problem, collects metadata from GitHub, PyPI, and Stack Overflow, and implements PySelect for decision support.

Result: High data extraction precision, better recommendations than generative AI, and positive user feedback on usefulness and ease of use.

Conclusion: The framework is scalable, interpretable, and reproducible, supporting evidence-based software selection.

Abstract: Selecting third-party software packages in open-source ecosystems like Python
is challenging due to the large number of alternatives and limited transparent
evidence for comparison. Generative AI tools are increasingly used in
development workflows, but their suggestions often overlook dependency
evaluation, emphasize popularity over suitability, and lack reproducibility.
This creates risks for projects that require transparency, long-term
reliability, maintainability, and informed architectural decisions. This study
formulates software package selection as a Multi-Criteria Decision-Making
(MCDM) problem and proposes a data-driven framework for technology evaluation.
Automated data pipelines continuously collect and integrate software metadata,
usage trends, vulnerability information, and developer sentiment from GitHub,
PyPI, and Stack Overflow. These data are structured into a decision model
representing relationships among packages, domain features, and quality
attributes. The framework is implemented in PySelect, a decision support system
that uses large language models to interpret user intent and query the model to
identify contextually appropriate packages. The approach is evaluated using
798,669 Python scripts from 16,887 GitHub repositories and a user study based
on the Technology Acceptance Model. Results show high data extraction
precision, improved recommendation quality over generative AI baselines, and
positive user evaluations of usefulness and ease of use. This work introduces a
scalable, interpretable, and reproducible framework that supports
evidence-based software selection using MCDM principles, empirical data, and
AI-assisted intent modeling.

</details>


### [283] [Klear-CodeTest: Scalable Test Case Generation for Code Reinforcement Learning](https://arxiv.org/abs/2508.05710)
*Jia Fu,Xinyu Yang,Hongzhi Zhang,Yahui Liu,Jingyuan Zhang,Qi Wang,Fuzheng Zhang,Guorui Zhou*

Main category: cs.SE

TL;DR: Klear-CodeTest is a test case synthesis framework for training LLMs in code reinforcement learning, featuring a Generator-Validation framework and a security sandbox for reliable test case generation and execution.


<details>
  <summary>Details</summary>
Motivation: High-quality test cases are essential for training LLMs in code reinforcement learning, but synthesizing them is challenging.

Method: Uses a Generator-Validation (G-V) framework for test case synthesis, including regular and corner cases, and a multi-layered security sandbox for safe execution.

Result: Demonstrates significant improvements in model performance and training stability through comprehensive experiments.

Conclusion: Klear-CodeTest provides a reliable and effective solution for generating high-quality test cases, enhancing LLM training in code reinforcement learning.

Abstract: Precise, correct feedback is crucial for effectively training large language
models (LLMs) in code reinforcement learning. However, synthesizing
high-quality test cases remains a profoundly challenging and unsolved problem.
In this work, we present Klear-CodeTest, a comprehensive test case synthesis
framework featuring rigorous verification to ensure quality and reliability of
test cases. Our approach achieves broad coverage of programming problems via a
novel Generator-Validation (G-V) framework, ensuring correctness through a
consistency validation mechanism that verifies outputs against gold solutions.
The proposed G-V framework generates comprehensive test cases including both
regular and corner cases, enhancing test coverage and discriminative power for
solution correctness assessment in code reinforcement learning. In addition, we
design a multi-layered security sandbox system optimized for online
verification platforms, guaranteeing safe and reliable code execution. Through
comprehensive experiments, we demonstrate the effectiveness of our curated
dataset, showing significant improvements in model performance and training
stability. The source codes, curated dataset and sandbox system are available
at: https://github.com/Kwai-Klear/CodeTest.

</details>


### [284] [AI-Guided Exploration of Large-Scale Codebases](https://arxiv.org/abs/2508.05799)
*Yoseph Berhanu Alebachew*

Main category: cs.SE

TL;DR: A hybrid approach integrating reverse engineering and LLM-guided visual exploration enhances code comprehension by combining UML-based visualization, dynamic interfaces, and collaborative features.


<details>
  <summary>Details</summary>
Motivation: Traditional tools lack interactivity and contextual integration, while LLMs alone are ungrounded. A hybrid solution is needed for better code understanding.

Method: Combines deterministic reverse engineering with LLM-guided, intent-aware visual exploration, including UML-based visualization and dynamic interfaces.

Result: A prototype for Java demonstrates feasibility, improving navigation and understanding of complex codebases.

Conclusion: The approach lays groundwork for intelligent, interactive environments aligned with developer cognition and collaboration.

Abstract: Understanding large-scale, complex software systems is a major challenge for
developers, who spend a significant portion of their time on program
comprehension. Traditional tools such as static visualizations and reverse
engineering techniques provide structural insights but often lack
interactivity, adaptability, and integration with contextual information.
Recent advancements in large language models (LLMs) offer new opportunities to
enhance code exploration workflows, yet their lack of grounding and integration
with structured views limits their effectiveness. This work introduces a hybrid
approach that integrates deterministic reverse engineering with LLM-guided,
intent-aware visual exploration. The proposed system combines UML-based
visualization, dynamic user interfaces, historical context, and collaborative
features into an adaptive tool for code comprehension. By interpreting user
queries and interaction patterns, the LLM helps developers navigate and
understand complex codebases more effectively. A prototype implementation for
Java demonstrates the feasibility of this approach. Future work includes
empirical evaluation, scaling to polyglot systems, and exploring GUI-driven LLM
interaction models. This research lays the groundwork for intelligent,
interactive environments that align with developer cognition and collaborative
workflows.

</details>


### [285] [Enhancing Software Vulnerability Detection Through Adaptive Test Input Generation Using Genetic Algorithm](https://arxiv.org/abs/2508.05923)
*Yanusha Mehendran,Maolin Tang,Yi Lu*

Main category: cs.SE

TL;DR: A genetic algorithm-based method for test input generation improves software vulnerability detection by integrating genetic operators and adaptive learning, achieving significant coverage gains over benchmarks.


<details>
  <summary>Details</summary>
Motivation: Software complexity outpaces traditional detection methods, necessitating innovative approaches to enhance vulnerability detection.

Method: Uses genetic algorithm with crossover operators and adaptive feedback to evolve structurally valid test cases, balancing exploration and exploitation.

Result: Substantial improvements in coverage (e.g., 166.0% in branch coverage) across nine JSON-processing libraries.

Conclusion: The method effectively detects deeper vulnerabilities, offering a scalable and adaptive solution for security testing.

Abstract: Software vulnerabilities continue to undermine the reliability and security
of modern systems, particularly as software complexity outpaces the
capabilities of traditional detection methods. This study introduces a genetic
algorithm-based method for test input generation that innovatively integrates
genetic operators and adaptive learning to enhance software vulnerability
detection. A key contribution is the application of the crossover operator,
which facilitates exploration by searching across a broader space of potential
test inputs. Complementing this, an adaptive feedback mechanism continuously
learns from the system's execution behavior and dynamically guides input
generation toward promising areas of the input space. Rather than relying on
fixed or randomly selected inputs, the approach evolves a population of
structurally valid test cases using feedback-driven selection, enabling deeper
and more effective code traversal. This strategic integration of exploration
and exploitation ensures that both diverse and targeted test inputs are
developed over time. Evaluation was conducted across nine open-source
JSON-processing libraries. The proposed method achieved substantial
improvements in coverage compared to a benchmark evolutionary fuzzing method,
with average gains of 39.8% in class coverage, 62.4% in method coverage, 105.0%
in line coverage, 114.0% in instruction coverage, and 166.0% in branch
coverage. These results highlight the method's capacity to detect deeper and
more complex vulnerabilities, offering a scalable and adaptive solution to
software security testing.

</details>


### [286] [Impact-driven Context Filtering For Cross-file Code Completion](https://arxiv.org/abs/2508.05970)
*Yanzhou Li,Shangqing Liu,Kangjie Chen,Tianwei Zhang,Yang Liu*

Main category: cs.SE

TL;DR: The paper introduces CODEFILTER, an adaptive retrieval context filtering framework for repository-level code completion, improving accuracy and efficiency by filtering irrelevant retrieved code chunks.


<details>
  <summary>Details</summary>
Motivation: To understand and mitigate the negative impact of irrelevant retrieved code chunks in retrieval-augmented generation (RAG) for code completion.

Method: Introduces a likelihood-based metric to evaluate retrieved code chunks, constructs a labeled dataset, and develops CODEFILTER to filter harmful contexts.

Result: CODEFILTER improves completion accuracy, reduces input prompt length, and enhances computational efficiency on benchmarks like RepoEval and CrossCodeLongEval.

Conclusion: CODEFILTER enhances the accuracy, efficiency, and generalizability of repository-level code completion by filtering negative contexts.

Abstract: Retrieval-augmented generation (RAG) has recently demonstrated considerable
potential for repository-level code completion, as it integrates cross-file
knowledge with in-file preceding code to provide comprehensive contexts for
generation. To better understand the contribution of the retrieved cross-file
contexts, we introduce a likelihood-based metric to evaluate the impact of each
retrieved code chunk on the completion. Our analysis reveals that, despite
retrieving numerous chunks, only a small subset positively contributes to the
completion, while some chunks even degrade performance. To address this issue,
we leverage this metric to construct a repository-level dataset where each
retrieved chunk is labeled as positive, neutral, or negative based on its
relevance to the target completion. We then propose an adaptive retrieval
context filtering framework, CODEFILTER, trained on this dataset to mitigate
the harmful effects of negative retrieved contexts in code completion.
Extensive evaluation on the RepoEval and CrossCodeLongEval benchmarks
demonstrates that CODEFILTER consistently improves completion accuracy compared
to approaches without filtering operations across various tasks. Additionally,
CODEFILTER significantly reduces the length of the input prompt, enhancing
computational efficiency while exhibiting strong generalizability across
different models. These results underscore the potential of CODEFILTER to
enhance the accuracy, efficiency, and attributability of repository-level code
completion.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [287] [Integrating Vision Foundation Models with Reinforcement Learning for Enhanced Object Interaction](https://arxiv.org/abs/2508.05838)
*Ahmad Farooq,Kamran Iqbal*

Main category: cs.RO

TL;DR: Integration of vision foundation models (SAM and YOLOv5) with reinforcement learning (PPO) improves object interaction and navigation in simulated environments.


<details>
  <summary>Details</summary>
Motivation: To enhance robotic agents' object interaction capabilities by leveraging advanced perception models.

Method: Combines Segment Anything Model (SAM) and YOLOv5 with PPO in the AI2-THOR simulation environment.

Result: 68% increase in cumulative reward, 52.5% higher interaction success, and 33% better navigation efficiency.

Conclusion: Integrating foundation models with reinforcement learning advances robotic task performance.

Abstract: This paper presents a novel approach that integrates vision foundation models
with reinforcement learning to enhance object interaction capabilities in
simulated environments. By combining the Segment Anything Model (SAM) and
YOLOv5 with a Proximal Policy Optimization (PPO) agent operating in the
AI2-THOR simulation environment, we enable the agent to perceive and interact
with objects more effectively. Our comprehensive experiments, conducted across
four diverse indoor kitchen settings, demonstrate significant improvements in
object interaction success rates and navigation efficiency compared to a
baseline agent without advanced perception. The results show a 68% increase in
average cumulative reward, a 52.5% improvement in object interaction success
rate, and a 33% increase in navigation efficiency. These findings highlight the
potential of integrating foundation models with reinforcement learning for
complex robotic tasks, paving the way for more sophisticated and capable
autonomous agents.

</details>


### [288] [Bounding Distributional Shifts in World Modeling through Novelty Detection](https://arxiv.org/abs/2508.06096)
*Eric Jing,Abdeslam Boularias*

Main category: cs.RO

TL;DR: A novel method using a variational autoencoder as a novelty detector improves robustness in model-based planning by preventing deviations from training data.


<details>
  <summary>Details</summary>
Motivation: Current visual world models are sensitive to training quality, requiring extensive coverage of state and action spaces to avoid divergence during inference.

Method: Proposes a variational autoencoder as a novelty detector to ensure action trajectories stay within the training data distribution, integrated into a model-predictive control policy loop.

Result: Experiments in simulated robot environments show improved data efficiency over state-of-the-art solutions.

Conclusion: The method enhances robustness and data efficiency in model-based planning.

Abstract: Recent work on visual world models shows significant promise in latent state
dynamics obtained from pre-trained image backbones. However, most of the
current approaches are sensitive to training quality, requiring near-complete
coverage of the action and state space during training to prevent divergence
during inference. To make a model-based planning algorithm more robust to the
quality of the learned world model, we propose in this work to use a
variational autoencoder as a novelty detector to ensure that proposed action
trajectories during planning do not cause the learned model to deviate from the
training data distribution. To evaluate the effectiveness of this approach, a
series of experiments in challenging simulated robot environments was carried
out, with the proposed method incorporated into a model-predictive control
policy loop extending the DINO-WM architecture. The results clearly show that
the proposed method improves over state-of-the-art solutions in terms of data
efficiency.

</details>


### [289] [Affordance-R1: Reinforcement Learning for Generalizable Affordance Reasoning in Multimodal Large Language Model](https://arxiv.org/abs/2508.06206)
*Hanqing Wang,Shaoyang Wang,Yiming Zhong,Zemin Yang,Jiamin Wang,Zhiqing Cui,Jiahao Yuan,Yifan Han,Mingyu Liu,Yuexin Ma*

Main category: cs.RO

TL;DR: Affordance-R1 is a unified affordance grounding framework using CoT-guided GRPO in RL, outperforming existing methods with zero-shot generalization and reasoning capabilities.


<details>
  <summary>Details</summary>
Motivation: Existing models lack CoT reasoning, limiting OOD generalization and explicit reasoning in affordance grounding.

Method: Proposes Affordance-R1 with a sophisticated affordance function (format, perception, cognition rewards) and GRPO in RL, trained on ReasonAff dataset.

Result: Achieves robust zero-shot generalization, emergent reasoning, and outperforms established methods.

Conclusion: Affordance-R1 is the first to integrate GRPO-based RL with reasoning, showing open-world generalization.

Abstract: Affordance grounding focuses on predicting the specific regions of objects
that are associated with the actions to be performed by robots. It plays a
vital role in the fields of human-robot interaction, human-object interaction,
embodied manipulation, and embodied perception. Existing models often neglect
the affordance shared among different objects because they lack the
Chain-of-Thought(CoT) reasoning abilities, limiting their out-of-domain (OOD)
generalization and explicit reasoning capabilities. To address these
challenges, we propose Affordance-R1, the first unified affordance grounding
framework that integrates cognitive CoT guided Group Relative Policy
Optimization (GRPO) within a reinforcement learning paradigm. Specifically, we
designed a sophisticated affordance function, which contains format,
perception, and cognition rewards to effectively guide optimization directions.
Furthermore, we constructed a high-quality affordance-centric reasoning
dataset, ReasonAff, to support training. Trained exclusively via reinforcement
learning with GRPO and without explicit reasoning data, Affordance-R1 achieves
robust zero-shot generalization and exhibits emergent test-time reasoning
capabilities. Comprehensive experiments demonstrate that our model outperforms
well-established methods and exhibits open-world generalization. To the best of
our knowledge, Affordance-R1 is the first to integrate GRPO-based RL with
reasoning into affordance reasoning. The code of our method and our dataset is
released on https://github.com/hq-King/Affordance-R1.

</details>


### [290] [Shortcut Learning in Generalist Robot Policies: The Role of Dataset Diversity and Fragmentation](https://arxiv.org/abs/2508.06426)
*Youguang Xing,Xu Luo,Junlin Xie,Lianli Gao,Hengtao Shen,Jingkuan Song*

Main category: cs.RO

TL;DR: The paper identifies shortcut learning as a key issue limiting the generalization of generalist robot policies trained on large datasets like OXE. It highlights dataset diversity and distribution disparities as causes and suggests data collection and augmentation strategies to improve generalization.


<details>
  <summary>Details</summary>
Motivation: To understand why generalist robot policies trained on large datasets (e.g., OXE) struggle with generalization beyond their training data.

Method: Theoretical and empirical analysis of shortcut learning, focusing on dataset diversity and distribution disparities. Also explores data augmentation strategies.

Result: Identified shortcut learning as a major issue, with limited sub-dataset diversity and distributional disparities as primary causes. Demonstrated that data augmentation can mitigate these issues.

Conclusion: Improved dataset collection and augmentation strategies can reduce shortcut learning and enhance generalization in generalist robot policies.

Abstract: Generalist robot policies trained on large-scale datasets such as Open
X-Embodiment (OXE) demonstrate strong performance across a wide range of tasks.
However, they often struggle to generalize beyond the distribution of their
training data. In this paper, we investigate the underlying cause of this
limited generalization capability. We identify shortcut learning -- the
reliance on task-irrelevant features -- as a key impediment to generalization.
Through comprehensive theoretical and empirical analysis, we uncover two
primary contributors to shortcut learning: (1) limited diversity within
individual sub-datasets, and (2) significant distributional disparities across
sub-datasets, leading to dataset fragmentation. These issues arise from the
inherent structure of large-scale datasets like OXE, which are typically
composed of multiple sub-datasets collected independently across varied
environments and embodiments. Our findings provide critical insights into
dataset collection strategies that can reduce shortcut learning and enhance the
generalization ability of generalist robot policies. Moreover, in scenarios
where acquiring new large-scale data is impractical, we demonstrate that
carefully selected robotic data augmentation strategies can effectively reduce
shortcut learning in existing offline datasets, thereby improving
generalization capabilities of generalist robot policies, e.g., $\pi_0$, in
both simulation and real-world environments. More information at
https://lucky-light-sun.github.io/proj/shortcut-learning-in-grps/.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [291] [Data-Driven Density Steering via the Gromov-Wasserstein Optimal Transport Distance](https://arxiv.org/abs/2508.06052)
*Haruto Nakashima,Siddhartha Ganguly,Kenji Kashima*

Main category: math.OC

TL;DR: The paper addresses data-driven chance-constrained density steering using the Gromov-Wasserstein metric for an unknown linear system, solved via a difference-of-convex program.


<details>
  <summary>Details</summary>
Motivation: To control an unknown linear system's state distribution, matching a terminal Gaussian distribution using pre-operational data.

Method: Reformulates the problem as a difference-of-convex program and solves it with the DC algorithm.

Result: Numerical results demonstrate the approach's effectiveness across data-driven schemes.

Conclusion: The proposed method efficiently solves the density steering problem for unknown linear systems.

Abstract: We tackle the data-driven chance-constrained density steering problem using
the Gromov-Wasserstein metric. The underlying dynamical system is an unknown
linear controlled recursion, with the assumption that sufficiently rich
input-output data from pre-operational experiments are available. The initial
state is modeled as a Gaussian mixture, while the terminal state is required to
match a specified Gaussian distribution. We reformulate the resulting optimal
control problem as a difference-of-convex program and show that it can be
efficiently and tractably solved using the DC algorithm. Numerical results
validate our approach through various data-driven schemes.

</details>


### [292] [LLM Serving Optimization with Variable Prefill and Decode Lengths](https://arxiv.org/abs/2508.06133)
*Meixuan Wang,Yinyu Ye,Zijie Zhou*

Main category: math.OC

TL;DR: The paper addresses the NP-hard problem of scheduling LLM requests with heterogeneous prefill and decode lengths to minimize total completion time. It critiques common strategies like FCFS and SF, proposes a novel algorithm with a constant competitive ratio, and evaluates improved variants.


<details>
  <summary>Details</summary>
Motivation: The motivation is to optimize LLM serving by addressing the challenges of batching, memory constraints, and precedence relationships in scheduling requests with varying input and output lengths.

Method: The method involves analyzing common scheduling strategies, proving their limitations, and introducing a new algorithm with a selection metric for efficient batching. Variants like dynamic programming, local search, and LP-based schedulers are also developed and tested.

Result: The proposed algorithm achieves a constant competitive ratio, and its variants outperform standard baselines in simulations while maintaining computational efficiency.

Conclusion: The paper concludes that the novel algorithm and its variants offer significant improvements over existing scheduling strategies for LLM serving, particularly in memory-intensive scenarios.

Abstract: We study the problem of serving LLM (Large Language Model) requests where
each request has heterogeneous prefill and decode lengths. In LLM serving, the
prefill length corresponds to the input prompt length, which determines the
initial memory usage in the KV cache. The decode length refers to the number of
output tokens generated sequentially, with each additional token increasing the
KV cache memory usage by one unit. Given a set of n requests, our goal is to
schedule and process them to minimize the total completion time. We show that
this problem is NP-hard due to the interplay of batching, placement
constraints, precedence relationships, and linearly increasing memory usage. We
then analyze commonly used scheduling strategies in practice, such as
First-Come-First-Serve (FCFS) and Shortest-First (SF), and prove that their
competitive ratios scale up sublinearly with the memory limit-a significant
drawback in real-world settings where memory demand is large. To address this,
we propose a novel algorithm based on a new selection metric that efficiently
forms batches over time. We prove that this algorithm achieves a constant
competitive ratio. Finally, we develop and evaluate a few algorithm variants
inspired by this approach, including dynamic programming variants, local search
methods, and an LP-based scheduler, demonstrating through comprehensive
simulations that they outperform standard baselines while maintaining
computational efficiency.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [293] [KnapFormer: An Online Load Balancer for Efficient Diffusion Transformers Training](https://arxiv.org/abs/2508.06001)
*Kai Zhang,Peng Wang,Sai Bi,Jianming Zhang,Yuanjun Xiong*

Main category: cs.DC

TL;DR: KnapFormer is a framework for balancing workloads and sequence parallelism in distributed training of Diffusion Transformers, achieving significant speedups and minimal overhead.


<details>
  <summary>Details</summary>
Motivation: Addressing token imbalance in distributed training due to variable-length inputs and mixed-resolution data.

Method: Uses a global knapsack solver to redistribute tokens, integrating sequence parallelism and a workload model.

Result: Achieves <1% workload discrepancy, eliminates stragglers, and provides 2x-3x speedup in training.

Conclusion: KnapFormer efficiently balances workloads and enhances training performance for diffusion models.

Abstract: We present KnapFormer, an efficient and versatile framework to combine
workload balancing and sequence parallelism in distributed training of
Diffusion Transformers (DiT). KnapFormer builds on the insight that strong
synergy exists between sequence parallelism and the need to address the
significant token imbalance across ranks. This imbalance arises from
variable-length text inputs and varying visual token counts in mixed-resolution
and image-video joint training. KnapFormer redistributes tokens by first
gathering sequence length metadata across all ranks in a balancing group and
solving a global knapsack problem. The solver aims to minimize the variances of
total workload per-GPU, while accounting for the effect of sequence
parallelism. By integrating DeepSpeed-Ulysees-based sequence parallelism in the
load-balancing decision process and utilizing a simple semi-empirical workload
model, KnapFormers achieves minimal communication overhead and less than 1%
workload discrepancy in real-world training workloads with sequence length
varying from a few hundred to tens of thousands. It eliminates straggler
effects and achieves 2x to 3x speedup when training state-of-the-art diffusion
models like FLUX on mixed-resolution and image-video joint data corpora. We
open-source the KnapFormer implementation at
https://github.com/Kai-46/KnapFormer/

</details>


### [294] [Blockchain-Enabled Federated Learning](https://arxiv.org/abs/2508.06406)
*Murtaza Rangwala,Venugopal K R,Rajkumar Buyya*

Main category: cs.DC

TL;DR: The paper explores Blockchain-enabled federated learning (BCFL) to address trust, privacy, and coordination in AI systems, analyzing its architecture, consensus mechanisms, and storage solutions, with a case study validating its practicality.


<details>
  <summary>Details</summary>
Motivation: To solve trust, privacy, and coordination issues in collaborative AI systems by leveraging blockchain technology.

Method: A four-dimensional taxonomy examines coordination structures, consensus mechanisms, storage architectures, and trust models, with a case study of the TrustMesh framework.

Result: BCFL systems achieve performance comparable to centralized methods while offering enhanced security and trustless collaboration, validated in real-world applications.

Conclusion: BCFL is a viable solution for secure, transparent, and efficient collaborative AI, balancing scalability, security, and performance.

Abstract: Blockchain-enabled federated learning (BCFL) addresses fundamental challenges
of trust, privacy, and coordination in collaborative AI systems. This chapter
provides comprehensive architectural analysis of BCFL systems through a
systematic four-dimensional taxonomy examining coordination structures,
consensus mechanisms, storage architectures, and trust models. We analyze
design patterns from blockchain-verified centralized coordination to fully
decentralized peer-to-peer networks, evaluating trade-offs in scalability,
security, and performance. Through detailed examination of consensus mechanisms
designed for federated learning contexts, including Proof of Quality and Proof
of Federated Learning, we demonstrate how computational work can be repurposed
from arbitrary cryptographic puzzles to productive machine learning tasks. The
chapter addresses critical storage challenges by examining multi-tier
architectures that balance blockchain's transaction constraints with neural
networks' large parameter requirements while maintaining cryptographic
integrity. A technical case study of the TrustMesh framework illustrates
practical implementation considerations in BCFL systems through distributed
image classification training, demonstrating effective collaborative learning
across IoT devices with highly non-IID data distributions while maintaining
complete transparency and fault tolerance. Analysis of real-world deployments
across healthcare consortiums, financial services, and IoT security
applications validates the practical viability of BCFL systems, achieving
performance comparable to centralized approaches while providing enhanced
security guarantees and enabling new models of trustless collaborative
intelligence.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [295] [Evaluating Universal Machine Learning Force Fields Against Experimental Measurements](https://arxiv.org/abs/2508.05762)
*Sajid Mannan,Vaibhav Bihani,Carmelo Gonzales,Kin Long Kelvin Lee,Nitya Nand Gosvami,Sayan Ranu,Santiago Miret,N M Anoop Krishnan*

Main category: cond-mat.mtrl-sci

TL;DR: UniFFBench evaluates universal machine learning force fields (UMLFFs) against experimental data, revealing a gap between computational benchmarks and real-world performance.


<details>
  <summary>Details</summary>
Motivation: To assess UMLFFs' real-world applicability by comparing them to experimental measurements, as computational benchmarks may overestimate reliability.

Method: Evaluated six UMLFFs using UniFFBench, a framework testing ~1,500 mineral structures for diverse properties.

Result: UMLFFs perform poorly on experimental data despite excelling in computational benchmarks, with errors exceeding practical thresholds.

Conclusion: Current benchmarks overestimate UMLFF reliability; UniFFBench highlights the need for experimental validation to achieve universal force field capabilities.

Abstract: Universal machine learning force fields (UMLFFs) promise to revolutionize
materials science by enabling rapid atomistic simulations across the periodic
table. However, their evaluation has been limited to computational benchmarks
that may not reflect real-world performance. Here, we present UniFFBench, a
comprehensive framework for evaluating UMLFFs against experimental measurements
of ~1,500 carefully curated mineral structures spanning diverse chemical
environments, bonding types, structural complexity, and elastic properties. Our
systematic evaluation of six state-of-the-art UMLFFs reveals a substantial
reality gap: models achieving impressive performance on computational
benchmarks often fail when confronted with experimental complexity. Even the
best-performing models exhibit higher density prediction error than the
threshold required for practical applications. Most strikingly, we observe
disconnects between simulation stability and mechanical property accuracy, with
prediction errors correlating with training data representation rather than the
modeling method. These findings demonstrate that while current computational
benchmarks provide valuable controlled comparisons, they may overestimate model
reliability when extrapolated to experimentally complex chemical spaces.
Altogether, UniFFBench establishes essential experimental validation standards
and reveals systematic limitations that must be addressed to achieve truly
universal force field capabilities.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [296] [Training chord recognition models on artificially generated audio](https://arxiv.org/abs/2508.05878)
*Martyna Majchrzak,Jacek Ma≈Ñdziuk*

Main category: cs.SD

TL;DR: The study compares Transformer-based models for chord recognition, testing artificial vs. human-composed datasets, finding artificial data useful in certain scenarios.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of non-copyrighted audio for training and evaluating chord recognition models in Music Information Retrieval.

Method: Two Transformer models trained on artificial (AAM) and human-composed datasets (Schubert's Winterreise, McGill Billboard), evaluated with Root, MajMin, and CCM metrics.

Result: Artificial datasets (AAM) can enrich human-composed datasets or serve as standalone training sets for chord recognition in pop music.

Conclusion: Artificially generated music, despite differences from human-composed music, is viable for training chord recognition models in specific contexts.

Abstract: One of the challenging problems in Music Information Retrieval is the
acquisition of enough non-copyrighted audio recordings for model training and
evaluation. This study compares two Transformer-based neural network models for
chord sequence recognition in audio recordings and examines the effectiveness
of using an artificially generated dataset for this purpose. The models are
trained on various combinations of Artificial Audio Multitracks (AAM),
Schubert's Winterreise Dataset, and the McGill Billboard Dataset and evaluated
with three metrics: Root, MajMin and Chord Content Metric (CCM). The
experiments prove that even though there are certainly differences in
complexity and structure between artificially generated and human-composed
music, the former can be useful in certain scenarios. Specifically, AAM can
enrich a smaller training dataset of music composed by a human or can even be
used as a standalone training set for a model that predicts chord sequences in
pop music, if no other data is available.

</details>


### [297] [DAFMSVC: One-Shot Singing Voice Conversion with Dual Attention Mechanism and Flow Matching](https://arxiv.org/abs/2508.05978)
*Wei Chen,Binzhu Sha,Dan Luo,Jing Yang,Zhuo Wang,Fan Fan,Zhiyong Wu*

Main category: cs.SD

TL;DR: DAFMSVC improves Singing Voice Conversion by replacing SSL features and using dual cross-attention for better timbre similarity and audio quality.


<details>
  <summary>Details</summary>
Motivation: Addressing timbre leakage and quality degradation in any-to-any SVC by adapting unseen speaker timbres effectively.

Method: Replaces SSL features with target audio's similar features, uses dual cross-attention for fusion, and employs flow matching for audio generation.

Result: DAFMSVC outperforms existing methods in timbre similarity and naturalness.

Conclusion: DAFMSVC is a robust solution for high-quality SVC with superior performance.

Abstract: Singing Voice Conversion (SVC) transfers a source singer's timbre to a target
while keeping melody and lyrics. The key challenge in any-to-any SVC is
adapting unseen speaker timbres to source audio without quality degradation.
Existing methods either face timbre leakage or fail to achieve satisfactory
timbre similarity and quality in the generated audio. To address these
challenges, we propose DAFMSVC, where the self-supervised learning (SSL)
features from the source audio are replaced with the most similar SSL features
from the target audio to prevent timbre leakage. It also incorporates a dual
cross-attention mechanism for the adaptive fusion of speaker embeddings,
melody, and linguistic content. Additionally, we introduce a flow matching
module for high quality audio generation from the fused features. Experimental
results show that DAFMSVC significantly enhances timbre similarity and
naturalness, outperforming state-of-the-art methods in both subjective and
objective evaluations.

</details>


### [298] [EmoAugNet: A Signal-Augmented Hybrid CNN-LSTM Framework for Speech Emotion Recognition](https://arxiv.org/abs/2508.06321)
*Durjoy Chandra Paul,Gaurob Saha,Md Amjad Hossain*

Main category: cs.SD

TL;DR: EmoAugNet, a hybrid deep learning framework combining LSTM and 1D-CNN, achieves high accuracy in Speech Emotion Recognition (SER) using data augmentation and feature extraction.


<details>
  <summary>Details</summary>
Motivation: Enhancing human-computer interaction (HCI) by improving Speech Emotion Recognition (SER) through robust feature extraction and data augmentation.

Method: Uses LSTM and 1D-CNN hybrid model with data augmentation (noise addition, pitch shifting, time stretching) and feature extraction (RMSE, MFCC, ZCR).

Result: Achieves weighted accuracy up to 96.75% (IEMOCAP) and 94.98% (RAVDESS) with ReLU/ELU activations.

Conclusion: EmoAugNet improves SER robustness and performance via integrated augmentation and hybrid modeling.

Abstract: Recognizing emotional signals in speech has a significant impact on enhancing
the effectiveness of human-computer interaction (HCI). This study introduces
EmoAugNet, a hybrid deep learning framework, that incorporates Long Short-Term
Memory (LSTM) layers with one-dimensional Convolutional Neural Networks
(1D-CNN) to enable reliable Speech Emotion Recognition (SER). The quality and
variety of the features that are taken from speech signals have a significant
impact on how well SER systems perform. A comprehensive speech data
augmentation strategy was used to combine both traditional methods, such as
noise addition, pitch shifting, and time stretching, with a novel
combination-based augmentation pipeline to enhance generalization and reduce
overfitting. Each audio sample was transformed into a high-dimensional feature
vector using root mean square energy (RMSE), Mel-frequency Cepstral Coefficient
(MFCC), and zero-crossing rate (ZCR). Our model with ReLU activation has a
weighted accuracy of 95.78\% and unweighted accuracy of 92.52\% on the IEMOCAP
dataset and, with ELU activation, has a weighted accuracy of 96.75\% and
unweighted accuracy of 91.28\%. On the RAVDESS dataset, we get a weighted
accuracy of 94.53\% and 94.98\% unweighted accuracy for ReLU activation and
93.72\% weighted accuracy and 94.64\% unweighted accuracy for ELU activation.
These results highlight EmoAugNet's effectiveness in improving the robustness
and performance of SER systems through integated data augmentation and hybrid
modeling.

</details>


### [299] [MeanAudio: Fast and Faithful Text-to-Audio Generation with Mean Flows](https://arxiv.org/abs/2508.06098)
*Xiquan Li,Junxi Liu,Yuzhe Liang,Zhikang Niu,Wenxi Chen,Xie Chen*

Main category: cs.SD

TL;DR: MeanAudio introduces a MeanFlow-based model for fast and high-quality text-to-audio generation, achieving 100x speedup over diffusion-based systems.


<details>
  <summary>Details</summary>
Motivation: Current TTA systems suffer from slow inference, limiting practical use. MeanAudio aims to address this by enabling fast and faithful generation.

Method: MeanAudio uses a Flux-style latent transformer to regress the average velocity field, incorporates classifier-free guidance, and employs a curriculum with flow field mix-up for stable training.

Result: Achieves a real-time factor of 0.013 (100x speedup) and strong performance in single- and multi-step generation.

Conclusion: MeanAudio sets a new benchmark for fast and high-quality text-to-audio synthesis.

Abstract: Recent developments in diffusion- and flow- based models have significantly
advanced Text-to-Audio Generation (TTA). While achieving great synthesis
quality and controllability, current TTA systems still suffer from slow
inference speed, which significantly limits their practical applicability. This
paper presents MeanAudio, a novel MeanFlow-based model tailored for fast and
faithful text-to-audio generation. Built on a Flux-style latent transformer,
MeanAudio regresses the average velocity field during training, enabling fast
generation by mapping directly from the start to the endpoint of the flow
trajectory. By incorporating classifier-free guidance (CFG) into the training
target, MeanAudio incurs no additional cost in the guided sampling process. To
further stabilize training, we propose an instantaneous-to-mean curriculum with
flow field mix-up, which encourages the model to first learn the foundational
instantaneous dynamics, and then gradually adapt to mean flows. This strategy
proves critical for enhancing training efficiency and generation quality.
Experimental results demonstrate that MeanAudio achieves state-of-the-art
performance in single-step audio generation. Specifically, it achieves a real
time factor (RTF) of 0.013 on a single NVIDIA RTX 3090, yielding a 100x speedup
over SOTA diffusion-based TTA systems. Moreover, MeanAudio also demonstrates
strong performance in multi-step generation, enabling smooth and coherent
transitions across successive synthesis steps.

</details>


### [300] [SpeakerLM: End-to-End Versatile Speaker Diarization and Recognition with Multimodal Large Language Models](https://arxiv.org/abs/2508.06372)
*Han Yin,Yafeng Chen,Chong Deng,Luyao Cheng,Hui Wang,Chao-Hong Tan,Qian Chen,Wen Wang,Xiangang Li*

Main category: cs.SD

TL;DR: SpeakerLM is a unified multimodal large language model for Speaker Diarization and Recognition (SDR), addressing limitations of cascaded systems by jointly performing SD and ASR tasks end-to-end, with a flexible speaker registration mechanism.


<details>
  <summary>Details</summary>
Motivation: Existing cascaded SDR systems suffer from error propagation, difficulty handling overlapping speech, and lack of joint optimization. SpeakerLM aims to overcome these issues.

Method: SpeakerLM integrates SD and ASR into a single end-to-end model, includes a speaker registration mechanism, and uses multi-stage training on large-scale real data.

Result: SpeakerLM outperforms state-of-the-art cascaded baselines on public benchmarks, showing strong scaling and generalizability. The speaker registration mechanism ensures robust performance across diverse conditions.

Conclusion: SpeakerLM provides a unified, scalable, and flexible solution for SDR, improving performance and adaptability in real-world scenarios.

Abstract: The Speaker Diarization and Recognition (SDR) task aims to predict "who spoke
when and what" within an audio clip, which is a crucial task in various
real-world multi-speaker scenarios such as meeting transcription and dialogue
systems. Existing SDR systems typically adopt a cascaded framework, combining
multiple modules such as speaker diarization (SD) and automatic speech
recognition (ASR). The cascaded systems suffer from several limitations, such
as error propagation, difficulty in handling overlapping speech, and lack of
joint optimization for exploring the synergy between SD and ASR tasks. To
address these limitations, we introduce SpeakerLM, a unified multimodal large
language model for SDR that jointly performs SD and ASR in an end-to-end
manner. Moreover, to facilitate diverse real-world scenarios, we incorporate a
flexible speaker registration mechanism into SpeakerLM, enabling SDR under
different speaker registration settings. SpeakerLM is progressively developed
with a multi-stage training strategy on large-scale real data. Extensive
experiments show that SpeakerLM demonstrates strong data scaling capability and
generalizability, outperforming state-of-the-art cascaded baselines on both
in-domain and out-of-domain public SDR benchmarks. Furthermore, experimental
results show that the proposed speaker registration mechanism effectively
ensures robust SDR performance of SpeakerLM across diverse speaker registration
conditions and varying numbers of registered speakers.

</details>


### [301] [Robust Target Speaker Diarization and Separation via Augmented Speaker Embedding Sampling](https://arxiv.org/abs/2508.06393)
*Md Asif Jalal,Luca Remaggi,Vasileios Moschopoulos,Thanasis Kotsiopoulos,Vandana Rajan,Karthikeyan Saravanan,Anastasis Drosou,Junho Heo,Hyuk Oh,Seokyeong Jeong*

Main category: cs.SD

TL;DR: A new enrollment-free method for simultaneous speech separation and diarization using automatic speaker embedding identification, outperforming SOTA with significant improvements.


<details>
  <summary>Details</summary>
Motivation: Overcome limitations of traditional methods requiring prior speaker knowledge or fixed participant counts.

Method: Dual-stage training pipeline for robust speaker representations and overlapping spectral loss for diarization accuracy.

Result: 71% relative improvement in DER and 69% in cpWER compared to SOTA.

Conclusion: The proposed approach effectively addresses enrollment-free challenges and enhances performance in noisy conditions.

Abstract: Traditional speech separation and speaker diarization approaches rely on
prior knowledge of target speakers or a predetermined number of participants in
audio signals. To address these limitations, recent advances focus on
developing enrollment-free methods capable of identifying targets without
explicit speaker labeling. This work introduces a new approach to train
simultaneous speech separation and diarization using automatic identification
of target speaker embeddings, within mixtures. Our proposed model employs a
dual-stage training pipeline designed to learn robust speaker representation
features that are resilient to background noise interference. Furthermore, we
present an overlapping spectral loss function specifically tailored for
enhancing diarization accuracy during overlapped speech frames. Experimental
results show significant performance gains compared to the current SOTA
baseline, achieving 71% relative improvement in DER and 69% in cpWER.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [302] [DINA: A Dual Defense Framework Against Internal Noise and External Attacks in Natural Language Processing](https://arxiv.org/abs/2508.05671)
*Ko-Wei Chuang,Hen-Hsen Huang,Tsai-Yen Li*

Main category: cs.CR

TL;DR: DINA is a unified framework for NLP that defends against both internal label corruption and external adversarial attacks, improving model robustness and accuracy.


<details>
  <summary>Details</summary>
Motivation: Address adversarial threats in LLMs and generative AI from external manipulations and internal label corruption in customer service and moderation applications.

Method: Introduces DINA, combining noisy-label learning from computer vision with adversarial training to mitigate dual threats.

Result: DINA significantly improves robustness and accuracy on a real-world dataset compared to baselines.

Conclusion: Dual-threat defenses are critical for NLP systems, with broader implications for fair and responsible AI deployment.

Abstract: As large language models (LLMs) and generative AI become increasingly
integrated into customer service and moderation applications, adversarial
threats emerge from both external manipulations and internal label corruption.
In this work, we identify and systematically address these dual adversarial
threats by introducing DINA (Dual Defense Against Internal Noise and
Adversarial Attacks), a novel unified framework tailored specifically for NLP.
Our approach adapts advanced noisy-label learning methods from computer vision
and integrates them with adversarial training to simultaneously mitigate
internal label sabotage and external adversarial perturbations. Extensive
experiments conducted on a real-world dataset from an online gaming service
demonstrate that DINA significantly improves model robustness and accuracy
compared to baseline models. Our findings not only highlight the critical
necessity of dual-threat defenses but also offer practical strategies for
safeguarding NLP systems in realistic adversarial scenarios, underscoring
broader implications for fair and responsible AI deployment.

</details>


### [303] [Can LLMs effectively provide game-theoretic-based scenarios for cybersecurity?](https://arxiv.org/abs/2508.05670)
*Daniele Proverbio,Alessio Buscemi,Alessandro Di Stefano,The Anh Han,German Castignani,Pietro Li√≤*

Main category: cs.CR

TL;DR: Investigates if classical game theory can model LLM-driven actors in cybersecurity, revealing biases and language sensitivity in LLM behavior.


<details>
  <summary>Details</summary>
Motivation: To assess whether traditional game-theoretic frameworks can accurately capture the strategic behaviors of LLM-driven agents in cybersecurity.

Method: Uses a reproducible framework to test LLMs in one-shot zero-sum games and dynamic Prisoner's Dilemma across five languages.

Result: LLM payoffs are influenced by agent traits and language choice, highlighting biases and instability in different linguistic contexts.

Conclusion: LLMs exhibit language-sensitive behaviors, cautioning against indiscriminate use in cybersecurity and calling for further stability studies.

Abstract: Game theory has long served as a foundational tool in cybersecurity to test,
predict, and design strategic interactions between attackers and defenders. The
recent advent of Large Language Models (LLMs) offers new tools and challenges
for the security of computer systems; In this work, we investigate whether
classical game-theoretic frameworks can effectively capture the behaviours of
LLM-driven actors and bots. Using a reproducible framework for game-theoretic
LLM agents, we investigate two canonical scenarios -- the one-shot zero-sum
game and the dynamic Prisoner's Dilemma -- and we test whether LLMs converge to
expected outcomes or exhibit deviations due to embedded biases. Our experiments
involve four state-of-the-art LLMs and span five natural languages, English,
French, Arabic, Vietnamese, and Mandarin Chinese, to assess linguistic
sensitivity. For both games, we observe that the final payoffs are influenced
by agents characteristics such as personality traits or knowledge of repeated
rounds. Moreover, we uncover an unexpected sensitivity of the final payoffs to
the choice of languages, which should warn against indiscriminate application
of LLMs in cybersecurity applications and call for in-depth studies, as LLMs
may behave differently when deployed in different countries. We also employ
quantitative metrics to evaluate the internal consistency and cross-language
stability of LLM agents, to help guide the selection of the most stable LLMs
and optimising models for secure applications.

</details>


### [304] [DMFI: Dual-Modality Fine-Tuning and Inference Framework for LLM-Based Insider Threat Detection](https://arxiv.org/abs/2508.05694)
*Kaichuan Kong,Dongjie Liu,Xiaobo Jin,Guanggang Geng,Zhiying Li,Jian Weng*

Main category: cs.CR

TL;DR: DMFI is a dual-modality framework for insider threat detection, combining semantic inference and behavior-aware fine-tuning to outperform traditional and LLM-based methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of traditional models and LLM-based solutions in detecting subtle, context-dependent insider threats.

Method: DMFI processes raw logs into semantic and behavioral views, fine-tunes two LoRA-enhanced LLMs, and fuses outputs via an MLP-based decision module. DMFI-B improves robustness under class imbalance.

Result: Outperforms state-of-the-art methods on CERT r4.2 and r5.2 datasets in detection accuracy.

Conclusion: DMFI effectively combines LLM reasoning with structured behavioral modeling, providing a scalable solution for insider threat detection.

Abstract: Insider threat detection (ITD) poses a persistent and high-impact challenge
in cybersecurity due to the subtle, long-term, and context-dependent nature of
malicious insider behaviors. Traditional models often struggle to capture
semantic intent and complex behavior dynamics, while existing LLM-based
solutions face limitations in prompt adaptability and modality coverage. To
bridge this gap, we propose DMFI, a dual-modality framework that integrates
semantic inference with behavior-aware fine-tuning. DMFI converts raw logs into
two structured views: (1) a semantic view that processes content-rich artifacts
(e.g., emails, https) using instruction-formatted prompts; and (2) a behavioral
abstraction, constructed via a 4W-guided (When-Where-What-Which) transformation
to encode contextual action sequences. Two LoRA-enhanced LLMs are fine-tuned
independently, and their outputs are fused via a lightweight MLP-based decision
module. We further introduce DMFI-B, a discriminative adaptation strategy that
separates normal and abnormal behavior representations, improving robustness
under severe class imbalance. Experiments on CERT r4.2 and r5.2 datasets
demonstrate that DMFI outperforms state-of-the-art methods in detection
accuracy. Our approach combines the semantic reasoning power of LLMs with
structured behavior modeling, offering a scalable and effective solution for
real-world insider threat detection. Our work demonstrates the effectiveness of
combining LLM reasoning with structured behavioral modeling, offering a
scalable and deployable solution for modern insider threat detection.

</details>


### [305] [Towards Effective Offensive Security LLM Agents: Hyperparameter Tuning, LLM as a Judge, and a Lightweight CTF Benchmark](https://arxiv.org/abs/2508.05674)
*Minghao Shao,Nanda Rani,Kimberly Milner,Haoran Xi,Meet Udeshi,Saksham Aggarwal,Venkata Sai Charan Putrevu,Sandeep Kumar Shukla,Prashanth Krishnamurthy,Farshad Khorrami,Ramesh Karri,Muhammad Shafique*

Main category: cs.CR

TL;DR: The paper introduces CTFJudge and CTFTiny for evaluating LLM-based offensive security agents in CTF challenges, proposes the CTF Competency Index (CCI), and studies the impact of LLM hyperparameters on performance.


<details>
  <summary>Details</summary>
Motivation: To improve automation of offensive security tasks in CTF challenges by identifying key success factors for LLM-based agents.

Method: Develops CTFJudge for granular evaluation, introduces CCI for partial correctness, and analyzes LLM hyperparameters' effects. Also creates CTFTiny, a benchmark of 50 CTF challenges.

Result: Identifies optimal multi-agent coordination settings and provides insights into LLM performance in cybersecurity tasks.

Conclusion: The work advances LLM agent research in cybersecurity and releases open-source tools (CTFJudge and CTFTiny) for future studies.

Abstract: Recent advances in LLM agentic systems have improved the automation of
offensive security tasks, particularly for Capture the Flag (CTF) challenges.
We systematically investigate the key factors that drive agent success and
provide a detailed recipe for building effective LLM-based offensive security
agents. First, we present CTFJudge, a framework leveraging LLM as a judge to
analyze agent trajectories and provide granular evaluation across CTF solving
steps. Second, we propose a novel metric, CTF Competency Index (CCI) for
partial correctness, revealing how closely agent solutions align with
human-crafted gold standards. Third, we examine how LLM hyperparameters, namely
temperature, top-p, and maximum token length, influence agent performance and
automated cybersecurity task planning. For rapid evaluation, we present
CTFTiny, a curated benchmark of 50 representative CTF challenges across binary
exploitation, web, reverse engineering, forensics, and cryptography. Our
findings identify optimal multi-agent coordination settings and lay the
groundwork for future LLM agent research in cybersecurity. We make CTFTiny open
source to public https://github.com/NYU-LLM-CTF/CTFTiny along with CTFJudge on
https://github.com/NYU-LLM-CTF/CTFJudge.

</details>


### [306] [Principle-Guided Verilog Optimization: IP-Safe Knowledge Transfer via Local-Cloud Collaboration](https://arxiv.org/abs/2508.05675)
*Jing Wang,Zheng Li,Lei Li,Fan He,Liyu Lin,Yao Lai,Yan Li,Xiaoyang Zeng,Yufeng Guo*

Main category: cs.CR

TL;DR: A novel IP-preserving edge-cloud framework for RTL code optimization using local and cloud LLMs achieves higher success rates than baselines.


<details>
  <summary>Details</summary>
Motivation: Addressing IP leakage risks in proprietary hardware design optimization while leveraging LLMs.

Method: Local small LLMs analyze code pairs for design principles, which guide cloud LLMs for targeted improvements without exposing IP.

Result: The framework achieves a 66.67% optimization success rate for power, outperforming standalone cloud LLMs and commercial models.

Conclusion: The work introduces a secure, collaborative approach for hardware design optimization, balancing performance and IP protection.

Abstract: Recent years have witnessed growing interest in adopting large language
models (LLMs) for Register Transfer Level (RTL) code optimization. While
powerful cloud-based LLMs offer superior optimization capabilities, they pose
unacceptable intellectual property (IP) leakage risks when processing
proprietary hardware designs. In this paper, we propose a new scenario where
Verilog code must be optimized for specific attributes without leaking
sensitive IP information. We introduce the first IP-preserving edge-cloud
collaborative framework that leverages the benefits of both paradigms. Our
approach employs local small LLMs (e.g., Qwen-2.5-Coder-7B) to perform secure
comparative analysis between paired high-quality target designs and novice
draft codes, yielding general design principles that summarize key insights for
improvements. These principles are then used to query stronger cloud LLMs
(e.g., Deepseek-V3) for targeted code improvement, ensuring that only
abstracted and IP-safe guidance reaches external services. Our experimental
results demonstrate that the framework achieves significantly higher
optimization success rates compared to baseline methods. For example, combining
Qwen-2.5-Coder-7B and Deepseek-V3 achieves a 66.67\% optimization success rate
for power utilization, outperforming Deepseek-V3 alone (49.81\%) and even
commercial models like GPT-4o (55.81\%). Further investigation of local and
cloud LLM combinations reveals that different model pairings exhibit varying
strengths for specific optimization objectives, with interesting trends
emerging when varying the number of comparative code pairs. Our work
establishes a new paradigm for secure hardware design optimization that
balances performance gains with IP protection.

</details>


### [307] [Adversarial Attacks on Reinforcement Learning-based Medical Questionnaire Systems: Input-level Perturbation Strategies and Medical Constraint Validation](https://arxiv.org/abs/2508.05677)
*Peizhuo Liu*

Main category: cs.CR

TL;DR: The study evaluates adversarial attacks on RL-based medical questionnaire systems, revealing significant vulnerabilities despite medical constraints.


<details>
  <summary>Details</summary>
Motivation: To assess the safety and robustness of RL-based medical questionnaire systems by identifying their vulnerabilities under adversarial attacks.

Method: Formulated the diagnosis process as an MDP, implemented six attack methods (FGSM, PGD, C&W, BIM, DeepFool, AutoAttack) with medical validation constraints, and tested on the NHIS dataset.

Result: Achieved a 97.6% success rate in generating clinically plausible adversarial samples, with attack success rates ranging from 33.08% to 64.70%.

Conclusion: RL-based medical systems remain vulnerable to adversarial attacks, highlighting the need for improved robustness.

Abstract: RL-based medical questionnaire systems have shown great potential in medical
scenarios. However, their safety and robustness remain unresolved. This study
performs a comprehensive evaluation on adversarial attack methods to identify
and analyze their potential vulnerabilities. We formulate the diagnosis process
as a Markov Decision Process (MDP), where the state is the patient responses
and unasked questions, and the action is either to ask a question or to make a
diagnosis. We implemented six prevailing major attack methods, including the
Fast Gradient Signed Method (FGSM), Projected Gradient Descent (PGD), Carlini &
Wagner Attack (C&W) attack, Basic Iterative Method (BIM), DeepFool, and
AutoAttack, with seven epsilon values each. To ensure the generated adversarial
examples remain clinically plausible, we developed a comprehensive medical
validation framework consisting of 247 medical constraints, including
physiological bounds, symptom correlations, and conditional medical
constraints. We achieved a 97.6% success rate in generating clinically
plausible adversarial samples. We performed our experiment on the National
Health Interview Survey (NHIS) dataset (https://www.cdc.gov/nchs/nhis/), which
consists of 182,630 samples, to predict the participant's 4-year mortality
rate. We evaluated our attacks on the AdaptiveFS framework proposed in
arXiv:2004.00994. Our results show that adversarial attacks could significantly
impact the diagnostic accuracy, with attack success rates ranging from 33.08%
(FGSM) to 64.70% (AutoAttack). Our work has demonstrated that even under strict
medical constraints on the input, such RL-based medical questionnaire systems
still show significant vulnerabilities.

</details>


### [308] [Selection-Based Vulnerabilities: Clean-Label Backdoor Attacks in Active Learning](https://arxiv.org/abs/2508.05681)
*Yuhan Zhi,Longtian Wang,Xiaofei Xie,Chao Shen,Qiang Hu,Xiaohong Guan*

Main category: cs.CR

TL;DR: ALA is the first framework to exploit acquisition functions in active learning for poisoning attacks, achieving high success rates while remaining undetectable.


<details>
  <summary>Details</summary>
Motivation: To investigate whether active learning (AL) is safe by revealing its vulnerability to poisoning attacks through acquisition functions.

Method: ALA optimizes poisoned inputs to appear uncertain, increasing their selection likelihood by acquisition functions.

Result: ALA achieves up to 94% success rates with low poisoning budgets (0.5%-1.0%), without compromising model utility or detectability.

Conclusion: Acquisition functions in AL can be exploited, urging caution in trusted data scenarios.

Abstract: Active learning(AL), which serves as the representative label-efficient
learning paradigm, has been widely applied in resource-constrained scenarios.
The achievement of AL is attributed to acquisition functions, which are
designed for identifying the most important data to label. Despite this
success, one question remains unanswered: is AL safe? In this work, we
introduce ALA, a practical and the first framework to utilize the acquisition
function as the poisoning attack surface to reveal the weakness of active
learning. Specifically, ALA optimizes imperceptibly poisoned inputs to exhibit
high uncertainty scores, increasing their probability of being selected by
acquisition functions. To evaluate ALA, we conduct extensive experiments across
three datasets, three acquisition functions, and two types of clean-label
backdoor triggers. Results show that our attack can achieve high success rates
(up to 94%) even under low poisoning budgets (0.5%-1.0%) while preserving model
utility and remaining undetectable to human annotators. Our findings remind
active learning users: acquisition functions can be easily exploited, and
active learning should be deployed with caution in trusted data scenarios.

</details>


### [309] [Fact2Fiction: Targeted Poisoning Attack to Agentic Fact-checking System](https://arxiv.org/abs/2508.06059)
*Haorui He,Yupeng Li,Bin Benjamin Zhu,Dacheng Wen,Reynold Cheng,Francis C. M. Lau*

Main category: cs.CR

TL;DR: Fact2Fiction is a poisoning attack framework targeting LLM-based fact-checking systems, achieving higher success rates than existing attacks.


<details>
  <summary>Details</summary>
Motivation: The security of autonomous fact-checking systems is critical, as compromised systems can amplify misinformation.

Method: Fact2Fiction mirrors the decomposition strategy of fact-checkers and exploits system-generated justifications to craft malicious evidence.

Result: The attack achieves 8.9%‚Äì21.2% higher success rates than state-of-the-art attacks.

Conclusion: Fact2Fiction exposes security weaknesses in fact-checking systems, emphasizing the need for defensive measures.

Abstract: State-of-the-art fact-checking systems combat misinformation at scale by
employing autonomous LLM-based agents to decompose complex claims into smaller
sub-claims, verify each sub-claim individually, and aggregate the partial
results to produce verdicts with justifications (explanatory rationales for the
verdicts). The security of these systems is crucial, as compromised
fact-checkers, which tend to be easily underexplored, can amplify
misinformation. This work introduces Fact2Fiction, the first poisoning attack
framework targeting such agentic fact-checking systems. Fact2Fiction mirrors
the decomposition strategy and exploits system-generated justifications to
craft tailored malicious evidences that compromise sub-claim verification.
Extensive experiments demonstrate that Fact2Fiction achieves 8.9\%--21.2\%
higher attack success rates than state-of-the-art attacks across various
poisoning budgets. Fact2Fiction exposes security weaknesses in current
fact-checking systems and highlights the need for defensive countermeasures.

</details>


### [310] [MM-FusionNet: Context-Aware Dynamic Fusion for Multi-modal Fake News Detection with Large Vision-Language Models](https://arxiv.org/abs/2508.05684)
*Junhao He,Tianyu Liu,Jingyuan Zhao,Benjamin Turner*

Main category: cs.CR

TL;DR: MM-FusionNet, a framework using LVLMs for fake news detection, introduces CADFM for dynamic fusion of text and image features, achieving a high F1-score of 0.938.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of text-only fake news detection by leveraging multi-modal data (text and images) for improved accuracy.

Method: Uses Context-Aware Dynamic Fusion Module (CADFM) with bi-directional cross-modal attention and dynamic modal gating to adaptively weight features.

Result: Achieves state-of-the-art F1-score of 0.938, outperforming existing multi-modal and single-modal methods.

Conclusion: MM-FusionNet is robust, interpretable, and close to human-level performance, making it practical for real-world fake news detection.

Abstract: The proliferation of multi-modal fake news on social media poses a
significant threat to public trust and social stability. Traditional detection
methods, primarily text-based, often fall short due to the deceptive interplay
between misleading text and images. While Large Vision-Language Models (LVLMs)
offer promising avenues for multi-modal understanding, effectively fusing
diverse modal information, especially when their importance is imbalanced or
contradictory, remains a critical challenge. This paper introduces
MM-FusionNet, an innovative framework leveraging LVLMs for robust multi-modal
fake news detection. Our core contribution is the Context-Aware Dynamic Fusion
Module (CADFM), which employs bi-directional cross-modal attention and a novel
dynamic modal gating network. This mechanism adaptively learns and assigns
importance weights to textual and visual features based on their contextual
relevance, enabling intelligent prioritization of information. Evaluated on the
large-scale Multi-modal Fake News Dataset (LMFND) comprising 80,000 samples,
MM-FusionNet achieves a state-of-the-art F1-score of 0.938, surpassing existing
multi-modal baselines by approximately 0.5% and significantly outperforming
single-modal approaches. Further analysis demonstrates the model's dynamic
weighting capabilities, its robustness to modality perturbations, and
performance remarkably close to human-level, underscoring its practical
efficacy and interpretability for real-world fake news detection.

</details>


### [311] [Leveraging large language models for SQL behavior-based database intrusion detection](https://arxiv.org/abs/2508.05690)
*Meital Shlezinger,Shay Akirav,Lei Zhou,Liang Guo,Avi Kessel,Guoliang Li*

Main category: cs.CR

TL;DR: A two-tiered anomaly detection approach for SQL using DistilBERT combines unsupervised and supervised methods to detect database intrusions with high precision, minimizing data labeling needs.


<details>
  <summary>Details</summary>
Motivation: The rise in abnormal database access behaviors, especially from internal and external masqueraders, highlights the need for granular anomaly detection at the operational level, which current methods lack.

Method: The approach uses unsupervised ensemble anomaly detectors for out-of-scope queries and supervised fine-tuned transformer models (DistilBERT) for in-scope queries, leveraging role-labeled classification.

Result: The method effectively identifies anomalous activities, including sophisticated threats, with high precision, even with limited labeled SQL data.

Conclusion: This approach significantly contributes to safeguarding critical database systems by addressing the limitations of existing detection methods.

Abstract: Database systems are extensively used to store critical data across various
domains. However, the frequency of abnormal database access behaviors, such as
database intrusion by internal and external attacks, continues to rise.
Internal masqueraders often have greater organizational knowledge, making it
easier to mimic employee behavior effectively. In contrast, external
masqueraders may behave differently due to their lack of familiarity with the
organization. Current approaches lack the granularity needed to detect
anomalies at the operational level, frequently misclassifying entire sequences
of operations as anomalies, even though most operations are likely to represent
normal behavior. On the other hand, some anomalous behaviors often resemble
normal activities, making them difficult for existing detection methods to
identify. This paper introduces a two-tiered anomaly detection approach for
Structured Query Language (SQL) using the Bidirectional Encoder Representations
from Transformers (BERT) model, specifically DistilBERT, a more efficient,
pre-trained version. Our method combines both unsupervised and supervised
machine learning techniques to accurately identify anomalous activities while
minimizing the need for data labeling. First, the unsupervised method uses
ensemble anomaly detectors that flag embedding vectors distant from learned
normal patterns of typical user behavior across the database (out-of-scope
queries). Second, the supervised method uses fine-tuned transformer-based
models to detect internal attacks with high precision (in-scope queries), using
role-labeled classification, even on limited labeled SQL data. Our findings
make a significant contribution by providing an effective solution for
safeguarding critical database systems from sophisticated threats.

</details>


### [312] [Log2Sig: Frequency-Aware Insider Threat Detection via Multivariate Behavioral Signal Decomposition](https://arxiv.org/abs/2508.05696)
*Kaichuan Kong,Dongjie Liu,Xiaobo Jin,Zhiying Li,Guanggang Geng*

Main category: cs.CR

TL;DR: Log2Sig is a novel framework for insider threat detection that transforms user logs into behavioral frequency signals, uses MVMD for multiscale analysis, and combines temporal and frequency features for improved anomaly detection.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to capture frequency dynamics and multiscale patterns in user behavior, limiting their effectiveness in detecting deceptive insider threats.

Method: Log2Sig converts logs into frequency signals, uses MVMD to extract IMFs, and jointly models behavioral sequences (via Mamba-based encoder) and frequency components for anomaly detection.

Result: Log2Sig outperforms state-of-the-art methods on CERT r4.2 and r5.2 datasets in accuracy and F1 score.

Conclusion: Log2Sig provides a robust solution for insider threat detection by leveraging multiscale behavioral analysis and dual-view feature fusion.

Abstract: Insider threat detection presents a significant challenge due to the
deceptive nature of malicious behaviors, which often resemble legitimate user
operations. However, existing approaches typically model system logs as flat
event sequences, thereby failing to capture the inherent frequency dynamics and
multiscale disturbance patterns embedded in user behavior. To address these
limitations, we propose Log2Sig, a robust anomaly detection framework that
transforms user logs into multivariate behavioral frequency signals,
introducing a novel representation of user behavior. Log2Sig employs
Multivariate Variational Mode Decomposition (MVMD) to extract Intrinsic Mode
Functions (IMFs), which reveal behavioral fluctuations across multiple temporal
scales. Based on this, the model further performs joint modeling of behavioral
sequences and frequency-decomposed signals: the daily behavior sequences are
encoded using a Mamba-based temporal encoder to capture long-term dependencies,
while the corresponding frequency components are linearly projected to match
the encoder's output dimension. These dual-view representations are then fused
to construct a comprehensive user behavior profile, which is fed into a
multilayer perceptron for precise anomaly detection. Experimental results on
the CERT r4.2 and r5.2 datasets demonstrate that Log2Sig significantly
outperforms state-of-the-art baselines in both accuracy and F1 score.

</details>


### [313] [ScamAgents: How AI Agents Can Simulate Human-Level Scam Calls](https://arxiv.org/abs/2508.06457)
*Sanket Badhe*

Main category: cs.CR

TL;DR: ScamAgent, an LLM-based multi-turn agent, generates realistic scam scripts, bypassing current safety measures, highlighting the need for improved safeguards.


<details>
  <summary>Details</summary>
Motivation: To demonstrate the misuse potential of LLMs in generating realistic scam calls and expose gaps in existing safety mechanisms.

Method: Developed ScamAgent, an autonomous agent with dialogue memory and dynamic adaptation to simulate fraud scenarios, and tested its evasion of LLM safety guardrails.

Result: Current LLM safety measures fail against agent-based threats; scam scripts can be transformed into lifelike voice calls.

Conclusion: Urgent need for multi-turn safety auditing, agent-level controls, and methods to counter AI-powered conversational deception.

Abstract: Large Language Models (LLMs) have demonstrated impressive fluency and
reasoning capabilities, but their potential for misuse has raised growing
concern. In this paper, we present ScamAgent, an autonomous multi-turn agent
built on top of LLMs, capable of generating highly realistic scam call scripts
that simulate real-world fraud scenarios. Unlike prior work focused on
single-shot prompt misuse, ScamAgent maintains dialogue memory, adapts
dynamically to simulated user responses, and employs deceptive persuasion
strategies across conversational turns. We show that current LLM safety
guardrails, including refusal mechanisms and content filters, are ineffective
against such agent-based threats. Even models with strong prompt-level
safeguards can be bypassed when prompts are decomposed, disguised, or delivered
incrementally within an agent framework. We further demonstrate the
transformation of scam scripts into lifelike voice calls using modern
text-to-speech systems, completing a fully automated scam pipeline. Our
findings highlight an urgent need for multi-turn safety auditing, agent-level
control frameworks, and new methods to detect and disrupt conversational
deception powered by generative AI.

</details>


### [314] [MambaITD: An Efficient Cross-Modal Mamba Network for Insider Threat Detection](https://arxiv.org/abs/2508.05695)
*Kaichuan Kong,Dongjie Liu,Xiaobo Jin,Zhiying Li,Guanggang Geng,Jian Weng*

Main category: cs.CR

TL;DR: MambaITD is a new insider threat detection framework using Mamba state space model and cross-modal fusion, addressing inefficiencies in current methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail due to poor temporal dynamic feature modeling, computational inefficiency, and cross-modal data isolation.

Method: Uses multi-source log preprocessing, Mamba encoder for long-range dependencies, gated feature fusion, and adaptive threshold optimization.

Result: Outperforms traditional and Transformer-based methods in efficiency and feature fusion.

Conclusion: MambaITD provides a more effective solution for insider threat detection.

Abstract: Enterprises are facing increasing risks of insider threats, while existing
detection methods are unable to effectively address these challenges due to
reasons such as insufficient temporal dynamic feature modeling, computational
efficiency and real-time bottlenecks and cross-modal information island
problem. This paper proposes a new insider threat detection framework MambaITD
based on the Mamba state space model and cross-modal adaptive fusion. First,
the multi-source log preprocessing module aligns heterogeneous data through
behavioral sequence encoding, interval smoothing, and statistical feature
extraction. Second, the Mamba encoder models long-range dependencies in
behavioral and interval sequences, and combines the sequence and statistical
information dynamically in combination with the gated feature fusion mechanism.
Finally, we propose an adaptive threshold optimization method based on
maximizing inter-class variance, which dynamically adjusts the decision
threshold by analyzing the probability distribution, effectively identifies
anomalies, and alleviates class imbalance and concept drift. Compared with
traditional methods, MambaITD shows significant advantages in modeling
efficiency and feature fusion capabilities, outperforming Transformer-based
methods, and provides a more effective solution for insider threat detection.

</details>


### [315] [Adaptive Backtracking for Privacy Protection in Large Language Models](https://arxiv.org/abs/2508.06087)
*Zhihao Yao,Yuxuan Gu,Xiachong Feng,Weitao Ma,Bo Li,Xiaocheng Feng*

Main category: cs.CR

TL;DR: The paper introduces enterprise-oriented privacy concerns in AI, addressing performance degradation and lack of datasets with ABack and PriGenQA, achieving a 15% privacy utility improvement.


<details>
  <summary>Details</summary>
Motivation: Current privacy work overlooks enterprise data leakage risks in Retrieval-Augmented Generation, creating a gap in addressing these concerns.

Method: Proposes ABack, a training-free mechanism using a Hidden State Model, and constructs PriGenQA, a benchmark for enterprise privacy.

Result: ABack improves privacy utility by 15% over baselines against adaptive attackers.

Conclusion: The solutions effectively address enterprise privacy challenges without performance trade-offs.

Abstract: The preservation of privacy has emerged as a critical topic in the era of
artificial intelligence. However, current work focuses on user-oriented
privacy, overlooking severe enterprise data leakage risks exacerbated by the
Retrieval-Augmented Generation paradigm. To address this gap, our paper
introduces a novel objective: enterprise-oriented privacy concerns. Achieving
this objective requires overcoming two fundamental challenges: existing methods
such as data sanitization severely degrade model performance, and the field
lacks public datasets for evaluation. We address these challenges with several
solutions. (1) To prevent performance degradation, we propose ABack, a
training-free mechanism that leverages a Hidden State Model to pinpoint the
origin of a leakage intention and rewrite the output safely. (2) To solve the
lack of datasets, we construct PriGenQA, a new benchmark for enterprise privacy
scenarios in healthcare and finance. To ensure a rigorous evaluation, we move
beyond simple static attacks by developing a powerful adaptive attacker with
Group Relative Policy Optimization. Experiments show that against this superior
adversary, ABack improves the overall privacy utility score by up to 15\% over
strong baselines, avoiding the performance trade-offs of prior methods.

</details>


### [316] [Universally Unfiltered and Unseen:Input-Agnostic Multimodal Jailbreaks against Text-to-Image Model Safeguards](https://arxiv.org/abs/2508.05658)
*Song Yan,Hui Wei,Jinlong Fei,Guoliang Yang,Zhengyu Zhao,Zheng Wamg*

Main category: cs.CR

TL;DR: The paper introduces U3-Attack, a scalable and efficient multimodal jailbreak method to bypass Text-to-Image (T2I) safeguards, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing jailbreak methods for T2I safeguards are limited by poor scalability and inefficiency, prompting the need for a more universal solution.

Method: U3-Attack optimizes an adversarial patch for image backgrounds and a safe paraphrase set for text prompts to bypass safety checkers and filters universally.

Result: U3-Attack achieves ~4√ó higher success rates than state-of-the-art methods on commercial T2I models like Runway-inpainting.

Conclusion: U3-Attack effectively addresses scalability and efficiency issues in bypassing T2I safeguards, demonstrating superior performance.

Abstract: Various (text) prompt filters and (image) safety checkers have been
implemented to mitigate the misuse of Text-to-Image (T2I) models in creating
Not-Safe-For-Work (NSFW) content.In order to expose potential security
vulnerabilities of such safeguards, multimodal jailbreaks have been
studied.However, existing jailbreaks are limited to prompt-specific and
image-specific perturbations, which suffer from poor scalability and
time-consuming optimization.To address these limitations, we propose
Universally Unfiltered and Unseen (U3)-Attack, a multimodal jailbreak attack
method against T2I safeguards.Specifically, U3-Attack optimizes an adversarial
patch on the image background to universally bypass safety checkers and
optimizes a safe paraphrase set from a sensitive word to universally bypass
prompt filters while eliminating redundant computations.Extensive experimental
results demonstrate the superiority of our U3-Attack on both open-source and
commercial T2I models.For example, on the commercial Runway-inpainting model
with both prompt filter and safety checker, our U3-Attack achieves $~4\times$
higher success rates than the state-of-the-art multimodal jailbreak attack,
MMA-Diffusion.Content Warning: This paper includes examples of NSFW content.

</details>


### [317] [Anti-Tamper Protection for Unauthorized Individual Image Generation](https://arxiv.org/abs/2508.06325)
*Zelin Li,Ruohan Zong,Yifan Liu,Ruichen Yao,Yaokun Liu,Yang Zhang,Dong Wang*

Main category: cs.CR

TL;DR: ATP introduces a tamper-proof mechanism combining protection and authorization perturbations to defend against forgery attacks and detect tampering.


<details>
  <summary>Details</summary>
Motivation: Growing concerns about forgery attacks infringing on portrait rights and privacy, and the ineffectiveness of existing protection algorithms against purification techniques.

Method: ATP uses protection and authorization perturbations in the frequency domain, guided by a mask, to defend against attacks and detect tampering.

Result: ATP effectively defends against forgery attacks in various settings, preserving portrait rights and privacy.

Conclusion: ATP provides a robust solution for protecting individuals' rights and privacy against forgery attacks.

Abstract: With the advancement of personalized image generation technologies, concerns
about forgery attacks that infringe on portrait rights and privacy are growing.
To address these concerns, protection perturbation algorithms have been
developed to disrupt forgery generation. However, the protection algorithms
would become ineffective when forgery attackers apply purification techniques
to bypass the protection. To address this issue, we present a novel approach,
Anti-Tamper Perturbation (ATP). ATP introduces a tamper-proof mechanism within
the perturbation. It consists of protection and authorization perturbations,
where the protection perturbation defends against forgery attacks, while the
authorization perturbation detects purification-based tampering. Both
protection and authorization perturbations are applied in the frequency domain
under the guidance of a mask, ensuring that the protection perturbation does
not disrupt the authorization perturbation. This design also enables the
authorization perturbation to be distributed across all image pixels,
preserving its sensitivity to purification-based tampering. ATP demonstrates
its effectiveness in defending forgery attacks across various attack settings
through extensive experiments, providing a robust solution for protecting
individuals' portrait rights and privacy. Our code is available at:
https://github.com/Seeyn/Anti-Tamper-Perturbation .

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [318] [Identity Increases Stability in Neural Cellular Automata](https://arxiv.org/abs/2508.06389)
*James Stovold*

Main category: cs.NE

TL;DR: The paper introduces an 'identity' layer to improve the stability of Neural Cellular Automata (NCA)-grown organisms, showing increased stability and emergent movement.


<details>
  <summary>Details</summary>
Motivation: Address instability issues in NCA-grown organisms, such as tumor-like growth or shape breakdown.

Method: Introduce an 'identity' layer with simple constraints during training.

Result: Stability improves, and emergent movement is observed, especially with multiple identity values.

Conclusion: This method enables further study of interactions between NCA-grown organisms, advancing research on artificial cellular social behavior.

Abstract: Neural Cellular Automata (NCAs) offer a way to study the growth of
two-dimensional artificial organisms from a single seed cell. From the outset,
NCA-grown organisms have had issues with stability, their natural boundary
often breaking down and exhibiting tumour-like growth or failing to maintain
the expected shape. In this paper, we present a method for improving the
stability of NCA-grown organisms by introducing an 'identity' layer with simple
constraints during training.
  Results show that NCAs grown in close proximity are more stable compared with
the original NCA model. Moreover, only a single identity value is required to
achieve this increase in stability. We observe emergent movement from the
stable organisms, with increasing prevalence for models with multiple identity
values.
  This work lays the foundation for further study of the interaction between
NCA-grown organisms, paving the way for studying social interaction at a
cellular level in artificial organisms.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [319] [Enhancing the Scalability of Classical Surrogates for Real-World Quantum Machine Learning Applications](https://arxiv.org/abs/2508.06131)
*Philip Anton Hernicht,Alona Sakhnenko,Corey O'Meara,Giorgio Cortiana,Jeanette Miriam Lorenz*

Main category: quant-ph

TL;DR: The paper proposes a lightweight classical surrogate method for quantum models, reducing computational demands and enabling industrial-scale deployment of QML solutions.


<details>
  <summary>Details</summary>
Motivation: Limited access to quantum hardware hinders QML adoption, necessitating classical alternatives for inference.

Method: An optimized pipeline minimizes redundancies in generating classical surrogates, requiring fewer resources than prior methods.

Result: The method achieves high accuracy with linear computational scaling, demonstrated on a real-world energy forecasting problem.

Conclusion: This approach facilitates industrial QML integration and serves as a tool for exploring practical quantum advantage.

Abstract: Quantum machine learning (QML) presents potential for early industrial
adoption, yet limited access to quantum hardware remains a significant
bottleneck for deployment of QML solutions. This work explores the use of
classical surrogates to bypass this restriction, which is a technique that
allows to build a lightweight classical representation of a (trained) quantum
model, enabling to perform inference on entirely classical devices. We reveal
prohibiting high computational demand associated with previously proposed
methods for generating classical surrogates from quantum models, and propose an
alternative pipeline enabling generation of classical surrogates at a larger
scale than was previously possible. Previous methods required at least a
high-performance computing (HPC) system for quantum models of below industrial
scale (ca. 20 qubits), which raises questions about its practicality. We
greatly minimize the redundancies of the previous approach, utilizing only a
minute fraction of the resources previously needed. We demonstrate the
effectiveness of our method on a real-world energy demand forecasting problem,
conducting rigorous testing of performance and computation demand in both
simulations and on quantum hardware. Our results indicate that our method
achieves high accuracy on the testing dataset while its computational resource
requirements scale linearly rather than exponentially. This work presents a
lightweight approach to transform quantum solutions into classically deployable
versions, facilitating faster integration of quantum technology in industrial
settings. Furthermore, it can serve as a powerful research tool in search
practical quantum advantage in an empirical setup.

</details>


<div id='physics.soc-ph'></div>

# physics.soc-ph [[Back]](#toc)

### [320] [Intuition emerges in Maximum Caliber models at criticality](https://arxiv.org/abs/2508.06477)
*Llu√≠s Arola-Fern√°ndez*

Main category: physics.soc-ph

TL;DR: The paper explores whether large predictive models develop genuine intuition or just mimic data, identifying a metastable phase of learning that balances prediction and entropy.


<details>
  <summary>Details</summary>
Motivation: To understand if predictive models can exhibit true intuition beyond mere data replication.

Method: Uses mind-tuning with a control parameter Œª to study phases of learning in deterministic mazes, analyzing imitation, hallucination, and intuition.

Result: Reveals a phase diagram with distinct behaviors (imitation, hallucination, and intuition) and multistability, captured by a low-dimensional theory.

Conclusion: Intuition emerges as a critical balance between memorization and exploration, framed as an emergent property.

Abstract: Whether large predictive models merely parrot their training data or produce
genuine insight lacks a physical explanation. This work reports a primitive
form of intuition that emerges as a metastable phase of learning that
critically balances next-token prediction against future path-entropy. The
intuition mechanism is discovered via mind-tuning, the minimal principle that
imposes Maximum Caliber in predictive models with a control temperature-like
parameter $\lambda$. Training on random walks in deterministic mazes reveals a
rich phase diagram: imitation (low $\lambda$), rule-breaking hallucination
(high $\lambda$), and a fragile in-between window exhibiting strong
protocol-dependence (hysteresis) and multistability, where models spontaneously
discover novel goal-directed strategies. These results are captured by an
effective low-dimensional theory and frame intuition as an emergent property at
the critical balance between memorizing what is and wondering what could be.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [321] [A Physiologically-Constrained Neural Network Digital Twin Framework for Replicating Glucose Dynamics in Type 1 Diabetes](https://arxiv.org/abs/2508.05705)
*Valentina Roquemen-Echeverri,Taisa Kushner,Peter G. Jacobs,Clara Mosquera-Lopez*

Main category: q-bio.QM

TL;DR: The paper introduces physiologically-constrained neural network digital twins to simulate glucose dynamics in type 1 diabetes, validated with real-world data showing clinically equivalent results.


<details>
  <summary>Details</summary>
Motivation: Existing models for glucose dynamics in type 1 diabetes lack key physiological aspects and personalization, limiting their clinical utility.

Method: A population-level NN state-space model aligned with ODEs is created, then personalized with individual data. Validation uses real-world data from the T1D Exercise Initiative study.

Result: Simulated glucose outcomes matched observed data closely (e.g., time in range: 75.1% vs. 74.4%).

Conclusion: The framework enables personalized treatment testing and integrates physics-based and data-driven modeling, enhancing clinical decision-making.

Abstract: Simulating glucose dynamics in individuals with type 1 diabetes (T1D) is
critical for developing personalized treatments and supporting data-driven
clinical decisions. Existing models often miss key physiological aspects and
are difficult to individualize. Here, we introduce physiologically-constrained
neural network (NN) digital twins to simulate glucose dynamics in T1D. To
ensure interpretability and physiological consistency, we first build a
population-level NN state-space model aligned with a set of ordinary
differential equations (ODEs) describing glucose regulation. This model is
formally verified to conform to known T1D dynamics. Digital twins are then
created by augmenting the population model with individual-specific models,
which include personal data, such as glucose management and contextual
information, capturing both inter- and intra-individual variability. We
validate our approach using real-world data from the T1D Exercise Initiative
study. Two weeks of data per participant were split into 5-hour sequences and
simulated glucose profiles were compared to observed ones. Clinically relevant
outcomes were used to assess similarity via paired equivalence t-tests with
predefined clinical equivalence margins. Across 394 digital twins, glucose
outcomes were equivalent between simulated and observed data: time in range
(70-180 mg/dL) was 75.1$\pm$21.2% (simulated) vs. 74.4$\pm$15.4% (real;
P<0.001); time below range (<70 mg/dL) 2.5$\pm$5.2% vs. 3.0$\pm$3.3% (P=0.022);
and time above range (>180 mg/dL) 22.4$\pm$22.0% vs. 22.6$\pm$15.9% (P<0.001).
Our framework can incorporate unmodeled factors like sleep and activity while
preserving key dynamics. This approach enables personalized in silico testing
of treatments, supports insulin optimization, and integrates physics-based and
data-driven modeling. Code: https://github.com/mosqueralopez/T1DSim_AI

</details>


<div id='cs.DM'></div>

# cs.DM [[Back]](#toc)

### [322] [On Approximate MMS Allocations on Restricted Graph Classes](https://arxiv.org/abs/2508.06343)
*V√°clav Bla≈æej,Micha≈Ç Dƒôbski ad Zbigniew Lonc,Marta Piecyk,Pawe≈Ç RzƒÖ≈ºewski*

Main category: cs.DM

TL;DR: The paper investigates fair division of indivisible goods with connectivity constraints, focusing on approximate allocations that guarantee a fraction of the maximin share value for agents. It extends known results to new graph classes.


<details>
  <summary>Details</summary>
Motivation: The problem arises from the need to fairly allocate goods represented as vertices in a graph, ensuring connected subgraphs for agents, even when exact maximin share allocations don't exist.

Method: The study systematically examines restricted graph classes to determine if approximate allocations exist, building on prior work for complete graphs, cycles, and other classes.

Result: The paper proves the existence of such approximate allocations for block graphs, cacti, complete multipartite graphs, and split graphs.

Conclusion: The findings extend the understanding of fair division under connectivity constraints, providing solutions for additional graph classes and highlighting open problems for general graphs.

Abstract: We study the problem of fair division of a set of indivisible goods with
connectivity constraints. Specifically, we assume that the goods are
represented as vertices of a connected graph, and sets of goods allocated to
the agents are connected subgraphs of this graph. We focus on the
widely-studied maximin share criterion of fairness. It has been shown that an
allocation satisfying this criterion may not exist even without connectivity
constraints, i.e., if the graph of goods is complete. In view of this, it is
natural to seek approximate allocations that guarantee each agent a connected
bundle of goods with value at least a constant fraction of the maximin share
value to the agent. It is known that for some classes of graphs, such as
complete graphs, cycles, and $d$-claw-free graphs for any fixed $d$, such
approximate allocations indeed exist. However, it is an open problem whether
they exist for the class of all graphs.
  In this paper, we continue the systematic study of the existence of
approximate allocations on restricted graph classes. In particular, we show
that such allocations exist for several well-studied classes, including block
graphs, cacti, complete multipartite graphs, and split graphs.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [323] [A Systematic Literature Review of Retrieval-Augmented Generation: Techniques, Metrics, and Challenges](https://arxiv.org/abs/2508.06401)
*Andrew Brown,Muhammad Roman,Barry Devereux*

Main category: cs.DL

TL;DR: A systematic review of retrieval-augmented generation (RAG) studies (2020-2025) analyzing 128 papers, focusing on datasets, architectures, and effectiveness, while addressing citation-lag bias.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive analysis of RAG research, identify gaps, and guide future directions.

Method: Used PRISMA 2020 framework with citation-based inclusion criteria, analyzed datasets, architectures, and evaluation practices.

Result: Synthesized empirical evidence on RAG's effectiveness and limitations, highlighting emerging breakthroughs.

Conclusion: Clarifies the RAG research landscape, identifies gaps, and suggests priority directions for future work.

Abstract: This systematic review of the research literature on retrieval-augmented
generation (RAG) provides a focused analysis of the most highly cited studies
published between 2020 and May 2025. A total of 128 articles met our inclusion
criteria. The records were retrieved from ACM Digital Library, IEEE Xplore,
Scopus, ScienceDirect, and the Digital Bibliography and Library Project (DBLP).
RAG couples a neural retriever with a generative language model, grounding
output in up-to-date, non-parametric memory while retaining the semantic
generalisation stored in model weights. Guided by the PRISMA 2020 framework, we
(i) specify explicit inclusion and exclusion criteria based on citation count
and research questions, (ii) catalogue datasets, architectures, and evaluation
practices, and (iii) synthesise empirical evidence on the effectiveness and
limitations of RAG. To mitigate citation-lag bias, we applied a lower
citation-count threshold to papers published in 2025 so that emerging
breakthroughs with naturally fewer citations were still captured. This review
clarifies the current research landscape, highlights methodological gaps, and
charts priority directions for future research.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [324] [NanoCodec: Towards High-Quality Ultra Fast Speech LLM Inference](https://arxiv.org/abs/2508.05835)
*Edresson Casanova,Paarth Neekhara,Ryan Langman,Shehzeen Hussain,Subhankar Ghosh,Xuesong Yang,Ante Jukiƒá,Jason Li,Boris Ginsburg*

Main category: eess.AS

TL;DR: NanoCodec, a low frame-rate audio codec, improves efficiency in Speech LLMs by reducing autoregressive steps while maintaining high-quality compression.


<details>
  <summary>Details</summary>
Motivation: Existing audio codecs operate at high frame rates, causing slow training and inference for autoregressive models in Speech LLMs.

Method: Ablation studies on frame rate, bitrate, and causality, leading to the development of NanoCodec at 12.5 FPS.

Result: NanoCodec outperforms existing codecs across bitrate ranges, setting a new benchmark for efficiency.

Conclusion: NanoCodec enables low-latency, efficient training and inference for Speech LLMs.

Abstract: Large Language Models (LLMs) have significantly advanced audio processing by
leveraging audio codecs to discretize audio into tokens, enabling the
application of language modeling techniques to speech data. However, existing
audio codecs often operate at high frame rates, leading to slow training and
inference, particularly for autoregressive models. To address this, there is
growing interest in low frame-rate audio codecs, which reduce the number of
autoregressive steps required to generate one second of audio. In this paper,
we conduct ablation studies to examine the impact of frame rate, bitrate, and
causality on codec reconstruction quality. Based on our findings, we introduce
NanoCodec, a state-of-the-art audio codec that achieves high-quality
compression at just 12.5 frames per second (FPS). NanoCodec outperforms related
works across various bitrate ranges, establishing a new benchmark for
low-latency and efficient Speech LLM training and inference.

</details>
