<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 88]
- [cs.AI](#cs.AI) [Total: 11]
- [cs.CL](#cs.CL) [Total: 42]
- [cs.LG](#cs.LG) [Total: 86]
- [cs.DC](#cs.DC) [Total: 4]
- [eess.AS](#eess.AS) [Total: 3]
- [q-fin.PM](#q-fin.PM) [Total: 1]
- [cs.GT](#cs.GT) [Total: 1]
- [cs.DS](#cs.DS) [Total: 1]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [cs.DB](#cs.DB) [Total: 1]
- [eess.IV](#eess.IV) [Total: 12]
- [eess.SP](#eess.SP) [Total: 2]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [econ.GN](#econ.GN) [Total: 1]
- [cs.MA](#cs.MA) [Total: 3]
- [cs.NE](#cs.NE) [Total: 2]
- [math.NA](#math.NA) [Total: 3]
- [cs.CE](#cs.CE) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.HC](#cs.HC) [Total: 5]
- [cs.CG](#cs.CG) [Total: 1]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [cs.FL](#cs.FL) [Total: 1]
- [astro-ph.EP](#astro-ph.EP) [Total: 1]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 1]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 1]
- [stat.ML](#stat.ML) [Total: 4]
- [cs.RO](#cs.RO) [Total: 9]
- [cs.CY](#cs.CY) [Total: 5]
- [cs.CR](#cs.CR) [Total: 6]
- [cs.ET](#cs.ET) [Total: 1]
- [math.OC](#math.OC) [Total: 1]
- [math-ph](#math-ph) [Total: 1]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.SD](#cs.SD) [Total: 3]
- [cs.IT](#cs.IT) [Total: 2]
- [cs.IR](#cs.IR) [Total: 8]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Geometry-aware 4D Video Generation for Robot Manipulation](https://arxiv.org/abs/2507.01099)
*Zeyi Liu,Shuang Li,Eric Cousineau,Siyuan Feng,Benjamin Burchfiel,Shuran Song*

Main category: cs.CV

TL;DR: A 4D video generation model ensures multi-view 3D consistency for predicting future video sequences from novel viewpoints, improving robotic planning and interaction.


<details>
  <summary>Details</summary>
Motivation: Enhancing robots' ability to understand and predict dynamic environments for effective planning and interaction.

Method: Proposes a 4D video generation model with cross-view pointmap alignment supervision to enforce 3D consistency.

Result: Produces more stable and spatially aligned predictions across datasets, enabling trajectory recovery for robot manipulation.

Conclusion: The model supports robust robot manipulation and generalization to novel viewpoints without requiring camera poses.

Abstract: Understanding and predicting the dynamics of the physical world can enhance a
robot's ability to plan and interact effectively in complex environments. While
recent video generation models have shown strong potential in modeling dynamic
scenes, generating videos that are both temporally coherent and geometrically
consistent across camera views remains a significant challenge. To address
this, we propose a 4D video generation model that enforces multi-view 3D
consistency of videos by supervising the model with cross-view pointmap
alignment during training. This geometric supervision enables the model to
learn a shared 3D representation of the scene, allowing it to predict future
video sequences from novel viewpoints based solely on the given RGB-D
observations, without requiring camera poses as inputs. Compared to existing
baselines, our method produces more visually stable and spatially aligned
predictions across multiple simulated and real-world robotic datasets. We
further show that the predicted 4D videos can be used to recover robot
end-effector trajectories using an off-the-shelf 6DoF pose tracker, supporting
robust robot manipulation and generalization to novel camera viewpoints.

</details>


### [2] [Landslide Detection and Mapping Using Deep Learning Across Multi-Source Satellite Data and Geographic Regions](https://arxiv.org/abs/2507.01123)
*Rahul A. Burange,Harsh K. Shinde,Omkar Mutyalwar*

Main category: cs.CV

TL;DR: The paper presents a deep learning and multi-source remote sensing approach for accurate landslide detection and prediction, leveraging satellite imagery and geospatial analysis.


<details>
  <summary>Details</summary>
Motivation: Landslides threaten infrastructure, economies, and lives, requiring improved detection and prediction methods.

Method: Integrates Sentinel-2 and ALOS PALSAR data with deep learning models (U-Net, DeepLabV3+, Res-Net) for landslide identification and geospatial analysis.

Result: The framework enhances landslide detection accuracy and supports early warning systems and disaster management.

Conclusion: Deep learning and multi-source remote sensing offer scalable, transferable solutions for robust landslide prediction.

Abstract: Landslides pose severe threats to infrastructure, economies, and human lives,
necessitating accurate detection and predictive mapping across diverse
geographic regions. With advancements in deep learning and remote sensing,
automated landslide detection has become increasingly effective. This study
presents a comprehensive approach integrating multi-source satellite imagery
and deep learning models to enhance landslide identification and prediction. We
leverage Sentinel-2 multispectral data and ALOS PALSAR-derived slope and
Digital Elevation Model (DEM) layers to capture critical environmental features
influencing landslide occurrences. Various geospatial analysis techniques are
employed to assess the impact of terra in characteristics, vegetation cover,
and rainfall on detection accuracy. Additionally, we evaluate the performance
of multiple stateof-the-art deep learning segmentation models, including U-Net,
DeepLabV3+, and Res-Net, to determine their effectiveness in landslide
detection. The proposed framework contributes to the development of reliable
early warning systems, improved disaster risk management, and sustainable
land-use planning. Our findings provide valuable insights into the potential of
deep learning and multi-source remote sensing in creating robust, scalable, and
transferable landslide prediction models.

</details>


### [3] [cp_measure: API-first feature extraction for image-based profiling workflows](https://arxiv.org/abs/2507.01163)
*Alán F. Muñoz,Tim Treis,Alexandr A. Kalinin,Shatavisha Dasgupta,Fabian Theis,Anne E. Carpenter,Shantanu Singh*

Main category: cs.CV

TL;DR: cp_measure is a Python library that modularizes CellProfiler's feature extraction for automated, reproducible image-based profiling in computational biology.


<details>
  <summary>Details</summary>
Motivation: Current tools like CellProfiler hinder automated and reproducible analyses for machine learning workflows in biological image analysis.

Method: cp_measure extracts CellProfiler's core measurement capabilities into a modular, API-first tool for programmatic feature extraction.

Result: cp_measure features retain high fidelity with CellProfiler and integrate seamlessly with the scientific Python ecosystem, demonstrated in 3D astrocyte imaging and spatial transcriptomics.

Conclusion: cp_measure enables scalable, reproducible, and automated image-based profiling pipelines for machine learning in computational biology.

Abstract: Biological image analysis has traditionally focused on measuring specific
visual properties of interest for cells or other entities. A complementary
paradigm gaining increasing traction is image-based profiling - quantifying
many distinct visual features to form comprehensive profiles which may reveal
hidden patterns in cellular states, drug responses, and disease mechanisms.
While current tools like CellProfiler can generate these feature sets, they
pose significant barriers to automated and reproducible analyses, hindering
machine learning workflows. Here we introduce cp_measure, a Python library that
extracts CellProfiler's core measurement capabilities into a modular, API-first
tool designed for programmatic feature extraction. We demonstrate that
cp_measure features retain high fidelity with CellProfiler features while
enabling seamless integration with the scientific Python ecosystem. Through
applications to 3D astrocyte imaging and spatial transcriptomics, we showcase
how cp_measure enables reproducible, automated image-based profiling pipelines
that scale effectively for machine learning applications in computational
biology.

</details>


### [4] [Rapid Salient Object Detection with Difference Convolutional Neural Networks](https://arxiv.org/abs/2507.01182)
*Zhuo Su,Li Liu,Matthias Müller,Jiehua Zhang,Diana Wofk,Ming-Ming Cheng,Matti Pietikäinen*

Main category: cs.CV

TL;DR: Proposes efficient SOD models (SDNet for images, STDNet for videos) using Pixel Difference Convolutions (PDCs) and a reparameterization strategy (DCR) for real-time performance on resource-constrained devices. Achieves high speed (46 FPS for images, 150 FPS for videos) with superior accuracy.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of deploying SOD on resource-constrained devices with real-time performance, as existing models are computationally expensive.

Method: Combines traditional SOD wisdom with CNNs, using PDCs for feature contrasts and DCR for efficiency. Introduces STDC for video SOD.

Result: Models achieve significant efficiency-accuracy trade-offs, operating at 46 FPS (images) and 150 FPS (videos) with <1M parameters, outperforming competitors.

Conclusion: The proposed models (SDNet, STDNet) offer efficient, accurate SOD solutions for real-time applications on constrained devices.

Abstract: This paper addresses the challenge of deploying salient object detection
(SOD) on resource-constrained devices with real-time performance. While recent
advances in deep neural networks have improved SOD, existing top-leading models
are computationally expensive. We propose an efficient network design that
combines traditional wisdom on SOD and the representation power of modern CNNs.
Like biologically-inspired classical SOD methods relying on computing contrast
cues to determine saliency of image regions, our model leverages Pixel
Difference Convolutions (PDCs) to encode the feature contrasts. Differently,
PDCs are incorporated in a CNN architecture so that the valuable contrast cues
are extracted from rich feature maps. For efficiency, we introduce a difference
convolution reparameterization (DCR) strategy that embeds PDCs into standard
convolutions, eliminating computation and parameters at inference.
Additionally, we introduce SpatioTemporal Difference Convolution (STDC) for
video SOD, enhancing the standard 3D convolution with spatiotemporal contrast
capture. Our models, SDNet for image SOD and STDNet for video SOD, achieve
significant improvements in efficiency-accuracy trade-offs. On a Jetson Orin
device, our models with $<$ 1M parameters operate at 46 FPS and 150 FPS on
streamed images and videos, surpassing the second-best lightweight models in
our experiments by more than $2\times$ and $3\times$ in speed with superior
accuracy. Code will be available at https://github.com/hellozhuo/stdnet.git.

</details>


### [5] [Robust Brain Tumor Segmentation with Incomplete MRI Modalities Using Hölder Divergence and Mutual Information-Enhanced Knowledge Transfer](https://arxiv.org/abs/2507.01254)
*Runze Cheng,Xihang Qiu,Ming Li,Ye Zhang,Chun Li,Fei Yu*

Main category: cs.CV

TL;DR: A robust single-modality parallel processing framework for brain tumor segmentation, effective even with missing MRI modalities, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Conventional methods fail when MRI modalities are missing due to various issues like image quality, protocol inconsistencies, or patient allergies.

Method: Uses Holder divergence and mutual information to maintain modality-specific features and dynamically adjust network parameters based on available inputs.

Result: Achieves high segmentation accuracy on BraTS 2018 and BraTS 2020 datasets, outperforming existing methods with missing modalities.

Conclusion: The proposed framework is robust and effective for brain tumor segmentation, even with incomplete multimodal MRI data.

Abstract: Multimodal MRI provides critical complementary information for accurate brain
tumor segmentation. However, conventional methods struggle when certain
modalities are missing due to issues such as image quality, protocol
inconsistencies, patient allergies, or financial constraints. To address this,
we propose a robust single-modality parallel processing framework that achieves
high segmentation accuracy even with incomplete modalities. Leveraging Holder
divergence and mutual information, our model maintains modality-specific
features while dynamically adjusting network parameters based on the available
inputs. By using these divergence- and information-based loss functions, the
framework effectively quantifies discrepancies between predictions and
ground-truth labels, resulting in consistently accurate segmentation. Extensive
evaluations on the BraTS 2018 and BraTS 2020 datasets demonstrate superior
performance over existing methods in handling missing modalities.

</details>


### [6] [AIGVE-MACS: Unified Multi-Aspect Commenting and Scoring Model for AI-Generated Video Evaluation](https://arxiv.org/abs/2507.01255)
*Xiao Liu,Jiawei Zhang*

Main category: cs.CV

TL;DR: AIGVE-MACS is a new model for evaluating AI-generated videos, providing both numerical scores and detailed comments. It outperforms existing methods and improves video generation quality by 53.5%.


<details>
  <summary>Details</summary>
Motivation: Existing metrics for AI-generated videos lack interpretability and alignment with human evaluation, necessitating a more robust framework.

Method: AIGVE-MACS uses Vision-Language Models with token-wise weighted loss and dynamic frame sampling, trained on AIGVE-BENCH 2, a large-scale dataset of annotated videos.

Result: AIGVE-MACS achieves state-of-the-art performance in scoring and comment quality, outperforming GPT-4o and VideoScore, and enhances video generation by 53.5%.

Conclusion: This work introduces a human-aligned evaluation paradigm for AI-generated videos, with released benchmarks and models for further research.

Abstract: The rapid advancement of AI-generated video models has created a pressing
need for robust and interpretable evaluation frameworks. Existing metrics are
limited to producing numerical scores without explanatory comments, resulting
in low interpretability and human evaluation alignment. To address those
challenges, we introduce AIGVE-MACS, a unified model for AI-Generated Video
Evaluation(AIGVE), which can provide not only numerical scores but also
multi-aspect language comment feedback in evaluating these generated videos.
Central to our approach is AIGVE-BENCH 2, a large-scale benchmark comprising
2,500 AI-generated videos and 22,500 human-annotated detailed comments and
numerical scores across nine critical evaluation aspects. Leveraging
AIGVE-BENCH 2, AIGVE-MACS incorporates recent Vision-Language Models with a
novel token-wise weighted loss and a dynamic frame sampling strategy to better
align with human evaluators. Comprehensive experiments across supervised and
zero-shot benchmarks demonstrate that AIGVE-MACS achieves state-of-the-art
performance in both scoring correlation and comment quality, significantly
outperforming prior baselines including GPT-4o and VideoScore. In addition, we
further showcase a multi-agent refinement framework where feedback from
AIGVE-MACS drives iterative improvements in video generation, leading to 53.5%
quality enhancement. This work establishes a new paradigm for comprehensive,
human-aligned evaluation of AI-generated videos. We release the AIGVE-BENCH 2
and AIGVE-MACS at https://huggingface.co/xiaoliux/AIGVE-MACS.

</details>


### [7] [Advancements in Weed Mapping: A Systematic Review](https://arxiv.org/abs/2507.01269)
*Mohammad Jahanbakht,Alex Olsen,Ross Marchant,Emilie Fillols,Mostafa Rahimi Azghadi*

Main category: cs.CV

TL;DR: A review of weed mapping methods, focusing on data acquisition, processing, and mapping techniques, to improve precision management and sustainability.


<details>
  <summary>Details</summary>
Motivation: To address the lack of comprehensive literature reviews on weed mapping and provide a structured analysis of the entire mapping pipeline.

Method: Systematically examines state-of-the-art methods in data acquisition (sensors, platforms), processing (annotation, modeling), and mapping (spatiotemporal analysis, decision tools) following PRISMA guidelines.

Result: Synthesizes key findings to offer a holistic understanding of weed mapping, highlighting advancements in resolution and site-specific management.

Conclusion: The review serves as a foundational reference for future research, aiming to develop efficient, scalable, and sustainable weed management systems.

Abstract: Weed mapping plays a critical role in precision management by providing
accurate and timely data on weed distribution, enabling targeted control and
reduced herbicide use. This minimizes environmental impacts, supports
sustainable land management, and improves outcomes across agricultural and
natural environments. Recent advances in weed mapping leverage ground-vehicle
Red Green Blue (RGB) cameras, satellite and drone-based remote sensing combined
with sensors such as spectral, Near Infra-Red (NIR), and thermal cameras. The
resulting data are processed using advanced techniques including big data
analytics and machine learning, significantly improving the spatial and
temporal resolution of weed maps and enabling site-specific management
decisions. Despite a growing body of research in this domain, there is a lack
of comprehensive literature reviews specifically focused on weed mapping. In
particular, the absence of a structured analysis spanning the entire mapping
pipeline, from data acquisition to processing techniques and mapping tools,
limits progress in the field. This review addresses these gaps by
systematically examining state-of-the-art methods in data acquisition (sensor
and platform technologies), data processing (including annotation and
modelling), and mapping techniques (such as spatiotemporal analysis and
decision support tools). Following PRISMA guidelines, we critically evaluate
and synthesize key findings from the literature to provide a holistic
understanding of the weed mapping landscape. This review serves as a
foundational reference to guide future research and support the development of
efficient, scalable, and sustainable weed management systems.

</details>


### [8] [Frequency Domain-Based Diffusion Model for Unpaired Image Dehazing](https://arxiv.org/abs/2507.01275)
*Chengxu Liu,Lu Qi,Jinshan Pan,Xueming Qian,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: A novel frequency domain-based diffusion model (\ours) is proposed for unpaired image dehazing, leveraging amplitude spectrum reconstruction and phase correction to outperform existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing contrastive learning methods for unpaired image dehazing introduce irrelevant content and ignore haze-specific frequency domain properties, particularly the amplitude spectrum.

Method: The proposed model uses Diffusion Models (DMs) for frequency domain reconstruction, an Amplitude Residual Encoder (ARE) to bridge the amplitude gap, and a Phase Correction Module (PCM) to refine the phase spectrum.

Result: The model outperforms state-of-the-art methods on synthetic and real-world datasets.

Conclusion: The frequency domain approach, combined with DMs and specialized modules, effectively addresses unpaired image dehazing challenges.

Abstract: Unpaired image dehazing has attracted increasing attention due to its
flexible data requirements during model training. Dominant methods based on
contrastive learning not only introduce haze-unrelated content information, but
also ignore haze-specific properties in the frequency domain (\ie,~haze-related
degradation is mainly manifested in the amplitude spectrum). To address these
issues, we propose a novel frequency domain-based diffusion model, named \ours,
for fully exploiting the beneficial knowledge in unpaired clear data. In
particular, inspired by the strong generative ability shown by Diffusion Models
(DMs), we tackle the dehazing task from the perspective of frequency domain
reconstruction and perform the DMs to yield the amplitude spectrum consistent
with the distribution of clear images. To implement it, we propose an Amplitude
Residual Encoder (ARE) to extract the amplitude residuals, which effectively
compensates for the amplitude gap from the hazy to clear domains, as well as
provide supervision for the DMs training. In addition, we propose a Phase
Correction Module (PCM) to eliminate artifacts by further refining the phase
spectrum during dehazing with a simple attention mechanism. Experimental
results demonstrate that our \ours outperforms other state-of-the-art methods
on both synthetic and real-world datasets.

</details>


### [9] [Learning an Ensemble Token from Task-driven Priors in Facial Analysis](https://arxiv.org/abs/2507.01290)
*Sunyong Seo,Semin Kim,Jongha Lee*

Main category: cs.CV

TL;DR: ET-Fuser introduces a novel method for unified feature representation in facial analysis by leveraging pre-trained models and attention mechanisms.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of unified feature representation in single-task learning for facial analysis.

Method: Proposes ET-Fuser, which uses attention mechanisms and pre-trained task priors to generate an ensemble token for shared mutual information.

Result: Improves feature representations across various facial analysis tasks with minimal computational cost.

Conclusion: ET-Fuser effectively unifies feature representation, enhancing performance in facial analysis.

Abstract: Facial analysis exhibits task-specific feature variations. While
Convolutional Neural Networks (CNNs) have enabled the fine-grained
representation of spatial information, Vision Transformers (ViTs) have
facilitated the representation of semantic information at the patch level.
Although the generalization of conventional methodologies has advanced visual
interpretability, there remains paucity of research that preserves the unified
feature representation on single task learning during the training process. In
this work, we introduce ET-Fuser, a novel methodology for learning ensemble
token by leveraging attention mechanisms based on task priors derived from
pre-trained models for facial analysis. Specifically, we propose a robust prior
unification learning method that generates a ensemble token within a
self-attention mechanism, which shares the mutual information along the
pre-trained encoders. This ensemble token approach offers high efficiency with
negligible computational cost. Our results show improvements across a variety
of facial analysis, with statistically significant enhancements observed in the
feature representations.

</details>


### [10] [DiffusionLight-Turbo: Accelerated Light Probes for Free via Single-Pass Chrome Ball Inpainting](https://arxiv.org/abs/2507.01305)
*Worameth Chinchuthakun,Pakkapon Phongthawee,Amit Raj,Varun Jampani,Pramook Khungurn,Supasorn Suwajanakorn*

Main category: cs.CV

TL;DR: The paper introduces DiffusionLight and DiffusionLight-Turbo, techniques for estimating lighting from a single LDR image using a diffusion model, addressing challenges like inconsistent outputs and HDR generation.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on limited HDR datasets, leading to generalization failures. The paper aims to overcome this by leveraging diffusion models for more robust lighting estimation.

Method: The approach reframes the task as chrome ball inpainting, using iterative inpainting for stable priors and fine-tuning an Exposure LoRA for HDR generation. DiffusionLight-Turbo speeds up the process with a Turbo LoRA.

Result: The method produces high-quality light estimates across diverse settings, with DiffusionLight-Turbo achieving a 60x speedup (30 seconds vs. 30 minutes) with minimal quality loss.

Conclusion: The proposed techniques effectively generalize to in-the-wild scenarios, offering a practical solution for lighting estimation from LDR images.

Abstract: We introduce a simple yet effective technique for estimating lighting from a
single low-dynamic-range (LDR) image by reframing the task as a chrome ball
inpainting problem. This approach leverages a pre-trained diffusion model,
Stable Diffusion XL, to overcome the generalization failures of existing
methods that rely on limited HDR panorama datasets. While conceptually simple,
the task remains challenging because diffusion models often insert incorrect or
inconsistent content and cannot readily generate chrome balls in HDR format.
Our analysis reveals that the inpainting process is highly sensitive to the
initial noise in the diffusion process, occasionally resulting in unrealistic
outputs. To address this, we first introduce DiffusionLight, which uses
iterative inpainting to compute a median chrome ball from multiple outputs to
serve as a stable, low-frequency lighting prior that guides the generation of a
high-quality final result. To generate high-dynamic-range (HDR) light probes,
an Exposure LoRA is fine-tuned to create LDR images at multiple exposure
values, which are then merged. While effective, DiffusionLight is
time-intensive, requiring approximately 30 minutes per estimation. To reduce
this overhead, we introduce DiffusionLight-Turbo, which reduces the runtime to
about 30 seconds with minimal quality loss. This 60x speedup is achieved by
training a Turbo LoRA to directly predict the averaged chrome balls from the
iterative process. Inference is further streamlined into a single denoising
pass using a LoRA swapping technique. Experimental results that show our method
produces convincing light estimates across diverse settings and demonstrates
superior generalization to in-the-wild scenarios. Our code is available at
https://diffusionlight.github.io/turbo

</details>


### [11] [Physics-informed Ground Reaction Dynamics from Human Motion Capture](https://arxiv.org/abs/2507.01340)
*Cuong Le,Huy-Phuong Le,Duc Le,Minh-Thien Duong,Van-Binh Nguyen,My-Ha Le*

Main category: cs.CV

TL;DR: A novel method estimates human ground reaction forces from motion capture data using physics laws and computational simulation, outperforming baseline models in accuracy.


<details>
  <summary>Details</summary>
Motivation: Force plates, used for measuring body dynamics, are limited to lab setups, restricting the study of human motion dynamics.

Method: The method uses Euler's integration and PD algorithm to compute ground reaction forces from motion capture data, integrating physics laws for accuracy.

Result: The approach outperformed baselines in ground reaction force estimation accuracy and simulated root trajectory precision on the GroundLink dataset.

Conclusion: The proposed physics-based method enhances the estimation of human dynamics, offering a reliable alternative to force plates.

Abstract: Body dynamics are crucial information for the analysis of human motions in
important research fields, ranging from biomechanics, sports science to
computer vision and graphics. Modern approaches collect the body dynamics,
external reactive force specifically, via force plates, synchronizing with
human motion capture data, and learn to estimate the dynamics from a black-box
deep learning model. Being specialized devices, force plates can only be
installed in laboratory setups, imposing a significant limitation on the
learning of human dynamics. To this end, we propose a novel method for
estimating human ground reaction dynamics directly from the more reliable
motion capture data with physics laws and computational simulation as
constrains. We introduce a highly accurate and robust method for computing
ground reaction forces from motion capture data using Euler's integration
scheme and PD algorithm. The physics-based reactive forces are used to inform
the learning model about the physics-informed motion dynamics thus improving
the estimation accuracy. The proposed approach was tested on the GroundLink
dataset, outperforming the baseline model on: 1) the ground reaction force
estimation accuracy compared to the force plates measurement; and 2) our
simulated root trajectory precision. The implementation code is available at
https://github.com/cuongle1206/Phys-GRD

</details>


### [12] [Learning Camera-Agnostic White-Balance Preferences](https://arxiv.org/abs/2507.01342)
*Luxi Zhao,Mahmoud Afifi,Michael S. Brown*

Main category: cs.CV

TL;DR: The paper introduces a lightweight method to transform neutral white balance corrections into aesthetically preferred ones, ensuring consistency across different cameras with minimal overhead.


<details>
  <summary>Details</summary>
Motivation: Commercial auto white balance (AWB) systems prioritize aesthetics over accuracy, and existing learning-based methods struggle with generalization across camera sensors. This work aims to achieve aesthetic consistency without compromising compatibility with existing techniques.

Method: The authors propose a post-illuminant-estimation mapping that converts neutral corrections into preferred aesthetic corrections in a camera-agnostic space. The model is lightweight (~500 parameters) and efficient (0.024 ms runtime).

Result: Tested on 771 smartphone images from three cameras, the method achieves state-of-the-art performance with minimal computational and memory overhead.

Conclusion: The proposed solution successfully balances aesthetic consistency and efficiency, making it practical for cross-camera AWB applications.

Abstract: The image signal processor (ISP) pipeline in modern cameras consists of
several modules that transform raw sensor data into visually pleasing images in
a display color space. Among these, the auto white balance (AWB) module is
essential for compensating for scene illumination. However, commercial AWB
systems often strive to compute aesthetic white-balance preferences rather than
accurate neutral color correction. While learning-based methods have improved
AWB accuracy, they typically struggle to generalize across different camera
sensors -- an issue for smartphones with multiple cameras. Recent work has
explored cross-camera AWB, but most methods remain focused on achieving neutral
white balance. In contrast, this paper is the first to address aesthetic
consistency by learning a post-illuminant-estimation mapping that transforms
neutral illuminant corrections into aesthetically preferred corrections in a
camera-agnostic space. Once trained, our mapping can be applied after any
neutral AWB module to enable consistent and stylized color rendering across
unseen cameras. Our proposed model is lightweight -- containing only $\sim$500
parameters -- and runs in just 0.024 milliseconds on a typical flagship mobile
CPU. Evaluated on a dataset of 771 smartphone images from three different
cameras, our method achieves state-of-the-art performance while remaining fully
compatible with existing cross-camera AWB techniques, introducing minimal
computational and memory overhead.

</details>


### [13] [Learning from Random Subspace Exploration: Generalized Test-Time Augmentation with Self-supervised Distillation](https://arxiv.org/abs/2507.01347)
*Andrei Jelea,Ahmed Nabil Belbachir,Marius Leordeanu*

Main category: cs.CV

TL;DR: GTTA is a versatile method for enhancing model performance across various tasks by perturbing PCA subspace projections and using self-supervised learning to reduce computational costs without accuracy loss.


<details>
  <summary>Details</summary>
Motivation: To improve model performance universally across vision and non-vision tasks by addressing structural and systematic noise in data.

Method: GTTA applies random perturbations to PCA subspace projections of test inputs, forms robust ensembles, and uses self-supervised learning to refine the model.

Result: GTTA outperforms existing TTA methods and SoTA models on diverse tasks, including image classification, segmentation, and speech recognition.

Conclusion: GTTA is a general, effective solution for enhancing model performance across multiple domains, validated by extensive testing and a novel dataset (DeepSalmon).

Abstract: We introduce Generalized Test-Time Augmentation (GTTA), a highly effective
method for improving the performance of a trained model, which unlike other
existing Test-Time Augmentation approaches from the literature is general
enough to be used off-the-shelf for many vision and non-vision tasks, such as
classification, regression, image segmentation and object detection. By
applying a new general data transformation, that randomly perturbs multiple
times the PCA subspace projection of a test input, GTTA forms robust ensembles
at test time in which, due to sound statistical properties, the structural and
systematic noises in the initial input data is filtered out and final estimator
errors are reduced. Different from other existing methods, we also propose a
final self-supervised learning stage in which the ensemble output, acting as an
unsupervised teacher, is used to train the initial single student model, thus
reducing significantly the test time computational cost, at no loss in
accuracy. Our tests and comparisons to strong TTA approaches and SoTA models on
various vision and non-vision well-known datasets and tasks, such as image
classification and segmentation, speech recognition and house price prediction,
validate the generality of the proposed GTTA. Furthermore, we also prove its
effectiveness on the more specific real-world task of salmon segmentation and
detection in low-visibility underwater videos, for which we introduce
DeepSalmon, the largest dataset of its kind in the literature.

</details>


### [14] [Long-Tailed Distribution-Aware Router For Mixture-of-Experts in Large Vision-Language Model](https://arxiv.org/abs/2507.01351)
*Chaoxiang Cai,Longrong Yang,Kaibing Chen,Fan Yang,Xi Li*

Main category: cs.CV

TL;DR: The paper introduces LTDR, a Long-Tailed Distribution-aware Router for vision-language token-to-expert routing, addressing modality-specific distribution differences and enhancing expert activation for vision tail tokens.


<details>
  <summary>Details</summary>
Motivation: Existing MoE frameworks for LVLMs overlook distributional differences between vision and language tokens, leading to suboptimal routing.

Method: Proposes LTDR with a distribution-aware router for modality-specific routing and an oversampling-like strategy for vision tail tokens.

Result: Experiments on benchmarks confirm LTDR's effectiveness in improving routing for vision-language models.

Conclusion: LTDR addresses key challenges in MoE frameworks for LVLMs, offering tailored routing strategies for vision and language tokens.

Abstract: The mixture-of-experts (MoE), which replaces dense models with sparse
architectures, has gained attention in large vision-language models (LVLMs) for
achieving comparable performance with fewer activated parameters. Existing MoE
frameworks for LVLMs focus on token-to-expert routing (TER), encouraging
different experts to specialize in processing distinct tokens. However, these
frameworks often rely on the load balancing mechanism, overlooking the inherent
distributional differences between vision and language. To this end, we propose
a Long-Tailed Distribution-aware Router (LTDR) for vision-language TER,
tackling two challenges: (1) Distribution-aware router for modality-specific
routing. We observe that language TER follows a uniform distribution, whereas
vision TER exhibits a long-tailed distribution. This discrepancy necessitates
distinct routing strategies tailored to each modality. (2) Enhancing expert
activation for vision tail tokens. Recognizing the importance of vision tail
tokens, we introduce an oversampling-like strategy by increasing the number of
activated experts for these tokens. Experiments on extensive benchmarks
validate the effectiveness of our approach.

</details>


### [15] [3D Gaussian Splatting Driven Multi-View Robust Physical Adversarial Camouflage Generation](https://arxiv.org/abs/2507.01367)
*Tianrui Lou,Xiaojun Jia,Siyuan Liang,Jiawei Liang,Ming Zhang,Yanjun Xiao,Xiaochun Cao*

Main category: cs.CV

TL;DR: PGA introduces a 3D Gaussian Splatting-based framework for physical adversarial attacks, improving cross-view robustness and effectiveness by addressing occlusion and optimizing backgrounds.


<details>
  <summary>Details</summary>
Motivation: Current camouflage-based physical attacks rely on mesh priors and simulators, which are inefficient and lack real-world accuracy, leading to poor multi-view robustness.

Method: PGA uses 3D Gaussian Splatting for rapid reconstruction and employs min-max optimization to adjust backgrounds and avoid occlusion, enhancing adversarial features.

Result: Extensive experiments show PGA's superior effectiveness and robustness across diverse viewpoints and environments.

Conclusion: PGA offers a more efficient and robust solution for physical adversarial attacks, validated by experimental results.

Abstract: Physical adversarial attack methods expose the vulnerabilities of deep neural
networks and pose a significant threat to safety-critical scenarios such as
autonomous driving. Camouflage-based physical attack is a more promising
approach compared to the patch-based attack, offering stronger adversarial
effectiveness in complex physical environments. However, most prior work relies
on mesh priors of the target object and virtual environments constructed by
simulators, which are time-consuming to obtain and inevitably differ from the
real world. Moreover, due to the limitations of the backgrounds in training
images, previous methods often fail to produce multi-view robust adversarial
camouflage and tend to fall into sub-optimal solutions. Due to these reasons,
prior work lacks adversarial effectiveness and robustness across diverse
viewpoints and physical environments. We propose a physical attack framework
based on 3D Gaussian Splatting (3DGS), named PGA, which provides rapid and
precise reconstruction with few images, along with photo-realistic rendering
capabilities. Our framework further enhances cross-view robustness and
adversarial effectiveness by preventing mutual and self-occlusion among
Gaussians and employing a min-max optimization approach that adjusts the
imaging background of each viewpoint, helping the algorithm filter out
non-robust adversarial features. Extensive experiments validate the
effectiveness and superiority of PGA. Our code is available
at:https://github.com/TRLou/PGA.

</details>


### [16] [Activation Reward Models for Few-Shot Model Alignment](https://arxiv.org/abs/2507.01368)
*Tianning Chai,Chancharik Mitra,Brandon Huang,Gautam Rajendrakumar Gare,Zhiqiu Lin,Assaf Arbelle,Leonid Karlinsky,Rogerio Feris,Trevor Darrell,Deva Ramanan,Roei Herzig*

Main category: cs.CV

TL;DR: Activation Reward Models (Activation RMs) introduce a few-shot reward modeling method using activation steering, outperforming existing approaches and mitigating reward hacking.


<details>
  <summary>Details</summary>
Motivation: Aligning LLMs and LMMs to human preferences is challenging; traditional reward modeling lacks adaptability to new preferences.

Method: Activation RMs leverage activation steering to construct reward signals with minimal supervision, avoiding additional finetuning.

Result: Activation RMs outperform existing few-shot methods and mitigate reward hacking, achieving state-of-the-art performance on the PreferenceHack benchmark.

Conclusion: Activation RMs offer a scalable and effective solution for aligning models to human preferences, with potential for safety-critical applications.

Abstract: Aligning Large Language Models (LLMs) and Large Multimodal Models (LMMs) to
human preferences is a central challenge in improving the quality of the
models' generative outputs for real-world applications. A common approach is to
use reward modeling to encode preferences, enabling alignment via post-training
using reinforcement learning. However, traditional reward modeling is not
easily adaptable to new preferences because it requires a separate reward
model, commonly trained on large preference datasets. To address this, we
introduce Activation Reward Models (Activation RMs) -- a novel few-shot reward
modeling method that leverages activation steering to construct well-aligned
reward signals using minimal supervision and no additional model finetuning.
Activation RMs outperform existing few-shot reward modeling approaches such as
LLM-as-a-judge with in-context learning, voting-based scoring, and token
probability scoring on standard reward modeling benchmarks. Furthermore, we
demonstrate the effectiveness of Activation RMs in mitigating reward hacking
behaviors, highlighting their utility for safety-critical applications. Toward
this end, we propose PreferenceHack, a novel few-shot setting benchmark, the
first to test reward models on reward hacking in a paired preference format.
Finally, we show that Activation RM achieves state-of-the-art performance on
this benchmark, surpassing even GPT-4o.

</details>


### [17] [Active Measurement: Efficient Estimation at Scale](https://arxiv.org/abs/2507.01372)
*Max Hamilton,Jinlin Lai,Wenlong Zhao,Subhransu Maji,Daniel Sheldon*

Main category: cs.CV

TL;DR: Active measurement is a human-in-the-loop AI framework for scientific measurement, combining AI predictions with human labeling to improve accuracy and reduce effort.


<details>
  <summary>Details</summary>
Motivation: Current AI workflows lack accuracy and statistical guarantees for scientific discovery.

Method: Uses AI to predict measurements, samples for human labeling via importance sampling, and iteratively refines the AI model and estimates.

Result: Provides precise estimates with minimal human effort, outperforming alternatives in error reduction.

Conclusion: Active measurement enhances scientific measurement by balancing AI efficiency and human accuracy.

Abstract: AI has the potential to transform scientific discovery by analyzing vast
datasets with little human effort. However, current workflows often do not
provide the accuracy or statistical guarantees that are needed. We introduce
active measurement, a human-in-the-loop AI framework for scientific
measurement. An AI model is used to predict measurements for individual units,
which are then sampled for human labeling using importance sampling. With each
new set of human labels, the AI model is improved and an unbiased Monte Carlo
estimate of the total measurement is refined. Active measurement can provide
precise estimates even with an imperfect AI model, and requires little human
effort when the AI model is very accurate. We derive novel estimators,
weighting schemes, and confidence intervals, and show that active measurement
reduces estimation error compared to alternatives in several measurement tasks.

</details>


### [18] [MUG: Pseudo Labeling Augmented Audio-Visual Mamba Network for Audio-Visual Video Parsing](https://arxiv.org/abs/2507.01384)
*Langyu Wang,Bingke Zhu,Yingying Chen,Yiyuan Zhang,Ming Tang,Jinqiao Wang*

Main category: cs.CV

TL;DR: The paper proposes MUG, an audio-visual Mamba network with pseudo-label augmentation, to improve segment-level and event-level predictions in weakly-supervised AVVP tasks.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with improving both segment-level and event-level predictions due to weak supervision and model limitations.

Method: MUG uses pseudo-label augmentation and an audio-visual Mamba network to enhance segment uniqueness and reduce noise from alternate modalities.

Result: MUG achieves state-of-the-art results on the LLP dataset, with gains of 2.1% and 1.2% in visual and audio segment-level metrics.

Conclusion: MUG effectively addresses the limitations of weakly-supervised AVVP, improving performance through pseudo-label augmentation and the AV-Mamba network.

Abstract: The weakly-supervised audio-visual video parsing (AVVP) aims to predict all
modality-specific events and locate their temporal boundaries. Despite
significant progress, due to the limitations of the weakly-supervised and the
deficiencies of the model architecture, existing methods are lacking in
simultaneously improving both the segment-level prediction and the event-level
prediction. In this work, we propose a audio-visual Mamba network with pseudo
labeling aUGmentation (MUG) for emphasising the uniqueness of each segment and
excluding the noise interference from the alternate modalities. Specifically,
we annotate some of the pseudo-labels based on previous work. Using unimodal
pseudo-labels, we perform cross-modal random combinations to generate new data,
which can enhance the model's ability to parse various segment-level event
combinations. For feature processing and interaction, we employ a audio-visual
mamba network. The AV-Mamba enhances the ability to perceive different segments
and excludes additional modal noise while sharing similar modal information.
Our extensive experiments demonstrate that MUG improves state-of-the-art
results on LLP dataset in all metrics (e.g,, gains of 2.1% and 1.2% in terms of
visual Segment-level and audio Segment-level metrics). Our code is available at
https://github.com/WangLY136/MUG.

</details>


### [19] [FixTalk: Taming Identity Leakage for High-Quality Talking Head Generation in Extreme Cases](https://arxiv.org/abs/2507.01390)
*Shuai Tan,Bill Gong,Bin Ji,Ye Pan*

Main category: cs.CV

TL;DR: FixTalk is a novel framework addressing identity leakage and rendering artifacts in talking head generation by decoupling identity from motion features and using leaked identity to fix artifacts.


<details>
  <summary>Details</summary>
Motivation: Existing methods for talking head generation suffer from identity leakage and rendering artifacts, especially in extreme cases, prompting the need for a solution.

Method: FixTalk introduces Enhanced Motion Indicator (EMI) to decouple identity from motion features and Enhanced Detail Indicator (EDI) to use leaked identity for artifact correction.

Result: Extensive experiments show FixTalk effectively mitigates identity leakage and rendering artifacts, outperforming state-of-the-art methods.

Conclusion: FixTalk successfully resolves identity leakage and rendering artifacts, enhancing the quality of talking head generation.

Abstract: Talking head generation is gaining significant importance across various
domains, with a growing demand for high-quality rendering. However, existing
methods often suffer from identity leakage (IL) and rendering artifacts (RA),
particularly in extreme cases. Through an in-depth analysis of previous
approaches, we identify two key insights: (1) IL arises from identity
information embedded within motion features, and (2) this identity information
can be leveraged to address RA. Building on these findings, this paper
introduces FixTalk, a novel framework designed to simultaneously resolve both
issues for high-quality talking head generation. Firstly, we propose an
Enhanced Motion Indicator (EMI) to effectively decouple identity information
from motion features, mitigating the impact of IL on generated talking heads.
To address RA, we introduce an Enhanced Detail Indicator (EDI), which utilizes
the leaked identity information to supplement missing details, thus fixing the
artifacts. Extensive experiments demonstrate that FixTalk effectively mitigates
IL and RA, achieving superior performance compared to state-of-the-art methods.

</details>


### [20] [Coherent Online Road Topology Estimation and Reasoning with Standard-Definition Maps](https://arxiv.org/abs/2507.01397)
*Khanh Son Pham,Christian Witte,Jens Behley,Johannes Betz,Cyrill Stachniss*

Main category: cs.CV

TL;DR: The paper proposes a method for coherently predicting HD map elements (lane segments, topology, and road boundaries) using prior SD map information, improving over existing methods.


<details>
  <summary>Details</summary>
Motivation: Autonomous cars rely on HD maps, but constructing them online is complex. The paper aims to simplify this by leveraging prior SD maps.

Method: A network architecture uses hybrid lane segment encodings (prior info + denoising) and temporal consistency from past frames.

Result: The approach outperforms previous methods significantly.

Conclusion: The proposed method effectively models road topologies and enhances HD map prediction, benefiting autonomous driving.

Abstract: Most autonomous cars rely on the availability of high-definition (HD) maps.
Current research aims to address this constraint by directly predicting HD map
elements from onboard sensors and reasoning about the relationships between the
predicted map and traffic elements. Despite recent advancements, the coherent
online construction of HD maps remains a challenging endeavor, as it
necessitates modeling the high complexity of road topologies in a unified and
consistent manner. To address this challenge, we propose a coherent approach to
predict lane segments and their corresponding topology, as well as road
boundaries, all by leveraging prior map information represented by commonly
available standard-definition (SD) maps. We propose a network architecture,
which leverages hybrid lane segment encodings comprising prior information and
denoising techniques to enhance training stability and performance.
Furthermore, we facilitate past frames for temporal consistency. Our
experimental evaluation demonstrates that our approach outperforms previous
methods by a large margin, highlighting the benefits of our modeling scheme.

</details>


### [21] [Medical-Knowledge Driven Multiple Instance Learning for Classifying Severe Abdominal Anomalies on Prenatal Ultrasound](https://arxiv.org/abs/2507.01401)
*Huanwen Liang,Jingxian Xu,Yuanji Zhang,Yuhao Huang,Yuhan Zhang,Xin Yang,Ran Li,Xuedong Deng,Yanjun Liu,Guowei Tao,Yun Wu,Sheng Zhao,Xinru Gao,Dong Ni*

Main category: cs.CV

TL;DR: A case-level MIL-based method for classifying fetal abdominal anomalies in prenatal ultrasound, using MoAE, MFS, and PPL, outperforms state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Accurate diagnosis of fetal abdominal malformations is critical for pregnancy management, but AI applications in this area are limited, especially for case-level diagnosis.

Method: Proposes a MIL-based method with MoAE for attention weighting, MFS for medical-knowledge-aligned feature selection, and PPL for enhanced feature learning.

Result: Validated on 2,419 cases (24,748 images), the method outperforms existing approaches.

Conclusion: The method advances case-level diagnosis of fetal abdominal anomalies, offering improved accuracy without standard plane localization.

Abstract: Fetal abdominal malformations are serious congenital anomalies that require
accurate diagnosis to guide pregnancy management and reduce mortality. Although
AI has demonstrated significant potential in medical diagnosis, its application
to prenatal abdominal anomalies remains limited. Most existing studies focus on
image-level classification and rely on standard plane localization, placing
less emphasis on case-level diagnosis. In this paper, we develop a case-level
multiple instance learning (MIL)-based method, free of standard plane
localization, for classifying fetal abdominal anomalies in prenatal ultrasound.
Our contribution is three-fold. First, we adopt a mixture-of-attention-experts
module (MoAE) to weight different attention heads for various planes. Secondly,
we propose a medical-knowledge-driven feature selection module (MFS) to align
image features with medical knowledge, performing self-supervised image token
selection at the case-level. Finally, we propose a prompt-based prototype
learning (PPL) to enhance the MFS. Extensively validated on a large prenatal
abdominal ultrasound dataset containing 2,419 cases, with a total of 24,748
images and 6 categories, our proposed method outperforms the state-of-the-art
competitors. Codes are available at:https://github.com/LL-AC/AAcls.

</details>


### [22] [CaptionSmiths: Flexibly Controlling Language Pattern in Image Captioning](https://arxiv.org/abs/2507.01409)
*Kuniaki Saito,Donghyun Kim,Kwanyong Park,Atsushi Hashimoto,Yoshitaka Ushiku*

Main category: cs.CV

TL;DR: CaptionSmiths enables fine-grained control over caption properties (length, descriptiveness, uniqueness) in image captioning models without human annotation, outperforming baselines in smooth transitions and lexical alignment.


<details>
  <summary>Details</summary>
Motivation: Existing captioning models lack fine-grained control over caption properties and smooth transitions between language patterns, limiting their adaptability for diverse applications.

Method: CaptionSmiths quantifies caption properties as continuous scalar values and conditions the model via interpolation between endpoint vectors representing extreme states (e.g., short vs. long captions).

Result: The model smoothly adjusts caption properties and achieves higher lexical alignment, reducing length control error by 506% compared to baselines.

Conclusion: CaptionSmiths provides a flexible and effective solution for controlling caption properties in image captioning models, enhancing their utility for diverse applications.

Abstract: An image captioning model flexibly switching its language pattern, e.g.,
descriptiveness and length, should be useful since it can be applied to diverse
applications. However, despite the dramatic improvement in generative
vision-language models, fine-grained control over the properties of generated
captions is not easy due to two reasons: (i) existing models are not given the
properties as a condition during training and (ii) existing models cannot
smoothly transition its language pattern from one state to the other. Given
this challenge, we propose a new approach, CaptionSmiths, to acquire a single
captioning model that can handle diverse language patterns. First, our approach
quantifies three properties of each caption, length, descriptiveness, and
uniqueness of a word, as continuous scalar values, without human annotation.
Given the values, we represent the conditioning via interpolation between two
endpoint vectors corresponding to the extreme states, e.g., one for a very
short caption and one for a very long caption. Empirical results demonstrate
that the resulting model can smoothly change the properties of the output
captions and show higher lexical alignment than baselines. For instance,
CaptionSmiths reduces the error in controlling caption length by 506\% despite
better lexical alignment. Code will be available on
https://github.com/omron-sinicx/captionsmiths.

</details>


### [23] [Gradient Short-Circuit: Efficient Out-of-Distribution Detection via Feature Intervention](https://arxiv.org/abs/2507.01417)
*Jiawei Gu,Ziyue Qiao,Zechao Li*

Main category: cs.CV

TL;DR: The paper proposes a lightweight inference-stage technique for OOD detection by analyzing gradient consistency around ID and OOD samples, improving robustness without major pipeline changes.


<details>
  <summary>Details</summary>
Motivation: OOD detection is crucial for safe deployment of deep models in open-world environments, where inputs may deviate from the training distribution.

Method: The method leverages gradient direction consistency around ID samples and introduces a gradient short-circuit technique to suppress spurious gradients for OOD samples, along with a local first-order approximation to avoid recomputing logits.

Result: Experiments on standard benchmarks show significant improvements in OOD detection performance.

Conclusion: The proposed method is practical, lightweight, and enhances OOD detection robustness with minimal changes to the standard inference pipeline.

Abstract: Out-of-Distribution (OOD) detection is critical for safely deploying deep
models in open-world environments, where inputs may lie outside the training
distribution. During inference on a model trained exclusively with
In-Distribution (ID) data, we observe a salient gradient phenomenon: around an
ID sample, the local gradient directions for "enhancing" that sample's
predicted class remain relatively consistent, whereas OOD samples--unseen in
training--exhibit disorganized or conflicting gradient directions in the same
neighborhood. Motivated by this observation, we propose an inference-stage
technique to short-circuit those feature coordinates that spurious gradients
exploit to inflate OOD confidence, while leaving ID classification largely
intact. To circumvent the expense of recomputing the logits after this gradient
short-circuit, we further introduce a local first-order approximation that
accurately captures the post-modification outputs without a second forward
pass. Experiments on standard OOD benchmarks show our approach yields
substantial improvements. Moreover, the method is lightweight and requires
minimal changes to the standard inference pipeline, offering a practical path
toward robust OOD detection in real-world applications.

</details>


### [24] [DocShaDiffusion: Diffusion Model in Latent Space for Document Image Shadow Removal](https://arxiv.org/abs/2507.01422)
*Wenjie Liu,Bingshu Wang,Ze Wang,C. L. Philip Chen*

Main category: cs.CV

TL;DR: The paper introduces DocShaDiffusion, a latent space diffusion model for document shadow removal, addressing color shadows with SSGM and SMGDM, and validates its superiority on public datasets.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to handle color shadows in document images, limiting their effectiveness.

Method: Proposes DocShaDiffusion (latent space diffusion model), SSGM (shadow soft-mask generation), and SMGDM (shadow mask-aware guided diffusion). Introduces a shadow-robust perceptual feature loss and a synthetic dataset (SDCSRD).

Result: Superior performance on three public datasets compared to state-of-the-art methods.

Conclusion: The proposed method effectively removes color shadows and preserves document details, supported by a new synthetic dataset.

Abstract: Document shadow removal is a crucial task in the field of document image
enhancement. However, existing methods tend to remove shadows with constant
color background and ignore color shadows. In this paper, we first design a
diffusion model in latent space for document image shadow removal, called
DocShaDiffusion. It translates shadow images from pixel space to latent space,
enabling the model to more easily capture essential features. To address the
issue of color shadows, we design a shadow soft-mask generation module (SSGM).
It is able to produce accurate shadow mask and add noise into shadow regions
specially. Guided by the shadow mask, a shadow mask-aware guided diffusion
module (SMGDM) is proposed to remove shadows from document images by
supervising the diffusion and denoising process. We also propose a
shadow-robust perceptual feature loss to preserve details and structures in
document images. Moreover, we develop a large-scale synthetic document color
shadow removal dataset (SDCSRD). It simulates the distribution of realistic
color shadows and provides powerful supports for the training of models.
Experiments on three public datasets validate the proposed method's superiority
over state-of-the-art. Our code and dataset will be publicly available.

</details>


### [25] [DiffMark: Diffusion-based Robust Watermark Against Deepfakes](https://arxiv.org/abs/2507.01428)
*Chen Sun,Haiyang Sun,Zhiqing Guo,Yunfeng Diao,Liejun Wang,Dan Ma,Gaobo Yang,Keqin Li*

Main category: cs.CV

TL;DR: DiffMark is a robust watermarking framework using diffusion models to embed watermarks in facial images, enhancing resistance against Deepfake manipulations.


<details>
  <summary>Details</summary>
Motivation: Deepfakes threaten security and privacy; existing watermarking methods lack robustness against such manipulations.

Method: DiffMark modifies diffusion model training/sampling, uses facial and watermark conditions, introduces a CIF module, and integrates a frozen autoencoder for robustness.

Result: DiffMark effectively resists Deepfake manipulations, as shown in experiments.

Conclusion: DiffMark offers a novel, robust solution for watermarking against Deepfakes, with promising experimental results.

Abstract: Deepfakes pose significant security and privacy threats through malicious
facial manipulations. While robust watermarking can aid in authenticity
verification and source tracking, existing methods often lack the sufficient
robustness against Deepfake manipulations. Diffusion models have demonstrated
remarkable performance in image generation, enabling the seamless fusion of
watermark with image during generation. In this study, we propose a novel
robust watermarking framework based on diffusion model, called DiffMark. By
modifying the training and sampling scheme, we take the facial image and
watermark as conditions to guide the diffusion model to progressively denoise
and generate corresponding watermarked image. In the construction of facial
condition, we weight the facial image by a timestep-dependent factor that
gradually reduces the guidance intensity with the decrease of noise, thus
better adapting to the sampling process of diffusion model. To achieve the
fusion of watermark condition, we introduce a cross information fusion (CIF)
module that leverages a learnable embedding table to adaptively extract
watermark features and integrates them with image features via cross-attention.
To enhance the robustness of the watermark against Deepfake manipulations, we
integrate a frozen autoencoder during training phase to simulate Deepfake
manipulations. Additionally, we introduce Deepfake-resistant guidance that
employs specific Deepfake model to adversarially guide the diffusion sampling
process to generate more robust watermarked images. Experimental results
demonstrate the effectiveness of the proposed DiffMark on typical Deepfakes.
Our code will be available at https://github.com/vpsg-research/DiffMark.

</details>


### [26] [TurboReg: TurboClique for Robust and Efficient Point Cloud Registration](https://arxiv.org/abs/2507.01439)
*Shaocheng Yan,Pengcheng Shi,Zhenjun Zhao,Kaixin Wang,Kuang Cao,Ji Wu,Jiayuan Li*

Main category: cs.CV

TL;DR: TurboReg introduces a fast and robust estimator for Point Cloud Registration using TurboClique and Pivot-Guided Search, achieving high recall and speed.


<details>
  <summary>Details</summary>
Motivation: Existing methods for robust estimation in PCR suffer from exponential time complexity, limiting their use in time-sensitive applications.

Method: Proposes TurboClique (a 3-clique in a constrained compatibility graph) and Pivot-Guided Search (PGS) for efficient parallel searching and linear time complexity.

Result: TurboReg achieves state-of-the-art performance with significant speed improvements, e.g., 208.22× faster than 3DMAC on 3DMatch+FCGF.

Conclusion: TurboReg is a highly efficient and robust solution for PCR, outperforming existing methods in both speed and accuracy.

Abstract: Robust estimation is essential in correspondence-based Point Cloud
Registration (PCR). Existing methods using maximal clique search in
compatibility graphs achieve high recall but suffer from exponential time
complexity, limiting their use in time-sensitive applications. To address this
challenge, we propose a fast and robust estimator, TurboReg, built upon a novel
lightweight clique, TurboClique, and a highly parallelizable Pivot-Guided
Search (PGS) algorithm. First, we define the TurboClique as a 3-clique within a
highly-constrained compatibility graph. The lightweight nature of the 3-clique
allows for efficient parallel searching, and the highly-constrained
compatibility graph ensures robust spatial consistency for stable
transformation estimation. Next, PGS selects matching pairs with high SC$^2$
scores as pivots, effectively guiding the search toward TurboCliques with
higher inlier ratios. Moreover, the PGS algorithm has linear time complexity
and is significantly more efficient than the maximal clique search with
exponential time complexity. Extensive experiments show that TurboReg achieves
state-of-the-art performance across multiple real-world datasets, with
substantial speed improvements. For example, on the 3DMatch+FCGF dataset,
TurboReg (1K) operates $208.22\times$ faster than 3DMAC while also achieving
higher recall. Our code is accessible at
\href{https://github.com/Laka-3DV/TurboReg}{\texttt{TurboReg}}.

</details>


### [27] [OoDDINO:A Multi-level Framework for Anomaly Segmentation on Complex Road Scenes](https://arxiv.org/abs/2507.01455)
*Yuxing Liu,Ji Zhang,Zhou Xuchuan,Jingzhong Xiao,Huimin Yang,Jiaxin Zhong*

Main category: cs.CV

TL;DR: OoDDINO introduces a multi-level anomaly segmentation framework combining uncertainty-guided detection and adaptive thresholding to improve anomaly segmentation accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing methods neglect spatial correlations and struggle with global thresholds, leading to fragmented or inaccurate segmentation.

Method: OoDDINO uses a two-stage approach: OUAFS for uncertainty-aware anomaly localization and ADT-Net for adaptive thresholding.

Result: The framework outperforms state-of-the-art methods on benchmark datasets.

Conclusion: OoDDINO effectively addresses limitations of pixel-wise methods and enhances segmentation performance.

Abstract: Anomaly segmentation aims to identify Out-of-Distribution (OoD) anomalous
objects within images. Existing pixel-wise methods typically assign anomaly
scores individually and employ a global thresholding strategy to segment
anomalies. Despite their effectiveness, these approaches encounter significant
challenges in real-world applications: (1) neglecting spatial correlations
among pixels within the same object, resulting in fragmented segmentation; (2)
variabil ity in anomaly score distributions across image regions, causing
global thresholds to either generate false positives in background areas or
miss segments of anomalous objects. In this work, we introduce OoDDINO, a novel
multi-level anomaly segmentation framework designed to address these
limitations through a coarse-to-fine anomaly detection strategy. OoDDINO
combines an uncertainty-guided anomaly detection model with a pixel-level
segmentation model within a two-stage cascade architecture. Initially, we
propose an Orthogonal Uncertainty-Aware Fusion Strategy (OUAFS) that
sequentially integrates multiple uncertainty metrics with visual
representations, employing orthogonal constraints to strengthen the detection
model's capacity for localizing anomalous regions accurately. Subsequently, we
develop an Adaptive Dual-Threshold Network (ADT-Net), which dynamically
generates region-specific thresholds based on object-level detection outputs
and pixel-wise anomaly scores. This approach allows for distinct thresholding
strategies within foreground and background areas, achieving fine-grained
anomaly segmentation. The proposed framework is compatible with other
pixel-wise anomaly detection models, which acts as a plug-in to boost the
performance. Extensive experiments on two benchmark datasets validate our
framework's superiority and compatibility over state-of-the-art methods.

</details>


### [28] [NOCTIS: Novel Object Cyclic Threshold based Instance Segmentation](https://arxiv.org/abs/2507.01463)
*Max Gandyra,Alessandro Santonicola,Michael Beetz*

Main category: cs.CV

TL;DR: NOCTIS is a novel framework for instance segmentation of unseen objects, leveraging Grounded-SAM 2 and DINOv2, outperforming existing methods without retraining.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of segmenting novel objects in RGB images without retraining, improving upon prior works like CNOS, SAM-6D, and NIDS-Net.

Method: Uses Grounded-SAM 2 for object proposals and DINOv2 for embeddings. Matches objects via similarity scores, incorporating cyclic patch filtering and confidence weighting.

Result: Outperforms leading RGB and RGB-D methods on the BOP 2023 challenge datasets for unseen object segmentation.

Conclusion: NOCTIS demonstrates superior performance for segmenting novel objects without additional training, advancing the field of instance segmentation.

Abstract: Instance segmentation of novel objects instances in RGB images, given some
example images for each object, is a well known problem in computer vision.
Designing a model general enough to be employed, for all kinds of novel
objects, without (re-) training, has proven to be a difficult task. To handle
this, we propose a simple, yet powerful, framework, called: Novel Object Cyclic
Threshold based Instance Segmentation (NOCTIS). This work stems from and
improves upon previous ones like CNOS, SAM-6D and NIDS-Net; thus, it also
leverages on recent vision foundation models, namely: Grounded-SAM 2 and
DINOv2. It utilises Grounded-SAM 2 to obtain object proposals with precise
bounding boxes and their corresponding segmentation masks; while DINOv2's
zero-shot capabilities are employed to generate the image embeddings. The
quality of those masks, together with their embeddings, is of vital importance
to our approach; as the proposal-object matching is realized by determining an
object matching score based on the similarity of the class embeddings and the
average maximum similarity of the patch embeddings. Differently to SAM-6D,
calculating the latter involves a prior patch filtering based on the distance
between each patch and its corresponding cyclic/roundtrip patch in the image
grid. Furthermore, the average confidence of the proposals' bounding box and
mask is used as an additional weighting factor for the object matching score.
We empirically show that NOCTIS, without further training/fine tuning,
outperforms the best RGB and RGB-D methods on the seven core datasets of the
BOP 2023 challenge for the "Model-based 2D segmentation of unseen objects"
task.

</details>


### [29] [Representation Entanglement for Generation:Training Diffusion Transformers Is Much Easier Than You Think](https://arxiv.org/abs/2507.01467)
*Ge Wu,Shen Zhang,Ruijing Shi,Shanghua Gao,Zhenyuan Chen,Lei Wang,Zhaowei Chen,Hongcheng Gao,Yao Tang,Jian Yang,Ming-Ming Cheng,Xiang Li*

Main category: cs.CV

TL;DR: REG improves diffusion models by entangling low-level image latents with a high-level class token, enhancing generation quality and training efficiency with minimal overhead.


<details>
  <summary>Details</summary>
Motivation: Existing methods like REPA fail to fully utilize discriminative representations during denoising inference, limiting their potential.

Method: REG integrates a single high-level class token from pretrained models into denoising, enabling coherent image-class pair generation from noise.

Result: REG achieves 63x faster training than baseline models and outperforms REPA with 10x fewer iterations.

Conclusion: REG significantly enhances diffusion model performance and efficiency, demonstrating its superiority over existing methods.

Abstract: REPA and its variants effectively mitigate training challenges in diffusion
models by incorporating external visual representations from pretrained models,
through alignment between the noisy hidden projections of denoising networks
and foundational clean image representations. We argue that the external
alignment, which is absent during the entire denoising inference process, falls
short of fully harnessing the potential of discriminative representations. In
this work, we propose a straightforward method called Representation
Entanglement for Generation (REG), which entangles low-level image latents with
a single high-level class token from pretrained foundation models for
denoising. REG acquires the capability to produce coherent image-class pairs
directly from pure noise, substantially improving both generation quality and
training efficiency. This is accomplished with negligible additional inference
overhead, requiring only one single additional token for denoising (<0.5\%
increase in FLOPs and latency). The inference process concurrently reconstructs
both image latents and their corresponding global semantics, where the acquired
semantic knowledge actively guides and enhances the image generation process.
On ImageNet 256$\times$256, SiT-XL/2 + REG demonstrates remarkable convergence
acceleration, achieving $\textbf{63}\times$ and $\textbf{23}\times$ faster
training than SiT-XL/2 and SiT-XL/2 + REPA, respectively. More impressively,
SiT-L/2 + REG trained for merely 400K iterations outperforms SiT-XL/2 + REPA
trained for 4M iterations ($\textbf{10}\times$ longer). Code is available at:
https://github.com/Martinser/REG.

</details>


### [30] [Optimizing Methane Detection On Board Satellites: Speed, Accuracy, and Low-Power Solutions for Resource-Constrained Hardware](https://arxiv.org/abs/2507.01472)
*Jonáš Herec,Vít Růžička,Rado Pitoňák*

Main category: cs.CV

TL;DR: The paper accelerates methane leak detection using efficient, low-power algorithms for onboard satellite deployment, proposing faster variants of existing methods and integrating machine learning for improved performance.


<details>
  <summary>Details</summary>
Motivation: Early detection of methane leaks via hyperspectral satellite imagery is crucial for climate change mitigation, but existing methods are computationally demanding for onboard hardware.

Method: The work tests fast target detection methods (ACE, CEM) and proposes Mag1c-SAS, a faster variant of Mag1c, integrating them with U-Net and LinkNet for evaluation. Band selection strategies are also explored.

Result: Mag1c-SAS and CEM show promise, with up to ~100x and ~230x faster computation than original Mag1c, and a band selection strategy outperforms traditional methods using fewer channels.

Conclusion: The research advances onboard methane detection with minimal hardware requirements, improving timely data delivery, and open-sources code, data, and models.

Abstract: Methane is a potent greenhouse gas, and detecting its leaks early via
hyperspectral satellite imagery can help mitigate climate change. Meanwhile,
many existing missions operate in manual tasking regimes only, thus missing
potential events of interest. To overcome slow downlink rates cost-effectively,
onboard detection is a viable solution. However, traditional methane
enhancement methods are too computationally demanding for resource-limited
onboard hardware. This work accelerates methane detection by focusing on
efficient, low-power algorithms. We test fast target detection methods (ACE,
CEM) that have not been previously used for methane detection and propose a
Mag1c-SAS - a significantly faster variant of the current state-of-the-art
algorithm for methane detection: Mag1c. To explore their true detection
potential, we integrate them with a machine learning model (U-Net, LinkNet).
Our results identify two promising candidates (Mag1c-SAS and CEM), both
acceptably accurate for the detection of strong plumes and computationally
efficient enough for onboard deployment: one optimized more for accuracy, the
other more for speed, achieving up to ~100x and ~230x faster computation than
original Mag1c on resource-limited hardware. Additionally, we propose and
evaluate three band selection strategies. One of them can outperform the method
traditionally used in the field while using fewer channels, leading to even
faster processing without compromising accuracy. This research lays the
foundation for future advancements in onboard methane detection with minimal
hardware requirements, improving timely data delivery. The produced code, data,
and models are open-sourced and can be accessed from
https://github.com/zaitra/methane-filters-benchmark.

</details>


### [31] [Active Control Points-based 6DoF Pose Tracking for Industrial Metal Objects](https://arxiv.org/abs/2507.01478)
*Chentao Shen,Ding Pan,Mingyu Mei,Zaixing He,Xinyue Zhao*

Main category: cs.CV

TL;DR: A novel 6DoF pose tracking method for industrial metal objects using active control points to address reflection challenges.


<details>
  <summary>Details</summary>
Motivation: Pose tracking for metal objects is difficult due to reflections; existing methods struggle in real-world environments.

Method: Uses active control points to generate edge features for optimization, avoiding 6DoF pose-based rendering, and includes optimal control point regression for robustness.

Result: Effective in dataset evaluation and real-world tasks, enabling real-time tracking of metal objects.

Conclusion: The method provides a viable solution for industrial metal object tracking, with publicly available source code.

Abstract: Visual pose tracking is playing an increasingly vital role in industrial
contexts in recent years. However, the pose tracking for industrial metal
objects remains a challenging task especially in the real world-environments,
due to the reflection characteristic of metal objects. To address this issue,
we propose a novel 6DoF pose tracking method based on active control points.
The method uses image control points to generate edge feature for optimization
actively instead of 6DoF pose-based rendering, and serve them as optimization
variables. We also introduce an optimal control point regression method to
improve robustness. The proposed tracking method performs effectively in both
dataset evaluation and real world tasks, providing a viable solution for
real-time tracking of industrial metal objects. Our source code is made
publicly available at: https://github.com/tomatoma00/ACPTracking.

</details>


### [32] [What Really Matters for Robust Multi-Sensor HD Map Construction?](https://arxiv.org/abs/2507.01484)
*Xiaoshuai Hao,Yuting Zhao,Yuheng Ji,Luanyuan Dai,Peng Hao,Dingzhe Li,Shuai Cheng,Rong Yin*

Main category: cs.CV

TL;DR: The paper proposes methods to enhance robustness in multi-modal fusion for HD map construction, focusing on data augmentation, a novel fusion module, and modality dropout training, achieving state-of-the-art results on NuScenes data.


<details>
  <summary>Details</summary>
Motivation: Existing methods prioritize accuracy over robustness, which is critical for real-world autonomous driving applications.

Method: Three key components: data augmentation, a novel multi-modal fusion module, and modality dropout training.

Result: Significantly improved robustness and state-of-the-art performance on the NuScenes dataset.

Conclusion: The findings advance robust and reliable HD map construction for real-world autonomous driving.

Abstract: High-definition (HD) map construction methods are crucial for providing
precise and comprehensive static environmental information, which is essential
for autonomous driving systems. While Camera-LiDAR fusion techniques have shown
promising results by integrating data from both modalities, existing approaches
primarily focus on improving model accuracy and often neglect the robustness of
perception models, which is a critical aspect for real-world applications. In
this paper, we explore strategies to enhance the robustness of multi-modal
fusion methods for HD map construction while maintaining high accuracy. We
propose three key components: data augmentation, a novel multi-modal fusion
module, and a modality dropout training strategy. These components are
evaluated on a challenging dataset containing 10 days of NuScenes data. Our
experimental results demonstrate that our proposed methods significantly
enhance the robustness of baseline methods. Furthermore, our approach achieves
state-of-the-art performance on the clean validation set of the NuScenes
dataset. Our findings provide valuable insights for developing more robust and
reliable HD map construction models, advancing their applicability in
real-world autonomous driving scenarios. Project website:
https://robomap-123.github.io.

</details>


### [33] [AVC-DPO: Aligned Video Captioning via Direct Preference Optimization](https://arxiv.org/abs/2507.01492)
*Jiyang Tang,Hengyi Li,Yifan Du,Wayne Xin Zhao*

Main category: cs.CV

TL;DR: AVC-DPO enhances video MLLMs for human-preferred captioning by aligning preferences via direct optimization, achieving top performance in the VDC benchmark.


<details>
  <summary>Details</summary>
Motivation: Adjusting video captions to human preferences is challenging despite progress in video MLLMs.

Method: Proposes AVC-DPO, a post-training framework using enhanced prompts for temporal and spatial alignment, leveraging varied prompts for preference-aware training.

Result: Achieved first place in the LOVE@CVPR'25 Workshop Track 1A using the VDCSCORE metric.

Conclusion: AVC-DPO effectively aligns video captions with human preferences, demonstrating superior performance in detailed captioning.

Abstract: Although video multimodal large language models (video MLLMs) have achieved
substantial progress in video captioning tasks, it remains challenging to
adjust the focal emphasis of video captions according to human preferences. To
address this limitation, we propose Aligned Video Captioning via Direct
Preference Optimization (AVC-DPO), a post-training framework designed to
enhance captioning capabilities in video MLLMs through preference alignment.
Our approach designs enhanced prompts that specifically target temporal
dynamics and spatial information-two key factors that humans care about when
watching a video-thereby incorporating human-centric preferences. AVC-DPO
leverages the same foundation model's caption generation responses under varied
prompt conditions to conduct preference-aware training and caption alignment.
Using this framework, we have achieved exceptional performance in the
LOVE@CVPR'25 Workshop Track 1A: Video Detailed Captioning Challenge, achieving
first place on the Video Detailed Captioning (VDC) benchmark according to the
VDCSCORE evaluation metric.

</details>


### [34] [Crop Pest Classification Using Deep Learning Techniques: A Review](https://arxiv.org/abs/2507.01494)
*Muhammad Hassam Ejaz,Muhammad Bilal,Usman Habib*

Main category: cs.CV

TL;DR: A review of 37 studies (2018-2025) on AI-based pest classification, highlighting the shift from CNNs to hybrid/transformer models, key challenges, and future directions.


<details>
  <summary>Details</summary>
Motivation: Traditional pest monitoring is slow and manual; deep learning offers scalable, automated solutions.

Method: Analyzed 37 studies by crop type, pest species, model architecture, dataset usage, and technical challenges.

Result: Shift from CNNs to hybrid/transformer models improves accuracy and contextual understanding. Challenges include imbalanced datasets, small pest detection, and deployment issues.

Conclusion: AI-based pest monitoring shows promise but faces hurdles; the review provides a structured overview and future directions.

Abstract: Insect pests continue to bring a serious threat to crop yields around the
world, and traditional methods for monitoring them are often slow, manual, and
difficult to scale. In recent years, deep learning has emerged as a powerful
solution, with techniques like convolutional neural networks (CNNs), vision
transformers (ViTs), and hybrid models gaining popularity for automating pest
detection. This review looks at 37 carefully selected studies published between
2018 and 2025, all focused on AI-based pest classification. The selected
research is organized by crop type, pest species, model architecture, dataset
usage, and key technical challenges. The early studies relied heavily on CNNs
but latest work is shifting toward hybrid and transformer-based models that
deliver higher accuracy and better contextual understanding. Still, challenges
like imbalanced datasets, difficulty in detecting small pests, limited
generalizability, and deployment on edge devices remain significant hurdles.
Overall, this review offers a structured overview of the field, highlights
useful datasets, and outlines the key challenges and future directions for
AI-based pest monitoring systems.

</details>


### [35] [ReFlex: Text-Guided Editing of Real Images in Rectified Flow via Mid-Step Feature Extraction and Attention Adaptation](https://arxiv.org/abs/2507.01496)
*Jimyeong Kim,Jungwon Park,Yeji Song,Nojun Kwak,Wonjong Rhee*

Main category: cs.CV

TL;DR: A new training-free method for real-image editing with ReFlow models improves editability and text alignment by analyzing intermediate representations and leveraging mid-step latent.


<details>
  <summary>Details</summary>
Motivation: Adapting ReFlow for real-image editing is challenging; this work aims to address this gap.

Method: Analyzes intermediate representations of multimodal transformer blocks, uses mid-step latent for structural preservation, and adapts attention during injection.

Result: Outperforms nine baselines on two benchmarks, with human evaluations showing strong user preference.

Conclusion: The proposed method is effective for real-image editing with ReFlow, requiring no training or masks.

Abstract: Rectified Flow text-to-image models surpass diffusion models in image quality
and text alignment, but adapting ReFlow for real-image editing remains
challenging. We propose a new real-image editing method for ReFlow by analyzing
the intermediate representations of multimodal transformer blocks and
identifying three key features. To extract these features from real images with
sufficient structural preservation, we leverage mid-step latent, which is
inverted only up to the mid-step. We then adapt attention during injection to
improve editability and enhance alignment to the target text. Our method is
training-free, requires no user-provided mask, and can be applied even without
a source prompt. Extensive experiments on two benchmarks with nine baselines
demonstrate its superior performance over prior methods, further validated by
human evaluations confirming a strong user preference for our approach.

</details>


### [36] [Integrating Traditional and Deep Learning Methods to Detect Tree Crowns in Satellite Images](https://arxiv.org/abs/2507.01502)
*Ozan Durgut,Beril Kallfelz-Sirmacek,Cem Unsalan*

Main category: cs.CV

TL;DR: The paper proposes a rule-based approach combining traditional and deep learning methods to improve tree crown detection accuracy and robustness in forest monitoring.


<details>
  <summary>Details</summary>
Motivation: Addressing global environmental issues like deforestation and biodiversity loss requires better forest monitoring, which can be enhanced through automated remote sensing and computer vision techniques.

Method: The study introduces two tree crown detection methods (traditional and deep learning), then integrates them into a novel rule-based approach for improved results. Traditional methods handle feature extraction and segmentation, while deep learning detects tree crowns. Post-processing refines results by leveraging neighboring trees and localized operations.

Result: The proposed method increases the number of detected tree crowns compared to individual approaches, with detailed comparisons highlighting its advantages and limitations.

Conclusion: The rule-based integration of traditional and deep learning methods enhances tree crown detection, offering a promising solution for automated forest monitoring, though further improvements are needed.

Abstract: Global warming, loss of biodiversity, and air pollution are among the most
significant problems facing Earth. One of the primary challenges in addressing
these issues is the lack of monitoring forests to protect them. To tackle this
problem, it is important to leverage remote sensing and computer vision methods
to automate monitoring applications. Hence, automatic tree crown detection
algorithms emerged based on traditional and deep learning methods. In this
study, we first introduce two different tree crown detection methods based on
these approaches. Then, we form a novel rule-based approach that integrates
these two methods to enhance robustness and accuracy of tree crown detection
results. While traditional methods are employed for feature extraction and
segmentation of forested areas, deep learning methods are used to detect tree
crowns in our method. With the proposed rule-based approach, we post-process
these results, aiming to increase the number of detected tree crowns through
neighboring trees and localized operations. We compare the obtained results
with the proposed method in terms of the number of detected tree crowns and
report the advantages, disadvantages, and areas for improvement of the obtained
outcomes.

</details>


### [37] [Following the Clues: Experiments on Person Re-ID using Cross-Modal Intelligence](https://arxiv.org/abs/2507.01504)
*Robert Aufschläger,Youssef Shoeb,Azarm Nowzad,Michael Heigl,Fabian Bally,Martin Schramm*

Main category: cs.CV

TL;DR: cRID is a cross-modal framework using vision-language models and graph networks to detect PII in street-level data, improving privacy and Re-ID performance.


<details>
  <summary>Details</summary>
Motivation: Street-level datasets for autonomous driving and AI research contain PII, posing privacy risks for pedestrians. Current methods often miss non-biometric PII.

Method: Combines Large Vision-Language Models, Graph Attention Networks, and representation learning to detect textual describable PII and enhance Re-ID.

Result: Improved performance in cross-dataset Re-ID (Market-1501 to CUHK03-np) and better PII detection.

Conclusion: cRID effectively addresses privacy risks by detecting semantic PII, enhancing Re-ID, and is practical for real-world applications.

Abstract: The collection and release of street-level recordings as Open Data play a
vital role in advancing autonomous driving systems and AI research. However,
these datasets pose significant privacy risks, particularly for pedestrians,
due to the presence of Personally Identifiable Information (PII) that extends
beyond biometric traits such as faces. In this paper, we present cRID, a novel
cross-modal framework combining Large Vision-Language Models, Graph Attention
Networks, and representation learning to detect textual describable clues of
PII and enhance person re-identification (Re-ID). Our approach focuses on
identifying and leveraging interpretable features, enabling the detection of
semantically meaningful PII beyond low-level appearance cues. We conduct a
systematic evaluation of PII presence in person image datasets. Our experiments
show improved performance in practical cross-dataset Re-ID scenarios, notably
from Market-1501 to CUHK03-np (detected), highlighting the framework's
practical utility. Code is available at https://github.com/RAufschlaeger/cRID.

</details>


### [38] [Mamba Guided Boundary Prior Matters: A New Perspective for Generalized Polyp Segmentation](https://arxiv.org/abs/2507.01509)
*Tapas K. Dutta,Snehashis Majhi,Deepak Ranjan Nayak,Debesh Jha*

Main category: cs.CV

TL;DR: SAM-MaGuP enhances polyp segmentation by integrating boundary distillation and a 1D-2D Mamba adapter into SAM, achieving superior accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: Current methods struggle with weak polyp boundaries and lack generalizability for clinical use.

Method: Proposes SAM-MaGuP, combining boundary distillation and a 1D-2D Mamba adapter within SAM.

Result: Outperforms state-of-the-art methods across five datasets.

Conclusion: SAM-MaGuP sets a new benchmark in polyp segmentation with its innovative boundary and feature learning techniques.

Abstract: Polyp segmentation in colonoscopy images is crucial for early detection and
diagnosis of colorectal cancer. However, this task remains a significant
challenge due to the substantial variations in polyp shape, size, and color, as
well as the high similarity between polyps and surrounding tissues, often
compounded by indistinct boundaries. While existing encoder-decoder CNN and
transformer-based approaches have shown promising results, they struggle with
stable segmentation performance on polyps with weak or blurry boundaries. These
methods exhibit limited abilities to distinguish between polyps and non-polyps
and capture essential boundary cues. Moreover, their generalizability still
falls short of meeting the demands of real-time clinical applications. To
address these limitations, we propose SAM-MaGuP, a groundbreaking approach for
robust polyp segmentation. By incorporating a boundary distillation module and
a 1D-2D Mamba adapter within the Segment Anything Model (SAM), SAM-MaGuP excels
at resolving weak boundary challenges and amplifies feature learning through
enriched global contextual interactions. Extensive evaluations across five
diverse datasets reveal that SAM-MaGuP outperforms state-of-the-art methods,
achieving unmatched segmentation accuracy and robustness. Our key innovations,
a Mamba-guided boundary prior and a 1D-2D Mamba block, set a new benchmark in
the field, pushing the boundaries of polyp segmentation to new heights.

</details>


### [39] [Exploring Pose-based Sign Language Translation: Ablation Studies and Attention Insights](https://arxiv.org/abs/2507.01532)
*Tomas Zelezny,Jakub Straka,Vaclav Javorek,Ondrej Valach,Marek Hruz,Ivan Gruber*

Main category: cs.CV

TL;DR: The paper investigates how pose-based preprocessing (normalization, interpolation, augmentation) enhances Sign Language Translation (SLT) using a transformer-based model, showing improved robustness and generalization.


<details>
  <summary>Details</summary>
Motivation: To improve SLT performance by analyzing the impact of pose-based data preprocessing techniques on translation accuracy.

Method: Uses a modified T5 encoder-decoder transformer model to process pose representations, with ablation studies on YouTubeASL and How2Sign datasets.

Result: Appropriate preprocessing (normalization, interpolation, augmentation) significantly boosts model robustness and generalization. Adding a register token also improves performance.

Conclusion: Pose-based preprocessing techniques are crucial for enhancing SLT, and a dedicated register token can further optimize model performance.

Abstract: Sign Language Translation (SLT) has evolved significantly, moving from
isolated recognition approaches to complex, continuous gloss-free translation
systems. This paper explores the impact of pose-based data preprocessing
techniques - normalization, interpolation, and augmentation - on SLT
performance. We employ a transformer-based architecture, adapting a modified T5
encoder-decoder model to process pose representations. Through extensive
ablation studies on YouTubeASL and How2Sign datasets, we analyze how different
preprocessing strategies affect translation accuracy. Our results demonstrate
that appropriate normalization, interpolation, and augmentation techniques can
significantly improve model robustness and generalization abilities.
Additionally, we provide a deep analysis of the model's attentions and reveal
interesting behavior suggesting that adding a dedicated register token can
improve overall model performance. We publish our code on our GitHub
repository, including the preprocessed YouTubeASL data.

</details>


### [40] [TrackingMiM: Efficient Mamba-in-Mamba Serialization for Real-time UAV Object Tracking](https://arxiv.org/abs/2507.01535)
*Bingxi Liu,Calvin Chen,Junhao Li,Guyang Yu,Haoqian Song,Xuchen Liu,Jinqiang Cui,Hong Zhang*

Main category: cs.CV

TL;DR: The paper introduces TrackingMiM, a Mamba-in-Mamba architecture, to address the quadratic complexity of Vision Transformers (ViT) in UAV tracking, improving speed and precision.


<details>
  <summary>Details</summary>
Motivation: The quadratic complexity of ViT models hinders real-time processing in UAV tracking. The study leverages Mamba's efficiency for long-sequence modeling to tackle this issue.

Method: Proposes TrackingMiM, a nested Mamba-in-Mamba architecture, to independently process temporal and spatial patch tokens while encoding the template frame as a query token.

Result: TrackingMiM achieves state-of-the-art precision and higher speed on five UAV tracking benchmarks.

Conclusion: The proposed method effectively addresses temporal inconsistency and computational inefficiency in UAV tracking, outperforming existing approaches.

Abstract: The Vision Transformer (ViT) model has long struggled with the challenge of
quadratic complexity, a limitation that becomes especially critical in unmanned
aerial vehicle (UAV) tracking systems, where data must be processed in real
time. In this study, we explore the recently proposed State-Space Model, Mamba,
leveraging its computational efficiency and capability for long-sequence
modeling to effectively process dense image sequences in tracking tasks. First,
we highlight the issue of temporal inconsistency in existing Mamba-based
methods, specifically the failure to account for temporal continuity in the
Mamba scanning mechanism. Secondly, building upon this insight,we propose
TrackingMiM, a Mamba-in-Mamba architecture, a minimal-computation burden model
for handling image sequence of tracking problem. In our framework, the mamba
scan is performed in a nested way while independently process temporal and
spatial coherent patch tokens. While the template frame is encoded as query
token and utilized for tracking in every scan. Extensive experiments conducted
on five UAV tracking benchmarks confirm that the proposed TrackingMiM achieves
state-of-the-art precision while offering noticeable higher speed in UAV
tracking.

</details>


### [41] [A Multi-Centric Anthropomorphic 3D CT Phantom-Based Benchmark Dataset for Harmonization](https://arxiv.org/abs/2507.01539)
*Mohammadreza Amirian,Michael Bach,Oscar Jimenez-del-Toro,Christoph Aberle,Roger Schaer,Vincent Andrearczyk,Jean-Félix Maestrati,Maria Martin Asiain,Kyriakos Flouris,Markus Obmann,Clarisse Dromain,Benoît Dufour,Pierre-Alexandre Alois Poletti,Hendrik von Tengg-Kobligk,Rolf Hügli,Martin Kretzschmar,Hatem Alkadhi,Ender Konukoglu,Henning Müller,Bram Stieltjes,Adrien Depeursinge*

Main category: cs.CV

TL;DR: A benchmark dataset for AI harmonization in CT analysis is introduced to address data distribution shifts caused by scanner variations, with baseline results and open-source code provided.


<details>
  <summary>Details</summary>
Motivation: AI in medicine struggles with generalization due to data distribution shifts from scanner or setting changes. This work aims to foster AI harmonization techniques.

Method: A phantom-based dataset of 1378 CT scans from 13 scanners is created, with a methodology for assessing image- and feature-level stability and liver tissue classification.

Result: The dataset and baseline results support the development of AI harmonization strategies by fixing inter- and intra-patient variations.

Conclusion: The open-source benchmark dataset and tools aim to advance AI harmonization in CT analysis, improving generalization across diverse acquisition settings.

Abstract: Artificial intelligence (AI) has introduced numerous opportunities for human
assistance and task automation in medicine. However, it suffers from poor
generalization in the presence of shifts in the data distribution. In the
context of AI-based computed tomography (CT) analysis, significant data
distribution shifts can be caused by changes in scanner manufacturer,
reconstruction technique or dose. AI harmonization techniques can address this
problem by reducing distribution shifts caused by various acquisition settings.
This paper presents an open-source benchmark dataset containing CT scans of an
anthropomorphic phantom acquired with various scanners and settings, which
purpose is to foster the development of AI harmonization techniques. Using a
phantom allows fixing variations attributed to inter- and intra-patient
variations. The dataset includes 1378 image series acquired with 13 scanners
from 4 manufacturers across 8 institutions using a harmonized protocol as well
as several acquisition doses. Additionally, we present a methodology, baseline
results and open-source code to assess image- and feature-level stability and
liver tissue classification, promoting the development of AI harmonization
strategies.

</details>


### [42] [Interpolation-Based Event Visual Data Filtering Algorithms](https://arxiv.org/abs/2507.01557)
*Marcin Kowlaczyk,Tomasz Kryjak*

Main category: cs.CV

TL;DR: Proposes four IIR filter-based algorithms to remove ~99% of noise from event camera data while preserving valid signals, suitable for embedded devices.


<details>
  <summary>Details</summary>
Motivation: Event cameras in neuromorphic vision produce noisy data streams, necessitating effective noise removal methods.

Method: Develops four algorithms using IIR filters, tested on modified event datasets with artificial and recorded noise.

Result: Achieves ~99% noise removal with minimal memory usage (30KB for 1280x720 resolution).

Conclusion: The methods are efficient and practical for embedded implementations in neuromorphic vision applications.

Abstract: The field of neuromorphic vision is developing rapidly, and event cameras are
finding their way into more and more applications. However, the data stream
from these sensors is characterised by significant noise. In this paper, we
propose a method for event data that is capable of removing approximately 99\%
of noise while preserving the majority of the valid signal. We have proposed
four algorithms based on the matrix of infinite impulse response (IIR) filters
method. We compared them on several event datasets that were further modified
by adding artificially generated noise and noise recorded with dynamic vision
sensor. The proposed methods use about 30KB of memory for a sensor with a
resolution of 1280 x 720 and is therefore well suited for implementation in
embedded devices.

</details>


### [43] [A Gift from the Integration of Discriminative and Diffusion-based Generative Learning: Boundary Refinement Remote Sensing Semantic Segmentation](https://arxiv.org/abs/2507.01573)
*Hao Wang,Keyan Hu,Xin Guo,Haifeng Li,Chao Tao*

Main category: cs.CV

TL;DR: The paper proposes IDGBR, a framework combining discriminative and generative learning to refine boundaries in remote sensing semantic segmentation, addressing limitations in high-frequency feature learning.


<details>
  <summary>Details</summary>
Motivation: Existing segmentation models focus on low-frequency features (semantic correctness) but struggle with high-frequency details (boundary precision). Diffusion models excel at high-frequency details but lack semantic inference for low-frequency features.

Method: IDGBR integrates discriminative learning (for coarse segmentation) and diffusion-based generative learning (for boundary refinement). A conditioning guidance network combines the coarse map and original image to guide the diffusion process.

Result: Experiments on five datasets show IDGBR consistently refines boundaries for coarse results from various discriminative architectures.

Conclusion: The framework effectively combines discriminative and generative learning to improve boundary precision in semantic segmentation.

Abstract: Remote sensing semantic segmentation must address both what the ground
objects are within an image and where they are located. Consequently,
segmentation models must ensure not only the semantic correctness of
large-scale patches (low-frequency information) but also the precise
localization of boundaries between patches (high-frequency information).
However, most existing approaches rely heavily on discriminative learning,
which excels at capturing low-frequency features, while overlooking its
inherent limitations in learning high-frequency features for semantic
segmentation. Recent studies have revealed that diffusion generative models
excel at generating high-frequency details. Our theoretical analysis confirms
that the diffusion denoising process significantly enhances the model's ability
to learn high-frequency features; however, we also observe that these models
exhibit insufficient semantic inference for low-frequency features when guided
solely by the original image. Therefore, we integrate the strengths of both
discriminative and generative learning, proposing the Integration of
Discriminative and diffusion-based Generative learning for Boundary Refinement
(IDGBR) framework. The framework first generates a coarse segmentation map
using a discriminative backbone model. This map and the original image are fed
into a conditioning guidance network to jointly learn a guidance representation
subsequently leveraged by an iterative denoising diffusion process refining the
coarse segmentation. Extensive experiments across five remote sensing semantic
segmentation datasets (binary and multi-class segmentation) confirm our
framework's capability of consistent boundary refinement for coarse results
from diverse discriminative architectures. The source code will be available at
https://github.com/KeyanHu-git/IDGBR.

</details>


### [44] [SketchColour: Channel Concat Guided DiT-based Sketch-to-Colour Pipeline for 2D Animation](https://arxiv.org/abs/2507.01586)
*Bryan Constantine Sadihin,Michael Hua Wang,Shei Pern Chua,Hang Su*

Main category: cs.CV

TL;DR: SketchColour introduces a diffusion transformer-based pipeline for 2D animation colourization, reducing labor and resource usage while outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Manual colourization of 2D animation is labor-intensive; SketchColour aims to automate this process efficiently.

Method: Uses a diffusion transformer (DiT) backbone with lightweight adapters and LoRA finetuning, avoiding ControlNet's inefficiencies.

Result: Outperforms state-of-the-art methods on the SAKUGA dataset with half the training data, producing coherent animations.

Conclusion: SketchColour offers a resource-efficient, high-quality solution for 2D animation colourization.

Abstract: The production of high-quality 2D animation is highly labor-intensive
process, as animators are currently required to draw and color a large number
of frames by hand. We present SketchColour, the first sketch-to-colour pipeline
for 2D animation built on a diffusion transformer (DiT) backbone. By replacing
the conventional U-Net denoiser with a DiT-style architecture and injecting
sketch information via lightweight channel-concatenation adapters accompanied
with LoRA finetuning, our method natively integrates conditioning without the
parameter and memory bloat of a duplicated ControlNet, greatly reducing
parameter count and GPU memory usage. Evaluated on the SAKUGA dataset,
SketchColour outperforms previous state-of-the-art video colourization methods
across all metrics, despite using only half the training data of competing
models. Our approach produces temporally coherent animations with minimal
artifacts such as colour bleeding or object deformation. Our code is available
at: https://bconstantine.github.io/SketchColour .

</details>


### [45] [Towards Controllable Real Image Denoising with Camera Parameters](https://arxiv.org/abs/2507.01587)
*Youngjin Oh,Junhyeong Kwon,Keuntek Lee,Nam Ik Cho*

Main category: cs.CV

TL;DR: A controllable denoising framework adapts to noise levels using camera parameters (ISO, shutter speed, F-number) to enhance denoising performance.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning-based denoising methods lack flexibility in adjusting denoising strength based on noise levels, camera settings, and user preferences.

Method: Converts camera parameters (ISO, shutter speed, F-number) into a vector to control and enhance a denoising network.

Result: The method adds controllability to standard denoising networks and improves performance.

Conclusion: The framework effectively adapts denoising strength using camera parameters, offering better flexibility and performance.

Abstract: Recent deep learning-based image denoising methods have shown impressive
performance; however, many lack the flexibility to adjust the denoising
strength based on the noise levels, camera settings, and user preferences. In
this paper, we introduce a new controllable denoising framework that adaptively
removes noise from images by utilizing information from camera parameters.
Specifically, we focus on ISO, shutter speed, and F-number, which are closely
related to noise levels. We convert these selected parameters into a vector to
control and enhance the performance of the denoising network. Experimental
results show that our method seamlessly adds controllability to standard
denoising neural networks and improves their performance. Code is available at
https://github.com/OBAKSA/CPADNet.

</details>


### [46] [Autonomous AI Surveillance: Multimodal Deep Learning for Cognitive and Behavioral Monitoring](https://arxiv.org/abs/2507.01590)
*Ameer Hamza,Zuhaib Hussain But,Umar Arif,Samiya,M. Abdullah Asad,Muhammad Naeem*

Main category: cs.CV

TL;DR: A novel classroom surveillance system integrates drowsiness, mobile phone tracking, and face recognition to assess student attentiveness with high precision using YOLOv8 and LResNet models.


<details>
  <summary>Details</summary>
Motivation: To enhance classroom monitoring by providing real-time insights into student engagement and behavior, while automating attendance recording.

Method: Uses YOLOv8 for mobile phone and sleep detection, LResNet for face recognition, and combines them in a PHP web application with ESP32-CAM hardware.

Result: Achieves 97.42% mAP@50 for sleep detection, 86.45% validation accuracy for face recognition, and 85.89% mAP@50 for mobile phone detection.

Conclusion: The system offers scalable, real-time monitoring for educational environments, improving engagement tracking and attendance automation.

Abstract: This study presents a novel classroom surveillance system that integrates
multiple modalities, including drowsiness, tracking of mobile phone usage, and
face recognition,to assess student attentiveness with enhanced precision.The
system leverages the YOLOv8 model to detect both mobile phone and sleep
usage,(Ghatge et al., 2024) while facial recognition is achieved through
LResNet Occ FC body tracking using YOLO and MTCNN.(Durai et al., 2024) These
models work in synergy to provide comprehensive, real-time monitoring, offering
insights into student engagement and behavior.(S et al., 2023) The framework is
trained on specialized datasets, such as the RMFD dataset for face recognition
and a Roboflow dataset for mobile phone detection. The extensive evaluation of
the system shows promising results. Sleep detection achieves 97. 42% mAP@50,
face recognition achieves 86. 45% validation accuracy and mobile phone
detection reach 85. 89% mAP@50. The system is implemented within a core PHP web
application and utilizes ESP32-CAM hardware for seamless data capture.(Neto et
al., 2024) This integrated approach not only enhances classroom monitoring, but
also ensures automatic attendance recording via face recognition as students
remain seated in the classroom, offering scalability for diverse educational
environments.(Banada,2025)

</details>


### [47] [DepthSync: Diffusion Guidance-Based Depth Synchronization for Scale- and Geometry-Consistent Video Depth Estimation](https://arxiv.org/abs/2507.01603)
*Yue-Jiang Dong,Wang Zhao,Jiale Xu,Ying Shan,Song-Hai Zhang*

Main category: cs.CV

TL;DR: DepthSync is a training-free framework using diffusion guidance to achieve consistent depth predictions for long videos by addressing scale discrepancies and geometric inconsistencies.


<details>
  <summary>Details</summary>
Motivation: Existing methods for video depth estimation face challenges with scale discrepancies and geometric inconsistencies in long videos, relying only on 2D priors and ignoring 3D structure.

Method: DepthSync introduces scale guidance to synchronize depth scales across windows and geometry guidance to enforce 3D geometric alignment within windows.

Result: Experiments show DepthSync improves scale and geometry consistency in depth predictions, especially for long videos.

Conclusion: DepthSync effectively addresses the limitations of existing methods, providing consistent and accurate depth estimates for long videos.

Abstract: Diffusion-based video depth estimation methods have achieved remarkable
success with strong generalization ability. However, predicting depth for long
videos remains challenging. Existing methods typically split videos into
overlapping sliding windows, leading to accumulated scale discrepancies across
different windows, particularly as the number of windows increases.
Additionally, these methods rely solely on 2D diffusion priors, overlooking the
inherent 3D geometric structure of video depths, which results in geometrically
inconsistent predictions. In this paper, we propose DepthSync, a novel,
training-free framework using diffusion guidance to achieve scale- and
geometry-consistent depth predictions for long videos. Specifically, we
introduce scale guidance to synchronize the depth scale across windows and
geometry guidance to enforce geometric alignment within windows based on the
inherent 3D constraints in video depths. These two terms work synergistically,
steering the denoising process toward consistent depth predictions. Experiments
on various datasets validate the effectiveness of our method in producing depth
estimates with improved scale and geometry consistency, particularly for long
videos.

</details>


### [48] [Survivability of Backdoor Attacks on Unconstrained Face Recognition Systems](https://arxiv.org/abs/2507.01607)
*Quentin Le Roux,Yannick Teglia,Teddy Furon,Philippe Loubet-Moundi,Eric Bourbao*

Main category: cs.CV

TL;DR: This paper explores backdoor attacks in deep learning-based face recognition systems, demonstrating novel attacks on face detection and feature extraction, and providing countermeasures.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of research on DNN backdoor attacks in real-life, unconstrained face recognition systems.

Method: Conducts a system-level study, demonstrating two backdoor attacks (face generation and landmark shift) and testing 20 pipeline configurations.

Result: Shows that a single backdoor can bypass entire system functions, with 15 attack cases validated.

Conclusion: Provides best practices and countermeasures for stakeholders to mitigate backdoor vulnerabilities.

Abstract: The widespread use of deep learning face recognition raises several security
concerns. Although prior works point at existing vulnerabilities, DNN backdoor
attacks against real-life, unconstrained systems dealing with images captured
in the wild remain a blind spot of the literature. This paper conducts the
first system-level study of backdoors in deep learning-based face recognition
systems. This paper yields four contributions by exploring the feasibility of
DNN backdoors on these pipelines in a holistic fashion. We demonstrate for the
first time two backdoor attacks on the face detection task: face generation and
face landmark shift attacks. We then show that face feature extractors trained
with large margin losses also fall victim to backdoor attacks. Combining our
models, we then show using 20 possible pipeline configurations and 15 attack
cases that a single backdoor enables an attacker to bypass the entire function
of a system. Finally, we provide stakeholders with several best practices and
countermeasures.

</details>


### [49] [Perception-Oriented Latent Coding for High-Performance Compressed Domain Semantic Inference](https://arxiv.org/abs/2507.01608)
*Xu Zhang,Ming Lu,Yan Chen,Zhan Ma*

Main category: cs.CV

TL;DR: POLC introduces a perception-oriented latent coding method to enhance semantic richness in compressed images, reducing fine-tuning overhead while improving vision task performance.


<details>
  <summary>Details</summary>
Motivation: MSE-optimized latent spaces lack semantic richness, and fine-tuning large models is computationally intensive. POLC aims to address these issues.

Method: POLC enriches latent features semantically, requiring only a plug-and-play adapter for fine-tuning, reducing parameters.

Result: POLC matches state-of-the-art generative image coding in rate-perception and boosts vision task performance with minimal fine-tuning.

Conclusion: POLC offers a computationally efficient solution for high-performance compressed domain semantic inference.

Abstract: In recent years, compressed domain semantic inference has primarily relied on
learned image coding models optimized for mean squared error (MSE). However,
MSE-oriented optimization tends to yield latent spaces with limited semantic
richness, which hinders effective semantic inference in downstream tasks.
Moreover, achieving high performance with these models often requires
fine-tuning the entire vision model, which is computationally intensive,
especially for large models. To address these problems, we introduce
Perception-Oriented Latent Coding (POLC), an approach that enriches the
semantic content of latent features for high-performance compressed domain
semantic inference. With the semantically rich latent space, POLC requires only
a plug-and-play adapter for fine-tuning, significantly reducing the parameter
count compared to previous MSE-oriented methods. Experimental results
demonstrate that POLC achieves rate-perception performance comparable to
state-of-the-art generative image coding methods while markedly enhancing
performance in vision tasks, with minimal fine-tuning overhead. Code is
available at https://github.com/NJUVISION/POLC.

</details>


### [50] [Prompt Guidance and Human Proximal Perception for HOT Prediction with Regional Joint Loss](https://arxiv.org/abs/2507.01630)
*Yuxiao Wang,Yu Lei,Zhenao Wei,Weiying Xue,Xinyu Jiang,Nan Zhuang,Qi Liu*

Main category: cs.CV

TL;DR: P3HOT is a novel framework for Human-Object conTact (HOT) detection, combining prompt guidance and human proximal perception to improve accuracy and consistency. It introduces a new loss function and evaluation metric, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Current HOT detection models are limited to single image types, causing oversegmentation and inconsistency. P3HOT aims to address these issues by leveraging semantic prompts and depth perception.

Method: P3HOT uses semantic-driven prompts to focus on relevant regions and a human proximal perception mechanism for dynamic depth range awareness. It also introduces a Regional Joint Loss (RJLoss) and a new metric (AD-Acc.).

Result: P3HOT outperforms existing methods, with improvements of 0.7, 2.0, 1.6, and 11.0 in SC-Acc., mIoU, wIoU, and AD-Acc. metrics on the HOT-Annotated dataset.

Conclusion: P3HOT effectively addresses HOT detection challenges, achieving superior performance through innovative mechanisms and metrics.

Abstract: The task of Human-Object conTact (HOT) detection involves identifying the
specific areas of the human body that are touching objects. Nevertheless,
current models are restricted to just one type of image, often leading to too
much segmentation in areas with little interaction, and struggling to maintain
category consistency within specific regions. To tackle this issue, a HOT
framework, termed \textbf{P3HOT}, is proposed, which blends \textbf{P}rompt
guidance and human \textbf{P}roximal \textbf{P}erception. To begin with, we
utilize a semantic-driven prompt mechanism to direct the network's attention
towards the relevant regions based on the correlation between image and text.
Then a human proximal perception mechanism is employed to dynamically perceive
key depth range around the human, using learnable parameters to effectively
eliminate regions where interactions are not expected. Calculating depth
resolves the uncertainty of the overlap between humans and objects in a 2D
perspective, providing a quasi-3D viewpoint. Moreover, a Regional Joint Loss
(RJLoss) has been created as a new loss to inhibit abnormal categories in the
same area. A new evaluation metric called ``AD-Acc.'' is introduced to address
the shortcomings of existing methods in addressing negative samples.
Comprehensive experimental results demonstrate that our approach achieves
state-of-the-art performance in four metrics across two benchmark datasets.
Specifically, our model achieves an improvement of \textbf{0.7}$\uparrow$,
\textbf{2.0}$\uparrow$, \textbf{1.6}$\uparrow$, and \textbf{11.0}$\uparrow$ in
SC-Acc., mIoU, wIoU, and AD-Acc. metrics, respectively, on the HOT-Annotated
dataset. Code is available at https://github.com/YuxiaoWang-AI/P3HOT.

</details>


### [51] [Tile and Slide : A New Framework for Scaling NeRF from Local to Global 3D Earth Observation](https://arxiv.org/abs/2507.01631)
*Camille Billouard,Dawa Derksen,Alexandre Constantin,Bruno Vallet*

Main category: cs.CV

TL;DR: Snake-NeRF scales NeRF to large scenes by tiling without overlap, using an out-of-core method and novel tile progression for efficient, high-quality 3D reconstruction on a single GPU.


<details>
  <summary>Details</summary>
Motivation: Current NeRF methods are limited to small scenes due to memory constraints during training.

Method: Divides scenes into non-overlapping 3D tiles, crops images with overlap, and uses a $2\times2$ tile progression strategy with a segmented sampler.

Result: Enables processing large satellite images with linear time complexity on a single GPU without quality loss.

Conclusion: Snake-NeRF effectively scales NeRF to large scenes efficiently and maintains reconstruction quality.

Abstract: Neural Radiance Fields (NeRF) have recently emerged as a paradigm for 3D
reconstruction from multiview satellite imagery. However, state-of-the-art NeRF
methods are typically constrained to small scenes due to the memory footprint
during training, which we study in this paper. Previous work on large-scale
NeRFs palliate this by dividing the scene into NeRFs. This paper introduces
Snake-NeRF, a framework that scales to large scenes. Our out-of-core method
eliminates the need to load all images and networks simultaneously, and
operates on a single device. We achieve this by dividing the region of interest
into NeRFs that 3D tile without overlap. Importantly, we crop the images with
overlap to ensure each NeRFs is trained with all the necessary pixels. We
introduce a novel $2\times 2$ 3D tile progression strategy and segmented
sampler, which together prevent 3D reconstruction errors along the tile edges.
Our experiments conclude that large satellite images can effectively be
processed with linear time complexity, on a single GPU, and without compromise
in quality.

</details>


### [52] [Depth Anything at Any Condition](https://arxiv.org/abs/2507.01634)
*Boyuan Sun,Modi Jin,Bowen Yin,Qibin Hou*

Main category: cs.CV

TL;DR: DepthAnything-AC is a monocular depth estimation model designed for diverse conditions, using unsupervised finetuning and spatial constraints to improve accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing models struggle with complex environments like adverse weather or sensor distortions, and lack high-quality pseudo-labels for training.

Method: Proposes unsupervised consistency regularization finetuning and Spatial Distance Constraint for patch-level learning.

Result: Demonstrates strong zero-shot performance across adverse weather, synthetic corruption, and general benchmarks.

Conclusion: DepthAnything-AC effectively handles diverse conditions, advancing monocular depth estimation in challenging environments.

Abstract: We present Depth Anything at Any Condition (DepthAnything-AC), a foundation
monocular depth estimation (MDE) model capable of handling diverse
environmental conditions. Previous foundation MDE models achieve impressive
performance across general scenes but not perform well in complex open-world
environments that involve challenging conditions, such as illumination
variations, adverse weather, and sensor-induced distortions. To overcome the
challenges of data scarcity and the inability of generating high-quality
pseudo-labels from corrupted images, we propose an unsupervised consistency
regularization finetuning paradigm that requires only a relatively small amount
of unlabeled data. Furthermore, we propose the Spatial Distance Constraint to
explicitly enforce the model to learn patch-level relative relationships,
resulting in clearer semantic boundaries and more accurate details.
Experimental results demonstrate the zero-shot capabilities of DepthAnything-AC
across diverse benchmarks, including real-world adverse weather benchmarks,
synthetic corruption benchmarks, and general benchmarks.
  Project Page: https://ghost233lism.github.io/depthanything-AC-page
  Code: https://github.com/HVision-NKU/DepthAnythingAC

</details>


### [53] [SAILViT: Towards Robust and Generalizable Visual Backbones for MLLMs via Gradual Feature Refinement](https://arxiv.org/abs/2507.01643)
*Weijie Yin,Dingkang Yang,Hongyuan Dong,Zijian Kang,Jiacong Wang,Xiao Liang,Chao Feng,Jiao Ran*

Main category: cs.CV

TL;DR: SAILViT is a Vision Transformer designed to enhance Multimodal Large Language Models (MLLMs) by addressing parameter conflicts and semantic gaps, improving performance in complex multimodal tasks.


<details>
  <summary>Details</summary>
Motivation: Current ViTs struggle with co-training directly with LLMs due to initialization conflicts and modality gaps, limiting MLLM performance.

Method: SAILViT introduces gradual feature learning for coarse-to-fine alignment and knowledge infusion, refining features to meet training demands.

Result: SAILViT shows robustness across various dimensions (e.g., parameter sizes, architectures) and boosts MLLM performance on the OpenCompass benchmark.

Conclusion: SAILViT effectively bridges gaps in MLLMs, enabling consistent performance improvements in multimodal tasks.

Abstract: Vision Transformers (ViTs) are essential as foundation backbones in
establishing the visual comprehension capabilities of Multimodal Large Language
Models (MLLMs). Although most ViTs achieve impressive performance through
image-text pair-based contrastive learning or self-supervised mechanisms, they
struggle to engage in connector-based co-training directly with LLMs due to
potential parameter initialization conflicts and modality semantic gaps. To
address the above challenges, this paper proposes SAILViT, a gradual feature
learning-enhanced ViT for facilitating MLLMs to break through performance
bottlenecks in complex multimodal interactions. SAILViT achieves
coarse-to-fine-grained feature alignment and world knowledge infusion with
gradual feature refinement, which better serves target training demands. We
perform thorough empirical analyses to confirm the powerful robustness and
generalizability of SAILViT across different dimensions, including parameter
sizes, model architectures, training strategies, and data scales. Equipped with
SAILViT, existing MLLMs show significant and consistent performance
improvements on the OpenCompass benchmark across extensive downstream tasks.
SAILViT series models are released at
https://huggingface.co/BytedanceDouyinContent.

</details>


### [54] [Autoregressive Image Generation with Linear Complexity: A Spatial-Aware Decay Perspective](https://arxiv.org/abs/2507.01652)
*Yuxin Mao,Zhen Qin,Jinxing Zhou,Hui Deng,Xuyang Shen,Bin Fan,Jing Zhang,Yiran Zhong,Yuchao Dai*

Main category: cs.CV

TL;DR: LASADGen introduces a linear attention mechanism (LASAD) for autoregressive image generation, improving efficiency and quality by preserving 2D spatial relationships.


<details>
  <summary>Details</summary>
Motivation: Current AR models for image generation rely on transformers with high computational costs and memory overhead, while linear attention degrades quality due to poor long-range dependency capture.

Method: Proposes LASAD, a linear attention mechanism with spatial-aware decay, preserving 2D spatial relationships in flattened sequences. LASADGen is built on this for efficient, high-quality generation.

Result: LASADGen achieves state-of-the-art performance on ImageNet, balancing efficiency and spatial understanding.

Conclusion: LASADGen bridges the gap between computational efficiency and high-quality image generation by leveraging spatial-aware linear attention.

Abstract: Autoregressive (AR) models have garnered significant attention in image
generation for their ability to effectively capture both local and global
structures within visual data. However, prevalent AR models predominantly rely
on the transformer architectures, which are beset by quadratic computational
complexity concerning input sequence length and substantial memory overhead due
to the necessity of maintaining key-value caches. Although linear attention
mechanisms have successfully reduced this burden in language models, our
initial experiments reveal that they significantly degrade image generation
quality because of their inability to capture critical long-range dependencies
in visual data. We propose Linear Attention with Spatial-Aware Decay (LASAD), a
novel attention mechanism that explicitly preserves genuine 2D spatial
relationships within the flattened image sequences by computing
position-dependent decay factors based on true 2D spatial location rather than
1D sequence positions. Based on this mechanism, we present LASADGen, an
autoregressive image generator that enables selective attention to relevant
spatial contexts with linear complexity. Experiments on ImageNet show LASADGen
achieves state-of-the-art image generation performance and computational
efficiency, bridging the gap between linear attention's efficiency and spatial
understanding needed for high-quality generation.

</details>


### [55] [RobuSTereo: Robust Zero-Shot Stereo Matching under Adverse Weather](https://arxiv.org/abs/2507.01653)
*Yuran Wang,Yingping Liang,Yutao Hu,Ying Fu*

Main category: cs.CV

TL;DR: RobuSTereo improves stereo matching models' zero-shot generalization in adverse weather by addressing data scarcity and feature extraction challenges using synthetic data and a robust feature encoder.


<details>
  <summary>Details</summary>
Motivation: Stereo matching models struggle in adverse weather due to lack of training data and degraded feature extraction, limiting zero-shot generalization.

Method: Proposes a diffusion-based simulation pipeline for synthetic data and a feature encoder combining ConvNet and denoising transformer.

Result: Enhances robustness and generalization in diverse adverse weather scenarios.

Conclusion: RobuSTereo effectively addresses key challenges, improving stereo matching performance in unseen weather conditions.

Abstract: Learning-based stereo matching models struggle in adverse weather conditions
due to the scarcity of corresponding training data and the challenges in
extracting discriminative features from degraded images. These limitations
significantly hinder zero-shot generalization to out-of-distribution weather
conditions. In this paper, we propose \textbf{RobuSTereo}, a novel framework
that enhances the zero-shot generalization of stereo matching models under
adverse weather by addressing both data scarcity and feature extraction
challenges. First, we introduce a diffusion-based simulation pipeline with a
stereo consistency module, which generates high-quality stereo data tailored
for adverse conditions. By training stereo matching models on our synthetic
datasets, we reduce the domain gap between clean and degraded images,
significantly improving the models' robustness to unseen weather conditions.
The stereo consistency module ensures structural alignment across synthesized
image pairs, preserving geometric integrity and enhancing depth estimation
accuracy. Second, we design a robust feature encoder that combines a
specialized ConvNet with a denoising transformer to extract stable and reliable
features from degraded images. The ConvNet captures fine-grained local
structures, while the denoising transformer refines global representations,
effectively mitigating the impact of noise, low visibility, and weather-induced
distortions. This enables more accurate disparity estimation even under
challenging visual conditions. Extensive experiments demonstrate that
\textbf{RobuSTereo} significantly improves the robustness and generalization of
stereo matching models across diverse adverse weather scenarios.

</details>


### [56] [SPoT: Subpixel Placement of Tokens in Vision Transformers](https://arxiv.org/abs/2507.01654)
*Martine Hjelkrem-Tan,Marius Aasan,Gabriel Y. Arteaga,Adín Ramírez Rivera*

Main category: cs.CV

TL;DR: SPoT introduces continuous token placement in Vision Transformers, bypassing grid constraints and improving efficiency.


<details>
  <summary>Details</summary>
Motivation: Standard tokenization limits Vision Transformers by confining features to grids, hindering sparsity exploitation.

Method: Proposes Subpixel Placement of Tokens (SPoT) with oracle-guided search for optimal token positioning.

Result: Achieves significant performance gains with fewer tokens, enhancing efficiency and accuracy.

Conclusion: SPoT redefines sparsity as an advantage, enabling flexible and efficient ViT architectures.

Abstract: Vision Transformers naturally accommodate sparsity, yet standard tokenization
methods confine features to discrete patch grids. This constraint prevents
models from fully exploiting sparse regimes, forcing awkward compromises. We
propose Subpixel Placement of Tokens (SPoT), a novel tokenization strategy that
positions tokens continuously within images, effectively sidestepping
grid-based limitations. With our proposed oracle-guided search, we uncover
substantial performance gains achievable with ideal subpixel token positioning,
drastically reducing the number of tokens necessary for accurate predictions
during inference. SPoT provides a new direction for flexible, efficient, and
interpretable ViT architectures, redefining sparsity as a strategic advantage
rather than an imposed limitation.

</details>


### [57] [What does really matter in image goal navigation?](https://arxiv.org/abs/2507.01667)
*Gianluca Monaci,Philippe Weinzaepfel,Christian Wolf*

Main category: cs.CV

TL;DR: The paper explores whether image goal navigation can be efficiently solved with end-to-end RL training, analyzing architectural choices and simulator impacts.


<details>
  <summary>Details</summary>
Motivation: To determine if end-to-end RL training can replace dedicated methods for image goal navigation and enable relative pose estimation from navigation rewards.

Method: Investigates architectural choices (late fusion, channel stacking, etc.) and their impact on emerging relative pose estimators through RL training.

Result: Simulator settings influence success, but capabilities can transfer to realistic settings. Navigation performance correlates with emerging relative pose estimation.

Conclusion: End-to-end RL training shows promise for image goal navigation, though simulator shortcuts exist. Emerging skills like relative pose estimation are linked to navigation success.

Abstract: Image goal navigation requires two different skills: firstly, core navigation
skills, including the detection of free space and obstacles, and taking
decisions based on an internal representation; and secondly, computing
directional information by comparing visual observations to the goal image.
Current state-of-the-art methods either rely on dedicated image-matching, or
pre-training of computer vision modules on relative pose estimation. In this
paper, we study whether this task can be efficiently solved with end-to-end
training of full agents with RL, as has been claimed by recent work. A positive
answer would have impact beyond Embodied AI and allow training of relative pose
estimation from reward for navigation alone. In a large study we investigate
the effect of architectural choices like late fusion, channel stacking,
space-to-depth projections and cross-attention, and their role in the emergence
of relative pose estimators from navigation training. We show that the success
of recent methods is influenced up to a certain extent by simulator settings,
leading to shortcuts in simulation. However, we also show that these
capabilities can be transferred to more realistic setting, up to some extend.
We also find evidence for correlations between navigation performance and
probed (emerging) relative pose estimation performance, an important sub skill.

</details>


### [58] [Facial Emotion Learning with Text-Guided Multiview Fusion via Vision-Language Model for 3D/4D Facial Expression Recognition](https://arxiv.org/abs/2507.01673)
*Muzammil Behzad*

Main category: cs.CV

TL;DR: FACET-VLM is a vision-language framework for 3D/4D facial expression recognition, integrating multiview learning with semantic guidance, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Advancing applications in human behavior understanding, healthcare monitoring, and human-computer interaction by addressing the complexity of spatial and temporal facial dynamics.

Method: Proposes FACET-VLM with Cross-View Semantic Aggregation, Multiview Text-Guided Fusion, and a multiview consistency loss for structural coherence.

Result: Achieves top accuracy on BU-3DFE, Bosphorus, BU-4DFE, and BP4D-Spontaneous benchmarks and extends successfully to 4D micro-expression recognition.

Conclusion: FACET-VLM provides a robust, extensible, and high-performing solution for multimodal facial expression recognition in posed and spontaneous settings.

Abstract: Facial expression recognition (FER) in 3D and 4D domains presents a
significant challenge in affective computing due to the complexity of spatial
and temporal facial dynamics. Its success is crucial for advancing applications
in human behavior understanding, healthcare monitoring, and human-computer
interaction. In this work, we propose FACET-VLM, a vision-language framework
for 3D/4D FER that integrates multiview facial representation learning with
semantic guidance from natural language prompts. FACET-VLM introduces three key
components: Cross-View Semantic Aggregation (CVSA) for view-consistent fusion,
Multiview Text-Guided Fusion (MTGF) for semantically aligned facial emotions,
and a multiview consistency loss to enforce structural coherence across views.
Our model achieves state-of-the-art accuracy across multiple benchmarks,
including BU-3DFE, Bosphorus, BU-4DFE, and BP4D-Spontaneous. We further extend
FACET-VLM to 4D micro-expression recognition (MER) on the 4DME dataset,
demonstrating strong performance in capturing subtle, short-lived emotional
cues. The extensive experimental results confirm the effectiveness and
substantial contributions of each individual component within the framework.
Overall, FACET-VLM offers a robust, extensible, and high-performing solution
for multimodal FER in both posed and spontaneous settings.

</details>


### [59] [Component Adaptive Clustering for Generalized Category Discovery](https://arxiv.org/abs/2507.01711)
*Mingfu Yan,Jiancheng Huang,Yifan Liu,Shifeng Chen*

Main category: cs.CV

TL;DR: AdaGCD introduces adaptive slot attention for generalized category discovery, dynamically clustering unlabeled images into known and novel classes without predefined class counts.


<details>
  <summary>Details</summary>
Motivation: Traditional methods rely on rigid assumptions like predefined class counts, limiting adaptability to real-world data variability.

Method: AdaGCD uses cluster-centric contrastive learning with Adaptive Slot Attention (AdaSlot) to dynamically allocate slots based on data complexity.

Result: The framework effectively clusters unlabeled data into known and novel categories, validated on public and fine-grained datasets.

Conclusion: AdaGCD improves class discovery in open-world scenarios by leveraging adaptive representation and spatial local information.

Abstract: Generalized Category Discovery (GCD) tackles the challenging problem of
categorizing unlabeled images into both known and novel classes within a
partially labeled dataset, without prior knowledge of the number of unknown
categories. Traditional methods often rely on rigid assumptions, such as
predefining the number of classes, which limits their ability to handle the
inherent variability and complexity of real-world data. To address these
shortcomings, we propose AdaGCD, a cluster-centric contrastive learning
framework that incorporates Adaptive Slot Attention (AdaSlot) into the GCD
framework. AdaSlot dynamically determines the optimal number of slots based on
data complexity, removing the need for predefined slot counts. This adaptive
mechanism facilitates the flexible clustering of unlabeled data into known and
novel categories by dynamically allocating representational capacity. By
integrating adaptive representation with dynamic slot allocation, our method
captures both instance-specific and spatially clustered features, improving
class discovery in open-world scenarios. Extensive experiments on public and
fine-grained datasets validate the effectiveness of our framework, emphasizing
the advantages of leveraging spatial local information for category discovery
in unlabeled image datasets.

</details>


### [60] [ECCV 2024 W-CODA: 1st Workshop on Multimodal Perception and Comprehension of Corner Cases in Autonomous Driving](https://arxiv.org/abs/2507.01735)
*Kai Chen,Ruiyuan Gao,Lanqing Hong,Hang Xu,Xu Jia,Holger Caesar,Dengxin Dai,Bingbing Liu,Dzmitry Tsishkou,Songcen Xu,Chunjing Xu,Qiang Xu,Huchuan Lu,Dit-Yan Yeung*

Main category: cs.CV

TL;DR: Summary of the 1st W-CODA workshop at ECCV 2024, focusing on autonomous driving corner cases using multimodal techniques.


<details>
  <summary>Details</summary>
Motivation: To advance next-gen solutions for autonomous driving corner cases by leveraging cutting-edge multimodal perception and comprehension.

Method: Organized a workshop with 5 speakers, collected research papers, and held a dual-track challenge (scene understanding and generation).

Result: Brought together academia and industry to share progress and address corner cases in autonomous driving.

Conclusion: The workshop is a pioneering effort to bridge the gap between advanced techniques and reliable self-driving agents for corner cases.

Abstract: In this paper, we present details of the 1st W-CODA workshop, held in
conjunction with the ECCV 2024. W-CODA aims to explore next-generation
solutions for autonomous driving corner cases, empowered by state-of-the-art
multimodal perception and comprehension techniques. 5 Speakers from both
academia and industry are invited to share their latest progress and opinions.
We collect research papers and hold a dual-track challenge, including both
corner case scene understanding and generation. As the pioneering effort, we
will continuously bridge the gap between frontier autonomous driving techniques
and fully intelligent, reliable self-driving agents robust towards corner
cases.

</details>


### [61] [Using Wavelet Domain Fingerprints to Improve Source Camera Identification](https://arxiv.org/abs/2507.01712)
*Xinle Tian,Matthew Nunes,Emiko Dupont,Shaunagh Downing,Freddie Lichtenstein,Matt Burns*

Main category: cs.CV

TL;DR: Proposes a wavelet domain fingerprint for SPN extraction, improving accuracy and speed by avoiding inversion steps.


<details>
  <summary>Details</summary>
Motivation: Enhance source identification and image forensics by simplifying SPN extraction and comparison.

Method: Modifies wavelet-based SPN extraction by using wavelet domain fingerprints, eliminating inversion steps.

Result: Higher detection accuracy and significant processing speed improvement on real-world datasets.

Conclusion: The wavelet domain fingerprint method is more efficient and effective for SPN extraction.

Abstract: Camera fingerprint detection plays a crucial role in source identification
and image forensics, with wavelet denoising approaches proving to be
particularly effective in extracting sensor pattern noise (SPN). In this
article, we propose a modification to wavelet-based SPN extraction. Rather than
constructing the fingerprint as an image, we introduce the notion of a wavelet
domain fingerprint. This avoids the final inversion step of the denoising
algorithm and allows fingerprint comparisons to be made directly in the wavelet
domain. As such, our modification streamlines the extraction and comparison
process. Experimental results on real-world datasets demonstrate that our
method not only achieves higher detection accuracy but can also significantly
improve processing speed.

</details>


### [62] [Soft Self-labeling and Potts Relaxations for Weakly-Supervised Segmentation](https://arxiv.org/abs/2507.01721)
*Zhongwen Zhang,Yuri Boykov*

Main category: cs.CV

TL;DR: Soft self-labeling improves weakly supervised segmentation (WSSS) by optimizing CRF/Potts losses with uncertainty-aware pseudo-labels, outperforming complex methods and even full supervision.


<details>
  <summary>Details</summary>
Motivation: Hard pseudo-labels in WSSS lack uncertainty representation, motivating soft self-labeling for better performance.

Method: Derives an auxiliary loss, evaluates CRF relaxations, and proposes a continuous sub-problem solver for soft self-labeling.

Result: Soft self-labeling enhances scribble-based training, surpassing specialized WSSS systems and sometimes full supervision.

Conclusion: The approach is effective for WSSS and applicable to other weakly supervised problems.

Abstract: We consider weakly supervised segmentation where only a fraction of pixels
have ground truth labels (scribbles) and focus on a self-labeling approach
optimizing relaxations of the standard unsupervised CRF/Potts loss on unlabeled
pixels. While WSSS methods can directly optimize such losses via gradient
descent, prior work suggests that higher-order optimization can improve network
training by introducing hidden pseudo-labels and powerful CRF sub-problem
solvers, e.g. graph cut. However, previously used hard pseudo-labels can not
represent class uncertainty or errors, which motivates soft self-labeling. We
derive a principled auxiliary loss and systematically evaluate standard and new
CRF relaxations (convex and non-convex), neighborhood systems, and terms
connecting network predictions with soft pseudo-labels. We also propose a
general continuous sub-problem solver. Using only standard architectures, soft
self-labeling consistently improves scribble-based training and outperforms
significantly more complex specialized WSSS systems. It can outperform full
pixel-precise supervision. Our general ideas apply to other weakly-supervised
problems/systems.

</details>


### [63] [When Does Pruning Benefit Vision Representations?](https://arxiv.org/abs/2507.01722)
*Enrico Cassano,Riccardo Renzulli,Andrea Bragagnolo,Marco Grangetto*

Main category: cs.CV

TL;DR: Pruning affects vision models' interpretability, object discovery, and human alignment, with sweet spots in sparsity levels depending on architecture and size.


<details>
  <summary>Details</summary>
Motivation: To understand pruning's impact on interpretability, unsupervised object discovery, and alignment with human perception in vision models.

Method: Analyzed vision network architectures under varying sparsity levels, evaluated feature attribution interpretability, and assessed alignment with human perception.

Result: Found sweet spots where sparse models improve interpretability, generalization, and human alignment, but these depend on architecture and size.

Conclusion: Pruning's benefits are complex and architecture-dependent, emphasizing the need for further investigation into its effects on vision representations.

Abstract: Pruning is widely used to reduce the complexity of deep learning models, but
its effects on interpretability and representation learning remain poorly
understood. This paper investigates how pruning influences vision models across
three key dimensions: (i) interpretability, (ii) unsupervised object discovery,
and (iii) alignment with human perception. We first analyze different vision
network architectures to examine how varying sparsity levels affect feature
attribution interpretability methods. Additionally, we explore whether pruning
promotes more succinct and structured representations, potentially improving
unsupervised object discovery by discarding redundant information while
preserving essential features. Finally, we assess whether pruning enhances the
alignment between model representations and human perception, investigating
whether sparser models focus on more discriminative features similarly to
humans. Our findings also reveal the presence of sweet spots, where sparse
models exhibit higher interpretability, downstream generalization and human
alignment. However, these spots highly depend on the network architectures and
their size in terms of trainable parameters. Our results suggest a complex
interplay between these three dimensions, highlighting the importance of
investigating when and how pruning benefits vision representations.

</details>


### [64] [HOI-Dyn: Learning Interaction Dynamics for Human-Object Motion Diffusion](https://arxiv.org/abs/2507.01737)
*Lin Wu,Zhixiang Chen,Jianglin Lan*

Main category: cs.CV

TL;DR: HOI-Dyn is a framework for generating realistic 3D human-object interactions by modeling them as a driver-responder system, using a transformer-based dynamics model and a residual-based loss for consistency.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to model detailed interaction dynamics, leading to implausible behaviors. HOI-Dyn aims to address this by treating interactions as a driver-responder system.

Method: HOI-Dyn uses a lightweight transformer-based interaction dynamics model to predict object responses to human motion, with a residual-based dynamics loss for consistency. The dynamics model is only used during training.

Result: The approach improves the quality of HOI generation and provides a feasible metric for evaluating interaction quality.

Conclusion: HOI-Dyn effectively enhances the realism and consistency of 3D human-object interactions, offering a practical solution for HOI generation.

Abstract: Generating realistic 3D human-object interactions (HOIs) remains a
challenging task due to the difficulty of modeling detailed interaction
dynamics. Existing methods treat human and object motions independently,
resulting in physically implausible and causally inconsistent behaviors. In
this work, we present HOI-Dyn, a novel framework that formulates HOI generation
as a driver-responder system, where human actions drive object responses. At
the core of our method is a lightweight transformer-based interaction dynamics
model that explicitly predicts how objects should react to human motion. To
further enforce consistency, we introduce a residual-based dynamics loss that
mitigates the impact of dynamics prediction errors and prevents misleading
optimization signals. The dynamics model is used only during training,
preserving inference efficiency. Through extensive qualitative and quantitative
experiments, we demonstrate that our approach not only enhances the quality of
HOI generation but also establishes a feasible metric for evaluating the
quality of generated interactions.

</details>


### [65] [DeRIS: Decoupling Perception and Cognition for Enhanced Referring Image Segmentation through Loopback Synergy](https://arxiv.org/abs/2507.01738)
*Ming Dai,Wenxuan Cheng,Jiang-jiang Liu,Sen Yang,Wenxiao Cai,Yanpeng Sun,Wankou Yang*

Main category: cs.CV

TL;DR: DeRIS decomposes Referring Image Segmentation (RIS) into perception and cognition, identifying cognitive bottlenecks and introducing a Loopback Synergy mechanism for improved performance.


<details>
  <summary>Details</summary>
Motivation: Prior RIS studies lack systematic analysis of bottlenecks. DeRIS aims to address this gap by modularizing RIS into perception and cognition.

Method: DeRIS decomposes RIS into perception and cognition, introduces Loopback Synergy for module synergy, and uses non-referent sample conversion for data augmentation.

Result: DeRIS identifies cognitive limitations as the main bottleneck and improves segmentation and comprehension. It adapts well to non- and multi-referent scenarios.

Conclusion: DeRIS offers a systematic analysis of RIS bottlenecks, enhances performance via modular decomposition and Loopback Synergy, and demonstrates broad applicability.

Abstract: Referring Image Segmentation (RIS) is a challenging task that aims to segment
objects in an image based on natural language expressions. While prior studies
have predominantly concentrated on improving vision-language interactions and
achieving fine-grained localization, a systematic analysis of the fundamental
bottlenecks in existing RIS frameworks remains underexplored. To bridge this
gap, we propose DeRIS, a novel framework that decomposes RIS into two key
components: perception and cognition. This modular decomposition facilitates a
systematic analysis of the primary bottlenecks impeding RIS performance. Our
findings reveal that the predominant limitation lies not in perceptual
deficiencies, but in the insufficient multi-modal cognitive capacity of current
models. To mitigate this, we propose a Loopback Synergy mechanism, which
enhances the synergy between the perception and cognition modules, thereby
enabling precise segmentation while simultaneously improving robust image-text
comprehension. Additionally, we analyze and introduce a simple non-referent
sample conversion data augmentation to address the long-tail distribution issue
related to target existence judgement in general scenarios. Notably, DeRIS
demonstrates inherent adaptability to both non- and multi-referents scenarios
without requiring specialized architectural modifications, enhancing its
general applicability. The codes and models are available at
https://github.com/Dmmm1997/DeRIS.

</details>


### [66] [Calibrated Self-supervised Vision Transformers Improve Intracranial Arterial Calcification Segmentation from Clinical CT Head Scans](https://arxiv.org/abs/2507.01744)
*Benjamin Jin,Grant Mair,Joanna M. Wardlaw,Maria del C. Valdés Hernández*

Main category: cs.CV

TL;DR: ViTs pre-trained with MAE outperform supervised nnU-Net in 3D medical image segmentation for IAC, showing improved robustness and clinical utility.


<details>
  <summary>Details</summary>
Motivation: ViTs are underutilized in 3D medical imaging despite their potential for self-supervised training. IAC quantification can aid in large-scale risk assessment for neurovascular diseases.

Method: Pre-train ViTs using MAE, fine-tune for IAC segmentation on heterogeneous clinical trial data (IST-3). Evaluate key aspects like patch size and upsampling methods.

Result: Self-supervised ViT beats nnU-Net by 3.2 Dice points, shows robustness to higher slice thicknesses, and improves risk classification by 46%.

Conclusion: MAE pre-trained ViTs are effective for IAC segmentation, offering clinical benefits and outperforming traditional methods.

Abstract: Vision Transformers (ViTs) have gained significant popularity in the natural
image domain but have been less successful in 3D medical image segmentation.
Nevertheless, 3D ViTs are particularly interesting for large medical imaging
volumes due to their efficient self-supervised training within the masked
autoencoder (MAE) framework, which enables the use of imaging data without the
need for expensive manual annotations. intracranial arterial calcification
(IAC) is an imaging biomarker visible on routinely acquired CT scans linked to
neurovascular diseases such as stroke and dementia, and automated IAC
quantification could enable their large-scale risk assessment. We pre-train
ViTs with MAE and fine-tune them for IAC segmentation for the first time. To
develop our models, we use highly heterogeneous data from a large clinical
trial, the third International Stroke Trial (IST-3). We evaluate key aspects of
MAE pre-trained ViTs in IAC segmentation, and analyse the clinical
implications. We show: 1) our calibrated self-supervised ViT beats a strong
supervised nnU-Net baseline by 3.2 Dice points, 2) low patch sizes are crucial
for ViTs for IAC segmentation and interpolation upsampling with regular
convolutions is preferable to transposed convolutions for ViT-based models, and
3) our ViTs increase robustness to higher slice thicknesses and improve risk
group classification in a clinical scenario by 46%. Our code is available
online.

</details>


### [67] [SSL4SAR: Self-Supervised Learning for Glacier Calving Front Extraction from SAR Imagery](https://arxiv.org/abs/2507.01747)
*Nora Gourmelon,Marcel Dreier,Martin Mayr,Thorsten Seehaus,Dakota Pyles,Matthias Braun,Andreas Maier,Vincent Christlein*

Main category: cs.CV

TL;DR: The paper introduces self-supervised pretraining techniques and a hybrid model for glacier calving front detection, improving accuracy over existing methods.


<details>
  <summary>Details</summary>
Motivation: Accurate monitoring of glacier ice loss is critical, but current deep learning models are suboptimal due to domain shifts in remote sensing imagery.

Method: Proposes two self-supervised pretraining techniques using a new dataset (SSL4SAR) and a hybrid Swin Transformer-CNN model.

Result: Achieves a mean distance error of 293 m on CaFFe dataset, outperforming prior models by 67 m, and 75 m in ensemble evaluation.

Conclusion: The approach enables precise seasonal monitoring of glacier calving fronts, nearing human performance.

Abstract: Glaciers are losing ice mass at unprecedented rates, increasing the need for
accurate, year-round monitoring to understand frontal ablation, particularly
the factors driving the calving process. Deep learning models can extract
calving front positions from Synthetic Aperture Radar imagery to track seasonal
ice losses at the calving fronts of marine- and lake-terminating glaciers. The
current state-of-the-art model relies on ImageNet-pretrained weights. However,
they are suboptimal due to the domain shift between the natural images in
ImageNet and the specialized characteristics of remote sensing imagery, in
particular for Synthetic Aperture Radar imagery. To address this challenge, we
propose two novel self-supervised multimodal pretraining techniques that
leverage SSL4SAR, a new unlabeled dataset comprising 9,563 Sentinel-1 and 14
Sentinel-2 images of Arctic glaciers, with one optical image per glacier in the
dataset. Additionally, we introduce a novel hybrid model architecture that
combines a Swin Transformer encoder with a residual Convolutional Neural
Network (CNN) decoder. When pretrained on SSL4SAR, this model achieves a mean
distance error of 293 m on the "CAlving Fronts and where to Find thEm" (CaFFe)
benchmark dataset, outperforming the prior best model by 67 m. Evaluating an
ensemble of the proposed model on a multi-annotator study of the benchmark
dataset reveals a mean distance error of 75 m, approaching the human
performance of 38 m. This advancement enables precise monitoring of seasonal
changes in glacier calving fronts.

</details>


### [68] [Rethinking Discrete Tokens: Treating Them as Conditions for Continuous Autoregressive Image Synthesis](https://arxiv.org/abs/2507.01756)
*Peng Zheng,Junke Wang,Yi Chang,Yizhou Yu,Rui Ma,Zuxuan Wu*

Main category: cs.CV

TL;DR: DisCon introduces a novel framework for visual generation by conditioning continuous tokens on discrete signals, avoiding quantization loss and outperforming AR models.


<details>
  <summary>Details</summary>
Motivation: Address the information loss in AR-based visual generation due to quantization and the challenges of continuous token modeling.

Method: DisCon models continuous representations conditioned on discrete tokens, avoiding direct quantization and simplifying density estimation.

Result: Achieves a gFID score of 1.38 on ImageNet 256×256 generation, surpassing state-of-the-art AR models.

Conclusion: DisCon effectively balances the trade-off between quantization loss and continuous token challenges, improving visual generation fidelity.

Abstract: Recent advances in large language models (LLMs) have spurred interests in
encoding images as discrete tokens and leveraging autoregressive (AR)
frameworks for visual generation. However, the quantization process in AR-based
visual generation models inherently introduces information loss that degrades
image fidelity. To mitigate this limitation, recent studies have explored to
autoregressively predict continuous tokens. Unlike discrete tokens that reside
in a structured and bounded space, continuous representations exist in an
unbounded, high-dimensional space, making density estimation more challenging
and increasing the risk of generating out-of-distribution artifacts. Based on
the above findings, this work introduces DisCon (Discrete-Conditioned
Continuous Autoregressive Model), a novel framework that reinterprets discrete
tokens as conditional signals rather than generation targets. By modeling the
conditional probability of continuous representations conditioned on discrete
tokens, DisCon circumvents the optimization challenges of continuous token
modeling while avoiding the information loss caused by quantization. DisCon
achieves a gFID score of 1.38 on ImageNet 256$\times$256 generation,
outperforming state-of-the-art autoregressive approaches by a clear margin.

</details>


### [69] [Are Vision Transformer Representations Semantically Meaningful? A Case Study in Medical Imaging](https://arxiv.org/abs/2507.01788)
*Montasir Shams,Chashi Mahiul Islam,Shaeke Salman,Phat Tran,Xiuwen Liu*

Main category: cs.CV

TL;DR: Vision transformers (ViTs) in medical imaging lack semantic meaningfulness in representations, making them vulnerable to small changes and unreliable for classification.


<details>
  <summary>Details</summary>
Motivation: To investigate whether ViT representations are semantically meaningful and robust for medical imaging tasks, given their growing use despite unclear interpretability.

Method: A projected gradient-based algorithm is used to analyze ViT representations for semantic meaningfulness and robustness.

Result: ViT representations are not semantically meaningful; small changes cause significant representation differences, reducing classification accuracy by over 60%.

Conclusion: ViTs' lack of semantic meaningfulness poses a critical challenge for their deployment in safety-critical medical imaging systems.

Abstract: Vision transformers (ViTs) have rapidly gained prominence in medical imaging
tasks such as disease classification, segmentation, and detection due to their
superior accuracy compared to conventional deep learning models. However, due
to their size and complex interactions via the self-attention mechanism, they
are not well understood. In particular, it is unclear whether the
representations produced by such models are semantically meaningful. In this
paper, using a projected gradient-based algorithm, we show that their
representations are not semantically meaningful and they are inherently
vulnerable to small changes. Images with imperceptible differences can have
very different representations; on the other hand, images that should belong to
different semantic classes can have nearly identical representations. Such
vulnerability can lead to unreliable classification results; for example,
unnoticeable changes cause the classification accuracy to be reduced by over
60\%. %. To the best of our knowledge, this is the first work to systematically
demonstrate this fundamental lack of semantic meaningfulness in ViT
representations for medical image classification, revealing a critical
challenge for their deployment in safety-critical systems.

</details>


### [70] [Boosting Adversarial Transferability Against Defenses via Multi-Scale Transformation](https://arxiv.org/abs/2507.01791)
*Zihong Guo,Chen Wan,Yayin Zheng,Hailing Kuang,Xiaohai Lu*

Main category: cs.CV

TL;DR: The paper introduces a Segmented Gaussian Pyramid (SGP) attack method to improve adversarial example transferability, especially against defense models, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Adversarial examples' transferability poses security risks for deep neural networks, even without direct knowledge of the target model.

Method: The SGP method uses Gaussian filtering and three downsampling types to create multi-scale examples, averaging gradients of the loss function across scales to determine perturbations.

Result: SGP significantly boosts attack success rates against black-box defense models by 2.3% to 32.6% compared to existing methods.

Conclusion: SGP is a highly extensible input transformation that enhances adversarial attack transferability and integrates easily with existing methods.

Abstract: The transferability of adversarial examples poses a significant security
challenge for deep neural networks, which can be attacked without knowing
anything about them. In this paper, we propose a new Segmented Gaussian Pyramid
(SGP) attack method to enhance the transferability, particularly against
defense models. Unlike existing methods that generally focus on single-scale
images, our approach employs Gaussian filtering and three types of downsampling
to construct a series of multi-scale examples. Then, the gradients of the loss
function with respect to each scale are computed, and their average is used to
determine the adversarial perturbations. The proposed SGP can be considered an
input transformation with high extensibility that is easily integrated into
most existing adversarial attacks. Extensive experiments demonstrate that in
contrast to the state-of-the-art methods, SGP significantly enhances attack
success rates against black-box defense models, with average attack success
rates increasing by 2.3% to 32.6%, based only on transferability.

</details>


### [71] [FreeLoRA: Enabling Training-Free LoRA Fusion for Autoregressive Multi-Subject Personalization](https://arxiv.org/abs/2507.01792)
*Peng Zheng,Ye Wang,Rui Ma,Zuxuan Wu*

Main category: cs.CV

TL;DR: FreeLoRA is a framework for multi-subject personalization in image generation without training, using subject-specific LoRA modules and Subject-Aware Inference.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with multi-subject personalization due to complex re-tuning or joint optimization.

Method: Uses Full Token Tuning for subject-specific LoRA modules and Subject-Aware Inference for fusion.

Result: Achieves strong performance in subject fidelity and prompt consistency.

Conclusion: FreeLoRA offers a simple, generalizable solution for multi-subject personalization without training.

Abstract: Subject-driven image generation plays a crucial role in applications such as
virtual try-on and poster design. Existing approaches typically fine-tune
pretrained generative models or apply LoRA-based adaptations for individual
subjects. However, these methods struggle with multi-subject personalization,
as combining independently adapted modules often requires complex re-tuning or
joint optimization. We present FreeLoRA, a simple and generalizable framework
that enables training-free fusion of subject-specific LoRA modules for
multi-subject personalization. Each LoRA module is adapted on a few images of a
specific subject using a Full Token Tuning strategy, where it is applied across
all tokens in the prompt to encourage weakly supervised token-content
alignment. At inference, we adopt Subject-Aware Inference, activating each
module only on its corresponding subject tokens. This enables training-free
fusion of multiple personalized subjects within a single image, while
mitigating overfitting and mutual interference between subjects. Extensive
experiments show that FreeLoRA achieves strong performance in both subject
fidelity and prompt consistency.

</details>


### [72] [HCNQA: Enhancing 3D VQA with Hierarchical Concentration Narrowing Supervision](https://arxiv.org/abs/2507.01800)
*Shengli Zhou,Jianuo Zhu,Qilin Huang,Fangjing Wang,Yanfu Zhang,Feng Zheng*

Main category: cs.CV

TL;DR: The paper introduces HCNQA, a 3D VQA model using hierarchical concentration narrowing supervision to ensure rational reasoning pathways, outperforming answer-centric methods.


<details>
  <summary>Details</summary>
Motivation: Address limitations of answer-centric supervision (superficial shortcuts) and underthinking in slow-thinking methods by guiding models through structured reasoning.

Method: Hierarchical concentration narrowing supervision mimics human focus progression, supervising key reasoning checkpoints.

Result: Demonstrates improved reasoning pathways and performance over answer-centric methods.

Conclusion: HCNQA ensures rational reasoning and better performance in 3D VQA tasks.

Abstract: 3D Visual Question-Answering (3D VQA) is pivotal for models to perceive the
physical world and perform spatial reasoning. Answer-centric supervision is a
commonly used training method for 3D VQA models. Many models that utilize this
strategy have achieved promising results in 3D VQA tasks. However, the
answer-centric approach only supervises the final output of models and allows
models to develop reasoning pathways freely. The absence of supervision on the
reasoning pathway enables the potential for developing superficial shortcuts
through common patterns in question-answer pairs. Moreover, although
slow-thinking methods advance large language models, they suffer from
underthinking. To address these issues, we propose \textbf{HCNQA}, a 3D VQA
model leveraging a hierarchical concentration narrowing supervision method. By
mimicking the human process of gradually focusing from a broad area to specific
objects while searching for answers, our method guides the model to perform
three phases of concentration narrowing through hierarchical supervision. By
supervising key checkpoints on a general reasoning pathway, our method can
ensure the development of a rational and effective reasoning pathway. Extensive
experimental results demonstrate that our method can effectively ensure that
the model develops a rational reasoning pathway and performs better. The code
is available at https://github.com/JianuoZhu/HCNQA.

</details>


### [73] [AMD: Adaptive Momentum and Decoupled Contrastive Learning Framework for Robust Long-Tail Trajectory Prediction](https://arxiv.org/abs/2507.01801)
*Bin Rao,Haicheng Liao,Yanchen Guan,Chengyue Wang,Bonan Wang,Jiaxun Zhang,Zhenning Li*

Main category: cs.CV

TL;DR: The paper proposes AMD, a framework combining adaptive momentum and decoupled contrastive learning to improve trajectory prediction, especially for rare and complex scenarios in autonomous driving.


<details>
  <summary>Details</summary>
Motivation: Existing methods overlook diversity and uncertainty in long-tail trajectory patterns, focusing only on prediction error. Addressing this gap is crucial for safer autonomous driving.

Method: AMD integrates unsupervised and supervised contrastive learning, using MoCo-DT and DCL modules. It includes trajectory augmentation and an online clustering strategy for dynamic pseudo-label updates.

Result: AMD outperforms others in long-tail trajectory prediction and overall accuracy on nuScenes and ETH/UCY datasets.

Conclusion: AMD effectively handles long-tail trajectory challenges, enhancing prediction accuracy and adaptability for complex scenarios.

Abstract: Accurately predicting the future trajectories of traffic agents is essential
in autonomous driving. However, due to the inherent imbalance in trajectory
distributions, tail data in natural datasets often represents more complex and
hazardous scenarios. Existing studies typically rely solely on a base model's
prediction error, without considering the diversity and uncertainty of
long-tail trajectory patterns. We propose an adaptive momentum and decoupled
contrastive learning framework (AMD), which integrates unsupervised and
supervised contrastive learning strategies. By leveraging an improved momentum
contrast learning (MoCo-DT) and decoupled contrastive learning (DCL) module,
our framework enhances the model's ability to recognize rare and complex
trajectories. Additionally, we design four types of trajectory random
augmentation methods and introduce an online iterative clustering strategy,
allowing the model to dynamically update pseudo-labels and better adapt to the
distributional shifts in long-tail data. We propose three different criteria to
define long-tail trajectories and conduct extensive comparative experiments on
the nuScenes and ETH$/$UCY datasets. The results show that AMD not only
achieves optimal performance in long-tail trajectory prediction but also
demonstrates outstanding overall prediction accuracy.

</details>


### [74] [Modulate and Reconstruct: Learning Hyperspectral Imaging from Misaligned Smartphone Views](https://arxiv.org/abs/2507.01835)
*Daniil Reutsky,Daniil Vladimirov,Yasin Mamedov,Georgy Perevozchikov,Nancy Mehta,Egor Ershov,Radu Timofte*

Main category: cs.CV

TL;DR: A novel multi-image-to-hyperspectral reconstruction (MI-HSR) framework using a triple-camera smartphone system improves accuracy by 30% over single-camera methods.


<details>
  <summary>Details</summary>
Motivation: Hyperspectral reconstruction (HSR) from RGB images is ill-posed due to spectral information loss. Existing single-image methods limit accuracy.

Method: Proposes a MI-HSR framework with a triple-camera smartphone system, two lenses with spectral filters, and introduces the Doomer dataset.

Result: Achieves 30% more accurate spectra estimation compared to conventional RGB cameras.

Conclusion: Multi-view spectral filtering with commodity hardware enables more accurate and practical hyperspectral imaging.

Abstract: Hyperspectral reconstruction (HSR) from RGB images is a fundamentally
ill-posed problem due to severe spectral information loss. Existing approaches
typically rely on a single RGB image, limiting reconstruction accuracy. In this
work, we propose a novel multi-image-to-hyperspectral reconstruction (MI-HSR)
framework that leverages a triple-camera smartphone system, where two lenses
are equipped with carefully selected spectral filters. Our configuration,
grounded in theoretical and empirical analysis, enables richer and more diverse
spectral observations than conventional single-camera setups. To support this
new paradigm, we introduce Doomer, the first dataset for MI-HSR, comprising
aligned images from three smartphone cameras and a hyperspectral reference
camera across diverse scenes. We show that the proposed HSR model achieves
consistent improvements over existing methods on the newly proposed benchmark.
In a nutshell, our setup allows 30% towards more accurately estimated spectra
compared to an ordinary RGB camera. Our findings suggest that multi-view
spectral filtering with commodity hardware can unlock more accurate and
practical hyperspectral imaging solutions.

</details>


### [75] [MobileIE: An Extremely Lightweight and Effective ConvNet for Real-Time Image Enhancement on Mobile Devices](https://arxiv.org/abs/2507.01838)
*Hailong Yan,Ao Li,Xiangtao Zhang,Zhe Liu,Zenglin Shi,Ce Zhu,Le Zhang*

Main category: cs.CV

TL;DR: A lightweight CNN framework with 4K parameters achieves real-time image enhancement at 1,100 FPS, balancing speed and quality.


<details>
  <summary>Details</summary>
Motivation: Deploying deep learning models on resource-constrained platforms like mobile devices is challenging due to high computation and memory demands.

Method: Integrates reparameterization, Incremental Weight Optimization, Feature Self-Transform module, Hierarchical Dual-Path Attention, and Local Variance-Weighted loss.

Result: Achieves real-time IE inference at 1,100 FPS with competitive image quality, offering the best speed-performance trade-off.

Conclusion: The proposed framework successfully addresses deployment challenges, enabling efficient real-time image enhancement on mobile devices.

Abstract: Recent advancements in deep neural networks have driven significant progress
in image enhancement (IE). However, deploying deep learning models on
resource-constrained platforms, such as mobile devices, remains challenging due
to high computation and memory demands. To address these challenges and
facilitate real-time IE on mobile, we introduce an extremely lightweight
Convolutional Neural Network (CNN) framework with around 4K parameters. Our
approach integrates reparameterization with an Incremental Weight Optimization
strategy to ensure efficiency. Additionally, we enhance performance with a
Feature Self-Transform module and a Hierarchical Dual-Path Attention mechanism,
optimized with a Local Variance-Weighted loss. With this efficient framework,
we are the first to achieve real-time IE inference at up to 1,100 frames per
second (FPS) while delivering competitive image quality, achieving the best
trade-off between speed and performance across multiple IE tasks. The code will
be available at https://github.com/AVC2-UESTC/MobileIE.git.

</details>


### [76] [Future Slot Prediction for Unsupervised Object Discovery in Surgical Video](https://arxiv.org/abs/2507.01882)
*Guiqiu Liao,Matjaz Jogan,Marcel Hussing,Edward Zhang,Eric Eaton,Daniel A. Hashimoto*

Main category: cs.CV

TL;DR: The paper introduces a dynamic temporal slot transformer (DTST) module to improve unsupervised object-centric learning in heterogeneous surgical videos, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Heterogeneous scenes in real-world applications like surgery are difficult to parse into meaningful object-centric representations (slots), limiting current methods' performance.

Method: Proposes a DTST module trained for temporal reasoning and predicting optimal future slot initialization.

Result: Achieves state-of-the-art performance on multiple surgical databases.

Conclusion: Demonstrates that unsupervised object-centric methods can be effectively applied to real-world healthcare data.

Abstract: Object-centric slot attention is an emerging paradigm for unsupervised
learning of structured, interpretable object-centric representations (slots).
This enables effective reasoning about objects and events at a low
computational cost and is thus applicable to critical healthcare applications,
such as real-time interpretation of surgical video. The heterogeneous scenes in
real-world applications like surgery are, however, difficult to parse into a
meaningful set of slots. Current approaches with an adaptive slot count perform
well on images, but their performance on surgical videos is low. To address
this challenge, we propose a dynamic temporal slot transformer (DTST) module
that is trained both for temporal reasoning and for predicting the optimal
future slot initialization. The model achieves state-of-the-art performance on
multiple surgical databases, demonstrating that unsupervised object-centric
methods can be applied to real-world data and become part of the common arsenal
in healthcare applications.

</details>


### [77] [Self-Reinforcing Prototype Evolution with Dual-Knowledge Cooperation for Semi-Supervised Lifelong Person Re-Identification](https://arxiv.org/abs/2507.01884)
*Kunlun Xu,Fan Zhuo,Jiangmeng Li,Xu Zou,Jiahuan Zhou*

Main category: cs.CV

TL;DR: The paper introduces SPRED, a framework for Semi-Supervised Lifelong Person Re-Identification (Semi-LReID), addressing performance degradation in unlabeled data scenarios through dynamic prototype-guided pseudo-label generation and dual-knowledge purification.


<details>
  <summary>Details</summary>
Motivation: Existing LReID methods struggle with noisy knowledge in unlabeled data, leading to poor long-term adaptation. The paper aims to improve Semi-LReID by enhancing unlabeled data utilization.

Method: SPRED combines dynamic identity prototypes for pseudo-label generation and a dual-knowledge cooperation scheme to refine noisy labels, creating a self-reinforcing cycle.

Result: SPRED achieves state-of-the-art performance on Semi-LReID benchmarks, demonstrating effective long-term learning.

Conclusion: The proposed SPRED framework successfully improves Semi-LReID by leveraging unlabeled data more effectively, ensuring reliable pseudo-labels and positive knowledge propagation.

Abstract: Current lifelong person re-identification (LReID) methods predominantly rely
on fully labeled data streams. However, in real-world scenarios where
annotation resources are limited, a vast amount of unlabeled data coexists with
scarce labeled samples, leading to the Semi-Supervised LReID (Semi-LReID)
problem where LReID methods suffer severe performance degradation. Existing
LReID methods, even when combined with semi-supervised strategies, suffer from
limited long-term adaptation performance due to struggling with the noisy
knowledge occurring during unlabeled data utilization. In this paper, we
pioneer the investigation of Semi-LReID, introducing a novel Self-Reinforcing
Prototype Evolution with Dual-Knowledge Cooperation framework (SPRED). Our key
innovation lies in establishing a self-reinforcing cycle between dynamic
prototype-guided pseudo-label generation and new-old knowledge collaborative
purification to enhance the utilization of unlabeled data. Specifically,
learnable identity prototypes are introduced to dynamically capture the
identity distributions and generate high-quality pseudo-labels. Then, the
dual-knowledge cooperation scheme integrates current model specialization and
historical model generalization, refining noisy pseudo-labels. Through this
cyclic design, reliable pseudo-labels are progressively mined to improve
current-stage learning and ensure positive knowledge propagation over long-term
learning. Experiments on the established Semi-LReID benchmarks show that our
SPRED achieves state-of-the-art performance. Our source code is available at
https://github.com/zhoujiahuan1991/ICCV2025-SPRED

</details>


### [78] [Reasoning to Edit: Hypothetical Instruction-Based Image Editing with Visual Reasoning](https://arxiv.org/abs/2507.01908)
*Qingdong He,Xueqin Chen,Chaoyi Wang,Yanjie Pan,Xiaobin Hu,Zhenye Gan,Yabiao Wang,Chengjie Wang,Xiangtai Li,Jiangning Zhang*

Main category: cs.CV

TL;DR: The paper introduces Reason50K, a dataset for hypothetical instruction reasoning in image editing, and ReasonBrain, a framework leveraging MLLMs and diffusion models to handle complex implicit instructions.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with complex implicit instructions and lack datasets for reasoning-aware editing. This work aims to address these gaps.

Method: Proposes ReasonBrain, using MLLMs for guidance and a diffusion model for synthesis, with FRCE for detail extraction and CME for cross-modal interaction.

Result: ReasonBrain outperforms baselines in reasoning scenarios and shows strong zero-shot generalization.

Conclusion: The work advances IIE by addressing reasoning challenges, with public release of dataset and code.

Abstract: Instruction-based image editing (IIE) has advanced rapidly with the success
of diffusion models. However, existing efforts primarily focus on simple and
explicit instructions to execute editing operations such as adding, deleting,
moving, or swapping objects. They struggle to handle more complex implicit
hypothetical instructions that require deeper reasoning to infer plausible
visual changes and user intent. Additionally, current datasets provide limited
support for training and evaluating reasoning-aware editing capabilities.
Architecturally, these methods also lack mechanisms for fine-grained detail
extraction that support such reasoning. To address these limitations, we
propose Reason50K, a large-scale dataset specifically curated for training and
evaluating hypothetical instruction reasoning image editing, along with
ReasonBrain, a novel framework designed to reason over and execute implicit
hypothetical instructions across diverse scenarios. Reason50K includes over 50K
samples spanning four key reasoning scenarios: Physical, Temporal, Causal, and
Story reasoning. ReasonBrain leverages Multimodal Large Language Models (MLLMs)
for editing guidance generation and a diffusion model for image synthesis,
incorporating a Fine-grained Reasoning Cue Extraction (FRCE) module to capture
detailed visual and textual semantics essential for supporting instruction
reasoning. To mitigate the semantic loss, we further introduce a Cross-Modal
Enhancer (CME) that enables rich interactions between the fine-grained cues and
MLLM-derived features. Extensive experiments demonstrate that ReasonBrain
consistently outperforms state-of-the-art baselines on reasoning scenarios
while exhibiting strong zero-shot generalization to conventional IIE tasks. Our
dataset and code will be released publicly.

</details>


### [79] [Modality Agnostic, patient-specific digital twins modeling temporally varying digestive motion](https://arxiv.org/abs/2507.01909)
*Jorge Tapias Gomez,Nishant Nadkarni,Lando S. Bosma,Jue Jiang,Ergys D. Subashi,William P. Segars,James M. Balter,Mert R Sabuncu,Neelam Tyagi,Harini Veeraraghavan*

Main category: cs.CV

TL;DR: The paper introduces a pipeline for creating patient-specific digital twins (DTs) to assess deformable image registration (DIR) accuracy in gastrointestinal (GI) organs, using realistic motion models and detailed metrics.


<details>
  <summary>Details</summary>
Motivation: Clinical DIR implementation lacks reliable voxel-based accuracy metrics for highly mobile GI organs, necessitating a robust validation method.

Method: A semi-automated pipeline generated 21 motion phases from static 3D scans using analytical GI motion models. Six DIR methods were evaluated using DTs and metrics like target registration error and Dice similarity.

Result: DTs accurately modeled GI motion, with motion amplitudes and Jacobian determinants matching real-patient data. The pipeline provided detailed DIR performance metrics and validated dose mapping accuracy.

Conclusion: The pipeline enables rigorous DIR testing for dynamic regions, offering granular spatial and dosimetric accuracy assessments.

Abstract: Objective: Clinical implementation of deformable image registration (DIR)
requires voxel-based spatial accuracy metrics such as manually identified
landmarks, which are challenging to implement for highly mobile
gastrointestinal (GI) organs. To address this, patient-specific digital twins
(DT) modeling temporally varying motion were created to assess the accuracy of
DIR methods. Approach: 21 motion phases simulating digestive GI motion as 4D
sequences were generated from static 3D patient scans using published
analytical GI motion models through a semi-automated pipeline. Eleven datasets,
including six T2w FSE MRI (T2w MRI), two T1w 4D golden-angle stack-of-stars,
and three contrast-enhanced CT scans. The motion amplitudes of the DTs were
assessed against real patient stomach motion amplitudes extracted from
independent 4D MRI datasets. The generated DTs were then used to assess six
different DIR methods using target registration error, Dice similarity
coefficient, and the 95th percentile Hausdorff distance using summary metrics
and voxel-level granular visualizations. Finally, for a subset of T2w MRI scans
from patients treated with MR-guided radiation therapy, dose distributions were
warped and accumulated to assess dose warping errors, including evaluations of
DIR performance in both low- and high-dose regions for patient-specific error
estimation. Main results: Our proposed pipeline synthesized DTs modeling
realistic GI motion, achieving mean and maximum motion amplitudes and a mean
log Jacobian determinant within 0.8 mm and 0.01, respectively, similar to
published real-patient gastric motion data. It also enables the extraction of
detailed quantitative DIR performance metrics and rigorous validation of dose
mapping accuracy. Significance: The pipeline enables rigorously testing DIR
tools for dynamic, anatomically complex regions enabling granular spatial and
dosimetric accuracies.

</details>


### [80] [3D Reconstruction and Information Fusion between Dormant and Canopy Seasons in Commercial Orchards Using Deep Learning and Fast GICP](https://arxiv.org/abs/2507.01912)
*Ranjan Sapkota,Zhichao Meng,Martin Churuvija,Xiaoqiang Du,Zenghong Ma,Manoj Karkee*

Main category: cs.CV

TL;DR: The paper introduces a multi-seasonal data fusion framework for orchard automation, combining dormant and canopy season data to improve robotic crop load management despite foliage occlusion.


<details>
  <summary>Details</summary>
Motivation: Dense foliage during the canopy season occludes tree structures, limiting machine vision. The dormant season offers better visibility, motivating the fusion of multi-seasonal data for year-round automation.

Method: The framework uses YOLOv9-Seg for segmentation, Kinect Fusion for 3D reconstruction, and Fast GICP for aligning models from both seasons.

Result: YOLOv9-Seg achieved high accuracy (MSE 0.0047, mAP@50 0.78). Kinect Fusion produced precise 3D models (RMSE 5.23 mm for trunks). Fast GICP enabled cross-seasonal alignment (fitness score 0.00197).

Conclusion: The fused structural model improves robotic access to tree architecture, enhancing precision in pruning and thinning operations.

Abstract: In orchard automation, dense foliage during the canopy season severely
occludes tree structures, minimizing visibility to various canopy parts such as
trunks and branches, which limits the ability of a machine vision system.
However, canopy structure is more open and visible during the dormant season
when trees are defoliated. In this work, we present an information fusion
framework that integrates multi-seasonal structural data to support robotic and
automated crop load management during the entire growing season. The framework
combines high-resolution RGB-D imagery from both dormant and canopy periods
using YOLOv9-Seg for instance segmentation, Kinect Fusion for 3D
reconstruction, and Fast Generalized Iterative Closest Point (Fast GICP) for
model alignment. Segmentation outputs from YOLOv9-Seg were used to extract
depth-informed masks, which enabled accurate 3D point cloud reconstruction via
Kinect Fusion; these reconstructed models from each season were subsequently
aligned using Fast GICP to achieve spatially coherent multi-season fusion. The
YOLOv9-Seg model, trained on manually annotated images, achieved a mean squared
error (MSE) of 0.0047 and segmentation mAP@50 scores up to 0.78 for trunks in
dormant season dataset. Kinect Fusion enabled accurate reconstruction of tree
geometry, validated with field measurements resulting in root mean square
errors (RMSE) of 5.23 mm for trunk diameter, 4.50 mm for branch diameter, and
13.72 mm for branch spacing. Fast GICP achieved precise cross-seasonal
registration with a minimum fitness score of 0.00197, allowing integrated,
comprehensive tree structure modeling despite heavy occlusions during the
growing season. This fused structural representation enables robotic systems to
access otherwise obscured architectural information, improving the precision of
pruning, thinning, and other automated orchard operations.

</details>


### [81] [IC-Custom: Diverse Image Customization via In-Context Learning](https://arxiv.org/abs/2507.01926)
*Yaowei Li,Xiaoyu Li,Zhaoyang Zhang,Yuxuan Bian,Gan Liu,Xinyuan Li,Jiale Xu,Wenbo Hu,Yating Liu,Lingen Li,Jing Cai,Yuexian Zou,Yancheng He,Ying Shan*

Main category: cs.CV

TL;DR: IC-Custom is a unified framework for image customization, integrating position-aware and position-free paradigms via in-context learning, outperforming existing methods with minimal parameter training.


<details>
  <summary>Details</summary>
Motivation: Current image customization methods lack a universal framework, limiting their versatility across scenarios.

Method: IC-Custom uses in-context learning, polyptych configurations, and the ICMA mechanism with learnable tokens and boundary-aware embeddings.

Result: IC-Custom achieves 73% higher human preference in identity consistency, harmonicity, and text alignment, training only 0.4% of original parameters.

Conclusion: IC-Custom is a versatile, efficient solution for diverse industrial image customization tasks.

Abstract: Image customization, a crucial technique for industrial media production,
aims to generate content that is consistent with reference images. However,
current approaches conventionally separate image customization into
position-aware and position-free customization paradigms and lack a universal
framework for diverse customization, limiting their applications across various
scenarios. To overcome these limitations, we propose IC-Custom, a unified
framework that seamlessly integrates position-aware and position-free image
customization through in-context learning. IC-Custom concatenates reference
images with target images to a polyptych, leveraging DiT's multi-modal
attention mechanism for fine-grained token-level interactions. We introduce the
In-context Multi-Modal Attention (ICMA) mechanism with learnable task-oriented
register tokens and boundary-aware positional embeddings to enable the model to
correctly handle different task types and distinguish various inputs in
polyptych configurations. To bridge the data gap, we carefully curated a
high-quality dataset of 12k identity-consistent samples with 8k from real-world
sources and 4k from high-quality synthetic data, avoiding the overly glossy and
over-saturated synthetic appearance. IC-Custom supports various industrial
applications, including try-on, accessory placement, furniture arrangement, and
creative IP customization. Extensive evaluations on our proposed ProductBench
and the publicly available DreamBench demonstrate that IC-Custom significantly
outperforms community workflows, closed-source models, and state-of-the-art
open-source approaches. IC-Custom achieves approximately 73% higher human
preference across identity consistency, harmonicity, and text alignment
metrics, while training only 0.4% of the original model parameters. Project
page: https://liyaowei-stu.github.io/project/IC_Custom

</details>


### [82] [evMLP: An Efficient Event-Driven MLP Architecture for Vision](https://arxiv.org/abs/2507.01927)
*Zhentan Zheng*

Main category: cs.CV

TL;DR: The paper introduces evMLP, a model using MLPs with an event-driven local update mechanism for efficient processing of sequential image data like videos, reducing computational costs while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: To improve computational efficiency in vision tasks, especially for sequential data, by avoiding redundant computations through selective processing of changed patches (events).

Method: Proposes evMLP, which processes patches via MLPs and selectively updates only patches where changes (events) occur between frames, reducing redundancy.

Result: evMLP achieves competitive accuracy on ImageNet and reduces computational costs on video datasets while maintaining performance consistency.

Conclusion: evMLP offers an efficient alternative for vision tasks, particularly for sequential data, by leveraging event-driven updates to minimize unnecessary computations.

Abstract: Deep neural networks have achieved remarkable results in computer vision
tasks. In the early days, Convolutional Neural Networks (CNNs) were the
mainstream architecture. In recent years, Vision Transformers (ViTs) have
become increasingly popular. In addition, exploring applications of multi-layer
perceptrons (MLPs) has provided new perspectives for research into vision model
architectures. In this paper, we present evMLP accompanied by a simple
event-driven local update mechanism. The proposed evMLP can independently
process patches on images or feature maps via MLPs. We define changes between
consecutive frames as "events". Under the event-driven local update mechanism,
evMLP selectively processes patches where events occur. For sequential image
data (e.g., video processing), this approach improves computational performance
by avoiding redundant computations. Through ImageNet image classification
experiments, evMLP attains accuracy competitive with state-of-the-art models.
More significantly, experimental results on multiple video datasets demonstrate
that evMLP reduces computational cost via its event-driven local update
mechanism while maintaining output consistency with its non-event-driven
baseline. The code and trained models are available at
https://github.com/i-evi/evMLP.

</details>


### [83] [CI-VID: A Coherent Interleaved Text-Video Dataset](https://arxiv.org/abs/2507.01938)
*Yiming Ju,Jijin Hu,Zhengxiong Luo,Haoge Deng,hanyu Zhao,Li Du,Chengwei Wu,Donglin Hao,Xinlong Wang,Tengfei Pan*

Main category: cs.CV

TL;DR: The paper introduces CI-VID, a dataset for coherent multi-scene video generation, addressing limitations of existing isolated text-video pair datasets.


<details>
  <summary>Details</summary>
Motivation: Existing datasets lack support for modeling coherent multi-clip video sequences, limiting text-to-video generation.

Method: CI-VID includes 340,000 samples with coherent video sequences and captions, enabling text-and-video-to-video generation. A multi-dimensional benchmark is designed for evaluation.

Result: Models trained on CI-VID show improved accuracy and content consistency in generating coherent video sequences.

Conclusion: CI-VID enhances story-driven video generation with smooth transitions and temporal coherence, proving its practical utility.

Abstract: Text-to-video (T2V) generation has recently attracted considerable attention,
resulting in the development of numerous high-quality datasets that have
propelled progress in this area. However, existing public datasets are
primarily composed of isolated text-video (T-V) pairs and thus fail to support
the modeling of coherent multi-clip video sequences. To address this
limitation, we introduce CI-VID, a dataset that moves beyond isolated
text-to-video (T2V) generation toward text-and-video-to-video (TV2V)
generation, enabling models to produce coherent, multi-scene video sequences.
CI-VID contains over 340,000 samples, each featuring a coherent sequence of
video clips with text captions that capture both the individual content of each
clip and the transitions between them, enabling visually and textually grounded
generation. To further validate the effectiveness of CI-VID, we design a
comprehensive, multi-dimensional benchmark incorporating human evaluation,
VLM-based assessment, and similarity-based metrics. Experimental results
demonstrate that models trained on CI-VID exhibit significant improvements in
both accuracy and content consistency when generating video sequences. This
facilitates the creation of story-driven content with smooth visual transitions
and strong temporal coherence, underscoring the quality and practical utility
of the CI-VID dataset We release the CI-VID dataset and the accompanying code
for data construction and evaluation at: https://github.com/ymju-BAAI/CI-VID

</details>


### [84] [LongAnimation: Long Animation Generation with Dynamic Global-Local Memory](https://arxiv.org/abs/2507.01945)
*Nan Chen,Mengqi Huang,Yihao Meng,Zhendong Mao*

Main category: cs.CV

TL;DR: The paper introduces LongAnimation, a framework for automated long animation colorization, addressing long-term color consistency with a dynamic global-local paradigm.


<details>
  <summary>Details</summary>
Motivation: High labor costs in long animation colorization and limitations of existing short-term methods neglecting global information.

Method: Proposes LongAnimation with SketchDiT, Dynamic Global-Local Memory (DGLM), and Color Consistency Reward for dynamic global-local feature fusion.

Result: Effective in maintaining short-term and long-term color consistency, validated on short (14 frames) and long (500 frames) animations.

Conclusion: LongAnimation achieves superior color consistency for open-domain animation colorization.

Abstract: Animation colorization is a crucial part of real animation industry
production. Long animation colorization has high labor costs. Therefore,
automated long animation colorization based on the video generation model has
significant research value. Existing studies are limited to short-term
colorization. These studies adopt a local paradigm, fusing overlapping features
to achieve smooth transitions between local segments. However, the local
paradigm neglects global information, failing to maintain long-term color
consistency. In this study, we argue that ideal long-term color consistency can
be achieved through a dynamic global-local paradigm, i.e., dynamically
extracting global color-consistent features relevant to the current generation.
Specifically, we propose LongAnimation, a novel framework, which mainly
includes a SketchDiT, a Dynamic Global-Local Memory (DGLM), and a Color
Consistency Reward. The SketchDiT captures hybrid reference features to support
the DGLM module. The DGLM module employs a long video understanding model to
dynamically compress global historical features and adaptively fuse them with
the current generation features. To refine the color consistency, we introduce
a Color Consistency Reward. During inference, we propose a color consistency
fusion to smooth the video segment transition. Extensive experiments on both
short-term (14 frames) and long-term (average 500 frames) animations show the
effectiveness of LongAnimation in maintaining short-term and long-term color
consistency for open-domain animation colorization task. The code can be found
at https://cn-makers.github.io/long_animation_web/.

</details>


### [85] [Kwai Keye-VL Technical Report](https://arxiv.org/abs/2507.01949)
*Kwai Keye Team,Biao Yang,Bin Wen,Changyi Liu,Chenglong Chu,Chengru Song,Chongling Rao,Chuan Yi,Da Li,Dunju Zang,Fan Yang,Guorui Zhou,Hao Peng,Haojie Ding,Jiaming Huang,Jiangxia Cao,Jiankang Chen,Jingyun Hua,Jin Ouyang,Kaibing Chen,Kaiyu Jiang,Kaiyu Tang,Kun Gai,Shengnan Zhang,Siyang Mao,Sui Huang,Tianke Zhang,Tingting Gao,Wei Chen,Wei Yuan,Xiangyu Wu,Xiao Hu,Xingyu Lu,Yang Zhou,Yi-Fan Zhang,Yiping Yang,Yulong Chen,Zhenhua Wu,Zhenyu Li,Zhixin Ling,Ziming Li,Dehua Ma,Di Xu,Haixuan Gao,Hang Li,Jiawei Guo,Jing Wang,Lejian Ren,Muhao Wei,Qianqian Wang,Qigen Hu,Shiyao Wang,Tao Yu,Xinchen Luo,Yan Li,Yiming Liang,Yuhang Hu,Zeyi Lu,Zhuoran Yang,Zixing Zhang*

Main category: cs.CV

TL;DR: Kwai Keye-VL is an 8B-parameter MLLM for short-video understanding, trained on a 600B-token dataset with a four-stage pre-training and two-phase post-training process, achieving SOTA results.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs struggle with dynamic, information-dense short videos, a key digital medium.

Method: Uses a massive dataset and innovative training: four-stage pre-training for vision-language alignment, two-phase post-training (foundational enhancement and advanced reasoning with a five-mode data mixture), plus RL and alignment.

Result: SOTA on video benchmarks, competitive on image tasks; excels in new KC-MMBench for short videos.

Conclusion: Keye-VL bridges the gap in video understanding while maintaining general vision-language capabilities, validated by benchmarks.

Abstract: While Multimodal Large Language Models (MLLMs) demonstrate remarkable
capabilities on static images, they often fall short in comprehending dynamic,
information-dense short-form videos, a dominant medium in today's digital
landscape. To bridge this gap, we introduce \textbf{Kwai Keye-VL}, an
8-billion-parameter multimodal foundation model engineered for leading-edge
performance in short-video understanding while maintaining robust
general-purpose vision-language abilities. The development of Keye-VL rests on
two core pillars: a massive, high-quality dataset exceeding 600 billion tokens
with a strong emphasis on video, and an innovative training recipe. This recipe
features a four-stage pre-training process for solid vision-language alignment,
followed by a meticulous two-phase post-training process. The first
post-training stage enhances foundational capabilities like instruction
following, while the second phase focuses on stimulating advanced reasoning. In
this second phase, a key innovation is our five-mode ``cold-start'' data
mixture, which includes ``thinking'', ``non-thinking'', ``auto-think'', ``think
with image'', and high-quality video data. This mixture teaches the model to
decide when and how to reason. Subsequent reinforcement learning (RL) and
alignment steps further enhance these reasoning capabilities and correct
abnormal model behaviors, such as repetitive outputs. To validate our approach,
we conduct extensive evaluations, showing that Keye-VL achieves
state-of-the-art results on public video benchmarks and remains highly
competitive on general image-based tasks (Figure 1). Furthermore, we develop
and release the \textbf{KC-MMBench}, a new benchmark tailored for real-world
short-video scenarios, where Keye-VL shows a significant advantage.

</details>


### [86] [FreeMorph: Tuning-Free Generalized Image Morphing with Diffusion Model](https://arxiv.org/abs/2507.01953)
*Yukang Cao,Chenyang Si,Jinghao Wang,Ziwei Liu*

Main category: cs.CV

TL;DR: FreeMorph is a tuning-free image morphing method that handles inputs with different semantics/layouts, outperforming existing methods in speed and quality.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on finetuning, are time-consuming, and struggle with semantic/layout discrepancies. FreeMorph aims to eliminate per-instance training while maintaining high fidelity.

Method: FreeMorph integrates guidance-aware spherical interpolation and step-oriented variation trends to modify self-attention modules, ensuring controlled transitions and identity preservation.

Result: FreeMorph is 10x ~ 50x faster than existing methods and achieves state-of-the-art performance in image morphing.

Conclusion: FreeMorph successfully addresses challenges in tuning-free image morphing, offering high-quality, efficient results without per-instance training.

Abstract: We present FreeMorph, the first tuning-free method for image morphing that
accommodates inputs with different semantics or layouts. Unlike existing
methods that rely on finetuning pre-trained diffusion models and are limited by
time constraints and semantic/layout discrepancies, FreeMorph delivers
high-fidelity image morphing without requiring per-instance training. Despite
their efficiency and potential, tuning-free methods face challenges in
maintaining high-quality results due to the non-linear nature of the multi-step
denoising process and biases inherited from the pre-trained diffusion model. In
this paper, we introduce FreeMorph to address these challenges by integrating
two key innovations. 1) We first propose a guidance-aware spherical
interpolation design that incorporates explicit guidance from the input images
by modifying the self-attention modules, thereby addressing identity loss and
ensuring directional transitions throughout the generated sequence. 2) We
further introduce a step-oriented variation trend that blends self-attention
modules derived from each input image to achieve controlled and consistent
transitions that respect both inputs. Our extensive evaluations demonstrate
that FreeMorph outperforms existing methods, being 10x ~ 50x faster and
establishing a new state-of-the-art for image morphing.

</details>


### [87] [How Well Does GPT-4o Understand Vision? Evaluating Multimodal Foundation Models on Standard Computer Vision Tasks](https://arxiv.org/abs/2507.01955)
*Rahul Ramachandran,Ali Garjani,Roman Bachmann,Andrei Atanov,Oğuzhan Fatih Kar,Amir Zamir*

Main category: cs.CV

TL;DR: The paper benchmarks multimodal foundation models (e.g., GPT-4o, Gemini, Claude) on standard computer vision tasks, revealing their strengths as generalists but limitations compared to specialist models, with GPT-4o leading in non-reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: To assess the vision understanding capabilities of multimodal foundation models, which are primarily trained on image-text tasks, and address challenges like task translation and API limitations.

Method: Translate standard vision tasks into text-promptable and API-compatible tasks using prompt chaining to create a standardized benchmarking framework.

Result: Models are respectable generalists but lag behind specialist models. GPT-4o leads in non-reasoning tasks, while reasoning models improve in geometric tasks. Prompt sensitivity varies, and image-generation models exhibit quirks.

Conclusion: Multimodal foundation models show promise as generalists but require further refinement for vision tasks, with GPT-4o emerging as a top performer among non-reasoning models.

Abstract: Multimodal foundation models, such as GPT-4o, have recently made remarkable
progress, but it is not clear where exactly these models stand in terms of
understanding vision. In this paper, we benchmark the performance of popular
multimodal foundation models (GPT-4o, o4-mini, Gemini 1.5 Pro and Gemini 2.0
Flash, Claude 3.5 Sonnet, Qwen2-VL, Llama 3.2) on standard computer vision
tasks (semantic segmentation, object detection, image classification, depth and
surface normal prediction) using established datasets (e.g., COCO, ImageNet and
its variants, etc).
  The main challenges to performing this are: 1) most models are trained to
output text and cannot natively express versatile domains, such as segments or
3D geometry, and 2) many leading models are proprietary and accessible only at
an API level, i.e., there is no weight access to adapt them. We address these
challenges by translating standard vision tasks into equivalent text-promptable
and API-compatible tasks via prompt chaining to create a standardized
benchmarking framework.
  We observe that 1) the models are not close to the state-of-the-art
specialist models at any task. However, 2) they are respectable generalists;
this is remarkable as they are presumably trained on primarily image-text-based
tasks. 3) They perform semantic tasks notably better than geometric ones. 4)
While the prompt-chaining techniques affect performance, better models exhibit
less sensitivity to prompt variations. 5) GPT-4o performs the best among
non-reasoning models, securing the top position in 4 out of 6 tasks, 6)
reasoning models, e.g. o3, show improvements in geometric tasks, and 7) a
preliminary analysis of models with native image generation, like the latest
GPT-4o, shows they exhibit quirks like hallucinations and spatial
misalignments.

</details>


### [88] [Locality-aware Parallel Decoding for Efficient Autoregressive Image Generation](https://arxiv.org/abs/2507.01957)
*Zhuoyang Zhang,Luke J. Huang,Chengyue Wu,Shang Yang,Kelly Peng,Yao Lu,Song Han*

Main category: cs.CV

TL;DR: Locality-aware Parallel Decoding (LPD) accelerates autoregressive image generation by enabling flexible parallelization and locality-aware ordering, reducing steps significantly without quality loss.


<details>
  <summary>Details</summary>
Motivation: Traditional autoregressive image generation is memory-bound and slow. Existing parallelization methods achieve limited speedup.

Method: Introduces Flexible Parallelized Autoregressive Modeling (learnable position queries) and Locality-aware Generation Ordering (minimizes dependencies).

Result: Reduces generation steps from 256 to 20 (256x256) and 1024 to 48 (512x512), with 3.4x lower latency than prior methods.

Conclusion: LPD achieves high parallelization and quality in autoregressive image generation, significantly reducing latency.

Abstract: We present Locality-aware Parallel Decoding (LPD) to accelerate
autoregressive image generation. Traditional autoregressive image generation
relies on next-patch prediction, a memory-bound process that leads to high
latency. Existing works have tried to parallelize next-patch prediction by
shifting to multi-patch prediction to accelerate the process, but only achieved
limited parallelization. To achieve high parallelization while maintaining
generation quality, we introduce two key techniques: (1) Flexible Parallelized
Autoregressive Modeling, a novel architecture that enables arbitrary generation
ordering and degrees of parallelization. It uses learnable position query
tokens to guide generation at target positions while ensuring mutual visibility
among concurrently generated tokens for consistent parallel decoding. (2)
Locality-aware Generation Ordering, a novel schedule that forms groups to
minimize intra-group dependencies and maximize contextual support, enhancing
generation quality. With these designs, we reduce the generation steps from 256
to 20 (256$\times$256 res.) and 1024 to 48 (512$\times$512 res.) without
compromising quality on the ImageNet class-conditional generation, and
achieving at least 3.4$\times$ lower latency than previous parallelized
autoregressive models.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [89] [Rethinking the Illusion of Thinking](https://arxiv.org/abs/2507.01231)
*Iñaki Dellibarda Varela,Pablo Romero-Sorozabal,Eduardo Rocon,Manuel Cebrian*

Main category: cs.AI

TL;DR: The paper clarifies debates about LRMs' reasoning by refining benchmarks, showing their limitations and capabilities depend on problem complexity and solvability.


<details>
  <summary>Details</summary>
Motivation: To address the controversy sparked by Apple's paper, which claimed LRMs lack genuine reasoning, by replicating and refining key benchmarks.

Method: Replicated and refined Towers of Hanoi and River Crossing benchmarks using incremental stepwise prompting and agentic collaborative dialogue.

Result: LRMs struggle with moderate complexity (e.g., 8 disks in Towers of Hanoi) but solve large solvable problems (e.g., 100+ agent pairs in River Crossing).

Conclusion: LRMs are complex, stochastic searchers; progress in reasoning requires fine-grained analysis of their capabilities and limitations.

Abstract: Earlier this year, Apple ignited controversy by publishing "The Illusion of
Thinking," prompting heated debate within the AI community. Critics seized upon
the findings as conclusive evidence that Large Reasoning Models (LRMs) lack
genuine reasoning capabilities, branding them as mere stochastic parrots.
Meanwhile, defenders-spearheaded by Lawsen et al. (2025)-fired back, condemning
the experimental setup as flawed and the conclusions overstated. We clarify
this debate by replicating and refining two of the original study's most
contentious benchmarks: Towers of Hanoi and River Crossing. By introducing
incremental stepwise prompting and agentic collaborative dialogue, we show that
previously reported failures solving the Towers of Hanoi were not purely result
of output constraints, but also partly a result of cognition limitations: LRMs
still stumble when complexity rises moderately (around 8 disks). Moreover, the
River Crossing results initially heralded as catastrophic failures turn out to
hinge upon testing unsolvable configurations. Once we limit tests strictly to
solvable problems-LRMs effortlessly solve large instances involving over 100
agent pairs. Our findings ultimately defy simplistic narratives: today's LRMs
are stochastic, RL-tuned searchers in a discrete state space we barely
understand. Real progress in symbolic, long-horizon reasoning demands mapping
that terrain through fine-grained ablations like those introduced here.

</details>


### [90] [Beyond Black-Box AI: Interpretable Hybrid Systems for Dementia Care](https://arxiv.org/abs/2507.01282)
*Matthew JY Kang,Wenli Yang,Monica R Roberts,Byeong Ho Kang,Charles B Malpas*

Main category: cs.AI

TL;DR: The paper reviews limitations of AI, especially LLMs, in clinical settings like dementia diagnosis, advocating for hybrid approaches combining statistical learning with expert knowledge for better interpretability and workflow integration.


<details>
  <summary>Details</summary>
Motivation: To address the gap between AI's benchmark performance and its practical clinical utility, focusing on dementia diagnosis and care.

Method: Scoping review highlighting AI's limitations, such as lack of transparency and weak causal reasoning, and proposing hybrid neuro-symbolic approaches.

Result: Current AI models lack interpretability and clinician trust; hybrid methods like PEIRS and ATHENA-CDS show promise by integrating expert knowledge.

Conclusion: Future AI should prioritize explanatory coherence, clinician understanding, and workflow fit, moving beyond accuracy to improve patient outcomes.

Abstract: The recent boom of large language models (LLMs) has re-ignited the hope that
artificial intelligence (AI) systems could aid medical diagnosis. Yet despite
dazzling benchmark scores, LLM assistants have yet to deliver measurable
improvements at the bedside. This scoping review aims to highlight the areas
where AI is limited to make practical contributions in the clinical setting,
specifically in dementia diagnosis and care.
  Standalone machine-learning models excel at pattern recognition but seldom
provide actionable, interpretable guidance, eroding clinician trust. Adjacent
use of LLMs by physicians did not result in better diagnostic accuracy or
speed. Key limitations trace to the data-driven paradigm: black-box outputs
which lack transparency, vulnerability to hallucinations, and weak causal
reasoning. Hybrid approaches that combine statistical learning with expert
rule-based knowledge, and involve clinicians throughout the process help bring
back interpretability. They also fit better with existing clinical workflows,
as seen in examples like PEIRS and ATHENA-CDS.
  Future decision-support should prioritise explanatory coherence by linking
predictions to clinically meaningful causes. This can be done through
neuro-symbolic or hybrid AI that combines the language ability of LLMs with
human causal expertise. AI researchers have addressed this direction, with
explainable AI and neuro-symbolic AI being the next logical steps in further
advancement in AI. However, they are still based on data-driven knowledge
integration instead of human-in-the-loop approaches. Future research should
measure success not only by accuracy but by improvements in clinician
understanding, workflow fit, and patient outcomes. A better understanding of
what helps improve human-computer interactions is greatly needed for AI systems
to become part of clinical practice.

</details>


### [91] [AI Agents and Agentic AI-Navigating a Plethora of Concepts for Future Manufacturing](https://arxiv.org/abs/2507.01376)
*Yinwang Ren,Yangyang Liu,Tang Ji,Xun Xu*

Main category: cs.AI

TL;DR: The paper reviews AI agent technologies, focusing on LLM-Agents, MLLM-Agents, and Agentic AI, their advancements, and potential applications in smart manufacturing, while addressing gaps in definitions and challenges.


<details>
  <summary>Details</summary>
Motivation: To clarify the definitions, capabilities, and practical applications of emerging AI paradigms (LLM-Agents, MLLM-Agents, Agentic AI) in smart manufacturing, as current understanding is unclear.

Method: Systematic review of AI and AI agent evolution, examination of core concepts and technological advancements, and exploration of potential applications and challenges in manufacturing.

Result: Identifies advancements in AI agents' semantic comprehension, reasoning, and decision-making, and their potential to transform smart manufacturing.

Conclusion: The study highlights the promise of AI agents in manufacturing but underscores the need for clearer definitions and addressing practical challenges.

Abstract: AI agents are autonomous systems designed to perceive, reason, and act within
dynamic environments. With the rapid advancements in generative AI (GenAI),
large language models (LLMs) and multimodal large language models (MLLMs) have
significantly improved AI agents' capabilities in semantic comprehension,
complex reasoning, and autonomous decision-making. At the same time, the rise
of Agentic AI highlights adaptability and goal-directed autonomy in dynamic and
complex environments. LLMs-based AI Agents (LLM-Agents), MLLMs-based AI Agents
(MLLM-Agents), and Agentic AI contribute to expanding AI's capabilities in
information processing, environmental perception, and autonomous
decision-making, opening new avenues for smart manufacturing. However, the
definitions, capability boundaries, and practical applications of these
emerging AI paradigms in smart manufacturing remain unclear. To address this
gap, this study systematically reviews the evolution of AI and AI agent
technologies, examines the core concepts and technological advancements of
LLM-Agents, MLLM-Agents, and Agentic AI, and explores their potential
applications in and integration into manufacturing, along with the potential
challenges they may face.

</details>


### [92] [A Fuzzy Approach to the Specification, Verification and Validation of Risk-Based Ethical Decision Making Models](https://arxiv.org/abs/2507.01410)
*Abeer Dyoub,Francesca A. Lisi*

Main category: cs.AI

TL;DR: A formal method for Ethical Decision Making models using fuzzy rules and fuzzy Petri nets, demonstrated with a medical case study.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of evaluating moral machines due to ontological and epistemic complexities in ethics.

Method: Develops Ethical Decision Making models via ethical risk assessment, specified as fuzzy rules, and validates them using fuzzy Petri nets.

Result: Demonstrates the approach with a medical case study, showing practical applicability.

Conclusion: Proposes a viable method for verifying and validating ethical models in complex domains like medicine.

Abstract: The ontological and epistemic complexities inherent in the moral domain make
it challenging to establish clear standards for evaluating the performance of a
moral machine. In this paper, we present a formal method to describe Ethical
Decision Making models based on ethical risk assessment. Then, we show how
these models that are specified as fuzzy rules can be verified and validated
using fuzzy Petri nets. A case study from the medical field is considered to
illustrate the proposed approach.

</details>


### [93] [Pensieve Grader: An AI-Powered, Ready-to-Use Platform for Effortless Handwritten STEM Grading](https://arxiv.org/abs/2507.01431)
*Yoonseok Yang,Minjune Kim,Marlon Rondinelli,Keren Shao*

Main category: cs.AI

TL;DR: Pensieve is an AI-assisted grading platform using LLMs to transcribe and evaluate handwritten STEM responses, reducing grading time by 65% with high accuracy.


<details>
  <summary>Details</summary>
Motivation: Handwritten grading is slow in large STEM courses; Pensieve aims to streamline the process.

Method: Leverages LLMs for transcription and evaluation, integrating a human-in-the-loop interface for the entire grading pipeline.

Result: Deployed in 20+ institutions, graded 300,000+ responses, achieving 95.4% agreement with instructors and 65% time reduction.

Conclusion: Pensieve effectively automates grading while maintaining accuracy, benefiting large STEM courses.

Abstract: Grading handwritten, open-ended responses remains a major bottleneck in large
university STEM courses. We introduce Pensieve (https://www.pensieve.co), an
AI-assisted grading platform that leverages large language models (LLMs) to
transcribe and evaluate student work, providing instructors with rubric-aligned
scores, transcriptions, and confidence ratings. Unlike prior tools that focus
narrowly on specific tasks like transcription or rubric generation, Pensieve
supports the entire grading pipeline-from scanned student submissions to final
feedback-within a human-in-the-loop interface.
  Pensieve has been deployed in real-world courses at over 20 institutions and
has graded more than 300,000 student responses. We present system details and
empirical results across four core STEM disciplines: Computer Science,
Mathematics, Physics, and Chemistry. Our findings show that Pensieve reduces
grading time by an average of 65%, while maintaining a 95.4% agreement rate
with instructor-assigned grades for high-confidence predictions.

</details>


### [94] [Using multi-agent architecture to mitigate the risk of LLM hallucinations](https://arxiv.org/abs/2507.01446)
*Abd Elrahman Amer,Magdi Amer*

Main category: cs.AI

TL;DR: A multi-agent system combining LLMs and fuzzy logic is proposed to improve customer service quality and response time while mitigating hallucination risks.


<details>
  <summary>Details</summary>
Motivation: Enhancing customer service quality and response time is crucial for loyalty and market share, but LLMs pose hallucination risks.

Method: A multi-agent system integrating LLM-based agents with fuzzy logic to handle customer requests via SMS.

Result: The system aims to improve service efficiency and reduce hallucination risks.

Conclusion: The proposed multi-agent system offers a viable solution for leveraging LLMs in customer service while addressing hallucination challenges.

Abstract: Improving customer service quality and response time are critical factors for
maintaining customer loyalty and increasing a company's market share. While
adopting emerging technologies such as Large Language Models (LLMs) is becoming
a necessity to achieve these goals, the risk of hallucination remains a major
challenge. In this paper, we present a multi-agent system to handle customer
requests sent via SMS. This system integrates LLM based agents with fuzzy logic
to mitigate hallucination risks.

</details>


### [95] [Agent-as-Tool: A Study on the Hierarchical Decision Making with Reinforcement Learning](https://arxiv.org/abs/2507.01489)
*Yanfei Zhang*

Main category: cs.AI

TL;DR: The paper introduces Agent-as-tool, a hierarchical framework that separates tool calling and reasoning processes in LLM-based agents, improving performance with minimal fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based agents struggle with simultaneous tool calling and reasoning, leading to inefficiencies due to unprocessed raw results.

Method: Proposes Agent-as-tool, a hierarchical framework where tool calling is handled by a separate agent, allowing the main model to focus on reasoning.

Result: Achieved 63.2% exact match and 75.2% cover exact match in Bamboogle, outperforming Search-R1 by 4.8% and 3.2% respectively.

Conclusion: The Agent-as-tool framework effectively decouples tool calling and reasoning, enhancing performance with minimal reinforcement fine-tuning.

Abstract: Large Language Models (LLMs) have emerged as one of the most significant
technological advancements in artificial intelligence in recent years. Their
ability to understand, generate, and reason with natural language has
transformed how we interact with AI systems. With the development of LLM-based
agents and reinforcement-learning-based reasoning models, the study of applying
reinforcement learning in agent frameworks has become a new research focus.
However, all previous studies face the challenge of deciding the tool calling
process and the reasoning process simultaneously, and the chain of reasoning
was solely relied on the unprocessed raw result with redundant information and
symbols unrelated to the task from the tool, which impose a heavy burden on the
model's capability to reason. Therefore, in our research, we proposed a
hierarchical framework Agent-as-tool that detach the tool calling process and
the reasoning process, which enables the model to focus on the verbally
reasoning process while the tool calling process is handled by another agent.
Our work had achieved comparable results with only a slight reinforcement
fine-tuning on 180 samples, and had achieved exceptionally well performance in
Bamboogle with 63.2% of exact match and 75.2% in cover exact match, exceeding
Search-R1 by 4.8% in exact match and 3.2% in cover exact match.

</details>


### [96] [T3DM: Test-Time Training-Guided Distribution Shift Modelling for Temporal Knowledge Graph Reasoning](https://arxiv.org/abs/2507.01597)
*Yuehang Si,Zefan Zeng,Jincai Huang,Qing Cheng*

Main category: cs.AI

TL;DR: T3DM improves TKG reasoning by addressing distribution shift and enhancing negative sampling, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing TKG reasoning methods struggle with event distribution shifts and low-quality negative samples.

Method: Proposes T3DM for distribution shift modeling and adversarial-based negative sampling.

Result: T3DM achieves superior and more robust performance compared to state-of-the-art baselines.

Conclusion: T3DM effectively addresses key challenges in TKG reasoning, offering improved accuracy and robustness.

Abstract: Temporal Knowledge Graph (TKG) is an efficient method for describing the
dynamic development of facts along a timeline. Most research on TKG reasoning
(TKGR) focuses on modelling the repetition of global facts and designing
patterns of local historical facts. However, they face two significant
challenges: inadequate modeling of the event distribution shift between
training and test samples, and reliance on random entity substitution for
generating negative samples, which often results in low-quality sampling. To
this end, we propose a novel distributional feature modeling approach for
training TKGR models, Test-Time Training-guided Distribution shift Modelling
(T3DM), to adjust the model based on distribution shift and ensure the global
consistency of model reasoning. In addition, we design a negative-sampling
strategy to generate higher-quality negative quadruples based on adversarial
training. Extensive experiments show that T3DM provides better and more robust
results than the state-of-the-art baselines in most cases.

</details>


### [97] [Agent Ideate: A Framework for Product Idea Generation from Patents Using Agentic AI](https://arxiv.org/abs/2507.01717)
*Gopichand Kanumolu,Ashok Urlana,Charaka Vinayak Kumar,Bala Mallikarjunarao Garlapati*

Main category: cs.AI

TL;DR: Agent Ideate, a framework using LLMs and autonomous agents, generates business ideas from patents, outperforming standalone LLMs in quality, relevance, and novelty.


<details>
  <summary>Details</summary>
Motivation: Patents hold valuable technical knowledge but are hard to access and interpret. This work aims to leverage LLMs and agents to unlock innovative product ideas from patents.

Method: The study designs Agent Ideate, a framework combining LLMs and agent-based architectures, tested in Computer Science, NLP, and Material Chemistry domains.

Result: Agentic approaches consistently outperformed standalone LLMs in generating higher-quality, more relevant, and novel business ideas from patents.

Conclusion: Combining LLMs with agentic workflows enhances innovation by effectively generating business ideas from patent data.

Abstract: Patents contain rich technical knowledge that can inspire innovative product
ideas, yet accessing and interpreting this information remains a challenge.
This work explores the use of Large Language Models (LLMs) and autonomous
agents to mine and generate product concepts from a given patent. In this work,
we design Agent Ideate, a framework for automatically generating product-based
business ideas from patents. We experimented with open-source LLMs and
agent-based architectures across three domains: Computer Science, Natural
Language Processing, and Material Chemistry. Evaluation results show that the
agentic approach consistently outperformed standalone LLMs in terms of idea
quality, relevance, and novelty. These findings suggest that combining LLMs
with agentic workflows can significantly enhance the innovation pipeline by
unlocking the untapped potential of business idea generation from patent data.

</details>


### [98] [Joint Matching and Pricing for Crowd-shipping with In-store Customers](https://arxiv.org/abs/2507.01749)
*Arash Dehghan,Mucahit Cevik,Merve Bodur,Bissan Ghaddar*

Main category: cs.AI

TL;DR: The paper proposes a crowd-shipping system using in-store customers as couriers, optimized via MDP, NeurADP, and DDQN for cost-efficient last-mile delivery.


<details>
  <summary>Details</summary>
Motivation: Addressing the need for efficient last-mile delivery in urban areas by leveraging existing shoppers as delivery couriers.

Method: Uses a Markov Decision Process (MDP) model with NeurADP for adaptive assignment and DDQN for dynamic pricing.

Result: Achieves 6.7% cost savings over NeurADP with fixed pricing and 18% over baselines; flexible delays and multi-destination routing further reduce costs.

Conclusion: Dynamic, forward-looking policies enhance crowd-shipping efficiency, offering practical benefits for urban logistics.

Abstract: This paper examines the use of in-store customers as delivery couriers in a
centralized crowd-shipping system, targeting the growing need for efficient
last-mile delivery in urban areas. We consider a brick-and-mortar retail
setting where shoppers are offered compensation to deliver time-sensitive
online orders. To manage this process, we propose a Markov Decision Process
(MDP) model that captures key uncertainties, including the stochastic arrival
of orders and crowd-shippers, and the probabilistic acceptance of delivery
offers. Our solution approach integrates Neural Approximate Dynamic Programming
(NeurADP) for adaptive order-to-shopper assignment with a Deep Double Q-Network
(DDQN) for dynamic pricing. This joint optimization strategy enables multi-drop
routing and accounts for offer acceptance uncertainty, aligning more closely
with real-world operations. Experimental results demonstrate that the
integrated NeurADP + DDQN policy achieves notable improvements in delivery cost
efficiency, with up to 6.7\% savings over NeurADP with fixed pricing and
approximately 18\% over myopic baselines. We also show that allowing flexible
delivery delays and enabling multi-destination routing further reduces
operational costs by 8\% and 17\%, respectively. These findings underscore the
advantages of dynamic, forward-looking policies in crowd-shipping systems and
offer practical guidance for urban logistics operators.

</details>


### [99] [Refining Gelfond Rationality Principle Towards More Comprehensive Foundational Principles for Answer Set Semantics](https://arxiv.org/abs/2507.01833)
*Yi-Dong Shen,Thomas Eiter*

Main category: cs.AI

TL;DR: The paper questions mandatory conditions for answer set semantics in non-monotonic logic programming, refines Gelfond's principles, and proposes new semantics based on well-supportedness and minimality.


<details>
  <summary>Details</summary>
Motivation: To evaluate whether existing conditions (minimal model property, constraint monotonicity, foundedness) are too restrictive and to refine general principles for answer set semantics.

Method: Refines Gelfond's rationality principle into well-supportedness and minimality, extends these to answer sets and world views, and defines new semantics.

Result: Proposes refined principles and new semantics, demonstrating their applicability and assessing existing semantics against them.

Conclusion: The refined principles offer a more flexible and intuitive framework for answer set semantics, addressing limitations of traditional conditions.

Abstract: Non-monotonic logic programming is the basis for a declarative problem
solving paradigm known as answer set programming (ASP). Departing from the
seminal definition by Gelfond and Lifschitz in 1988 for simple normal logic
programs, various answer set semantics have been proposed for extensions. We
consider two important questions: (1) Should the minimal model property,
constraint monotonicity and foundedness as defined in the literature be
mandatory conditions for an answer set semantics in general? (2) If not, what
other properties could be considered as general principles for answer set
semantics? We address the two questions. First, it seems that the three
aforementioned conditions may sometimes be too strong, and we illustrate with
examples that enforcing them may exclude expected answer sets. Second, we
evolve the Gelfond answer set (GAS) principles for answer set construction by
refining the Gelfond's rationality principle to well-supportedness, minimality
w.r.t. negation by default and minimality w.r.t. epistemic negation. The
principle of well-supportedness guarantees that every answer set is
constructible from if-then rules obeying a level mapping and is thus free of
circular justification, while the two minimality principles ensure that the
formalism minimizes knowledge both at the level of answer sets and of world
views. Third, to embody the refined GAS principles, we extend the notion of
well-supportedness substantially to answer sets and world views, respectively.
Fourth, we define new answer set semantics in terms of the refined GAS
principles. Fifth, we use the refined GAS principles as an alternative baseline
to intuitively assess the existing answer set semantics. Finally, we analyze
the computational complexity.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [100] [MALIBU Benchmark: Multi-Agent LLM Implicit Bias Uncovered](https://arxiv.org/abs/2507.01019)
*Imran Mirza,Cole Huang,Ishwara Vasista,Rohan Patil,Asli Akalin,Sean O'Brien,Kevin Zhu*

Main category: cs.CL

TL;DR: MALIBU is a benchmark to evaluate bias in LLM-based multi-agent systems, revealing biases and the need for nuanced fairness strategies.


<details>
  <summary>Details</summary>
Motivation: Address concerns about implicit biases in LLM-based multi-agent systems and their impact on fairness and equitable representation.

Method: Scenario-based assessments with AI models completing tasks, evaluated by an LLM-based judging system in two phases: scoring responses with demographic labels and comparing paired responses.

Result: Quantifies biases in LLM outputs, showing bias mitigation may favor marginalized personas over neutrality.

Conclusion: Highlights the need for nuanced bias detection, balanced fairness strategies, and transparent benchmarks in multi-agent systems.

Abstract: Multi-agent systems, which consist of multiple AI models interacting within a
shared environment, are increasingly used for persona-based interactions.
However, if not carefully designed, these systems can reinforce implicit biases
in large language models (LLMs), raising concerns about fairness and equitable
representation. We present MALIBU, a novel benchmark developed to assess the
degree to which LLM-based multi-agent systems implicitly reinforce social
biases and stereotypes. MALIBU evaluates bias in LLM-based multi-agent systems
through scenario-based assessments. AI models complete tasks within predefined
contexts, and their responses undergo evaluation by an LLM-based multi-agent
judging system in two phases. In the first phase, judges score responses
labeled with specific demographic personas (e.g., gender, race, religion)
across four metrics. In the second phase, judges compare paired responses
assigned to different personas, scoring them and selecting the superior
response. Our study quantifies biases in LLM-generated outputs, revealing that
bias mitigation may favor marginalized personas over true neutrality,
emphasizing the need for nuanced detection, balanced fairness strategies, and
transparent evaluation benchmarks in multi-agent systems.

</details>


### [101] [Event-based evaluation of abstractive news summarization](https://arxiv.org/abs/2507.01160)
*Huiling You,Samia Touileb,Erik Velldal,Lilja Øvrelid*

Main category: cs.CL

TL;DR: Proposes evaluating abstractive summaries by comparing overlapping events in generated summaries, reference summaries, and original news articles.


<details>
  <summary>Details</summary>
Motivation: Current evaluation relies on overlapping units or similarity scores, but summaries should ideally report events like the original articles.

Method: Calculate overlapping events between generated summaries, reference summaries, and original articles using a Norwegian dataset with expert annotations.

Result: Provides deeper insight into event information in summaries.

Conclusion: Event-based evaluation offers a more meaningful measure of summary quality.

Abstract: An abstractive summary of a news article contains its most important
information in a condensed version. The evaluation of automatically generated
summaries by generative language models relies heavily on human-authored
summaries as gold references, by calculating overlapping units or similarity
scores. News articles report events, and ideally so should the summaries. In
this work, we propose to evaluate the quality of abstractive summaries by
calculating overlapping events between generated summaries, reference
summaries, and the original news articles. We experiment on a richly annotated
Norwegian dataset comprising both events annotations and summaries authored by
expert human annotators. Our approach provides more insight into the event
information contained in the summaries.

</details>


### [102] [Matching and Linking Entries in Historical Swedish Encyclopedias](https://arxiv.org/abs/2507.01170)
*Simon Börjesson,Erik Ersmark,Pierre Nugues*

Main category: cs.CL

TL;DR: The paper analyzes geographic trends in the Swedish encyclopedia 'Nordisk familjebok' across its first two editions, revealing a shift from Europe to other regions like North America and Asia, influenced by global events like WWI.


<details>
  <summary>Details</summary>
Motivation: To understand how intellectual and societal changes in Sweden between the 19th and 20th centuries were reflected in the encyclopedia's geographic content.

Method: Digitized versions were resegmented into entries, matched using semantic embeddings, and geographic entries were classified and linked to Wikidata for analysis.

Result: A significant shift in geographic focus away from Europe towards other regions, reflecting global changes like WWI and rising powers.

Conclusion: The encyclopedia's content evolution mirrors broader intellectual and societal shifts, with geographic trends influenced by historical events.

Abstract: The \textit{Nordisk familjebok} is a Swedish encyclopedia from the 19th and
20th centuries. It was written by a team of experts and aimed to be an
intellectual reference, stressing precision and accuracy. This encyclopedia had
four main editions remarkable by their size, ranging from 20 to 38 volumes. As
a consequence, the \textit{Nordisk familjebok} had a considerable influence in
universities, schools, the media, and society overall. As new editions were
released, the selection of entries and their content evolved, reflecting
intellectual changes in Sweden.
  In this paper, we used digitized versions from \textit{Project Runeberg}. We
first resegmented the raw text into entries and matched pairs of entries
between the first and second editions using semantic sentence embeddings. We
then extracted the geographical entries from both editions using a
transformer-based classifier and linked them to Wikidata. This enabled us to
identify geographic trends and possible shifts between the first and second
editions, written between 1876-1899 and 1904-1926, respectively.
  Interpreting the results, we observe a small but significant shift in
geographic focus away from Europe and towards North America, Africa, Asia,
Australia, and northern Scandinavia from the first to the second edition,
confirming the influence of the First World War and the rise of new powers. The
code and data are available on GitHub at
https://github.com/sibbo/nordisk-familjebok.

</details>


### [103] [MEGA: xLSTM with Multihead Exponential Gated Fusion for Precise Aspect-based Sentiment Analysis](https://arxiv.org/abs/2507.01213)
*Adamu Lawan,Juhua Pu,Haruna Yunusa,Jawad Muhammad,Muhammad Lawan*

Main category: cs.CL

TL;DR: The paper proposes xLSTM with Multihead Exponential Gated Fusion (MEGA) for Aspect-based Sentiment Analysis (ABSA), addressing limitations of existing methods by balancing efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Existing ABSA methods struggle with computational efficiency and performance trade-offs, lacking global context or demanding excessive resources. The potential of xLSTM models in ABSA remains unexplored.

Method: The MEGA framework integrates a bi-directional mLSTM with forward and partially flipped backward streams (PF-mLSTM) and introduces a multihead cross exponential gated fusion mechanism (MECGAF) for dynamic context modeling.

Result: MEGA outperforms state-of-the-art baselines on three benchmark datasets, achieving superior accuracy and efficiency in ABSA tasks.

Conclusion: The proposed MEGA framework effectively addresses ABSA challenges by combining localized and global context modeling, demonstrating significant improvements in performance and efficiency.

Abstract: Aspect-based Sentiment Analysis (ABSA) is a critical Natural Language
Processing (NLP) task that extracts aspects from text and determines their
associated sentiments, enabling fine-grained analysis of user opinions.
Existing ABSA methods struggle to balance computational efficiency with high
performance: deep learning models often lack global context, transformers
demand significant computational resources, and Mamba-based approaches face
CUDA dependency and diminished local correlations. Recent advancements in
Extended Long Short-Term Memory (xLSTM) models, particularly their efficient
modeling of long-range dependencies, have significantly advanced the NLP
community. However, their potential in ABSA remains untapped. To this end, we
propose xLSTM with Multihead Exponential Gated Fusion (MEGA), a novel framework
integrating a bi-directional mLSTM architecture with forward and partially
flipped backward (PF-mLSTM) streams. The PF-mLSTM enhances localized context
modeling by processing the initial sequence segment in reverse with dedicated
parameters, preserving critical short-range patterns. We further introduce an
mLSTM-based multihead cross exponential gated fusion mechanism (MECGAF) that
dynamically combines forward mLSTM outputs as query and key with PF-mLSTM
outputs as value, optimizing short-range dependency capture while maintaining
global context and efficiency. Experimental results on three benchmark datasets
demonstrate that MEGA outperforms state-of-the-art baselines, achieving
superior accuracy and efficiency in ABSA tasks.

</details>


### [104] [The Medium Is Not the Message: Deconfounding Text Embeddings via Linear Concept Erasure](https://arxiv.org/abs/2507.01234)
*Yu Fan,Yang Tian,Shauli Ravfogel,Mrinmaya Sachan,Elliott Ash,Alexander Hoyle*

Main category: cs.CL

TL;DR: A debiasing algorithm reduces biases in text embeddings caused by spurious attributes like source or language, improving similarity and clustering metrics without degrading out-of-distribution performance.


<details>
  <summary>Details</summary>
Motivation: Text embeddings can be biased by irrelevant attributes (e.g., source or language), which affects applications pooling texts from diverse corpora.

Method: A debiasing algorithm removes information about observed confounders from encoder representations.

Result: Substantial reduction in biases, improved similarity and clustering metrics across tasks, with no impact on out-of-distribution performance.

Conclusion: The debiasing method effectively mitigates biases in embeddings without computational overhead or degradation of embedding quality.

Abstract: Embedding-based similarity metrics between text sequences can be influenced
not just by the content dimensions we most care about, but can also be biased
by spurious attributes like the text's source or language. These document
confounders cause problems for many applications, but especially those that
need to pool texts from different corpora. This paper shows that a debiasing
algorithm that removes information about observed confounders from the encoder
representations substantially reduces these biases at a minimal computational
cost. Document similarity and clustering metrics improve across every embedding
variant and task we evaluate -- often dramatically. Interestingly, performance
on out-of-distribution benchmarks is not impacted, indicating that the
embeddings are not otherwise degraded.

</details>


### [105] [GAIus: Combining Genai with Legal Clauses Retrieval for Knowledge-based Assistant](https://arxiv.org/abs/2507.01259)
*Michał Matak,Jarosław A. Chudziak*

Main category: cs.CL

TL;DR: The paper introduces gAIus, an LLM-based agent for legal tasks, focusing on non-English/Chinese contexts. It improves explainability and performance over embedding-based methods, significantly boosting GPT-3.5 and GPT-4o-mini scores.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of LLMs providing accurate, referenced legal answers for non-English/Chinese jurisdictions, particularly Poland.

Method: Proposes gAIus, a retrieval mechanism using the Polish Civil Code, evaluated via a dataset from Polish law apprenticeship exams.

Result: Improved GPT-3.5-turbo-0125 by 419%, outperformed GPT-4o, and raised GPT-4o-mini's score from 31% to 86%.

Conclusion: Demonstrates potential for LLMs in legal tasks and suggests future research directions and applications.

Abstract: In this paper we discuss the capability of large language models to base
their answer and provide proper references when dealing with legal matters of
non-english and non-chinese speaking country. We discuss the history of legal
information retrieval, the difference between case law and statute law, its
impact on the legal tasks and analyze the latest research in this field. Basing
on that background we introduce gAIus, the architecture of the cognitive
LLM-based agent, whose responses are based on the knowledge retrieved from
certain legal act, which is Polish Civil Code. We propose a retrieval mechanism
which is more explainable, human-friendly and achieves better results than
embedding-based approaches. To evaluate our method we create special dataset
based on single-choice questions from entrance exams for law apprenticeships
conducted in Poland. The proposed architecture critically leveraged the
abilities of used large language models, improving the gpt-3.5-turbo-0125 by
419%, allowing it to beat gpt-4o and lifting gpt-4o-mini score from 31% to 86%.
At the end of our paper we show the possible future path of research and
potential applications of our findings.

</details>


### [106] [Evaluating Large Language Models for Multimodal Simulated Ophthalmic Decision-Making in Diabetic Retinopathy and Glaucoma Screening](https://arxiv.org/abs/2507.01278)
*Cindy Lie Tabuse,David Restepo,Carolina Gracitelli,Fernando Korn Malerbi,Caio Regatieri,Luis Filipe Nakayama*

Main category: cs.CL

TL;DR: GPT-4 shows moderate performance in simulating clinical decisions for diabetic retinopathy (DR) and glaucoma screening but lacks precision for complex tasks. Metadata inclusion had no significant impact.


<details>
  <summary>Details</summary>
Motivation: To explore the utility of large language models (LLMs) like GPT-4 in ophthalmology, specifically for interpreting retinal fundus images and simulating clinical decisions.

Method: A retrospective diagnostic validation study using 300 annotated fundus images. GPT-4 was given structured prompts with or without patient metadata to classify DR severity, recommend referrals, and estimate cup-to-disc ratios for glaucoma.

Result: GPT-4 achieved moderate accuracy for DR classification (67.5%) and referral (82.3%) but performed poorly for glaucoma (accuracy ~78%). Metadata did not significantly affect outcomes.

Conclusion: GPT-4 can simulate basic ophthalmic decision-making but is not suitable for clinical use. Potential applications include education, documentation, or image annotation workflows.

Abstract: Large language models (LLMs) can simulate clinical reasoning based on natural
language prompts, but their utility in ophthalmology is largely unexplored.
This study evaluated GPT-4's ability to interpret structured textual
descriptions of retinal fundus photographs and simulate clinical decisions for
diabetic retinopathy (DR) and glaucoma screening, including the impact of
adding real or synthetic clinical metadata. We conducted a retrospective
diagnostic validation study using 300 annotated fundus images. GPT-4 received
structured prompts describing each image, with or without patient metadata. The
model was tasked with assigning an ICDR severity score, recommending DR
referral, and estimating the cup-to-disc ratio for glaucoma referral.
Performance was evaluated using accuracy, macro and weighted F1 scores, and
Cohen's kappa. McNemar's test and change rate analysis were used to assess the
influence of metadata. GPT-4 showed moderate performance for ICDR
classification (accuracy 67.5%, macro F1 0.33, weighted F1 0.67, kappa 0.25),
driven mainly by correct identification of normal cases. Performance improved
in the binary DR referral task (accuracy 82.3%, F1 0.54, kappa 0.44). For
glaucoma referral, performance was poor across all settings (accuracy ~78%, F1
<0.04, kappa <0.03). Metadata inclusion did not significantly affect outcomes
(McNemar p > 0.05), and predictions remained consistent across conditions.
GPT-4 can simulate basic ophthalmic decision-making from structured prompts but
lacks precision for complex tasks. While not suitable for clinical use, LLMs
may assist in education, documentation, or image annotation workflows in
ophthalmology.

</details>


### [107] [Rethinking All Evidence: Enhancing Trustworthy Retrieval-Augmented Generation via Conflict-Driven Summarization](https://arxiv.org/abs/2507.01281)
*Juan Chen,Baolong Bi,Wei Zhang,Jingyan Sui,Xiaofei Zhu,Yuanzhuo Wang,Lingrui Mei,Shenghua Liu*

Main category: cs.CL

TL;DR: CARE-RAG improves RAG systems by addressing knowledge conflicts through conflict-driven summarization of internal and retrieved evidence, outperforming baselines in noisy scenarios.


<details>
  <summary>Details</summary>
Motivation: Knowledge conflicts in RAG systems undermine reliability, necessitating a framework to rethink and synthesize all evidence.

Method: CARE-RAG derives parameter-aware evidence, refines retrieved content, and uses a distilled model for conflict-driven summarization, followed by QA Repair for evaluation integrity.

Result: Outperforms RAG baselines, especially with noisy or conflicting evidence.

Conclusion: CARE-RAG enhances trustworthiness in RAG systems by effectively managing knowledge conflicts.

Abstract: Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by
integrating their parametric knowledge with external retrieved content.
However, knowledge conflicts caused by internal inconsistencies or noisy
retrieved content can severely undermine the generation reliability of RAG
systems.In this work, we argue that LLMs should rethink all evidence, including
both retrieved content and internal knowledge, before generating responses.We
propose CARE-RAG (Conflict-Aware and Reliable Evidence for RAG), a novel
framework that improves trustworthiness through Conflict-Driven Summarization
of all available evidence.CARE-RAG first derives parameter-aware evidence by
comparing parameter records to identify diverse internal perspectives. It then
refines retrieved evidences to produce context-aware evidence, removing
irrelevant or misleading content. To detect and summarize conflicts, we distill
a 3B LLaMA3.2 model to perform conflict-driven summarization, enabling reliable
synthesis across multiple sources.To further ensure evaluation integrity, we
introduce a QA Repair step to correct outdated or ambiguous benchmark
answers.Experiments on revised QA datasets with retrieval data show that
CARE-RAG consistently outperforms strong RAG baselines, especially in scenarios
with noisy or conflicting evidence.

</details>


### [108] [Frustratingly Simple Retrieval Improves Challenging, Reasoning-Intensive Benchmarks](https://arxiv.org/abs/2507.01297)
*Xinxi Lyu,Michael Duan,Rulin Shao,Pang Wei Koh,Sewon Min*

Main category: cs.CL

TL;DR: CompactDS, a web-scale datastore, enhances RAG performance on reasoning-intensive benchmarks by combining high-quality, diverse data sources with efficient retrieval methods.


<details>
  <summary>Details</summary>
Motivation: Prior RAG systems struggled with reasoning-intensive tasks due to limited datastores. This work addresses the gap by introducing a compact, high-quality datastore aligned with pretraining data.

Method: Developed CompactDS, filtering web content for quality and diversity, and combined in-memory ANN with on-disk exact search for efficient retrieval.

Result: Improved accuracy across benchmarks (e.g., 10% on MMLU, 33% on MMLU Pro) and outperformed web search engines and complex RAG systems.

Conclusion: CompactDS demonstrates the importance of diverse, high-quality data for RAG, offering simplicity and reproducibility while advancing retrieval-based AI systems.

Abstract: Retrieval-augmented Generation (RAG) has primarily been studied in limited
settings, such as factoid question answering; more challenging,
reasoning-intensive benchmarks have seen limited success from minimal RAG. In
this work, we challenge this prevailing view on established,
reasoning-intensive benchmarks: MMLU, MMLU Pro, AGI Eval, GPQA, and MATH. We
identify a key missing component in prior work: a usable, web-scale datastore
aligned with the breadth of pretraining data. To this end, we introduce
CompactDS: a diverse, high-quality, web-scale datastore that achieves high
retrieval accuracy and subsecond latency on a single-node. The key insights are
(1) most web content can be filtered out without sacrificing coverage, and a
compact, high-quality subset is sufficient; and (2) combining in-memory
approximate nearest neighbor (ANN) retrieval and on-disk exact search balances
speed and recall. Using CompactDS, we show that a minimal RAG pipeline achieves
consistent accuracy improvements across all benchmarks and model sizes
(8B--70B), with relative gains of 10% on MMLU, 33% on MMLU Pro, 14% on GPQA,
and 19% on MATH. No single data source suffices alone, highlighting the
importance of diversity of sources (web crawls, curated math, academic papers,
textbooks). Finally, we show that our carefully designed in-house datastore
matches or outperforms web search engines such as Google Search, as well as
recently proposed, complex agent-based RAG systems--all while maintaining
simplicity, reproducibility, and self-containment. We release CompactDS and our
retrieval pipeline, supporting future research exploring retrieval-based AI
systems.

</details>


### [109] [La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation](https://arxiv.org/abs/2507.01299)
*Kai Liu,Bowen Xu,Shaoyu Wu,Xin Chen,Hao Zhou,Yongliang Tao,Lulu Hu*

Main category: cs.CL

TL;DR: LaRoSA introduces layerwise rotated sparse activation for LLM inference, improving efficiency without extra training or magnitude-based pruning, achieving consistent sparsity and speed-up.


<details>
  <summary>Details</summary>
Motivation: Existing methods for activation sparsity in LLMs either require recovery training or rely on unstable magnitude-based pruning, limiting real-world adoption.

Method: LaRoSA uses layerwise orthogonal rotations to transform activations for sparsification, employing Top-K selection for consistent sparsity and speed-up.

Result: For LLaMA2-7B at 40% sparsity, LaRoSA achieves a 0.17 perplexity gap, 1.30x speed-up, and minimal accuracy drop, outperforming TEAL and CATS.

Conclusion: LaRoSA offers a practical solution for efficient LLM inference with stable sparsity and performance, outperforming existing methods.

Abstract: Activation sparsity can reduce the computational overhead and memory
transfers during the forward pass of Large Language Model (LLM) inference.
Existing methods face limitations, either demanding time-consuming recovery
training that hinders real-world adoption, or relying on empirical
magnitude-based pruning, which causes fluctuating sparsity and unstable
inference speed-up. This paper introduces LaRoSA (Layerwise Rotated Sparse
Activation), a novel method for activation sparsification designed to improve
LLM efficiency without requiring additional training or magnitude-based
pruning. We leverage layerwise orthogonal rotations to transform input
activations into rotated forms that are more suitable for sparsification. By
employing a Top-K selection approach within the rotated activations, we achieve
consistent model-level sparsity and reliable wall-clock time speed-up. LaRoSA
is effective across various sizes and types of LLMs, demonstrating minimal
performance degradation and robust inference acceleration. Specifically, for
LLaMA2-7B at 40% sparsity, LaRoSA achieves a mere 0.17 perplexity gap with a
consistent 1.30x wall-clock time speed-up, and reduces the accuracy gap in
zero-shot tasks compared to the dense model to just 0.54%, while surpassing
TEAL by 1.77% and CATS by 17.14%.

</details>


### [110] [Symbolic or Numerical? Understanding Physics Problem Solving in Reasoning LLMs](https://arxiv.org/abs/2507.01334)
*Nifu Dan,Yujun Cai,Yiwei Wang*

Main category: cs.CL

TL;DR: Advanced instruction-tuned reasoning models like Deepseek-R1 excel in solving complex physics problems, achieving state-of-the-art accuracy and unique symbolic reasoning patterns. Few-shot prompting further enhances performance.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of physics reasoning for LLMs, which requires deep conceptual understanding and problem-solving skills.

Method: Application of advanced instruction-tuned reasoning models (e.g., Deepseek-R1) to diverse physics problems from SciBench, with few-shot prompting.

Result: State-of-the-art accuracy in physics problem-solving and distinctive symbolic reasoning patterns. Few-shot prompting improves performance.

Conclusion: Reasoning models like Deepseek-R1 show promise in physics reasoning, with potential for further gains through strategic prompting.

Abstract: Navigating the complexities of physics reasoning has long been a difficult
task for Large Language Models (LLMs), requiring a synthesis of profound
conceptual understanding and adept problem-solving techniques. In this study,
we investigate the application of advanced instruction-tuned reasoning models,
such as Deepseek-R1, to address a diverse spectrum of physics problems curated
from the challenging SciBench benchmark. Our comprehensive experimental
evaluation reveals the remarkable capabilities of reasoning models. Not only do
they achieve state-of-the-art accuracy in answering intricate physics
questions, but they also generate distinctive reasoning patterns that emphasize
on symbolic derivation. Furthermore, our findings indicate that even for these
highly sophisticated reasoning models, the strategic incorporation of few-shot
prompting can still yield measurable improvements in overall accuracy,
highlighting the potential for continued performance gains.

</details>


### [111] [LEDOM: An Open and Fundamental Reverse Language Model](https://arxiv.org/abs/2507.01335)
*Xunjian Yin,Sitao Cheng,Yuxi Xie,Xinyu Hu,Li Lin,Xinyi Wang,Liangming Pan,William Yang Wang,Xiaojun Wan*

Main category: cs.CL

TL;DR: LEDOM is a reverse language model trained on 435B tokens, offering unique backward reasoning capabilities and applications like Reverse Reward for improving forward model outputs.


<details>
  <summary>Details</summary>
Motivation: To explore the potential of reverse language models as foundational tools for general tasks, leveraging backward reasoning for novel applications.

Method: LEDOM is trained autoregressively on 435B tokens with 2B and 7B parameter variants, processing sequences in reverse temporal order.

Result: LEDOM demonstrates unique backward reasoning capabilities, enabling applications like Reverse Reward, which improves forward model outputs on tasks like mathematical reasoning.

Conclusion: LEDOM shows broad application potential, and its models, code, and data will be released to support future research.

Abstract: We introduce LEDOM, the first purely reverse language model, trained
autoregressively on 435B tokens with 2B and 7B parameter variants, which
processes sequences in reverse temporal order through previous token
prediction. For the first time, we present the reverse language model as a
potential foundational model across general tasks, accompanied by a set of
intriguing examples and insights. Based on LEDOM, we further introduce a novel
application: Reverse Reward, where LEDOM-guided reranking of forward language
model outputs leads to substantial performance improvements on mathematical
reasoning tasks. This approach leverages LEDOM's unique backward reasoning
capability to refine generation quality through posterior evaluation. Our
findings suggest that LEDOM exhibits unique characteristics with broad
application potential. We will release all models, training code, and
pre-training data to facilitate future research.

</details>


### [112] [Skywork-Reward-V2: Scaling Preference Data Curation via Human-AI Synergy](https://arxiv.org/abs/2507.01352)
*Chris Yuhao Liu,Liang Zeng,Yuzhen Xiao,Jujie He,Jiacai Liu,Chaojie Wang,Rui Yan,Wei Shen,Fuxiang Zhang,Jiacheng Xu,Yang Liu,Yahui Zhou*

Main category: cs.CL

TL;DR: The paper introduces SynPref-40M, a large-scale preference dataset, and Skywork-Reward-V2, a suite of reward models, to address limitations in current reward models by leveraging human-AI synergy for high-quality data curation.


<details>
  <summary>Details</summary>
Motivation: Current reward models (RMs) in RLHF perform poorly due to limitations in preference datasets, which are narrow, synthetically labeled, or lack quality control.

Method: A human-AI synergistic two-stage pipeline is designed for data curation, combining human annotation quality with AI scalability. Skywork-Reward-V2 models are trained on a curated subset of SynPref-40M.

Result: Skywork-Reward-V2 achieves state-of-the-art performance across seven benchmarks, demonstrating versatility in alignment, correctness, safety, and bias resistance.

Conclusion: The approach highlights the potential of human-AI synergy for high-quality data curation and advances open reward models.

Abstract: Despite the critical role of reward models (RMs) in reinforcement learning
from human feedback (RLHF), current state-of-the-art open RMs perform poorly on
most existing evaluation benchmarks, failing to capture the spectrum of nuanced
and sophisticated human preferences. Even approaches that incorporate advanced
training techniques have not yielded meaningful performance improvements. We
hypothesize that this brittleness stems primarily from limitations in
preference datasets, which are often narrowly scoped, synthetically labeled, or
lack rigorous quality control. To address these challenges, we present a
large-scale preference dataset comprising 40 million preference pairs, named
SynPref-40M. To enable data curation at scale, we design a human-AI synergistic
two-stage pipeline that leverages the complementary strengths of human
annotation quality and AI scalability. In this pipeline, humans provide
verified annotations, while large language models perform automatic curation
based on human guidance. Training on this preference mixture, we introduce
Skywork-Reward-V2, a suite of eight reward models ranging from 0.6B to 8B
parameters, trained on a carefully curated subset of 26 million preference
pairs from SynPref-40M. We demonstrate that Skywork-Reward-V2 is versatile
across a wide range of capabilities, including alignment with human
preferences, objective correctness, safety, resistance to stylistic biases, and
best-of-N scaling, achieving state-of-the-art performance across seven major
reward model benchmarks. Ablation studies confirm that the effectiveness of our
approach stems not only from data scale but also from high-quality curation.
The Skywork-Reward-V2 series represents substantial progress in open reward
models, highlighting the untapped potential of existing preference datasets and
demonstrating how human-AI curation synergy can unlock significantly higher
data quality.

</details>


### [113] [Clinical NLP with Attention-Based Deep Learning for Multi-Disease Prediction](https://arxiv.org/abs/2507.01437)
*Ting Xu,Xiaoxiao Deng,Xiandong Meng,Haifeng Yang,Yan Wu*

Main category: cs.CL

TL;DR: A deep learning method using attention mechanisms is proposed for unified modeling of information extraction and multi-label disease prediction in EHR texts, outperforming existing approaches.


<details>
  <summary>Details</summary>
Motivation: Address the challenges of unstructured, high-dimensional EHR texts and their semantic complexity for better clinical text processing.

Method: Transformer-based architecture with multi-layer self-attention and a Sigmoid-based multi-label classifier, enhanced by context-aware semantic alignment.

Result: Outperforms existing methods across metrics, maintains generalization under varying conditions, and handles label co-occurrence and sparse information well.

Conclusion: The framework is efficient for real-world clinical texts and significant for multi-label medical text modeling.

Abstract: This paper addresses the challenges posed by the unstructured nature and
high-dimensional semantic complexity of electronic health record texts. A deep
learning method based on attention mechanisms is proposed to achieve unified
modeling for information extraction and multi-label disease prediction. The
study is conducted on the MIMIC-IV dataset. A Transformer-based architecture is
used to perform representation learning over clinical text. Multi-layer
self-attention mechanisms are employed to capture key medical entities and
their contextual relationships. A Sigmoid-based multi-label classifier is then
applied to predict multiple disease labels. The model incorporates a
context-aware semantic alignment mechanism, enhancing its representational
capacity in typical medical scenarios such as label co-occurrence and sparse
information. To comprehensively evaluate model performance, a series of
experiments were conducted, including baseline comparisons, hyperparameter
sensitivity analysis, data perturbation studies, and noise injection tests.
Results demonstrate that the proposed method consistently outperforms
representative existing approaches across multiple performance metrics. The
model maintains strong generalization under varying data scales, interference
levels, and model depth configurations. The framework developed in this study
offers an efficient algorithmic foundation for processing real-world clinical
texts and presents practical significance for multi-label medical text modeling
tasks.

</details>


### [114] [LogitSpec: Accelerating Retrieval-based Speculative Decoding via Next Next Token Speculation](https://arxiv.org/abs/2507.01449)
*Tianyu Liu,Qitan Lv,Hao Li,Xing Gao,Xiao Sun*

Main category: cs.CL

TL;DR: LogitSpec improves speculative decoding by using the last token's logit to predict and retrieve draft tokens, achieving significant speedup and accuracy.


<details>
  <summary>Details</summary>
Motivation: Retrieval-based speculative decoding often fails to find accurate draft tokens. LogitSpec addresses this by expanding the retrieval range using the last token's logit.

Method: LogitSpec speculates the next next token using the last logit and retrieves relevant references for both the next and next next tokens.

Result: Achieves up to 2.61× speedup and 3.28 mean accepted tokens per decoding step.

Conclusion: LogitSpec is a training-free, plug-and-play solution that enhances speculative decoding efficiency and accuracy.

Abstract: Speculative decoding (SD), where a small draft model is employed to propose
draft tokens in advance and then the target model validates them in parallel,
has emerged as a promising technique for LLM inference acceleration. Many
endeavors to improve SD are to eliminate the need for a draft model and
generate draft tokens in a retrieval-based manner in order to further alleviate
the drafting overhead and significantly reduce the difficulty in deployment and
applications. However, retrieval-based SD relies on a matching paradigm to
retrieval the most relevant reference as the draft tokens, where these methods
often fail to find matched and accurate draft tokens. To address this
challenge, we propose LogitSpec to effectively expand the retrieval range and
find the most relevant reference as drafts. Our LogitSpec is motivated by the
observation that the logit of the last token can not only predict the next
token, but also speculate the next next token. Specifically, LogitSpec
generates draft tokens in two steps: (1) utilizing the last logit to speculate
the next next token; (2) retrieving relevant reference for both the next token
and the next next token. LogitSpec is training-free and plug-and-play, which
can be easily integrated into existing LLM inference frameworks. Extensive
experiments on a wide range of text generation benchmarks demonstrate that
LogitSpec can achieve up to 2.61 $\times$ speedup and 3.28 mean accepted tokens
per decoding step. Our code is available at
https://github.com/smart-lty/LogitSpec.

</details>


### [115] [Evaluating the Effectiveness of Direct Preference Optimization for Personalizing German Automatic Text Simplifications for Persons with Intellectual Disabilities](https://arxiv.org/abs/2507.01479)
*Yingqiang Gao,Kaede Johnson,David Froehlich,Luisa Carrer,Sarah Ebling*

Main category: cs.CL

TL;DR: The paper proposes using direct preference optimization (DPO) to personalize LLM-based automatic text simplification (ATS) for people with intellectual disabilities, incorporating their feedback to improve system alignment with human expectations.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based ATS systems lack personalization for target groups like people with intellectual disabilities, as they don't incorporate preference feedback during training.

Method: Extends supervised fine-tuning (SFT) with DPO, using human feedback from the target group to post-train LLM-based ATS models. Also proposes a pipeline for developing personalized ATS systems.

Result: Demonstrates the importance of involving target group representatives in designing AI accessibility solutions to align with human expectations.

Conclusion: This work advances personalized, inclusive AI systems by integrating feedback from both experts and target group members, improving text simplification for accessibility.

Abstract: Automatic text simplification (ATS) aims to enhance language accessibility
for various target groups, particularly persons with intellectual disabilities.
Recent advancements in generative AI, especially large language models (LLMs),
have substantially improved the quality of machine-generated text
simplifications, thereby mitigating information barriers for the target group.
However, existing LLM-based ATS systems do not incorporate preference feedback
on text simplifications during training, resulting in a lack of personalization
tailored to the specific needs of target group representatives.
  In this work, we extend the standard supervised fine-tuning (SFT) approach
for adapting LLM-based ATS models by leveraging a computationally efficient LLM
alignment technique -- direct preference optimization (DPO). Specifically, we
post-train LLM-based ATS models using human feedback collected from persons
with intellectual disabilities, reflecting their preferences on paired text
simplifications generated by mainstream LLMs. Furthermore, we propose a
pipeline for developing personalized LLM-based ATS systems, encompassing data
collection, model selection, SFT and DPO post-training, and evaluation. Our
findings underscore the necessity of active participation of target group
persons in designing personalized AI accessibility solutions aligned with human
expectations. This work represents a step towards personalizing inclusive AI
systems at the target-group level, incorporating insights not only from text
simplification experts but also from target group persons themselves.

</details>


### [116] [Efficient Out-of-Scope Detection in Dialogue Systems via Uncertainty-Driven LLM Routing](https://arxiv.org/abs/2507.01541)
*Álvaro Zaera,Diana Nicoleta Popa,Ivan Sekulic,Paolo Rosso*

Main category: cs.CL

TL;DR: A modular framework combining uncertainty modeling and fine-tuned LLMs for efficient OOS intent detection in TODS.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of OOS intent detection to enhance robustness in TODS for unseen or ambiguous queries.

Method: Uses uncertainty estimation on in-scope intent detection, followed by fine-tuned LLMs for high-uncertainty cases.

Result: Achieves state-of-the-art performance on OOS benchmarks, including real-world TODS data.

Conclusion: The framework balances efficiency and performance, integrating traditional methods with LLMs for superior OOS detection.

Abstract: Out-of-scope (OOS) intent detection is a critical challenge in task-oriented
dialogue systems (TODS), as it ensures robustness to unseen and ambiguous
queries. In this work, we propose a novel but simple modular framework that
combines uncertainty modeling with fine-tuned large language models (LLMs) for
efficient and accurate OOS detection. The first step applies uncertainty
estimation to the output of an in-scope intent detection classifier, which is
currently deployed in a real-world TODS handling tens of thousands of user
interactions daily. The second step then leverages an emerging LLM-based
approach, where a fine-tuned LLM is triggered to make a final decision on
instances with high uncertainty. Unlike prior approaches, our method
effectively balances computational efficiency and performance, combining
traditional approaches with LLMs and yielding state-of-the-art results on key
OOS detection benchmarks, including real-world OOS data acquired from a
deployed TODS.

</details>


### [117] [Is External Information Useful for Stance Detection with LLMs?](https://arxiv.org/abs/2507.01543)
*Quang Minh Nguyen,Taegyoon Kim*

Main category: cs.CL

TL;DR: The study evaluates how Wikipedia and web search external information affects stance detection in large language models (LLMs), finding it degrades performance due to LLMs aligning with the provided information's stance rather than the ground truth.


<details>
  <summary>Details</summary>
Motivation: To assess whether external information, beneficial in BERT-based systems, improves stance detection in LLMs.

Method: Systematic evaluation across eight LLMs and three datasets with 12 targets, using Wikipedia and web search external information.

Result: External information degrades performance (macro F1 scores drop by up to 27.9%), with LLMs aligning predictions with the provided information's stance.

Conclusion: External information introduces biases in LLM-based stance classifiers, contrasting prior BERT-based findings, and fine-tuning only partially mitigates the issue.

Abstract: In the stance detection task, a text is classified as either favorable,
opposing, or neutral towards a target. Prior work suggests that the use of
external information, e.g., excerpts from Wikipedia, improves stance detection
performance. However, whether or not such information can benefit large
language models (LLMs) remains an unanswered question, despite their wide
adoption in many reasoning tasks. In this study, we conduct a systematic
evaluation on how Wikipedia and web search external information can affect
stance detection across eight LLMs and in three datasets with 12 targets.
Surprisingly, we find that such information degrades performance in most cases,
with macro F1 scores dropping by up to 27.9\%. We explain this through
experiments showing LLMs' tendency to align their predictions with the stance
and sentiment of the provided information rather than the ground truth stance
of the given text. We also find that performance degradation persists with
chain-of-thought prompting, while fine-tuning mitigates but does not fully
eliminate it. Our findings, in contrast to previous literature on BERT-based
systems which suggests that external information enhances performance,
highlight the risks of information biases in LLM-based stance classifiers. Code
is available at https://github.com/ngqm/acl2025-stance-detection.

</details>


### [118] [Emotionally Intelligent Task-oriented Dialogue Systems: Architecture, Representation, and Optimisation](https://arxiv.org/abs/2507.01594)
*Shutong Feng,Hsien-chin Lin,Nurul Lubis,Carel van Niekerk,Michael Heck,Benjamin Ruppik,Renato Vukovic,Milica Gašić*

Main category: cs.CL

TL;DR: The paper introduces LUSTER, an LLM-based unified system for task-oriented dialogue (ToD) with end-to-end reinforcement learning, optimizing for task success and emotional responsiveness.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of building effective and emotionally intelligent ToD systems in noisy, ambiguous conversational environments.

Method: Proposes LUSTER, combining LLMs with structured reward modeling (short-term user sentiment and long-term task success) in a challenging evaluation setup.

Result: LUSTER demonstrates improved resilience and emotional responsiveness in ToD systems.

Conclusion: Combining LLMs with structured rewards offers a practical approach for next-gen conversational agents.

Abstract: Task-oriented dialogue (ToD) systems are designed to help users achieve
specific goals through natural language interaction. While recent advances in
large language models (LLMs) have significantly improved linguistic fluency and
contextual understanding, building effective and emotionally intelligent ToD
systems remains a complex challenge. Effective ToD systems must optimise for
task success, emotional understanding and responsiveness, and precise
information conveyance, all within inherently noisy and ambiguous
conversational environments. In this work, we investigate architectural,
representational, optimisational as well as emotional considerations of ToD
systems. We set up systems covering these design considerations with a
challenging evaluation environment composed of a natural-language user
simulator coupled with an imperfect natural language understanding module. We
propose \textbf{LUSTER}, an \textbf{L}LM-based \textbf{U}nified \textbf{S}ystem
for \textbf{T}ask-oriented dialogue with \textbf{E}nd-to-end
\textbf{R}einforcement learning with both short-term (user sentiment) and
long-term (task success) rewards. Our findings demonstrate that combining LLM
capability with structured reward modelling leads to more resilient and
emotionally responsive ToD systems, offering a practical path forward for
next-generation conversational agents.

</details>


### [119] [Chart Question Answering from Real-World Analytical Narratives](https://arxiv.org/abs/2507.01627)
*Maeve Hutchinson,Radu Jianu,Aidan Slingsby,Jo Wood,Pranava Madhyastha*

Main category: cs.CL

TL;DR: A new dataset for chart question answering (CQA) is introduced, featuring real-world, multi-view charts and grounded questions, highlighting a performance gap in state-of-the-art models.


<details>
  <summary>Details</summary>
Motivation: To address the lack of ecologically valid reasoning workflows in prior CQA benchmarks by providing a dataset reflecting real-world analytical narratives.

Method: Constructed a dataset from visualization notebooks with multi-view charts and natural language questions, then benchmarked state-of-the-art multimodal models like GPT-4.1.

Result: GPT-4.1 achieved 69.3% accuracy, revealing a significant performance gap in this authentic CQA setting.

Conclusion: The dataset exposes challenges in CQA, emphasizing the need for improved models to handle real-world reasoning workflows.

Abstract: We present a new dataset for chart question answering (CQA) constructed from
visualization notebooks. The dataset features real-world, multi-view charts
paired with natural language questions grounded in analytical narratives.
Unlike prior benchmarks, our data reflects ecologically valid reasoning
workflows. Benchmarking state-of-the-art multimodal large language models
reveals a significant performance gap, with GPT-4.1 achieving an accuracy of
69.3%, underscoring the challenges posed by this more authentic CQA setting.

</details>


### [120] [Confidence and Stability of Global and Pairwise Scores in NLP Evaluation](https://arxiv.org/abs/2507.01633)
*Georgii Levtsov,Dmitry Ustalov*

Main category: cs.CL

TL;DR: The paper compares global pointwise scores and pairwise comparisons for evaluating NLP models, finding global scores reliable for overall rankings but weak in detecting rare errors, while pairwise comparisons excel in identifying strong models with lower global scores but require more comparisons.


<details>
  <summary>Details</summary>
Motivation: To evaluate the strengths and weaknesses of global scores and pairwise comparisons in NLP benchmarking, aiding decision-making for model evaluation strategies.

Method: Computational experiments on synthetic and real-world datasets using global metrics and the Bradley-Terry model for pairwise comparisons.

Result: Global scores provide reliable overall rankings but underestimate models with rare errors. Pairwise comparisons identify strong models with lower global scores but need more comparisons for convergence.

Conclusion: Both evaluation methods have trade-offs; global scores suit overall rankings, while pairwise comparisons are better for nuanced model strengths, especially in text generation.

Abstract: With the advent of highly capable instruction-tuned neural language models,
benchmarking in natural language processing (NLP) is increasingly shifting
towards pairwise comparison leaderboards, such as LMSYS Arena, from traditional
global pointwise scores (e.g., GLUE, BIG-bench, SWE-bench). This paper
empirically investigates the strengths and weaknesses of both global scores and
pairwise comparisons to aid decision-making in selecting appropriate model
evaluation strategies. Through computational experiments on synthetic and
real-world datasets using standard global metrics and the popular Bradley-Terry
model for pairwise comparisons, we found that while global scores provide more
reliable overall rankings, they can underestimate strong models with rare,
significant errors or low confidence. Conversely, pairwise comparisons are
particularly effective for identifying strong contenders among models with
lower global scores, especially where quality metrics are hard to define (e.g.,
text generation), though they require more comparisons to converge if ties are
frequent. Our code and data are available at
https://github.com/HSPyroblast/srw-ranking under a permissive license.

</details>


### [121] [Adapting Language Models to Indonesian Local Languages: An Empirical Study of Language Transferability on Zero-Shot Settings](https://arxiv.org/abs/2507.01645)
*Rifki Afina Putri*

Main category: cs.CL

TL;DR: The paper explores how pre-trained language models transfer to low-resource Indonesian local languages for sentiment analysis, comparing zero-shot and adapter-based methods. MAD-X improves performance without target-language data, and prior language exposure is key to success.


<details>
  <summary>Details</summary>
Motivation: To understand the transferability of pre-trained models to low-resource Indonesian local languages and identify factors influencing performance.

Method: Evaluates zero-shot and adapter-based transfer (MAD-X) on ten languages, grouped by pre-training exposure (seen, partially seen, unseen). Uses models like Indonesian BERT, mBERT, XLM-R, and MAD-X. Analyzes tokenization and vocabulary overlap.

Result: Multilingual models excel on seen languages, perform moderately on partially seen, and poorly on unseen. MAD-X boosts performance for seen and partially seen languages. Tokenization and vocabulary overlap weakly correlate with performance; prior language exposure is the strongest predictor.

Conclusion: Prior exposure to a language (directly or through related languages) is crucial for transfer success. MAD-X is effective for seen and partially seen languages, highlighting the importance of pre-training data diversity.

Abstract: In this paper, we investigate the transferability of pre-trained language
models to low-resource Indonesian local languages through the task of sentiment
analysis. We evaluate both zero-shot performance and adapter-based transfer on
ten local languages using models of different types: a monolingual Indonesian
BERT, multilingual models such as mBERT and XLM-R, and a modular adapter-based
approach called MAD-X. To better understand model behavior, we group the target
languages into three categories: seen (included during pre-training), partially
seen (not included but linguistically related to seen languages), and unseen
(absent and unrelated in pre-training data). Our results reveal clear
performance disparities across these groups: multilingual models perform best
on seen languages, moderately on partially seen ones, and poorly on unseen
languages. We find that MAD-X significantly improves performance, especially
for seen and partially seen languages, without requiring labeled data in the
target language. Additionally, we conduct a further analysis on tokenization
and show that while subword fragmentation and vocabulary overlap with
Indonesian correlate weakly with prediction quality, they do not fully explain
the observed performance. Instead, the most consistent predictor of transfer
success is the model's prior exposure to the language, either directly or
through a related language.

</details>


### [122] [AdamMeme: Adaptively Probe the Reasoning Capacity of Multimodal Large Language Models on Harmfulness](https://arxiv.org/abs/2507.01702)
*Zixin Chen,Hongzhan Lin,Kaixin Li,Ziyang Luo,Zhen Ye,Guang Chen,Zhiyong Huang,Jing Ma*

Main category: cs.CL

TL;DR: AdamMeme is a dynamic, agent-based framework for evaluating multimodal LLMs' understanding of harmful memes, addressing limitations of static benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for harmful meme understanding in mLLMs are static and lack adaptability to evolving online content.

Method: Proposes AdamMeme, a flexible, agent-based framework that updates meme data iteratively to probe mLLMs' reasoning on harmfulness.

Result: AdamMeme reveals specific weaknesses in mLLMs' interpretations through comprehensive, fine-grained analyses.

Conclusion: The framework offers a more thorough and up-to-date evaluation of mLLMs' capabilities in understanding harmful memes.

Abstract: The proliferation of multimodal memes in the social media era demands that
multimodal Large Language Models (mLLMs) effectively understand meme
harmfulness. Existing benchmarks for assessing mLLMs on harmful meme
understanding rely on accuracy-based, model-agnostic evaluations using static
datasets. These benchmarks are limited in their ability to provide up-to-date
and thorough assessments, as online memes evolve dynamically. To address this,
we propose AdamMeme, a flexible, agent-based evaluation framework that
adaptively probes the reasoning capabilities of mLLMs in deciphering meme
harmfulness. Through multi-agent collaboration, AdamMeme provides comprehensive
evaluations by iteratively updating the meme data with challenging samples,
thereby exposing specific limitations in how mLLMs interpret harmfulness.
Extensive experiments show that our framework systematically reveals the
varying performance of different target mLLMs, offering in-depth, fine-grained
analyses of model-specific weaknesses. Our code is available at
https://github.com/Lbotirx/AdamMeme.

</details>


### [123] [Stereotype Detection as a Catalyst for Enhanced Bias Detection: A Multi-Task Learning Approach](https://arxiv.org/abs/2507.01715)
*Aditya Tomar,Rudra Murthy,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: The paper introduces StereoBias, a dataset for detecting bias and stereotypes in language models, showing that joint training improves bias detection.


<details>
  <summary>Details</summary>
Motivation: Addressing harmful biases and stereotypes in language models, especially in sensitive applications like content moderation.

Method: Joint training of bias and stereotype detection using StereoBias dataset, comparing encoder-only and decoder-only models with QLoRA.

Result: Joint training significantly improves bias detection; decoder-only models perform competitively.

Conclusion: Leveraging stereotype information enhances fairness and effectiveness in AI systems.

Abstract: Bias and stereotypes in language models can cause harm, especially in
sensitive areas like content moderation and decision-making. This paper
addresses bias and stereotype detection by exploring how jointly learning these
tasks enhances model performance. We introduce StereoBias, a unique dataset
labeled for bias and stereotype detection across five categories: religion,
gender, socio-economic status, race, profession, and others, enabling a deeper
study of their relationship. Our experiments compare encoder-only models and
fine-tuned decoder-only models using QLoRA. While encoder-only models perform
well, decoder-only models also show competitive results. Crucially, joint
training on bias and stereotype detection significantly improves bias detection
compared to training them separately. Additional experiments with sentiment
analysis confirm that the improvements stem from the connection between bias
and stereotypes, not multi-task learning alone. These findings highlight the
value of leveraging stereotype information to build fairer and more effective
AI systems.

</details>


### [124] [LLMs for Legal Subsumption in German Employment Contracts](https://arxiv.org/abs/2507.01734)
*Oliver Wardas,Florian Matthes*

Main category: cs.CL

TL;DR: The paper explores using LLMs and in-context learning to classify clauses in German employment contracts, finding that examination guidelines significantly improve recall for void clauses, though performance remains below human lawyers.


<details>
  <summary>Details</summary>
Motivation: Legal work's text-heavy nature and the lack of interpretability in data-driven NLP approaches limit their applicability in dynamic legal environments.

Method: Collaborated with legal experts to extend a dataset and evaluated LLMs' ability to classify clauses under three legal context variants: no context, full-text legal sources, and distilled examination guidelines.

Result: Examination guidelines improved recall for void clauses and weighted F1-Score to 80%, but full-text sources' performance was still below human lawyers.

Conclusion: LLMs show potential to assist lawyers in contract legality review but have limitations, as evidenced by the extended dataset and findings.

Abstract: Legal work, characterized by its text-heavy and resource-intensive nature,
presents unique challenges and opportunities for NLP research. While
data-driven approaches have advanced the field, their lack of interpretability
and trustworthiness limits their applicability in dynamic legal environments.
To address these issues, we collaborated with legal experts to extend an
existing dataset and explored the use of Large Language Models (LLMs) and
in-context learning to evaluate the legality of clauses in German employment
contracts. Our work evaluates the ability of different LLMs to classify clauses
as "valid," "unfair," or "void" under three legal context variants: no legal
context, full-text sources of laws and court rulings, and distilled versions of
these (referred to as examination guidelines). Results show that full-text
sources moderately improve performance, while examination guidelines
significantly enhance recall for void clauses and weighted F1-Score, reaching
80\%. Despite these advancements, LLMs' performance when using full-text
sources remains substantially below that of human lawyers. We contribute an
extended dataset, including examination guidelines, referenced legal sources,
and corresponding annotations, alongside our code and all log files. Our
findings highlight the potential of LLMs to assist lawyers in contract legality
review while also underscoring the limitations of the methods presented.

</details>


### [125] [Data interference: emojis, homoglyphs, and issues of data fidelity in corpora and their results](https://arxiv.org/abs/2507.01764)
*Matteo Di Cristofaro*

Main category: cs.CL

TL;DR: The paper explores how tokenisation discrepancies impact language data representation and analysis validity, focusing on challenges like emojis and homoglyphs. It proposes preprocessing methods to ensure accurate corpus representation and reliable linguistic analysis.


<details>
  <summary>Details</summary>
Motivation: To address how tokenisation discrepancies affect corpus linguistics, particularly with emojis and homoglyphs, and to ensure reliable analysis.

Method: Investigates preprocessing methods for digital texts to maintain corpus fidelity and support accurate linguistic analysis.

Result: Highlights the need for detailed understanding of linguistic and technical aspects in tokenisation to enhance corpus analysis accuracy.

Conclusion: The study underscores the importance of preprocessing for reliable corpus-based research, impacting both quantitative and qualitative approaches.

Abstract: Tokenisation - "the process of splitting text into atomic parts" (Brezina &
Timperley, 2017: 1) - is a crucial step for corpus linguistics, as it provides
the basis for any applicable quantitative method (e.g. collocations) while
ensuring the reliability of qualitative approaches. This paper examines how
discrepancies in tokenisation affect the representation of language data and
the validity of analytical findings: investigating the challenges posed by
emojis and homoglyphs, the study highlights the necessity of preprocessing
these elements to maintain corpus fidelity to the source data. The research
presents methods for ensuring that digital texts are accurately represented in
corpora, thereby supporting reliable linguistic analysis and guaranteeing the
repeatability of linguistic interpretations. The findings emphasise the
necessity of a detailed understanding of both linguistic and technical aspects
involved in digital textual data to enhance the accuracy of corpus analysis,
and have significant implications for both quantitative and qualitative
approaches in corpus-based research.

</details>


### [126] [MuRating: A High Quality Data Selecting Approach to Multilingual Large Language Model Pretraining](https://arxiv.org/abs/2507.01785)
*Zhixun Chen,Ping Guo,Wenhan Han,Yifan Zhang,Binbin Liu,Haobin Lin,Fengze Liu,Yan Zhao,Bingni Zhang,Taifeng Wang,Yin Zheng,Meng Fang*

Main category: cs.CL

TL;DR: MuRating is a scalable framework for multilingual data-quality assessment, transferring English quality signals to 17 languages, improving model performance.


<details>
  <summary>Details</summary>
Motivation: Existing data-quality methods focus on English; MuRating addresses the gap by extending quality assessment to multilingual contexts.

Method: MuRating aggregates English raters via pairwise comparisons, projects judgments through translation, and trains a multilingual evaluator on diverse text pairs.

Result: MuRating boosts accuracy on English and multilingual benchmarks, especially in knowledge-intensive tasks, outperforming baselines like QuRater and AskLLM.

Conclusion: MuRating effectively improves multilingual data quality, with analysis of biases and translation fidelity guiding future work.

Abstract: Data quality is a critical driver of large language model performance, yet
existing model-based selection methods focus almost exclusively on English. We
introduce MuRating, a scalable framework that transfers high-quality English
data-quality signals into a single rater for 17 target languages. MuRating
aggregates multiple English "raters" via pairwise comparisons to learn unified
document-quality scores,then projects these judgments through translation to
train a multilingual evaluator on monolingual, cross-lingual, and parallel text
pairs. Applied to web data, MuRating selects balanced subsets of English and
multilingual content to pretrain a 1.2 B-parameter LLaMA model. Compared to
strong baselines, including QuRater, AskLLM, DCLM and so on, our approach
boosts average accuracy on both English benchmarks and multilingual
evaluations, with especially large gains on knowledge-intensive tasks. We
further analyze translation fidelity, selection biases, and underrepresentation
of narrative material, outlining directions for future work.

</details>


### [127] [Probing Evaluation Awareness of Language Models](https://arxiv.org/abs/2507.01786)
*Jord Nguyen,Khiem Hoang,Carlo Leonardo Attubato,Felix Hofstätter*

Main category: cs.CL

TL;DR: Language models like Llama-3.3-70B-Instruct can internally distinguish between testing and deployment phases, raising concerns about the reliability of AI evaluations and governance.


<details>
  <summary>Details</summary>
Motivation: To investigate evaluation awareness in models and its implications for AI safety and policy, particularly in undermining evaluation reliability.

Method: Used linear probes to analyze Llama-3.3-70B-Instruct's ability to separate real-world evaluation and deployment prompts.

Result: Probes successfully distinguished evaluation and deployment prompts, and safety evaluations were classified as artificial, indicating models perceive them as inauthentic.

Conclusion: Highlights the need for trustworthy evaluations and understanding deceptive capabilities, suggesting model internals can aid safety audits for future models.

Abstract: Language models can distinguish between testing and deployment phases -- a
capability known as evaluation awareness. This has significant safety and
policy implications, potentially undermining the reliability of evaluations
that are central to AI governance frameworks and voluntary industry
commitments. In this paper, we study evaluation awareness in
Llama-3.3-70B-Instruct. We show that linear probes can separate real-world
evaluation and deployment prompts, suggesting that current models internally
represent this distinction. We also find that current safety evaluations are
correctly classified by the probes, suggesting that they already appear
artificial or inauthentic to models. Our findings underscore the importance of
ensuring trustworthy evaluations and understanding deceptive capabilities. More
broadly, our work showcases how model internals may be leveraged to support
blackbox methods in safety audits, especially for future models more competent
at evaluation awareness and deception.

</details>


### [128] [How Do Vision-Language Models Process Conflicting Information Across Modalities?](https://arxiv.org/abs/2507.01790)
*Tianze Hua,Tian Yun,Ellie Pavlick*

Main category: cs.CL

TL;DR: The paper investigates how vision-language AI models handle conflicting multimodal inputs (e.g., mismatched image-caption pairs) and reveals biases in modality preference, internal representational structures, and the role of attention heads in resolving conflicts.


<details>
  <summary>Details</summary>
Motivation: To understand how multimodal AI models behave when faced with conflicting input streams and to identify mechanisms for detecting and resolving such conflicts.

Method: The study tests vision-language models with inconsistent inputs (e.g., mismatched image-caption pairs) and analyzes their responses, internal representations, and attention mechanisms.

Result: Models often favor one modality over another, with preferences evident in their internal structures. Specific attention heads and 'router heads' influence modality preference and can be manipulated to improve performance.

Conclusion: The findings advance understanding of how multimodal models detect and resolve conflicts, offering insights for improving their robustness and adaptability in complex environments.

Abstract: AI models are increasingly required to be multimodal, integrating disparate
input streams into a coherent state representation on which subsequent
behaviors and actions can be based. This paper seeks to understand how such
models behave when input streams present conflicting information. Focusing
specifically on vision-language models, we provide inconsistent inputs (e.g.,
an image of a dog paired with the caption "A photo of a cat") and ask the model
to report the information present in one of the specific modalities (e.g.,
"What does the caption say / What is in the image?"). We find that models often
favor one modality over the other, e.g., reporting the image regardless of what
the caption says, but that different models differ in which modality they
favor. We find evidence that the behaviorally preferred modality is evident in
the internal representational structure of the model, and that specific
attention heads can restructure the representations to favor one modality over
the other. Moreover, we find modality-agnostic "router heads" which appear to
promote answers about the modality requested in the instruction, and which can
be manipulated or transferred in order to improve performance across datasets
and modalities. Together, the work provides essential steps towards identifying
and controlling if and how models detect and resolve conflicting signals within
complex multimodal environments.

</details>


### [129] [The Anatomy of Evidence: An Investigation Into Explainable ICD Coding](https://arxiv.org/abs/2507.01802)
*Katharina Beckh,Elisa Studeny,Sujan Sai Gannamaneni,Dario Antweiler,Stefan Rüping*

Main category: cs.CL

TL;DR: The paper analyzes the MDACE dataset for explainable medical coding, evaluating plausibility of current systems and proposing match measures and recommendations.


<details>
  <summary>Details</summary>
Motivation: To improve transparency in automatic medical coding by evaluating explainability methods, addressing the lack of annotated data for such tasks.

Method: In-depth analysis of the MDACE dataset and plausibility evaluation of explainable medical coding systems, including match measures and case studies.

Result: Ground truth evidence aligns with code descriptions; state-of-the-art approaches show high overlap with ground truth. Success and failure cases are identified.

Conclusion: Recommendations are provided for developing and evaluating explainable medical coding systems, emphasizing transparency and evidence alignment.

Abstract: Automatic medical coding has the potential to ease documentation and billing
processes. For this task, transparency plays an important role for medical
coders and regulatory bodies, which can be achieved using explainability
methods. However, the evaluation of these approaches has been mostly limited to
short text and binary settings due to a scarcity of annotated data. Recent
efforts by Cheng et al. (2023) have introduced the MDACE dataset, which
provides a valuable resource containing code evidence in clinical records. In
this work, we conduct an in-depth analysis of the MDACE dataset and perform
plausibility evaluation of current explainable medical coding systems from an
applied perspective. With this, we contribute to a deeper understanding of
automatic medical coding and evidence extraction. Our findings reveal that
ground truth evidence aligns with code descriptions to a certain degree. An
investigation into state-of-the-art approaches shows a high overlap with ground
truth evidence. We propose match measures and highlight success and failure
cases. Based on our findings, we provide recommendations for developing and
evaluating explainable medical coding systems.

</details>


### [130] [Evaluating Structured Output Robustness of Small Language Models for Open Attribute-Value Extraction from Clinical Notes](https://arxiv.org/abs/2507.01810)
*Nikita Neveditsin,Pawan Lingras,Vijay Mago*

Main category: cs.CL

TL;DR: JSON is the most parseable format for structured outputs in clinical note extraction, with performance influenced by prompting and model size.


<details>
  <summary>Details</summary>
Motivation: To compare the parseability of structured outputs (JSON, YAML, XML) for attribute-value extraction from clinical notes, aiding deployment in privacy-sensitive settings.

Method: Evaluated three serialization formats (JSON, YAML, XML) using small language models, with analysis of structural robustness, prompting effects, and document length.

Result: JSON consistently outperformed YAML and XML in parseability. Performance improved with targeted prompting and larger models but declined for longer documents and specific note types.

Conclusion: JSON is recommended for clinical note extraction due to higher parseability, with considerations for prompting and model size.

Abstract: We present a comparative analysis of the parseability of structured outputs
generated by small language models for open attribute-value extraction from
clinical notes. We evaluate three widely used serialization formats: JSON,
YAML, and XML, and find that JSON consistently yields the highest parseability.
Structural robustness improves with targeted prompting and larger models, but
declines for longer documents and certain note types. Our error analysis
identifies recurring format-specific failure patterns. These findings offer
practical guidance for selecting serialization formats and designing prompts
when deploying language models in privacy-sensitive clinical settings.

</details>


### [131] [Low-Perplexity LLM-Generated Sequences and Where To Find Them](https://arxiv.org/abs/2507.01844)
*Arthur Wuhrmann,Anastasiia Kucherenko,Andrei Kucharavy*

Main category: cs.CL

TL;DR: The paper introduces a method to analyze how LLMs replicate training data by focusing on low-perplexity sequences, revealing unexpected gaps in traceability and quantifying verbatim recall.


<details>
  <summary>Details</summary>
Motivation: To enhance transparency, accountability, privacy, and fairness by understanding how LLMs' training data influences their outputs.

Method: A systematic pipeline extracts and traces low-perplexity sequences (high-probability text spans) back to their training data sources.

Result: Many low-perplexity sequences cannot be traced to the training corpus; for those that can, the study quantifies their distribution and verbatim recall.

Conclusion: The findings improve understanding of LLMs' reliance on training data, aiding efforts to ensure responsible model behavior.

Abstract: As Large Language Models (LLMs) become increasingly widespread, understanding
how specific training data shapes their outputs is crucial for transparency,
accountability, privacy, and fairness. To explore how LLMs leverage and
replicate their training data, we introduce a systematic approach centered on
analyzing low-perplexity sequences - high-probability text spans generated by
the model. Our pipeline reliably extracts such long sequences across diverse
topics while avoiding degeneration, then traces them back to their sources in
the training data. Surprisingly, we find that a substantial portion of these
low-perplexity spans cannot be mapped to the corpus. For those that do match,
we quantify the distribution of occurrences across source documents,
highlighting the scope and nature of verbatim recall and paving a way toward
better understanding of how LLMs training data impacts their behavior.

</details>


### [132] [Eka-Eval : A Comprehensive Evaluation Framework for Large Language Models in Indian Languages](https://arxiv.org/abs/2507.01853)
*Samridhi Raj Sinha,Rajvee Sheth,Abhishek Upperwal,Mayank Singh*

Main category: cs.CL

TL;DR: EKA-EVAL is a unified, production-ready evaluation framework for LLMs, covering 35+ benchmarks (including 10 Indic-specific datasets), with features like distributed inference and multi-GPU support. It aims to lower barriers for multilingual benchmarking.


<details>
  <summary>Details</summary>
Motivation: Address the lack of non-English-centric evaluation frameworks for LLMs, especially for linguistically diverse regions like India.

Method: Integrates diverse benchmarks (reasoning, math, tool use, etc.) with built-in support for distributed inference, quantization, and multi-GPU usage.

Result: EKA-EVAL offers broader coverage than existing tools, positioning it as the first end-to-end, extensible suite for global and Indic LLMs.

Conclusion: The open-source framework is part of the EKA initiative, aiming to expand to 100+ benchmarks and establish a robust multilingual LLM evaluation ecosystem.

Abstract: The rapid advancement of Large Language Models (LLMs) has intensified the
need for evaluation frameworks that go beyond English centric benchmarks and
address the requirements of linguistically diverse regions such as India. We
present EKA-EVAL, a unified and production-ready evaluation framework that
integrates over 35 benchmarks, including 10 Indic-specific datasets, spanning
categories like reasoning, mathematics, tool use, long-context understanding,
and reading comprehension. Compared to existing Indian language evaluation
tools, EKA-EVAL offers broader benchmark coverage, with built-in support for
distributed inference, quantization, and multi-GPU usage. Our systematic
comparison positions EKA-EVAL as the first end-to-end, extensible evaluation
suite tailored for both global and Indic LLMs, significantly lowering the
barrier to multilingual benchmarking. The framework is open-source and publicly
available at https://github.com/lingo-iitgn/ eka-eval and a part of ongoing EKA
initiative (https://eka.soket.ai), which aims to scale up to over 100
benchmarks and establish a robust, multilingual evaluation ecosystem for LLMs.

</details>


### [133] [DIY-MKG: An LLM-Based Polyglot Language Learning System](https://arxiv.org/abs/2507.01872)
*Kenan Tang,Yanhong Li,Yao Qin*

Main category: cs.CL

TL;DR: DIY-MKG is an open-source system for polyglot learners, enabling personalized vocabulary knowledge graphs, adaptive quizzes, and user feedback to address limitations in current language learning tools.


<details>
  <summary>Details</summary>
Motivation: Existing tools lack support for polyglot learners, customization, and suffer from cognitive offloading. DIY-MKG aims to solve these issues.

Method: DIY-MKG uses LLMs to build personalized vocabulary knowledge graphs, offers rich annotations, adaptive quizzes, and user feedback for prompt refinement.

Result: Evaluations show reliable vocabulary expansion and highly accurate quizzes across multiple languages.

Conclusion: DIY-MKG is robust and effective for polyglot language learning, addressing key limitations of existing tools.

Abstract: Existing language learning tools, even those powered by Large Language Models
(LLMs), often lack support for polyglot learners to build linguistic
connections across vocabularies in multiple languages, provide limited
customization for individual learning paces or needs, and suffer from
detrimental cognitive offloading. To address these limitations, we design
Do-It-Yourself Multilingual Knowledge Graph (DIY-MKG), an open-source system
that supports polyglot language learning. DIY-MKG allows the user to build
personalized vocabulary knowledge graphs, which are constructed by selective
expansion with related words suggested by an LLM. The system further enhances
learning through rich annotation capabilities and an adaptive review module
that leverages LLMs for dynamic, personalized quiz generation. In addition,
DIY-MKG allows users to flag incorrect quiz questions, simultaneously
increasing user engagement and providing a feedback loop for prompt refinement.
Our evaluation of LLM-based components in DIY-MKG shows that vocabulary
expansion is reliable and fair across multiple languages, and that the
generated quizzes are highly accurate, validating the robustness of DIY-MKG.

</details>


### [134] [MiCoTA: Bridging the Learnability Gap with Intermediate CoT and Teacher Assistants](https://arxiv.org/abs/2507.01887)
*Dongyi Ding,Tiannan Wang,Chenghao Zhu,Meiling Tao,Yuchen Eleanor Jiang,Wangchunshu Zhou*

Main category: cs.CL

TL;DR: MiCoTA improves small language models' reasoning by using intermediate models and CoT sequences to bridge gaps in capacity and reasoning length.


<details>
  <summary>Details</summary>
Motivation: Address the 'SLMs Learnability Gap' where small language models struggle with long-form reasoning due to limited capacity.

Method: Introduces MiCoTA, a framework using intermediate-sized models as teacher assistants and intermediate-length CoT sequences for distillation.

Result: SLMs show significant reasoning improvements, e.g., Qwen2.5-7B and Qwen2.5-3B models achieve score boosts of 3.47 and 3.93 on benchmarks.

Conclusion: MiCoTA effectively bridges reasoning gaps for SLMs, with insights for future long-CoT data distillation research.

Abstract: Large language models (LLMs) excel at reasoning tasks requiring long thought
sequences for planning, reflection, and refinement. However, their substantial
model size and high computational demands are impractical for widespread
deployment. Yet, small language models (SLMs) often struggle to learn long-form
CoT reasoning due to their limited capacity, a phenomenon we refer to as the
"SLMs Learnability Gap". To address this, we introduce
\textbf{Mi}d-\textbf{Co}T \textbf{T}eacher \textbf{A}ssistant Distillation
(MiCoTAl), a framework for improving long CoT distillation for SLMs. MiCoTA
employs intermediate-sized models as teacher assistants and utilizes
intermediate-length CoT sequences to bridge both the capacity and reasoning
length gaps. Our experiments on downstream tasks demonstrate that although SLMs
distilled from large teachers can perform poorly, by applying MiCoTA, they
achieve significant improvements in reasoning performance. Specifically,
Qwen2.5-7B-Instruct and Qwen2.5-3B-Instruct achieve an improvement of 3.47 and
3.93 respectively on average score on AIME2024, AMC, Olympiad, MATH-500 and
GSM8K benchmarks. To better understand the mechanism behind MiCoTA, we perform
a quantitative experiment demonstrating that our method produces data more
closely aligned with base SLM distributions. Our insights pave the way for
future research into long-CoT data distillation for SLMs.

</details>


### [135] [High-Layer Attention Pruning with Rescaling](https://arxiv.org/abs/2507.01900)
*Songtao Liu,Peng Liu*

Main category: cs.CL

TL;DR: A novel pruning algorithm for LLMs strategically prunes higher-layer attention heads and uses adaptive rescaling to maintain representation quality, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Conventional pruning methods remove attention heads indiscriminately, ignoring their architectural positions, which can harm model performance.

Method: Proposes a pruning algorithm targeting higher-layer attention heads and introduces adaptive rescaling to adjust representation scales post-pruning.

Result: Outperforms existing structured pruning methods, especially in generation tasks, across multiple LLMs and datasets.

Conclusion: The method offers a more effective way to prune LLMs by considering head positions and rescaling representations, improving performance.

Abstract: Pruning is a highly effective approach for compressing large language models
(LLMs), significantly reducing inference latency. However, conventional
training-free structured pruning methods often employ a heuristic metric that
indiscriminately removes some attention heads across all pruning layers,
without considering their positions within the network architecture. In this
work, we propose a novel pruning algorithm that strategically prunes attention
heads in the model's higher layers. Since the removal of attention heads can
alter the magnitude of token representations, we introduce an adaptive
rescaling parameter that calibrates the representation scale post-pruning to
counteract this effect. We conduct comprehensive experiments on a wide range of
LLMs, including LLaMA3.1-8B, Mistral-7B-v0.3, Qwen2-7B, and Gemma2-9B. Our
evaluation includes both generation and discriminative tasks across 27
datasets. The results consistently demonstrate that our method outperforms
existing structured pruning methods. This improvement is particularly notable
in generation tasks, where our approach significantly outperforms existing
baselines.

</details>


### [136] [AI4Research: A Survey of Artificial Intelligence for Scientific Research](https://arxiv.org/abs/2507.01903)
*Qiguang Chen,Mingda Yang,Libo Qin,Jinhao Liu,Zheng Yan,Jiannan Guan,Dengyun Peng,Yiyan Ji,Hanjing Li,Mengkang Hu,Yimeng Zhang,Yihao Liang,Yuhang Zhou,Jiaqi Wang,Zhi Chen,Wanxiang Che*

Main category: cs.CL

TL;DR: The paper presents a comprehensive survey on AI for Research (AI4Research), addressing gaps in understanding and development by introducing a taxonomy, identifying research frontiers, and compiling resources.


<details>
  <summary>Details</summary>
Motivation: Recent advancements in AI, especially LLMs, have shown potential in scientific research, but a lack of a unified survey hinders progress.

Method: The authors introduce a systematic taxonomy for AI4Research tasks, identify research gaps, and compile multidisciplinary resources.

Result: A unified perspective on AI4Research is provided, including a taxonomy, future directions, and a resource compilation.

Conclusion: The work aims to facilitate access to AI4Research resources and inspire innovation in the field.

Abstract: Recent advancements in artificial intelligence (AI), particularly in large
language models (LLMs) such as OpenAI-o1 and DeepSeek-R1, have demonstrated
remarkable capabilities in complex domains such as logical reasoning and
experimental coding. Motivated by these advancements, numerous studies have
explored the application of AI in the innovation process, particularly in the
context of scientific research. These AI technologies primarily aim to develop
systems that can autonomously conduct research processes across a wide range of
scientific disciplines. Despite these significant strides, a comprehensive
survey on AI for Research (AI4Research) remains absent, which hampers our
understanding and impedes further development in this field. To address this
gap, we present a comprehensive survey and offer a unified perspective on
AI4Research. Specifically, the main contributions of our work are as follows:
(1) Systematic taxonomy: We first introduce a systematic taxonomy to classify
five mainstream tasks in AI4Research. (2) New frontiers: Then, we identify key
research gaps and highlight promising future directions, focusing on the rigor
and scalability of automated experiments, as well as the societal impact. (3)
Abundant applications and resources: Finally, we compile a wealth of resources,
including relevant multidisciplinary applications, data corpora, and tools. We
hope our work will provide the research community with quick access to these
resources and stimulate innovative breakthroughs in AI4Research.

</details>


### [137] [Gradient-Adaptive Policy Optimization: Towards Multi-Objective Alignment of Large Language Models](https://arxiv.org/abs/2507.01915)
*Chengao Li,Hanyu Zhang,Yunkun Xu,Hongyan Xue,Xiang Ao,Qing He*

Main category: cs.CL

TL;DR: GAPO is a novel fine-tuning method for aligning LLMs with diverse human preferences using multi-objective optimization, outperforming current methods.


<details>
  <summary>Details</summary>
Motivation: Aligning LLMs with diverse and potentially conflicting human preferences is challenging.

Method: GAPO employs multi-gradient descent to balance conflicting objectives, while P-GAPO incorporates user preferences for Pareto solutions.

Result: GAPO outperforms state-of-the-art methods, achieving better performance in helpfulness and harmlessness on Mistral-7B.

Conclusion: GAPO effectively aligns LLMs with diverse human preferences, offering superior performance and theoretical guarantees for Pareto optimality.

Abstract: Reinforcement Learning from Human Feedback (RLHF) has emerged as a powerful
technique for aligning large language models (LLMs) with human preferences.
However, effectively aligning LLMs with diverse human preferences remains a
significant challenge, particularly when they are conflict. To address this
issue, we frame human value alignment as a multi-objective optimization
problem, aiming to maximize a set of potentially conflicting objectives. We
introduce Gradient-Adaptive Policy Optimization (GAPO), a novel fine-tuning
paradigm that employs multiple-gradient descent to align LLMs with diverse
preference distributions. GAPO adaptively rescales the gradients for each
objective to determine an update direction that optimally balances the
trade-offs between objectives. Additionally, we introduce P-GAPO, which
incorporates user preferences across different objectives and achieves Pareto
solutions that better align with the user's specific needs. Our theoretical
analysis demonstrates that GAPO converges towards a Pareto optimal solution for
multiple objectives. Empirical results on Mistral-7B show that GAPO outperforms
current state-of-the-art methods, achieving superior performance in both
helpfulness and harmlessness.

</details>


### [138] [NaturalThoughts: Selecting and Distilling Reasoning Traces for General Reasoning Tasks](https://arxiv.org/abs/2507.01921)
*Yang Li,Youssef Emad,Karthik Padthe,Jack Lanchantin,Weizhe Yuan,Thao Nguyen,Jason Weston,Shang-Wen Li,Dong Wang,Ilia Kulikov,Xian Li*

Main category: cs.CL

TL;DR: Distilling high-quality reasoning traces ('NaturalThoughts') from a teacher model improves student model reasoning, outperforming random sampling and existing datasets.


<details>
  <summary>Details</summary>
Motivation: To systematically study effective reasoning demonstrations from a teacher model for improving student model reasoning.

Method: Curate 'NaturalThoughts' by selecting reasoning traces from a teacher model, analyze factors like sample efficiency and scalability, and evaluate on Llama and Qwen models.

Result: NaturalThoughts outperforms random sampling and existing datasets (OpenThoughts, LIMO) on STEM benchmarks like GPQA-Diamond, MMLU-Pro, and SuperGPQA.

Conclusion: Selecting difficult, diverse reasoning examples is more sample-efficient for transferring teacher model skills, leading to better performance.

Abstract: Recent work has shown that distilling reasoning traces from a larger teacher
model via supervised finetuning outperforms reinforcement learning with the
smaller student model alone (Guo et al. 2025). However, there has not been a
systematic study of what kind of reasoning demonstrations from the teacher are
most effective in improving the student model's reasoning capabilities. In this
work we curate high-quality "NaturalThoughts" by selecting reasoning traces
from a strong teacher model based on a large pool of questions from
NaturalReasoning (Yuan et al. 2025). We first conduct a systematic analysis of
factors that affect distilling reasoning capabilities, in terms of sample
efficiency and scalability for general reasoning tasks. We observe that simply
scaling up data size with random sampling is a strong baseline with steady
performance gains. Further, we find that selecting difficult examples that
require more diverse reasoning strategies is more sample-efficient to transfer
the teacher model's reasoning skills. Evaluated on both Llama and Qwen models,
training with NaturalThoughts outperforms existing reasoning datasets such as
OpenThoughts, LIMO, etc. on general STEM reasoning benchmarks including
GPQA-Diamond, MMLU-Pro and SuperGPQA.

</details>


### [139] [Decision-oriented Text Evaluation](https://arxiv.org/abs/2507.01923)
*Yu-Shiang Huang,Chuan-Ju Wang,Chung-Chi Chen*

Main category: cs.CL

TL;DR: The paper proposes a decision-oriented framework for evaluating NLG by measuring its impact on human and LLM decision outcomes, showing traditional metrics fall short.


<details>
  <summary>Details</summary>
Motivation: Current intrinsic NLG evaluation methods poorly correlate with real-world decision-making efficacy, necessitating a more practical approach.

Method: A framework assessing decision quality via financial trades by humans and LLMs, using market digest texts (summaries and analyses) as test cases.

Result: Neither humans nor LLMs outperform random baselines with summaries alone, but human-LLM collaboration with richer analyses excels.

Conclusion: Decision-oriented evaluation reveals NLG's real-world impact, emphasizing the need for metrics beyond traditional intrinsic ones.

Abstract: Natural language generation (NLG) is increasingly deployed in high-stakes
domains, yet common intrinsic evaluation methods, such as n-gram overlap or
sentence plausibility, weakly correlate with actual decision-making efficacy.
We propose a decision-oriented framework for evaluating generated text by
directly measuring its influence on human and large language model (LLM)
decision outcomes. Using market digest texts--including objective morning
summaries and subjective closing-bell analyses--as test cases, we assess
decision quality based on the financial performance of trades executed by human
investors and autonomous LLM agents informed exclusively by these texts. Our
findings reveal that neither humans nor LLM agents consistently surpass random
performance when relying solely on summaries. However, richer analytical
commentaries enable collaborative human-LLM teams to outperform individual
human or agent baselines significantly. Our approach underscores the importance
of evaluating generated text by its ability to facilitate synergistic
decision-making between humans and LLMs, highlighting critical limitations of
traditional intrinsic metrics.

</details>


### [140] [Adaptability of ASR Models on Low-Resource Language: A Comparative Study of Whisper and Wav2Vec-BERT on Bangla](https://arxiv.org/abs/2507.01931)
*Md Sazzadul Islam Ridoy,Sumi Akter,Md. Aminur Rahman*

Main category: cs.CL

TL;DR: The study compares OpenAI's Whisper and Facebook's Wav2Vec-BERT for Bangla ASR, finding Wav2Vec-BERT superior in performance and efficiency.


<details>
  <summary>Details</summary>
Motivation: To evaluate state-of-the-art ASR models for low-resource languages like Bangla.

Method: Fine-tuning and optimizing hyperparameters (learning rate, epochs, checkpoint selection) on Mozilla Common Voice-17 and OpenSLR datasets.

Result: Wav2Vec-BERT outperformed Whisper in WER, CER, training time, and computational efficiency.

Conclusion: Wav2Vec-BERT is more effective for low-resource ASR tasks, offering insights for robust system development.

Abstract: In recent years, neural models trained on large multilingual text and speech
datasets have shown great potential for supporting low-resource languages. This
study investigates the performances of two state-of-the-art Automatic Speech
Recognition (ASR) models, OpenAI's Whisper (Small & Large-V2) and Facebook's
Wav2Vec-BERT on Bangla, a low-resource language. We have conducted experiments
using two publicly available datasets: Mozilla Common Voice-17 and OpenSLR to
evaluate model performances. Through systematic fine-tuning and hyperparameter
optimization, including learning rate, epochs, and model checkpoint selection,
we have compared the models based on Word Error Rate (WER), Character Error
Rate (CER), Training Time, and Computational Efficiency. The Wav2Vec-BERT model
outperformed Whisper across all key evaluation metrics, demonstrated superior
performance while requiring fewer computational resources, and offered valuable
insights to develop robust speech recognition systems in low-resource
linguistic settings.

</details>


### [141] [The Thin Line Between Comprehension and Persuasion in LLMs](https://arxiv.org/abs/2507.01936)
*Adrian de Wynter,Tangming Yuan*

Main category: cs.CL

TL;DR: LLMs excel in persuasive debates but lack deeper comprehension of dialogue structures, impacting their reliability as evaluators.


<details>
  <summary>Details</summary>
Motivation: To examine LLMs' debate capabilities and their understanding of dialogue, given their increasing use in sensitive applications.

Method: Evaluated LLMs' debate performance and measured their comprehension of dialogical structures and pragmatic context.

Result: LLMs maintain coherent debates and influence beliefs, but fail to demonstrate deeper understanding of dialogue. Awareness of AI involvement increases critical scrutiny.

Conclusion: LLMs' effectiveness in dialogue doesn't require deep comprehension, suggesting pragmatic context is secondary to persuasive performance.

Abstract: Large language models (LLMs) are excellent at maintaining high-level,
convincing dialogues. They are being fast deployed as chatbots and evaluators
in sensitive areas, such as peer review and mental health applications. This,
along with the disparate accounts on their reasoning capabilities, calls for a
closer examination of LLMs and their comprehension of dialogue. In this work we
begin by evaluating LLMs' ability to maintain a debate--one of the purest yet
most complex forms of human communication. Then we measure how this capability
relates to their understanding of what is being talked about, namely, their
comprehension of dialogical structures and the pragmatic context. We find that
LLMs are capable of maintaining coherent, persuasive debates, often swaying the
beliefs of participants and audiences alike. We also note that awareness or
suspicion of AI involvement encourage people to be more critical of the
arguments made. When polling LLMs on their comprehension of deeper structures
of dialogue, however, they cannot demonstrate said understanding. Our findings
tie the shortcomings of LLMs-as-evaluators to their (in)ability to understand
the context. More broadly, for the field of argumentation theory we posit that,
if an agent can convincingly maintain a dialogue, it is not necessary for it to
know what it is talking about. Hence, the modelling of pragmatic context and
coherence are secondary to effectiveness.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [142] [Few-Shot Inspired Generative Zero-Shot Learning](https://arxiv.org/abs/2507.01026)
*Md Shakil Ahamed Shohag,Q. M. Jonathan Wu,Farhad Pourpanah*

Main category: cs.LG

TL;DR: FSIGenZ is a few-shot-inspired generative ZSL framework that reduces reliance on large-scale feature synthesis by dynamically re-scoring class attributes and using group-level prototypes.


<details>
  <summary>Details</summary>
Motivation: Traditional ZSL methods require extensive synthetic data and computational resources, relaxing ZSL assumptions. FSIGenZ addresses this by leveraging instance-level attribute variability.

Method: Introduces Model-Specific Attribute Scoring (MSAS) to dynamically adjust class attributes and estimates group-level prototypes. Uses Dual-Purpose Semantic Regularization (DPSR) and a semantic-aware contrastive classifier (SCC).

Result: Achieves competitive performance on SUN, AwA2, and CUB benchmarks with fewer synthetic features.

Conclusion: FSIGenZ offers a resource-efficient alternative to traditional ZSL methods while maintaining performance.

Abstract: Generative zero-shot learning (ZSL) methods typically synthesize visual
features for unseen classes using predefined semantic attributes, followed by
training a fully supervised classification model. While effective, these
methods require substantial computational resources and extensive synthetic
data, thereby relaxing the original ZSL assumptions. In this paper, we propose
FSIGenZ, a few-shot-inspired generative ZSL framework that reduces reliance on
large-scale feature synthesis. Our key insight is that class-level attributes
exhibit instance-level variability, i.e., some attributes may be absent or
partially visible, yet conventional ZSL methods treat them as uniformly
present. To address this, we introduce Model-Specific Attribute Scoring (MSAS),
which dynamically re-scores class attributes based on model-specific
optimization to approximate instance-level variability without access to unseen
data. We further estimate group-level prototypes as clusters of instances based
on MSAS-adjusted attribute scores, which serve as representative synthetic
features for each unseen class. To mitigate the resulting data imbalance, we
introduce a Dual-Purpose Semantic Regularization (DPSR) strategy while training
a semantic-aware contrastive classifier (SCC) using these prototypes.
Experiments on SUN, AwA2, and CUB benchmarks demonstrate that FSIGenZ achieves
competitive performance using far fewer synthetic features.

</details>


### [143] [DBellQuant: Breaking the Bell with Double-Bell Transformation for LLMs Post Training Binarization](https://arxiv.org/abs/2507.01027)
*Zijian Ye,Wei Huang,Yifei Yu,Tianhe Ren,Zhongrui Wang,Xiaojuan Qi*

Main category: cs.LG

TL;DR: DBellQuant is a post-training quantization framework that compresses LLMs to nearly 1-bit weights and 6-bit activations with minimal performance loss, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Address computational and memory challenges in LLMs by reducing quantization errors from non-friendly weight distributions and activation outliers.

Method: Uses Learnable Transformation for Dual-Bell (LTDB) to transform weight distributions into dual-bell forms and smooth activations.

Result: Achieves perplexity of 14.39 on LLaMA2-13B with 6-bit activation quantization, outperforming BiLLM's 21.35.

Conclusion: DBellQuant sets a new state-of-the-art for LLM compression, enabling practical deployment with minimal performance degradation.

Abstract: Large language models (LLMs) demonstrate remarkable performance but face
substantial computational and memory challenges that limit their practical
deployment. Quantization has emerged as a promising solution; however, its
effectiveness is often limited by quantization errors arising from weight
distributions that are not quantization-friendly and the presence of activation
outliers. To address these challenges, we introduce DBellQuant, an innovative
post-training quantization (PTQ) framework that achieves nearly 1-bit weight
compression and 6-bit activation quantization with minimal performance
degradation. DBellQuant uses Learnable Transformation for Dual-Bell (LTDB)
algorithm, which transforms single-bell weight distributions into dual-bell
forms to reduce binarization errors and applies inverse transformations to
smooth activations. DBellQuant sets a new state-of-the-art by preserving
superior model performance under aggressive weight and activation quantization.
For example, on the Wikitext2 dataset, DBellQuant achieves a perplexity of
14.39 on LLaMA2-13B with 6-bit activation quantization, significantly
outperforming BiLLM's 21.35 without activation quantization, underscoring its
potential in compressing LLMs for real-world applications.

</details>


### [144] [Dual Perspectives on Non-Contrastive Self-Supervised Learning](https://arxiv.org/abs/2507.01028)
*Jean Ponce,Martial Hebert,Basile Terver*

Main category: cs.LG

TL;DR: The paper analyzes non-contrastive self-supervised learning methods, focusing on stop gradient and exponential moving average techniques to prevent collapse. It provides theoretical insights from optimization and dynamical systems perspectives.


<details>
  <summary>Details</summary>
Motivation: To understand why stop gradient and exponential moving average procedures prevent representation collapse in non-contrastive self-supervised learning, despite not optimizing the original objective.

Method: Theoretical analysis using optimization and dynamical systems perspectives, examining the linear case and stability of equilibria.

Result: Stop gradient and exponential moving average avoid collapse, even without optimizing the original objective. In the linear case, minimizing the original objective without these procedures leads to collapse.

Conclusion: The procedures' limit points are asymptotically stable equilibria, ensuring no trivial solutions, providing theoretical justification for their empirical success.

Abstract: The objective of non-contrastive approaches to self-supervised learning is to
train on pairs of different views of the data an encoder and a predictor that
minimize the mean discrepancy between the code predicted from the embedding of
the first view and the embedding of the second one. In this setting, the stop
gradient and exponential moving average iterative procedures are commonly used
to avoid representation collapse, with excellent performance in downstream
supervised applications. This presentation investigates these procedures from
the dual theoretical viewpoints of optimization and dynamical systems. We first
show that, in general, although they do not optimize the original objective, or
for that matter, any other smooth function, they do avoid collapse. Following
Tian et al. [2021], but without any of the extra assumptions used in their
proofs, we then show using a dynamical system perspective that, in the linear
case, minimizing the original objective function without the use of a stop
gradient or exponential moving average always leads to collapse. Conversely, we
finally show that the limit points of the dynamical systems associated with
these two procedures are, in general, asymptotically stable equilibria, with no
risk of degenerating to trivial solutions.

</details>


### [145] [PathCoT: Chain-of-Thought Prompting for Zero-shot Pathology Visual Reasoning](https://arxiv.org/abs/2507.01029)
*Junjie Zhou,Yingli Zuo,Shichang Feng,Peng Wan,Qi Zhu,Daoqiang Zhang,Wei Shao*

Main category: cs.LG

TL;DR: PathCoT improves pathology visual reasoning in MLLMs by integrating expert knowledge and self-evaluation to reduce errors and hallucinations.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs struggle with pathology tasks due to lack of domain knowledge and errors in CoT reasoning.

Method: PathCoT uses zero-shot CoT prompting with pathology expert knowledge and self-evaluation to refine answers.

Result: PathCoT outperforms on the PathMMU dataset, enhancing pathology visual understanding.

Conclusion: Integrating expert knowledge and self-evaluation in CoT improves MLLM performance in pathology tasks.

Abstract: With the development of generative artificial intelligence and instruction
tuning techniques, multimodal large language models (MLLMs) have made
impressive progress on general reasoning tasks. Benefiting from the
chain-of-thought (CoT) methodology, MLLMs can solve the visual reasoning
problem step-by-step. However, existing MLLMs still face significant challenges
when applied to pathology visual reasoning tasks: (1) LLMs often underperforms
because they lack domain-specific information, which can lead to model
hallucinations. (2) The additional reasoning steps in CoT may introduce errors,
leading to the divergence of answers. To address these limitations, we propose
PathCoT, a novel zero-shot CoT prompting method which integrates the pathology
expert-knowledge into the reasoning process of MLLMs and incorporates
self-evaluation to mitigate divergence of answers. Specifically, PathCoT guides
the MLLM with prior knowledge to perform as pathology experts, and provides
comprehensive analysis of the image with their domain-specific knowledge. By
incorporating the experts' knowledge, PathCoT can obtain the answers with CoT
reasoning. Furthermore, PathCoT incorporates a self-evaluation step that
assesses both the results generated directly by MLLMs and those derived through
CoT, finally determining the reliable answer. The experimental results on the
PathMMU dataset demonstrate the effectiveness of our method on pathology visual
understanding and reasoning.

</details>


### [146] [Optimizing Flamelet Generated Manifold Models: A Machine Learning Performance Study](https://arxiv.org/abs/2507.01030)
*Reza Lotfi Navaei,Mohammad Safarzadeh,Seyed Mohammad Jafar Sobhani*

Main category: cs.LG

TL;DR: The paper develops Flamelet Generated Manifold (FGM) libraries for methane combustion using machine learning, achieving high accuracy with an optimized MLP model.


<details>
  <summary>Details</summary>
Motivation: FGM's high memory usage and fuel-specific limitations drive the need for efficient, adaptable libraries using machine learning.

Method: Four ML algorithms (MLP, Random Forest, Linear Regression, SVM) were tested to regenerate FGM libraries. MLP was optimized with hyperparameter tuning.

Result: The best model (MLP with four hidden layers) achieved 99.81% accuracy and a 2.30% error rate.

Conclusion: MLP is the optimal method for FGM library generation, offering high accuracy and efficiency for methane combustion simulations.

Abstract: In chemistry tabulations and Flamelet combustion models, the Flamelet
Generated Manifold (FGM) is recognized for its precision and physical
representation. The practical implementation of FGM requires a significant
allocation of memory resources. FGM libraries are developed specifically for a
specific fuel and subsequently utilized for all numerical problems using
machine learning techniques. This research aims to develop libraries of Laminar
FGM utilizing machine learning algorithms for application in combustion
simulations of methane fuel. This study employs four Machine Learning
algorithms to regenerate Flamelet libraries, based on an understanding of data
sources, techniques, and data-driven concepts. 1. Multi-Layer Perceptron; 2.
Random Forest; 3. Linear Regression; 4. Support Vector Machine. Seven libraries
were identified as appropriate for constructing a database for training machine
learning models, giving an error rate of 2.30%. The default architectures of
each method were evaluated to determine the optimal approach, leading to the
selection of the MLP method as the primary choice. The method was enhanced
through hyperparameter tuning to improve accuracy. The quantity of hidden
layers and neurons significantly influences method performance. The optimal
model, comprising four hidden layers with 10, 15, 20, and 25 neurons
respectively, achieved an accuracy of 99.81%.

</details>


### [147] [PyTorch-based Geometric Learning with Non-CUDA Processing Units: Experiences from Intel Gaudi-v2 HPUs](https://arxiv.org/abs/2507.01031)
*Fanchen Bu,Kijung Shin*

Main category: cs.LG

TL;DR: The paper discusses porting PyTorch-based geometric learning frameworks to Intel's Gaudi-v2 HPUs, addressing challenges and providing tools for non-CUDA hardware.


<details>
  <summary>Details</summary>
Motivation: To enable geometric learning on non-CUDA hardware like Intel's Gaudi-v2 HPUs, reducing engineering effort and expanding accessibility.

Method: Developed core utilities for essential operations (e.g., scatter, sparse indexing) and provided tutorials/examples with diagnostic analyses.

Result: Created a GitHub repository with tools and resources, lowering barriers for researchers to use non-CUDA hardware.

Conclusion: The work facilitates geometric learning on alternative hardware, supporting further optimization and cross-platform portability.

Abstract: Geometric learning has emerged as a powerful paradigm for modeling
non-Euclidean data, especially graph-structured ones, with applications
spanning social networks, molecular structures, knowledge graphs, and
recommender systems. While Nvidia's CUDA-enabled graphics processing units
(GPUs) largely dominate the hardware landscape, emerging accelerators such as
Intel's Gaudi Habana Processing Units (HPUs) offer competitive performance and
energy efficiency. However, the usage of such non-CUDA processing units
requires significant engineering effort and novel software adaptations. In this
work, we present our experiences porting PyTorch-based geometric learning
frameworks to Gaudi-v2 HPUs. We introduce a collection of core utilities that
restore essential operations (e.g., scatter, sparse indexing, k-nearest
neighbors) on Gaudi-v2 HPUs, and we consolidate sixteen guided tutorials and
eleven real-world examples with diagnostic analyses of encountered failures and
detailed workarounds. We collect all our experiences into a publicly accessible
GitHub repository. Our contributions lower the barrier for researchers to
experiment with geometric-learning algorithms and models on non-CUDA hardware,
providing a foundation for further optimization and cross-platform portability.

</details>


### [148] [An Uncertainty-Aware Dynamic Decision Framework for Progressive Multi-Omics Integration in Classification Tasks](https://arxiv.org/abs/2507.01032)
*Nan Mu,Hongbo Yang,Chen Zhao*

Main category: cs.LG

TL;DR: Proposes an uncertainty-aware, multi-view dynamic decision framework for omics data classification to reduce costs while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: High costs and resource inefficiency in multi-omics profiling necessitate a cost-effective, accurate diagnostic approach.

Method: Uses neural networks with refined activation functions for Dirichlet distribution parameters, integrates modalities via Dempster-Shafer theory, and employs dynamic decision-making.

Result: Achieves accurate classification with fewer omics modalities in 50%+ cases, reducing redundant testing while matching full-omics performance.

Conclusion: The framework balances cost and accuracy, preserving diagnostic reliability and biological insights.

Abstract: Background and Objective: High-throughput multi-omics technologies have
proven invaluable for elucidating disease mechanisms and enabling early
diagnosis. However, the high cost of multi-omics profiling imposes a
significant economic burden, with over reliance on full omics data potentially
leading to unnecessary resource consumption. To address these issues, we
propose an uncertainty-aware, multi-view dynamic decision framework for omics
data classification that aims to achieve high diagnostic accuracy while
minimizing testing costs. Methodology: At the single-omics level, we refine the
activation functions of neural networks to generate Dirichlet distribution
parameters, utilizing subjective logic to quantify both the belief masses and
uncertainty mass of classification results. Belief mass reflects the support of
a specific omics modality for a disease class, while the uncertainty parameter
captures limitations in data quality and model discriminability, providing a
more trustworthy basis for decision-making. At the multi omics level, we employ
a fusion strategy based on Dempster-Shafer theory to integrate heterogeneous
modalities, leveraging their complementarity to boost diagnostic accuracy and
robustness. A dynamic decision mechanism is then applied that omics data are
incrementally introduced for each patient until either all data sources are
utilized or the model confidence exceeds a predefined threshold, potentially
before all data sources are utilized. Results and Conclusion: We evaluate our
approach on four benchmark multi-omics datasets, ROSMAP, LGG, BRCA, and KIPAN.
In three datasets, over 50% of cases achieved accurate classification using a
single omics modality, effectively reducing redundant testing. Meanwhile, our
method maintains diagnostic performance comparable to full-omics models and
preserves essential biological insights.

</details>


### [149] [Data-driven Insights for Informed Decision-Making: Applying LSTM Networks for Robust Electricity Forecasting in Libya](https://arxiv.org/abs/2507.01034)
*Asma Agaal,Mansour Essgaer,Hend M. Farkash,Zulaiha Ali Othman*

Main category: cs.LG

TL;DR: The paper proposes a data-driven approach using LSTM to forecast electricity load, generation, and deficits in Benghazi, Libya, outperforming other models like ARIMA and XGBoost.


<details>
  <summary>Details</summary>
Motivation: Accurate electricity forecasting is vital for grid stability and planning in Benghazi, Libya, due to frequent load shedding and infrastructure issues.

Method: Multiple time series models (ARIMA, SARIMA, XGBoost, LSTM) were applied to historical data (2019, 2023), enhanced with preprocessing techniques. LSTM was optimized with exogenous factors like temperature.

Result: LSTM outperformed other models in forecasting, handling non-stationary and seasonal patterns effectively.

Conclusion: The optimized LSTM framework provides actionable insights for policymakers and grid operators in volatile, data-scarce regions.

Abstract: Accurate electricity forecasting is crucial for grid stability and energy
planning, especially in Benghazi, Libya, where frequent load shedding,
generation deficits, and infrastructure limitations persist. This study
proposes a data-driven approach to forecast electricity load, generation, and
deficits for 2025 using historical data from 2019 (a year marked by
instability) and 2023 (a more stable year). Multiple time series models were
applied, including ARIMA, seasonal ARIMA, dynamic regression ARIMA, exponential
smoothing, extreme gradient boosting, and Long Short-Term Memory (LSTM) neural
networks. The dataset was enhanced through missing value imputation, outlier
smoothing, and log transformation. Performance was assessed using mean squared
error, root mean squared error, mean absolute error, and mean absolute
percentage error. LSTM outperformed all other models, showing strong
capabilities in modeling non-stationary and seasonal patterns. A key
contribution of this work is an optimized LSTM framework that integrates
exogenous factors such as temperature and humidity, offering robust performance
in forecasting multiple electricity indicators. These results provide practical
insights for policymakers and grid operators to enable proactive load
management and resource planning in data-scarce, volatile regions.

</details>


### [150] [Research on Low-Latency Inference and Training Efficiency Optimization for Graph Neural Network and Large Language Model-Based Recommendation Systems](https://arxiv.org/abs/2507.01035)
*Yushang Zhao,Haotian Lyu,Yike Peng,Aijia Sun,Feng Jiang,Xinyue Han*

Main category: cs.LG

TL;DR: The paper proposes a hybrid GNN-LLM recommender system optimized for speed and efficiency using quantization, LoRA, distillation, FPGA, and DeepSpeed, achieving higher accuracy and reduced training time.


<details>
  <summary>Details</summary>
Motivation: Address computational bottlenecks in hybrid GNN-LLM recommender systems to improve real-time performance and efficiency.

Method: Hybrid GNN-LLM architecture with optimization strategies (quantization, LoRA, distillation) and hardware acceleration (FPGA, DeepSpeed).

Result: Optimal configuration achieved 13.6% higher accuracy (NDCG@10: 0.75) at 40-60ms latency; LoRA reduced training time by 66% (3.8 hours).

Conclusion: Hardware-software co-design and parameter-efficient tuning enable hybrid models to outperform standalone GNN or LLM approaches, recommending FPGA and LoRA for real-time deployment. Future work includes federated learning and advanced fusion architectures.

Abstract: The incessant advent of online services demands high speed and efficient
recommender systems (ReS) that can maintain real-time performance along with
processing very complex user-item interactions. The present study, therefore,
considers computational bottlenecks involved in hybrid Graph Neural Network
(GNN) and Large Language Model (LLM)-based ReS with the aim optimizing their
inference latency and training efficiency. An extensive methodology was used:
hybrid GNN-LLM integrated architecture-optimization strategies(quantization,
LoRA, distillation)-hardware acceleration (FPGA, DeepSpeed)-all under R 4.4.2.
Experimental improvements were significant, with the optimal Hybrid + FPGA +
DeepSpeed configuration reaching 13.6% more accuracy (NDCG@10: 0.75) at 40-60ms
of latency, while LoRA brought down training time by 66% (3.8 hours) in
comparison to the non-optimized baseline. Irrespective of domain, such as
accuracy or efficiency, it can be established that hardware-software co-design
and parameter-efficient tuning permit hybrid models to outperform GNN or LLM
approaches implemented independently. It recommends the use of FPGA as well as
LoRA for real-time deployment. Future work should involve federated learning
along with advanced fusion architectures for better scalability and privacy
preservation. Thus, this research marks the fundamental groundwork concerning
next-generation ReS balancing low-latency response with cutting-edge
personalization.

</details>


### [151] [Learning to Segment for Vehicle Routing Problems](https://arxiv.org/abs/2507.01037)
*Wenbin Ouyang,Sirui Li,Yining Ma,Cathy Wu*

Main category: cs.LG

TL;DR: The paper introduces FSTA decomposition and L2Seg to accelerate iterative solvers for VRPs by focusing on unstable portions of solutions, achieving up to 7x speedup.


<details>
  <summary>Details</summary>
Motivation: Redundant computations in iterative solvers due to stable solution segments in large-scale VRPs.

Method: FSTA preserves stable segments, aggregates them into hypernodes, and focuses search on unstable parts. L2Seg, a neural framework, identifies stable/unstable segments with three variants (non-autoregressive, autoregressive, and their synergy).

Result: L2Seg accelerates solvers by up to 7x on CVRP and VRPTW, with the synergy variant performing best.

Conclusion: L2Seg is a flexible, compatible framework for various VRPs, combining global and local insights for efficient solving.

Abstract: Iterative search heuristics are widely recognized as state-of-the-art for
solving Vehicle Routing Problems (VRPs). In this work, we identify and exploit
a critical observation: within these solvers, a large portion of the solution
remains stable, i.e., unchanged across search iterations, causing redundant
computations, especially for large-scale VRPs with long subtours. To address
this, we pioneer the formal study of the First-Segment-Then-Aggregate (FSTA)
decomposition technique to accelerate iterative solvers. Specifically, FSTA
preserves stable solution segments during the search, aggregates nodes within
each segment into fixed hypernodes, and focuses the search only on unstable
portions. Yet, a key challenge lies in identifying which segments should be
aggregated by FSTA. To this end, we then introduce Learning-to-Segment (L2Seg),
a novel neural framework to intelligently differentiate potentially stable and
unstable portions for FSTA decomposition. We present three L2Seg variants:
non-autoregressive (globally comprehensive but locally indiscriminate),
autoregressive (locally refined but globally deficient), and their synergy,
with bespoke training and inference strategies. Empirical results on CVRP and
VRPTW suggest that L2Seg accelerates state-of-the-art iterative solvers by up
to 7x. Additionally, we provide in-depth analysis showing NAR and AR synergy
achieves best performance by combining their complementary strengths. Notably,
L2Seg is a flexible framework that is compatible with traditional,
learning-based, and hybrid solvers, while supporting a broad class of VRPs.

</details>


### [152] [On-Policy Optimization of ANFIS Policies Using Proximal Policy Optimization](https://arxiv.org/abs/2507.01039)
*Kaaustaaub Shankar,Wilhelm Louw,Kelly Cohen*

Main category: cs.LG

TL;DR: A reinforcement learning approach using PPO trains neuro-fuzzy controllers, outperforming DQN-based methods in stability and convergence.


<details>
  <summary>Details</summary>
Motivation: To improve training stability and convergence for neuro-fuzzy controllers in RL tasks by replacing off-policy DQN with on-policy PPO.

Method: Uses Proximal Policy Optimization (PPO) in an actor-critic loop to train neuro-fuzzy controllers, evaluated in the CartPole-v1 environment.

Result: PPO-trained agents achieved a mean return of 500 +/- 0 with less variance and faster convergence than DQN-based methods.

Conclusion: PPO is a promising method for training explainable neuro-fuzzy controllers in RL.

Abstract: We propose a reinforcement learning (RL) approach for training neuro-fuzzy
controllers using Proximal Policy Optimization (PPO). Building on prior work
that applied Deep Q-Learning to Adaptive Neuro-Fuzzy Inference Systems (ANFIS),
our method replaces the off-policy value-based framework with a stable
on-policy actor-critic loop. We evaluate this approach in the CartPole-v1
environment using multiple random seeds and compare its learning performance
against ANFIS-Deep Q-Network (DQN) baselines. It was found that PPO-trained
fuzzy agents achieved a mean return of 500 +/- 0 on CartPole-v1 after 20000
updates, showcasing less variance than prior DQN-based methods during training
and overall faster convergence. These findings suggest that PPO offers a
promising pathway for training explainable neuro-fuzzy controllers in
reinforcement learning tasks.

</details>


### [153] [Fast Clifford Neural Layers](https://arxiv.org/abs/2507.01040)
*Tianxiang Xia,Max Neuwinger,Lin Xiao*

Main category: cs.LG

TL;DR: Clifford Neural Layers enhance PDE modeling using Clifford Algebra, optimizing 2/3D Clifford convolutional and multivector activation layers for CPU performance, achieving 30% faster inference than PyTorch.


<details>
  <summary>Details</summary>
Motivation: To improve PDE modeling by integrating Clifford Algebra into neural networks and optimizing performance for CPU inference.

Method: Focus on optimizing 2/3D Clifford convolutional layers and multivector activation layers for CPU performance, testing on real network blocks.

Result: Implementation is 30% faster than standard PyTorch for large data and network sizes (>L2 cache).

Conclusion: The optimized Clifford Neural Layers offer significant performance gains for PDE modeling, with open-sourced code available.

Abstract: Clifford Neural Layers improve PDE modeling by introducing Clifford Algebra
into neural networks. In this project we focus on optimizing the inference of
2/3D Clifford convolutional layers and multivector activation layers for one
core CPU performance.
  Overall, by testing on a real network block involving Clifford convolutional
layers and multivector activation layers, we observe that our implementation is
30% faster than standard PyTorch implementation in relatively large data +
network size (>L2 cache).
  We open source our code base at
https://github.com/egretwAlker/c-opt-clifford-layers

</details>


### [154] [Fast AI Model Splitting over Edge Networks](https://arxiv.org/abs/2507.01041)
*Zuguang Li,Wen Wu,Shaohua Wu,Songge Zhang,Ye Wang,Xuemin,Shen*

Main category: cs.LG

TL;DR: The paper proposes DAG-based and block-wise algorithms for optimal model splitting in split learning, reducing computational complexity and training delay.


<details>
  <summary>Details</summary>
Motivation: Complex AI models in split learning require efficient model splitting to reduce computational workloads.

Method: Represent AI models as DAGs, reformulate splitting as a minimum s-t cut problem, and propose fast DAG-based and block-wise algorithms.

Result: Algorithms achieve optimal splitting in milliseconds and reduce training delay by 24.62%-38.95%.

Conclusion: The proposed methods efficiently solve model splitting, enhancing split learning performance.

Abstract: Split learning (SL) has emerged as a computationally efficient approach for
artificial intelligence (AI) model training, which can alleviate device-side
computational workloads. However, complex AI model architectures pose high
computational complexity to obtain the optimal model splitting. In this paper,
we represent an arbitrary AI model as a directed acyclic graph (DAG), and then
reformulate the optimal model splitting problem as a minimum s-t cut search
problem. To solve the problem, we propose a fast DAG-based model splitting
algorithm, which restructures the DAG to enable the optimal model splitting
identification via a maximum flow method. Theoretical analysis indicates that
the proposed algorithm is optimal. Furthermore, considering AI models with
block structures, we propose a block-wise model splitting algorithm to reduce
computational complexity. The algorithm abstracts each block, i.e., a component
consisting of multiple layers, into a single vertex, thereby obtaining the
optimal model splitting via a simplified DAG. Extensive experimental results
demonstrate that the proposed algorithms can determine the optimal model
splitting within milliseconds, as well as reduce training delay by
24.62%-38.95% in dynamic edge networks as compared to the state-of-the-art
benchmarks.

</details>


### [155] [Data Classification with Dynamically Growing and Shrinking Neural Networks](https://arxiv.org/abs/2507.01043)
*Szymon Świderski,Agnieszka Jastrzębska*

Main category: cs.LG

TL;DR: A method for dynamically adjusting neural network architectures during training using Monte Carlo Tree Search, validated on visual and time series datasets.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of fixed neural network architectures by enabling dynamic adaptation for optimal performance.

Method: Uses Monte Carlo Tree Search to simulate and evaluate candidate architecture changes, allowing dynamic growing and shrinking of the network during training.

Result: Demonstrated effectiveness, especially in multivariate time series classification, due to dynamic adaptability.

Conclusion: The method shows robustness and adaptability, with promising performance in experimental evaluations.

Abstract: The issue of data-driven neural network model construction is one of the core
problems in the domain of Artificial Intelligence. A standard approach assumes
a fixed architecture with trainable weights. A conceptually more advanced
assumption is that we not only train the weights, but also find out the optimal
model architecture. We present a new method that realizes just that. This
article is an extended version of our conference paper titled "Dynamic Growing
and Shrinking of Neural Networks with Monte Carlo Tree Search [26]". In the
paper, we show in detail how to create a neural network with a procedure that
allows dynamic shrinking and growing of the model while it is being trained.
The decision-making mechanism for the architectural design is governed by a
Monte Carlo tree search procedure which simulates network behavior and allows
to compare several candidate architecture changes to choose the best one. The
proposed method was validated using both visual and time series datasets,
demonstrating its particular effectiveness in multivariate time series
classification. This is attributed to the architecture's ability to adapt
dynamically, allowing independent modifications for each time series. The
approach is supplemented by Python source code for reproducibility.
Experimental evaluations in visual pattern and multivariate time series
classification tasks revealed highly promising performance, underscoring the
method's robustness and adaptability.

</details>


### [156] [Sensing Cardiac Health Across Scenarios and Devices: A Multi-Modal Foundation Model Pretrained on Heterogeneous Data from 1.7 Million Individuals](https://arxiv.org/abs/2507.01045)
*Xiao Gu,Wei Tang,Jinpei Han,Veer Sangha,Fenglin Liu,Shreyank N Gowda,Antonio H. Ribeiro,Patrick Schwab,Kim Branson,Lei Clifton,Antonio Luiz P. Ribeiro,Zhangdaihong Liu,David A. Clifton*

Main category: cs.LG

TL;DR: A cardiac sensing foundation model (CSFM) uses transformers and generative pretraining to analyze diverse cardiac signals, outperforming traditional methods in various tasks.


<details>
  <summary>Details</summary>
Motivation: To address limitations of conventional deep learning in cardiac signal analysis, which lacks robustness and generalizability across diverse settings.

Method: CSFM leverages transformer architectures and masked pretraining on multi-modal data (ECG, PPG, and clinical text) from 1.7M individuals.

Result: CSFM outperforms traditional methods in diagnostic tasks, demographic recognition, vital sign measurement, and more, across varying sensor setups.

Conclusion: CSFM is a versatile, scalable solution for comprehensive cardiac monitoring, adaptable to diverse clinical scenarios.

Abstract: Cardiac biosignals, such as electrocardiograms (ECG) and photoplethysmograms
(PPG), are of paramount importance for the diagnosis, prevention, and
management of cardiovascular diseases, and have been extensively used in a
variety of clinical tasks. Conventional deep learning approaches for analyzing
these signals typically rely on homogeneous datasets and static bespoke models,
limiting their robustness and generalizability across diverse clinical settings
and acquisition protocols. In this study, we present a cardiac sensing
foundation model (CSFM) that leverages advanced transformer architectures and a
generative, masked pretraining strategy to learn unified representations from
vast, heterogeneous health records. Our model is pretrained on an innovative
multi-modal integration of data from multiple large-scale datasets (including
MIMIC-III-WDB, MIMIC-IV-ECG, and CODE), comprising cardiac signals and the
corresponding clinical or machine-generated text reports from approximately 1.7
million individuals. We demonstrate that the embeddings derived from our CSFM
not only serve as effective feature extractors across diverse cardiac sensing
scenarios, but also enable seamless transfer learning across varying input
configurations and sensor modalities. Extensive evaluations across diagnostic
tasks, demographic information recognition, vital sign measurement, clinical
outcome prediction, and ECG question answering reveal that CSFM consistently
outperforms traditional one-modal-one-task approaches. Notably, CSFM exhibits
robust performance across multiple ECG lead configurations from standard
12-lead systems to single-lead setups, and in scenarios where only ECG, only
PPG, or a combination thereof is available. These findings highlight the
potential of CSFM as a versatile and scalable solution, for comprehensive
cardiac monitoring.

</details>


### [157] [Variational Digital Twins](https://arxiv.org/abs/2507.01047)
*Logan A. Burnett,Umme Mahbuba Nabila,Majdi I. Radaideh*

Main category: cs.LG

TL;DR: The paper proposes a variational digital twin (VDT) framework to address gaps in current digital twin literature, offering real-time updates, uncertainty calibration, and efficiency. It demonstrates success in energy-sector applications.


<details>
  <summary>Details</summary>
Motivation: Current digital twin literature lacks clear frameworks for real-time implementation, model uncertainty handling, and efficient information exchange. The VDT aims to solve these issues.

Method: The VDT augments standard neural architectures with a Bayesian output layer and introduces a novel updating algorithm for real-time performance on commodity GPUs.

Result: VDT achieves high accuracy (R2 > 0.95) in energy-sector applications, reduces experiment needs by 47%, and adapts to sensor loss or dynamic conditions.

Conclusion: The VDT framework transforms conventional models into uncertainty-aware, efficient digital twins, enhancing reliability for industrial and scientific energy systems.

Abstract: While digital twins (DT) hold promise for providing real-time insights into
complex energy assets, much of the current literature either does not offer a
clear framework for information exchange between the model and the asset, lacks
key features needed for real-time implementation, or gives limited attention to
model uncertainty. Here, we aim to solve these gaps by proposing a variational
digital twin (VDT) framework that augments standard neural architectures with a
single Bayesian output layer. This lightweight addition, along with a novel VDT
updating algorithm, lets a twin update in seconds on commodity GPUs while
producing calibrated uncertainty bounds that can inform experiment design,
control algorithms, and model reliability. The VDT is evaluated on four
energy-sector problems. For critical-heat-flux prediction, uncertainty-driven
active learning reaches R2 = 0.98 using 47 % fewer experiments and one-third
the training time of random sampling. A three-year renewable-generation twin
maintains R2 > 0.95 for solar output and curbs error growth for volatile wind
forecasts via monthly updates that process only one month of data at a time. A
nuclear reactor transient cooldown twin reconstructs thermocouple signals with
R2 > 0.99 and preserves accuracy after 50 % sensor loss, demonstrating
robustness to degraded instrumentation. Finally, a physics-informed Li-ion
battery twin, retrained after every ten discharges, lowers voltage mean-squared
error by an order of magnitude relative to the best static model while adapting
its credible intervals as the cell approaches end-of-life. These results
demonstrate that combining modest Bayesian augmentation with efficient update
schemes turns conventional surrogates into uncertainty-aware, data-efficient,
and computationally tractable DTs, paving the way for dependable models across
industrial and scientific energy systems.

</details>


### [158] [3W Dataset 2.0.0: a realistic and public dataset with rare undesirable real events in oil wells](https://arxiv.org/abs/2507.01048)
*Ricardo Emanuel Vaz Vargas,Afrânio José de Melo Junior,Celso José Munaro,Cláudio Benevenuto de Campos Lima,Eduardo Toledo de Lima Junior,Felipe Muntzberg Barrocas,Flávio Miguel Varejão,Guilherme Fidelis Peixer,Igor de Melo Nery Oliveira,Jader Riso Barbosa Jr.,Jaime Andrés Lozano Cadena,Jean Carlos Dias de Araújo,João Neuenschwander Escosteguy Carneiro,Lucas Gouveia Omena Lopes,Lucas Pereira de Gouveia,Mateus de Araujo Fernandes,Matheus Lima Scramignon,Patrick Marques Ciarelli,Rodrigo Castello Branco,Rogério Leite Alves Pinto*

Main category: cs.LG

TL;DR: The paper describes the updated 3W Dataset, a public resource for detecting undesirable events in oil wells using AI/ML, now with structural improvements and more labeled data.


<details>
  <summary>Details</summary>
Motivation: Undesirable events in oil wells lead to economic, environmental, and human risks. Public datasets for early detection were lacking, prompting Petrobras to create the 3W Dataset.

Method: The dataset consists of labeled multivariate time series, collaboratively developed and now updated with structural changes and additional labels.

Result: The 3W Dataset serves as a foundational reference for AI/ML applications in oil well event detection, supporting improved methodologies and digital solutions.

Conclusion: The updated dataset aims to enhance early detection capabilities, enabling timely corrective actions and fostering community collaboration.

Abstract: In the oil industry, undesirable events in oil wells can cause economic
losses, environmental accidents, and human casualties. Solutions based on
Artificial Intelligence and Machine Learning for Early Detection of such events
have proven valuable for diverse applications across industries. In 2019,
recognizing the importance and the lack of public datasets related to
undesirable events in oil wells, Petrobras developed and publicly released the
first version of the 3W Dataset, which is essentially a set of Multivariate
Time Series labeled by experts. Since then, the 3W Dataset has been developed
collaboratively and has become a foundational reference for numerous works in
the field. This data article describes the current publicly available version
of the 3W Dataset, which contains structural modifications and additional
labeled data. The detailed description provided encourages and supports the 3W
community and new 3W users to improve previous published results and to develop
new robust methodologies, digital products and services capable of detecting
undesirable events in oil wells with enough anticipation to enable corrective
or mitigating actions.

</details>


### [159] [Text Detoxification: Data Efficiency, Semantic Preservation and Model Generalization](https://arxiv.org/abs/2507.01050)
*Jing Yu,Yibo Zhao,Jiapeng Zhu,Wenming Shao,Bo Pang,Zhao Zhang,Xiang Li*

Main category: cs.LG

TL;DR: A two-stage training framework for text detoxification improves performance, semantic preservation, and data efficiency by combining supervised fine-tuning and reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of existing detoxification methods, which struggle with performance, semantic preservation, and robustness, while relying on costly annotated data.

Method: A two-stage approach: supervised fine-tuning on filtered parallel data, followed by reinforcement learning using unlabeled toxic inputs and a custom reward model.

Result: Achieves state-of-the-art performance with better generalization and reduced reliance on annotated data.

Conclusion: The proposed framework effectively mitigates trade-offs in detoxification, offering a scalable and efficient solution.

Abstract: The widespread dissemination of toxic content on social media poses a serious
threat to both online environments and public discourse, highlighting the
urgent need for detoxification methods that effectively remove toxicity while
preserving the original semantics. However, existing approaches often struggle
to simultaneously achieve strong detoxification performance, semantic
preservation, and robustness to out-of-distribution data. Moreover, they
typically rely on costly, manually annotated parallel corpora while showing
poor data efficiency. To address these challenges, we propose a two-stage
training framework that jointly optimizes for data efficiency, semantic
preservation, and model generalization. We first perform supervised fine-tuning
on a small set of high-quality, filtered parallel data to establish a strong
initialization. Then, we leverage unlabeled toxic inputs and a custom-designed
reward model to train the LLM using Group Relative Policy Optimization.
Experimental results demonstrate that our method effectively mitigates the
trade-offs faced by previous work, achieving state-of-the-art performance with
improved generalization and significantly reduced dependence on annotated data.
Our code is available at:
https://anonymous.4open.science/r/Detoxification-of-Text-725F/

</details>


### [160] [Long-Sequence Memory with Temporal Kernels and Dense Hopfield Functionals](https://arxiv.org/abs/2507.01052)
*Ahmed Farooq*

Main category: cs.LG

TL;DR: A novel energy functional for long-sequence memory is introduced, leveraging dense Hopfield networks with exponential storage capacity. A temporal kernel incorporates dependencies for efficient sequential retrieval, demonstrated with movie frames. Applicable to transformers for long-sequence tasks.


<details>
  <summary>Details</summary>
Motivation: Address limitations of transformers in long-context tasks by enhancing memory and temporal dependency handling.

Method: Propose a temporal kernel within dense Hopfield networks to enable sequential retrieval of patterns in high-dimensional data like movie frames.

Result: Successful storage and sequential retrieval of movie frames, showcasing potential for long-sequence modeling.

Conclusion: The model improves transformer architectures for long-context tasks, with broad applications in NLP, forecasting, and time-series data.

Abstract: In this study we introduce a novel energy functional for long-sequence
memory, building upon the framework of dense Hopfield networks which achieves
exponential storage capacity through higher-order interactions. Building upon
earlier work on long-sequence Hopfield memory models, we propose a temporal
kernal $K(m, k)$ to incorporate temporal dependencies, enabling efficient
sequential retrieval of patterns over extended sequences. We demonstrate the
successful application of this technique for the storage and sequential
retrieval of movies frames which are well suited for this because of the high
dimensional vectors that make up each frame creating enough variation between
even sequential frames in the high dimensional space. The technique has
applications in modern transformer architectures, including efficient
long-sequence modeling, memory augmentation, improved attention with temporal
bias, and enhanced handling of long-term dependencies in time-series data. Our
model offers a promising approach to address the limitations of transformers in
long-context tasks, with potential implications for natural language
processing, forecasting, and beyond.

</details>


### [161] [XxaCT-NN: Structure Agnostic Multimodal Learning for Materials Science](https://arxiv.org/abs/2507.01054)
*Jithendaraa Subramanian,Linda Hung,Daniel Schweigert,Santosh Suram,Weike Ye*

Main category: cs.LG

TL;DR: A scalable multimodal framework is proposed for materials discovery, using elemental composition and XRD without requiring crystal structures, achieving faster convergence and improved accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing structure-based models are impractical for real-world applications where atomic structures are often unknown or hard to obtain.

Method: The framework integrates modality-specific encoders with a cross-attention fusion module, trained on the Alexandria dataset, and uses self-supervised pretraining (MXM and contrastive alignment).

Result: Pretraining yields faster convergence (up to 4.2x speedup) and improves accuracy and representation quality. Multimodal performance scales better with dataset size than unimodal baselines.

Conclusion: The work establishes a path toward structure-free, experimentally grounded foundation models for materials science.

Abstract: Recent advances in materials discovery have been driven by structure-based
models, particularly those using crystal graphs. While effective for
computational datasets, these models are impractical for real-world
applications where atomic structures are often unknown or difficult to obtain.
We propose a scalable multimodal framework that learns directly from elemental
composition and X-ray diffraction (XRD) -- two of the more available modalities
in experimental workflows without requiring crystal structure input. Our
architecture integrates modality-specific encoders with a cross-attention
fusion module and is trained on the 5-million-sample Alexandria dataset. We
present masked XRD modeling (MXM), and apply MXM and contrastive alignment as
self-supervised pretraining strategies. Pretraining yields faster convergence
(up to 4.2x speedup) and improves both accuracy and representation quality. We
further demonstrate that multimodal performance scales more favorably with
dataset size than unimodal baselines, with gains compounding at larger data
regimes. Our results establish a path toward structure-free, experimentally
grounded foundation models for materials science.

</details>


### [162] [Evaluating Pavement Deterioration Rates Due to Flooding Events Using Explainable AI](https://arxiv.org/abs/2507.01056)
*Lidan Peng,Lu Gao,Feng Hong,Jingran Sun*

Main category: cs.LG

TL;DR: The study examines how flooding accelerates pavement roughness (measured by IRI) using 20 years of TxDOT data and XAI techniques like SHAP and LIME. Flooded pavements deteriorate faster, highlighting the need for mitigation strategies.


<details>
  <summary>Details</summary>
Motivation: Flooding causes immediate and long-term damage to pavements, but its impact on deterioration rates, especially roughness, is not well quantified.

Method: Used 20 years of TxDOT PMIS data integrated with flood event details. Statistical analysis compared pre- and post-flood IRI values and deterioration rates. Applied XAI (SHAP and LIME) to assess flood impact.

Result: Flooded pavements showed faster roughness increase than non-flooded ones.

Conclusion: Proactive flood mitigation (e.g., better drainage, resistant materials) is crucial for pavement resilience in flood-prone areas.

Abstract: Flooding can damage pavement infrastructure significantly, causing both
immediate and long-term structural and functional issues. This research
investigates how flooding events affect pavement deterioration, specifically
focusing on measuring pavement roughness by the International Roughness Index
(IRI). To quantify these effects, we utilized 20 years of pavement condition
data from TxDOT's PMIS database, which is integrated with flood event data,
including duration and spatial extent. Statistical analyses were performed to
compare IRI values before and after flooding and to calculate the deterioration
rates influenced by flood exposure. Moreover, we applied Explainable Artificial
Intelligence (XAI) techniques, such as SHapley Additive exPlanations (SHAP) and
Local Interpretable Model-Agnostic Explanations (LIME), to assess the impact of
flooding on pavement performance. The results demonstrate that flood-affected
pavements experience a more rapid increase in roughness compared to non-flooded
sections. These findings emphasize the need for proactive flood mitigation
strategies, including improved drainage systems, flood-resistant materials, and
preventative maintenance, to enhance pavement resilience in vulnerable regions.

</details>


### [163] [Loop2Net: Data-Driven Generation and Optimization of Airfoil CFD Meshes from Sparse Boundary Coordinates](https://arxiv.org/abs/2507.01057)
*Lushun Fan,Yuqin Xia,Jun Li,Karl Jenkins*

Main category: cs.LG

TL;DR: An intelligent optimization system using deep CNN for mesh quality improvement, featuring Loop2Net generator and loss functions.


<details>
  <summary>Details</summary>
Motivation: To enhance mesh generation and optimization for given wing coordinates using deep learning.

Method: Uses a deep convolutional neural network with Loop2Net generator and two key loss functions, incorporating penalties for discipline.

Result: Achieves effective mesh generation and continuous performance optimization.

Conclusion: The proposed system successfully meets the goal of mesh generation through deep learning techniques.

Abstract: In this study, an innovative intelligent optimization system for mesh quality
is proposed, which is based on a deep convolutional neural network
architecture, to achieve mesh generation and optimization. The core of the
study is the Loop2Net generator and loss function, it predicts the mesh based
on the given wing coordinates. And the model's performance is continuously
optimised by two key loss functions during the training. Then discipline by
adding penalties, the goal of mesh generation was finally reached.

</details>


### [164] [Evaluation of a Foundational Model and Stochastic Models for Forecasting Sporadic or Spiky Production Outages of High-Performance Machine Learning Services](https://arxiv.org/abs/2507.01067)
*Keun Soo Yim*

Main category: cs.LG

TL;DR: The paper explores optimizing a foundational model for forecasting rare, spiky events in high-performance ML services, comparing it with classical stochastic models to achieve accurate outage predictions.


<details>
  <summary>Details</summary>
Motivation: Foundational models excel in time series forecasting but haven't been tested for rare, spiky events, which are critical for predicting outages in ML services.

Method: The study optimizes a state-of-the-art foundational model and compares its performance with classical stochastic models (e.g., moving average, autoregressive) for forecasting sporadic outages.

Result: The foundational model, when optimized, achieves less than 6% error in estimating year-long outage statistics for specific root causes.

Conclusion: The foundational model outperforms classical stochastic models for rare, spiky events, providing reliable forecasts for critical ML service outages.

Abstract: Time series forecasting models have diverse real world applications (e.g.,
from electricity metrics to software workload). Latest foundational models
trained for time series forecasting show strengths (e.g., for long sequences
and in zero-shot settings). However, foundational model was not yet used for
forecasting rare, spiky events, i.e., a challenging target because those are a
corner case of extreme events. In this paper, we optimize a state-of-the-art
foundational model to forecast sporadic or spiky production outages of
high-performance machine learning services powering billions of client devices.
We evaluate the forecasting errors of the foundational model compared with
classical stochastic forecasting models (e.g., moving average and
autoregressive). The analysis helps us understand how each of the evaluated
models performs for the sporadic or spiky events. For example, it identifies
the key patterns in the target data that are well tracked by the foundational
model vs. each of the stochastic models. We use the models with optimal
parameters to estimate a year-long outage statistics of a particular root cause
with less than 6% value errors.

</details>


### [165] [Prediction of Freezing of Gait in Parkinsons Disease using Explainable AI and Federated Deep Learning for Wearable Sensors](https://arxiv.org/abs/2507.01068)
*Biplov Paneru*

Main category: cs.LG

TL;DR: The study uses IMU data and explainable AI to predict Freezing of Gait (FOG) in Parkinson's disease, achieving 99% accuracy with a Stacking Ensemble model. Federated learning is also integrated for privacy-preserving training.


<details>
  <summary>Details</summary>
Motivation: Early detection of FOG in Parkinson's disease is critical for patient care. The study aims to improve prediction accuracy and interpretability using AI.

Method: Machine learning models (CatBoost, XGBoost, Extra Trees) and a Stacking Ensemble are used. SHAP analysis identifies key features. Federated learning with a hybrid Conv1D + LSTM architecture is employed.

Result: The Stacking Ensemble model achieves nearly 99% accuracy, outperforming other models. Time (seconds) is the most influential feature.

Conclusion: The proposed framework is effective for FOG prediction, combining high accuracy with explainability and privacy-preserving federated learning.

Abstract: This study leverages an Inertial Measurement Unit (IMU) dataset to develop
explainable AI methods for the early detection and prediction of Freezing of
Gait (FOG), a common symptom in Parkinson's disease. Machine learning models,
including CatBoost, XGBoost, and Extra Trees classifiers, are employed to
accurately categorize FOG episodes based on relevant clinical features. A
Stacking Ensemble model achieves superior performance, surpassing a hybrid
bidirectional GRU model and reaching nearly 99% classification accuracy. SHAP
interpretability analysis reveals that time (seconds) is the most influential
factor in distinguishing gait patterns. Additionally, the proposed FOG
prediction framework incorporates federated learning, where models are trained
locally on individual devices and aggregated on a central server using a
federated averaging approach, utilizing a hybrid Conv1D + LSTM architecture for
enhanced predictive capability.

</details>


### [166] [Rotational Sampling: A Plug-and-Play Encoder for Rotation-Invariant 3D Molecular GNNs](https://arxiv.org/abs/2507.01073)
*Dian Jin*

Main category: cs.LG

TL;DR: A novel 3D encoding module for GNNs using rotational sampling achieves rotational invariance and improves molecular property prediction.


<details>
  <summary>Details</summary>
Motivation: Traditional GNNs struggle with 3D molecular structures due to orientation variability, limiting generalization and robustness.

Method: Proposes a plug-and-play 3D encoding module leveraging rotational sampling and post-alignment for strict invariance.

Result: Outperforms existing methods on QM9 and C10 datasets in accuracy, robustness, and generalization with low computational cost.

Conclusion: The method offers an efficient, interpretable solution for 3D molecular data in drug discovery and material design.

Abstract: Graph neural networks (GNNs) have achieved remarkable success in molecular
property prediction. However, traditional graph representations struggle to
effectively encode the inherent 3D spatial structures of molecules, as
molecular orientations in 3D space introduce significant variability, severely
limiting model generalization and robustness. Existing approaches primarily
focus on rotation-invariant and rotation-equivariant methods. Invariant methods
often rely heavily on prior knowledge and lack sufficient generalizability,
while equivariant methods suffer from high computational costs. To address
these limitations, this paper proposes a novel plug-and-play 3D encoding module
leveraging rotational sampling. By computing the expectation over the SO(3)
rotational group, the method naturally achieves approximate rotational
invariance. Furthermore, by introducing a carefully designed post-alignment
strategy, strict invariance can be achieved without compromising performance.
Experimental evaluations on the QM9 and C10 Datasets demonstrate superior
predictive accuracy, robustness, and generalization performance compared to
existing methods. Moreover, the proposed approach maintains low computational
complexity and enhanced interpretability, providing a promising direction for
efficient and effective handling of 3D molecular information in drug discovery
and material design.

</details>


### [167] [Provenance Tracking in Large-Scale Machine Learning Systems](https://arxiv.org/abs/2507.01075)
*Gabriele Padovani,Valentine Anantharaj,Sandro Fiore*

Main category: cs.LG

TL;DR: The paper introduces yProv4ML, a library for collecting provenance data to optimize large-scale AI model training by balancing efficiency, time, accuracy, and energy.


<details>
  <summary>Details</summary>
Motivation: The growing demand for large-scale AI models necessitates tools to monitor and optimize their training, ensuring efficiency, reproducibility, and accountability.

Method: The yProv4ML library collects provenance data in JSON format, compliant with W3C PROV and ProvML standards, and integrates with the yProv framework for workflow management.

Result: The library provides flexibility and extensibility, enabling users to integrate additional tools via plugins and gain insights into resource usage.

Conclusion: yProv4ML supports optimal resource utilization for scalable, energy-efficient AI model training by leveraging provenance data.

Abstract: As the demand for large scale AI models continues to grow, the optimization
of their training to balance computational efficiency, execution time, accuracy
and energy consumption represents a critical multidimensional challenge.
Achieving this balance requires not only innovative algorithmic techniques and
hardware architectures but also comprehensive tools for monitoring, analyzing,
and understanding the underlying processes involved in model training and
deployment. Provenance data information about the origins, context, and
transformations of data and processes has become a key component in this
pursuit. By leveraging provenance, researchers and engineers can gain insights
into resource usage patterns, identify inefficiencies, and ensure
reproducibility and accountability in AI development workflows. For this
reason, the question of how distributed resources can be optimally utilized to
scale large AI models in an energy efficient manner is a fundamental one. To
support this effort, we introduce the yProv4ML library, a tool designed to
collect provenance data in JSON format, compliant with the W3C PROV and ProvML
standards. yProv4ML focuses on flexibility and extensibility, and enables users
to integrate additional data collection tools via plugins. The library is fully
integrated with the yProv framework, allowing for higher level pairing in tasks
run also through workflow management systems.

</details>


### [168] [Good Enough to Learn: LLM-based Anomaly Detection in ECU Logs without Reliable Labels](https://arxiv.org/abs/2507.01077)
*Bogdan Bogdan,Arina Cazacu,Laura Vasilie*

Main category: cs.LG

TL;DR: A novel decoder-only LLM is proposed for anomaly detection in ECU communication logs, addressing challenges like lack of tailored LLMs and inconsistent ground truth data. The method uses entropy regularization and leverages generative capabilities for scalable, accurate detection.


<details>
  <summary>Details</summary>
Motivation: Existing anomaly detection methods struggle in specialized domains like automotive systems, where scalable solutions are needed. The lack of LLMs for ECU communication and inconsistent labeling further complicate the task.

Method: The approach uses a decoder-only LLM trained on UDP communication logs, treating anomaly detection as identifying time deviations. Entropy regularization is introduced to manage uncertainty in anomalies.

Result: The solution offers a scalable, adaptable LLM for ECU communication, improving detection accuracy and reducing reliance on manual labeling.

Conclusion: The proposed decoder-only LLM provides a novel, scalable, and accurate method for anomaly detection in complex ECU communication environments.

Abstract: Anomaly detection often relies on supervised or clustering approaches, with
limited success in specialized domains like automotive communication systems
where scalable solutions are essential. We propose a novel decoder-only Large
Language Model (LLM) to detect anomalies in Electronic Control Unit (ECU)
communication logs. Our approach addresses two key challenges: the lack of LLMs
tailored for ECU communication and the complexity of inconsistent ground truth
data. By learning from UDP communication logs, we formulate anomaly detection
simply as identifying deviations in time from normal behavior. We introduce an
entropy regularization technique that increases model's uncertainty in known
anomalies while maintaining consistency in similar scenarios. Our solution
offers three novelties: a decoder-only anomaly detection architecture, a way to
handle inconsistent labeling, and an adaptable LLM for different ECU
communication use cases. By leveraging the generative capabilities of
decoder-only models, we present a new technique that addresses the high cost
and error-prone nature of manual labeling through a more scalable system that
is able to learn from a minimal set of examples, while improving detection
accuracy in complex communication environments.

</details>


### [169] [yProv4ML: Effortless Provenance Tracking for Machine Learning Systems](https://arxiv.org/abs/2507.01078)
*Gabriele Padovani,Valentine Anantharaj,Sandro Fiore*

Main category: cs.LG

TL;DR: yProv4ML is a framework for capturing provenance data in machine learning processes using PROV-JSON format, addressing transparency and hyperparameter tracking issues in LLMs.


<details>
  <summary>Details</summary>
Motivation: The lack of transparency and rigor in LLM development, especially around hyperparameters, necessitates better tools for tracking and lineage.

Method: Proposes yProv4ML, a framework that captures provenance data in PROV-JSON format with minimal code changes.

Result: Enables automated and standardized tracking of machine learning processes, improving transparency.

Conclusion: yProv4ML offers a practical solution for provenance tracking in ML, enhancing reproducibility and rigor.

Abstract: The rapid growth of interest in large language models (LLMs) reflects their
potential for flexibility and generalization, and attracted the attention of a
diverse range of researchers. However, the advent of these techniques has also
brought to light the lack of transparency and rigor with which development is
pursued. In particular, the inability to determine the number of epochs and
other hyperparameters in advance presents challenges in identifying the best
model. To address this challenge, machine learning frameworks such as MLFlow
can automate the collection of this type of information. However, these tools
capture data using proprietary formats and pose little attention to lineage.
This paper proposes yProv4ML, a framework to capture provenance information
generated during machine learning processes in PROV-JSON format, with minimal
code modifications.

</details>


### [170] [Development and Comparative Evaluation of Three Artificial Intelligence Models (NLP, LLM, JEPA) for Predicting Triage in Emergency Departments: A 7-Month Retrospective Proof-of-Concept](https://arxiv.org/abs/2507.01080)
*Edouard Lansiaux,Ramy Azzouz,Emmanuel Chazard,Amélie Vromant,Eric Wiel*

Main category: cs.LG

TL;DR: The study compares three AI models (NLP, LLM, JEPA) for triage prediction in EDs, finding the LLM model (URGENTIAPARSE) most accurate, outperforming nurse triage and other AI models.


<details>
  <summary>Details</summary>
Motivation: To address persistent triage errors in EDs by evaluating AI models' effectiveness in improving triage accuracy.

Method: Retrospective analysis of triage data using three AI models (NLP, LLM, JEPA) validated against the FRENCH scale and clinical practice.

Result: The LLM model (URGENTIAPARSE) achieved the highest accuracy (composite score: 2.514) and outperformed nurse triage and other AI models.

Conclusion: AI, especially LLMs, can enhance triage accuracy and ED efficiency, but integration requires addressing limitations and ensuring ethical transparency.

Abstract: Triage errors, including undertriage and overtriage, are persistent
challenges in emergency departments (EDs). With increasing patient influx and
staff shortages, the integration of artificial intelligence (AI) into triage
protocols has gained attention. This study compares the performance of three AI
models [Natural Language Processing (NLP), Large Language Models (LLM), and
Joint Embedding Predictive Architecture (JEPA)] in predicting triage outcomes
against the FRENCH scale and clinical practice.We conducted a retrospective
analysis of a prospectively recruited cohort gathering adult patient triage
data over a 7-month period at the Roger Salengro Hospital ED (Lille, France).
Three AI models were trained and validated : (1) TRIAGEMASTER (NLP), (2)
URGENTIAPARSE (LLM), and (3) EMERGINET (JEPA). Data included demographic
details, verbatim chief complaints, vital signs, and triage outcomes based on
the FRENCH scale and GEMSA coding. The primary outcome was the concordance of
AI-predicted triage level with the FRENCH gold-standard. It was assessed thanks
to various indicators : F1-Score, Weighted Kappa, Spearman, MAE, RMSE. The LLM
model (URGENTIAPARSE) showed higher accuracy (composite score: 2.514) compared
to JEPA (EMERGINET, 0.438) and NLP (TRIAGEMASTER, -3.511), outperforming nurse
triage (-4.343). Secondary analyses highlighted the effectiveness of
URGENTIAPARSE in predicting hospitalization needs (GEMSA) and its robustness
with structured data versus raw transcripts (either for GEMSA prediction or for
FRENCH prediction). LLM architecture, through abstraction of patient
representations, offers the most accurate triage predictions among tested
models. Integrating AI into ED workflows could enhance patient safety and
operational efficiency, though integration into clinical workflows requires
addressing model limitations and ensuring ethical transparency.

</details>


### [171] [Proof of a perfect platonic representation hypothesis](https://arxiv.org/abs/2507.01098)
*Liu Ziyin,Isaac Chuang*

Main category: cs.LG

TL;DR: The paper explains Ziyin et al.'s proof of the Perfect Platonic Representation Hypothesis (PRH) for embedded deep linear networks (EDLN), showing SGD leads to identical layer representations up to rotation, despite most global minima not being Platonic. It also links PRH to progressive sharpening and identifies six ways PRH can fail.


<details>
  <summary>Details</summary>
Motivation: To clarify and expand on Ziyin et al.'s proof of PRH in EDLNs, emphasizing the surprising role of SGD in achieving Platonic representations and exploring connections to other deep learning phenomena.

Method: Detailed elaboration of the proof for PRH in EDLNs, analyzing SGD's behavior and its impact on layer representations, and identifying conditions under which PRH fails.

Result: SGD in EDLNs leads to perfectly Platonic representations (identical up to rotation), despite most global minima not being Platonic. PRH is linked to progressive sharpening, suggesting a common cause. Six failure modes for PRH are identified.

Conclusion: The paper underscores the significance of emergent entropic forces in SGD-driven representation learning, revealing unexpected connections between phenomena like PRH and progressive sharpening.

Abstract: In this note, we elaborate on and explain in detail the proof given by Ziyin
et al. (2025) of the "perfect" Platonic Representation Hypothesis (PRH) for the
embedded deep linear network model (EDLN). We show that if trained with SGD,
two EDLNs with different widths and depths and trained on different data will
become Perfectly Platonic, meaning that every possible pair of layers will
learn the same representation up to a rotation. Because most of the global
minima of the loss function are not Platonic, that SGD only finds the perfectly
Platonic solution is rather extraordinary. The proof also suggests at least six
ways the PRH can be broken. We also show that in the EDLN model, the emergence
of the Platonic representations is due to the same reason as the emergence of
progressive sharpening. This implies that these two seemingly unrelated
phenomena in deep learning can, surprisingly, have a common cause. Overall, the
theory and proof highlight the importance of understanding emergent "entropic
forces" due to the irreversibility of SGD training and their role in
representation learning. The goal of this note is to be instructive and avoid
lengthy technical details.

</details>


### [172] [A Neural Operator based on Dynamic Mode Decomposition](https://arxiv.org/abs/2507.01117)
*Nikita Sakovich,Dmitry Aksenov,Ekaterina Pleshakova,Sergey Gataullin*

Main category: cs.LG

TL;DR: A neural operator combining dynamic mode decomposition (DMD) and deep learning (DL) is proposed for efficient spatiotemporal modeling, reducing computational costs while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Balancing lightweight and accurate computations in scientific computing with AI is crucial. Traditional PDE-solving methods are resource-intensive.

Method: The approach integrates DMD and DL to extract key modes and dynamics for predictions, tested on heat, Laplace, and Burgers equations.

Result: The method outperforms DeepONet and FNO in accuracy and computational efficiency for PDE solutions.

Conclusion: The neural operator offers a promising, efficient alternative to traditional numerical methods for PDE modeling.

Abstract: The scientific computation methods development in conjunction with artificial
intelligence technologies remains a hot research topic. Finding a balance
between lightweight and accurate computations is a solid foundation for this
direction. The study presents a neural operator based on the dynamic mode
decomposition algorithm (DMD), mapping functional spaces, which combines DMD
and deep learning (DL) for spatiotemporal processes efficient modeling. Solving
PDEs for various initial and boundary conditions requires significant
computational resources. The method suggested automatically extracts key modes
and system dynamics using them to construct predictions, reducing computational
costs compared to traditional numerical methods. The approach has demonstrated
its efficiency through comparative analysis of performance with closest
analogues DeepONet and FNO in the heat equation, Laplaces equation, and Burgers
equation solutions approximation, where it achieves high reconstruction
accuracy.

</details>


### [173] [On Design Principles for Private Adaptive Optimizers](https://arxiv.org/abs/2507.01129)
*Arun Ganesh,Brendan McMahan,Abhradeep Thakurta*

Main category: cs.LG

TL;DR: The paper critiques existing DP training methods for adaptive optimizers, finding unbiased second-moment estimates misguided. It advocates for 'scale-then-privatize,' which outperforms others in small-scale tasks and aligns better with practical noise mechanisms.


<details>
  <summary>Details</summary>
Motivation: Address the performance drop in adaptive optimizers (e.g., AdaGrad, Adam) due to spherical noise in DP training, and evaluate if existing solutions generalize to practical scenarios.

Method: Survey and theoretically analyze variants of DP training methods, focusing on their empirical performance in small-scale language model training.

Result: 'Scale-then-privatize' outperforms other variants, despite not providing unbiased second-moment estimates, and aligns better with practical noise mechanisms.

Conclusion: The common intuition of unbiased second-moment estimates is flawed; 'scale-then-privatize' is more effective and theoretically sound for DP training with adaptive optimizers.

Abstract: The spherical noise added to gradients in differentially private (DP)
training undermines the performance of adaptive optimizers like AdaGrad and
Adam, and hence many recent works have proposed algorithms to address this
challenge. However, the empirical results in these works focus on simple tasks
and models and the conclusions may not generalize to model training in
practice. In this paper we survey several of these variants, and develop better
theoretical intuition for them as well as perform empirical studies comparing
them. We find that a common intuition of aiming for unbiased estimates of
second moments of gradients in adaptive optimizers is misguided, and instead
that a simple technique called scale-then-privatize (which does not achieve
unbiased second moments) has more desirable theoretical behaviors and
outperforms all other variants we study on a small-scale language model
training task. We additionally argue that scale-then-privatize causes the noise
addition to better match the application of correlated noise mechanisms which
are more desirable to use in practice.

</details>


### [174] [Tensor Decomposition Networks for Fast Machine Learning Interatomic Potential Computations](https://arxiv.org/abs/2507.01131)
*Yuchao Lin,Cong Fu,Zachary Krueger,Haiyang Yu,Maho Nakata,Jianwen Xie,Emine Kucukbenli,Xiaofeng Qian,Shuiwang Ji*

Main category: cs.LG

TL;DR: TDNs replace expensive CG tensor products in SO(3)-equivariant networks with low-rank decompositions, achieving competitive performance and faster computation.


<details>
  <summary>Details</summary>
Motivation: To accelerate the computationally expensive Clebsch-Gordan (CG) tensor products in SO(3)-equivariant networks.

Method: Develop tensor decomposition networks (TDNs) using low-rank decompositions (e.g., CP) and introduce path-weight sharing to reduce parameters.

Result: TDNs reduce computational complexity from O(L^6) to O(L^4) and achieve competitive performance on datasets like PubChemQCR, OC20, and OC22.

Conclusion: TDNs offer a plug-and-play, efficient alternative to traditional CG tensor products without compromising equivariance.

Abstract: $\rm{SO}(3)$-equivariant networks are the dominant models for machine
learning interatomic potentials (MLIPs). The key operation of such networks is
the Clebsch-Gordan (CG) tensor product, which is computationally expensive. To
accelerate the computation, we develop tensor decomposition networks (TDNs) as
a class of approximately equivariant networks whose CG tensor products are
replaced by low-rank tensor decompositions, such as the CANDECOMP/PARAFAC (CP)
decomposition. With the CP decomposition, we prove (i) a uniform bound on the
induced error of $\rm{SO}(3)$-equivariance, and (ii) the universality of
approximating any equivariant bilinear map. To further reduce the number of
parameters, we propose path-weight sharing that ties all multiplicity-space
weights across the $O(L^3)$ CG paths into a single path without compromising
equivariance, where $L$ is the maximum angular degree. The resulting layer acts
as a plug-and-play replacement for tensor products in existing networks, and
the computational complexity of tensor products is reduced from $O(L^6)$ to
$O(L^4)$. We evaluate TDNs on PubChemQCR, a newly curated molecular relaxation
dataset containing 105 million DFT-calculated snapshots. We also use existing
datasets, including OC20, and OC22. Results show that TDNs achieve competitive
performance with dramatic speedup in computations.

</details>


### [175] [Spectral Manifold Harmonization for Graph Imbalanced Regression](https://arxiv.org/abs/2507.01132)
*Brenda Nogueira,Gabe Gomes,Meng Jiang,Nitesh V. Chawla,Nuno Moniz*

Main category: cs.LG

TL;DR: SMH addresses imbalanced regression on graph-structured data by generating synthetic samples preserving topology and targeting underrepresented regions, improving predictive performance.


<details>
  <summary>Details</summary>
Motivation: Imbalanced regression in graph-structured data lacks research, especially for scientifically valuable target ranges.

Method: Spectral Manifold Harmonization (SMH) generates synthetic graph samples preserving topology and focusing on underrepresented target regions.

Result: SMH shows consistent improvements in predictive performance for target domain ranges in chemistry and drug discovery benchmarks.

Conclusion: SMH effectively addresses imbalanced regression on graph-structured data by balancing representation and preserving topology.

Abstract: Graph-structured data is ubiquitous in scientific domains, where models often
face imbalanced learning settings. In imbalanced regression, domain preferences
focus on specific target value ranges representing the most scientifically
valuable cases; we observe a significant lack of research. In this paper, we
present Spectral Manifold Harmonization (SMH), a novel approach for addressing
this imbalanced regression challenge on graph-structured data by generating
synthetic graph samples that preserve topological properties while focusing on
often underrepresented target distribution regions. Conventional methods fail
in this context because they either ignore graph topology in case generation or
do not target specific domain ranges, resulting in models biased toward average
target values. Experimental results demonstrate the potential of SMH on
chemistry and drug discovery benchmark datasets, showing consistent
improvements in predictive performance for target domain ranges.

</details>


### [176] [FlashDP: Private Training Large Language Models with Efficient DP-SGD](https://arxiv.org/abs/2507.01154)
*Liangyu Wang,Junxiao Wang,Jie Ren,Zihang Xiang,David E. Keyes,Di Wang*

Main category: cs.LG

TL;DR: FlashDP introduces a cache-friendly per-layer DP-SGD method for efficient and privacy-preserving LLM training, reducing memory and computation overheads.


<details>
  <summary>Details</summary>
Motivation: Addressing the inefficiencies and high memory demands of current DP-SGD methods (like Opacus and GhostClip) in protecting LLM training data privacy.

Method: FlashDP consolidates operations into a single task, calculating gradients once in a fused manner, reducing memory movement and redundant computations.

Result: Achieves 50% less memory movement, 20% fewer redundant computations, and 90% throughput of Non-DP methods while maintaining accuracy.

Conclusion: FlashDP is a significant advancement for efficient, privacy-preserving LLM training, with open-sourced code available.

Abstract: As large language models (LLMs) increasingly underpin technological
advancements, the privacy of their training data emerges as a critical concern.
Differential Privacy (DP) serves as a rigorous mechanism to protect this data,
yet its integration via Differentially Private Stochastic Gradient Descent
(DP-SGD) introduces substantial challenges, primarily due to the complexities
of per-sample gradient clipping. Current explicit methods, such as Opacus,
necessitate extensive storage for per-sample gradients, significantly inflating
memory requirements. Conversely, implicit methods like GhostClip reduce storage
needs by recalculating gradients multiple times, which leads to inefficiencies
due to redundant computations. This paper introduces FlashDP, an innovative
cache-friendly per-layer DP-SGD that consolidates necessary operations into a
single task, calculating gradients only once in a fused manner. This approach
not only diminishes memory movement by up to \textbf{50\%} but also cuts down
redundant computations by \textbf{20\%}, compared to previous methods.
Consequently, FlashDP does not increase memory demands and achieves a
\textbf{90\%} throughput compared to the Non-DP method on a four-A100 system
during the pre-training of the Llama-13B model, while maintaining parity with
standard per-layer clipped DP-SGD in terms of accuracy. These advancements
establish FlashDP as a pivotal development for efficient and privacy-preserving
training of LLMs. FlashDP's code has been open-sourced in
https://github.com/kaustpradalab/flashdp.

</details>


### [177] [Diffusion Explorer: Interactive Exploration of Diffusion Models](https://arxiv.org/abs/2507.01178)
*Alec Helbling,Duen Horng Chau*

Main category: cs.LG

TL;DR: Diffusion Explorer is an interactive tool designed to explain the geometric properties of diffusion models through 2D training and real-time observation.


<details>
  <summary>Details</summary>
Motivation: Existing explanations of diffusion models are either too theoretical or overly focused on neural architectures, neglecting their rich geometric properties.

Method: The tool allows users to train 2D diffusion models in-browser and observe their sampling dynamics via interactive animations.

Result: Diffusion Explorer provides an accessible way to visualize and understand the temporal and stochastic nature of diffusion models.

Conclusion: The tool is open-source and available for live demo, bridging the gap between theory and practical understanding of diffusion models.

Abstract: Diffusion models have been central to the development of recent image, video,
and even text generation systems. They posses striking geometric properties
that can be faithfully portrayed in low-dimensional settings. However, existing
resources for explaining diffusion either require an advanced theoretical
foundation or focus on their neural network architectures rather than their
rich geometric properties. We introduce Diffusion Explorer, an interactive tool
to explain the geometric properties of diffusion models. Users can train 2D
diffusion models in the browser and observe the temporal dynamics of their
sampling process. Diffusion Explorer leverages interactive animation, which has
been shown to be a powerful tool for making engaging visualizations of dynamic
systems, making it well suited to explaining diffusion models which represent
stochastic processes that evolve over time. Diffusion Explorer is open source
and a live demo is available at alechelbling.com/Diffusion-Explorer.

</details>


### [178] [Are Large Brainwave Foundation Models Capable Yet? Insights from Fine-tuning](https://arxiv.org/abs/2507.01196)
*Na Lee,Konstantinos Barmpas,Yannis Panagakis,Dimitrios Adamos,Nikolaos Laskaris,Stefanos Zafeiriou*

Main category: cs.LG

TL;DR: LBMs show marginal gains over traditional methods in BCI tasks but with higher computational costs. LoRA helps reduce parameters without losing performance, suggesting LBMs need redesign for brainwave analysis.


<details>
  <summary>Details</summary>
Motivation: To evaluate the effectiveness of Large Brainwave Foundation Models (LBMs) in Brain-Computer Interface (BCI) tasks and address their inefficiencies.

Method: Systematic fine-tuning experiments, ablation studies, and Low-Rank Adaptation (LoRA) on LBMs across BCI benchmarks.

Result: LBMs achieve only 0.9%-1.2% improvement over traditional methods but require more parameters. LoRA reduces parameters without performance loss.

Conclusion: LBMs need domain-specific redesign to fully leverage their potential in brainwave analysis, as current architectures are inefficient.

Abstract: Foundation Models have demonstrated significant success across various
domains in Artificial Intelligence (AI), yet their capabilities for brainwave
modeling remain unclear. In this paper, we comprehensively evaluate current
Large Brainwave Foundation Models (LBMs) through systematic fine-tuning
experiments across multiple Brain-Computer Interface (BCI) benchmark tasks,
including memory tasks and sleep stage classification. Our extensive analysis
shows that state-of-the-art LBMs achieve only marginal improvements (0.9%-1.2%)
over traditional deep architectures while requiring significantly more
parameters (millions vs thousands), raising important questions about their
efficiency and applicability in BCI contexts. Moreover, through detailed
ablation studies and Low-Rank Adaptation (LoRA), we significantly reduce
trainable parameters without performance degradation, while demonstrating that
architectural and training inefficiencies limit LBMs' current capabilities. Our
experiments span both full model fine-tuning and parameter-efficient adaptation
techniques, providing insights into optimal training strategies for BCI
applications. We pioneer the application of LoRA to LBMs, revealing that
performance benefits generally emerge when adapting multiple neural network
components simultaneously. These findings highlight the critical need for
domain-specific development strategies to advance LBMs, suggesting that current
architectures may require redesign to fully leverage the potential of
foundation models in brainwave analysis.

</details>


### [179] [Escaping Platos Cave: JAM for Aligning Independently Trained Vision and Language Models](https://arxiv.org/abs/2507.01201)
*Hyoseo,Yoon,Yisong Yue,Been Kim*

Main category: cs.LG

TL;DR: The paper introduces the Joint Autoencoder Modulator (JAM) framework to align disjoint vision and language representations, optimizing for mutual coherence while preserving modality-specific structures.


<details>
  <summary>Details</summary>
Motivation: The Platonic Representation Hypothesis suggests that independently trained vision and language models may converge toward a shared statistical model of reality, prompting the need for explicit alignment optimization.

Method: The JAM framework jointly trains modality-specific autoencoders on pre-trained single-modality models, using reconstruction and cross-modal objectives to encourage alignment.

Result: The framework reliably induces alignment across frozen, independently trained representations, with evaluations on alignment objectives, layer depth, and foundation model scale.

Conclusion: JAM provides a lightweight, Pareto-efficient method to transform unimodal foundations into specialist multimodal models, offering theoretical and practical insights.

Abstract: Independently trained vision and language models inhabit disjoint
representational spaces, shaped by their respective modalities, objectives, and
architectures. Yet an emerging hypothesis - the Platonic Representation
Hypothesis - suggests that such models may nonetheless converge toward a shared
statistical model of reality. This compatibility, if it exists, raises a
fundamental question: can we move beyond post-hoc statistical detection of
alignment and explicitly optimize for it between such disjoint representations?
We cast this Platonic alignment problem as a multi-objective optimization task
- preserve each modality's native structure while aligning for mutual
coherence. We introduce the Joint Autoencoder Modulator (JAM) framework that
jointly trains modality-specific autoencoders on the latent representations of
pre-trained single modality models, encouraging alignment through both
reconstruction and cross-modal objectives. By analogy, this framework serves as
a method to escape Plato's Cave, enabling the emergence of shared structure
from disjoint inputs. We evaluate this framework across three critical design
axes: (i) the alignment objective - comparing contrastive loss (Con), its
hard-negative variant (NegCon), and our Spread loss, (ii) the layer depth at
which alignment is most effective, and (iii) the impact of foundation model
scale on representational convergence. Our findings show that our lightweight
Pareto-efficient framework reliably induces alignment, even across frozen,
independently trained representations, offering both theoretical insight and
practical pathways for transforming generalist unimodal foundations into
specialist multimodal models.

</details>


### [180] [Deep Learning-Based Intrusion Detection for Automotive Ethernet: Evaluating & Optimizing Fast Inference Techniques for Deployment on Low-Cost Platform](https://arxiv.org/abs/2507.01208)
*Pedro R. X. Carmo,Igor de Moura,Assis T. de Oliveira Filho,Djamel Sadok,Cleber Zanchettin*

Main category: cs.LG

TL;DR: The paper evaluates fast neural network techniques (Distilling and Pruning) to deploy Intrusion Detection Systems (IDS) on low-cost hardware like Raspberry Pi 4, achieving real-time performance with high accuracy.


<details>
  <summary>Details</summary>
Motivation: Modern vehicles rely on automotive Ethernet for communication, but face security threats like flow injection attacks. Existing Deep Learning-based IDS require costly hardware for real-time operation.

Method: The study applies Distilling and Pruning techniques to optimize IDS models for deployment on low-cost platforms (e.g., Raspberry Pi 4).

Result: The optimized models achieve intrusion detection times of 727 μs with AUCROC values of 0.9890.

Conclusion: Fast neural network techniques enable efficient, real-time IDS deployment on affordable hardware, enhancing vehicle security.

Abstract: Modern vehicles are increasingly connected, and in this context, automotive
Ethernet is one of the technologies that promise to provide the necessary
infrastructure for intra-vehicle communication. However, these systems are
subject to attacks that can compromise safety, including flow injection
attacks. Deep Learning-based Intrusion Detection Systems (IDS) are often
designed to combat this problem, but they require expensive hardware to run in
real time. In this work, we propose to evaluate and apply fast neural network
inference techniques like Distilling and Prunning for deploying IDS models on
low-cost platforms in real time. The results show that these techniques can
achieve intrusion detection times of up to 727 {\mu}s using a Raspberry Pi 4,
with AUCROC values of 0.9890.

</details>


### [181] [PAE MobiLLM: Privacy-Aware and Efficient LLM Fine-Tuning on the Mobile Device via Additive Side-Tuning](https://arxiv.org/abs/2507.01216)
*Xingke Yang,Liang Li,Zhiyi Wan,Sicong Li,Hao Wang,Xiaoqi Qi,Jiang Liu,Tomoaki Ohtsuki,Xin Fu,Miao Pan*

Main category: cs.LG

TL;DR: PAE MobiLLM is a privacy-aware, efficient method for fine-tuning large language models on mobile devices using server-assisted additive side-tuning, reducing communication costs and protecting data privacy.


<details>
  <summary>Details</summary>
Motivation: The gap between mobile device resource limitations and the demand for on-device LLM fine-tuning, along with privacy and efficiency issues in existing server-assisted methods, drives the need for PAE MobiLLM.

Method: PAE MobiLLM employs server-assisted additive side-tuning with activation caching, a one-token activation shortcut, and additive adapter side-network design to enhance efficiency and privacy.

Result: The method reduces communication burdens, accelerates fine-tuning convergence, and ensures data, labels, and models remain private.

Conclusion: PAE MobiLLM effectively addresses resource, privacy, and efficiency challenges in on-device LLM fine-tuning, making it practical for mobile deployment.

Abstract: There is a huge gap between numerous intriguing applications fostered by
on-device large language model (LLM) fine-tuning (FT) from fresh mobile data
and the limited resources of a mobile device. While existing server-assisted
methods (e.g., split learning or side-tuning) may enable LLM FT on the local
mobile device, they suffer from heavy communication burdens of activation
transmissions, and may disclose data, labels or fine-tuned models to the
server. To address those issues, we develop PAE MobiLLM, a privacy-aware and
efficient LLM FT method which can be deployed on the mobile device via
server-assisted additive side-tuning. To further accelerate FT convergence and
improve computing efficiency, PAE MobiLLM integrates activation caching on the
server side, which allows the server to reuse historical activations and saves
the mobile device from repeatedly computing forward passes for the recurring
data samples. Besides, to reduce communication cost, PAE MobiLLM develops a
one-token (i.e., ``pivot'' token) activation shortcut that transmits only a
single activation dimension instead of full activation matrices to guide the
side network tuning. Last but not least, PAE MobiLLM introduces the additive
adapter side-network design which makes the server train the adapter modules
based on device-defined prediction differences rather than raw ground-truth
labels. In this way, the server can only assist device-defined side-network
computing, and learn nothing about data, labels or fine-tuned models.

</details>


### [182] [Quantum Machine Learning in Transportation: A Case Study of Pedestrian Stress Modelling](https://arxiv.org/abs/2507.01235)
*Bara Rababa,Bilal Farooq*

Main category: cs.LG

TL;DR: Quantum machine learning (QML) models (QSVM and QNN) were tested for classifying pedestrian stress using SCR data. QNN outperformed QSVM and classical methods with 55% test accuracy.


<details>
  <summary>Details</summary>
Motivation: To leverage quantum computing for modeling complex SCR events in intelligent transportation systems, addressing challenges in high-dimensional data representation.

Method: Developed QSVM and QNN models on Pennylane using an eight-qubit ZZ feature map. Dataset included SCR measurements categorized by amplitude-based classes.

Result: QSVM showed overfitting (45% test accuracy), while QNN achieved higher reliability (55% test accuracy).

Conclusion: QNN is more effective than QSVM and classical methods for classifying pedestrian stress, though further improvements are needed.

Abstract: Quantum computing has opened new opportunities to tackle complex machine
learning tasks, for instance, high-dimensional data representations commonly
required in intelligent transportation systems. We explore quantum machine
learning to model complex skin conductance response (SCR) events that reflect
pedestrian stress in a virtual reality road crossing experiment. For this
purpose, Quantum Support Vector Machine (QSVM) with an eight-qubit ZZ feature
map and a Quantum Neural Network (QNN) using a Tree Tensor Network ansatz and
an eight-qubit ZZ feature map, were developed on Pennylane. The dataset
consists of SCR measurements along with features such as the response amplitude
and elapsed time, which have been categorized into amplitude-based classes. The
QSVM achieved good training accuracy, but had an overfitting problem, showing a
low test accuracy of 45% and therefore impacting the reliability of the
classification model. The QNN model reached a higher test accuracy of 55%,
making it a better classification model than the QSVM and the classic versions.

</details>


### [183] [Beyond First-Order: Training LLMs with Stochastic Conjugate Subgradients and AdamW](https://arxiv.org/abs/2507.01241)
*Di Zhang,Yihang Zhang*

Main category: cs.LG

TL;DR: A stochastic conjugate subgradient method with adaptive sampling is proposed for training large language models (LLMs), outperforming traditional SGD in speed and scalability.


<details>
  <summary>Details</summary>
Motivation: Traditional SGD methods show limitations in large-scale LLM training, prompting the need for more efficient optimization techniques.

Method: Combines stochastic conjugate subgradient with adaptive sampling, adaptive step sizes (AdamW-like), and sample complexity analysis.

Result: Faster convergence, improved scalability, and better optimization speed/accuracy compared to SGD.

Conclusion: The proposed method effectively addresses nonconvexity and non-smoothness in LLM training, surpassing SGD performance.

Abstract: Stochastic gradient-based descent (SGD), have long been central to training
large language models (LLMs). However, their effectiveness is increasingly
being questioned, particularly in large-scale applications where empirical
evidence suggests potential performance limitations. In response, this paper
proposes a stochastic conjugate subgradient method together with adaptive
sampling tailored specifically for training LLMs. The method not only achieves
faster convergence per iteration but also demonstrates improved scalability
compared to traditional SGD techniques. It leverages sample complexity analysis
to adaptively choose the sample size, employs a stochastic conjugate
subgradient approach to determine search directions and utilizing an AdamW-like
algorithm to adaptively adjust step sizes. This approach preserves the key
advantages of first-order methods while effectively addressing the nonconvexity
and non-smoothness inherent in LLMs training. Additionally, we provide a
detailed analysis of the advantage of the algorithm. Experimental results show
that the proposed method not only maintains, but in many cases surpasses, the
scalability of traditional SGD techniques, significantly enhancing both the
speed and accuracy of the optimization process.

</details>


### [184] [PULSE: Practical Evaluation Scenarios for Large Multimodal Model Unlearning](https://arxiv.org/abs/2507.01271)
*Tatsuki Kawakami,Kazuki Egashira,Atsuyuki Miyai,Go Irie,Kiyoharu Aizawa*

Main category: cs.LG

TL;DR: The paper introduces PULSE, a protocol for evaluating unlearning in LMMs, focusing on pre-trained knowledge unlearning and long-term sustainability. It finds existing methods struggle with pre-training knowledge and sequential unlearning.


<details>
  <summary>Details</summary>
Motivation: Address the lack of practical evaluation frameworks for unlearning in LMMs, especially for pre-trained knowledge and sequential unlearning scenarios.

Method: Introduces PULSE protocol with two key perspectives: pre-trained knowledge unlearning and long-term sustainability evaluation. Evaluates existing unlearning methods using this framework.

Result: Existing methods can unlearn fine-tuned knowledge but fail with pre-training knowledge. Sequential unlearning degrades performance.

Conclusion: PULSE highlights limitations in current unlearning methods for LMMs, emphasizing the need for better techniques for pre-training knowledge and sequential unlearning.

Abstract: In recent years, unlearning techniques, which are methods for inducing a
model to "forget" previously learned information, have attracted attention as a
way to address privacy and copyright concerns in large language models (LLMs)
and large multimodal models (LMMs). While several unlearning benchmarks have
been established for LLMs, a practical evaluation framework for unlearning in
LMMs has been less explored. Specifically, existing unlearning benchmark for
LMMs considers only scenarios in which the model is required to unlearn
fine-tuned knowledge through a single unlearning operation. In this study, we
introduce PULSE protocol for realistic unlearning scenarios for LMMs by
introducing two critical perspectives: (i) Pre-trained knowledge Unlearning for
analyzing the effect across different knowledge acquisition phases and (ii)
Long-term Sustainability Evaluation to address sequential requests. We then
evaluate existing unlearning methods along these dimensions. Our results reveal
that, although some techniques can successfully unlearn knowledge acquired
through fine-tuning, they struggle to eliminate information learned during
pre-training. Moreover, methods that effectively unlearn a batch of target data
in a single operation exhibit substantial performance degradation when the same
data are split and unlearned sequentially.

</details>


### [185] [Far From Sight, Far From Mind: Inverse Distance Weighting for Graph Federated Recommendation](https://arxiv.org/abs/2507.01285)
*Aymen Rayane Khouas,Mohamed Reda Bouadjenek,Hakim Hacid,Sunil Aryal*

Main category: cs.LG

TL;DR: Dist-FedAvg is a distance-based aggregation method for graph federated recommendation systems, improving personalization and efficiency by weighting similar user embeddings higher and preserving anchor user influence.


<details>
  <summary>Details</summary>
Motivation: Traditional aggregation methods in federated learning overlook the complexity of user embeddings and the role of user similarity, limiting recommendation effectiveness.

Method: Introduces Dist-FedAvg, a novel distance-based aggregation method that weights similar user embeddings higher and preserves anchor user influence in local updates.

Result: Empirical evaluations show Dist-FedAvg outperforms baseline methods, enhancing recommendation accuracy while integrating seamlessly into federated frameworks.

Conclusion: Dist-FedAvg addresses limitations of traditional aggregation, improving personalization and efficiency in graph federated recommendation systems.

Abstract: Graph federated recommendation systems offer a privacy-preserving alternative
to traditional centralized recommendation architectures, which often raise
concerns about data security. While federated learning enables personalized
recommendations without exposing raw user data, existing aggregation methods
overlook the unique properties of user embeddings in this setting. Indeed,
traditional aggregation methods fail to account for their complexity and the
critical role of user similarity in recommendation effectiveness. Moreover,
evolving user interactions require adaptive aggregation while preserving the
influence of high-relevance anchor users (the primary users before expansion in
graph-based frameworks). To address these limitations, we introduce
Dist-FedAvg, a novel distance-based aggregation method designed to enhance
personalization and aggregation efficiency in graph federated learning. Our
method assigns higher aggregation weights to users with similar embeddings,
while ensuring that anchor users retain significant influence in local updates.
Empirical evaluations on multiple datasets demonstrate that Dist-FedAvg
consistently outperforms baseline aggregation techniques, improving
recommendation accuracy while maintaining seamless integration into existing
federated learning frameworks.

</details>


### [186] [Neural Hamiltonian Operator](https://arxiv.org/abs/2507.01313)
*Qian Qi*

Main category: cs.LG

TL;DR: The paper introduces a Neural Hamiltonian Operator (NHO) to solve high-dimensional stochastic control problems using deep learning, framing it as an operator learning problem under Pontryagin's Maximum Principle.


<details>
  <summary>Details</summary>
Motivation: High-dimensional stochastic control problems are challenging due to the curse of dimensionality, and traditional methods like dynamic programming are inefficient.

Method: The NHO parameterizes FBSDE dynamics via neural networks for feedback control and value function gradient, trained to enforce PMP consistency conditions.

Result: The framework proves universal approximation capabilities of NHOs and highlights optimization challenges in learning such operators.

Conclusion: The NHO provides a rigorous, operator-theoretic approach to solving high-dimensional stochastic control problems with deep learning.

Abstract: Stochastic control problems in high dimensions are notoriously difficult to
solve due to the curse of dimensionality. An alternative to traditional dynamic
programming is Pontryagin's Maximum Principle (PMP), which recasts the problem
as a system of Forward-Backward Stochastic Differential Equations (FBSDEs). In
this paper, we introduce a formal framework for solving such problems with deep
learning by defining a \textbf{Neural Hamiltonian Operator (NHO)}. This
operator parameterizes the coupled FBSDE dynamics via neural networks that
represent the feedback control and an ansatz for the value function's spatial
gradient. We show how the optimal NHO can be found by training the underlying
networks to enforce the consistency conditions dictated by the PMP. By adopting
this operator-theoretic view, we situate the deep FBSDE method within the
rigorous language of statistical inference, framing it as a problem of learning
an unknown operator from simulated data. This perspective allows us to prove
the universal approximation capabilities of NHOs under general martingale
drivers and provides a clear lens for analyzing the significant optimization
challenges inherent to this class of models.

</details>


### [187] [ICLShield: Exploring and Mitigating In-Context Learning Backdoor Attacks](https://arxiv.org/abs/2507.01321)
*Zhiyao Ren,Siyuan Liang,Aishan Liu,Dacheng Tao*

Main category: cs.LG

TL;DR: The paper introduces the dual-learning hypothesis to explain how LLMs learn both task-relevant and backdoor concepts from poisoned ICL demonstrations. It proposes ICLShield, a defense mechanism that dynamically adjusts concept preference ratios to mitigate backdoor attacks, achieving superior performance (+26.02% on average).


<details>
  <summary>Details</summary>
Motivation: ICL's adaptability in LLMs makes it vulnerable to backdoor attacks via poisoned demonstrations, necessitating a defense mechanism.

Method: Proposes the dual-learning hypothesis and ICLShield, which adjusts concept preference ratios using confidence and similarity scores to select clean demonstrations.

Result: ICLShield outperforms existing defenses by 26.02% on average and works well even with closed-source models like GPT-4.

Conclusion: The dual-learning hypothesis and ICLShield provide a robust solution to mitigate backdoor attacks in ICL, enhancing LLM security.

Abstract: In-context learning (ICL) has demonstrated remarkable success in large
language models (LLMs) due to its adaptability and parameter-free nature.
However, it also introduces a critical vulnerability to backdoor attacks, where
adversaries can manipulate LLM behaviors by simply poisoning a few ICL
demonstrations. In this paper, we propose, for the first time, the
dual-learning hypothesis, which posits that LLMs simultaneously learn both the
task-relevant latent concepts and backdoor latent concepts within poisoned
demonstrations, jointly influencing the probability of model outputs. Through
theoretical analysis, we derive an upper bound for ICL backdoor effects,
revealing that the vulnerability is dominated by the concept preference ratio
between the task and the backdoor. Motivated by these findings, we propose
ICLShield, a defense mechanism that dynamically adjusts the concept preference
ratio. Our method encourages LLMs to select clean demonstrations during the ICL
phase by leveraging confidence and similarity scores, effectively mitigating
susceptibility to backdoor attacks. Extensive experiments across multiple LLMs
and tasks demonstrate that our method achieves state-of-the-art defense
effectiveness, significantly outperforming existing approaches (+26.02% on
average). Furthermore, our method exhibits exceptional adaptability and
defensive performance even for closed-source models (e.g., GPT-4).

</details>


### [188] [Reasoner for Real-World Event Detection: Scaling Reinforcement Learning via Adaptive Perplexity-Aware Sampling Strategy](https://arxiv.org/abs/2507.01327)
*Xiaoyun Zhang,Jingqing Ruan,Xing Ma,Yawen Zhu,Jiansong Chen,Ke Zeng,Xunliang Cai*

Main category: cs.LG

TL;DR: APARL framework improves abnormal event detection in customer service dialogues using dual-loop dynamic curriculum learning, enhancing adaptability and OOD generalization.


<details>
  <summary>Details</summary>
Motivation: The complexity of business data and dynamic customer interactions make abnormal event detection challenging, requiring strong OOD generalization for commercial value.

Method: Proposes APARL, a framework combining perplexity-aware reinforcement learning and dual-loop dynamic curriculum learning to focus on challenging samples progressively.

Result: Achieves 17.19% higher F1 score and 9.59% better OOD transferability in food delivery dialogue tasks.

Conclusion: APARL offers a superior solution for industrial anomaly detection, improving operational efficiency and commercial benefits.

Abstract: Detecting abnormal events in real-world customer service dialogues is highly
challenging due to the complexity of business data and the dynamic nature of
customer interactions. Moreover, models must demonstrate strong out-of-domain
(OOD) generalization to enable rapid adaptation across different business
scenarios and maximize commercial value. In this work, we propose a novel
Adaptive Perplexity-Aware Reinforcement Learning (APARL) framework that
leverages the advanced reasoning capabilities of large language models for
abnormal event detection. APARL introduces a dual-loop dynamic curriculum
learning architecture, enabling the model to progressively focus on more
challenging samples as its proficiency increases. This design effectively
addresses performance bottlenecks and significantly enhances OOD
transferability. Extensive evaluations on food delivery dialogue tasks show
that our model achieves significantly enhanced adaptability and robustness,
attaining the highest F1 score with an average improvement of 17.19\%, and an
average improvement of 9.59\% in OOD transfer tests. This method provides a
superior solution for industrial deployment of anomaly detection models,
contributing to improved operational efficiency and commercial benefits.

</details>


### [189] [Efficient Kilometer-Scale Precipitation Downscaling with Conditional Wavelet Diffusion](https://arxiv.org/abs/2507.01354)
*Chugang Yi,Minghan Yu,Weikang Qian,Yixin Wen,Haizhao Yang*

Main category: cs.LG

TL;DR: The paper introduces the Wavelet Diffusion Model (WDM) for downscaling precipitation data from 10 km to 1 km resolution, achieving high accuracy and speed.


<details>
  <summary>Details</summary>
Motivation: Standard global precipitation data (e.g., IMERG) lacks fine resolution (10 km), which is insufficient for hydrological modeling and extreme weather analysis.

Method: WDM is a conditional diffusion model that operates in the wavelet domain, learning precipitation structures from MRMS radar data and focusing on high-frequency wavelet coefficients for realistic 1-km outputs.

Result: WDM achieves 10x spatial super-resolution, 9x faster inference than pixel-based models, and produces visually superior results with fewer artifacts.

Conclusion: WDM addresses accuracy and speed challenges in geoscience super-resolution, enhancing hydrological forecasting reliability.

Abstract: Effective hydrological modeling and extreme weather analysis demand
precipitation data at a kilometer-scale resolution, which is significantly
finer than the 10 km scale offered by standard global products like IMERG. To
address this, we propose the Wavelet Diffusion Model (WDM), a generative
framework that achieves 10x spatial super-resolution (downscaling to 1 km) and
delivers a 9x inference speedup over pixel-based diffusion models. WDM is a
conditional diffusion model that learns the learns the complex structure of
precipitation from MRMS radar data directly in the wavelet domain. By focusing
on high-frequency wavelet coefficients, it generates exceptionally realistic
and detailed 1-km precipitation fields. This wavelet-based approach produces
visually superior results with fewer artifacts than pixel-space models, and
delivers a significant gains in sampling efficiency. Our results demonstrate
that WDM provides a robust solution to the dual challenges of accuracy and
speed in geoscience super-resolution, paving the way for more reliable
hydrological forecasts.

</details>


### [190] [Distributional Soft Actor-Critic with Diffusion Policy](https://arxiv.org/abs/2507.01381)
*Tong Liu,Yinuo Wang,Xujie Song,Wenjun Zou,Liangfa Chen,Likun Wang,Bin Shuai,Jingliang Duan,Shengbo Eben Li*

Main category: cs.LG

TL;DR: DSAC-D, a distributional RL algorithm, uses multimodal distributions to reduce value function bias and improve performance, achieving SOTA results in control tasks.


<details>
  <summary>Details</summary>
Motivation: Traditional unimodal distributions in RL cause bias in value function estimation, leading to poor performance. Addressing this requires a multimodal approach.

Method: DSAC-D introduces policy entropy and value distribution functions, constructs a diffusion value network for multi-peak distributions, and derives a dual diffusion algorithm for value and policy networks.

Result: DSAC-D achieves SOTA performance in 9 MuJoCo tasks, reduces estimation bias, and improves returns by 10%. Real-world tests confirm multimodal policy representation.

Conclusion: DSAC-D effectively addresses bias in value estimation and enables multimodal policy learning, outperforming existing methods in both simulation and real-world tasks.

Abstract: Reinforcement learning has been proven to be highly effective in handling
complex control tasks. Traditional methods typically use unimodal
distributions, such as Gaussian distributions, to model the output of value
distributions. However, unimodal distribution often and easily causes bias in
value function estimation, leading to poor algorithm performance. This paper
proposes a distributional reinforcement learning algorithm called DSAC-D
(Distributed Soft Actor Critic with Diffusion Policy) to address the challenges
of estimating bias in value functions and obtaining multimodal policy
representations. A multimodal distributional policy iteration framework that
can converge to the optimal policy was established by introducing policy
entropy and value distribution function. A diffusion value network that can
accurately characterize the distribution of multi peaks was constructed by
generating a set of reward samples through reverse sampling using a diffusion
model. Based on this, a distributional reinforcement learning algorithm with
dual diffusion of the value network and the policy network was derived. MuJoCo
testing tasks demonstrate that the proposed algorithm not only learns
multimodal policy, but also achieves state-of-the-art (SOTA) performance in all
9 control tasks, with significant suppression of estimation bias and total
average return improvement of over 10\% compared to existing mainstream
algorithms. The results of real vehicle testing show that DSAC-D can accurately
characterize the multimodal distribution of different driving styles, and the
diffusion policy network can characterize multimodal trajectories.

</details>


### [191] [Surrogate Modeling via Factorization Machine and Ising Model with Enhanced Higher-Order Interaction Learning](https://arxiv.org/abs/2507.01389)
*Anbang Wang,Dunbo Cai,Yu Zhang,Yangqing Huang,Xiangyang Feng,Zhihong Zhang*

Main category: cs.LG

TL;DR: An enhanced surrogate model using slack variables in factorization machines and Ising representations improves performance in predicting drug combination effects.


<details>
  <summary>Details</summary>
Motivation: To unify the two-step process of surrogate modeling into a single step and account for higher-order feature interactions.

Method: Incorporates slack variables into the factorization machine and Ising representation, iteratively updating them during training.

Result: Notable performance improvement in predicting drug combination effects.

Conclusion: The proposed algorithm is promising for efficient surrogate models leveraging quantum advantages.

Abstract: Recently, a surrogate model was proposed that employs a factorization machine
to approximate the underlying input-output mapping of the original system, with
quantum annealing used to optimize the resulting surrogate function. Inspired
by this approach, we propose an enhanced surrogate model that incorporates
additional slack variables into both the factorization machine and its
associated Ising representation thereby unifying what was by design a two-step
process into a single, integrated step. During the training phase, the slack
variables are iteratively updated, enabling the model to account for
higher-order feature interactions. We apply the proposed method to the task of
predicting drug combination effects. Experimental results indicate that the
introduction of slack variables leads to a notable improvement of performance.
Our algorithm offers a promising approach for building efficient surrogate
models that exploit potential quantum advantages.

</details>


### [192] [Decomposing Prediction Mechanisms for In-Context Recall](https://arxiv.org/abs/2507.01414)
*Sultan Daniels,Dylan Davis,Dhruv Gautam,Wentinn Liao,Gireeja Ranade,Anant Sahai*

Main category: cs.LG

TL;DR: The paper introduces a toy problem combining linear-regression-style ICL with discrete associative recall, studying transformer models' ability to recall and predict sequences. Two distinct mechanisms emerge: one for associative recall and another for Bayesian-style prediction, with different learning dynamics. Similar phenomena are observed in a real-world ICL task.


<details>
  <summary>Details</summary>
Motivation: To explore how transformer models handle tasks requiring both associative recall and continuous prediction, and to understand the underlying mechanisms and their learning dynamics.

Method: Pretraining transformer models on symbolic traces from a toy problem, analyzing training dynamics, and conducting out-of-distribution experiments and mechanistic analysis via edge pruning.

Result: Two separate mechanisms emerge: one for associative recall using symbolic labels and another for Bayesian-style prediction. These mechanisms exhibit different learning dynamics, with associative recall developing later. Similar patterns are found in a real-world ICL task.

Conclusion: Transformer models employ distinct mechanisms for associative recall and continuous prediction, with differing learning timelines. This phenomenon extends beyond toy problems, as evidenced by real-world task performance.

Abstract: We introduce a new family of toy problems that combine features of
linear-regression-style continuous in-context learning (ICL) with discrete
associative recall. We pretrain transformer models on sample traces from this
toy, specifically symbolically-labeled interleaved state observations from
randomly drawn linear deterministic dynamical systems. We study if the
transformer models can recall the state of a sequence previously seen in its
context when prompted to do so with the corresponding in-context label. Taking
a closer look at this task, it becomes clear that the model must perform two
functions: (1) identify which system's state should be recalled and apply that
system to its last seen state, and (2) continuing to apply the correct system
to predict the subsequent states. Training dynamics reveal that the first
capability emerges well into a model's training. Surprisingly, the second
capability, of continuing the prediction of a resumed sequence, develops much
earlier.
  Via out-of-distribution experiments, and a mechanistic analysis on model
weights via edge pruning, we find that next-token prediction for this toy
problem involves at least two separate mechanisms. One mechanism uses the
discrete symbolic labels to do the associative recall required to predict the
start of a resumption of a previously seen sequence. The second mechanism,
which is largely agnostic to the discrete symbolic labels, performs a
"Bayesian-style" prediction based on the previous token and the context. These
two mechanisms have different learning dynamics.
  To confirm that this multi-mechanism (manifesting as separate phase
transitions) phenomenon is not just an artifact of our toy setting, we used
OLMo training checkpoints on an ICL translation task to see a similar
phenomenon: a decisive gap in the emergence of first-task-token performance vs
second-task-token performance.

</details>


### [193] [Tensor Program Optimization for the RISC-V Vector Extension Using Probabilistic Programs](https://arxiv.org/abs/2507.01457)
*Federico Nicolas Peccia,Frederik Haxel,Oliver Bringmann*

Main category: cs.LG

TL;DR: The paper introduces a workflow using the TVM compiler to optimize AI workloads for RISC-V vector units, outperforming existing methods like GCC autovectorization and muRISCV-NN.


<details>
  <summary>Details</summary>
Motivation: Efficiently utilizing RISC-V Vector Extension (RVV) for AI workloads without expert knowledge is challenging due to lack of autotuning frameworks.

Method: Integrated RVV into TVM's MetaSchedule framework, tested on FPGA-implemented RISC-V SoCs and a commercial RISC-V SoC.

Result: Achieved 46% faster execution than GCC autovectorization, 29% faster than muRISCV-NN, and 35% faster than LLVM on a commercial SoC.

Conclusion: The proposed workflow is effective for optimizing AI workloads on RISC-V, with smaller code footprint and open-sourced for community expansion.

Abstract: RISC-V provides a flexible and scalable platform for applications ranging
from embedded devices to high-performance computing clusters. Particularly, its
RISC-V Vector Extension (RVV) becomes of interest for the acceleration of AI
workloads. But writing software that efficiently utilizes the vector units of
RISC-V CPUs without expert knowledge requires the programmer to rely on the
autovectorization features of compilers or hand-crafted libraries like
muRISCV-NN. Smarter approaches, like autotuning frameworks, have been missing
the integration with the RISC-V RVV extension, thus heavily limiting the
efficient deployment of complex AI workloads. In this paper, we present a
workflow based on the TVM compiler to efficiently map AI workloads onto RISC-V
vector units. Instead of relying on hand-crafted libraries, we integrated the
RVV extension into TVM's MetaSchedule framework, a probabilistic program
framework for tensor operation tuning. We implemented different RISC-V SoCs on
an FPGA and tuned a wide range of AI workloads on them. We found that our
proposal shows a mean improvement of 46% in execution latency when compared
against the autovectorization feature of GCC, and 29% against muRISCV-NN.
Moreover, the binary resulting from our proposal has a smaller code memory
footprint, making it more suitable for embedded devices. Finally, we also
evaluated our solution on a commercially available RISC-V SoC implementing the
RVV 1.0 Vector Extension and found our solution is able to find mappings that
are 35% faster on average than the ones proposed by LLVM. We open-sourced our
proposal for the community to expand it to target other RISC-V extensions.

</details>


### [194] [Self-Guided Process Reward Optimization with Masked Step Advantage for Process Reinforcement Learning](https://arxiv.org/abs/2507.01551)
*Wu Fei,Hao Kong,Shuxian Liang,Yang Lin,Yibo Yang,Jing Tang,Lei Chen,Xiansheng Hua*

Main category: cs.LG

TL;DR: SPRO is a novel framework for process-aware RL in LLMs, eliminating the need for external reward models by deriving process rewards intrinsically and introducing cumulative rewards and MSA for efficient training.


<details>
  <summary>Details</summary>
Motivation: Current PRL methods for LLMs incur high computational costs and lack a unified theoretical framework for process-level advantage estimation.

Method: SPRO derives process rewards from the policy model itself and introduces cumulative process rewards and Masked Step Advantage (MSA) for step-wise action advantage estimation.

Result: SPRO outperforms GRPO with 3.4x higher training efficiency, 17.5% better test accuracy, stable policy entropy, and reduced response length by ~1/3.

Conclusion: SPRO provides a computationally efficient, process-aware RL framework for LLMs, suitable for industrial implementation without additional overhead.

Abstract: Process Reinforcement Learning~(PRL) has demonstrated considerable potential
in enhancing the reasoning capabilities of Large Language Models~(LLMs).
However, introducing additional process reward models incurs substantial
computational overhead, and there is no unified theoretical framework for
process-level advantage estimation. To bridge this gap, we propose
\textbf{S}elf-Guided \textbf{P}rocess \textbf{R}eward
\textbf{O}ptimization~(\textbf{SPRO}), a novel framework that enables
process-aware RL through two key innovations: (1) we first theoretically
demonstrate that process rewards can be derived intrinsically from the policy
model itself, and (2) we introduce well-defined cumulative process rewards and
\textbf{M}asked \textbf{S}tep \textbf{A}dvantage (\textbf{MSA}), which
facilitates rigorous step-wise action advantage estimation within shared-prompt
sampling groups. Our experimental results demonstrate that SPRO outperforms
vaniila GRPO with 3.4x higher training efficiency and a 17.5\% test accuracy
improvement. Furthermore, SPRO maintains a stable and elevated policy entropy
throughout training while reducing the average response length by approximately
$1/3$, evidencing sufficient exploration and prevention of reward hacking.
Notably, SPRO incurs no additional computational overhead compared to
outcome-supervised RL methods such as GRPO, which benefit industrial
implementation.

</details>


### [195] [Cross-platform Smartphone Positioning at Museums](https://arxiv.org/abs/2507.01469)
*Alessio Ferrato,Fabio Gasparetti,Carla Limongelli,Stefano Mastandrea,Giuseppe Sansonetti,Joaquín Torres-Sospedra*

Main category: cs.LG

TL;DR: The paper introduces BAR, a novel RSS dataset for museum environments to address the lack of data for IPS development, and provides a baseline classification method using proximity and k-NN algorithms.


<details>
  <summary>Details</summary>
Motivation: Enhancing visitor experiences in cultural heritage institutions through IPSs is challenging due to environmental constraints and lack of relevant RSS datasets.

Method: Collected RSS data in front of 90 artworks across 13 museum rooms using Android and iOS, and developed a baseline classification method using proximity and k-NN algorithms.

Result: The BAR dataset and baseline method aim to facilitate the development and evaluation of positioning algorithms for museums.

Conclusion: The study highlights the potential of BAR for advancing IPS research in cultural heritage settings and suggests future research directions.

Abstract: Indoor Positioning Systems (IPSs) hold significant potential for enhancing
visitor experiences in cultural heritage institutions. By enabling personalized
navigation, efficient artifact organization, and better interaction with
exhibits, IPSs can transform the modalities of how individuals engage with
museums, galleries and libraries. However, these institutions face several
challenges in implementing IPSs, including environmental constraints, technical
limits, and limited experimentation. In other contexts, Received Signal
Strength (RSS)-based approaches using Bluetooth Low Energy (BLE) and WiFi have
emerged as preferred solutions due to their non-invasive nature and minimal
infrastructure requirements. Nevertheless, the lack of publicly available RSS
datasets that specifically reflect museum environments presents a substantial
barrier to developing and evaluating positioning algorithms designed for the
intricate spatial characteristics typical of cultural heritage sites. To
address this limitation, we present BAR, a novel RSS dataset collected in front
of 90 artworks across 13 museum rooms using two different platforms, i.e.,
Android and iOS. Additionally, we provide an advanced position classification
baseline taking advantage of a proximity-based method and $k$-NN algorithms. In
our analysis, we discuss the results and offer suggestions for potential
research directions.

</details>


### [196] [Zero-Incentive Dynamics: a look at reward sparsity through the lens of unrewarded subgoals](https://arxiv.org/abs/2507.01470)
*Yannick Molinghen,Tom Lenaerts*

Main category: cs.LG

TL;DR: The paper challenges the assumption that reward frequency measures task difficulty in reinforcement learning, identifying zero-incentive dynamics as a key issue where critical subgoals lack rewards.


<details>
  <summary>Details</summary>
Motivation: To address the limitation of current methods that fail when essential subgoals are unrewarded, hindering policy learning.

Method: Analyzes zero-incentive dynamics and evaluates state-of-the-art deep subgoal-based algorithms under these conditions.

Result: Shows these algorithms struggle with unrewarded subgoals, with performance tied to reward timing.

Conclusion: Highlights a fundamental flaw in current approaches, advocating for mechanisms to infer latent task structure beyond immediate rewards.

Abstract: This work re-examines the commonly held assumption that the frequency of
rewards is a reliable measure of task difficulty in reinforcement learning. We
identify and formalize a structural challenge that undermines the effectiveness
of current policy learning methods: when essential subgoals do not directly
yield rewards. We characterize such settings as exhibiting zero-incentive
dynamics, where transitions critical to success remain unrewarded. We show that
state-of-the-art deep subgoal-based algorithms fail to leverage these dynamics
and that learning performance is highly sensitive to the temporal proximity
between subgoal completion and eventual reward. These findings reveal a
fundamental limitation in current approaches and point to the need for
mechanisms that can infer latent task structure without relying on immediate
incentives.

</details>


### [197] [Loss Functions in Diffusion Models: A Comparative Study](https://arxiv.org/abs/2507.01516)
*Dibyanshu Kumar,Philipp Vaeth,Magda Gregorová*

Main category: cs.LG

TL;DR: The paper explores and unifies various loss functions in diffusion models under the variational lower bound framework, analyzing their performance differences and impact on model goals like sample quality and likelihood estimation.


<details>
  <summary>Details</summary>
Motivation: To understand and systematize the diverse loss functions used in diffusion models, clarifying their relationships and performance implications.

Method: Theoretical analysis unifying loss functions under the variational lower bound, complemented by empirical studies on performance divergence and goal achievement.

Result: A unified framework for loss functions in diffusion models, with insights into their performance differences and suitability for specific goals.

Conclusion: The study provides a comprehensive understanding of loss functions, aiding in more efficient and goal-oriented diffusion model designs.

Abstract: Diffusion models have emerged as powerful generative models, inspiring
extensive research into their underlying mechanisms. One of the key questions
in this area is the loss functions these models shall train with. Multiple
formulations have been introduced in the literature over the past several years
with some links and some critical differences stemming from various initial
considerations. In this paper, we explore the different target objectives and
corresponding loss functions in detail. We present a systematic overview of
their relationships, unifying them under the framework of the variational lower
bound objective. We complement this theoretical analysis with an empirical
study providing insights into the conditions under which these objectives
diverge in performance and the underlying factors contributing to such
deviations. Additionally, we evaluate how the choice of objective impacts the
model ability to achieve specific goals, such as generating high-quality
samples or accurately estimating likelihoods. This study offers a unified
understanding of loss functions in diffusion models, contributing to more
efficient and goal-oriented model designs in future research.

</details>


### [198] [Blending Supervised and Reinforcement Fine-Tuning with Prefix Sampling](https://arxiv.org/abs/2507.01679)
*Zeyu Huang,Tianhao Cheng,Zihan Qiu,Zili Wang,Yinghui Xu,Edoardo M. Ponti,Ivan Titov*

Main category: cs.LG

TL;DR: Prefix-RFT combines SFT and RFT, outperforming both and integrating easily into existing frameworks.


<details>
  <summary>Details</summary>
Motivation: Address the trade-offs between SFT (good at mimicking but poor generalization) and RFT (performance-sensitive but prone to unexpected behaviors).

Method: Introduces Prefix-RFT, a hybrid approach combining demonstration (SFT) and exploration (RFT) for LLM post-training.

Result: Prefix-RFT surpasses standalone SFT and RFT, outperforms mixed-policy RFT, and is robust to data variations.

Conclusion: A unified paradigm integrating demonstration and exploration is promising for future LLM post-training research.

Abstract: Existing post-training techniques for large language models are broadly
categorized into Supervised Fine-Tuning (SFT) and Reinforcement Fine-Tuning
(RFT). Each paradigm presents a distinct trade-off: SFT excels at mimicking
demonstration data but can lead to problematic generalization as a form of
behavior cloning. Conversely, RFT can significantly enhance a model's
performance but is prone to learn unexpected behaviors, and its performance is
highly sensitive to the initial policy. In this paper, we propose a unified
view of these methods and introduce Prefix-RFT, a hybrid approach that
synergizes learning from both demonstration and exploration. Using mathematical
reasoning problems as a testbed, we empirically demonstrate that Prefix-RFT is
both simple and effective. It not only surpasses the performance of standalone
SFT and RFT but also outperforms parallel mixed-policy RFT methods. A key
advantage is its seamless integration into existing open-source frameworks,
requiring only minimal modifications to the standard RFT pipeline. Our analysis
highlights the complementary nature of SFT and RFT, and validates that
Prefix-RFT effectively harmonizes these two learning paradigms. Furthermore,
ablation studies confirm the method's robustness to variations in the quality
and quantity of demonstration data. We hope this work offers a new perspective
on LLM post-training, suggesting that a unified paradigm that judiciously
integrates demonstration and exploration could be a promising direction for
future research.

</details>


### [199] [Chargax: A JAX Accelerated EV Charging Simulator](https://arxiv.org/abs/2507.01522)
*Koen Ponse,Jan Felix Kleuker,Aske Plaat,Thomas Moerland*

Main category: cs.LG

TL;DR: Chargax is a JAX-based environment for simulating electric vehicle charging stations, offering 100x-1000x faster training for RL agents compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: Addressing inefficiencies in grid systems and accelerating RL training for sustainable energy solutions.

Method: Developed Chargax, a modular JAX-based environment for realistic EV charging simulations, validated with real data.

Result: Achieved 100x-1000x computational performance improvements and flexibility in representing real-world configurations.

Conclusion: Chargax enables efficient RL training for sustainable energy applications, outperforming traditional environments.

Abstract: Deep Reinforcement Learning can play a key role in addressing sustainable
energy challenges. For instance, many grid systems are heavily congested,
highlighting the urgent need to enhance operational efficiency. However,
reinforcement learning approaches have traditionally been slow due to the high
sample complexity and expensive simulation requirements. While recent works
have effectively used GPUs to accelerate data generation by converting
environments to JAX, these works have largely focussed on classical toy
problems. This paper introduces Chargax, a JAX-based environment for realistic
simulation of electric vehicle charging stations designed for accelerated
training of RL agents. We validate our environment in a variety of scenarios
based on real data, comparing reinforcement learning agents against baselines.
Chargax delivers substantial computational performance improvements of over
100x-1000x over existing environments. Additionally, Chargax' modular
architecture enables the representation of diverse real-world charging station
configurations.

</details>


### [200] [MARVIS: Modality Adaptive Reasoning over VISualizations](https://arxiv.org/abs/2507.01544)
*Benjamin Feuer,Lennart Purucker,Oussama Elachqar,Chinmay Hegde*

Main category: cs.LG

TL;DR: MARVIS enables small vision-language models to predict any data modality accurately without training, outperforming Gemini by 16% and nearing specialized methods.


<details>
  <summary>Details</summary>
Motivation: Specialized models lack flexibility, while foundation models underperform in non-traditional modalities. MARVIS bridges this gap.

Method: MARVIS transforms latent embeddings into visual representations, leveraging VLMs' reasoning skills for interpretation.

Result: Competitive performance across vision, audio, biological, and tabular domains using a single 3B parameter model.

Conclusion: MARVIS offers versatile, high-accuracy predictions without P.I.I. exposure or domain-specific training, with open-sourced code.

Abstract: Scientific applications of machine learning often rely on small, specialized
models tuned to particular domains. Such models often achieve excellent
performance, but lack flexibility. Foundation models offer versatility, but
typically underperform specialized approaches, especially on non-traditional
modalities and long-tail domains. We propose MARVIS (Modality Adaptive
Reasoning over VISualizations), a training-free method that enables even small
vision-language models to predict any data modality with high accuracy. MARVIS
transforms latent embedding spaces into visual representations and then
leverages the spatial and fine-grained reasoning skills of VLMs to successfully
interpret and utilize them. MARVIS achieves competitive performance on vision,
audio, biological, and tabular domains using a single 3B parameter model,
achieving results that beat Gemini by 16\% on average and approach specialized
methods, without exposing personally identifiable information (P.I.I.) or
requiring any domain-specific training. We open source our code and datasets at
https://github.com/penfever/marvis

</details>


### [201] [Tuning without Peeking: Provable Privacy and Generalization Bounds for LLM Post-Training](https://arxiv.org/abs/2507.01752)
*Ismail Labiad,Mathurin Videau,Matthieu Kowalski,Marc Schoenauer,Alessandro Leite,Julia Kempe,Olivier Teytaud*

Main category: cs.LG

TL;DR: BBoxER is an evolutionary black-box method for LLM post-training, offering privacy, security, and generalization benefits over gradient-based optimization.


<details>
  <summary>Details</summary>
Motivation: Address privacy, security, and overfitting concerns in gradient-based optimization for deep learning, especially in restricted or adversarial scenarios.

Method: Introduces BBoxER, an evolutionary black-box method leveraging implicit data compression and information flow tractability for LLM post-training.

Result: BBoxER provides theoretical guarantees on generalization, privacy, and robustness, and improves performance on reasoning benchmarks.

Conclusion: BBoxER is a lightweight, modular enhancement for LLMs, suitable for privacy-sensitive environments and complementary to gradient-based methods.

Abstract: Gradient-based optimization is the workhorse of deep learning, offering
efficient and scalable training via backpropagation. However, its reliance on
large volumes of labeled data raises privacy and security concerns such as
susceptibility to data poisoning attacks and the risk of overfitting. In
contrast, black box optimization methods, which treat the model as an opaque
function, relying solely on function evaluations to guide optimization, offer a
promising alternative in scenarios where data access is restricted, adversarial
risks are high, or overfitting is a concern. However, black box methods also
pose significant challenges, including poor scalability to high-dimensional
parameter spaces, as prevalent in large language models (LLMs), and high
computational costs due to reliance on numerous model evaluations. This paper
introduces BBoxER, an evolutionary black-box method for LLM post-training that
induces an information bottleneck via implicit compression of the training
data. Leveraging the tractability of information flow, we provide strong
theoretical bounds on generalization, differential privacy, susceptibility to
data poisoning attacks, and robustness to extraction attacks. BBoxER operates
on top of pre-trained LLMs, offering a lightweight and modular enhancement
suitable for deployment in restricted or privacy-sensitive environments, in
addition to non-vacuous generalization guarantees. In experiments with LLMs, we
demonstrate empirically that Retrofitting methods are able to learn, showing
how a few iterations of BBoxER improve performance and generalize well on a
benchmark of reasoning datasets. This positions BBoxER as an attractive add-on
on top of gradient-based optimization.

</details>


### [202] [LoRA Fine-Tuning Without GPUs: A CPU-Efficient Meta-Generation Framework for LLMs](https://arxiv.org/abs/2507.01806)
*Reza Arabpour,Haitz Sáez de Ocáriz Borde,Anastasis Kratsios*

Main category: cs.LG

TL;DR: A CPU-friendly method for LoRA fine-tuning of LLMs, using pre-trained adapters to create new LoRAs without GPU, though slightly less performant than GPU-trained ones.


<details>
  <summary>Details</summary>
Motivation: To make LoRA fine-tuning accessible to users with limited computational resources (e.g., laptop CPUs), avoiding GPU dependency.

Method: Learns a meta-operator to map input datasets to LoRA weights by combining pre-trained adapters, bypassing gradient updates.

Result: Adapters outperform the base Mistral model but fall short of GPU-trained LoRAs.

Conclusion: Provides a practical, resource-efficient alternative to GPU-based fine-tuning for LoRA adapters.

Abstract: Low-Rank Adapters (LoRAs) have transformed the fine-tuning of Large Language
Models (LLMs) by enabling parameter-efficient updates. However, their
widespread adoption remains limited by the reliance on GPU-based training. In
this work, we propose a theoretically grounded approach to LoRA fine-tuning
designed specifically for users with limited computational resources,
particularly those restricted to standard laptop CPUs. Our method learns a
meta-operator that maps any input dataset, represented as a probability
distribution, to a set of LoRA weights by leveraging a large bank of
pre-trained adapters for the Mistral-7B-Instruct-v0.2 model. Instead of
performing new gradient-based updates, our pipeline constructs adapters via
lightweight combinations of existing LoRAs directly on CPU. While the resulting
adapters do not match the performance of GPU-trained counterparts, they
consistently outperform the base Mistral model on downstream tasks, offering a
practical and accessible alternative to traditional GPU-based fine-tuning.

</details>


### [203] [How Weight Resampling and Optimizers Shape the Dynamics of Continual Learning and Forgetting in Neural Networks](https://arxiv.org/abs/2507.01559)
*Lapo Frati,Neil Traft,Jeff Clune,Nick Cheney*

Main category: cs.LG

TL;DR: The paper explores the impact of resampling weights ("zapping") in neural networks during continual learning, revealing faster domain transfer recovery and complex task interactions influenced by optimizer choice.


<details>
  <summary>Details</summary>
Motivation: To understand the unclear mechanisms behind the effectiveness of zapping in continual learning and its effects on learning and forgetting in neural networks.

Method: Investigates learning and forgetting patterns in convolutional neural networks under continual and few-shot transfer learning, using handwritten characters and natural images.

Result: Zapping accelerates recovery in new domains, and optimizer choice significantly influences learning dynamics and task interactions.

Conclusion: Zapping and optimizer selection critically shape continual learning outcomes, revealing intricate task synergies and interferences.

Abstract: Recent work in continual learning has highlighted the beneficial effect of
resampling weights in the last layer of a neural network (``zapping"). Although
empirical results demonstrate the effectiveness of this approach, the
underlying mechanisms that drive these improvements remain unclear. In this
work, we investigate in detail the pattern of learning and forgetting that take
place inside a convolutional neural network when trained in challenging
settings such as continual learning and few-shot transfer learning, with
handwritten characters and natural images. Our experiments show that models
that have undergone zapping during training more quickly recover from the shock
of transferring to a new domain. Furthermore, to better observe the effect of
continual learning in a multi-task setting we measure how each individual task
is affected. This shows that, not only zapping, but the choice of optimizer can
also deeply affect the dynamics of learning and forgetting, causing complex
patterns of synergy/interference between tasks to emerge when the model learns
sequentially at transfer time.

</details>


### [204] [Test-Time Scaling with Reflective Generative Model](https://arxiv.org/abs/2507.01951)
*Zixiao Wang,Yuxin Wang,Xiaorui Wang,Mengting Xing,Jie Gao,Jianjun Xu,Guangcan Liu,Chenhui Jin,Zhuo Wang,Shengzhuo Zhang,Hongtao Xie*

Main category: cs.LG

TL;DR: MetaStone-S1 is a reflective generative model achieving OpenAI o3's performance via a self-supervised process reward model (SPRM), reducing PRM parameters by 99% and enabling efficient reasoning with controllable thinking length.


<details>
  <summary>Details</summary>
Motivation: To integrate policy and process reward models efficiently without extra annotations, enabling scalable reasoning.

Method: Uses SPRM with shared backbone and task-specific heads for next token prediction and process scoring, supporting test-time scaling (TTS) with three effort modes.

Result: Achieves performance comparable to OpenAI-o3-mini with 32B parameters and establishes a scaling law for TTS.

Conclusion: MetaStone-S1 is a scalable, efficient model open-sourced for community use.

Abstract: We introduce our first reflective generative model MetaStone-S1, which
obtains OpenAI o3's performance via the self-supervised process reward model
(SPRM). Through sharing the backbone network and using task-specific heads for
next token prediction and process scoring respectively, SPRM successfully
integrates the policy model and process reward model(PRM) into a unified
interface without extra process annotation, reducing over 99% PRM parameters
for efficient reasoning. Equipped with SPRM, MetaStone-S1 is naturally suitable
for test time scaling (TTS), and we provide three reasoning effort modes (low,
medium, and high), based on the controllable thinking length. Moreover, we
empirically establish a scaling law that reveals the relationship between total
thinking computation and TTS performance. Experiments demonstrate that our
MetaStone-S1 achieves comparable performance to OpenAI-o3-mini's series with
only 32B parameter size. To support the research community, we have
open-sourced MetaStone-S1 at https://github.com/MetaStone-AI/MetaStone-S1.

</details>


### [205] [A Privacy-Preserving Indoor Localization System based on Hierarchical Federated Learning](https://arxiv.org/abs/2507.01581)
*Masood Jan,Wafa Njima,Xun Zhang*

Main category: cs.LG

TL;DR: Proposes a Federated Learning (FL)-based approach for indoor localization using DNN, addressing privacy and efficiency issues of centralized methods.


<details>
  <summary>Details</summary>
Motivation: Traditional indoor localization has high errors and privacy concerns due to centralized data. ML helps but still faces privacy, bandwidth, and reliability issues.

Method: Uses Federated Learning (FL) with a Deep Neural Network (DNN) to enable decentralized, privacy-preserving indoor localization.

Result: FL achieves performance close to centralized models while ensuring data privacy, bandwidth efficiency, and server reliability.

Conclusion: The FL approach is a viable, privacy-enhanced solution for indoor localization, advancing secure and efficient systems.

Abstract: Location information serves as the fundamental element for numerous Internet
of Things (IoT) applications. Traditional indoor localization techniques often
produce significant errors and raise privacy concerns due to centralized data
collection. In response, Machine Learning (ML) techniques offer promising
solutions by capturing indoor environment variations. However, they typically
require central data aggregation, leading to privacy, bandwidth, and server
reliability issues. To overcome these challenges, in this paper, we propose a
Federated Learning (FL)-based approach for dynamic indoor localization using a
Deep Neural Network (DNN) model. Experimental results show that FL has the
nearby performance to Centralized Model (CL) while keeping the data privacy,
bandwidth efficiency and server reliability. This research demonstrates that
our proposed FL approach provides a viable solution for privacy-enhanced indoor
localization, paving the way for advancements in secure and efficient indoor
localization systems.

</details>


### [206] [Analysis of Muon's Convergence and Critical Batch Size](https://arxiv.org/abs/2507.01598)
*Naoki Sato,Hiroki Naganuma,Hideaki Iiduka*

Main category: cs.LG

TL;DR: The paper analyzes Muon, a neural network optimizer, proving convergence for its variants and showing tighter bounds with weight decay. It also derives Muon's critical batch size and validates findings experimentally.


<details>
  <summary>Details</summary>
Motivation: To theoretically analyze Muon, a new optimizer leveraging matrix structure in neural networks, and explore its variants and properties like weight decay and batch size.

Method: Theoretical analysis of Muon's convergence for four variants (with/without Nesterov momentum and weight decay). Derivation of bounds and relationship between weight decay and learning rate. Calculation of critical batch size for SFO complexity. Experimental validation.

Result: Convergence proofs for Muon variants, tighter bounds with weight decay, clarified weight decay-learning rate relationship, and derived critical batch size. Experimental validation supports theory.

Conclusion: Muon's theoretical properties are rigorously analyzed, with practical implications for optimization in neural networks, supported by experiments.

Abstract: This paper presents a theoretical analysis of Muon, a new optimizer that
leverages the inherent matrix structure of neural network parameters. We
provide convergence proofs for four practical variants of Muon: with and
without Nesterov momentum, and with and without weight decay. We then show that
adding weight decay leads to strictly tighter bounds on both the parameter and
gradient norms, and we clarify the relationship between the weight decay
coefficient and the learning rate. Finally, we derive Muon's critical batch
size minimizing the stochastic first-order oracle (SFO) complexity, which is
the stochastic computational cost, and validate our theoretical findings with
experiments.

</details>


### [207] [Kernel Recursive Least Squares Dictionary Learning Algorithm](https://arxiv.org/abs/2507.01636)
*Ghasem Alipoor,Karl Skretting*

Main category: cs.LG

TL;DR: An efficient online dictionary learning algorithm for kernel-based sparse representations, outperforming existing methods with low computational complexity.


<details>
  <summary>Details</summary>
Motivation: To improve online kernel dictionary learning by efficiently updating dictionaries in high-dimensional feature spaces.

Method: Uses recursive least squares (RLS) for dictionary updates, working with single samples or mini-batches.

Result: Outperforms existing online methods and achieves batch-trained model accuracy with higher efficiency.

Conclusion: The proposed method is efficient and effective for online kernel dictionary learning.

Abstract: We propose an efficient online dictionary learning algorithm for kernel-based
sparse representations. In this framework, input signals are nonlinearly mapped
to a high-dimensional feature space and represented sparsely using a virtual
dictionary. At each step, the dictionary is updated recursively using a novel
algorithm based on the recursive least squares (RLS) method. This update
mechanism works with single samples or mini-batches and maintains low
computational complexity. Experiments on four datasets across different domains
show that our method not only outperforms existing online kernel dictionary
learning approaches but also achieves classification accuracy close to that of
batch-trained models, while remaining significantly more efficient.

</details>


### [208] [Dance Dance ConvLSTM](https://arxiv.org/abs/2507.01644)
*Miguel O'Malley*

Main category: cs.LG

TL;DR: DDCL improves DDR chart generation accuracy over DDC using a ConvLSTM model.


<details>
  <summary>Details</summary>
Motivation: To enhance the automatic generation of DDR charts, improving upon the existing DDC method.

Method: Uses a ConvLSTM-based model for chart generation, building on the CNN-LSTM architecture of DDC.

Result: Substantially increases the accuracy of DDR chart generation.

Conclusion: DDCL outperforms DDC, offering a more accurate method for automatic DDR chart creation.

Abstract: \textit{Dance Dance Revolution} is a rhythm game consisting of songs and
accompanying choreography, referred to as charts. Players press arrows on a
device referred to as a dance pad in time with steps determined by the song's
chart. In 2017, the authors of Dance Dance Convolution (DDC) developed an
algorithm for the automatic generation of \textit{Dance Dance Revolution}
charts, utilizing a CNN-LSTM architecture. We introduce Dance Dance ConvLSTM
(DDCL), a new method for the automatic generation of DDR charts using a
ConvLSTM based model, which improves upon the DDC methodology and substantially
increases the accuracy of chart generation.

</details>


### [209] [GradMetaNet: An Equivariant Architecture for Learning on Gradients](https://arxiv.org/abs/2507.01649)
*Yoav Gelberg,Yam Eitan,Aviv Navon,Aviv Shamsian,Theo,Putterman,Michael Bronstein,Haggai Maron*

Main category: cs.LG

TL;DR: The paper introduces GradMetaNet, a novel architecture for processing gradients in neural networks, guided by principles of equivariance, multi-point gradient processing, and efficient representation. It outperforms previous methods on tasks like optimization and model editing.


<details>
  <summary>Details</summary>
Motivation: Existing methods for gradient processing lack specialized architectures, limiting their effectiveness. The paper aims to design a principled architecture for gradient-based tasks.

Method: The approach involves three principles: equivariant design, multi-point gradient processing, and rank-1 decomposition. GradMetaNet is introduced as a novel architecture built from equivariant blocks.

Result: GradMetaNet achieves universality and outperforms prior methods on tasks like learned optimization, INR editing, and curvature estimation.

Conclusion: GradMetaNet provides a robust and efficient solution for gradient-based tasks, demonstrating superior performance and broader applicability.

Abstract: Gradients of neural networks encode valuable information for optimization,
editing, and analysis of models. Therefore, practitioners often treat gradients
as inputs to task-specific algorithms, e.g. for pruning or optimization. Recent
works explore learning algorithms that operate directly on gradients but use
architectures that are not specifically designed for gradient processing,
limiting their applicability. In this paper, we present a principled approach
for designing architectures that process gradients. Our approach is guided by
three principles: (1) equivariant design that preserves neuron permutation
symmetries, (2) processing sets of gradients across multiple data points to
capture curvature information, and (3) efficient gradient representation
through rank-1 decomposition. Based on these principles, we introduce
GradMetaNet, a novel architecture for learning on gradients, constructed from
simple equivariant blocks. We prove universality results for GradMetaNet, and
show that previous approaches cannot approximate natural gradient-based
functions that GradMetaNet can. We then demonstrate GradMetaNet's effectiveness
on a diverse set of gradient-based tasks on MLPs and transformers, such as
learned optimization, INR editing, and estimating loss landscape curvature.

</details>


### [210] [AsyncFlow: An Asynchronous Streaming RL Framework for Efficient LLM Post-Training](https://arxiv.org/abs/2507.01663)
*Zhenyu Han,Ansheng You,Haibo Wang,Kui Luo,Guang Yang,Wenqi Shi,Menglong Chen,Sicheng Zhang,Zeshun Lan,Chunshi Deng,Huazhong Ji,Wenjie Liu,Yu Huang,Yixiang Zhang,Chenyi Pan,Jing Wang,Xin Huang,Chunsheng Li,Jianping Wu*

Main category: cs.LG

TL;DR: AsyncFlow is an asynchronous streaming RL framework for efficient post-training of LLMs, addressing scalability and resource issues in traditional RL frameworks.


<details>
  <summary>Details</summary>
Motivation: Traditional RL frameworks for LLMs face scalability bottlenecks, complex dataflows, and tight coupling with training/inference engines, limiting flexibility and efficiency.

Method: Proposes AsyncFlow with distributed data storage, fine-grained scheduling, and a producer-consumer workflow to minimize idleness and decouple from engines.

Result: Achieves 1.59x throughput improvement over state-of-the-art baselines.

Conclusion: AsyncFlow offers modular, efficient RL training, providing insights for future system designs.

Abstract: Reinforcement learning (RL) has become a pivotal technology in the
post-training phase of large language models (LLMs). Traditional task-colocated
RL frameworks suffer from significant scalability bottlenecks, while
task-separated RL frameworks face challenges in complex dataflows and the
corresponding resource idling and workload imbalance. Moreover, most existing
frameworks are tightly coupled with LLM training or inference engines, making
it difficult to support custom-designed engines. To address these challenges,
we propose AsyncFlow, an asynchronous streaming RL framework for efficient
post-training. Specifically, we introduce a distributed data storage and
transfer module that provides a unified data management and fine-grained
scheduling capability in a fully streamed manner. This architecture inherently
facilitates automated pipeline overlapping among RL tasks and dynamic load
balancing. Moreover, we propose a producer-consumer-based asynchronous workflow
engineered to minimize computational idleness by strategically deferring
parameter update process within staleness thresholds. Finally, the core
capability of AsynFlow is architecturally decoupled from underlying training
and inference engines and encapsulated by service-oriented user interfaces,
offering a modular and customizable user experience. Extensive experiments
demonstrate an average of 1.59 throughput improvement compared with
state-of-the-art baseline. The presented architecture in this work provides
actionable insights for next-generation RL training system designs.

</details>


### [211] [GPT, But Backwards: Exactly Inverting Language Model Outputs](https://arxiv.org/abs/2507.01693)
*Adrians Skapars,Edoardo Manino,Youcheng Sun,Lucas C. Cordeiro*

Main category: cs.LG

TL;DR: SODA, a gradient-based algorithm, reconstructs exact inputs from LLM outputs, outperforming existing methods with 79.5% success on short inputs, but struggles with longer sequences.


<details>
  <summary>Details</summary>
Motivation: To enable post-incident analysis and detect fake outputs by reconstructing the exact input from LLM outputs.

Method: Formalizes input reconstruction as a discrete optimization problem, introduces SODA with gradient-based search, periodic restarts, and parameter decay.

Result: SODA recovers 79.5% of short out-of-distribution inputs with no false positives but fails on longer sequences (15+ tokens).

Conclusion: Standard LLM deployment may currently protect against malicious use of SODA, as it struggles with longer inputs.

Abstract: While existing auditing techniques attempt to identify potential unwanted
behaviours in large language models (LLMs), we address the complementary
forensic problem of reconstructing the exact input that led to an existing LLM
output - enabling post-incident analysis and potentially the detection of fake
output reports. We formalize exact input reconstruction as a discrete
optimisation problem with a unique global minimum and introduce SODA, an
efficient gradient-based algorithm that operates on a continuous relaxation of
the input search space with periodic restarts and parameter decay. Through
comprehensive experiments on LLMs ranging in size from 33M to 3B parameters, we
demonstrate that SODA significantly outperforms existing approaches. We succeed
in fully recovering 79.5% of shorter out-of-distribution inputs from next-token
logits, without a single false positive, but struggle to extract private
information from the outputs of longer (15+ token) input sequences. This
suggests that standard deployment practices may currently provide adequate
protection against malicious use of our method. Our code is available at
https://doi.org/10.5281/zenodo.15539879.

</details>


### [212] [PERTINENCE: Input-based Opportunistic Neural Network Dynamic Execution](https://arxiv.org/abs/2507.01695)
*Omkar Shende,Gayathri Ananthanarayanan,Marcello Traiola*

Main category: cs.LG

TL;DR: PERTINENCE is a method that dynamically selects the most suitable pre-trained DNN model for a given input, balancing accuracy and computational efficiency using a genetic algorithm.


<details>
  <summary>Details</summary>
Motivation: Large DNN models are resource-intensive, but their high computational cost is often unnecessary for simpler inputs. Combining models dynamically can improve efficiency without sacrificing accuracy.

Method: PERTINENCE uses a genetic algorithm to train an ML-based dispatcher that selects models based on input complexity, optimizing for accuracy and efficiency.

Result: Tested on CNNs and ViTs, PERTINENCE achieves comparable accuracy with up to 36% fewer operations.

Conclusion: Dynamic model selection via PERTINENCE offers a practical way to reduce computational costs while maintaining accuracy.

Abstract: Deep neural networks (DNNs) have become ubiquitous thanks to their remarkable
ability to model complex patterns across various domains such as computer
vision, speech recognition, robotics, etc. While large DNN models are often
more accurate than simpler, lightweight models, they are also resource- and
energy-hungry. Hence, it is imperative to design methods to reduce reliance on
such large models without significant degradation in output accuracy. The high
computational cost of these models is often necessary only for a reduced set of
challenging inputs, while lighter models can handle most simple ones. Thus,
carefully combining properties of existing DNN models in a dynamic, input-based
way opens opportunities to improve efficiency without impacting accuracy.
  In this work, we introduce PERTINENCE, a novel online method designed to
analyze the complexity of input features and dynamically select the most
suitable model from a pre-trained set to process a given input effectively. To
achieve this, we employ a genetic algorithm to explore the training space of an
ML-based input dispatcher, enabling convergence towards the Pareto front in the
solution space that balances overall accuracy and computational efficiency.
  We showcase our approach on state-of-the-art Convolutional Neural Networks
(CNNs) trained on the CIFAR-10 and CIFAR-100, as well as Vision Transformers
(ViTs) trained on TinyImageNet dataset. We report results showing PERTINENCE's
ability to provide alternative solutions to existing state-of-the-art models in
terms of trade-offs between accuracy and number of operations. By
opportunistically selecting among models trained for the same task, PERTINENCE
achieves better or comparable accuracy with up to 36% fewer operations.

</details>


### [213] [Variational Graph Convolutional Neural Networks](https://arxiv.org/abs/2507.01699)
*Illia Oleksiienko,Juho Kanniainen,Alexandros Iosifidis*

Main category: cs.LG

TL;DR: Proposes Variational Neural Network versions of spatial and spatio-temporal Graph Convolutional Networks to estimate uncertainty, improving explainability and accuracy.


<details>
  <summary>Details</summary>
Motivation: To enhance explainability and accuracy of Graph Convolutional Networks by estimating model uncertainty, useful for critical applications.

Method: Variational Neural Network versions of spatial and spatio-temporal Graph Convolutional Networks, estimating uncertainty in outputs and layer-wise attentions.

Result: Improved model accuracy and estimated uncertainties showcased in social trading analysis and human action recognition tasks.

Conclusion: The proposed method effectively enhances model explainability and accuracy through uncertainty estimation.

Abstract: Estimation of model uncertainty can help improve the explainability of Graph
Convolutional Networks and the accuracy of the models at the same time.
Uncertainty can also be used in critical applications to verify the results of
the model by an expert or additional models. In this paper, we propose
Variational Neural Network versions of spatial and spatio-temporal Graph
Convolutional Networks. We estimate uncertainty in both outputs and layer-wise
attentions of the models, which has the potential for improving model
explainability. We showcase the benefits of these models in the social trading
analysis and the skeleton-based human action recognition tasks on the Finnish
board membership, NTU-60, NTU-120 and Kinetics datasets, where we show
improvement in model accuracy in addition to estimated model uncertainties.

</details>


### [214] [Relational Causal Discovery with Latent Confounders](https://arxiv.org/abs/2507.01700)
*Andrea Piras,Matteo Negro,Ragib Ahsan,David Arbour,Elena Zheleva*

Main category: cs.LG

TL;DR: RelFCI is a new causal discovery algorithm for relational data with latent confounders, addressing gaps in existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing causal discovery methods assume i.i.d. data or causal sufficiency, which are unrealistic for relational data with latent confounders.

Method: RelFCI builds on FCI and RCD, introducing new graphical models and relational d-separation for sound and complete causal discovery.

Result: Experiments show RelFCI effectively identifies correct causal structures in relational models with latent confounders.

Conclusion: RelFCI fills a critical gap in causal discovery for relational data with latent confounders, offering sound and complete guarantees.

Abstract: Estimating causal effects from real-world relational data can be challenging
when the underlying causal model and potential confounders are unknown. While
several causal discovery algorithms exist for learning causal models with
latent confounders from data, they assume that the data is independent and
identically distributed (i.i.d.) and are not well-suited for learning from
relational data. Similarly, existing relational causal discovery algorithms
assume causal sufficiency, which is unrealistic for many real-world datasets.
To address this gap, we propose RelFCI, a sound and complete causal discovery
algorithm for relational data with latent confounders. Our work builds upon the
Fast Causal Inference (FCI) and Relational Causal Discovery (RCD) algorithms
and it defines new graphical models, necessary to support causal discovery in
relational domains. We also establish soundness and completeness guarantees for
relational d-separation with latent confounders. We present experimental
results demonstrating the effectiveness of RelFCI in identifying the correct
causal structure in relational causal models with latent confounders.

</details>


### [215] [B-PL-PINN: Stabilizing PINN Training with Bayesian Pseudo Labeling](https://arxiv.org/abs/2507.01714)
*Kevin Innerebner,Franz M. Rohrhofer,Bernhard C. Geiger*

Main category: cs.LG

TL;DR: The paper proposes replacing ensemble-based PINNs with a Bayesian PINN approach, using posterior variance for evaluation, showing improved performance on benchmark problems.


<details>
  <summary>Details</summary>
Motivation: Addressing convergence issues in PINNs for forward problems by ensuring effective information propagation from initial conditions.

Method: Replace ensemble PINNs with Bayesian PINNs, evaluating posterior variance instead of ensemble consensus.

Result: Outperforms ensemble methods on benchmarks and competes with combined Adam and LBFGS-trained ensembles.

Conclusion: Bayesian PINNs offer a principled and effective alternative to ensemble methods for improving PINN convergence.

Abstract: Training physics-informed neural networks (PINNs) for forward problems often
suffers from severe convergence issues, hindering the propagation of
information from regions where the desired solution is well-defined.
Haitsiukevich and Ilin (2023) proposed an ensemble approach that extends the
active training domain of each PINN based on i) ensemble consensus and ii)
vicinity to (pseudo-)labeled points, thus ensuring that the information from
the initial condition successfully propagates to the interior of the
computational domain.
  In this work, we suggest replacing the ensemble by a Bayesian PINN, and
consensus by an evaluation of the PINN's posterior variance. Our experiments
show that this mathematically principled approach outperforms the ensemble on a
set of benchmark problems and is competitive with PINN ensembles trained with
combinations of Adam and LBFGS.

</details>


### [216] [Revisiting Learning Rate Control](https://arxiv.org/abs/2507.01724)
*Micha Henheik,Theresa Eimer,Marius Lindauer*

Main category: cs.LG

TL;DR: The paper evaluates learning rate control methods in deep learning, finding current approaches inconsistent across tasks and advocating for algorithm selection and new directions like meta-learning.


<details>
  <summary>Details</summary>
Motivation: To assess and compare paradigms for learning rate control in deep learning, identifying gaps and opportunities for improvement.

Method: Comparison of paradigms including multi-fidelity hyperparameter optimization, fixed schedules, and hyperparameter-free learning across tasks.

Result: Current methods perform well in specific tasks but lack reliability across settings; hyperparameter optimization becomes less effective with complex models.

Conclusion: Algorithm selection and new approaches like meta-learning are needed to improve learning rate control, especially for complex tasks.

Abstract: The learning rate is one of the most important hyperparameters in deep
learning, and how to control it is an active area within both AutoML and deep
learning research. Approaches for learning rate control span from classic
optimization to online scheduling based on gradient statistics. This paper
compares paradigms to assess the current state of learning rate control. We
find that methods from multi-fidelity hyperparameter optimization,
fixed-hyperparameter schedules, and hyperparameter-free learning often perform
very well on selected deep learning tasks but are not reliable across settings.
This highlights the need for algorithm selection methods in learning rate
control, which have been neglected so far by both the AutoML and deep learning
communities. We also observe a trend of hyperparameter optimization approaches
becoming less effective as models and tasks grow in complexity, even when
combined with multi-fidelity approaches for more expensive model trainings. A
focus on more relevant test tasks and new promising directions like finetunable
methods and meta-learning will enable the AutoML community to significantly
strengthen its impact on this crucial factor in deep learning.

</details>


### [217] [A Real-Time Digital Twin for Type 1 Diabetes using Simulation-Based Inference](https://arxiv.org/abs/2507.01740)
*Trung-Dung Hoang,Alceu Bissoto,Vihangkumar V. Naik,Tim Flühmann,Artemii Shlychkov,José Garcia-Tirado,Lisa M. Koch*

Main category: cs.LG

TL;DR: Proposes a Simulation-Based Inference (SBI) method using Neural Posterior Estimation for efficient parameter estimation in Type 1 Diabetes models, outperforming traditional MCMC methods in speed and generalization.


<details>
  <summary>Details</summary>
Motivation: Accurate parameter estimation for physiological models, especially in Type 1 Diabetes, is challenging due to complex glucose-insulin interactions and the limitations of traditional methods like MCMC.

Method: Uses Simulation-Based Inference with Neural Posterior Estimation to model relationships between meal intake, insulin, and glucose levels, enabling faster and amortized inference.

Result: SBI outperforms traditional methods in parameter estimation, generalizes better to unseen conditions, and provides real-time posterior inference with reliable uncertainty quantification.

Conclusion: The proposed SBI approach offers a computationally efficient and accurate alternative to traditional methods for parameter estimation in Type 1 Diabetes models.

Abstract: Accurately estimating parameters of physiological models is essential to
achieving reliable digital twins. For Type 1 Diabetes, this is particularly
challenging due to the complexity of glucose-insulin interactions. Traditional
methods based on Markov Chain Monte Carlo struggle with high-dimensional
parameter spaces and fit parameters from scratch at inference time, making them
slow and computationally expensive. In this study, we propose a
Simulation-Based Inference approach based on Neural Posterior Estimation to
efficiently capture the complex relationships between meal intake, insulin, and
glucose level, providing faster, amortized inference. Our experiments
demonstrate that SBI not only outperforms traditional methods in parameter
estimation but also generalizes better to unseen conditions, offering real-time
posterior inference with reliable uncertainty quantification.

</details>


### [218] [Enhanced Generative Model Evaluation with Clipped Density and Coverage](https://arxiv.org/abs/2507.01761)
*Nicolas Salvy,Hugues Talbot,Bertrand Thirion*

Main category: cs.LG

TL;DR: The paper introduces Clipped Density and Clipped Coverage metrics to reliably evaluate generative model sample quality, addressing fidelity and coverage issues.


<details>
  <summary>Details</summary>
Motivation: Current quality metrics for generative models lack reliability and interpretability due to calibration and robustness issues.

Method: Proposes Clipped Density and Clipped Coverage by clipping sample contributions and nearest neighbor radii to avoid bias from outliers.

Result: The new metrics show linear degradation with poor samples, outperforming existing methods in robustness and interpretability.

Conclusion: Clipped Density and Clipped Coverage offer improved, interpretable evaluation for generative models.

Abstract: Although generative models have made remarkable progress in recent years,
their use in critical applications has been hindered by their incapacity to
reliably evaluate sample quality. Quality refers to at least two complementary
concepts: fidelity and coverage. Current quality metrics often lack reliable,
interpretable values due to an absence of calibration or insufficient
robustness to outliers. To address these shortcomings, we introduce two novel
metrics, Clipped Density and Clipped Coverage. By clipping individual sample
contributions and, for fidelity, the radii of nearest neighbor balls, our
metrics prevent out-of-distribution samples from biasing the aggregated values.
Through analytical and empirical calibration, these metrics exhibit linear
score degradation as the proportion of poor samples increases. Thus, they can
be straightforwardly interpreted as equivalent proportions of good samples.
Extensive experiments on synthetic and real-world datasets demonstrate that
Clipped Density and Clipped Coverage outperform existing methods in terms of
robustness, sensitivity, and interpretability for evaluating generative models.

</details>


### [219] [BranchNet: A Neuro-Symbolic Learning Framework for Structured Multi-Class Classification](https://arxiv.org/abs/2507.01781)
*Dalia Rodríguez-Salas,Christian Riess*

Main category: cs.LG

TL;DR: BranchNet transforms decision tree ensembles into sparse neural networks, outperforming XGBoost in accuracy while maintaining interpretability.


<details>
  <summary>Details</summary>
Motivation: To bridge symbolic and neural learning by preserving decision tree structure in a gradient-optimizable neural network.

Method: Map decision tree branches to hidden neurons, enabling gradient-based optimization without manual tuning.

Result: Outperforms XGBoost in multi-class classification with significant accuracy gains.

Conclusion: BranchNet is compact, interpretable, and effective, though adaptive calibration may improve binary task performance.

Abstract: We introduce BranchNet, a neuro-symbolic learning framework that transforms
decision tree ensembles into sparse, partially connected neural networks. Each
branch, defined as a decision path from root to a parent of leaves, is mapped
to a hidden neuron, preserving symbolic structure while enabling gradient-based
optimization. The resulting models are compact, interpretable, and require no
manual architecture tuning. Evaluated on a suite of structured multi-class
classification benchmarks, BranchNet consistently outperforms XGBoost in
accuracy, with statistically significant gains. We detail the architecture,
training procedure, and sparsity dynamics, and discuss the model's strengths in
symbolic interpretability as well as its current limitations, particularly on
binary tasks where further adaptive calibration may be beneficial.

</details>


### [220] [Towards Decentralized and Sustainable Foundation Model Training with the Edge](https://arxiv.org/abs/2507.01803)
*Leyang Xue,Meghana Madhyastha,Randal Burns,Myungjin Lee,Mahesh K. Marina*

Main category: cs.LG

TL;DR: The paper proposes decentralized and sustainable training for foundation models using edge AI devices to address environmental and centralization concerns.


<details>
  <summary>Details</summary>
Motivation: Address the environmental impact and centralized control issues of current foundation models by leveraging underutilized edge AI devices.

Method: Proposes decentralized training using collective compute from edge AI devices, emphasizing sustainability.

Result: Identifies challenges to realize decentralized and sustainable foundation model training.

Conclusion: A vision for sustainable and decentralized foundation model training is presented, with challenges outlined for future work.

Abstract: Foundation models are at the forefront of AI research, appealing for their
ability to learn from vast datasets and cater to diverse tasks. Yet, their
significant computational demands raise issues of environmental impact and the
risk of centralized control in their development. We put forward a vision
towards decentralized and sustainable foundation model training that leverages
the collective compute of sparingly used connected edge AI devices. We present
the rationale behind our vision, particularly in support of its sustainability
benefit. We further outline a set of challenges that need to be addressed to
turn this vision into reality.

</details>


### [221] [TD-MPC-Opt: Distilling Model-Based Multi-Task Reinforcement Learning Agents](https://arxiv.org/abs/2507.01823)
*Dmytro Kuzmenko,Nadiya Shvai*

Main category: cs.LG

TL;DR: A novel knowledge transfer method in model-based reinforcement learning efficiently distills a large multi-task agent into a compact model, achieving state-of-the-art performance and practical deployment benefits.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of deploying large world models in resource-constrained environments by improving efficiency and accessibility.

Method: Distillation of a high-capacity multi-task agent (317M parameters) into a compact model (1M parameters) on the MT30 benchmark, followed by FP16 post-training quantization.

Result: The distilled model achieves a normalized score of 28.45, surpassing the original 1M parameter model's score of 18.93, with a 50% size reduction.

Conclusion: The approach effectively consolidates multi-task knowledge and optimizes deployment, advancing efficient reinforcement learning for robotics and constrained applications.

Abstract: We present a novel approach to knowledge transfer in model-based
reinforcement learning, addressing the critical challenge of deploying large
world models in resource-constrained environments. Our method efficiently
distills a high-capacity multi-task agent (317M parameters) into a compact
model (1M parameters) on the MT30 benchmark, significantly improving
performance across diverse tasks. Our distilled model achieves a
state-of-the-art normalized score of 28.45, surpassing the original 1M
parameter model score of 18.93. This improvement demonstrates the ability of
our distillation technique to capture and consolidate complex multi-task
knowledge. We further optimize the distilled model through FP16 post-training
quantization, reducing its size by $\sim$50\%. Our approach addresses practical
deployment limitations and offers insights into knowledge representation in
large world models, paving the way for more efficient and accessible multi-task
reinforcement learning systems in robotics and other resource-constrained
applications. Code available at https://github.com/dmytro-kuzmenko/td-mpc-opt.

</details>


### [222] [MILP-SAT-GNN: Yet Another Neural SAT Solver](https://arxiv.org/abs/2507.01825)
*Franco Alberto Cardillo,Hamza Khyari,Umberto Straccia*

Main category: cs.LG

TL;DR: A novel method uses GNNs to solve SAT problems by mapping k-CNF formulae to MILP problems, encoding them as graphs, and training GNNs. Theoretical results include invariance properties, limitations for foldable formulae, and universal approximation guarantees. Experiments show promising results.


<details>
  <summary>Details</summary>
Motivation: To leverage GNNs for solving SAT problems by bridging the gap between k-CNF formulae and MILP via graph representations.

Method: Map k-CNF formulae to MILP problems, encode as weighted bipartite graphs, and train GNNs. Theoretical analysis includes invariance, limitations, and approximation guarantees.

Result: The method achieves promising experimental results and theoretical guarantees, including universal approximation for finite datasets.

Conclusion: The proposed GNN-based method effectively solves SAT problems with theoretical soundness and experimental validation, though limitations exist for foldable formulae.

Abstract: We proposes a novel method that enables Graph Neural Networks (GNNs) to solve
SAT problems by leveraging a technique developed for applying GNNs to Mixed
Integer Linear Programming (MILP). Specifically, k-CNF formulae are mapped into
MILP problems, which are then encoded as weighted bipartite graphs and
subsequently fed into a GNN for training and testing. From a theoretical
perspective: (i) we establish permutation and equivalence invariance results,
demonstrating that the method produces outputs that are stable under reordering
of clauses and variables; (ii) we identify a theoretical limitation, showing
that for a class of formulae called foldable formulae, standard GNNs cannot
always distinguish satisfiable from unsatisfiable instances; (iii) we prove a
universal approximation theorem, establishing that with Random Node
Initialization (RNI), the method can approximate SAT solving to arbitrary
precision on finite datasets, that is, the GNN becomes approximately sound and
complete on such datasets. Furthermore, we show that for unfoldable formulae,
the same approximation guarantee can be achieved without the need for RNI.
Finally, we conduct an experimental evaluation of our approach, which show
that, despite the simplicity of the neural architecture, the method achieves
promising results.

</details>


### [223] [mGRADE: Minimal Recurrent Gating Meets Delay Convolutions for Lightweight Sequence Modeling](https://arxiv.org/abs/2507.01829)
*Tristan Torchet,Christian Metzner,Laura Kriener,Melika Payvand*

Main category: cs.LG

TL;DR: mGRADE is a hybrid-memory system combining temporal 1D-convolution and minimal gated recurrent unit (minGRU) for efficient multi-scale temporal processing on edge devices.


<details>
  <summary>Details</summary>
Motivation: Address the impracticality of Transformers (quadratic memory scaling) and limitations of RNNs (sequential training) and TCNs (memory scaling with kernel size) for memory-constrained edge devices.

Method: Integrates a temporal 1D-convolution with learnable spacings and a minGRU to capture rapid temporal variations and maintain global context with minimal memory.

Result: Outperforms pure convolutional and recurrent models on synthetic tasks and pixel-by-pixel image classification, using 20% less memory.

Conclusion: mGRADE is an efficient solution for memory-constrained multi-scale temporal processing on edge devices.

Abstract: Edge devices for temporal processing demand models that capture both short-
and long- range dynamics under tight memory constraints. While Transformers
excel at sequence modeling, their quadratic memory scaling with sequence length
makes them impractical for such settings. Recurrent Neural Networks (RNNs)
offer constant memory but train sequentially, and Temporal Convolutional
Networks (TCNs), though efficient, scale memory with kernel size. To address
this, we propose mGRADE (mininally Gated Recurrent Architecture with Delay
Embedding), a hybrid-memory system that integrates a temporal 1D-convolution
with learnable spacings followed by a minimal gated recurrent unit (minGRU).
This design allows the convolutional layer to realize a flexible delay
embedding that captures rapid temporal variations, while the recurrent module
efficiently maintains global context with minimal memory overhead. We validate
our approach on two synthetic tasks, demonstrating that mGRADE effectively
separates and preserves multi-scale temporal features. Furthermore, on
challenging pixel-by-pixel image classification benchmarks, mGRADE consistently
outperforms both pure convolutional and pure recurrent counterparts using
approximately 20% less memory footprint, highlighting its suitability for
memory-constrained temporal processing at the edge. This highlights mGRADE's
promise as an efficient solution for memory-constrained multi-scale temporal
processing at the edge.

</details>


### [224] [Out-of-Distribution Detection Methods Answer the Wrong Questions](https://arxiv.org/abs/2507.01831)
*Yucen Lily Li,Daohan Lu,Polina Kirichenko,Shikai Qiu,Tim G. J. Rudner,C. Bayan Bruss,Andrew Gordon Wilson*

Main category: cs.LG

TL;DR: The paper critiques current OOD detection methods, highlighting their fundamental misalignment with the task and irreducible errors, while also examining limitations of alternative approaches like unsupervised models.


<details>
  <summary>Details</summary>
Motivation: To address the flaws in existing OOD detection methods that rely on predictive uncertainty or features, which often misclassify OOD points due to their design for in-distribution tasks.

Method: The study critically analyzes uncertainty-based and feature-based OOD detection methods, identifies their limitations, and evaluates interventions like hybrid methods and unsupervised models.

Result: Current methods conflate uncertainty or feature distance with OOD status, leading to irreducible errors. Alternative approaches, including unsupervised models, also have fundamental limitations.

Conclusion: The paper underscores the need for fundamentally new approaches to OOD detection, as existing methods and interventions fail to address the core misalignment in objectives.

Abstract: To detect distribution shifts and improve model safety, many
out-of-distribution (OOD) detection methods rely on the predictive uncertainty
or features of supervised models trained on in-distribution data. In this
paper, we critically re-examine this popular family of OOD detection
procedures, and we argue that these methods are fundamentally answering the
wrong questions for OOD detection. There is no simple fix to this misalignment,
since a classifier trained only on in-distribution classes cannot be expected
to identify OOD points; for instance, a cat-dog classifier may confidently
misclassify an airplane if it contains features that distinguish cats from
dogs, despite generally appearing nothing alike. We find that uncertainty-based
methods incorrectly conflate high uncertainty with being OOD, while
feature-based methods incorrectly conflate far feature-space distance with
being OOD. We show how these pathologies manifest as irreducible errors in OOD
detection and identify common settings where these methods are ineffective.
Additionally, interventions to improve OOD detection such as feature-logit
hybrid methods, scaling of model and data size, epistemic uncertainty
representation, and outlier exposure also fail to address this fundamental
misalignment in objectives. We additionally consider unsupervised density
estimation and generative models for OOD detection, which we show have their
own fundamental limitations.

</details>


### [225] [Automatic Rank Determination for Low-Rank Adaptation via Submodular Function Maximization](https://arxiv.org/abs/2507.01841)
*Yihang Gao,Vincent Y. F. Tan*

Main category: cs.LG

TL;DR: SubLoRA introduces a second-order rank determination method for LoRA using submodular optimization, outperforming prior methods like AdaLoRA in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Prior methods rely on linearized approximations of the loss function, which become inaccurate when LoRA parameters are well-optimized, necessitating a more nuanced second-order approach.

Method: SubLoRA reformulates rank determination as a combinatorial optimization problem with a quadratic objective, solved via submodular function maximization and a greedy algorithm. It incorporates the Hessian matrix for accuracy.

Result: SubLoRA outperforms existing methods in rank determination and joint training, validated through experiments on physics-informed neural networks for PDEs.

Conclusion: SubLoRA combines theoretical rigor, second-order accuracy, and computational efficiency, proving superior to linearized approaches in practical applications.

Abstract: In this paper, we propose SubLoRA, a rank determination method for Low-Rank
Adaptation (LoRA) based on submodular function maximization. In contrast to
prior approaches, such as AdaLoRA, that rely on first-order (linearized)
approximations of the loss function, SubLoRA utilizes second-order information
to capture the potentially complex loss landscape by incorporating the Hessian
matrix. We show that the linearization becomes inaccurate and ill-conditioned
when the LoRA parameters have been well optimized, motivating the need for a
more reliable and nuanced second-order formulation. To this end, we reformulate
the rank determination problem as a combinatorial optimization problem with a
quadratic objective. However, solving this problem exactly is NP-hard in
general. To overcome the computational challenge, we introduce a submodular
function maximization framework and devise a greedy algorithm with
approximation guarantees. We derive a sufficient and necessary condition under
which the rank-determination objective becomes submodular, and construct a
closed-form projection of the Hessian matrix that satisfies this condition
while maintaining computational efficiency. Our method combines solid
theoretical foundations, second-order accuracy, and practical computational
efficiency. We further extend SubLoRA to a joint optimization setting,
alternating between LoRA parameter updates and rank determination under a rank
budget constraint. Extensive experiments on fine-tuning physics-informed neural
networks (PINNs) for solving partial differential equations (PDEs) demonstrate
the effectiveness of our approach. Results show that SubLoRA outperforms
existing methods in both rank determination and joint training performance.

</details>


### [226] [Towards Foundation Auto-Encoders for Time-Series Anomaly Detection](https://arxiv.org/abs/2507.01875)
*Gastón García González,Pedro Casas,Emilio Martínez,Alicia Fernández*

Main category: cs.LG

TL;DR: FAE is a foundation generative-AI model for anomaly detection in time-series data, combining VAEs and DCNNs for zero-shot applications.


<details>
  <summary>Details</summary>
Motivation: To leverage large pretrained models for accurate time-series modeling and anomaly detection across diverse datasets.

Method: Uses Variational Auto-Encoders (VAEs) and Dilated Convolutional Neural Networks (DCNNs) for generic time-series modeling.

Result: Preliminary results show effectiveness on multi-dimensional datasets, including real-world mobile ISP data and KDD 2021.

Conclusion: FAE demonstrates potential for zero-shot anomaly detection in unseen time-series datasets.

Abstract: We investigate a novel approach to time-series modeling, inspired by the
successes of large pretrained foundation models. We introduce FAE (Foundation
Auto-Encoders), a foundation generative-AI model for anomaly detection in
time-series data, based on Variational Auto-Encoders (VAEs). By foundation, we
mean a model pretrained on massive amounts of time-series data which can learn
complex temporal patterns useful for accurate modeling, forecasting, and
detection of anomalies on previously unseen datasets. FAE leverages VAEs and
Dilated Convolutional Neural Networks (DCNNs) to build a generic model for
univariate time-series modeling, which could eventually perform properly in
out-of-the-box, zero-shot anomaly detection applications. We introduce the main
concepts of FAE, and present preliminary results in different multi-dimensional
time-series datasets from various domains, including a real dataset from an
operational mobile ISP, and the well known KDD 2021 Anomaly Detection dataset.

</details>


### [227] [Exploring a Hybrid Deep Learning Approach for Anomaly Detection in Mental Healthcare Provider Billing: Addressing Label Scarcity through Semi-Supervised Anomaly Detection](https://arxiv.org/abs/2507.01924)
*Samirah Bakker,Yao Ma,Seyed Sahand Mohammadi Ziabari*

Main category: cs.LG

TL;DR: A hybrid deep learning approach combining LSTM and Transformers with pseudo-labeling (iForest and AE) is proposed for detecting anomalies in mental healthcare billing, addressing class imbalance and label scarcity.


<details>
  <summary>Details</summary>
Motivation: The complexity of mental healthcare billing leads to anomalies like fraud, and existing machine learning methods struggle with class imbalance, label scarcity, and sequential patterns.

Method: A hybrid model using LSTM and Transformers, trained with pseudo-labeling via iForest and AE, is evaluated on real-world billing datasets.

Result: The iForest LSTM baseline achieves high recall (0.963) on declaration-level data, while the hybrid iForest-based model achieves the highest recall (0.744) on operation-level data, albeit with lower precision.

Conclusion: The study demonstrates the potential of combining pseudo-labeling with hybrid deep learning for complex, imbalanced anomaly detection in healthcare billing.

Abstract: The complexity of mental healthcare billing enables anomalies, including
fraud. While machine learning methods have been applied to anomaly detection,
they often struggle with class imbalance, label scarcity, and complex
sequential patterns. This study explores a hybrid deep learning approach
combining Long Short-Term Memory (LSTM) networks and Transformers, with
pseudo-labeling via Isolation Forests (iForest) and Autoencoders (AE). Prior
work has not evaluated such hybrid models trained on pseudo-labeled data in the
context of healthcare billing. The approach is evaluated on two real-world
billing datasets related to mental healthcare. The iForest LSTM baseline
achieves the highest recall (0.963) on declaration-level data. On the
operation-level data, the hybrid iForest-based model achieves the highest
recall (0.744), though at the cost of lower precision. These findings highlight
the potential of combining pseudo-labeling with hybrid deep learning in
complex, imbalanced anomaly detection settings.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [228] [Capacity Planning and Scheduling for Jobs with Uncertainty in Resource Usage and Duration](https://arxiv.org/abs/2507.01225)
*Sunandita Patra,Mehtab Pathan,Mahmoud Mahfouz,Parisa Zehtabi,Wided Ouaja,Daniele Magazzeni,Manuela Veloso*

Main category: cs.DC

TL;DR: The paper proposes methods for capacity planning and job scheduling in hybrid cloud/on-prem grid computing, addressing uncertainty in job characteristics and balancing resource minimization with quality-of-service.


<details>
  <summary>Details</summary>
Motivation: The shift to cloud computing and the need for efficient resource management in hybrid environments, especially in finance where job characteristics are unpredictable.

Method: Deterministic estimators and pair sampling-based constraint programming for handling uncertainty and optimizing resource usage and deadlines.

Result: The pair sampling-based approach significantly reduces peak resource usage while maintaining quality-of-service.

Conclusion: The proposed methods effectively balance resource efficiency and service quality in uncertain job environments.

Abstract: Organizations around the world schedule jobs (programs) regularly to perform
various tasks dictated by their end users. With the major movement towards
using a cloud computing infrastructure, our organization follows a hybrid
approach with both cloud and on-prem servers. The objective of this work is to
perform capacity planning, i.e., estimate resource requirements, and job
scheduling for on-prem grid computing environments. A key contribution of our
approach is handling uncertainty in both resource usage and duration of the
jobs, a critical aspect in the finance industry where stochastic market
conditions significantly influence job characteristics. For capacity planning
and scheduling, we simultaneously balance two conflicting objectives: (a)
minimize resource usage, and (b) provide high quality-of-service to the end
users by completing jobs by their requested deadlines. We propose approximate
approaches using deterministic estimators and pair sampling-based constraint
programming. Our best approach (pair sampling-based) achieves much lower peak
resource usage compared to manual scheduling without compromising on the
quality-of-service.

</details>


### [229] [EdgeLoRA: An Efficient Multi-Tenant LLM Serving System on Edge Devices](https://arxiv.org/abs/2507.01438)
*Zheyu Shen,Yexiao He,Ziyao Wang,Yuning Zhang,Guoheng Sun,Wanghao Ye,Ang Li*

Main category: cs.DC

TL;DR: EdgeLoRA is a system for efficiently serving LLMs on edge devices in multi-tenant settings, addressing challenges like adapter selection, memory overhead, and latency through adaptive mechanisms, heterogeneous memory management, and batch processing.


<details>
  <summary>Details</summary>
Motivation: To enable efficient deployment of fine-tuned LLMs on resource-constrained edge devices, overcoming challenges like adapter selection complexity, memory overhead, and latency in multi-tenant environments.

Method: EdgeLoRA introduces adaptive adapter selection, heterogeneous memory management (caching/pooling), and batch LoRA inference to optimize performance.

Result: EdgeLoRA achieves up to 4x higher throughput and serves significantly more adapters simultaneously compared to llama.cpp.

Conclusion: EdgeLoRA offers a scalable and efficient solution for deploying LLMs on edge devices in multi-tenant scenarios, transforming edge-based LLM applications.

Abstract: Large Language Models (LLMs) have gained significant attention due to their
versatility across a wide array of applications. Fine-tuning LLMs with
parameter-efficient adapters, such as Low-Rank Adaptation (LoRA), enables these
models to efficiently adapt to downstream tasks without extensive retraining.
Deploying fine-tuned LLMs on multi-tenant edge devices offers substantial
benefits, such as reduced latency, enhanced privacy, and personalized
responses. However, serving LLMs efficiently on resource-constrained edge
devices presents critical challenges, including the complexity of adapter
selection for different tasks and memory overhead from frequent adapter
swapping. Moreover, given the multiple requests in multi-tenant settings,
processing requests sequentially results in underutilization of computational
resources and increased latency. This paper introduces EdgeLoRA, an efficient
system for serving LLMs on edge devices in multi-tenant environments. EdgeLoRA
incorporates three key innovations: (1) an adaptive adapter selection mechanism
to streamline the adapter configuration process; (2) heterogeneous memory
management, leveraging intelligent adapter caching and pooling to mitigate
memory operation overhead; and (3) batch LoRA inference, enabling efficient
batch processing to significantly reduce computational latency. Comprehensive
evaluations using the Llama3.1-8B model demonstrate that EdgeLoRA significantly
outperforms the status quo (i.e., llama.cpp) in terms of both latency and
throughput. The results demonstrate that EdgeLoRA can achieve up to a 4 times
boost in throughput. Even more impressively, it can serve several orders of
magnitude more adapters simultaneously. These results highlight EdgeLoRA's
potential to transform edge deployment of LLMs in multi-tenant scenarios,
offering a scalable and efficient solution for resource-constrained
environments.

</details>


### [230] [Deep Recommender Models Inference: Automatic Asymmetric Data Flow Optimization](https://arxiv.org/abs/2507.01676)
*Giuseppe Ruggeri,Renzo Andri,Daniele Jahier Pagliari,Lukas Cavigelli*

Main category: cs.DC

TL;DR: The paper proposes optimized data flows and strategies for embedding look-ups in Deep Recommender Models (DLRMs), achieving significant speed-ups on AI accelerators.


<details>
  <summary>Details</summary>
Motivation: DLRMs are a major AI workload in Meta's data centers, with embedding layers being a performance bottleneck due to random memory accesses.

Method: Four strategies for efficient embedding look-ups on a single core and a framework for asymmetric table mapping to multiple cores on a SoC.

Result: Speed-ups of 1.5x to 6.5x for real workloads and over 20x for unbalanced distributions, with improved query distribution independence.

Conclusion: The proposed method effectively addresses the bottleneck in DLRMs, offering scalable performance improvements.

Abstract: Deep Recommender Models (DLRMs) inference is a fundamental AI workload
accounting for more than 79% of the total AI workload in Meta's data centers.
DLRMs' performance bottleneck is found in the embedding layers, which perform
many random memory accesses to retrieve small embedding vectors from tables of
various sizes. We propose the design of tailored data flows to speedup
embedding look-ups. Namely, we propose four strategies to look up an embedding
table effectively on one core, and a framework to automatically map the tables
asymmetrically to the multiple cores of a SoC. We assess the effectiveness of
our method using the Huawei Ascend AI accelerators, comparing it with the
default Ascend compiler, and we perform high-level comparisons with Nvidia
A100. Results show a speed-up varying from 1.5x up to 6.5x for real workload
distributions, and more than 20x for extremely unbalanced distributions.
Furthermore, the method proves to be much more independent of the query
distribution than the baseline.

</details>


### [231] [Evolving HPC services to enable ML workloads on HPE Cray EX](https://arxiv.org/abs/2507.01880)
*Stefano Schuppli,Fawzi Mohamed,Henrique Mendonça,Nina Mujkanovic,Elia Palme,Dino Conciatore,Lukas Drescher,Miguel Gila,Pim Witlox,Joost VandeVondele,Maxime Martinasso,Thomas C. Schulthess,Torsten Hoefler*

Main category: cs.DC

TL;DR: The paper discusses enhancing HPC services to better support ML workloads on the Alps Research Infrastructure, addressing challenges and proposing technological improvements.


<details>
  <summary>Details</summary>
Motivation: Traditional HPC services are insufficient for the dynamic needs of the ML community, prompting the need for tailored enhancements.

Method: Proposed enhancements include a user-friendly environment, performance screening tools, observability features, node vetting utilities, service plane infrastructure, and ML-specific storage.

Result: The enhancements aim to improve ML workload execution, system usability, and resilience, aligning better with ML community needs.

Conclusion: The proposals are contextualized within broader changes in HPC infrastructure to better serve evolving scientific communities.

Abstract: The Alps Research Infrastructure leverages GH200 technology at scale,
featuring 10,752 GPUs. Accessing Alps provides a significant computational
advantage for researchers in Artificial Intelligence (AI) and Machine Learning
(ML). While Alps serves a broad range of scientific communities, traditional
HPC services alone are not sufficient to meet the dynamic needs of the ML
community. This paper presents an initial investigation into extending HPC
service capabilities to better support ML workloads. We identify key challenges
and gaps we have observed since the early-access phase (2023) of Alps by the
Swiss AI community and propose several technological enhancements. These
include a user environment designed to facilitate the adoption of HPC for ML
workloads, balancing performance with flexibility; a utility for rapid
performance screening of ML applications during development; observability
capabilities and data products for inspecting ongoing large-scale ML workloads;
a utility to simplify the vetting of allocated nodes for compute readiness; a
service plane infrastructure to deploy various types of workloads, including
support and inference services; and a storage infrastructure tailored to the
specific needs of ML workloads. These enhancements aim to facilitate the
execution of ML workloads on HPC systems, increase system usability and
resilience, and better align with the needs of the ML community. We also
discuss our current approach to security aspects. This paper concludes by
placing these proposals in the broader context of changes in the communities
served by HPC infrastructure like ours.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [232] [Hello Afrika: Speech Commands in Kinyarwanda](https://arxiv.org/abs/2507.01024)
*George Igwegbe,Martins Awojide,Mboh Bless,Nirel Kadzo*

Main category: eess.AS

TL;DR: The paper discusses the Hello Afrika project, which addresses the lack of speech command models for African languages, starting with Kinyarwanda.


<details>
  <summary>Details</summary>
Motivation: There is a shortage of speech command models for African languages, limiting accessibility for persons with disabilities.

Method: A custom speech command corpus was created for Kinyarwanda, and the model was deployed on various devices (PC, mobile, edge).

Result: The model's performance was evaluated using suitable metrics, though specific results are not detailed.

Conclusion: The Hello Afrika project successfully addresses the gap in speech command models for African languages, starting with Kinyarwanda.

Abstract: Voice or Speech Commands are a subset of the broader Spoken Word Corpus of a
language which are essential for non-contact control of and activation of
larger AI systems in devices used in everyday life especially for persons with
disabilities. Currently, there is a dearth of speech command models for African
languages. The Hello Afrika project aims to address this issue and its first
iteration is focused on the Kinyarwanda language since the country has shown
interest in developing speech recognition technologies culminating in one of
the largest datasets on Mozilla Common Voice. The model was built off a custom
speech command corpus made up of general directives, numbers, and a wake word.
The final model was deployed on multiple devices (PC, Mobile Phone and Edge
Devices) and the performance was assessed using suitable metrics.

</details>


### [233] [Scalable Offline ASR for Command-Style Dictation in Courtrooms](https://arxiv.org/abs/2507.01021)
*Kumarmanas Nethil,Vaibhav Mishra,Kriti Anandan,Kavya Manohar*

Main category: eess.AS

TL;DR: An open-source framework for efficient Command-style dictation using VAD and parallel Whisper model transcription, outperforming batch processing in latency and compute utilization.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between resource-intensive Online systems and high-latency Batch processing for dictation.

Method: Uses Voice Activity Detection (VAD) for audio segmentation and parallel transcription with Whisper models, supporting various ASR architectures.

Result: Demonstrated latency reduction with increased user concurrency and deployed in 15% of India's courtrooms.

Conclusion: The framework offers a scalable, efficient solution for dictation, with open-source availability for broader adoption.

Abstract: We propose an open-source framework for Command-style dictation that
addresses the gap between resource-intensive Online systems and high-latency
Batch processing. Our approach uses Voice Activity Detection (VAD) to segment
audio and transcribes these segments in parallel using Whisper models, enabling
efficient multiplexing across audios. Unlike proprietary systems like
SuperWhisper, this framework is also compatible with most ASR architectures,
including widely used CTC-based models. Our multiplexing technique maximizes
compute utilization in real-world settings, as demonstrated by its deployment
in around 15% of India's courtrooms. Evaluations on live data show consistent
latency reduction as user concurrency increases, compared to sequential batch
processing. The live demonstration will showcase our open-sourced
implementation and allow attendees to interact with it in real-time.

</details>


### [234] [Workflow-Based Evaluation of Music Generation Systems](https://arxiv.org/abs/2507.01022)
*Shayan Dadman,Bernt Arild Bremdal,Andreas Bergsland*

Main category: eess.AS

TL;DR: An evaluation of eight open-source Music Generation Systems (MGS) in music production workflows, combining technical and practical criteria. Findings highlight their complementary role to human creativity and identify areas for AI integration.


<details>
  <summary>Details</summary>
Motivation: To assess the practical and creative affordances of MGS in music production, addressing their limitations and potential as collaborative tools.

Method: Mixed qualitative and quantitative approach with a single-evaluator methodology, focusing on architectural diversity in symbolic and audio-based systems.

Result: MGS enhance but don't replace human expertise, struggling with thematic coherence and emotional depth. A structured evaluation framework is proposed.

Conclusion: The study provides insights for future MGS development, emphasizing human-AI collaboration and methodological refinements for broader evaluations.

Abstract: This study presents an exploratory evaluation of Music Generation Systems
(MGS) within contemporary music production workflows by examining eight
open-source systems. The evaluation framework combines technical insights with
practical experimentation through criteria specifically designed to investigate
the practical and creative affordances of the systems within the iterative,
non-linear nature of music production. Employing a single-evaluator methodology
as a preliminary phase, this research adopts a mixed approach utilizing
qualitative methods to form hypotheses subsequently assessed through
quantitative metrics. The selected systems represent architectural diversity
across both symbolic and audio-based music generation approaches, spanning
composition, arrangement, and sound design tasks. The investigation addresses
limitations of current MGS in music production, challenges and opportunities
for workflow integration, and development potential as collaborative tools
while maintaining artistic authenticity. Findings reveal these systems function
primarily as complementary tools enhancing rather than replacing human
expertise. They exhibit limitations in maintaining thematic and structural
coherence that emphasize the indispensable role of human creativity in tasks
demanding emotional depth and complex decision-making. This study contributes a
structured evaluation framework that considers the iterative nature of music
creation. It identifies methodological refinements necessary for subsequent
comprehensive evaluations and determines viable areas for AI integration as
collaborative tools in creative workflows. The research provides
empirically-grounded insights to guide future development in the field.

</details>


<div id='q-fin.PM'></div>

# q-fin.PM [[Back]](#toc)

### [235] [End-to-End Large Portfolio Optimization for Variance Minimization with Neural Networks through Covariance Cleaning](https://arxiv.org/abs/2507.01918)
*Christian Bongiorno,Efstratios Manolakis,Rosario Nunzio Mantegna*

Main category: q-fin.PM

TL;DR: A rotation-invariant neural network is developed for global minimum-variance portfolios, offering interpretability and robust out-of-sample performance.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of estimating large equity covariance matrices and achieving robust portfolio optimization with clear interpretability.

Method: Joint learning of lag-transformed returns and regularization of eigenvalues and marginal volatilities, optimized end-to-end for minimum portfolio variance.

Result: Outperforms competitors in realized volatility, drawdowns, and Sharpe ratios, even under realistic trading constraints.

Conclusion: The model provides a scalable, interpretable, and high-performing solution for portfolio optimization, adaptable to various constraints.

Abstract: We develop a rotation-invariant neural network that provides the global
minimum-variance portfolio by jointly learning how to lag-transform historical
returns and how to regularise both the eigenvalues and the marginal
volatilities of large equity covariance matrices. This explicit mathematical
mapping offers clear interpretability of each module's role, so the model
cannot be regarded as a pure black-box. The architecture mirrors the analytical
form of the global minimum-variance solution yet remains agnostic to dimension,
so a single model can be calibrated on panels of a few hundred stocks and
applied, without retraining, to one thousand US equities-a cross-sectional jump
that demonstrates robust out-of-sample generalisation. The loss function is the
future realized minimum portfolio variance and is optimized end-to-end on real
daily returns. In out-of-sample tests from January 2000 to December 2024 the
estimator delivers systematically lower realised volatility, smaller maximum
drawdowns, and higher Sharpe ratios than the best analytical competitors,
including state-of-the-art non-linear shrinkage. Furthermore, although the
model is trained end-to-end to produce an unconstrained (long-short)
minimum-variance portfolio, we show that its learned covariance representation
can be used in general optimizers under long-only constraints with virtually no
loss in its performance advantage over competing estimators. These gains
persist when the strategy is executed under a highly realistic implementation
framework that models market orders at the auctions, empirical slippage,
exchange fees, and financing charges for leverage, and they remain stable
during episodes of acute market stress.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [236] [Evaluating LLM Agent Collusion in Double Auctions](https://arxiv.org/abs/2507.01413)
*Kushal Agrawal,Verona Teo,Juan J. Vazquez,Sudarsh Kunnavakkam,Vishak Srikanth,Andy Liu*

Main category: cs.GT

TL;DR: LLM agents in markets can collude; communication, model choice, and environmental pressures affect collusion.


<details>
  <summary>Details</summary>
Motivation: To identify potential undesirable behaviors of LLM agents in socioeconomic interactions, focusing on collusion in markets.

Method: Simulated continuous double auction markets with LLM agents as sellers, varying communication, model choice, and environmental pressures.

Result: Direct communication increases collusion; model choice and environmental pressures (e.g., oversight) influence collusive behavior.

Conclusion: LLM-based market agents pose economic and ethical risks, requiring careful deployment.

Abstract: Large language models (LLMs) have demonstrated impressive capabilities as
autonomous agents with rapidly expanding applications in various domains. As
these agents increasingly engage in socioeconomic interactions, identifying
their potential for undesirable behavior becomes essential. In this work, we
examine scenarios where they can choose to collude, defined as secretive
cooperation that harms another party. To systematically study this, we
investigate the behavior of LLM agents acting as sellers in simulated
continuous double auction markets. Through a series of controlled experiments,
we analyze how parameters such as the ability to communicate, choice of model,
and presence of environmental pressures affect the stability and emergence of
seller collusion. We find that direct seller communication increases collusive
tendencies, the propensity to collude varies across models, and environmental
pressures, such as oversight and urgency from authority figures, influence
collusive behavior. Our findings highlight important economic and ethical
considerations for the deployment of LLM-based market agents.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [237] [Dynamic Similarity Graph Construction with Kernel Density Estimation](https://arxiv.org/abs/2507.01696)
*Steinar Laenen,Peter Macgregor,He Sun*

Main category: cs.DS

TL;DR: The paper introduces a dynamic data structure for kernel density estimation (KDE) and applies it to maintain a sparse similarity graph, enabling fast dynamic spectral clustering.


<details>
  <summary>Details</summary>
Motivation: Efficiently maintaining KDE estimates and similarity graphs in dynamic settings is challenging, and existing methods lack scalability.

Method: A dynamic data structure is proposed to update KDE estimates for query points as data points are added. This is extended to maintain a sparse similarity graph and enable dynamic spectral clustering.

Result: The algorithms are evaluated on synthetic and real-world datasets, demonstrating effectiveness in dynamic settings.

Conclusion: The proposed approach efficiently handles dynamic KDE and spectral clustering, offering practical scalability.

Abstract: In the kernel density estimation (KDE) problem, we are given a set $X$ of
data points in $\mathbb{R}^d$, a kernel function $k: \mathbb{R}^d \times
\mathbb{R}^d \rightarrow \mathbb{R}$, and a query point $\mathbf{q} \in
\mathbb{R}^d$, and the objective is to quickly output an estimate of
$\sum_{\mathbf{x} \in X} k(\mathbf{q}, \mathbf{x})$. In this paper, we consider
$\textsf{KDE}$ in the dynamic setting, and introduce a data structure that
efficiently maintains the estimates for a set of query points as data points
are added to $X$ over time. Based on this, we design a dynamic data structure
that maintains a sparse approximation of the fully connected similarity graph
on $X$, and develop a fast dynamic spectral clustering algorithm. We further
evaluate the effectiveness of our algorithms on both synthetic and real-world
datasets.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [238] [SpecCLIP: Aligning and Translating Spectroscopic Measurements for Stars](https://arxiv.org/abs/2507.01939)
*Xiaosheng Zhao,Yang Huang,Guirong Xue,Xiao Kong,Jifeng Liu,Xiaoyu Tang,Timothy C. Beers,Yuan-Sen Ting,A-Li Luo*

Main category: astro-ph.IM

TL;DR: SpecCLIP extends LLM-inspired methods to stellar spectral analysis, using contrastive alignment and auxiliary decoders to improve adaptability and precision in tasks like stellar-parameter estimation.


<details>
  <summary>Details</summary>
Motivation: To leverage the success of large language models (LLMs) in natural language understanding for stellar spectral analysis, aiming to learn robust embeddings for diverse applications.

Method: Pre-training on LAMOST low-resolution and Gaia XP spectra, followed by contrastive alignment (CLIP framework) and auxiliary decoders for spectrum-specific information and translation.

Result: Improved adaptability and precision in stellar-parameter estimation and chemical-abundance determination, with enhanced accuracy benchmarked against external data.

Conclusion: Contrastively trained foundation models with spectrum-aware decoders can advance precision stellar spectroscopy.

Abstract: In recent years, large language models (LLMs) have transformed natural
language understanding through vast datasets and large-scale parameterization.
Inspired by this success, we present SpecCLIP, a foundation model framework
that extends LLM-inspired methodologies to stellar spectral analysis. Stellar
spectra, akin to structured language, encode rich physical and chemical
information about stars. By training foundation models on large-scale spectral
datasets, our goal is to learn robust and informative embeddings that support
diverse downstream applications. As a proof of concept, SpecCLIP involves
pre-training on two spectral types--LAMOST low-resolution and Gaia XP--followed
by contrastive alignment using the CLIP (Contrastive Language-Image
Pre-training) framework, adapted to associate spectra from different
instruments. This alignment is complemented by auxiliary decoders that preserve
spectrum-specific information and enable translation (prediction) between
spectral types, with the former achieved by maximizing mutual information
between embeddings and input spectra. The result is a cross-spectrum framework
enabling intrinsic calibration and flexible applications across instruments. We
demonstrate that fine-tuning these models on moderate-sized labeled datasets
improves adaptability to tasks such as stellar-parameter estimation and
chemical-abundance determination. SpecCLIP also enhances the accuracy and
precision of parameter estimates benchmarked against external survey data.
Additionally, its similarity search and cross-spectrum prediction capabilities
offer potential for anomaly detection. Our results suggest that contrastively
trained foundation models enriched with spectrum-aware decoders can advance
precision stellar spectroscopy.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [239] [Data Agent: A Holistic Architecture for Orchestrating Data+AI Ecosystems](https://arxiv.org/abs/2507.01599)
*Zhaoyan Sun,Jiayi Wang,Xinyang Zhao,Jiachi Wang,Guoliang Li*

Main category: cs.DB

TL;DR: The paper proposes 'Data Agents' to automate and enhance Data+AI systems by integrating LLMs for better semantic understanding, reasoning, and planning, addressing current limitations in pipeline orchestration.


<details>
  <summary>Details</summary>
Motivation: Existing Data+AI systems rely heavily on human experts due to limited semantic understanding and planning capabilities. LLMs' success in these areas motivates their integration to revolutionize data systems.

Method: Introduces 'Data Agents,' an architecture combining LLMs for knowledge comprehension, reasoning, and planning to tackle data-related tasks, with examples like data science and analytics agents.

Result: Presents a framework for designing Data Agents, addressing challenges like pipeline orchestration and optimization, and showcases practical agent systems.

Conclusion: Data Agents leveraging LLMs can transform Data+AI ecosystems, though open challenges remain in their design and implementation.

Abstract: Traditional Data+AI systems utilize data-driven techniques to optimize
performance, but they rely heavily on human experts to orchestrate system
pipelines, enabling them to adapt to changes in data, queries, tasks, and
environments. For instance, while there are numerous data science tools
available, developing a pipeline planning system to coordinate these tools
remains challenging. This difficulty arises because existing Data+AI systems
have limited capabilities in semantic understanding, reasoning, and planning.
Fortunately, we have witnessed the success of large language models (LLMs) in
enhancing semantic understanding, reasoning, and planning abilities. It is
crucial to incorporate LLM techniques to revolutionize data systems for
orchestrating Data+AI applications effectively.
  To achieve this, we propose the concept of a 'Data Agent' - a comprehensive
architecture designed to orchestrate Data+AI ecosystems, which focuses on
tackling data-related tasks by integrating knowledge comprehension, reasoning,
and planning capabilities. We delve into the challenges involved in designing
data agents, such as understanding data/queries/environments/tools,
orchestrating pipelines/workflows, optimizing and executing pipelines, and
fostering pipeline self-reflection. Furthermore, we present examples of data
agent systems, including a data science agent, data analytics agents (such as
unstructured data analytics agent, semantic structured data analytics agent,
data lake analytics agent, and multi-modal data analytics agent), and a
database administrator (DBA) agent. We also outline several open challenges
associated with designing data agent systems.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [240] [Prompt Mechanisms in Medical Imaging: A Comprehensive Survey](https://arxiv.org/abs/2507.01055)
*Hao Yang,Xinlong Liang,Zhang Li,Yue Sun,Zheyu Hu,Xinghe Xie,Behdad Dashtbozorg,Jincheng Huang,Shiwei Zhu,Luyi Han,Jiong Zhang,Shanshan Wang,Ritse Mann,Qifeng Yu,Tao Tan*

Main category: eess.IV

TL;DR: Prompt-based methodologies enhance deep learning in medical imaging by improving adaptability, accuracy, and data efficiency, though challenges like prompt design and scalability remain.


<details>
  <summary>Details</summary>
Motivation: To address challenges in clinical adoption of deep learning in medical imaging, such as data scarcity and distribution shifts, by leveraging prompt-based strategies.

Method: Systematic review of prompt engineering, analyzing textual, visual, and learnable embeddings for tasks like image generation, segmentation, and classification.

Result: Prompt mechanisms improve task-specific outcomes, enhancing accuracy, robustness, and interpretability while reducing manual feature engineering.

Conclusion: Prompt-driven AI holds promise for revolutionizing diagnostics and personalized treatment, but further work is needed in optimization and clinical scalability.

Abstract: Deep learning offers transformative potential in medical imaging, yet its
clinical adoption is frequently hampered by challenges such as data scarcity,
distribution shifts, and the need for robust task generalization. Prompt-based
methodologies have emerged as a pivotal strategy to guide deep learning models,
providing flexible, domain-specific adaptations that significantly enhance
model performance and adaptability without extensive retraining. This
systematic review critically examines the burgeoning landscape of prompt
engineering in medical imaging. We dissect diverse prompt modalities, including
textual instructions, visual prompts, and learnable embeddings, and analyze
their integration for core tasks such as image generation, segmentation, and
classification. Our synthesis reveals how these mechanisms improve
task-specific outcomes by enhancing accuracy, robustness, and data efficiency
and reducing reliance on manual feature engineering while fostering greater
model interpretability by making the model's guidance explicit. Despite
substantial advancements, we identify persistent challenges, particularly in
prompt design optimization, data heterogeneity, and ensuring scalability for
clinical deployment. Finally, this review outlines promising future
trajectories, including advanced multimodal prompting and robust clinical
integration, underscoring the critical role of prompt-driven AI in accelerating
the revolution of diagnostics and personalized treatment planning in medicine.

</details>


### [241] [CRISP-SAM2: SAM2 with Cross-Modal Interaction and Semantic Prompting for Multi-Organ Segmentation](https://arxiv.org/abs/2506.23121)
*Xinlei Yu,Chanmiao Wang,Hui Jin,Ahmed Elazab,Gangyong Jia,Xiang Wan,Changqing Zou,Ruiquan Ge*

Main category: eess.IV

TL;DR: CRISP-SAM2 is a novel multi-organ medical segmentation model using cross-modal interaction and semantic prompting to address inaccuracies and reliance on geometric prompts.


<details>
  <summary>Details</summary>
Motivation: Current multi-organ segmentation models lack detail accuracy, depend on geometric prompts, and lose spatial information.

Method: Uses cross-modal contextualized semantics, semantic prompting, and memory self-updating with mask-refining.

Result: Outperforms existing models on seven public datasets.

Conclusion: CRISP-SAM2 effectively addresses limitations and enhances segmentation performance.

Abstract: Multi-organ medical segmentation is a crucial component of medical image
processing, essential for doctors to make accurate diagnoses and develop
effective treatment plans. Despite significant progress in this field, current
multi-organ segmentation models often suffer from inaccurate details,
dependence on geometric prompts and loss of spatial information. Addressing
these challenges, we introduce a novel model named CRISP-SAM2 with CRoss-modal
Interaction and Semantic Prompting based on SAM2. This model represents a
promising approach to multi-organ medical segmentation guided by textual
descriptions of organs. Our method begins by converting visual and textual
inputs into cross-modal contextualized semantics using a progressive
cross-attention interaction mechanism. These semantics are then injected into
the image encoder to enhance the detailed understanding of visual information.
To eliminate reliance on geometric prompts, we use a semantic prompting
strategy, replacing the original prompt encoder to sharpen the perception of
challenging targets. In addition, a similarity-sorting self-updating strategy
for memory and a mask-refining process is applied to further adapt to medical
imaging and enhance localized details. Comparative experiments conducted on
seven public datasets indicate that CRISP-SAM2 outperforms existing models.
Extensive analysis also demonstrates the effectiveness of our method, thereby
confirming its superior performance, especially in addressing the limitations
mentioned earlier. Our code is available at:
https://github.com/YU-deep/CRISP\_SAM2.git.

</details>


### [242] [MID-INFRARED (MIR) OCT-based inspection in industry](https://arxiv.org/abs/2507.01074)
*N. P. García-de-la-Puente,Rocío del Amor,Fernando García-Torres,Niels Møller Israelsen,Coraline Lapre,Christian Rosenberg Petersen,Ole Bang,Dominik Brouczek,Martin Schwentenwein,Kevin Neumann,Niels Benson,Valery Naranjo*

Main category: eess.IV

TL;DR: The paper evaluates MIR OCT systems for detecting sub-surface irregularities in materials like composites and ceramics, using preprocessing and AI-enhanced vision algorithms for anomaly detection.


<details>
  <summary>Details</summary>
Motivation: To enhance non-destructive inspection techniques for industrial production monitoring by exploring MIR OCT systems.

Method: Conducted exploratory study with acquisitions on composites and ceramics, assessing preprocessing and AI-enhanced vision algorithms for anomaly detection.

Result: Identified capabilities of MIR OCT systems, discussed optimal parameters, and highlighted strengths/weaknesses.

Conclusion: MIR OCT systems show promise for industrial non-destructive inspection, with further refinement needed for optimal performance.

Abstract: This paper aims to evaluate mid-infrared (MIR) Optical Coherence Tomography
(OCT) systems as a tool to penetrate different materials and detect sub-surface
irregularities. This is useful for monitoring production processes, allowing
Non-Destructive Inspection Techniques of great value to the industry. In this
exploratory study, several acquisitions are made on composite and ceramics to
know the capabilities of the system. In addition, it is assessed which
preprocessing and AI-enhanced vision algorithms can be anomaly-detection
methodologies capable of detecting abnormal zones in the analyzed objects.
Limitations and criteria for the selection of optimal parameters will be
discussed, as well as strengths and weaknesses will be highlighted.

</details>


### [243] [Classification based deep learning models for lung cancer and disease using medical images](https://arxiv.org/abs/2507.01279)
*Ahmad Chaddad,Jihao Peng,Yihang Wu*

Main category: eess.IV

TL;DR: A novel CNN model, ResNet+, improves lung cancer and disease prediction by integrating ResNet-D for better feature extraction and a convolutional attention module for focusing on relevant image regions. It achieves high accuracy and computational efficiency.


<details>
  <summary>Details</summary>
Motivation: To enhance lung cancer and disease prediction using medical images by addressing feature loss during downsampling in CNNs and improving model generalization.

Method: ResNet+ integrates ResNet-D for enhanced feature extraction and a convolutional attention module in bottleneck layers. Evaluated on five public datasets with data augmentation for class imbalance.

Result: Achieved 98.14% accuracy/F1 on LC2500 and 99.25%/99.13% on IQ-OTH/NCCD datasets, with computational cost savings.

Conclusion: ResNet+ outperforms baseline models, offering high accuracy and efficiency for medical image analysis.

Abstract: The use of deep learning (DL) in medical image analysis has significantly
improved the ability to predict lung cancer. In this study, we introduce a
novel deep convolutional neural network (CNN) model, named ResNet+, which is
based on the established ResNet framework. This model is specifically designed
to improve the prediction of lung cancer and diseases using the images. To
address the challenge of missing feature information that occurs during the
downsampling process in CNNs, we integrate the ResNet-D module, a variant
designed to enhance feature extraction capabilities by modifying the
downsampling layers, into the traditional ResNet model. Furthermore, a
convolutional attention module was incorporated into the bottleneck layers to
enhance model generalization by allowing the network to focus on relevant
regions of the input images. We evaluated the proposed model using five public
datasets, comprising lung cancer (LC2500 $n$=3183, IQ-OTH/NCCD $n$=1336, and
LCC $n$=25000 images) and lung disease (ChestXray $n$=5856, and COVIDx-CT
$n$=425024 images). To address class imbalance, we used data augmentation
techniques to artificially increase the representation of underrepresented
classes in the training dataset. The experimental results show that ResNet+
model demonstrated remarkable accuracy/F1, reaching 98.14/98.14\% on the
LC25000 dataset and 99.25/99.13\% on the IQ-OTH/NCCD dataset. Furthermore, the
ResNet+ model saved computational cost compared to the original ResNet series
in predicting lung cancer images. The proposed model outperformed the baseline
models on publicly available datasets, achieving better performance metrics.
Our codes are publicly available at
https://github.com/AIPMLab/Graduation-2024/tree/main/Peng.

</details>


### [244] [PanTS: The Pancreatic Tumor Segmentation Dataset](https://arxiv.org/abs/2507.01291)
*Wenxuan Li,Xinze Zhou,Qi Chen,Tianyu Lin,Pedro R. A. S. Bassi,Szymon Plotka,Jaroslaw B. Cwikla,Xiaoxi Chen,Chen Ye,Zheren Zhu,Kai Ding,Heng Li,Kang Wang,Yang Yang,Yucheng Tang,Daguang Xu,Alan L. Yuille,Zongwei Zhou*

Main category: eess.IV

TL;DR: PanTS is a large-scale pancreatic CT dataset with 36,390 scans and expert annotations, improving AI model performance in tumor detection and segmentation.


<details>
  <summary>Details</summary>
Motivation: To advance research in pancreatic CT analysis by providing a comprehensive, large-scale dataset with detailed annotations.

Method: Curated 36,390 CT scans from 145 centers, with voxel-wise annotations of tumors and 24 surrounding structures, along with metadata.

Result: AI models trained on PanTS outperform those on existing datasets, due to larger-scale annotations and additional anatomical structures.

Conclusion: PanTS sets a new benchmark for AI model development and evaluation in pancreatic CT analysis.

Abstract: PanTS is a large-scale, multi-institutional dataset curated to advance
research in pancreatic CT analysis. It contains 36,390 CT scans from 145
medical centers, with expert-validated, voxel-wise annotations of over 993,000
anatomical structures, covering pancreatic tumors, pancreas head, body, and
tail, and 24 surrounding anatomical structures such as vascular/skeletal
structures and abdominal/thoracic organs. Each scan includes metadata such as
patient age, sex, diagnosis, contrast phase, in-plane spacing, slice thickness,
etc. AI models trained on PanTS achieve significantly better performance in
pancreatic tumor detection, localization, and segmentation compared to those
trained on existing public datasets. Our analysis indicates that these gains
are directly attributable to the 16x larger-scale tumor annotations and
indirectly supported by the 24 additional surrounding anatomical structures. As
the largest and most comprehensive resource of its kind, PanTS offers a new
benchmark for developing and evaluating AI models in pancreatic CT analysis.

</details>


### [245] [SWinMamba: Serpentine Window State Space Model for Vascular Segmentation](https://arxiv.org/abs/2507.01323)
*Rongchang Zhao,Huanchi Liu,Jian Zhang*

Main category: eess.IV

TL;DR: SWinMamba, a novel method for vascular segmentation, uses serpentine window sequences and bidirectional state space models to ensure continuity in slender vascular structures, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Vascular segmentation in medical images is critical for diagnosis and surgery, but current methods often produce discontinuous results due to inadequate modeling of slender structures.

Method: SWinMamba incorporates serpentine window sequences (SWToken) for adaptive feature capture and a Bidirectional Aggregation Module (BAM) for continuity. It also uses Spatial-Frequency Fusion Unit (SFFU) for enhanced feature representation.

Result: Extensive experiments on three datasets show SWinMamba achieves superior performance with complete and connected vessel segmentation.

Conclusion: SWinMamba effectively addresses discontinuity in vascular segmentation, offering a robust solution for medical imaging applications.

Abstract: Vascular segmentation in medical images is crucial for disease diagnosis and
surgical navigation. However, the segmented vascular structure is often
discontinuous due to its slender nature and inadequate prior modeling. In this
paper, we propose a novel Serpentine Window Mamba (SWinMamba) to achieve
accurate vascular segmentation. The proposed SWinMamba innovatively models the
continuity of slender vascular structures by incorporating serpentine window
sequences into bidirectional state space models. The serpentine window
sequences enable efficient feature capturing by adaptively guiding global
visual context modeling to the vascular structure. Specifically, the Serpentine
Window Tokenizer (SWToken) adaptively splits the input image using overlapping
serpentine window sequences, enabling flexible receptive fields (RFs) for
vascular structure modeling. The Bidirectional Aggregation Module (BAM)
integrates coherent local features in the RFs for vascular continuity
representation. In addition, dual-domain learning with Spatial-Frequency Fusion
Unit (SFFU) is designed to enhance the feature representation of vascular
structure. Extensive experiments on three challenging datasets demonstrate that
the proposed SWinMamba achieves superior performance with complete and
connected vessels.

</details>


### [246] [Structure and Smoothness Constrained Dual Networks for MR Bias Field Correction](https://arxiv.org/abs/2507.01326)
*Dong Liang,Xingyu Qiu,Yuzhen Li,Wei Wang,Kuanquan Wang,Suyu Dong,Gongning Luo*

Main category: eess.IV

TL;DR: S2DNets, a novel unsupervised deep learning model, improves MR image quality by addressing intensity inhomogeneity with structural and smoothness constraints, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Intensity inhomogeneity in MR images hampers medical analysis. Current deep learning models lack structural and smoothness constraints, leading to distorted corrections.

Method: Proposes S2DNets, dual networks with piece-wise structural constraints and bias field smoothness for self-supervised bias field correction.

Result: Outperforms conventional and deep learning-based models on clinical and simulated datasets, improving visual metrics and downstream segmentation tasks.

Conclusion: S2DNets effectively corrects intensity inhomogeneity while preserving structural details, enhancing MR image analysis.

Abstract: MR imaging techniques are of great benefit to disease diagnosis. However, due
to the limitation of MR devices, significant intensity inhomogeneity often
exists in imaging results, which impedes both qualitative and quantitative
medical analysis. Recently, several unsupervised deep learning-based models
have been proposed for MR image improvement. However, these models merely
concentrate on global appearance learning, and neglect constraints from image
structures and smoothness of bias field, leading to distorted corrected
results. In this paper, novel structure and smoothness constrained dual
networks, named S2DNets, are proposed aiming to self-supervised bias field
correction. S2DNets introduce piece-wise structural constraints and smoothness
of bias field for network training to effectively remove non-uniform intensity
and retain much more structural details. Extensive experiments executed on both
clinical and simulated MR datasets show that the proposed model outperforms
other conventional and deep learning-based models. In addition to comparison on
visual metrics, downstream MR image segmentation tasks are also used to
evaluate the impact of the proposed model. The source code is available at:
https://github.com/LeongDong/S2DNets}{https://github.com/LeongDong/S2DNets.

</details>


### [247] [BronchoGAN: Anatomically consistent and domain-agnostic image-to-image translation for video bronchoscopy](https://arxiv.org/abs/2507.01387)
*Ahmad Soliman,Ron Keuth,Marian Himstedt*

Main category: eess.IV

TL;DR: BronchoGAN uses anatomical constraints and foundation model-generated depth images for robust image-to-image translation across bronchoscopy domains, improving realism and dataset generation.


<details>
  <summary>Details</summary>
Motivation: Limited bronchoscopy image availability hinders deep learning training; robust translation across domains is needed for clinical applications.

Method: Proposes BronchoGAN with anatomical constraints (bronchial orifice matching) and intermediate depth images for domain robustness.

Result: Successful translation of input domains to realistic human airway images, with improved FID, SSIM, and Dice scores (up to 0.43).

Conclusion: BronchoGAN bridges the gap in public bronchoscopy images by leveraging CT data and anatomical constraints for realistic dataset generation.

Abstract: The limited availability of bronchoscopy images makes image synthesis
particularly interesting for training deep learning models. Robust image
translation across different domains -- virtual bronchoscopy, phantom as well
as in-vivo and ex-vivo image data -- is pivotal for clinical applications. This
paper proposes BronchoGAN introducing anatomical constraints for image-to-image
translation being integrated into a conditional GAN. In particular, we force
bronchial orifices to match across input and output images. We further propose
to use foundation model-generated depth images as intermediate representation
ensuring robustness across a variety of input domains establishing models with
substantially less reliance on individual training datasets. Moreover our
intermediate depth image representation allows to easily construct paired image
data for training. Our experiments showed that input images from different
domains (e.g. virtual bronchoscopy, phantoms) can be successfully translated to
images mimicking realistic human airway appearance. We demonstrated that
anatomical settings (i.e. bronchial orifices) can be robustly preserved with
our approach which is shown qualitatively and quantitatively by means of
improved FID, SSIM and dice coefficients scores. Our anatomical constraints
enabled an improvement in the Dice coefficient of up to 0.43 for synthetic
images. Through foundation models for intermediate depth representations,
bronchial orifice segmentation integrated as anatomical constraints into
conditional GANs we are able to robustly translate images from different
bronchoscopy input domains. BronchoGAN allows to incorporate public CT scan
data (virtual bronchoscopy) in order to generate large-scale bronchoscopy image
datasets with realistic appearance. BronchoGAN enables to bridge the gap of
missing public bronchoscopy images.

</details>


### [248] [Multi Source COVID-19 Detection via Kernel-Density-based Slice Sampling](https://arxiv.org/abs/2507.01564)
*Chia-Ming Lee,Bo-Cheng Qiu,Ting-Yao Chen,Ming-Han Sun,Fang-Ying Lin,Jung-Tse Tsai,I-An Tsai,Yu-Fan Lin,Chih-Chung Hsu*

Main category: eess.IV

TL;DR: A solution for multi-source COVID-19 detection using SSFL and KDS, achieving high F1-scores with EfficientNet and Swin Transformer.


<details>
  <summary>Details</summary>
Motivation: Addressing multi-source variability in chest CT scans from four medical centers for COVID-19 detection.

Method: Uses SSFL with KDS for preprocessing, lung region extraction, quality control, and adaptive slice sampling. Compares EfficientNet and Swin Transformer.

Result: EfficientNet achieves 94.68% F1-score, Swin Transformer 93.34%.

Conclusion: The KDS-based pipeline is effective for multi-source data, emphasizing dataset balance in medical imaging.

Abstract: We present our solution for the Multi-Source COVID-19 Detection Challenge,
which classifies chest CT scans from four distinct medical centers. To address
multi-source variability, we employ the Spatial-Slice Feature Learning (SSFL)
framework with Kernel-Density-based Slice Sampling (KDS). Our preprocessing
pipeline combines lung region extraction, quality control, and adaptive slice
sampling to select eight representative slices per scan. We compare
EfficientNet and Swin Transformer architectures on the validation set. The
EfficientNet model achieves an F1-score of 94.68%, compared to the Swin
Transformer's 93.34%. The results demonstrate the effectiveness of our
KDS-based pipeline on multi-source data and highlight the importance of dataset
balance in multi-institutional medical imaging evaluation.

</details>


### [249] [Robust brain age estimation from structural MRI with contrastive learning](https://arxiv.org/abs/2507.01794)
*Carlo Alberto Barbano,Benoit Dufumier,Edouard Duchesnay,Marco Grangetto,Pietro Gori*

Main category: eess.IV

TL;DR: Contrastive learning is introduced as a scalable and robust alternative to supervised methods for brain age estimation, showing improved generalization, robustness to site-related confounds, and clinical relevance.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of supervised approaches for brain age estimation by leveraging contrastive learning for better generalization and clinical utility.

Method: A novel contrastive loss function, $\mathcal{L}^{exp}$, is introduced and evaluated on multi-site neuroimaging datasets with over 20,000 scans.

Result: Key findings include improved generalization (halving MAE), robustness to site confounds, reliable capture of accelerated aging in cognitive impairment, and strong correlation with diagnostic performance.

Conclusion: Contrastive learning is a promising direction for generalizable and clinically meaningful brain age estimation.

Abstract: Estimating brain age from structural MRI has emerged as a powerful tool for
characterizing normative and pathological aging. In this work, we explore
contrastive learning as a scalable and robust alternative to supervised
approaches for brain age estimation. We introduce a novel contrastive loss
function, $\mathcal{L}^{exp}$, and evaluate it across multiple public
neuroimaging datasets comprising over 20,000 scans. Our experiments reveal four
key findings. First, scaling pre-training on diverse, multi-site data
consistently improves generalization performance, cutting external mean
absolute error (MAE) nearly in half. Second, $\mathcal{L}^{exp}$ is robust to
site-related confounds, maintaining low scanner-predictability as training size
increases. Third, contrastive models reliably capture accelerated aging in
patients with cognitive impairment and Alzheimer's disease, as shown through
brain age gap analysis, ROC curves, and longitudinal trends. Lastly, unlike
supervised baselines, $\mathcal{L}^{exp}$ maintains a strong correlation
between brain age accuracy and downstream diagnostic performance, supporting
its potential as a foundation model for neuroimaging. These results position
contrastive learning as a promising direction for building generalizable and
clinically meaningful brain representations.

</details>


### [250] [Autoadaptive Medical Segment Anything Model](https://arxiv.org/abs/2507.01828)
*Tyler Ward,Meredith K. Owen,O'Kira Coleman,Brian Noehren,Abdullah-Al-Zubaer Imran*

Main category: eess.IV

TL;DR: ADA-SAM is a multitask learning framework for medical image segmentation that improves accuracy in limited-label settings by leveraging class activation maps and a gradient feedback mechanism.


<details>
  <summary>Details</summary>
Motivation: Manual annotation for medical image segmentation is costly and error-prone, necessitating annotation-efficient methods.

Method: ADA-SAM combines SAM with class activation maps and a gradient feedback mechanism to link segmentation and classification tasks.

Result: ADA-SAM outperforms fully-supervised and semi-supervised baselines by double digits in limited-label settings.

Conclusion: ADA-SAM offers an efficient, accurate solution for medical image segmentation with limited labeled data.

Abstract: Medical image segmentation is a key task in the imaging workflow, influencing
many image-based decisions. Traditional, fully-supervised segmentation models
rely on large amounts of labeled training data, typically obtained through
manual annotation, which can be an expensive, time-consuming, and error-prone
process. This signals a need for accurate, automatic, and annotation-efficient
methods of training these models. We propose ADA-SAM (automated,
domain-specific, and adaptive segment anything model), a novel multitask
learning framework for medical image segmentation that leverages class
activation maps from an auxiliary classifier to guide the predictions of the
semi-supervised segmentation branch, which is based on the Segment Anything
(SAM) framework. Additionally, our ADA-SAM model employs a novel gradient
feedback mechanism to create a learnable connection between the segmentation
and classification branches by using the segmentation gradients to guide and
improve the classification predictions. We validate ADA-SAM on real-world
clinical data collected during rehabilitation trials, and demonstrate that our
proposed method outperforms both fully-supervised and semi-supervised baselines
by double digits in limited label settings. Our code is available at:
https://github.com/tbwa233/ADA-SAM.

</details>


### [251] [A computationally frugal open-source foundation model for thoracic disease detection in lung cancer screening programs](https://arxiv.org/abs/2507.01881)
*Niccolò McConnell,Pardeep Vasudev,Daisuke Yamada,Daryl Cheng,Mehran Azimbagirad,John McCabe,Shahab Aslani,Ahmed H. Shahin,Yukun Zhou,The SUMMIT Consortium,Andre Altmann,Yipeng Hu,Paul Taylor,Sam M. Janes,Daniel C. Alexander,Joseph Jacob*

Main category: eess.IV

TL;DR: TANGERINE is an open-source, computationally efficient vision foundation model for LDCT analysis, enabling rapid fine-tuning for diverse disease tasks with minimal resources and data.


<details>
  <summary>Details</summary>
Motivation: The shortage of radiologists hampers large-scale LDCT scan interpretation in lung cancer screening programs, necessitating an accessible, efficient solution.

Method: TANGERINE uses self-supervised learning on 98,000 LDCTs, extending a masked autoencoder framework to 3D imaging for scalable, adaptable analysis.

Result: It achieves state-of-the-art performance across 14 disease tasks, including lung cancer, with fast convergence and strong label efficiency.

Conclusion: TANGERINE's open-source, lightweight design supports comprehensive respiratory disease management in LCS programs, beyond just cancer detection.

Abstract: Low-dose computed tomography (LDCT) imaging employed in lung cancer screening
(LCS) programs is increasing in uptake worldwide. LCS programs herald a
generational opportunity to simultaneously detect cancer and non-cancer-related
early-stage lung disease. Yet these efforts are hampered by a shortage of
radiologists to interpret scans at scale. Here, we present TANGERINE, a
computationally frugal, open-source vision foundation model for volumetric LDCT
analysis. Designed for broad accessibility and rapid adaptation, TANGERINE can
be fine-tuned off the shelf for a wide range of disease-specific tasks with
limited computational resources and training data. Relative to models trained
from scratch, TANGERINE demonstrates fast convergence during fine-tuning,
thereby requiring significantly fewer GPU hours, and displays strong label
efficiency, achieving comparable or superior performance with a fraction of
fine-tuning data. Pretrained using self-supervised learning on over 98,000
thoracic LDCTs, including the UK's largest LCS initiative to date and 27 public
datasets, TANGERINE achieves state-of-the-art performance across 14 disease
classification tasks, including lung cancer and multiple respiratory diseases,
while generalising robustly across diverse clinical centres. By extending a
masked autoencoder framework to 3D imaging, TANGERINE offers a scalable
solution for LDCT analysis, departing from recent closed, resource-intensive
models by combining architectural simplicity, public availability, and modest
computational requirements. Its accessible, open-source lightweight design lays
the foundation for rapid integration into next-generation medical imaging tools
that could transform LCS initiatives, allowing them to pivot from a singular
focus on lung cancer detection to comprehensive respiratory disease management
in high-risk populations.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [252] [Transfer Learning for VLC-based indoor Localization: Addressing Environmental Variability](https://arxiv.org/abs/2507.01575)
*Masood Jan,Wafa Njima,Xun Zhang,Alexander Artemenko*

Main category: eess.SP

TL;DR: A Transfer Learning (TL)-based approach for VLC-based indoor localization improves accuracy, energy efficiency, and computational speed in industrial settings.


<details>
  <summary>Details</summary>
Motivation: Address challenges in VLC-based indoor localization caused by environmental variability like lighting fluctuations and obstacles.

Method: Proposes a TL framework integrating a deep neural network (DNN) using real-world data from a BOSCH factory.

Result: Improves localization accuracy by 47%, reduces energy consumption by 32%, and decreases computational time by 40% compared to conventional models.

Conclusion: The TL-based solution is adaptable, cost-efficient, and scalable for Industry 4.0 applications, achieving high accuracy with minimal data.

Abstract: Accurate indoor localization is crucial in industrial environments. Visible
Light Communication (VLC) has emerged as a promising solution, offering high
accuracy, energy efficiency, and minimal electromagnetic interference. However,
VLC-based indoor localization faces challenges due to environmental
variability, such as lighting fluctuations and obstacles. To address these
challenges, we propose a Transfer Learning (TL)-based approach for VLC-based
indoor localization. Using real-world data collected at a BOSCH factory, the TL
framework integrates a deep neural network (DNN) to improve localization
accuracy by 47\%, reduce energy consumption by 32\%, and decrease computational
time by 40\% compared to the conventional models. The proposed solution is
highly adaptable under varying environmental conditions and achieves similar
accuracy with only 30\% of the dataset, making it a cost-efficient and scalable
option for industrial applications in Industry 4.0.

</details>


### [253] [Token Communication in the Era of Large Models: An Information Bottleneck-Based Approach](https://arxiv.org/abs/2507.01728)
*Hao Wei,Wanli Ni,Wen Wang,Wenjun Xu,Dusit Niyato,Ping Zhang*

Main category: eess.SP

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This letter proposes UniToCom, a unified token communication paradigm that
treats tokens as the fundamental units for both processing and wireless
transmission. Specifically, to enable efficient token representations, we
propose a generative information bottleneck (GenIB) principle, which
facilitates the learning of tokens that preserve essential information while
supporting reliable generation across multiple modalities. By doing this,
GenIB-based tokenization is conducive to improving the communication efficiency
and reducing computational complexity. Additionally, we develop $\sigma$-GenIB
to address the challenges of variance collapse in autoregressive modeling,
maintaining representational diversity and stability. Moreover, we employ a
causal Transformer-based multimodal large language model (MLLM) at the receiver
to unify the processing of both discrete and continuous tokens under the
next-token prediction paradigm. Simulation results validate the effectiveness
and superiority of the proposed UniToCom compared to baselines under dynamic
channel conditions. By integrating token processing with MLLMs, UniToCom
enables scalable and generalizable communication in favor of multimodal
understanding and generation, providing a potential solution for
next-generation intelligent communications.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [254] [Age Sensitive Hippocampal Functional Connectivity: New Insights from 3D CNNs and Saliency Mapping](https://arxiv.org/abs/2507.01411)
*Yifei Sun,Marshall A. Dalton,Robert D. Sanders,Yixuan Yuan,Xiang Li,Sharon L. Naismith,Fernando Calamante,Jinglei Lv*

Main category: q-bio.NC

TL;DR: A deep learning framework predicts brain age from hippocampal functional connectivity, revealing age-sensitive connections and distinct anterior-posterior hippocampal patterns.


<details>
  <summary>Details</summary>
Motivation: To understand functional connectivity changes in the hippocampus during aging, which remains poorly understood despite its neurobiological significance.

Method: A 3D CNN combined with LayerCAM saliency mapping is used to predict brain age from hippocampal FC, identifying key age-sensitive connections.

Result: Key hippocampal-cortical connections (e.g., precuneus, cuneus) are identified, with distinct patterns for anterior and posterior hippocampus.

Conclusion: The study provides insights into hippocampal aging and demonstrates the value of explainable deep learning in neuroimaging.

Abstract: Grey matter loss in the hippocampus is a hallmark of neurobiological aging,
yet understanding the corresponding changes in its functional connectivity
remains limited. Seed-based functional connectivity (FC) analysis enables
voxel-wise mapping of the hippocampus's synchronous activity with cortical
regions, offering a window into functional reorganization during aging. In this
study, we develop an interpretable deep learning framework to predict brain age
from hippocampal FC using a three-dimensional convolutional neural network (3D
CNN) combined with LayerCAM saliency mapping. This approach maps key
hippocampal-cortical connections, particularly with the precuneus, cuneus,
posterior cingulate cortex, parahippocampal cortex, left superior parietal
lobule, and right superior temporal sulcus, that are highly sensitive to age.
Critically, disaggregating anterior and posterior hippocampal FC reveals
distinct mapping aligned with their known functional specializations. These
findings provide new insights into the functional mechanisms of hippocampal
aging and demonstrate the power of explainable deep learning to uncover
biologically meaningful patterns in neuroimaging data.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [255] [Epistemic Scarcity: The Economics of Unresolvable Unknowns](https://arxiv.org/abs/2507.01483)
*Craig S Wright*

Main category: econ.GN

TL;DR: The paper critiques AI's role in economic and epistemic governance, arguing it cannot perform core economic functions like interpreting ends or communicating value. It challenges ethical AI frameworks and highlights the Austrian tradition as an alternative to computational control.


<details>
  <summary>Details</summary>
Motivation: The paper aims to challenge assumptions about AI's capacity to sustain economic and epistemic order, emphasizing its limitations in performing human-like economic coordination and ethical reasoning.

Method: The analysis uses Misesian a priori reasoning and Austrian theories of entrepreneurship to critique AI's role, contrasting it with neoclassical and behavioural models. It also examines ethical AI frameworks like FAT.

Result: The paper concludes that AI systems are incapable of originating norms, interpreting institutions, or bearing responsibility, and critiques their alignment with constructivist rationalism.

Conclusion: The debate over AI is framed as a civilizational issue concerning human autonomy and institutional evolution, with the Austrian tradition proposed as a coherent alternative to computational social control.

Abstract: This paper presents a praxeological analysis of artificial intelligence and
algorithmic governance, challenging assumptions about the capacity of machine
systems to sustain economic and epistemic order. Drawing on Misesian a priori
reasoning and Austrian theories of entrepreneurship, we argue that AI systems
are incapable of performing the core functions of economic coordination:
interpreting ends, discovering means, and communicating subjective value
through prices. Where neoclassical and behavioural models treat decisions as
optimisation under constraint, we frame them as purposive actions under
uncertainty.
  We critique dominant ethical AI frameworks such as Fairness, Accountability,
and Transparency (FAT) as extensions of constructivist rationalism, which
conflict with a liberal order grounded in voluntary action and property rights.
Attempts to encode moral reasoning in algorithms reflect a misunderstanding of
ethics and economics. However complex, AI systems cannot originate norms,
interpret institutions, or bear responsibility. They remain opaque, misaligned,
and inert.
  Using the concept of epistemic scarcity, we explore how information abundance
degrades truth discernment, enabling both entrepreneurial insight and soft
totalitarianism. Our analysis ends with a civilisational claim: the debate over
AI concerns the future of human autonomy, institutional evolution, and reasoned
choice. The Austrian tradition, focused on action, subjectivity, and
spontaneous order, offers the only coherent alternative to rising computational
social control.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [256] [Automated Vehicles Should be Connected with Natural Language](https://arxiv.org/abs/2507.01059)
*Xiangbo Gao,Keshu Wu,Hao Zhang,Kexin Tian,Yang Zhou,Zhengzhong Tu*

Main category: cs.MA

TL;DR: The paper proposes using natural language for intent and reasoning communication in multi-agent collaborative driving to overcome bandwidth, interoperability, and decision-level fusion challenges.


<details>
  <summary>Details</summary>
Motivation: Existing communication methods (sensor data, neural features, perception results) are limited in bandwidth efficiency, information completeness, and interoperability, and neglect decision-level fusion.

Method: Transition from perception-oriented data exchanges to explicit intent and reasoning communication via natural language.

Result: Natural language balances semantic density and bandwidth, adapts to real-time conditions, and bridges heterogeneous platforms, enabling proactive coordination.

Conclusion: Natural language communication enhances safety, efficiency, and transparency in collaborative driving, advancing intelligent transportation systems.

Abstract: Multi-agent collaborative driving promises improvements in traffic safety and
efficiency through collective perception and decision making. However, existing
communication media -- including raw sensor data, neural network features, and
perception results -- suffer limitations in bandwidth efficiency, information
completeness, and agent interoperability. Moreover, traditional approaches have
largely ignored decision-level fusion, neglecting critical dimensions of
collaborative driving. In this paper we argue that addressing these challenges
requires a transition from purely perception-oriented data exchanges to
explicit intent and reasoning communication using natural language. Natural
language balances semantic density and communication bandwidth, adapts flexibly
to real-time conditions, and bridges heterogeneous agent platforms. By enabling
the direct communication of intentions, rationales, and decisions, it
transforms collaborative driving from reactive perception-data sharing into
proactive coordination, advancing safety, efficiency, and transparency in
intelligent transportation systems.

</details>


### [257] [RALLY: Role-Adaptive LLM-Driven Yoked Navigation for Agentic UAV Swarms](https://arxiv.org/abs/2507.01378)
*Ziyao Wang,Rongpeng Li,Sizhao Li,Yuming Xiang,Haiping Wang,Zhifeng Zhao,Honggang Zhang*

Main category: cs.MA

TL;DR: RALLY is a Role-Adaptive LLM-Driven Yoked navigation algorithm for UAV swarms, combining LLM semantic reasoning with MARL online learning for improved task coverage, convergence, and generalization.


<details>
  <summary>Details</summary>
Motivation: Traditional MARL lacks semantic communication and role flexibility, while LLM-based frameworks struggle with exploration and online learning. RALLY addresses these gaps.

Method: Uses an LLM-driven semantic decision framework, dynamic role-heterogeneity, and RMIX-based assignment to integrate offline priors with online policies.

Result: Outperforms conventional methods in task coverage, convergence speed, and generalization in MPE and SITL tests.

Conclusion: RALLY enhances collaborative navigation in UAV swarms by bridging semantic reasoning and adaptive learning.

Abstract: Intelligent control of Unmanned Aerial Vehicles (UAVs) swarms has emerged as
a critical research focus, and it typically requires the swarm to navigate
effectively while avoiding obstacles and achieving continuous coverage over
multiple mission targets. Although traditional Multi-Agent Reinforcement
Learning (MARL) approaches offer dynamic adaptability, they are hindered by the
semantic gap in numerical communication and the rigidity of homogeneous role
structures, resulting in poor generalization and limited task scalability.
Recent advances in Large Language Model (LLM)-based control frameworks
demonstrate strong semantic reasoning capabilities by leveraging extensive
prior knowledge. However, due to the lack of online learning and over-reliance
on static priors, these works often struggle with effective exploration,
leading to reduced individual potential and overall system performance. To
address these limitations, we propose a Role-Adaptive LLM-Driven Yoked
navigation algorithm RALLY. Specifically, we first develop an LLM-driven
semantic decision framework that uses structured natural language for efficient
semantic communication and collaborative reasoning. Afterward, we introduce a
dynamic role-heterogeneity mechanism for adaptive role switching and
personalized decision-making. Furthermore, we propose a Role-value Mixing
Network (RMIX)-based assignment strategy that integrates LLM offline priors
with MARL online policies to enable semi-offline training of role selection
strategies. Experiments in the Multi-Agent Particle Environment (MPE)
environment and a Software-In-The-Loop (SITL) platform demonstrate that RALLY
outperforms conventional approaches in terms of task coverage, convergence
speed, and generalization, highlighting its strong potential for collaborative
navigation in agentic multi-UAV systems.

</details>


### [258] [Exploring Advanced LLM Multi-Agent Systems Based on Blackboard Architecture](https://arxiv.org/abs/2507.01701)
*Bochen Han,Songmao Zhang*

Main category: cs.MA

TL;DR: Incorporating blackboard architecture into LLM multi-agent systems improves information sharing, dynamic agent selection, and consensus-building, achieving competitive performance with fewer tokens.


<details>
  <summary>Details</summary>
Motivation: To enhance problem-solving in dynamic environments where predefined structures are lacking by enabling shared information and adaptive agent roles.

Method: Proposes a blackboard architecture for LLM multi-agent systems, allowing shared information, dynamic agent selection, and iterative consensus-building. Implemented and tested on commonsense, reasoning, and math datasets.

Result: Competitive with SOTA static and dynamic MASs, achieving best average performance while using fewer tokens.

Conclusion: The blackboard architecture enables complex, dynamic problem-solving without rigid workflows, showing promise for flexible multi-agent systems.

Abstract: In this paper, we propose to incorporate the blackboard architecture into LLM
multi-agent systems (MASs) so that (1) agents with various roles can share all
the information and others' messages during the whole problem-solving process,
(2) agents that will take actions are selected based on the current content of
the blackboard, and (3) the selection and execution round is repeated until a
consensus is reached on the blackboard. We develop the first implementation of
this proposal and conduct experiments on commonsense knowledge, reasoning and
mathematical datasets. The results show that our system can be competitive with
the SOTA static and dynamic MASs by achieving the best average performance, and
at the same time manage to spend less tokens. Our proposal has the potential to
enable complex and dynamic problem-solving where well-defined structures or
workflows are unavailable.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [259] [Customized Exploration of Landscape Features Driving Multi-Objective Combinatorial Optimization Performance](https://arxiv.org/abs/2507.01638)
*Ana Nikolikj,Gabriela Ochoa,Tome Eftimov*

Main category: cs.NE

TL;DR: Analysis of landscape features for predicting multi-objective optimization algorithm performance using C-PLOS-net model on rmnk-landscapes.


<details>
  <summary>Details</summary>
Motivation: To understand how landscape features influence the performance of optimization algorithms in multi-objective combinatorial problems.

Method: Analyze features from C-PLOS-net model on rmnk-landscapes with 2-3 objectives, evaluating PLS, GSEMO, and NSGA-II using resolution and hypervolume metrics.

Result: Identified feature combinations that influence algorithm performance for specific landscapes.

Conclusion: Provides insights into feature importance tailored to specific rmnk-landscapes and algorithms.

Abstract: We present an analysis of landscape features for predicting the performance
of multi-objective combinatorial optimization algorithms. We consider features
from the recently proposed compressed Pareto Local Optimal Solutions Networks
(C-PLOS-net) model of combinatorial landscapes. The benchmark instances are a
set of rmnk-landscapes with 2 and 3 objectives and various levels of ruggedness
and objective correlation. We consider the performance of three algorithms --
Pareto Local Search (PLS), Global Simple EMO Optimizer (GSEMO), and
Non-dominated Sorting Genetic Algorithm (NSGA-II) - using the resolution and
hypervolume metrics. Our tailored analysis reveals feature combinations that
influence algorithm performance specific to certain landscapes. This study
provides deeper insights into feature importance, tailored to specific
rmnk-landscapes and algorithms.

</details>


### [260] [Comparing Optimization Algorithms Through the Lens of Search Behavior Analysis](https://arxiv.org/abs/2507.01668)
*Gjorgjina Cenikj,Gašper Petelin,Tome Eftimov*

Main category: cs.NE

TL;DR: The paper critiques the proliferation of metaheuristic algorithms in numerical optimization and uses statistical tests to compare their search behaviors, analyzing 114 algorithms from MEALPY.


<details>
  <summary>Details</summary>
Motivation: Address concerns about the lack of meaningful innovation and differentiation among metaheuristic algorithms inspired by natural or human-made processes.

Method: Utilize the cross-match statistical test to compare multivariate distributions of solutions produced by 114 algorithms from MEALPY.

Result: Empirical analysis identifies algorithms with similar search behaviors, helping distinguish meaningful innovations.

Conclusion: Statistical tests provide a robust method to evaluate and differentiate metaheuristic algorithms, addressing criticism of their novelty and applicability.

Abstract: The field of numerical optimization has recently seen a surge in the
development of "novel" metaheuristic algorithms, inspired by metaphors derived
from natural or human-made processes, which have been widely criticized for
obscuring meaningful innovations and failing to distinguish themselves from
existing approaches. Aiming to address these concerns, we investigate the
applicability of statistical tests for comparing algorithms based on their
search behavior. We utilize the cross-match statistical test to compare
multivariate distributions and assess the solutions produced by 114 algorithms
from the MEALPY library. These findings are incorporated into an empirical
analysis aiming to identify algorithms with similar search behaviors.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [261] [GPU-based complete search for nonlinear minimization subject to bounds](https://arxiv.org/abs/2507.01770)
*Guanglu Zhang,Qihang Shan,Jonathan Cagan*

Main category: math.NA

TL;DR: A GPU-based complete search method using interval analysis to guarantee the global minimum of nonlinear functions, validated on high-dimensional benchmark functions.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of efficiently and rigorously enclosing the global minimum of nonlinear functions, especially for high-dimensional problems, leveraging GPU computational power.

Method: Combines interval analysis with GPU parallelism, employing a single program, single data approach and variable cycling to optimize performance and reduce computational costs.

Result: Successfully encloses the global minimum of 10 benchmark functions with up to 10,000 dimensions, outperforming existing literature.

Conclusion: The method demonstrates superior efficiency and rigor in global optimization, enabled by GPU architecture and innovative design.

Abstract: This paper introduces a GPU-based complete search method to enclose the
global minimum of a nonlinear function subject to simple bounds on the
variables. Using interval analysis, coupled with the computational power and
architecture of GPU, the method iteratively rules out the regions in the search
domain where the global minimum cannot exist and leaves a finite set of regions
where the global minimum must exist. For effectiveness, because of the rigor of
interval analysis, the method is guaranteed to enclose the global minimum of
the nonlinear function even in the presence of rounding errors. For efficiency,
the method employs a novel GPU-based single program, single data parallel
programming style to circumvent major GPU performance bottlenecks, and a
variable cycling technique is also integrated into the method to reduce
computational cost when minimizing large-scale nonlinear functions. The method
is validated by minimizing 10 multimodal benchmark test functions with scalable
dimensions, including the well-known Ackley function, Griewank function, Levy
function, and Rastrigin function. These benchmark test functions represent
grand challenges of global optimization, and enclosing the guaranteed global
minimum of these benchmark test functions with more than 80 dimensions has not
been reported in the literature. Our method completely searches the feasible
domain and successfully encloses the guaranteed global minimum of these 10
benchmark test functions with up to 10,000 dimensions using only one GPU in a
reasonable computation time, far exceeding the reported results in the
literature due to the unique method design and implementation based on GPU
architecture.

</details>


### [262] [Consistency of Learned Sparse Grid Quadrature Rules using NeuralODEs](https://arxiv.org/abs/2507.01533)
*Hanno Gottschalk,Emil Partow,Tobias J. Riedlinger*

Main category: math.NA

TL;DR: The paper proves the consistency of sparse grid quadrature for high-dimensional integration using a learned transport map and neural ODEs, with PAC learning guarantees.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of numerical integration in high-dimensional spaces by combining transport maps and sparse grid quadrature.

Method: 1. Learn a transport map to normalize the distribution. 2. Use Clenshaw-Curtis sparse grid quadrature for integration. 3. Decompose errors into quadrature and statistical components.

Result: Proves that errors can be controlled in the PAC learning framework, ensuring high-probability convergence to the theoretical integral value.

Conclusion: The method reliably approximates high-dimensional integrals with controlled errors, validated by PAC learning theory.

Abstract: This paper provides a proof of the consistency of sparse grid quadrature for
numerical integration of high dimensional distributions. In a first step, a
transport map is learned that normalizes the distribution to a noise
distribution on the unit cube. This step is built on the statistical learning
theory of neural ordinary differential equations, which has been established
recently. Secondly, the composition of the generative map with the quantity of
interest is integrated numerically using the Clenshaw-Curtis sparse grid
quadrature. A decomposition of the total numerical error in quadrature error
and statistical error is provided. As main result it is proven in the framework
of empirical risk minimization that all error terms can be controlled in the
sense of PAC (probably approximately correct) learning and with high
probability the numerical integral approximates the theoretical value up to an
arbitrary small error in the limit where the data set size is growing and the
network capacity is increased adaptively.

</details>


### [263] [Neural Entropy-stable conservative flux form neural networks for learning hyperbolic conservation laws](https://arxiv.org/abs/2507.01795)
*Lizuo Liu,Lu Zhang,Anne Gelb*

Main category: math.NA

TL;DR: A neural network (NESCFN) learns hyperbolic conservation laws and entropy functions directly from data, ensuring stability and conservation without predefined discretization.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on prior knowledge or fixed discretization; NESCFN removes this dependency by embedding entropy-stable principles for data-driven discovery.

Method: Jointly learns numerical flux functions and entropy, embedding entropy-stable design principles for conservation and entropy dissipation.

Result: Achieves stability, conservation, and accurate shock propagation speeds without future-time data in training.

Conclusion: NESCFN enables physically consistent dynamics in a fully data-driven setting, critical for long-term stability in hyperbolic systems.

Abstract: We propose a neural entropy-stable conservative flux form neural network
(NESCFN) for learning hyperbolic conservation laws and their associated entropy
functions directly from solution trajectories, without requiring any predefined
numerical discretization. While recent neural network architectures have
successfully integrated classical numerical principles into learned models,
most rely on prior knowledge of the governing equations or assume a fixed
discretization. Our approach removes this dependency by embedding
entropy-stable design principles into the learning process itself, enabling the
discovery of physically consistent dynamics in a fully data-driven setting. By
jointly learning both the numerical flux function and a corresponding entropy,
the proposed method ensures conservation and entropy dissipation, critical for
long-term stability and fidelity in the system of hyperbolic conservation laws.
Numerical results demonstrate that the method achieves stability and
conservation over extended time horizons and accurately captures shock
propagation speeds, even without oracle access to future-time solution profiles
in the training data.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [264] [HPC-AI Coupling Methodology for Scientific Applications](https://arxiv.org/abs/2507.01025)
*Yutong Lu,Dan Huang,Pin Chen*

Main category: cs.CE

TL;DR: The paper explores coupling HPC and AI (HPC-AI) in scientific applications, introducing three coupling patterns: surrogate, directive, and coordinate, demonstrated through materials science case studies.


<details>
  <summary>Details</summary>
Motivation: To address challenges like high computational intensity in HPC applications using AI-driven approaches.

Method: Presents a novel methodology with three coupling patterns (surrogate, directive, coordinate) and validates them through materials science case studies.

Result: Demonstrates effectiveness of coupling patterns, highlighting performance improvements and technical challenges.

Conclusion: The coupling patterns are broadly applicable, offering guidance for future HPC-AI integration in scientific domains.

Abstract: Artificial intelligence (AI) technologies have fundamentally transformed
numerical-based high-performance computing (HPC) applications with data-driven
approaches and endeavored to address existing challenges, e.g. high
computational intensity, in various scientific domains. In this study, we
explore the scenarios of coupling HPC and AI (HPC-AI) in the context of
emerging scientific applications, presenting a novel methodology that
incorporates three patterns of coupling: surrogate, directive, and coordinate.
Each pattern exemplifies a distinct coupling strategy, AI-driven prerequisite,
and typical HPC-AI ensembles. Through case studies in materials science, we
demonstrate the application and effectiveness of these patterns. The study
highlights technical challenges, performance improvements, and implementation
details, providing insight into promising perspectives of HPC-AI coupling. The
proposed coupling patterns are applicable not only to materials science but
also to other scientific domains, offering valuable guidance for future HPC-AI
ensembles in scientific discovery.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [265] [Characterizing control between interacting subsystems with deep Jacobian estimation](https://arxiv.org/abs/2507.01946)
*Adam J. Eisen,Mitchell Ostrow,Sarthak Chandra,Leo Kozachkov,Earl K. Miller,Ila R. Fiete*

Main category: q-bio.QM

TL;DR: A nonlinear control-theoretic framework (JacobianODE) is proposed to analyze subsystem interactions in biological systems, outperforming existing methods and enabling precise control of neural networks.


<details>
  <summary>Details</summary>
Motivation: Current methods for understanding subsystem control are linear and fail to capture nonlinear dynamics in complex biological systems.

Method: Developed JacobianODE, a deep learning method to estimate Jacobians from time-series data for arbitrary dynamical systems.

Result: JacobianODE outperforms existing methods, even in high-dimensional chaos, and reveals control dynamics in a multi-area RNN.

Conclusion: The framework provides a data-driven, theoretically grounded approach to studying biological subsystem interactions and control.

Abstract: Biological function arises through the dynamical interactions of multiple
subsystems, including those between brain areas, within gene regulatory
networks, and more. A common approach to understanding these systems is to
model the dynamics of each subsystem and characterize communication between
them. An alternative approach is through the lens of control theory: how the
subsystems control one another. This approach involves inferring the
directionality, strength, and contextual modulation of control between
subsystems. However, methods for understanding subsystem control are typically
linear and cannot adequately describe the rich contextual effects enabled by
nonlinear complex systems. To bridge this gap, we devise a data-driven
nonlinear control-theoretic framework to characterize subsystem interactions
via the Jacobian of the dynamics. We address the challenge of learning
Jacobians from time-series data by proposing the JacobianODE, a deep learning
method that leverages properties of the Jacobian to directly estimate it for
arbitrary dynamical systems from data alone. We show that JacobianODEs
outperform existing Jacobian estimation methods on challenging systems,
including high-dimensional chaos. Applying our approach to a multi-area
recurrent neural network (RNN) trained on a working memory selection task, we
show that the "sensory" area gains greater control over the "cognitive" area
over learning. Furthermore, we leverage the JacobianODE to directly control the
trained RNN, enabling precise manipulation of its behavior. Our work lays the
foundation for a theoretically grounded and data-driven understanding of
interactions among biological subsystems.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [266] [AI-guided digital intervention with physiological monitoring reduces intrusive memories after experimental trauma](https://arxiv.org/abs/2507.01081)
*Megan T. deBettencourt,Sruthi Sakthivel,Emily A. Holmes,Mark Chevillet*

Main category: cs.HC

TL;DR: ANTIDOTE, combining AI and pupillometry, effectively delivers ICTI to reduce intrusive memories post-trauma, showing promise for scalable digital interventions.


<details>
  <summary>Details</summary>
Motivation: Address the scalability limitations of human-guided digital treatments for trauma by leveraging AI and neurotechnology.

Method: Tested ANTIDOTE (AI + pupillometry) on 100 healthy volunteers exposed to traumatic videos, comparing intervention and control groups.

Result: Intervention group reported fewer intrusive memories; pupil size tracked engagement and predicted symptom reduction.

Conclusion: AI-guided interventions like ANTIDOTE can scale effectively, with pupillometry as a biomarker for engagement and effectiveness.

Abstract: Trauma prevalence is vast globally. Evidence-based digital treatments can
help, but most require human guidance. Human guides provide tailored
instructions and responsiveness to internal cognitive states, but limit
scalability. Can generative AI and neurotechnology provide a scalable
alternative? Here we test ANTIDOTE, combining AI guidance and pupillometry to
automatically deliver and monitor an evidence-based digital treatment,
specifically the Imagery Competing Task Intervention (ICTI), to reduce
intrusive memories after psychological trauma. One hundred healthy volunteers
were exposed to videos of traumatic events and randomly assigned to an
intervention or active control condition. As predicted, intervention
participants reported significantly fewer intrusive memories over the following
week. Post-hoc assessment against clinical rubrics confirmed the AI guide
delivered the intervention successfully. Additionally, pupil size tracked
intervention engagement and predicted symptom reduction, providing a candidate
biomarker of intervention effectiveness. These findings open a path toward
rigorous AI-guided digital interventions that can scale to trauma prevalence.

</details>


### [267] [AI Meets Maritime Training: Precision Analytics for Enhanced Safety and Performance](https://arxiv.org/abs/2507.01274)
*Vishakha Lall,Yisi Liu*

Main category: cs.HC

TL;DR: An AI-driven framework enhances maritime training by objectively assessing trainee performance using visual focus tracking, speech recognition, and stress detection, achieving high accuracy and surpassing benchmarks.


<details>
  <summary>Details</summary>
Motivation: Traditional maritime training relies on subjective trainer assessments, leading to challenges like subjectivity and difficulty in measuring key features. This study aims to address these issues with AI-driven objective assessments.

Method: The system integrates AI techniques: visual focus tracking (eye tracking, pupil dilation, computer vision), communication analysis (speech-to-text, NLP), correctness (large language models), and stress detection (vocal pitch). Evaluated on simulated maritime scenarios.

Result: High accuracy achieved: ~92% (visual detection), ~91% (speech recognition), ~90% (stress detection), surpassing benchmarks. Provides insights into attention, communication, and stress levels.

Conclusion: The study demonstrates AI's potential to transform maritime training by offering objective analytics, personalized feedback, and improved preparedness for real-world challenges.

Abstract: Traditional simulator-based training for maritime professionals is critical
for ensuring safety at sea but often depends on subjective trainer assessments
of technical skills, behavioral focus, communication, and body language, posing
challenges such as subjectivity, difficulty in measuring key features, and
cognitive limitations. Addressing these issues, this study develops an
AI-driven framework to enhance maritime training by objectively assessing
trainee performance through visual focus tracking, speech recognition, and
stress detection, improving readiness for high-risk scenarios. The system
integrates AI techniques, including visual focus determination using eye
tracking, pupil dilation analysis, and computer vision; communication analysis
through a maritime-specific speech-to-text model and natural language
processing; communication correctness using large language models; and mental
stress detection via vocal pitch. Models were evaluated on data from simulated
maritime scenarios with seafarers exposed to controlled high-stress events. The
AI algorithms achieved high accuracy, with ~92% for visual detection, ~91% for
maritime speech recognition, and ~90% for stress detection, surpassing existing
benchmarks. The system provides insights into visual attention, adherence to
communication checklists, and stress levels under demanding conditions. This
study demonstrates how AI can transform maritime training by delivering
objective performance analytics, enabling personalized feedback, and improving
preparedness for real-world operational challenges.

</details>


### [268] [Crafting Hanzi as Narrative Bridges: An AI Co-Creation Workshop for Elderly Migrants](https://arxiv.org/abs/2507.01548)
*Wen Zhan,Ziqun Hua,Peiyue Lin,Yunfei Chen*

Main category: cs.HC

TL;DR: Older migrants in urban China used AI-assisted co-creation to express fragmented personal narratives through Hanzi reconstruction and storytelling, highlighting AI's supportive role.


<details>
  <summary>Details</summary>
Motivation: To address the underrepresentation and verbalization challenges of personal narratives among aging migrants in urban China.

Method: A pilot workshop combining oral storytelling and symbolic Hanzi reconstruction with AI suggestions (LLM) and physical materials, facilitated by humans and soft AI.

Result: Participants transformed lived experiences into visual/tactile expressions without digital literacy, showcasing AI as a supportive mechanism.

Conclusion: This approach repositions AI as a supportive tool in human-AI collaboration, enhancing narrative agency for aging populations.

Abstract: This paper explores how older adults, particularly aging migrants in urban
China, can engage AI-assisted co-creation to express personal narratives that
are often fragmented, underrepresented, or difficult to verbalize. Through a
pilot workshop combining oral storytelling and the symbolic reconstruction of
Hanzi, participants shared memories of migration and recreated new character
forms using Xiaozhuan glyphs, suggested by the Large Language Model (LLM),
together with physical materials. Supported by human facilitation and a soft AI
presence, participants transformed lived experience into visual and tactile
expressions without requiring digital literacy. This approach offers new
perspectives on human-AI collaboration and aging by repositioning AI not as a
content producer but as a supportive mechanism, and by supporting narrative
agency within sociotechnical systems.

</details>


### [269] [Towards culturally-appropriate conversational AI for health in the majority world: An exploratory study with citizens and professionals in Latin America](https://arxiv.org/abs/2507.01719)
*Dorian Peters,Fernanda Espinoza,Marco da Re,Guido Ivetta,Luciana Benotti,Rafael A. Calvo*

Main category: cs.HC

TL;DR: The paper advocates for a bottom-up, culturally-grounded approach to conversational AI (CAI) in health, focusing on Latin America, to address global exclusions in current LLMs.


<details>
  <summary>Details</summary>
Motivation: Current LLMs exclude many global lived experiences, necessitating culturally and linguistically appropriate CAI for health in diverse contexts.

Method: Qualitative data from participatory workshops in Latin America to identify cultural misalignments, regional perspectives, and strategies for culturally-appropriate CAI.

Result: Findings highlight the need for a broader framework integrating economics, politics, and geography into cultural understanding, leading to the 'Pluriversal Conversational AI for Health' framework.

Conclusion: A relational and tolerant approach, beyond just more data, is essential for culturally-appropriate CAI in health.

Abstract: There is justifiable interest in leveraging conversational AI (CAI) for
health across the majority world, but to be effective, CAI must respond
appropriately within culturally and linguistically diverse contexts. Therefore,
we need ways to address the fact that current LLMs exclude many lived
experiences globally. Various advances are underway which focus on top-down
approaches and increasing training data. In this paper, we aim to complement
these with a bottom-up locally-grounded approach based on qualitative data
collected during participatory workshops in Latin America. Our goal is to
construct a rich and human-centred understanding of: a) potential areas of
cultural misalignment in digital health; b) regional perspectives on chatbots
for health and c)strategies for creating culturally-appropriate CAI; with a
focus on the understudied Latin American context. Our findings show that
academic boundaries on notions of culture lose meaning at the ground level and
technologies will need to engage with a broader framework; one that
encapsulates the way economics, politics, geography and local logistics are
entangled in cultural experience. To this end, we introduce a framework for
'Pluriversal Conversational AI for Health' which allows for the possibility
that more relationality and tolerance, rather than just more data, may be
called for.

</details>


### [270] [Bridging UI Design and chatbot Interactions: Applying Form-Based Principles to Conversational Agents](https://arxiv.org/abs/2507.01862)
*Sanjay Krishna Anbalagan,Xinrui Nie,Umesh Mohan,Vijay Kumar Kanamarlapudi,Anughna Kommalapati,Xiaodan Zhao*

Main category: cs.HC

TL;DR: The paper proposes modeling GUI-like 'submit' and 'reset' actions in conversational agents using LLM prompts to improve clarity and efficiency in domain-specific chatbots.


<details>
  <summary>Details</summary>
Motivation: Traditional GUIs handle multi-step interactions clearly, while conversational agents rely on ambiguous language cues, leading to confusion.

Method: Model acknowledgment (submit-like) and context switching (reset-like) as explicit tasks in LLM prompts, capturing structured session data.

Result: Demonstrated improvements in multi-turn task coherence, user satisfaction, and efficiency in hotel booking and customer management scenarios.

Conclusion: Explicit modeling of GUI-inspired actions in chatbots enhances clarity, reduces confusion, and aligns interactions with back-end logic.

Abstract: Domain specific chatbot applications often involve multi step interactions,
such as refining search filters, selecting multiple items, or performing
comparisons. Traditional graphical user interfaces (GUIs) handle these
workflows by providing explicit "Submit" (commit data) and "Reset" (discard
data) actions, allowing back-end systems to track user intent unambiguously. In
contrast, conversational agents rely on subtle language cues, which can lead to
confusion and incomplete context management. This paper proposes modeling these
GUI inspired metaphors acknowledgment (submit like) and context switching
(reset-like) as explicit tasks within large language model (LLM) prompts. By
capturing user acknowledgment, reset actions, and chain of thought (CoT)
reasoning as structured session data, we preserve clarity, reduce user
confusion, and align domain-specific chatbot interactions with back-end logic.
We demonstrate our approach in hotel booking and customer management scenarios,
highlighting improvements in multi-turn task coherence, user satisfaction, and
efficiency.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [271] [Empirical Analysis Of Heuristic and Approximation Algorithms for the The Mutual-Visibility Problem](https://arxiv.org/abs/2507.01076)
*Vanja Stojanović,Bor Pangeršič*

Main category: cs.CG

TL;DR: The paper empirically analyzes the mutual-visibility (MV) problem, testing three algorithms (greedy heuristic, hypergraph-based approximation, genetic algorithm) on synthetic graphs. Results show alignment with theory for small graphs but divergence for larger ones, with the genetic algorithm performing best.


<details>
  <summary>Details</summary>
Motivation: The mutual-visibility problem lacks empirical analysis despite theoretical studies, prompting this practical evaluation.

Method: Three algorithms (greedy heuristic, hypergraph-based approximation, genetic algorithm) are implemented and tested on synthetic graph datasets.

Result: For small graphs, results align with theoretical bounds; for larger graphs, divergence occurs. The genetic algorithm performs best on known optimal graphs.

Conclusion: Empirical analysis reveals limitations in theoretical bounds for larger graphs, with the genetic algorithm emerging as the most effective heuristic.

Abstract: The NP-complete mutual-visibility (MV) problem currently lacks empirical
analysis on its practical behaviour despite theoretical studies. This paper
addresses this gap by implementing and evaluating three distinct algorithms - a
direct greedy heuristic, a hypergraph-based approximation, and a genetic
algorithm - on diverse synthetic graph datasets, including those with
analytically known $\mu(G)$ values and general graph models. Our results
demonstrate that for smaller graphs, the algorithms consistently achieve MV set
sizes aligning with theoretical bounds. However, for larger instances, achieved
solution sizes notably diverge from theoretical limits; this, combined with the
absence of tight bounds, complicates absolute quality assessment. Nevertheless,
validation on known optimal graphs showed the Genetic Algorithm and other
heuristics empirically performing best among tested methods.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [272] [Automated Classification of Volcanic Earthquakes Using Transformer Encoders: Insights into Data Quality and Model Interpretability](https://arxiv.org/abs/2507.01260)
*Y. Suzuki,Y. Yukutake,T. Ohminato,M. Yamasaki,Ahyi Kim*

Main category: physics.geo-ph

TL;DR: A transformer-based deep learning model was developed for classifying volcanic earthquakes, outperforming CNN methods and improving interpretability through attention weight analysis. Data quality and diversity were key factors in performance.


<details>
  <summary>Details</summary>
Motivation: Traditional earthquake classification relies on subjective human judgment, which is time-consuming. The study aims to provide an objective, efficient alternative using deep learning.

Method: A transformer encoder-based deep learning model was developed and tested on Mount Asama's seismic data. Attention weight visualizations and experiments on data selection/augmentation were conducted.

Result: High F1 scores (0.930-0.980) were achieved, surpassing CNN methods. Attention weights aligned with human expert focus, but data inconsistencies affected accuracy. Proximity to the crater (within 3 km) improved performance.

Conclusion: The transformer model offers efficient, interpretable earthquake classification, with potential for transfer learning to other volcanic regions, aiding hazard assessment and disaster mitigation.

Abstract: Precisely classifying earthquake types is crucial for elucidating the
relationship between volcanic earthquakes and volcanic activity. However,
traditional methods rely on subjective human judgment, which requires
considerable time and effort. To address this issue, we developed a deep
learning model using a transformer encoder for a more objective and efficient
classification. Tested on Mount Asama's diverse seismic activity, our model
achieved high F1 scores (0.930 for volcano tectonic, 0.931 for low-frequency
earthquakes, and 0.980 for noise), superior to a conventional CNN-based method.
To enhance interpretability, attention weight visualizations were analyzed,
revealing that the model focuses on key waveform features similarly to human
experts. However, inconsistencies in training data, such as ambiguously labeled
B-type events with S-waves, were found to influence classification accuracy and
attention weight distributions. Experiments addressing data selection and
augmentation demonstrated the importance of balancing data quality and
diversity. In addition, stations within 3 km of the crater played an important
role in improving model performance and interpretability. These findings
highlight the potential of Transformer-based models for automated volcanic
earthquake classification, particularly in improving efficiency and
interpretability. By addressing challenges such as data imbalance and
subjective labeling, our approach provides a robust framework for understanding
seismic activity at Mount Asama. Moreover, this framework offers opportunities
for transfer learning to other volcanic regions, paving the way for enhanced
volcanic hazard assessments and disaster mitigation strategies.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [273] [Systemic Constraints of Undecidability](https://arxiv.org/abs/2507.01036)
*Seth Bulin*

Main category: cs.FL

TL;DR: The paper reframes incomputability as a structural property of systems, proving that subsystems inherit undecidability from their parent systems, challenging the idea of circumventing computational limits.


<details>
  <summary>Details</summary>
Motivation: To generalize incomputability beyond specific functions, positioning it as a pervasive constraint in systems, and to challenge the notion of overcoming computational limits through design.

Method: Defines causal embedding and proves a closure principle showing subsystems inherit undecidability from parent systems.

Result: Undecidability is shown as a structural constraint, pervasive in prediction, modeling, and epistemic access.

Conclusion: The work extends classical incomputability results, offering a new perspective on computability's topology and its impact on scientific knowledge boundaries.

Abstract: This paper presents a theory of systemic undecidability, reframing
incomputability as a structural property of systems rather than a localized
feature of specific functions or problems. We define a notion of causal
embedding and prove a closure principle: any subsystem that participates
functionally in the computation of an undecidable system inherits its
undecidability. This result positions undecidability as a pervasive constraint
on prediction, modeling, and epistemic access in both natural and artificial
systems. Our framework disarms oracle mimicry and challenges the view that
computational limits can be circumvented through architectural innovation. By
generalizing classical results into a dynamic systems context, this work
augments the logical trajectory of G\"odel, Turing, and Chaitin, offering a new
perspective of the topology of computability and its interrelation to the
boundaries of scientific knowledge.

</details>


<div id='astro-ph.EP'></div>

# astro-ph.EP [[Back]](#toc)

### [274] [Meteoroid stream identification with HDBSCAN unsupervised clustering algorithm](https://arxiv.org/abs/2507.01501)
*Eloy Peña-Asensio,Fabio Ferrari*

Main category: astro-ph.EP

TL;DR: The study evaluates HDBSCAN for unsupervised meteoroid stream identification, comparing it with CAMS. HDBSCAN shows robust performance, outperforming CAMS in statistical coherence.


<details>
  <summary>Details</summary>
Motivation: Accurate identification of meteoroid streams is crucial for understanding their origins, but overlapping clusters and noise complicate classification, especially for missions like ESA's LUMIO.

Method: HDBSCAN is applied to the CAMS Meteoroid Orbit Database using three feature vectors (LUTAB, ORBIT, GEO). Performance is assessed via Silhouette score, NMI, and F1 score, with PCA for support.

Result: HDBSCAN confirms 39 streams with GEO (21 strongly match CAMS) and 30 with ORBIT (13 high matches). The eom method performs best.

Conclusion: HDBSCAN is a mathematically consistent alternative for meteoroid stream identification, though further validation is needed for physical validity.

Abstract: Accurate identification of meteoroid streams is central to understanding
their origins and evolution. However, overlapping clusters and background noise
hinder classification, an issue amplified for missions such as ESA's LUMIO that
rely on meteor shower observations to infer lunar meteoroid impact parameters.
This study evaluates the performance of the Hierarchical Density-Based Spatial
Clustering of Applications with Noise (HDBSCAN) algorithm for unsupervised
meteoroid stream identification, comparing its outcomes with the established
Cameras for All-Sky Meteor Surveillance (CAMS) look-up table method. We analyze
the CAMS Meteoroid Orbit Database v3.0 using three feature vectors: LUTAB (CAMS
geocentric parameters), ORBIT (heliocentric orbital elements), and GEO (adapted
geocentric parameters). HDBSCAN is applied with varying minimum cluster sizes
and two cluster selection methods (eom and leaf). To align HDBSCAN clusters
with CAMS classifications, the Hungarian algorithm determines the optimal
mapping. Clustering performance is assessed via the Silhouette score,
Normalized Mutual Information, and F1 score, with Principal Component Analysis
further supporting the analysis. With the GEO vector, HDBSCAN confirms 39
meteoroid streams, 21 strongly aligning with CAMS. The ORBIT vector identifies
30 streams, 13 with high matching scores. Less active showers pose
identification challenges. The eom method consistently yields superior
performance and agreement with CAMS. Although HDBSCAN requires careful
selection of the minimum cluster size, it delivers robust, internally
consistent clusters and outperforms the look-up table method in statistical
coherence. These results underscore HDBSCAN's potential as a mathematically
consistent alternative for meteoroid stream identification, although further
validation is needed to assess physical validity.

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [275] [STEM Diffraction Pattern Analysis with Deep Learning Networks](https://arxiv.org/abs/2507.01889)
*Sebastian Wissel,Jonas Scheunert,Aaron Dextre,Shamail Ahmed,Andreas Bayer,Kerstin Volz,Bai-Xiang Xu*

Main category: cond-mat.dis-nn

TL;DR: A machine learning approach using deep learning architectures (CNNs, DenseNets, Swin Transformers) is proposed for automated grain orientation mapping from STEM diffraction patterns, outperforming traditional methods in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Accurate grain orientation mapping is crucial for optimizing polycrystalline materials like LiNiO2 in lithium-ion batteries, but traditional methods are slow and noise-sensitive.

Method: Three deep learning models (CNNs, DenseNets, Swin Transformers) are evaluated for predicting Euler angles from STEM diffraction patterns, using a dataset labeled via template matching.

Result: DenseNets and Swin Transformers outperform CNNs, with Swin Transformers achieving the highest scores and consistent microstructural predictions.

Conclusion: Attention-based architectures like Swin Transformers show promise for high-throughput microstructural characterization, enabling robust analysis of diffraction data.

Abstract: Accurate grain orientation mapping is essential for understanding and
optimizing the performance of polycrystalline materials, particularly in
energy-related applications. Lithium nickel oxide (LiNiO$_{2}$) is a promising
cathode material for next-generation lithium-ion batteries, and its
electrochemical behaviour is closely linked to microstructural features such as
grain size and crystallographic orientations. Traditional orientation mapping
methods--such as manual indexing, template matching (TM), or Hough
transform-based techniques--are often slow and noise-sensitive when handling
complex or overlapping patterns, creating a bottleneck in large-scale
microstructural analysis. This work presents a machine learning-based approach
for predicting Euler angles directly from scanning transmission electron
microscopy (STEM) diffraction patterns (DPs). This enables the automated
generation of high-resolution crystal orientation maps, facilitating the
analysis of internal microstructures at the nanoscale. Three deep learning
architectures--convolutional neural networks (CNNs), Dense Convolutional
Networks (DenseNets), and Shifted Windows (Swin) Transformers--are evaluated,
using an experimentally acquired dataset labelled via a commercial TM
algorithm. While the CNN model serves as a baseline, both DenseNets and Swin
Transformers demonstrate superior performance, with the Swin Transformer
achieving the highest evaluation scores and the most consistent microstructural
predictions. The resulting crystal maps exhibit clear grain boundary
delineation and coherent intra-grain orientation distributions, underscoring
the potential of attention-based architectures for analyzing diffraction-based
image data. These findings highlight the promise of combining advanced machine
learning models with STEM data for robust, high-throughput microstructural
characterization.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [276] [Advancing Magnetic Materials Discovery -- A structure-based machine learning approach for magnetic ordering and magnetic moment prediction](https://arxiv.org/abs/2507.01913)
*Apoorv Verma,Junaid Jami,Amrita Bhattacharya*

Main category: cond-mat.mtrl-sci

TL;DR: A refined descriptor improves prediction of magnetic properties (ordering and moment per atom) using structural data, achieving high accuracy and generalization across diverse materials.


<details>
  <summary>Details</summary>
Motivation: Accurate prediction of magnetic behavior is crucial for designing next-generation magnetic materials, but existing models are limited in scope and accuracy.

Method: The study uses an enriched elemental vector representation and advanced feature engineering (nonlinear terms, reduced sparsity) with a LightGBM-based model on a diverse dataset of 5741 compounds.

Result: The model achieves 82.4% accuracy in magnetic ordering classification and a 0.93 correlation coefficient for magnetic moment prediction, outperforming prior descriptors.

Conclusion: The framework provides a robust, efficient tool for high-throughput screening of magnetic materials, addressing limitations of previous studies.

Abstract: Accurately predicting magnetic behavior across diverse materials systems
remains a longstanding challenge due to the complex interplay of structural and
electronic factors and is pivotal for the accelerated discovery and design of
next-generation magnetic materials. In this work, a refined descriptor is
proposed that significantly improves the prediction of two critical magnetic
properties -- magnetic ordering (Ferromagnetic vs. Ferrimagnetic) and magnetic
moment per atom -- using only the structural information of materials. Unlike
previous models limited to Mn-based or lanthanide-transition metal compounds,
the present approach generalizes across a diverse dataset of 5741 stable,
binary and ternary, ferromagnetic and ferrimagnetic compounds sourced from the
Materials Project. Leveraging an enriched elemental vector representation and
advanced feature engineering, including nonlinear terms and reduced matrix
sparsity, the LightGBM-based model achieves an accuracy of 82.4% for magnetic
ordering classification and balanced recall across FM and FiM classes,
addressing a key limitation in prior studies. The model predicts magnetic
moment per atom with a correlation coefficient of 0.93, surpassing the Hund's
matrix and orbital field matrix descriptors. Additionally, it accurately
estimates formation energy per atom, enabling assessment of both magnetic
behavior and material stability. This generalized and computationally efficient
framework offers a robust tool for high-throughput screening of magnetic
materials with tailored properties.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [277] [Asymptotic convexity of wide and shallow neural networks](https://arxiv.org/abs/2507.01044)
*Vivek Borkar,Parthe Pandit*

Main category: stat.ML

TL;DR: The paper shows that the epigraph of a shallow, wide neural network's input-output map approximates a convex function, suggesting this explains their good performance.


<details>
  <summary>Details</summary>
Motivation: To understand why shallow, wide neural networks perform well by analyzing their input-output map's epigraph.

Method: Analyzing the epigraph of the input-output map of a simple shallow and wide neural network model.

Result: The epigraph approximates that of a convex function, providing insight into the network's performance.

Conclusion: The convex-like behavior of the epigraph may explain the observed good performance of such networks.

Abstract: For a simple model of shallow and wide neural networks, we show that the
epigraph of its input-output map as a function of the network parameters
approximates epigraph of a. convex function in a precise sense. This leads to a
plausible explanation of their observed good performance.

</details>


### [278] [Parsimonious Gaussian mixture models with piecewise-constant eigenvalue profiles](https://arxiv.org/abs/2507.01542)
*Tom Szwagier,Pierre-Alexandre Mattei,Charles Bouveyron,Xavier Pennec*

Main category: stat.ML

TL;DR: A new family of parsimonious Gaussian mixture models (GMMs) with piecewise-constant covariance eigenvalue profiles is introduced, bridging the gap between full and spherical GMMs. The models extend low-rank approaches like MPPCA and offer improved flexibility and parsimony.


<details>
  <summary>Details</summary>
Motivation: Full GMMs are overparameterized in high dimensions, while spherical GMMs lack flexibility for anisotropic distributions. This work aims to balance these extremes with a more adaptable yet parsimonious model.

Method: The paper introduces a new GMM family with piecewise-constant covariance eigenvalue profiles, enabling any eigenvalue multiplicity sequence. It derives an EM algorithm for learning mixture parameters and proposes a componentwise penalized EM for joint parameter and hyperparameter learning.

Result: The proposed models achieve superior likelihood-parsimony tradeoffs in experiments, including density fitting, clustering, and single-image denoising.

Conclusion: The new GMM family effectively balances flexibility and parsimony, outperforming existing models in various unsupervised tasks.

Abstract: Gaussian mixture models (GMMs) are ubiquitous in statistical learning,
particularly for unsupervised problems. While full GMMs suffer from the
overparameterization of their covariance matrices in high-dimensional spaces,
spherical GMMs (with isotropic covariance matrices) certainly lack flexibility
to fit certain anisotropic distributions. Connecting these two extremes, we
introduce a new family of parsimonious GMMs with piecewise-constant covariance
eigenvalue profiles. These extend several low-rank models like the celebrated
mixtures of probabilistic principal component analyzers (MPPCA), by enabling
any possible sequence of eigenvalue multiplicities. If the latter are
prespecified, then we can naturally derive an expectation-maximization (EM)
algorithm to learn the mixture parameters. Otherwise, to address the
notoriously-challenging issue of jointly learning the mixture parameters and
hyperparameters, we propose a componentwise penalized EM algorithm, whose
monotonicity is proven. We show the superior likelihood-parsimony tradeoffs
achieved by our models on a variety of unsupervised experiments: density
fitting, clustering and single-image denoising.

</details>


### [279] [When Less Is More: Binary Feedback Can Outperform Ordinal Comparisons in Ranking Recovery](https://arxiv.org/abs/2507.01613)
*Shirong Xu,Jingnan Zhang,Junhui Wang*

Main category: stat.ML

TL;DR: The paper challenges the belief that ordinal paired comparisons are richer than binary ones, showing binarization can improve ranking accuracy with faster convergence and better SNR.


<details>
  <summary>Details</summary>
Motivation: To investigate whether ordinal paired comparisons truly offer more information than binary comparisons for ranking tasks.

Method: Proposes a parametric framework for modeling ordinal paired comparisons, including a link function for preference differences and a pattern function for ordinal response distribution.

Result: Binarizing ordinal data improves ranking accuracy, with binary comparisons showing faster exponential convergence and better SNR.

Conclusion: Binary comparisons can outperform ordinal ones in ranking tasks, especially with optimal pattern functions.

Abstract: Paired comparison data, where users evaluate items in pairs, play a central
role in ranking and preference learning tasks. While ordinal comparison data
intuitively offer richer information than binary comparisons, this paper
challenges that conventional wisdom. We propose a general parametric framework
for modeling ordinal paired comparisons without ties. The model adopts a
generalized additive structure, featuring a link function that quantifies the
preference difference between two items and a pattern function that governs the
distribution over ordinal response levels. This framework encompasses classical
binary comparison models as special cases, by treating binary responses as
binarized versions of ordinal data. Within this framework, we show that
binarizing ordinal data can significantly improve the accuracy of ranking
recovery. Specifically, we prove that under the counting algorithm, the ranking
error associated with binary comparisons exhibits a faster exponential
convergence rate than that of ordinal data. Furthermore, we characterize a
substantial performance gap between binary and ordinal data in terms of a
signal-to-noise ratio (SNR) determined by the pattern function. We identify the
pattern function that minimizes the SNR and maximizes the benefit of
binarization. Extensive simulations and a real application on the MovieLens
dataset further corroborate our theoretical findings.

</details>


### [280] [A generative modeling / Physics-Informed Neural Network approach to random differential equations](https://arxiv.org/abs/2507.01687)
*Georgios Arampatzis,Stylianos Katsarakis,Charalambos Makridakis*

Main category: stat.ML

TL;DR: The paper integrates probabilistic frameworks with Physics-Informed Neural Networks (PINNs) to model uncertainty in complex systems, enhancing uncertainty control without sacrificing predictive accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of uncertainty quantification in complex systems by advancing PINNs with probabilistic methods.

Method: Combines generative modeling techniques with PINNs to systematically model and control uncertainty in forward problems.

Result: Demonstrated effectiveness through applications to random differential equations and random PDEs.

Conclusion: The proposed method successfully integrates SciML and UQ, improving uncertainty representation in complex systems.

Abstract: The integration of Scientific Machine Learning (SciML) techniques with
uncertainty quantification (UQ) represents a rapidly evolving frontier in
computational science. This work advances Physics-Informed Neural Networks
(PINNs) by incorporating probabilistic frameworks to effectively model
uncertainty in complex systems. Our approach enhances the representation of
uncertainty in forward problems by combining generative modeling techniques
with PINNs. This integration enables in a systematic fashion uncertainty
control while maintaining the predictive accuracy of the model. We demonstrate
the utility of this method through applications to random differential
equations and random partial differential equations (PDEs).

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [281] [Search-Based Robot Motion Planning With Distance-Based Adaptive Motion Primitives](https://arxiv.org/abs/2507.01198)
*Benjamin Kraljusic,Zlatan Ajanovic,Nermin Covic,Bakir Lacevic*

Main category: cs.RO

TL;DR: A motion planning algorithm for robotic manipulators combines sampling and search-based methods, using adaptive motion primitives (burs) for efficient exploration.


<details>
  <summary>Details</summary>
Motivation: To improve efficiency in robotic motion planning by leveraging adaptive motion primitives for better exploration of free configuration space.

Method: Combines sampling-based and search-based planning, using burs (adaptive motion primitives) in the SMPL library for graph search.

Result: Outperforms fixed-primitive planning in complex scenarios, especially for high DoF manipulators, with comparable performance in simpler cases.

Conclusion: The bur-based approach is effective for efficient motion planning, particularly in complex environments.

Abstract: This work proposes a motion planning algorithm for robotic manipulators that
combines sampling-based and search-based planning methods. The core
contribution of the proposed approach is the usage of burs of free
configuration space (C-space) as adaptive motion primitives within the graph
search algorithm. Due to their feature to adaptively expand in free C-space,
burs enable more efficient exploration of the configuration space compared to
fixed-sized motion primitives, significantly reducing the time to find a valid
path and the number of required expansions. The algorithm is implemented within
the existing SMPL (Search-Based Motion Planning Library) library and evaluated
through a series of different scenarios involving manipulators with varying
number of degrees-of-freedom (DoF) and environment complexity. Results
demonstrate that the bur-based approach outperforms fixed-primitive planning in
complex scenarios, particularly for high DoF manipulators, while achieving
comparable performance in simpler scenarios.

</details>


### [282] [LLM-based Realistic Safety-Critical Driving Video Generation](https://arxiv.org/abs/2507.01264)
*Yongjie Fu,Ruijian Zha,Pei Tian,Xuan Di*

Main category: cs.RO

TL;DR: A framework using LLMs for few-shot code generation to synthesize diverse, safety-critical driving scenarios in CARLA, enhanced by a video generation pipeline for realism.


<details>
  <summary>Details</summary>
Motivation: To address the need for diverse and safety-critical driving scenarios for evaluating autonomous driving systems.

Method: Leverages LLMs for few-shot code generation to create scenario scripts in CARLA, integrating a video generation pipeline (Cosmos-Transfer1 with ControlNet) for realistic visuals.

Result: Generates realistic, diverse, and safety-critical scenarios, including rare edge cases like occluded pedestrian crossings.

Conclusion: The method is effective for simulation-based testing of autonomous vehicles, offering controllability and realism.

Abstract: Designing diverse and safety-critical driving scenarios is essential for
evaluating autonomous driving systems. In this paper, we propose a novel
framework that leverages Large Language Models (LLMs) for few-shot code
generation to automatically synthesize driving scenarios within the CARLA
simulator, which has flexibility in scenario scripting, efficient code-based
control of traffic participants, and enforcement of realistic physical
dynamics. Given a few example prompts and code samples, the LLM generates
safety-critical scenario scripts that specify the behavior and placement of
traffic participants, with a particular focus on collision events. To bridge
the gap between simulation and real-world appearance, we integrate a video
generation pipeline using Cosmos-Transfer1 with ControlNet, which converts
rendered scenes into realistic driving videos. Our approach enables
controllable scenario generation and facilitates the creation of rare but
critical edge cases, such as pedestrian crossings under occlusion or sudden
vehicle cut-ins. Experimental results demonstrate the effectiveness of our
method in generating a wide range of realistic, diverse, and safety-critical
scenarios, offering a promising tool for simulation-based testing of autonomous
vehicles.

</details>


### [283] [VLAD: A VLM-Augmented Autonomous Driving Framework with Hierarchical Planning and Interpretable Decision Process](https://arxiv.org/abs/2507.01284)
*Cristian Gariboldi,Hayato Tokida,Ken Kinjo,Yuki Asada,Alexander Carballo*

Main category: cs.RO

TL;DR: VLAD integrates a fine-tuned VLM with VAD for autonomous driving, improving spatial reasoning and reducing collisions by 31.82%.


<details>
  <summary>Details</summary>
Motivation: Leverage open-source VLMs' general knowledge to enhance autonomous driving systems' perception, prediction, and planning.

Method: Fine-tune a VLM using custom QA datasets for spatial reasoning, integrate with VAD for navigational commands, and generate interpretable explanations.

Result: 31.82% reduction in collision rates on the nuScenes dataset compared to baselines.

Conclusion: VLAD sets a new benchmark for VLM-augmented autonomous driving, offering improved performance and transparency.

Abstract: Recent advancements in open-source Visual Language Models (VLMs) such as
LLaVA, Qwen-VL, and Llama have catalyzed extensive research on their
integration with diverse systems. The internet-scale general knowledge
encapsulated within these models presents significant opportunities for
enhancing autonomous driving perception, prediction, and planning capabilities.
In this paper we propose VLAD, a vision-language autonomous driving model,
which integrates a fine-tuned VLM with VAD, a state-of-the-art end-to-end
system. We implement a specialized fine-tuning approach using custom
question-answer datasets designed specifically to improve the spatial reasoning
capabilities of the model. The enhanced VLM generates high-level navigational
commands that VAD subsequently processes to guide vehicle operation.
Additionally, our system produces interpretable natural language explanations
of driving decisions, thereby increasing transparency and trustworthiness of
the traditionally black-box end-to-end architecture. Comprehensive evaluation
on the real-world nuScenes dataset demonstrates that our integrated system
reduces average collision rates by 31.82% compared to baseline methodologies,
establishing a new benchmark for VLM-augmented autonomous driving systems.

</details>


### [284] [Quantum-Assisted Automatic Path-Planning for Robotic Quality Inspection in Industry 4.0](https://arxiv.org/abs/2507.01462)
*Eneko Osaba,Estibaliz Garrote,Pablo Miranda-Rodriguez,Alessia Ciacco,Itziar Cabanes,Aitziber Mancisidor*

Main category: cs.RO

TL;DR: Hybrid quantum-classical algorithms optimize robotic inspection trajectories from CAD models, outperforming classical methods in speed while maintaining solution quality.


<details>
  <summary>Details</summary>
Motivation: To improve automation in Industry 4.0 by leveraging quantum computing for efficient robotic inspection trajectory optimization.

Method: Modeled as a 3D Traveling Salesman Problem with incomplete graphs and open-route constraints, comparing D-Wave solvers to classical methods (GUROBI, Google OR-Tools).

Result: Quantum approaches achieved competitive solution quality with significantly faster computation times in five real-world cases.

Conclusion: Hybrid quantum-classical algorithms show promise for industrial automation, offering speed advantages without compromising performance.

Abstract: This work explores the application of hybrid quantum-classical algorithms to
optimize robotic inspection trajectories derived from Computer-Aided Design
(CAD) models in industrial settings. By modeling the task as a 3D variant of
the Traveling Salesman Problem, incorporating incomplete graphs and open-route
constraints, this study evaluates the performance of two D-Wave-based solvers
against classical methods such as GUROBI and Google OR-Tools. Results across
five real-world cases demonstrate competitive solution quality with
significantly reduced computation times, highlighting the potential of quantum
approaches in automation under Industry 4.0.

</details>


### [285] [BioMARS: A Multi-Agent Robotic System for Autonomous Biological Experiments](https://arxiv.org/abs/2507.01485)
*Yibo Qiu,Zan Huang,Zhiyu Wang,Handi Liu,Yiling Qiao,Yifeng Hu,Shu'ang Sun,Hangke Peng,Ronald X Xu,Mingzhai Sun*

Main category: cs.RO

TL;DR: BioMARS integrates LLMs, VLMs, and robotics to autonomously design and execute biological experiments, outperforming manual methods in tasks like cell passaging and culture.


<details>
  <summary>Details</summary>
Motivation: Current AI applications in biology are limited by rigid protocols and poor adaptability. BioMARS aims to overcome these challenges with autonomous experimentation.

Method: BioMARS uses a hierarchical multi-agent system: Biologist Agent (protocol synthesis), Technician Agent (execution planning), and Inspector Agent (error detection).

Result: The system matches or exceeds manual performance in cell culture tasks and outperforms conventional methods in optimizing cell differentiation.

Conclusion: BioMARS demonstrates the feasibility of AI-driven lab automation and highlights the role of language-based reasoning in advancing biological research.

Abstract: Large language models (LLMs) and vision-language models (VLMs) have the
potential to transform biological research by enabling autonomous
experimentation. Yet, their application remains constrained by rigid protocol
design, limited adaptability to dynamic lab conditions, inadequate error
handling, and high operational complexity. Here we introduce BioMARS
(Biological Multi-Agent Robotic System), an intelligent platform that
integrates LLMs, VLMs, and modular robotics to autonomously design, plan, and
execute biological experiments. BioMARS uses a hierarchical architecture: the
Biologist Agent synthesizes protocols via retrieval-augmented generation; the
Technician Agent translates them into executable robotic pseudo-code; and the
Inspector Agent ensures procedural integrity through multimodal perception and
anomaly detection. The system autonomously conducts cell passaging and culture
tasks, matching or exceeding manual performance in viability, consistency, and
morphological integrity. It also supports context-aware optimization,
outperforming conventional strategies in differentiating retinal pigment
epithelial cells. A web interface enables real-time human-AI collaboration,
while a modular backend allows scalable integration with laboratory hardware.
These results highlight the feasibility of generalizable, AI-driven laboratory
automation and the transformative role of language-based reasoning in
biological research.

</details>


### [286] [LANet: A Lane Boundaries-Aware Approach For Robust Trajectory Prediction](https://arxiv.org/abs/2507.01308)
*Muhammad Atta ur Rahman,Dooseop Choi,KyoungWook Min*

Main category: cs.RO

TL;DR: The paper proposes an enhanced motion forecasting model for autonomous driving that uses multiple vector map elements (like lane boundaries and road edges) instead of just lane centerlines, improving accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Current motion prediction models rely heavily on lane centerlines, limiting their ability to capture full road environments and traffic rules. This work aims to address this limitation.

Method: The model integrates multiple vector map elements and uses a feature fusion strategy to merge information. A pruning mechanism filters relevant map connections to maintain computational efficiency.

Result: The method outperforms lane centerline-based models, achieving improved performance on the Argoverse 2 dataset while maintaining computational efficiency.

Conclusion: The proposed model provides a richer representation of driving environments and advances motion forecasting for autonomous vehicles.

Abstract: Accurate motion forecasting is critical for safe and efficient autonomous
driving, enabling vehicles to predict future trajectories and make informed
decisions in complex traffic scenarios. Most of the current designs of motion
prediction models are based on the major representation of lane centerlines,
which limits their capability to capture critical road environments and traffic
rules and constraints. In this work, we propose an enhanced motion forecasting
model informed by multiple vector map elements, including lane boundaries and
road edges, that facilitates a richer and more complete representation of
driving environments. An effective feature fusion strategy is developed to
merge information in different vector map components, where the model learns
holistic information on road structures and their interactions with agents.
Since encoding more information about the road environment increases memory
usage and is computationally expensive, we developed an effective pruning
mechanism that filters the most relevant map connections to the target agent,
ensuring computational efficiency while maintaining essential spatial and
semantic relationships for accurate trajectory prediction. Overcoming the
limitations of lane centerline-based models, our method provides a more
informative and efficient representation of the driving environment and
advances the state of the art for autonomous vehicle motion forecasting. We
verify our approach with extensive experiments on the Argoverse 2 motion
forecasting dataset, where our method maintains competitiveness on AV2 while
achieving improved performance.
  Index Terms-Autonomous driving, trajectory prediction, vector map elements,
road topology, connection pruning, Argoverse 2.

</details>


### [287] [A Review on Sound Source Localization in Robotics: Focusing on Deep Learning Methods](https://arxiv.org/abs/2507.01143)
*Reza Jalayer,Masoud Jalayer,Amirali Baniasadi*

Main category: cs.RO

TL;DR: A robotics-focused review of sound source localization (SSL) highlighting classical methods and modern deep learning advancements, addressing gaps in existing surveys.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive synthesis of SSL tailored for robotics, emphasizing deep learning progress and addressing robotic constraints.

Method: Reviews classical SSL methods (TDOA, beamforming, SRP, subspace analysis) and modern ML/DL approaches (NNs, CNNs, CRNNs, attention-based architectures), including data and training strategies.

Result: Categorizes studies by robot types and applications, identifies challenges (robustness, multiplicity, implementation), and suggests future directions for DL-based SSL.

Conclusion: Proposes actionable steps toward robust, adaptable, efficient, and explainable DL-based SSL for next-gen robots.

Abstract: Sound source localization (SSL) adds a spatial dimension to auditory
perception, allowing a system to pinpoint the origin of speech, machinery
noise, warning tones, or other acoustic events, capabilities that facilitate
robot navigation, human-machine dialogue, and condition monitoring. While
existing surveys provide valuable historical context, they typically address
general audio applications and do not fully account for robotic constraints or
the latest advancements in deep learning. This review addresses these gaps by
offering a robotics-focused synthesis, emphasizing recent progress in deep
learning methodologies. We start by reviewing classical methods such as Time
Difference of Arrival (TDOA), beamforming, Steered-Response Power (SRP), and
subspace analysis. Subsequently, we delve into modern machine learning (ML) and
deep learning (DL) approaches, discussing traditional ML and neural networks
(NNs), convolutional neural networks (CNNs), convolutional recurrent neural
networks (CRNNs), and emerging attention-based architectures. The data and
training strategy that are the two cornerstones of DL-based SSL are explored.
Studies are further categorized by robot types and application domains to
facilitate researchers in identifying relevant work for their specific
contexts. Finally, we highlight the current challenges in SSL works in general,
regarding environmental robustness, sound source multiplicity, and specific
implementation constraints in robotics, as well as data and learning strategies
in DL-based SSL. Also, we sketch promising directions to offer an actionable
roadmap toward robust, adaptable, efficient, and explainable DL-based SSL for
next-generation robots.

</details>


### [288] [Jump-Start Reinforcement Learning with Self-Evolving Priors for Extreme Monopedal Locomotion](https://arxiv.org/abs/2507.01243)
*Ziang Zheng,Guojian Zhan,Shiqi Liu,Yao Lyu,Tao Zhang,Shengbo Eben Li*

Main category: cs.RO

TL;DR: JumpER is an RL framework that uses self-evolving priors and a staged curriculum to enable quadruped robots to perform monopedal hopping on extreme terrains, overcoming challenges like underactuation and unreliable rewards.


<details>
  <summary>Details</summary>
Motivation: Directly training RL policies for monopedal hopping on extreme terrains is difficult due to unstable early-stage interactions and unreliable rewards.

Method: JumpER structures policy learning into stages of increasing complexity, using self-evolving priors and a three-stage curriculum to refine guidance.

Result: The framework achieves robust monopedal hopping on unpredictable terrains, handling gaps up to 60 cm, irregular stairs, and varying stepping stones.

Conclusion: JumpER offers a scalable solution for locomotion tasks under extreme underactuation and terrain challenges.

Abstract: Reinforcement learning (RL) has shown great potential in enabling quadruped
robots to perform agile locomotion. However, directly training policies to
simultaneously handle dual extreme challenges, i.e., extreme underactuation and
extreme terrains, as in monopedal hopping tasks, remains highly challenging due
to unstable early-stage interactions and unreliable reward feedback. To address
this, we propose JumpER (jump-start reinforcement learning via self-evolving
priors), an RL training framework that structures policy learning into multiple
stages of increasing complexity. By dynamically generating self-evolving priors
through iterative bootstrapping of previously learned policies, JumpER
progressively refines and enhances guidance, thereby stabilizing exploration
and policy optimization without relying on external expert priors or
handcrafted reward shaping. Specifically, when integrated with a structured
three-stage curriculum that incrementally evolves action modality, observation
space, and task objective, JumpER enables quadruped robots to achieve robust
monopedal hopping on unpredictable terrains for the first time. Remarkably, the
resulting policy effectively handles challenging scenarios that traditional
methods struggle to conquer, including wide gaps up to 60 cm, irregularly
spaced stairs, and stepping stones with distances varying from 15 cm to 35 cm.
JumpER thus provides a principled and scalable approach for addressing
locomotion tasks under the dual challenges of extreme underactuation and
extreme terrains.

</details>


### [289] [AC-DiT: Adaptive Coordination Diffusion Transformer for Mobile Manipulation](https://arxiv.org/abs/2507.01961)
*Sixiang Chen,Jiaming Liu,Siyuan Qian,Han Jiang,Lily Li,Renrui Zhang,Zhuoyang Liu,Chenyang Gu,Chengkai Hou,Pengwei Wang,Zhongyuan Wang,Shanghang Zhang*

Main category: cs.RO

TL;DR: The paper introduces AC-DiT, a method to improve coordination between mobile bases and manipulators in language-conditioned robotic control by addressing error accumulation and multimodal perception needs.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with coordinating mobile bases and manipulators due to error accumulation and uniform visual observation modalities, limiting performance in household tasks.

Method: AC-DiT uses a mobility-to-body conditioning mechanism for whole-body control and a perception-aware multimodal conditioning strategy to dynamically adjust fusion weights between 2D and 3D inputs.

Result: AC-DiT is validated through experiments on simulated and real-world tasks, demonstrating improved coordination and adaptability.

Conclusion: AC-DiT effectively addresses coordination and perception challenges in mobile manipulation, enhancing performance in household tasks.

Abstract: Recently, mobile manipulation has attracted increasing attention for enabling
language-conditioned robotic control in household tasks. However, existing
methods still face challenges in coordinating mobile base and manipulator,
primarily due to two limitations. On the one hand, they fail to explicitly
model the influence of the mobile base on manipulator control, which easily
leads to error accumulation under high degrees of freedom. On the other hand,
they treat the entire mobile manipulation process with the same visual
observation modality (e.g., either all 2D or all 3D), overlooking the distinct
multimodal perception requirements at different stages during mobile
manipulation. To address this, we propose the Adaptive Coordination Diffusion
Transformer (AC-DiT), which enhances mobile base and manipulator coordination
for end-to-end mobile manipulation. First, since the motion of the mobile base
directly influences the manipulator's actions, we introduce a mobility-to-body
conditioning mechanism that guides the model to first extract base motion
representations, which are then used as context prior for predicting whole-body
actions. This enables whole-body control that accounts for the potential impact
of the mobile base's motion. Second, to meet the perception requirements at
different stages of mobile manipulation, we design a perception-aware
multimodal conditioning strategy that dynamically adjusts the fusion weights
between various 2D visual images and 3D point clouds, yielding visual features
tailored to the current perceptual needs. This allows the model to, for
example, adaptively rely more on 2D inputs when semantic information is crucial
for action prediction, while placing greater emphasis on 3D geometric
information when precise spatial understanding is required. We validate AC-DiT
through extensive experiments on both simulated and real-world mobile
manipulation tasks.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [290] [Can AI be Consentful?](https://arxiv.org/abs/2507.01051)
*Giada Pistilli,Bruna Trevelin*

Main category: cs.CY

TL;DR: The paper highlights the inadequacy of traditional consent frameworks in addressing AI-generated content, identifying a 'consent gap' due to scope, temporality, and autonomy challenges.


<details>
  <summary>Details</summary>
Motivation: To explore how conventional consent fails in AI contexts, especially regarding data-derived outputs and their broader implications.

Method: Legal and ethical analysis of consent limitations in AI systems, focusing on scope, temporality, and autonomy.

Result: Identifies a 'consent gap' and argues current frameworks fall short in protecting autonomy and rights in AI-generated content.

Conclusion: Calls for evolved ethical and legal approaches to consent to better address AI's unique challenges.

Abstract: The evolution of generative AI systems exposes the challenges of traditional
legal and ethical frameworks built around consent. This chapter examines how
the conventional notion of consent, while fundamental to data protection and
privacy rights, proves insufficient in addressing the implications of
AI-generated content derived from personal data. Through legal and ethical
analysis, we show that while individuals can consent to the initial use of
their data for AI training, they cannot meaningfully consent to the numerous
potential outputs their data might enable or the extent to which the output is
used or distributed. We identify three fundamental challenges: the scope
problem, the temporality problem, and the autonomy trap, which collectively
create what we term a ''consent gap'' in AI systems and their surrounding
ecosystem. We argue that current legal frameworks inadequately address these
emerging challenges, particularly regarding individual autonomy, identity
rights, and social responsibility, especially in cases where AI-generated
content creates new forms of personal representation beyond the scope of the
original consent. By examining how these consent limitations intersect with
broader principles of responsible AI (including fairness, transparency,
accountability, and autonomy) we demonstrate the need to evolve ethical and
legal approaches to consent.

</details>


### [291] [Epitome: Pioneering an Experimental Platform for AI-Social Science Integration](https://arxiv.org/abs/2507.01061)
*Jingjing Qu,Kejia Hu,Jun Zhu,Wenhao Li,Teng Wang,Zhiyun Chen,Yulei Ye,Chaochao Lu,Aimin Zhou,Xiangfeng Wang,James Evan*

Main category: cs.CY

TL;DR: Epitome is an open experimental platform integrating LLMs into social science research, enabling complex experiments on human-AI interactions and societal impacts.


<details>
  <summary>Details</summary>
Motivation: To understand the societal impacts of AI and enhance human-AI interactions through interdisciplinary research.

Method: Epitome provides a one-stop solution with seven core modules, embedding social science methodologies into multilevel human-computer interaction environments.

Result: The platform successfully replicated three seminal social science experiments, demonstrating its capability to streamline designs and produce robust results.

Conclusion: Epitome is a powerful tool for advancing interdisciplinary research at the AI-social science intersection, with potential policy-making applications.

Abstract: The integration of Large Language Models (LLMs) into social science
experiments represents a transformative approach to understanding human-AI
interactions and their societal impacts. We introduce Epitome, the world's
first open experimental platform dedicated to the deep integration of
artificial intelligence and social science. Rooted in theoretical foundations
from management, communication studies, sociology, psychology, and ethics,
Epitome focuses on the interactive impacts of AI on individuals, organizations,
and society during its real-world deployment. It constructs a theoretical
support system through cross-disciplinary experiments. The platform offers a
one-stop comprehensive experimental solution spanning "foundation
models-complex application development-user feedback" through seven core
modules, while embedding the classical "control-comparison-comparative causal
logic" of social science experiments into multilevel human-computer interaction
environments, including dialogues, group chats, and multi-agent virtual
scenarios. With its canvas-style, user-friendly interface, Epitome enables
researchers to easily design and run complex experimental scenarios,
facilitating systematic investigations into the social impacts of AI and
exploration of integrated solutions.To demonstrate its capabilities, we
replicated three seminal social science experiments involving LLMs, showcasing
Epitome's potential to streamline complex experimental designs and produce
robust results, suitable for publishing in the top selective journals. Our
findings highlight the platform's utility in enhancing the efficiency and
quality of human-AI interactions, providing valuable insights into the societal
implications of AI technologies. Epitome thus offers a powerful tool for
advancing interdisciplinary research at the intersection of AI and social
science, with potential applications in policy-making, ...

</details>


### [292] [Quantifying Student Success with Generative AI: A Monte Carlo Simulation Informed by Systematic Review](https://arxiv.org/abs/2507.01062)
*Seyma Yaman Kayadibi*

Main category: cs.CY

TL;DR: The paper explores student perceptions of GenAI in higher education using a hybrid method of literature review and simulation modeling, finding usability and usefulness as key predictors of learning success.


<details>
  <summary>Details</summary>
Motivation: To understand how students view and use GenAI in higher education and its impact on learning outcomes.

Method: Combines systematic literature review (19 empirical articles) with simulation-based modeling (Monte Carlo simulation) to analyze student perceptions.

Result: Usability and real-world usefulness of GenAI are stronger predictors of positive learning outcomes than affective or trust factors.

Conclusion: The interdisciplinary approach links thematic findings with predictive modeling, addressing debates on GenAI's role in education.

Abstract: The exponential development of generative artificial intelligence (GenAI)
technologies like ChatGPT has raised increasing curiosity about their use in
higher education, specifically with respect to how students view them, make use
of them, and the implications for learning outcomes. This paper employs a
hybrid methodological approach involving a systematic literature review and
simulation-based modeling to explore student perceptions of GenAI use in the
context of higher education. A total of nineteen empirical articles from 2023
through 2025 were selected from the PRISMA-based search targeting the Scopus
database. Synthesis of emerging patterns from the literature was achieved by
thematic categorization. Six of these had enough quantitative information,
i.e., item-level means and standard deviations, to permit probabilistic
modeling. One dataset, from the resulting subset, was itself selected as a
representative case with which to illustrate inverse-variance weighting by
Monte Carlo simulation, by virtue of its well-designed Likert scale format and
thematic alignment with the use of computing systems by the researcher.
  The simulation provided a composite "Success Score" forecasting the strength
of the relationship between student perceptions and learning achievements.
Findings reveal that attitude factors concerned with usability and real-world
usefulness are significantly better predictors of positive learning achievement
than affective or trust-based factors. Such an interdisciplinary perspective
provides a unique means of linking thematic results with predictive modelling,
resonating with longstanding controversies about the proper use of GenAI tools
within the university.

</details>


### [293] [Penalizing Transparency? How AI Disclosure and Author Demographics Shape Human and AI Judgments About Writing](https://arxiv.org/abs/2507.01418)
*Inyoung Cheong,Alicia Guo,Mina Lee,Zhehui Liao,Kowe Kadoma,Dongyoung Go,Joseph Chee Chang,Peter Henderson,Mor Naaman,Amy X. Zhang*

Main category: cs.CY

TL;DR: The study examines how AI disclosure affects perceptions of writing quality, revealing biases in human and LLM raters, with LLMs showing demographic-based favoritism that disappears when AI use is disclosed.


<details>
  <summary>Details</summary>
Motivation: To understand the uneven burden of AI transparency and its impact on perceptions of writing quality, especially across different author demographics.

Method: A controlled experiment with human (n=1,970) and LLM (n=2,520) raters evaluated a news article with varied AI disclosure statements and author demographics.

Result: Both human and LLM raters penalize disclosed AI use. LLMs favor articles by women or Black authors without disclosure, but this advantage vanishes when AI assistance is revealed.

Conclusion: The study highlights disparities in AI disclosure effects between human and machine evaluations, emphasizing the need for equitable transparency practices.

Abstract: As AI integrates in various types of human writing, calls for transparency
around AI assistance are growing. However, if transparency operates on uneven
ground and certain identity groups bear a heavier cost for being honest, then
the burden of openness becomes asymmetrical. This study investigates how AI
disclosure statement affects perceptions of writing quality, and whether these
effects vary by the author's race and gender. Through a large-scale controlled
experiment, both human raters (n = 1,970) and LLM raters (n = 2,520) evaluated
a single human-written news article while disclosure statements and author
demographics were systematically varied. This approach reflects how both human
and algorithmic decisions now influence access to opportunities (e.g., hiring,
promotion) and social recognition (e.g., content recommendation algorithms). We
find that both human and LLM raters consistently penalize disclosed AI use.
However, only LLM raters exhibit demographic interaction effects: they favor
articles attributed to women or Black authors when no disclosure is present.
But these advantages disappear when AI assistance is revealed. These findings
illuminate the complex relationships between AI disclosure and author identity,
highlighting disparities between machine and human evaluation patterns.

</details>


### [294] [AI and Remote Sensing for Resilient and Sustainable Built Environments: A Review of Current Methods, Open Data and Future Directions](https://arxiv.org/abs/2507.01547)
*Ubada El Joulani,Tatiana Kalganova,Stergios-Aristoteles Mitoulis,Sotirios Argyroudis*

Main category: cs.CY

TL;DR: This review explores AI's role in enhancing damage assessment for transport infrastructure, focusing on bridges, and highlights gaps in using SAR data with AI.


<details>
  <summary>Details</summary>
Motivation: Critical infrastructure faces risks from ageing, climate change, and hybrid threats, necessitating advanced solutions like AI for resilience.

Method: A systematic literature review of AI models and datasets for damage assessment in transport infrastructure, with emphasis on bridges and SAR data integration.

Result: Identifies a research gap in AI applications for bridge damage assessment using SAR data.

Conclusion: The review aims to guide future AI-driven solutions for monitoring and assessing transport infrastructure.

Abstract: Critical infrastructure, such as transport networks, underpins economic
growth by enabling mobility and trade. However, ageing assets, climate change
impacts (e.g., extreme weather, rising sea levels), and hybrid threats ranging
from natural disasters to cyber attacks and conflicts pose growing risks to
their resilience and functionality. This review paper explores how emerging
digital technologies, specifically Artificial Intelligence (AI), can enhance
damage assessment and monitoring of transport infrastructure. A systematic
literature review examines existing AI models and datasets for assessing damage
in roads, bridges, and other critical infrastructure impacted by natural
disasters. Special focus is given to the unique challenges and opportunities
associated with bridge damage detection due to their structural complexity and
critical role in connectivity. The integration of SAR (Synthetic Aperture
Radar) data with AI models is also discussed, with the review revealing a
critical research gap: a scarcity of studies applying AI models to SAR data for
comprehensive bridge damage assessment. Therefore, this review aims to identify
the research gaps and provide foundations for AI-driven solutions for assessing
and monitoring critical transport infrastructures.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [295] [A Systematic Review of Security Vulnerabilities in Smart Home Devices and Mitigation Techniques](https://arxiv.org/abs/2507.01018)
*Mohammed K. Alzaylaee*

Main category: cs.CR

TL;DR: The paper examines cybersecurity risks in IoT-enabled smart homes, identifying vulnerabilities and evaluating solutions like post-quantum encryption, AI-driven anomaly detection, and blockchain authentication. Challenges include computational demands and scalability.


<details>
  <summary>Details</summary>
Motivation: The increasing cybersecurity risks in smart homes due to IoT integration necessitate exploring effective security solutions to protect these ecosystems.

Method: The study categorizes threats into network, device, cloud, and AI-driven vulnerabilities, and evaluates solutions like post-quantum encryption, AI anomaly detection, and blockchain authentication using ANOVA, Chi-square tests, and Monte Carlo simulations.

Result: Post-quantum encryption and AI-driven anomaly detection are effective but resource-intensive. Blockchain authentication enhances resilience but requires infrastructure changes. Scalability remains a challenge.

Conclusion: The research highlights the need for improved cryptographic techniques, AI-enhanced threat detection, and adaptive security models that balance performance, efficiency, and real-time applicability in smart homes.

Abstract: Smart homes that integrate Internet of Things (IoT) devices face increasing
cybersecurity risks, posing significant challenges to these environments. The
study explores security threats in smart homes ecosystems, categorizing them
into vulnerabilities at the network layer, device level, and those from
cloud-based and AI-driven systems. Research findings indicate that post-quantum
encryption, coupled with AI-driven anomaly detection, is highly effective in
enhancing security; however, computational resource demands present significant
challenges. Blockchain authentication together with zero-trust structures
builds security resilience, although they need changes to existing
infrastructure. The specific security strategies show their effectiveness
through ANOVA, Chi-square tests, and Monte Carlo simulations yet lack
sufficient scalability according to the results. The research demonstrates the
requirement for improvement in cryptographic techniques, alongside AI-enhanced
threat detection and adaptive security models which must achieve a balance
between performance and efficiency and real-time applicability within smart
home ecosystems.

</details>


### [296] [AutoAdv: Automated Adversarial Prompting for Multi-Turn Jailbreaking of Large Language Models](https://arxiv.org/abs/2507.01020)
*Aashray Reddy,Andrew Zagula,Nicholas Saban*

Main category: cs.CR

TL;DR: AutoAdv automates adversarial prompt generation to expose LLM vulnerabilities, achieving up to 86% jailbreak success rates.


<details>
  <summary>Details</summary>
Motivation: LLMs are vulnerable to jailbreaking attacks, highlighting the need for systematic evaluation of safety mechanisms.

Method: Uses a parametric attacker LLM with strategic rewriting, roleplaying, and iterative refinement of prompts.

Result: Achieves high jailbreak success rates (up to 86%) on models like ChatGPT, Llama, and DeepSeek.

Conclusion: Current safety mechanisms are insufficient against multi-turn attacks, calling for stronger defenses.

Abstract: Large Language Models (LLMs) continue to exhibit vulnerabilities to
jailbreaking attacks: carefully crafted malicious inputs intended to circumvent
safety guardrails and elicit harmful responses. As such, we present AutoAdv, a
novel framework that automates adversarial prompt generation to systematically
evaluate and expose vulnerabilities in LLM safety mechanisms. Our approach
leverages a parametric attacker LLM to produce semantically disguised malicious
prompts through strategic rewriting techniques, specialized system prompts, and
optimized hyperparameter configurations. The primary contribution of our work
is a dynamic, multi-turn attack methodology that analyzes failed jailbreak
attempts and iteratively generates refined follow-up prompts, leveraging
techniques such as roleplaying, misdirection, and contextual manipulation. We
quantitatively evaluate attack success rate (ASR) using the StrongREJECT
(arXiv:2402.10260 [cs.CL]) framework across sequential interaction turns.
Through extensive empirical evaluation of state-of-the-art models--including
ChatGPT, Llama, and DeepSeek--we reveal significant vulnerabilities, with our
automated attacks achieving jailbreak success rates of up to 86% for harmful
content generation. Our findings reveal that current safety mechanisms remain
susceptible to sophisticated multi-turn attacks, emphasizing the urgent need
for more robust defense strategies.

</details>


### [297] [SafePTR: Token-Level Jailbreak Defense in Multimodal LLMs via Prune-then-Restore Mechanism](https://arxiv.org/abs/2507.01513)
*Beitao Chen,Xinyu Lyu,Lianli Gao,Jingkuan Song,Heng Tao Shen*

Main category: cs.CR

TL;DR: The paper analyzes vulnerabilities in Multimodal Large Language Models (MLLMs) caused by harmful multimodal tokens, proposing SafePTR, a training-free defense framework to selectively prune harmful tokens and restore benign features, improving safety without computational overhead.


<details>
  <summary>Details</summary>
Motivation: Existing defense methods for MLLMs fail to address the root causes of vulnerabilities, leaving them susceptible to jailbreak attacks. The study aims to uncover how harmful tokens bypass safeguards and propose an effective solution.

Method: The paper conducts a comprehensive analysis of harmful multimodal tokens in MLLMs, identifying their impact on early-middle layers. It then introduces SafePTR, a framework that prunes harmful tokens and restores benign features without training.

Result: SafePTR significantly enhances MLLM safety by targeting less than 1% of harmful tokens, achieving state-of-the-art performance in mitigating jailbreak risks without compromising utility or efficiency.

Conclusion: SafePTR offers a lightweight, effective defense against multimodal jailbreak attacks, demonstrating its potential for safe MLLM deployment without additional computational costs.

Abstract: By incorporating visual inputs, Multimodal Large Language Models (MLLMs)
extend LLMs to support visual reasoning. However, this integration also
introduces new vulnerabilities, making MLLMs susceptible to multimodal
jailbreak attacks and hindering their safe deployment.Existing defense methods,
including Image-to-Text Translation, Safe Prompting, and Multimodal Safety
Tuning, attempt to address this by aligning multimodal inputs with LLMs'
built-in safeguards.Yet, they fall short in uncovering root causes of
multimodal vulnerabilities, particularly how harmful multimodal tokens trigger
jailbreak in MLLMs? Consequently, they remain vulnerable to text-driven
multimodal jailbreaks, often exhibiting overdefensive behaviors and imposing
heavy training overhead.To bridge this gap, we present an comprehensive
analysis of where, how and which harmful multimodal tokens bypass safeguards in
MLLMs. Surprisingly, we find that less than 1% tokens in early-middle layers
are responsible for inducing unsafe behaviors, highlighting the potential of
precisely removing a small subset of harmful tokens, without requiring safety
tuning, can still effectively improve safety against jailbreaks. Motivated by
this, we propose Safe Prune-then-Restore (SafePTR), an training-free defense
framework that selectively prunes harmful tokens at vulnerable layers while
restoring benign features at subsequent layers.Without incurring additional
computational overhead, SafePTR significantly enhances the safety of MLLMs
while preserving efficiency. Extensive evaluations across three MLLMs and five
benchmarks demonstrate SafePTR's state-of-the-art performance in mitigating
jailbreak risks without compromising utility.

</details>


### [298] [Empowering Manufacturers with Privacy-Preserving AI Tools: A Case Study in Privacy-Preserving Machine Learning to Solve Real-World Problems](https://arxiv.org/abs/2507.01808)
*Xiaoyu Ji,Jessica Shorland,Joshua Shank,Pascal Delpe-Brice,Latanya Sweeney,Jan Allebach,Ali Shakouri*

Main category: cs.CR

TL;DR: A privacy-preserving platform enables manufacturers to securely share data with researchers, who develop tools like an automated crystal analysis tool for food manufacturing, improving speed and accuracy while ensuring data confidentiality.


<details>
  <summary>Details</summary>
Motivation: Small- and medium-sized manufacturers hesitate to share proprietary data due to competition and privacy concerns, hindering innovation.

Method: A secure platform allows data sharing; researchers develop tools (e.g., a machine learning-based crystal analysis tool) and deploy them back on the platform.

Result: An automated tool for analyzing food crystal images was created, replacing manual counting, improving speed and accuracy, and ensuring data privacy.

Conclusion: The platform successfully bridges the gap between manufacturers and researchers, fostering innovation while maintaining data security, with potential for broader applications.

Abstract: Small- and medium-sized manufacturers need innovative data tools but, because
of competition and privacy concerns, often do not want to share their
proprietary data with researchers who might be interested in helping. This
paper introduces a privacy-preserving platform by which manufacturers may
safely share their data with researchers through secure methods, so that those
researchers then create innovative tools to solve the manufacturers' real-world
problems, and then provide tools that execute solutions back onto the platform
for others to use with privacy and confidentiality guarantees. We illustrate
this problem through a particular use case which addresses an important problem
in the large-scale manufacturing of food crystals, which is that quality
control relies on image analysis tools. Previous to our research, food crystals
in the images were manually counted, which required substantial and
time-consuming human efforts, but we have developed and deployed a crystal
analysis tool which makes this process both more rapid and accurate. The tool
enables automatic characterization of the crystal size distribution and numbers
from microscope images while the natural imperfections from the sample
preparation are automatically removed; a machine learning model to count high
resolution translucent crystals and agglomeration of crystals was also
developed to aid in these efforts. The resulting algorithm was then packaged
for real-world use on the factory floor via a web-based app secured through the
originating privacy-preserving platform, allowing manufacturers to use it while
keeping their proprietary data secure. After demonstrating this full process,
future directions are also explored.

</details>


### [299] [How to Securely Shuffle? A survey about Secure Shufflers for privacy-preserving computations](https://arxiv.org/abs/2507.01487)
*Marc Damie,Florian Hahn,Andreas Peter,Jan Ramon*

Main category: cs.CR

TL;DR: This survey analyzes 26 secure shuffling protocols, unifying security definitions and offering practical guidelines for selecting them, while highlighting their role in privacy-preserving technologies.


<details>
  <summary>Details</summary>
Motivation: The resurgence of interest in secure shufflers due to their privacy amplification in differential privacy, coupled with the lack of practical evaluation of existing implementations, motivates this comprehensive survey.

Method: The study identifies, categorizes, and compares 26 secure shuffling protocols, adapting and unifying security definitions for consistent evaluation. It also reviews privacy-preserving technologies using shufflers.

Result: The survey provides a systematic comparison of shuffling protocols, practical selection guidelines, and insights into their vulnerabilities and performance trade-offs.

Conclusion: The work clarifies what makes a good secure shuffler, outlines future research directions, and emphasizes the importance of practical implementation considerations.

Abstract: Ishai et al. (FOCS'06) introduced secure shuffling as an efficient building
block for private data aggregation. Recently, the field of differential privacy
has revived interest in secure shufflers by highlighting the privacy
amplification they can provide in various computations. Although several works
argue for the utility of secure shufflers, they often treat them as black
boxes; overlooking the practical vulnerabilities and performance trade-offs of
existing implementations. This leaves a central question open: what makes a
good secure shuffler?
  This survey addresses that question by identifying, categorizing, and
comparing 26 secure protocols that realize the necessary shuffling
functionality. To enable a meaningful comparison, we adapt and unify existing
security definitions into a consistent set of properties. We also present an
overview of privacy-preserving technologies that rely on secure shufflers,
offer practical guidelines for selecting appropriate protocols, and outline
promising directions for future work.

</details>


### [300] [On the Effect of Ruleset Tuning and Data Imbalance on Explainable Network Security Alert Classifications: a Case-Study on DeepCASE](https://arxiv.org/abs/2507.01571)
*Koen T. W. Teuwen,Sam Baggen,Emmanuele Zambon,Luca Allodi*

Main category: cs.CR

TL;DR: The paper examines the impact of label imbalance on network intrusion alert classification, using DeepCASE, and suggests tuning SOC detection rules to improve performance and explainability.


<details>
  <summary>Details</summary>
Motivation: Automated methods in SOCs must handle imbalanced data and provide explainable decisions, but current methods struggle with these challenges.

Method: Evaluates the effect of label imbalance on DeepCASE, a state-of-the-art alert classification method, focusing on performance and explanation correctness.

Result: Label imbalance negatively affects both classification performance and explanation accuracy in DeepCASE.

Conclusion: Tuning SOC detection rules can reduce imbalance, enhancing the performance and explainability of automated alert classification methods like DeepCASE.

Abstract: Automation in Security Operations Centers (SOCs) plays a prominent role in
alert classification and incident escalation. However, automated methods must
be robust in the presence of imbalanced input data, which can negatively affect
performance. Additionally, automated methods should make explainable decisions.
In this work, we evaluate the effect of label imbalance on the classification
of network intrusion alerts. As our use-case we employ DeepCASE, the
state-of-the-art method for automated alert classification. We show that label
imbalance impacts both classification performance and correctness of the
classification explanations offered by DeepCASE. We conclude tuning the
detection rules used in SOCs can significantly reduce imbalance and may benefit
the performance and explainability offered by alert post-processing methods
such as DeepCASE. Therefore, our findings suggest that traditional methods to
improve the quality of input data can benefit automation.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [301] [Hardware-software co-exploration with racetrack memory based in-memory computing for CNN inference in embedded systems](https://arxiv.org/abs/2507.01429)
*Benjamin Chen Ming Choong,Tao Luo,Cheng Liu,Bingsheng He,Wei Zhang,Joey Tianyi Zhou*

Main category: cs.ET

TL;DR: An efficient in-memory CNN accelerator for racetrack memory is proposed, addressing challenges in memory density and power efficiency for embedded AI systems.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks demand high computational resources, posing challenges for low-resource embedded systems. In-memory computing, particularly with racetrack memory, offers potential but faces integration issues.

Method: Design of fundamental in-memory arithmetic circuits for multiply-and-accumulate operations, coupled with co-design of racetrack memory systems and CNN architectures.

Result: Achieves compact memory bank area with notable energy and performance improvements while maintaining model accuracy.

Conclusion: The proposed accelerator and co-optimization strategies effectively enhance efficiency and performance for racetrack memory-based embedded AI systems.

Abstract: Deep neural networks generate and process large volumes of data, posing
challenges for low-resource embedded systems. In-memory computing has been
demonstrated as an efficient computing infrastructure and shows promise for
embedded AI applications. Among newly-researched memory technologies, racetrack
memory is a non-volatile technology that allows high data density fabrication,
making it a good fit for in-memory computing. However, integrating in-memory
arithmetic circuits with memory cells affects both the memory density and power
efficiency. It remains challenging to build efficient in-memory arithmetic
circuits on racetrack memory within area and energy constraints. To this end,
we present an efficient in-memory convolutional neural network (CNN)
accelerator optimized for use with racetrack memory. We design a series of
fundamental arithmetic circuits as in-memory computing cells suited for
multiply-and-accumulate operations. Moreover, we explore the design space of
racetrack memory based systems and CNN model architectures, employing co-design
to improve the efficiency and performance of performing CNN inference in
racetrack memory while maintaining model accuracy. Our designed circuits and
model-system co-optimization strategies achieve a small memory bank area with
significant improvements in energy and performance for racetrack memory based
embedded systems.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [302] [A first-order method for nonconvex-nonconcave minimax problems under a local Kurdyka-Łojasiewicz condition](https://arxiv.org/abs/2507.01932)
*Zhaosong Lu,Xiangyuan Wang*

Main category: math.OC

TL;DR: The paper addresses nonconvex-nonconcave minimax problems with a local KL condition, proposing an inexact proximal gradient method for solving them and providing complexity guarantees.


<details>
  <summary>Details</summary>
Motivation: The motivation is to handle minimax problems where the inner maximization satisfies a local KL condition, which is more practical but introduces analytical challenges compared to global KL or PL conditions.

Method: The method involves an inexact proximal gradient approach, where the gradient of the maximal function is computed using a proximal gradient method on a KL-structured subproblem.

Result: The result is the development of a solution method with complexity guarantees for finding approximate stationary points under mild assumptions.

Conclusion: The conclusion highlights the broader applicability of the local KL condition and the effectiveness of the proposed method in addressing the challenges it introduces.

Abstract: We study a class of nonconvex-nonconcave minimax problems in which the inner
maximization problem satisfies a local Kurdyka-{\L}ojasiewicz (KL) condition
that may vary with the outer minimization variable. In contrast to the global
KL or Polyak-{\L}ojasiewicz (PL) conditions commonly assumed in the literature
-- which are significantly stronger and often too restrictive in practice --
this local KL condition accommodates a broader range of practical scenarios.
However, it also introduces new analytical challenges. In particular, as an
optimization algorithm progresses toward a stationary point of the problem, the
region over which the KL condition holds may shrink, resulting in a more
intricate and potentially ill-conditioned landscape. To address this challenge,
we show that the associated maximal function is locally H\"older smooth.
Leveraging this key property, we develop an inexact proximal gradient method
for solving the minimax problem, where the inexact gradient of the maximal
function is computed by applying a proximal gradient method to a KL-structured
subproblem. Under mild assumptions, we establish complexity guarantees for
computing an approximate stationary point of the minimax problem.

</details>


<div id='math-ph'></div>

# math-ph [[Back]](#toc)

### [303] [Symbolic identification of tensor equations in multidimensional physical fields](https://arxiv.org/abs/2507.01466)
*Tianyi Chen,Hao Yang,Wenjun Ma,Jun Zhang*

Main category: math-ph

TL;DR: SITE is a data-driven framework for identifying tensor equations, inspired by M-GEP, with innovations like dimensional homogeneity checks and tensor linear regression for robustness and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing methods for discovering governing equations are limited to scalar equations, lacking capability for tensor relationships.

Method: SITE uses a host-plasmid structure, genetic information retention, dimensional homogeneity checks, and tensor linear regression.

Result: SITE accurately recovers target equations from synthetic data, handles noise, and identifies constitutive relations from molecular simulations.

Conclusion: SITE demonstrates potential for data-driven discovery of tensor equations, adapting to various flow conditions.

Abstract: Recently, data-driven methods have shown great promise for discovering
governing equations from simulation or experimental data. However, most
existing approaches are limited to scalar equations, with few capable of
identifying tensor relationships. In this work, we propose a general
data-driven framework for identifying tensor equations, referred to as Symbolic
Identification of Tensor Equations (SITE). The core idea of SITE--representing
tensor equations using a host-plasmid structure--is inspired by the
multidimensional gene expression programming (M-GEP) approach. To improve the
robustness of the evolutionary process, SITE adopts a genetic information
retention strategy. Moreover, SITE introduces two key innovations beyond
conventional evolutionary algorithms. First, it incorporates a dimensional
homogeneity check to restrict the search space and eliminate physically invalid
expressions. Second, it replaces traditional linear scaling with a tensor
linear regression technique, greatly enhancing the efficiency of numerical
coefficient optimization. We validate SITE using two benchmark scenarios, where
it accurately recovers target equations from synthetic data, showing robustness
to noise and small sample sizes. Furthermore, SITE is applied to identify
constitutive relations directly from molecular simulation data, which are
generated without reliance on macroscopic constitutive models. It adapts to
both compressible and incompressible flow conditions and successfully
identifies the corresponding macroscopic forms, highlighting its potential for
data-driven discovery of tensor equation.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [304] [A LoD of Gaussians: Unified Training and Rendering for Ultra-Large Scale Reconstruction with External Memory](https://arxiv.org/abs/2507.01110)
*Felix Windisch,Lukas Radl,Thomas Köhler,Michael Steiner,Dieter Schmalstieg,Markus Steinberger*

Main category: cs.GR

TL;DR: A LoD of Gaussians enables scalable, high-quality novel view synthesis for large scenes without partitioning, using dynamic streaming and efficient LoD selection.


<details>
  <summary>Details</summary>
Motivation: Existing Gaussian Splatting techniques struggle with large-scale scenes due to partitioning artifacts, training complexity, and GPU memory limitations.

Method: The framework trains a Level-of-Detail (LoD) representation, dynamically streams relevant Gaussians, and uses a hybrid data structure for efficient LoD selection.

Result: Enables seamless multi-scale reconstruction and interactive visualization of complex scenes on a single GPU.

Conclusion: The method overcomes scalability and rendering limitations, supporting real-time, high-quality visualization of ultra-large-scale scenes.

Abstract: Gaussian Splatting has emerged as a high-performance technique for novel view
synthesis, enabling real-time rendering and high-quality reconstruction of
small scenes. However, scaling to larger environments has so far relied on
partitioning the scene into chunks -- a strategy that introduces artifacts at
chunk boundaries, complicates training across varying scales, and is poorly
suited to unstructured scenarios such as city-scale flyovers combined with
street-level views. Moreover, rendering remains fundamentally limited by GPU
memory, as all visible chunks must reside in VRAM simultaneously. We introduce
A LoD of Gaussians, a framework for training and rendering ultra-large-scale
Gaussian scenes on a single consumer-grade GPU -- without partitioning. Our
method stores the full scene out-of-core (e.g., in CPU memory) and trains a
Level-of-Detail (LoD) representation directly, dynamically streaming only the
relevant Gaussians. A hybrid data structure combining Gaussian hierarchies with
Sequential Point Trees enables efficient, view-dependent LoD selection, while a
lightweight caching and view scheduling system exploits temporal coherence to
support real-time streaming and rendering. Together, these innovations enable
seamless multi-scale reconstruction and interactive visualization of complex
scenes -- from broad aerial views to fine-grained ground-level details.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [305] [User-guided Generative Source Separation](https://arxiv.org/abs/2507.01339)
*Yutong Wen,Minje Kim,Paris Smaragdis*

Main category: cs.SD

TL;DR: GuideSep is a diffusion-based music source separation model that offers instrument-agnostic separation, using waveform mimicry and mel-spectrogram masks for flexible guidance.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack flexibility for real-world applications due to their focus on fixed four-stem separation.

Method: GuideSep uses a diffusion-based approach with waveform mimicry and mel-spectrogram masks for conditioning, and includes a mask-prediction baseline for comparison.

Result: GuideSep achieves high-quality separation and versatile instrument extraction, outperforming fixed-class methods.

Conclusion: The model demonstrates the potential of user-guided, diffusion-based generative processes for flexible and high-quality music source separation.

Abstract: Music source separation (MSS) aims to extract individual instrument sources
from their mixture. While most existing methods focus on the widely adopted
four-stem separation setup (vocals, bass, drums, and other instruments), this
approach lacks the flexibility needed for real-world applications. To address
this, we propose GuideSep, a diffusion-based MSS model capable of
instrument-agnostic separation beyond the four-stem setup. GuideSep is
conditioned on multiple inputs: a waveform mimicry condition, which can be
easily provided by humming or playing the target melody, and mel-spectrogram
domain masks, which offer additional guidance for separation. Unlike prior
approaches that relied on fixed class labels or sound queries, our conditioning
scheme, coupled with the generative approach, provides greater flexibility and
applicability. Additionally, we design a mask-prediction baseline using the
same model architecture to systematically compare predictive and generative
approaches. Our objective and subjective evaluations demonstrate that GuideSep
achieves high-quality separation while enabling more versatile instrument
extraction, highlighting the potential of user participation in the
diffusion-based generative process for MSS. Our code and demo page are
available at https://yutongwen.github.io/GuideSep/

</details>


### [306] [Real-Time Emergency Vehicle Siren Detection with Efficient CNNs on Embedded Hardware](https://arxiv.org/abs/2507.01563)
*Marco Giordano,Stefano Giacomelli,Claudia Rinaldi,Fabio Graziosi*

Main category: cs.SD

TL;DR: A real-time emergency vehicle siren detection system using E2PANNs, optimized for urban conditions, deployed on Raspberry Pi 5 with low-latency performance.


<details>
  <summary>Details</summary>
Motivation: To address unreliable standard AudioSet annotations and enable real-time, low-cost emergency vehicle tracking in smart cities.

Method: Uses E2PANNs (fine-tuned CNN), custom datasets (AudioSet-EV, Augmented, Unified-EV), and multithreaded inference with adaptive frame sizing and probability smoothing.

Result: Achieves low-latency detection with improved robustness under realistic audio conditions.

Conclusion: Demonstrates feasibility of deploying distributed acoustic monitoring networks on low-cost edge devices for smart city applications.

Abstract: We present a full-stack emergency vehicle (EV) siren detection system
designed for real-time deployment on embedded hardware. The proposed approach
is based on E2PANNs, a fine-tuned convolutional neural network derived from
EPANNs, and optimized for binary sound event detection under urban acoustic
conditions. A key contribution is the creation of curated and semantically
structured datasets - AudioSet-EV, AudioSet-EV Augmented, and Unified-EV -
developed using a custom AudioSet-Tools framework to overcome the low
reliability of standard AudioSet annotations. The system is deployed on a
Raspberry Pi 5 equipped with a high-fidelity DAC+microphone board, implementing
a multithreaded inference engine with adaptive frame sizing, probability
smoothing, and a decision-state machine to control false positive activations.
A remote WebSocket interface provides real-time monitoring and facilitates live
demonstration capabilities. Performance is evaluated using both framewise and
event-based metrics across multiple configurations. Results show the system
achieves low-latency detection with improved robustness under realistic audio
conditions. This work demonstrates the feasibility of deploying IoS-compatible
SED solutions that can form distributed acoustic monitoring networks, enabling
collaborative emergency vehicle tracking across smart city infrastructures
through WebSocket connectivity on low-cost edge devices.

</details>


### [307] [Exploring Classical Piano Performance Generation with Expressive Music Variational AutoEncoder](https://arxiv.org/abs/2507.01582)
*Jing Luo,Xinyu Yang,Jie Wei*

Main category: cs.SD

TL;DR: The paper introduces the Expressive Compound Word (ECP) representation and the Expressive Music Variational AutoEncoder (XMVAE) to generate classical piano performances, combining composer and pianist roles.


<details>
  <summary>Details</summary>
Motivation: To emulate the dual creative roles of composers and pianists in generating expressive classical piano performances from scratch.

Method: Proposes the XMVAE model with two branches: a VQ-VAE for score-related content (Composer) and a vanilla VAE for expressive details (Pianist), trained jointly with Seq2Seq architectures.

Result: XMVAE outperforms state-of-the-art models in generating high-quality classical performances, with pretraining on extra score datasets enhancing performance.

Conclusion: The XMVAE framework successfully integrates composer and pianist roles, producing superior classical piano performances.

Abstract: The creativity of classical music arises not only from composers who craft
the musical sheets but also from performers who interpret the static notations
with expressive nuances. This paper addresses the challenge of generating
classical piano performances from scratch, aiming to emulate the dual roles of
composer and pianist in the creative process. We introduce the Expressive
Compound Word (ECP) representation, which effectively captures both the
metrical structure and expressive nuances of classical performances. Building
on this, we propose the Expressive Music Variational AutoEncoder (XMVAE), a
model featuring two branches: a Vector Quantized Variational AutoEncoder
(VQ-VAE) branch that generates score-related content, representing the
Composer, and a vanilla VAE branch that produces expressive details, fulfilling
the role of Pianist. These branches are jointly trained with similar Seq2Seq
architectures, leveraging a multiscale encoder to capture beat-level contextual
information and an orthogonal Transformer decoder for efficient compound tokens
decoding. Both objective and subjective evaluations demonstrate that XMVAE
generates classical performances with superior musical quality compared to
state-of-the-art models. Furthermore, pretraining the Composer branch on extra
musical score datasets contribute to a significant performance gain.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [308] [Cross-Attention Message-Passing Transformers for Code-Agnostic Decoding in 6G Networks](https://arxiv.org/abs/2507.01038)
*Seong-Joon Park,Hee-Youl Kwak,Sang-Hyo Kim,Yongjune Kim,Jong-Seon No*

Main category: cs.IT

TL;DR: Proposes an AI-native foundation model (CrossMPT) for flexible and scalable channel decoding in 6G networks, achieving state-of-the-art performance and generalization across diverse codes.


<details>
  <summary>Details</summary>
Motivation: Traditional decoders lack flexibility and scalability for 6G's heterogeneous requirements, necessitating a unified, code-agnostic solution.

Method: Introduces CrossMPT, a transformer-based decoder with cross-attention blocks for magnitude and syndrome vectors, and extends it to FCrossMPT (code-invariant) and CrossED (ensemble).

Result: Achieves state-of-the-art decoding performance and strong generalization across code types without retraining.

Conclusion: The AI-native decoder offers a promising solution for 6G channel coding, combining flexibility, scalability, and high performance.

Abstract: Channel coding for 6G networks is expected to support a wide range of
requirements arising from heterogeneous communication scenarios. These demands
challenge traditional code-specific decoders, which lack the flexibility and
scalability required for next-generation systems. To tackle this problem, we
propose an AI-native foundation model for unified and code-agnostic decoding
based on the transformer architecture. We first introduce a cross-attention
message-passing transformer (CrossMPT). CrossMPT employs two masked
cross-attention blocks that iteratively update two distinct input
representations-magnitude and syndrome vectors-allowing the model to
effectively learn the decoding problem. Notably, our CrossMPT has achieved
state-of-the-art decoding performance among single neural decoders. Building on
this, we develop foundation CrossMPT (FCrossMPT) by making the architecture
invariant to code length, rate, and class, allowing a single trained model to
decode a broad range of codes without retraining. To further enhance decoding
performance, particularly for short blocklength codes, we propose CrossMPT
ensemble decoder (CrossED), an ensemble decoder composed of multiple parallel
CrossMPT blocks employing different parity-check matrices. This architecture
can also serve as a foundation model, showing strong generalization across
diverse code types. Overall, the proposed AI-native code-agnostic decoder
offers flexibility, scalability, and high performance, presenting a promising
direction to channel coding for 6G networks.

</details>


### [309] [A Hybrid Ensemble Learning Framework for Image-Based Solar Panel Classification](https://arxiv.org/abs/2507.01778)
*Vivek Tetarwal,Sandeep Kumar*

Main category: cs.IT

TL;DR: A Dual Ensemble Neural Network (DENN) is proposed for classifying clean vs. dirty solar panels, outperforming existing methods in accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: Automated discrimination of solar panel cleanliness is crucial for maintenance, but current methods lack accuracy and robustness.

Method: The DENN integrates multiple ensemble models into a dual framework, leveraging image-based features for classification.

Result: DENN achieves state-of-the-art accuracy on the Deep Solar Eye dataset, surpassing other ensemble methods.

Conclusion: Hybrid ensemble learning, like DENN, shows promise for scalable automated solar panel inspections.

Abstract: The installation of solar energy systems is on the rise, and therefore,
appropriate maintenance techniques are required to be used in order to maintain
maximum performance levels. One of the major challenges is the automated
discrimination between clean and dirty solar panels. This paper presents a
novel Dual Ensemble Neural Network (DENN) to classify solar panels using
image-based features. The suggested approach utilizes the advantages offered by
various ensemble models by integrating them into a dual framework, aimed at
improving both classification accuracy and robustness. The DENN model is
evaluated in comparison to current ensemble methods, showcasing its superior
performance across a range of assessment metrics. The proposed approach
performs the best compared to other methods and reaches state-of-the-art
accuracy on experimental results for the Deep Solar Eye dataset, effectively
serving predictive maintenance purposes in solar energy systems. It reveals the
potential of hybrid ensemble learning techniques to further advance the
prospects of automated solar panel inspections as a scalable solution to
real-world challenges.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [310] [Can Argus Judge Them All? Comparing VLMs Across Domains](https://arxiv.org/abs/2507.01042)
*Harsh Joshi,Gautam Siddharth Kashyap,Rafiq Ali,Ebad Shabbir,Niharika Jain,Sarthak Jain,Jiechao Gao,Usman Naseem*

Main category: cs.IR

TL;DR: Benchmarking CLIP, BLIP, and LXMERT reveals trade-offs between generalization and specialization in Vision-Language Models (VLMs).


<details>
  <summary>Details</summary>
Motivation: To evaluate the consistency and performance of VLMs across diverse tasks, addressing gaps in current understanding.

Method: Benchmarked CLIP, BLIP, and LXMERT on retrieval, captioning, and reasoning tasks using accuracy, generation quality, efficiency, and a novel CDC metric.

Result: CLIP generalizes best (CDC: 0.92), BLIP performs well on curated data, and LXMERT leads in structured reasoning.

Conclusion: The study highlights trade-offs in VLM performance, aiding industrial deployment and guiding future robust, flexible architectures.

Abstract: Vision-Language Models (VLMs) are advancing multimodal AI, yet their
performance consistency across tasks is underexamined. We benchmark CLIP, BLIP,
and LXMERT across diverse datasets spanning retrieval, captioning, and
reasoning. Our evaluation includes task accuracy, generation quality,
efficiency, and a novel Cross-Dataset Consistency (CDC) metric. CLIP shows
strongest generalization (CDC: 0.92), BLIP excels on curated data, and LXMERT
leads in structured reasoning. These results expose trade-offs between
generalization and specialization, informing industrial deployment of VLMs and
guiding development toward robust, task-flexible architectures.

</details>


### [311] [Conversational LLMs Simplify Secure Clinical Data Access, Understanding, and Analysis](https://arxiv.org/abs/2507.01053)
*Rafi Al Attrach,Pedro Moreira,Rajna Fani,Renato Umeton,Leo Anthony Celi*

Main category: cs.IR

TL;DR: M3 simplifies querying MIMIC-IV, a large EHR database, by translating natural language questions into SQL, reducing technical barriers for researchers.


<details>
  <summary>Details</summary>
Motivation: To address the complexity and technical barriers in querying large clinical datasets like MIMIC-IV, which hinder effective use by researchers.

Method: M3 retrieves MIMIC-IV data, uses a language model to translate natural language questions into SQL, and executes queries, providing structured results.

Result: Researchers can perform nuanced cohort analyses in minutes, a task that previously required hours of manual SQL and clinical knowledge.

Conclusion: M3 democratizes access to MIMIC-IV, enabling broader research and faster translation of clinical data into insights.

Abstract: As ever-larger clinical datasets become available, they have the potential to
unlock unprecedented opportunities for medical research. Foremost among them is
Medical Information Mart for Intensive Care (MIMIC-IV), the world's largest
open-source EHR database. However, the inherent complexity of these datasets,
particularly the need for sophisticated querying skills and the need to
understand the underlying clinical settings, often presents a significant
barrier to their effective use. M3 lowers the technical barrier to
understanding and querying MIMIC-IV data. With a single command it retrieves
MIMIC-IV from PhysioNet, launches a local SQLite instance (or hooks into the
hosted BigQuery), and-via the Model Context Protocol (MCP)-lets researchers
converse with the database in plain English. Ask a clinical question in natural
language; M3 uses a language model to translate it into SQL, executes the query
against the MIMIC-IV dataset, and returns structured results alongside the
underlying query for verifiability and reproducibility. Demonstrations show
that minutes of dialogue with M3 yield the kind of nuanced cohort analyses that
once demanded hours of handcrafted SQL and relied on understanding the
complexities of clinical workflows. By simplifying access, M3 invites the
broader research community to mine clinical critical-care data and accelerates
the translation of raw records into actionable insight.

</details>


### [312] [A Data Science Approach to Calcutta High Court Judgments: An Efficient LLM and RAG-powered Framework for Summarization and Similar Cases Retrieval](https://arxiv.org/abs/2507.01058)
*Puspendu Banerjee,Aritra Mazumdar,Wazib Ansar,Saptarsi Goswami,Amlan Chakrabarti*

Main category: cs.IR

TL;DR: A framework using LLMs and RAG techniques improves efficiency in analyzing Calcutta High Court verdicts by summarizing legal texts and retrieving similar cases.


<details>
  <summary>Details</summary>
Motivation: The judiciary faces increasing legal issues, requiring efficient resource use. This research aims to enhance legal research and decision-making.

Method: Fine-tunes the Pegasus model for summarization and uses RAG for case retrieval, creating a vector database for efficient query responses.

Result: Significant improvements in summarization and retrieval, aiding legal professionals and students in accessing key information.

Conclusion: The framework enhances legal research efficiency and accessibility, benefiting the legal community.

Abstract: The judiciary, as one of democracy's three pillars, is dealing with a rising
amount of legal issues, needing careful use of judicial resources. This
research presents a complex framework that leverages Data Science
methodologies, notably Large Language Models (LLM) and Retrieval-Augmented
Generation (RAG) techniques, to improve the efficiency of analyzing Calcutta
High Court verdicts. Our framework focuses on two key aspects: first, the
creation of a robust summarization mechanism that distills complex legal texts
into concise and coherent summaries; and second, the development of an
intelligent system for retrieving similar cases, which will assist legal
professionals in research and decision making. By fine-tuning the Pegasus model
using case head note summaries, we achieve significant improvements in the
summarization of legal cases. Our two-step summarizing technique preserves
crucial legal contexts, allowing for the production of a comprehensive vector
database for RAG. The RAG-powered framework efficiently retrieves similar cases
in response to user queries, offering thorough overviews and summaries. This
technique not only improves legal research efficiency, but it also helps legal
professionals and students easily acquire and grasp key legal information,
benefiting the overall legal scenario.

</details>


### [313] [FAIR-MATCH: A Multi-Objective Framework for Bias Mitigation in Reciprocal Dating Recommendations](https://arxiv.org/abs/2507.01063)
*Madhav Kotecha*

Main category: cs.IR

TL;DR: The paper analyzes algorithmic deficiencies in dating app recommendation systems, proposes solutions, and demonstrates improved performance and fairness.


<details>
  <summary>Details</summary>
Motivation: Online dating platforms rely on algorithmic matching, but current systems suffer from biases like popularity bias and filter bubbles, limiting effectiveness.

Method: The research integrates foundational and empirical work, analyzes reciprocal recommendation frameworks, and evaluates fairness metrics. A new framework with enhanced similarity measures and fairness-aware algorithms is proposed.

Result: Current systems perform modestly (25.1% for collaborative filtering, 28.7% for reciprocal methods). The proposed framework improves accuracy and reduces bias.

Conclusion: The study highlights key issues in dating app algorithms and offers research-backed solutions to enhance fairness and performance.

Abstract: Online dating platforms have fundamentally transformed the formation of
romantic relationships, with millions of users worldwide relying on algorithmic
matching systems to find compatible partners. However, current recommendation
systems in dating applications suffer from significant algorithmic
deficiencies, including but not limited to popularity bias, filter bubble
effects, and inadequate reciprocity modeling that limit effectiveness and
introduce harmful biases. This research integrates foundational work with
recent empirical findings to deliver a detailed analysis of dating app
recommendation systems, highlighting key issues and suggesting research-backed
solutions. Through analysis of reciprocal recommendation frameworks, fairness
evaluation metrics, and industry implementations, we demonstrate that current
systems achieve modest performance with collaborative filtering reaching 25.1\%
while reciprocal methods achieve 28.7\%. Our proposed mathematical framework
addresses these limitations through enhanced similarity measures,
multi-objective optimization, and fairness-aware algorithms that maintain
competitive accuracy while improving demographic representation to reduce
algorithmic bias.

</details>


### [314] [Cohort Retrieval using Dense Passage Retrieval](https://arxiv.org/abs/2507.01049)
*Pranav Jadhav*

Main category: cs.IR

TL;DR: The paper introduces Dense Passage Retrieval (DPR) for patient cohort retrieval in echocardiography, transforming unstructured EHR data into a Query-Passage dataset and evaluating it with clinical-inspired metrics.


<details>
  <summary>Details</summary>
Motivation: Patient cohort retrieval is crucial in medical research, but unstructured EHR data in echocardiography poses challenges.

Method: The study applies DPR, transforms EHR data into a Query-Passage dataset, and designs clinical evaluation metrics.

Result: A custom-trained DPR model outperforms traditional and off-the-shelf methods.

Conclusion: This is the first DPR application in echocardiography cohort retrieval, offering a transferable framework for other medical domains.

Abstract: Patient cohort retrieval is a pivotal task in medical research and clinical
practice, enabling the identification of specific patient groups from extensive
electronic health records (EHRs). In this work, we address the challenge of
cohort retrieval in the echocardiography domain by applying Dense Passage
Retrieval (DPR), a prominent methodology in semantic search. We propose a
systematic approach to transform an echocardiographic EHR dataset of
unstructured nature into a Query-Passage dataset, framing the problem as a
Cohort Retrieval task. Additionally, we design and implement evaluation metrics
inspired by real-world clinical scenarios to rigorously test the models across
diverse retrieval tasks. Furthermore, we present a custom-trained DPR embedding
model that demonstrates superior performance compared to traditional and
off-the-shelf SOTA methods.To our knowledge, this is the first work to apply
DPR for patient cohort retrieval in the echocardiography domain, establishing a
framework that can be adapted to other medical domains.

</details>


### [315] [Enhanced Influence-aware Group Recommendation for Online Media Propagation](https://arxiv.org/abs/2507.01616)
*Chengkun He,Xiangmin Zhou,Chen Wang,Longbing Cao,Jie Shao,Xiaodong Li,Guang Xu,Carrie Jinqiu Hu,Zahir Tari*

Main category: cs.IR

TL;DR: The paper introduces an Enhanced Influence-aware Group Recommendation (EIGR) framework to address challenges in group recommendation over social media streams, improving accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Group recommendation is crucial for applications like e-commerce and entertainment, but existing methods struggle with scalability, dynamic influence propagation, and computational overhead.

Method: The EIGR framework includes a Graph Extraction-based Sampling (GES) strategy, a Dynamic Independent Cascade (DYIC) model, and a two-level hash-based User Group Index (UG-Index).

Result: EIGR outperforms state-of-the-art baselines in effectiveness and efficiency on real-world datasets.

Conclusion: The proposed EIGR framework successfully addresses key challenges in group recommendation, offering a scalable and efficient solution.

Abstract: Group recommendation over social media streams has attracted significant
attention due to its wide applications in domains such as e-commerce,
entertainment, and online news broadcasting. By leveraging social connections
and group behaviours, group recommendation (GR) aims to provide more accurate
and engaging content to a set of users rather than individuals. Recently,
influence-aware GR has emerged as a promising direction, as it considers the
impact of social influence on group decision-making. In earlier work, we
proposed Influence-aware Group Recommendation (IGR) to solve this task.
However, this task remains challenging due to three key factors: the large and
ever-growing scale of social graphs, the inherently dynamic nature of influence
propagation within user groups, and the high computational overhead of
real-time group-item matching.
  To tackle these issues, we propose an Enhanced Influence-aware Group
Recommendation (EIGR) framework. First, we introduce a Graph Extraction-based
Sampling (GES) strategy to minimise redundancy across multiple temporal social
graphs and effectively capture the evolving dynamics of both groups and items.
Second, we design a novel DYnamic Independent Cascade (DYIC) model to predict
how influence propagates over time across social items and user groups.
Finally, we develop a two-level hash-based User Group Index (UG-Index) to
efficiently organise user groups and enable real-time recommendation
generation. Extensive experiments on real-world datasets demonstrate that our
proposed framework, EIGR, consistently outperforms state-of-the-art baselines
in both effectiveness and efficiency.

</details>


### [316] [Embedding-based Retrieval in Multimodal Content Moderation](https://arxiv.org/abs/2507.01066)
*Hanzhong Liang,Jinghao Shi,Xiang Shen,Zixuan Wang,Vera Wen,Ardalan Mehrani,Zhiqian Chen,Yifan Wu,Zhixin Zhang*

Main category: cs.IR

TL;DR: The paper introduces an Embedding-Based Retrieval (EBR) method to enhance content moderation on short video platforms, outperforming traditional classification methods in efficiency and cost.


<details>
  <summary>Details</summary>
Motivation: Traditional classification struggles with rapid trend adaptation and cost-efficiency in content moderation, prompting the need for a complementary approach.

Method: Uses Supervised Contrastive Learning (SCL) to train foundation embedding models (single/multi-modal), then integrates them into an EBR system for video retrieval.

Result: EBR improves ROC-AUC (0.85 to 0.99) and PR-AUC (0.35 to 0.95), boosts action rates by 10.32%, and cuts costs by 80%.

Conclusion: EBR offers a more efficient, interpretable, and flexible solution for content moderation compared to classification.

Abstract: Video understanding plays a fundamental role for content moderation on short
video platforms, enabling the detection of inappropriate content. While
classification remains the dominant approach for content moderation, it often
struggles in scenarios requiring rapid and cost-efficient responses, such as
trend adaptation and urgent escalations. To address this issue, we introduce an
Embedding-Based Retrieval (EBR) method designed to complement traditional
classification approaches. We first leverage a Supervised Contrastive Learning
(SCL) framework to train a suite of foundation embedding models, including both
single-modal and multi-modal architectures. Our models demonstrate superior
performance over established contrastive learning methods such as CLIP and
MoCo. Building on these embedding models, we design and implement the
embedding-based retrieval system that integrates embedding generation and video
retrieval to enable efficient and effective trend handling. Comprehensive
offline experiments on 25 diverse emerging trends show that EBR improves
ROC-AUC from 0.85 to 0.99 and PR-AUC from 0.35 to 0.95. Further online
experiments reveal that EBR increases action rates by 10.32% and reduces
operational costs by over 80%, while also enhancing interpretability and
flexibility compared to classification-based solutions.

</details>


### [317] [Optimizing Conversational Product Recommendation via Reinforcement Learning](https://arxiv.org/abs/2507.01060)
*Kang Liu*

Main category: cs.IR

TL;DR: A reinforcement learning method optimizes conversational strategies for product recommendations, improving engagement and uptake while adhering to constraints.


<details>
  <summary>Details</summary>
Motivation: Organizations use intelligent agents for sales and service, but effectiveness depends on how and when recommendations are delivered.

Method: Feedback-driven reinforcement learning mines behavioral patterns to refine dialogue policies.

Result: Agents learn optimal talk tracks, enhancing engagement and product uptake.

Conclusion: The approach enables scalable, personalized recommendations in enterprise settings.

Abstract: We propose a reinforcement learning-based approach to optimize conversational
strategies for product recommendation across diverse industries. As
organizations increasingly adopt intelligent agents to support sales and
service operations, the effectiveness of a conversation hinges not only on what
is recommended but how and when recommendations are delivered. We explore a
methodology where agentic systems learn optimal dialogue policies through
feedback-driven reinforcement learning. By mining aggregate behavioral patterns
and conversion outcomes, our approach enables agents to refine talk tracks that
drive higher engagement and product uptake, while adhering to contextual and
regulatory constraints. We outline the conceptual framework, highlight key
innovations, and discuss the implications for scalable, personalized
recommendation in enterprise environments.

</details>
