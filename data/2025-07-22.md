<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 141]
- [cs.AI](#cs.AI) [Total: 58]
- [cs.CL](#cs.CL) [Total: 86]
- [cs.LG](#cs.LG) [Total: 128]
- [q-fin.CP](#q-fin.CP) [Total: 1]
- [stat.ME](#stat.ME) [Total: 3]
- [eess.AS](#eess.AS) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [hep-ph](#hep-ph) [Total: 1]
- [cs.MA](#cs.MA) [Total: 2]
- [q-fin.TR](#q-fin.TR) [Total: 1]
- [math.DS](#math.DS) [Total: 1]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.DC](#cs.DC) [Total: 3]
- [cs.HC](#cs.HC) [Total: 7]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [cs.IR](#cs.IR) [Total: 10]
- [cs.MM](#cs.MM) [Total: 1]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [eess.IV](#eess.IV) [Total: 17]
- [physics.geo-ph](#physics.geo-ph) [Total: 2]
- [quant-ph](#quant-ph) [Total: 1]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 1]
- [cs.CE](#cs.CE) [Total: 2]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [cs.SD](#cs.SD) [Total: 4]
- [physics.app-ph](#physics.app-ph) [Total: 1]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [eess.SY](#eess.SY) [Total: 2]
- [cs.DS](#cs.DS) [Total: 3]
- [cs.IT](#cs.IT) [Total: 1]
- [cs.GR](#cs.GR) [Total: 5]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [gr-qc](#gr-qc) [Total: 1]
- [cs.SE](#cs.SE) [Total: 12]
- [astro-ph.HE](#astro-ph.HE) [Total: 1]
- [math.RT](#math.RT) [Total: 1]
- [cs.PL](#cs.PL) [Total: 1]
- [math.OC](#math.OC) [Total: 3]
- [stat.ML](#stat.ML) [Total: 12]
- [cs.RO](#cs.RO) [Total: 10]
- [stat.AP](#stat.AP) [Total: 1]
- [cs.CR](#cs.CR) [Total: 14]
- [cs.NI](#cs.NI) [Total: 9]
- [math.PR](#math.PR) [Total: 1]
- [cs.NE](#cs.NE) [Total: 2]
- [math.AT](#math.AT) [Total: 1]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [cs.SI](#cs.SI) [Total: 2]
- [math.NA](#math.NA) [Total: 2]
- [econ.TH](#econ.TH) [Total: 1]
- [eess.SP](#eess.SP) [Total: 26]
- [cs.CY](#cs.CY) [Total: 9]
- [cs.ET](#cs.ET) [Total: 1]
- [cs.GT](#cs.GT) [Total: 3]
- [cs.AR](#cs.AR) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Comparative Analysis of Algorithms for the Fitting of Tessellations to 3D Image Data](https://arxiv.org/abs/2507.14268)
*Andreas Alpers,Orkun Furat,Christian Jung,Matthias Neumann,Claudia Redenbach,Aigerim Saken,Volker Schmidt*

Main category: cs.CV

TL;DR: Comparative analysis of algorithmic strategies for fitting tessellation models to 3D image data, evaluating trade-offs between model complexity, optimization methods, and approximation quality.


<details>
  <summary>Details</summary>
Motivation: To assess and guide the selection of optimization-based methods for approximating voxel-based grain structures in materials like polycrystals and foams.

Method: Review and evaluation of linear/nonlinear programming, stochastic optimization (cross-entropy method), and gradient descent for generating Voronoi, Laguerre, and GBPDs.

Result: Trade-offs identified between model complexity, optimization routine complexity, and approximation quality, with practical guidance for method selection.

Conclusion: Provides insights for choosing appropriate tessellation methods based on data characteristics and application requirements.

Abstract: This paper presents a comparative analysis of algorithmic strategies for
fitting tessellation models to 3D image data of materials such as polycrystals
and foams. In this steadily advancing field, we review and assess
optimization-based methods -- including linear and nonlinear programming,
stochastic optimization via the cross-entropy method, and gradient descent --
for generating Voronoi, Laguerre, and generalized balanced power diagrams
(GBPDs) that approximate voxelbased grain structures. The quality of fit is
evaluated on real-world datasets using discrepancy measures that quantify
differences in grain volume, surface area, and topology. Our results highlight
trade-offs between model complexity, the complexity of the optimization
routines involved, and the quality of approximation, providing guidance for
selecting appropriate methods based on data characteristics and application
needs.

</details>


### [2] [Semantic Segmentation based Scene Understanding in Autonomous Vehicles](https://arxiv.org/abs/2507.14303)
*Ehsan Rassekh*

Main category: cs.CV

TL;DR: The paper explores efficient models for scene understanding via semantic segmentation in self-driving cars, using the BDD100k dataset and various backbones, showing improved performance metrics.


<details>
  <summary>Details</summary>
Motivation: To reduce reliance on human expertise by leveraging AI, particularly deep learning, for critical tasks like autonomous driving, focusing on semantic segmentation for better scene understanding.

Method: Proposes several models for semantic segmentation, utilizing different backbones as encoders, and evaluates them using the BDD100k dataset.

Result: Results indicate that selecting the right backbone significantly enhances model performance, improving accuracy, mean IoU, and loss function metrics.

Conclusion: The study demonstrates the importance of backbone selection in semantic segmentation models, leading to better scene understanding for autonomous systems.

Abstract: In recent years, the concept of artificial intelligence (AI) has become a
prominent keyword because it is promising in solving complex tasks. The need
for human expertise in specific areas may no longer be needed because machines
have achieved successful results using artificial intelligence and can make the
right decisions in critical situations. This process is possible with the help
of deep learning (DL), one of the most popular artificial intelligence
technologies. One of the areas in which the use of DL is used is in the
development of self-driving cars, which is very effective and important. In
this work, we propose several efficient models to investigate scene
understanding through semantic segmentation. We use the BDD100k dataset to
investigate these models. Another contribution of this work is the usage of
several Backbones as encoders for models. The obtained results show that
choosing the appropriate backbone has a great effect on the performance of the
model for semantic segmentation. Better performance in semantic segmentation
allows us to understand better the scene and the environment around the agent.
In the end, we analyze and evaluate the proposed models in terms of accuracy,
mean IoU, and loss function, and the results show that these metrics are
improved.

</details>


### [3] [CLIPTTA: Robust Contrastive Vision-Language Test-Time Adaptation](https://arxiv.org/abs/2507.14312)
*Marc Lafon,Gustavo Adolfo Vargas Hakim,Clément Rambour,Christian Desrosier,Nicolas Thome*

Main category: cs.CV

TL;DR: CLIPTTA is a gradient-based test-time adaptation method for vision-language models, addressing limitations of entropy minimization by using a soft contrastive loss aligned with CLIP's pre-training. It improves generalization under distribution shifts and outperforms existing methods.


<details>
  <summary>Details</summary>
Motivation: Vision-language models like CLIP struggle with distribution shifts during inference. Traditional test-time adaptation methods, like entropy minimization, are misaligned with CLIP's contrastive training, leading to poor adaptation and failure modes.

Method: CLIPTTA introduces a soft contrastive loss aligned with CLIP's pre-training, with a batch-aware design to prevent collapse. It also extends to open-set scenarios using an Outlier Contrastive Exposure (OCE) loss for better OOD detection.

Result: Evaluated on 75 datasets, CLIPTTA outperforms entropy-based methods and competes with state-of-the-art TTA methods, showing stable performance across diverse shifts.

Conclusion: CLIPTTA effectively adapts vision-language models at test time, addressing misalignment issues and improving generalization, making it a robust solution for diverse distribution shifts.

Abstract: Vision-language models (VLMs) like CLIP exhibit strong zero-shot capabilities
but often fail to generalize under distribution shifts. Test-time adaptation
(TTA) allows models to update at inference time without labeled data, typically
via entropy minimization. However, this objective is fundamentally misaligned
with the contrastive image-text training of VLMs, limiting adaptation
performance and introducing failure modes such as pseudo-label drift and class
collapse. We propose CLIPTTA, a new gradient-based TTA method for
vision-language models that leverages a soft contrastive loss aligned with
CLIP's pre-training objective. We provide a theoretical analysis of CLIPTTA's
gradients, showing how its batch-aware design mitigates the risk of collapse.
We further extend CLIPTTA to the open-set setting, where both in-distribution
(ID) and out-of-distribution (OOD) samples are encountered, using an Outlier
Contrastive Exposure (OCE) loss to improve OOD detection. Evaluated on 75
datasets spanning diverse distribution shifts, CLIPTTA consistently outperforms
entropy-based objectives and is highly competitive with state-of-the-art TTA
methods, outperforming them on a large number of datasets and exhibiting more
stable performance across diverse shifts.

</details>


### [4] [A Hidden Stumbling Block in Generalized Category Discovery: Distracted Attention](https://arxiv.org/abs/2507.14315)
*Qiyu Xu,Zhanxuan Hu,Yu Duan,Ercheng Pei,Yonghang Tai*

Main category: cs.CV

TL;DR: The paper introduces Attention Focusing (AF), a mechanism to improve Generalized Category Discovery (GCD) by reducing distracted attention in models, leading to better feature extraction.


<details>
  <summary>Details</summary>
Motivation: Existing GCD methods often suffer from distracted attention, where models focus on irrelevant background regions, degrading performance.

Method: AF uses Token Importance Measurement (TIME) and Token Adaptive Pruning (TAP) to prune non-informative tokens and sharpen focus.

Result: AF improves performance by up to 15.4% in SimGCD with minimal computational overhead.

Conclusion: AF is a lightweight, plug-and-play solution that enhances GCD methods effectively.

Abstract: Generalized Category Discovery (GCD) aims to classify unlabeled data from
both known and unknown categories by leveraging knowledge from labeled known
categories. While existing methods have made notable progress, they often
overlook a hidden stumbling block in GCD: distracted attention. Specifically,
when processing unlabeled data, models tend to focus not only on key objects in
the image but also on task-irrelevant background regions, leading to suboptimal
feature extraction. To remove this stumbling block, we propose Attention
Focusing (AF), an adaptive mechanism designed to sharpen the model's focus by
pruning non-informative tokens. AF consists of two simple yet effective
components: Token Importance Measurement (TIME) and Token Adaptive Pruning
(TAP), working in a cascade. TIME quantifies token importance across multiple
scales, while TAP prunes non-informative tokens by utilizing the multi-scale
importance scores provided by TIME. AF is a lightweight, plug-and-play module
that integrates seamlessly into existing GCD methods with minimal computational
overhead. When incorporated into one prominent GCD method, SimGCD, AF achieves
up to 15.4% performance improvement over the baseline with minimal
computational overhead. The implementation code is provided in
https://github.com/Afleve/AFGCD.

</details>


### [5] [Hallucination Score: Towards Mitigating Hallucinations in Generative Image Super-Resolution](https://arxiv.org/abs/2507.14367)
*Weiming Ren,Raghav Goyal,Zhiming Hu,Tristan Ty Aumentado-Armstrong,Iqbal Mohomed,Alex Levinshtein*

Main category: cs.CV

TL;DR: The paper addresses hallucination artifacts in generative super-resolution (GSR) models, proposing a method to measure and mitigate them using a multimodal large language model (MLLM) and deep feature alignment.


<details>
  <summary>Details</summary>
Motivation: GSR models produce perceptual artifacts where generated details mismatch low-resolution or ground-truth images, limiting practical use. Existing metrics fail to capture these hallucinations.

Method: A 'Hallucination Score' (HS) is introduced using MLLM prompts. Deep feature distances correlated with HS are used as differentiable rewards to align GSR models.

Result: HS aligns well with human evaluations and complements existing SR metrics. Deep feature distances show strong correlation with HS.

Conclusion: The proposed HS and feature alignment method effectively measures and mitigates hallucinations in GSR, improving perceptual fidelity.

Abstract: Generative super-resolution (GSR) currently sets the state-of-the-art in
terms of perceptual image quality, overcoming the "regression-to-the-mean" blur
of prior non-generative models. However, from a human perspective, such models
do not fully conform to the optimal balance between quality and fidelity.
Instead, a different class of artifacts, in which generated details fail to
perceptually match the low resolution image (LRI) or ground-truth image (GTI),
is a critical but under studied issue in GSR, limiting its practical
deployments. In this work, we focus on measuring, analyzing, and mitigating
these artifacts (i.e., "hallucinations"). We observe that hallucinations are
not well-characterized with existing image metrics or quality models, as they
are orthogonal to both exact fidelity and no-reference quality. Instead, we
take advantage of a multimodal large language model (MLLM) by constructing a
prompt that assesses hallucinatory visual elements and generates a
"Hallucination Score" (HS). We find that our HS is closely aligned with human
evaluations, and also provides complementary insights to prior image metrics
used for super-resolution (SR) models. In addition, we find certain deep
feature distances have strong correlations with HS. We therefore propose to
align the GSR models by using such features as differentiable reward functions
to mitigate hallucinations.

</details>


### [6] [DUSTrack: Semi-automated point tracking in ultrasound videos](https://arxiv.org/abs/2507.14368)
*Praneeth Namburi,Roger Pallarès-López,Jessica Rosendorf,Duarte Folgado,Brian W. Anthony*

Main category: cs.CV

TL;DR: DUSTrack is a semi-automated toolkit combining deep learning and optical flow for robust point tracking in B-mode ultrasound videos, outperforming zero-shot trackers and matching specialized methods.


<details>
  <summary>Details</summary>
Motivation: Accurate tissue motion tracking in B-mode ultrasound is hindered by speckle noise, low edge contrast, and out-of-plane movement, necessitating a reliable solution for clinical and research applications.

Method: DUSTrack integrates deep learning with optical flow, includes a GUI for training data generation, and employs optical-flow-based filtering to reduce noise while preserving motion details.

Result: DUSTrack achieves superior accuracy compared to zero-shot trackers and matches specialized methods, validated in cardiac, muscle, and fascicle tracking use cases.

Conclusion: DUSTrack is a versatile, open-source tool for quantifying tissue motion in ultrasound videos, with potential for widespread clinical and biomechanical research use.

Abstract: Ultrasound technology enables safe, non-invasive imaging of dynamic tissue
behavior, making it a valuable tool in medicine, biomechanics, and sports
science. However, accurately tracking tissue motion in B-mode ultrasound
remains challenging due to speckle noise, low edge contrast, and out-of-plane
movement. These challenges complicate the task of tracking anatomical landmarks
over time, which is essential for quantifying tissue dynamics in many clinical
and research applications. This manuscript introduces DUSTrack (Deep learning
and optical flow-based toolkit for UltraSound Tracking), a semi-automated
framework for tracking arbitrary points in B-mode ultrasound videos. We combine
deep learning with optical flow to deliver high-quality and robust tracking
across diverse anatomical structures and motion patterns. The toolkit includes
a graphical user interface that streamlines the generation of high-quality
training data and supports iterative model refinement. It also implements a
novel optical-flow-based filtering technique that reduces high-frequency
frame-to-frame noise while preserving rapid tissue motion. DUSTrack
demonstrates superior accuracy compared to contemporary zero-shot point
trackers and performs on par with specialized methods, establishing its
potential as a general and foundational tool for clinical and biomechanical
research. We demonstrate DUSTrack's versatility through three use cases:
cardiac wall motion tracking in echocardiograms, muscle deformation analysis
during reaching tasks, and fascicle tracking during ankle plantarflexion. As an
open-source solution, DUSTrack offers a powerful, flexible framework for point
tracking to quantify tissue motion from ultrasound videos. DUSTrack is
available at https://github.com/praneethnamburi/DUSTrack.

</details>


### [7] [CRAFT: A Neuro-Symbolic Framework for Visual Functional Affordance Grounding](https://arxiv.org/abs/2507.14426)
*Zhou Chen,Joe Lin,Sathyanarayanan N. Aakur*

Main category: cs.CV

TL;DR: CRAFT is a neuro-symbolic framework for interpretable affordance grounding, combining commonsense priors and visual evidence for transparent, goal-driven decisions.


<details>
  <summary>Details</summary>
Motivation: To improve interpretability and accuracy in identifying objects enabling specific actions (e.g., 'cut') in scenes.

Method: Integrates ConceptNet, language models, and CLIP in an energy-based reasoning loop for iterative refinement.

Result: Enhances accuracy and interpretability in multi-object, label-free settings.

Conclusion: CRAFT advances robust and trustworthy scene understanding by grounding symbolic and perceptual structures.

Abstract: We introduce CRAFT, a neuro-symbolic framework for interpretable affordance
grounding, which identifies the objects in a scene that enable a given action
(e.g., "cut"). CRAFT integrates structured commonsense priors from ConceptNet
and language models with visual evidence from CLIP, using an energy-based
reasoning loop to refine predictions iteratively. This process yields
transparent, goal-driven decisions to ground symbolic and perceptual
structures. Experiments in multi-object, label-free settings demonstrate that
CRAFT enhances accuracy while improving interpretability, providing a step
toward robust and trustworthy scene understanding.

</details>


### [8] [Adaptive 3D Gaussian Splatting Video Streaming](https://arxiv.org/abs/2507.14432)
*Han Gong,Qiyue Li,Zhi Liu,Hao Zhou,Peng Yuan Zhou,Zhu Li,Jie Li*

Main category: cs.CV

TL;DR: A framework for streaming 3D Gaussian splatting (3DGS) videos is introduced, addressing challenges like large data volume and compression complexity. It uses Gaussian deformation fields, hybrid saliency tiling, and differentiated quality modeling for efficient compression and transmission.


<details>
  <summary>Details</summary>
Motivation: 3DGS videos have improved volumetric video quality but pose streaming challenges due to large data size and compression complexity.

Method: The framework includes a 3DGS video construction method using Gaussian deformation fields, hybrid saliency tiling, and differentiated quality modeling for compression and bandwidth adaptation.

Result: The method outperforms existing approaches in video quality, compression effectiveness, and transmission rate.

Conclusion: The proposed framework effectively addresses 3DGS video streaming challenges, offering superior performance in quality and efficiency.

Abstract: The advent of 3D Gaussian splatting (3DGS) has significantly enhanced the
quality of volumetric video representation. Meanwhile, in contrast to
conventional volumetric video, 3DGS video poses significant challenges for
streaming due to its substantially larger data volume and the heightened
complexity involved in compression and transmission. To address these issues,
we introduce an innovative framework for 3DGS volumetric video streaming.
Specifically, we design a 3DGS video construction method based on the Gaussian
deformation field. By employing hybrid saliency tiling and differentiated
quality modeling of 3DGS video, we achieve efficient data compression and
adaptation to bandwidth fluctuations while ensuring high transmission quality.
Then we build a complete 3DGS video streaming system and validate the
transmission performance. Through experimental evaluation, our method
demonstrated superiority over existing approaches in various aspects, including
video quality, compression effectiveness, and transmission rate.

</details>


### [9] [IRGPT: Understanding Real-world Infrared Image with Bi-cross-modal Curriculum on Large-scale Benchmark](https://arxiv.org/abs/2507.14449)
*Zhe Cao,Jin Zhang,Ruiheng Zhang*

Main category: cs.CV

TL;DR: IRGPT is a multi-modal LLM for real-world infrared images, leveraging a large-scale IR-TD dataset and a bi-cross-modal transfer learning strategy to outperform existing methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the scarcity of aligned text data and domain-specific challenges in infrared imagery, which synthetic datasets fail to capture.

Method: Uses the IR-TD dataset (260K real image-text pairs) and a bi-cross-modal curriculum transfer learning strategy to transfer knowledge from visible to infrared domains.

Result: Achieves state-of-the-art performance on 9 benchmark tasks, surpassing larger-scale models.

Conclusion: IRGPT effectively bridges the gap in infrared vision-language tasks by leveraging real data and innovative transfer learning.

Abstract: Real-world infrared imagery presents unique challenges for vision-language
models due to the scarcity of aligned text data and domain-specific
characteristics. Although existing methods have advanced the field, their
reliance on synthetic infrared images generated through style transfer from
visible images, which limits their ability to capture the unique
characteristics of the infrared modality. To address this, we propose IRGPT,
the first multi-modal large language model for real-world infrared images,
built upon a large-scale InfraRed-Text Dataset (IR-TD) comprising over 260K
authentic image-text pairs. The proposed IR-TD dataset contains real infrared
images paired with meticulously handcrafted texts, where the initial drafts
originated from two complementary processes: (1) LLM-generated descriptions of
visible images, and (2) rule-based descriptions of annotations. Furthermore, we
introduce a bi-cross-modal curriculum transfer learning strategy that
systematically transfers knowledge from visible to infrared domains by
considering the difficulty scores of both infrared-visible and infrared-text.
Evaluated on a benchmark of 9 tasks (e.g., recognition, grounding), IRGPT
achieves state-of-the-art performance even compared with larger-scale models.

</details>


### [10] [GPI-Net: Gestalt-Guided Parallel Interaction Network via Orthogonal Geometric Consistency for Robust Point Cloud Registration](https://arxiv.org/abs/2507.14452)
*Weikang Gu,Mingyue Han,Li Xue,Heng Dong,Changcai Yang,Riqing Chen,Lifang Wei*

Main category: cs.CV

TL;DR: The paper proposes GPI-Net, a Gestalt-guided network for point cloud registration, combining local and global features via orthogonal integration and attention mechanisms, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Accurate point cloud registration requires high-quality correspondences, but fusing local and global features is challenging due to redundancy and spatial complexity. Gestalt principles offer a solution.

Method: GPI-Net uses Gestalt principles for feature fusion, employing an orthogonal integration strategy, Gestalt Feature Attention (GFA) block, and Dual-path Multi-Granularity (DMG) block for parallel interaction.

Result: Experiments show GPI-Net outperforms existing methods in various tasks.

Conclusion: GPI-Net effectively integrates local and global features for superior point cloud registration, with code available for reproducibility.

Abstract: The accurate identification of high-quality correspondences is a prerequisite
task in feature-based point cloud registration. However, it is extremely
challenging to handle the fusion of local and global features due to feature
redundancy and complex spatial relationships. Given that Gestalt principles
provide key advantages in analyzing local and global relationships, we propose
a novel Gestalt-guided Parallel Interaction Network via orthogonal geometric
consistency (GPI-Net) in this paper. It utilizes Gestalt principles to
facilitate complementary communication between local and global information.
Specifically, we introduce an orthogonal integration strategy to optimally
reduce redundant information and generate a more compact global structure for
high-quality correspondences. To capture geometric features in correspondences,
we leverage a Gestalt Feature Attention (GFA) block through a hybrid
utilization of self-attention and cross-attention mechanisms. Furthermore, to
facilitate the integration of local detail information into the global
structure, we design an innovative Dual-path Multi-Granularity parallel
interaction aggregation (DMG) block to promote information exchange across
different granularities. Extensive experiments on various challenging tasks
demonstrate the superior performance of our proposed GPI-Net in comparison to
existing methods. The code will be released at https://github.com/gwk/GPI-Net.

</details>


### [11] [Adaptive 3D Gaussian Splatting Video Streaming: Visual Saliency-Aware Tiling and Meta-Learning-Based Bitrate Adaptation](https://arxiv.org/abs/2507.14454)
*Han Gong,Qiyue Li,Jie Li,Zhi Liu*

Main category: cs.CV

TL;DR: The paper addresses challenges in 3D Gaussian splatting (3DGS) video streaming, proposing adaptive tiling, quality assessment, and bitrate adaptation solutions to enhance immersive 3D video experiences.


<details>
  <summary>Details</summary>
Motivation: 3DGS streaming is a growing research area, but lacks solutions for tiling, quality assessment, and bitrate adaptation, hindering optimal performance.

Method: Proposes adaptive 3DGS tiling with saliency analysis, a quality assessment framework for 3DGS, and a meta-learning-based bitrate algorithm.

Result: The solutions outperform state-of-the-art methods in experiments.

Conclusion: The proposed approaches effectively tackle key challenges in 3DGS streaming, advancing the field.

Abstract: 3D Gaussian splatting video (3DGS) streaming has recently emerged as a
research hotspot in both academia and industry, owing to its impressive ability
to deliver immersive 3D video experiences. However, research in this area is
still in its early stages, and several fundamental challenges, such as tiling,
quality assessment, and bitrate adaptation, require further investigation. In
this paper, we tackle these challenges by proposing a comprehensive set of
solutions. Specifically, we propose an adaptive 3DGS tiling technique guided by
saliency analysis, which integrates both spatial and temporal features. Each
tile is encoded into versions possessing dedicated deformation fields and
multiple quality levels for adaptive selection. We also introduce a novel
quality assessment framework for 3DGS video that jointly evaluates
spatial-domain degradation in 3DGS representations during streaming and the
quality of the resulting 2D rendered images. Additionally, we develop a
meta-learning-based adaptive bitrate algorithm specifically tailored for 3DGS
video streaming, achieving optimal performance across varying network
conditions. Extensive experiments demonstrate that our proposed approaches
significantly outperform state-of-the-art methods.

</details>


### [12] [GEMINUS: Dual-aware Global and Scene-Adaptive Mixture-of-Experts for End-to-End Autonomous Driving](https://arxiv.org/abs/2507.14456)
*Chi Wan,Yixin Cui,Jiatong Du,Shuo Yang,Yulong Bai,Yanjun Huang*

Main category: cs.CV

TL;DR: GEMINUS is a Mixture-of-Experts framework for autonomous driving, combining a Global Expert and Scene-Adaptive Experts with a Dual-aware Router, achieving adaptive and robust performance in diverse scenarios.


<details>
  <summary>Details</summary>
Motivation: Single-mode planning struggles with diverse scenarios, necessitating a framework that adapts to varied driving conditions.

Method: Uses a Global Expert for robustness, Scene-Adaptive Experts for adaptability, and a Dual-aware Router to dynamically activate experts.

Result: Outperforms existing methods in Bench2Drive, achieving top scores in Driving Score and Success Rate with monocular vision.

Conclusion: GEMINUS effectively combines adaptability and robustness, significantly improving over single-expert baselines.

Abstract: End-to-end autonomous driving requires adaptive and robust handling of
complex and diverse traffic environments. However, prevalent single-mode
planning methods attempt to learn an overall policy while struggling to acquire
diversified driving skills to handle diverse scenarios. Therefore, this paper
proposes GEMINUS, a Mixture-of-Experts end-to-end autonomous driving framework
featuring a Global Expert, a Scene-Adaptive Experts Group, and equipped with a
Dual-aware Router. Specifically, the Global Expert is trained on the overall
dataset, possessing robust performance. The Scene-Adaptive Experts are trained
on corresponding scene subsets, achieving adaptive performance. The Dual-aware
Router simultaneously considers scenario-level features and routing uncertainty
to dynamically activate expert modules. Through the effective coupling of the
Global Expert and the Scene-Adaptive Experts Group via the Dual-aware Router,
GEMINUS achieves adaptive and robust performance in diverse scenarios. GEMINUS
outperforms existing methods in the Bench2Drive closed-loop benchmark and
achieves state-of-the-art performance in Driving Score and Success Rate, even
with only monocular vision input. Furthermore, ablation studies demonstrate
significant improvements over the original single-expert baseline: 7.67% in
Driving Score, 22.06% in Success Rate, and 19.41% in MultiAbility-Mean. The
code will be available at https://github.com/newbrains1/GEMINUS.

</details>


### [13] [VisGuard: Securing Visualization Dissemination through Tamper-Resistant Data Retrieval](https://arxiv.org/abs/2507.14459)
*Huayuan Ye,Juntong Chen,Shenzhuo Zhang,Yipeng Zhang,Changbo Wang,Chenhui Li*

Main category: cs.CV

TL;DR: VisGuard is a tamper-resistant framework for embedding metadata links in visualization images, ensuring recoverability even after tampering.


<details>
  <summary>Details</summary>
Motivation: Current methods for embedding metadata in visualization images are fragile to tampering, limiting their practicality.

Method: VisGuard uses repetitive data tiling, invertible information broadcasting, and an anchor-based crop localization scheme.

Result: VisGuard shows superior performance in retrieval accuracy, embedding capacity, and security against tampering.

Conclusion: VisGuard effectively safeguards visualization dissemination and enhances information conveyance.

Abstract: The dissemination of visualizations is primarily in the form of raster
images, which often results in the loss of critical information such as source
code, interactive features, and metadata. While previous methods have proposed
embedding metadata into images to facilitate Visualization Image Data Retrieval
(VIDR), most existing methods lack practicability since they are fragile to
common image tampering during online distribution such as cropping and editing.
To address this issue, we propose VisGuard, a tamper-resistant VIDR framework
that reliably embeds metadata link into visualization images. The embedded data
link remains recoverable even after substantial tampering upon images. We
propose several techniques to enhance robustness, including repetitive data
tiling, invertible information broadcasting, and an anchor-based scheme for
crop localization. VisGuard enables various applications, including interactive
chart reconstruction, tampering detection, and copyright protection. We conduct
comprehensive experiments on VisGuard's superior performance in data retrieval
accuracy, embedding capacity, and security against tampering and steganalysis,
demonstrating VisGuard's competence in facilitating and safeguarding
visualization dissemination and information conveyance.

</details>


### [14] [OptiCorNet: Optimizing Sequence-Based Context Correlation for Visual Place Recognition](https://arxiv.org/abs/2507.14477)
*Zhenyu Li,Tianyi Shang,Pengjie Xu,Ruirui Zhang,Fanchen Kong*

Main category: cs.CV

TL;DR: OptiCorNet introduces a sequence modeling framework for VPR, combining spatial feature extraction and temporal differencing into an end-to-end trainable module, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of VPR in dynamic and perceptually aliased environments by leveraging temporal coherence in image sequences, which existing single-frame methods neglect.

Method: Uses a lightweight 1D convolutional encoder and a learnable differential temporal operator (DSD) to capture spatial and temporal context, refined with LSTM and quadruplet loss for discriminative descriptors.

Result: Outperforms state-of-the-art baselines in challenging conditions like seasonal and viewpoint variations.

Conclusion: OptiCorNet effectively learns sequence-level embeddings, improving end-to-end place recognition in dynamic environments.

Abstract: Visual Place Recognition (VPR) in dynamic and perceptually aliased
environments remains a fundamental challenge for long-term localization.
Existing deep learning-based solutions predominantly focus on single-frame
embeddings, neglecting the temporal coherence present in image sequences. This
paper presents OptiCorNet, a novel sequence modeling framework that unifies
spatial feature extraction and temporal differencing into a differentiable,
end-to-end trainable module. Central to our approach is a lightweight 1D
convolutional encoder combined with a learnable differential temporal operator,
termed Differentiable Sequence Delta (DSD), which jointly captures short-term
spatial context and long-range temporal transitions. The DSD module models
directional differences across sequences via a fixed-weight differencing
kernel, followed by an LSTM-based refinement and optional residual projection,
yielding compact, discriminative descriptors robust to viewpoint and appearance
shifts. To further enhance inter-class separability, we incorporate a
quadruplet loss that optimizes both positive alignment and multi-negative
divergence within each batch. Unlike prior VPR methods that treat temporal
aggregation as post-processing, OptiCorNet learns sequence-level embeddings
directly, enabling more effective end-to-end place recognition. Comprehensive
evaluations on multiple public benchmarks demonstrate that our approach
outperforms state-of-the-art baselines under challenging seasonal and viewpoint
variations.

</details>


### [15] [DFQ-ViT: Data-Free Quantization for Vision Transformers without Fine-tuning](https://arxiv.org/abs/2507.14481)
*Yujia Tong,Jingling Yuan,Tian Zhang,Jianquan Liu,Chuang Hu*

Main category: cs.CV

TL;DR: DFQ-ViT improves data-free quantization for Vision Transformers by enhancing synthetic data quality and aligning activations, outperforming existing methods without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Existing DFQ methods for ViTs struggle with synthetic data quality and activation distribution mismatches, leading to performance degradation.

Method: Proposes DFQ-ViT: synthesizes samples by difficulty and uses an activation correction matrix to align quantized and full-precision model activations.

Result: DFQ-ViT outperforms state-of-the-art DFQ methods, e.g., 4.29% higher accuracy for 3-bit DeiT-T, and matches real-data quantization performance.

Conclusion: DFQ-ViT offers efficient, fine-tuning-free quantization, reducing computational overhead and deployment barriers for edge devices, aligning with Green Learning principles.

Abstract: Data-Free Quantization (DFQ) enables the quantization of Vision Transformers
(ViTs) without requiring access to data, allowing for the deployment of ViTs on
devices with limited resources. In DFQ, the quantization model must be
calibrated using synthetic samples, making the quality of these synthetic
samples crucial. Existing methods fail to fully capture and balance the global
and local features within the samples, resulting in limited synthetic data
quality. Moreover, we have found that during inference, there is a significant
difference in the distributions of intermediate layer activations between the
quantized and full-precision models. These issues lead to a severe performance
degradation of the quantized model. To address these problems, we propose a
pipeline for Data-Free Quantization for Vision Transformers (DFQ-ViT).
Specifically, we synthesize samples in order of increasing difficulty,
effectively enhancing the quality of synthetic data. During the calibration and
inference stage, we introduce the activation correction matrix for the
quantized model to align the intermediate layer activations with those of the
full-precision model. Extensive experiments demonstrate that DFQ-ViT achieves
remarkable superiority over existing DFQ methods and its performance is on par
with models quantized through real data. For example, the performance of DeiT-T
with 3-bit weights quantization is 4.29% higher than the state-of-the-art. Our
method eliminates the need for fine-tuning, which not only reduces
computational overhead but also lowers the deployment barriers for edge
devices. This characteristic aligns with the principles of Green Learning by
improving energy efficiency and facilitating real-world applications in
resource-constrained environments.

</details>


### [16] [Benefit from Reference: Retrieval-Augmented Cross-modal Point Cloud Completion](https://arxiv.org/abs/2507.14485)
*Hongye Hou,Liu Zhan,Yang Yang*

Main category: cs.CV

TL;DR: A novel retrieval-augmented point cloud completion framework is proposed, leveraging cross-modal retrieval to enhance structural feature learning and generate fine-grained point clouds.


<details>
  <summary>Details</summary>
Motivation: Existing methods for 3D point cloud completion are limited by their focus on specific input classes, lacking generalization. Cross-modal learning is explored to improve structural feature learning.

Method: The framework includes a Structural Shared Feature Encoder (SSFE) for cross-modal feature extraction and a Progressive Retrieval-Augmented Generator (PRAG) for hierarchical feature fusion. A dual-channel control gate in SSFE enhances relevant features and suppresses noise.

Result: The method demonstrates effectiveness in generating fine-grained point clouds and shows strong generalization with sparse data and unseen categories.

Conclusion: The proposed framework advances point cloud completion by integrating cross-modal retrieval and hierarchical feature fusion, improving both detail and generalization.

Abstract: Completing the whole 3D structure based on an incomplete point cloud is a
challenging task, particularly when the residual point cloud lacks typical
structural characteristics. Recent methods based on cross-modal learning
attempt to introduce instance images to aid the structure feature learning.
However, they still focus on each particular input class, limiting their
generation abilities. In this work, we propose a novel retrieval-augmented
point cloud completion framework. The core idea is to incorporate cross-modal
retrieval into completion task to learn structural prior information from
similar reference samples. Specifically, we design a Structural Shared Feature
Encoder (SSFE) to jointly extract cross-modal features and reconstruct
reference features as priors. Benefiting from a dual-channel control gate in
the encoder, relevant structural features in the reference sample are enhanced
and irrelevant information interference is suppressed. In addition, we propose
a Progressive Retrieval-Augmented Generator (PRAG) that employs a hierarchical
feature fusion mechanism to integrate reference prior information with input
features from global to local. Through extensive evaluations on multiple
datasets and real-world scenes, our method shows its effectiveness in
generating fine-grained point clouds, as well as its generalization capability
in handling sparse data and unseen categories.

</details>


### [17] [Efficient Whole Slide Pathology VQA via Token Compression](https://arxiv.org/abs/2507.14497)
*Weimin Lyu,Qingqiao Hu,Kehan Qi,Zhan Shi,Wentao Huang,Saumya Gupta,Chao Chen*

Main category: cs.CV

TL;DR: TCP-LLaVA introduces token compression for WSI VQA, reducing computational costs while improving accuracy.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of high-resolution WSIs in MLLMs, such as long context length and resource demands.

Method: Uses trainable compression tokens to aggregate visual/textual info, inspired by BERT's [CLS] token. Only compressed tokens are fed to the LLM.

Result: Outperforms baselines in VQA accuracy on TCGA tumor subtypes and reduces resource use.

Conclusion: TCP-LLaVA is an efficient MLLM solution for WSI VQA, balancing performance and computational cost.

Abstract: Whole-slide images (WSIs) in pathology can reach up to 10,000 x 10,000
pixels, posing significant challenges for multimodal large language model
(MLLM) due to long context length and high computational demands. Previous
methods typically focus on patch-level analysis or slide-level classification
using CLIP-based models with multi-instance learning, but they lack the
generative capabilities needed for visual question answering (VQA). More recent
MLLM-based approaches address VQA by feeding thousands of patch tokens directly
into the language model, which leads to excessive resource consumption. To
address these limitations, we propose Token Compression Pathology LLaVA
(TCP-LLaVA), the first MLLM architecture to perform WSI VQA via token
compression. TCP-LLaVA introduces a set of trainable compression tokens that
aggregate visual and textual information through a modality compression module,
inspired by the [CLS] token mechanism in BERT. Only the compressed tokens are
forwarded to the LLM for answer generation, significantly reducing input length
and computational cost. Experiments on ten TCGA tumor subtypes show that
TCP-LLaVA outperforms existing MLLM baselines in VQA accuracy while reducing
training resource consumption by a substantial margin.

</details>


### [18] [Motion Segmentation and Egomotion Estimation from Event-Based Normal Flow](https://arxiv.org/abs/2507.14500)
*Zhiyuan Hua,Dehao Yuan,Cornelia Fermüller*

Main category: cs.CV

TL;DR: A robust framework for motion segmentation and egomotion estimation using event-based normal flow, leveraging sparse event data and geometric constraints for accurate results without full optical flow.


<details>
  <summary>Details</summary>
Motivation: Traditional methods rely on optical flow or depth estimation, which can be computationally intensive. The paper aims to exploit high-temporal-resolution event data for efficient motion segmentation and egomotion estimation.

Method: An optimization-based pipeline iteratively segments events, isolates moving objects via residual analysis, and refines segmentations using hierarchical clustering with motion similarity and temporal consistency.

Result: Validated on the EVIMO2v2 dataset, the method achieves accurate segmentation and translational motion estimation without full optical flow, excelling at object boundaries.

Conclusion: The approach is advantageous for scalable, real-time robotic and navigation applications due to its efficiency and accuracy.

Abstract: This paper introduces a robust framework for motion segmentation and
egomotion estimation using event-based normal flow, tailored specifically for
neuromorphic vision sensors. In contrast to traditional methods that rely
heavily on optical flow or explicit depth estimation, our approach exploits the
sparse, high-temporal-resolution event data and incorporates geometric
constraints between normal flow, scene structure, and inertial measurements.
The proposed optimization-based pipeline iteratively performs event
over-segmentation, isolates independently moving objects via residual analysis,
and refines segmentations using hierarchical clustering informed by motion
similarity and temporal consistency. Experimental results on the EVIMO2v2
dataset validate that our method achieves accurate segmentation and
translational motion estimation without requiring full optical flow
computation. This approach demonstrates significant advantages at object
boundaries and offers considerable potential for scalable, real-time robotic
and navigation applications.

</details>


### [19] [Advances in Feed-Forward 3D Reconstruction and View Synthesis: A Survey](https://arxiv.org/abs/2507.14501)
*Jiahui Zhang,Yuelei Li,Anpei Chen,Muyu Xu,Kunhao Liu,Jianyuan Wang,Xiao-Xiao Long,Hanxue Liang,Zexiang Xu,Hao Su,Christian Theobalt,Christian Rupprecht,Andrea Vedaldi,Hanspeter Pfister,Shijian Lu,Fangneng Zhan*

Main category: cs.CV

TL;DR: A survey on feed-forward deep learning techniques for 3D reconstruction and view synthesis, covering representations like NeRF and 3DGS, applications in AR/VR, and future challenges.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of traditional iterative optimization methods in 3D vision tasks by leveraging fast, generalizable deep learning approaches.

Method: Reviews feed-forward techniques, categorizing them by representation architectures (e.g., NeRF, 3DGS) and tasks like pose-free reconstruction.

Result: Highlights advancements in speed and applicability for real-world scenarios, with applications in digital humans, robotics, and more.

Conclusion: Identifies open challenges and future directions, emphasizing the transformative potential of feed-forward methods in 3D vision.

Abstract: 3D reconstruction and view synthesis are foundational problems in computer
vision, graphics, and immersive technologies such as augmented reality (AR),
virtual reality (VR), and digital twins. Traditional methods rely on
computationally intensive iterative optimization in a complex chain, limiting
their applicability in real-world scenarios. Recent advances in feed-forward
approaches, driven by deep learning, have revolutionized this field by enabling
fast and generalizable 3D reconstruction and view synthesis. This survey offers
a comprehensive review of feed-forward techniques for 3D reconstruction and
view synthesis, with a taxonomy according to the underlying representation
architectures including point cloud, 3D Gaussian Splatting (3DGS), Neural
Radiance Fields (NeRF), etc. We examine key tasks such as pose-free
reconstruction, dynamic 3D reconstruction, and 3D-aware image and video
synthesis, highlighting their applications in digital humans, SLAM, robotics,
and beyond. In addition, we review commonly used datasets with detailed
statistics, along with evaluation protocols for various downstream tasks. We
conclude by discussing open research challenges and promising directions for
future work, emphasizing the potential of feed-forward approaches to advance
the state of the art in 3D vision.

</details>


### [20] [DCHM: Depth-Consistent Human Modeling for Multiview Detection](https://arxiv.org/abs/2507.14505)
*Jiahao Ma,Tianyu Wang,Miaomiao Liu,David Ahmedt-Aristizabal,Chuong Nguyen*

Main category: cs.CV

TL;DR: DCHM improves multiview pedestrian detection by ensuring depth consistency and reducing noise in human modeling without relying on costly annotations.


<details>
  <summary>Details</summary>
Motivation: Existing methods for human modeling in multiview pedestrian detection introduce noise and lack generalization. DCHM aims to eliminate reliance on human-labeled annotations and improve accuracy.

Method: DCHM uses a pipeline with superpixel-wise Gaussian Splatting for consistent depth estimation and multiview fusion in global coordinates.

Result: DCHM significantly reduces noise, outperforms state-of-the-art baselines, and is the first to reconstruct pedestrians in challenging settings.

Conclusion: DCHM provides a robust solution for accurate human modeling and pedestrian localization in multiview scenarios.

Abstract: Multiview pedestrian detection typically involves two stages: human modeling
and pedestrian localization. Human modeling represents pedestrians in 3D space
by fusing multiview information, making its quality crucial for detection
accuracy. However, existing methods often introduce noise and have low
precision. While some approaches reduce noise by fitting on costly multiview 3D
annotations, they often struggle to generalize across diverse scenes. To
eliminate reliance on human-labeled annotations and accurately model humans, we
propose Depth-Consistent Human Modeling (DCHM), a framework designed for
consistent depth estimation and multiview fusion in global coordinates.
Specifically, our proposed pipeline with superpixel-wise Gaussian Splatting
achieves multiview depth consistency in sparse-view, large-scaled, and crowded
scenarios, producing precise point clouds for pedestrian localization.
Extensive validations demonstrate that our method significantly reduces noise
during human modeling, outperforming previous state-of-the-art baselines.
Additionally, to our knowledge, DCHM is the first to reconstruct pedestrians
and perform multiview segmentation in such a challenging setting. Code is
available on the \href{https://jiahao-ma.github.io/DCHM/}{project page}.

</details>


### [21] [ArtiMuse: Fine-Grained Image Aesthetics Assessment with Joint Scoring and Expert-Level Understanding](https://arxiv.org/abs/2507.14533)
*Shuo Cao,Nan Ma,Jiayang Li,Xiaohui Li,Lihao Shao,Kaiwen Zhu,Yu Zhou,Yuandong Pu,Jiarui Wu,Jiaquan Wang,Bo Qu,Wenhai Wang,Yu Qiao,Dajuin Yao,Yihao Liu*

Main category: cs.CV

TL;DR: ArtiMuse is a new MLLM-based IAA model offering joint scoring and expert-level understanding, addressing modality bias and lack of fine-grained analysis. It introduces ArtiMuse-10K, a 10K-image expert-curated dataset with detailed annotations.


<details>
  <summary>Details</summary>
Motivation: The rise of AIGC and educational/artistic applications demands advanced IAA methods with quantitative scoring and professional insights, which current MLLM-based methods lack due to modality bias and insufficient attribute decomposition.

Method: Proposes ArtiMuse, an MLLM-based IAA model combining scoring and expert-level understanding, and ArtiMuse-10K, a dataset with 10K expert-annotated images across 5 categories and 15 subcategories, featuring 8D attribute analysis.

Result: ArtiMuse addresses limitations of existing MLLM-based IAA methods by providing comprehensive scoring and expert insights, supported by a high-quality dataset.

Conclusion: ArtiMuse and ArtiMuse-10K advance IAA by enabling detailed aesthetic assessment and will be publicly released to benefit the field.

Abstract: The rapid advancement of educational applications, artistic creation, and
AI-generated content (AIGC) technologies has substantially increased practical
requirements for comprehensive Image Aesthetics Assessment (IAA), particularly
demanding methods capable of delivering both quantitative scoring and
professional understanding. Multimodal Large Language Model (MLLM)-based IAA
methods demonstrate stronger perceptual and generalization capabilities
compared to traditional approaches, yet they suffer from modality bias
(score-only or text-only) and lack fine-grained attribute decomposition,
thereby failing to support further aesthetic assessment. In this paper, we
present:(1) ArtiMuse, an innovative MLLM-based IAA model with Joint Scoring and
Expert-Level Understanding capabilities; (2) ArtiMuse-10K, the first
expert-curated image aesthetic dataset comprising 10,000 images spanning 5 main
categories and 15 subcategories, each annotated by professional experts with
8-dimensional attributes analysis and a holistic score. Both the model and
dataset will be made public to advance the field.

</details>


### [22] [Real Time Captioning of Sign Language Gestures in Video Meetings](https://arxiv.org/abs/2507.14543)
*Sharanya Mukherjee,Md Hishaam Akhtar,Kannadasan R*

Main category: cs.CV

TL;DR: A browser extension for translating sign language to subtitles in video calls, addressing communication barriers for the hearing-impaired.


<details>
  <summary>Details</summary>
Motivation: The communication gap between hearing-impaired individuals and others, exacerbated by the pandemic's shift to video calls, where signing is preferred over typing.

Method: Uses a large-scale dataset of 2000+ Word-Level ASL videos from 100+ signers to train the system for automatic translation.

Result: Proposes a solution to enable real-time sign language translation in video meetings.

Conclusion: The extension aims to improve accessibility and inclusivity in digital communication for the hearing-impaired.

Abstract: It has always been a rather tough task to communicate with someone possessing
a hearing impairment. One of the most tested ways to establish such a
communication is through the use of sign based languages. However, not many
people are aware of the smaller intricacies involved with sign language. Sign
language recognition using computer vision aims at eliminating the
communication barrier between deaf-mute and ordinary people so that they can
properly communicate with others. Recently the pandemic has left the whole
world shaken up and has transformed the way we communicate. Video meetings have
become essential for everyone, even people with a hearing disability. In recent
studies, it has been found that people with hearing disabilities prefer to sign
over typing during these video calls. In this paper, we are proposing a browser
extension that will automatically translate sign language to subtitles for
everyone else in the video call. The Large-scale dataset which contains more
than 2000 Word-Level ASL videos, which were performed by over 100 signers will
be used.

</details>


### [23] [Multimodal AI for Gastrointestinal Diagnostics: Tackling VQA in MEDVQA-GI 2025](https://arxiv.org/abs/2507.14544)
*Sujata Gaihre,Amir Thapa Magar,Prasuna Pokharel,Laxmi Tiwari*

Main category: cs.CV

TL;DR: The paper presents a VQA approach for gastrointestinal endoscopy using the Florence model, achieving strong results on the KASVIR dataset.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of visual question answering in medical endoscopy by leveraging a large multimodal model.

Method: Uses the Florence model with domain-specific augmentations for training diversity and fine-tunes it on the KASVIR dataset.

Result: Fine-tuned Florence achieves accurate responses on challenge metrics, demonstrating its potential for medical VQA.

Conclusion: The work establishes a strong baseline for future research on explainability, robustness, and clinical integration in medical VQA.

Abstract: This paper describes our approach to Subtask 1 of the ImageCLEFmed MEDVQA
2025 Challenge, which targets visual question answering (VQA) for
gastrointestinal endoscopy. We adopt the Florence model-a large-scale
multimodal foundation model-as the backbone of our VQA pipeline, pairing a
powerful vision encoder with a text encoder to interpret endoscopic images and
produce clinically relevant answers. To improve generalization, we apply
domain-specific augmentations that preserve medical features while increasing
training diversity. Experiments on the KASVIR dataset show that fine-tuning
Florence yields accurate responses on the official challenge metrics. Our
results highlight the potential of large multimodal models in medical VQA and
provide a strong baseline for future work on explainability, robustness, and
clinical integration. The code is publicly available at:
https://github.com/TiwariLaxuu/VQA-Florence.git

</details>


### [24] [Synthesizing Images on Perceptual Boundaries of ANNs for Uncovering Human Perceptual Variability on Facial Expressions](https://arxiv.org/abs/2507.14549)
*Haotian Deng,Chi Zhang,Chen Wei,Quanying Liu*

Main category: cs.CV

TL;DR: The study explores how ambiguous facial expression stimuli, identified by ANN decision boundaries, correlate with human perceptual variability, revealing shared computational principles in emotion perception.


<details>
  <summary>Details</summary>
Motivation: To address the gap in understanding inter-individual differences in emotion perception and the alignment between ANN models and human perceptual variability.

Method: Introduces a perceptual boundary sampling method to create ambiguous facial expression stimuli (varEmotion dataset) and conducts large-scale human experiments.

Result: ANN-confusing stimuli also cause perceptual uncertainty in humans, and fine-tuning ANNs with behavioral data aligns ANN predictions with human perceptual patterns.

Conclusion: The findings link ANN decision boundaries to human perceptual variability, advancing personalized modeling of emotional interpretation.

Abstract: A fundamental challenge in affective cognitive science is to develop models
that accurately capture the relationship between external emotional stimuli and
human internal experiences. While ANNs have demonstrated remarkable accuracy in
facial expression recognition, their ability to model inter-individual
differences in human perception remains underexplored. This study investigates
the phenomenon of high perceptual variability-where individuals exhibit
significant differences in emotion categorization even when viewing the same
stimulus. Inspired by the similarity between ANNs and human perception, we
hypothesize that facial expression samples that are ambiguous for ANN
classifiers also elicit divergent perceptual judgments among human observers.
To examine this hypothesis, we introduce a novel perceptual boundary sampling
method to generate facial expression stimuli that lie along ANN decision
boundaries. These ambiguous samples form the basis of the varEmotion dataset,
constructed through large-scale human behavioral experiments. Our analysis
reveals that these ANN-confusing stimuli also provoke heightened perceptual
uncertainty in human participants, highlighting shared computational principles
in emotion perception. Finally, by fine-tuning ANN representations using
behavioral data, we achieve alignment between ANN predictions and both
group-level and individual-level human perceptual patterns. Our findings
establish a systematic link between ANN decision boundaries and human
perceptual variability, offering new insights into personalized modeling of
emotional interpretation.

</details>


### [25] [Clutter Detection and Removal by Multi-Objective Analysis for Photographic Guidance](https://arxiv.org/abs/2507.14553)
*Xiaoran Wu*

Main category: cs.CV

TL;DR: A camera guidance system helps users identify and remove clutter in photos using aesthetic evaluation and image inpainting, improving photo quality.


<details>
  <summary>Details</summary>
Motivation: Clutter in photos distracts from intended emotions or stories, especially for amateurs. The goal is to provide tools for clutter identification and removal.

Method: The system uses a clutter distinguishment algorithm with aesthetic evaluations and an iterative image inpainting algorithm based on GANs for high-resolution image reconstruction.

Result: User studies show the system helps users identify distractions and take higher quality photos more efficiently.

Conclusion: The system offers flexible interfaces and accurate algorithms to enhance photo aesthetics by addressing clutter.

Abstract: Clutter in photos is a distraction preventing photographers from conveying
the intended emotions or stories to the audience. Photography amateurs
frequently include clutter in their photos due to unconscious negligence or the
lack of experience in creating a decluttered, aesthetically appealing scene for
shooting. We are thus motivated to develop a camera guidance system that
provides solutions and guidance for clutter identification and removal. We
estimate and visualize the contribution of objects to the overall aesthetics
and content of a photo, based on which users can interactively identify
clutter. Suggestions on getting rid of clutter, as well as a tool that removes
cluttered objects computationally, are provided to guide users to deal with
different kinds of clutter and improve their photographic work. Two technical
novelties underpin interactions in our system: a clutter distinguishment
algorithm with aesthetics evaluations for objects and an iterative image
inpainting algorithm based on generative adversarial nets that reconstructs
missing regions of removed objects for high-resolution images. User studies
demonstrate that our system provides flexible interfaces and accurate
algorithms that allow users to better identify distractions and take higher
quality images within less time.

</details>


### [26] [Descrip3D: Enhancing Large Language Model-based 3D Scene Understanding with Object-Level Text Descriptions](https://arxiv.org/abs/2507.14555)
*Jintang Xue,Ganning Zhao,Jie-En Yao,Hong-En Chen,Yue Hu,Meida Chen,Suya You,C. -C. Jay Kuo*

Main category: cs.CV

TL;DR: Descrip3D introduces a framework using natural language to encode object relationships in 3D scenes, outperforming baselines on multiple tasks.


<details>
  <summary>Details</summary>
Motivation: Current 3D scene-language models lack relational understanding; Descrip3D addresses this by incorporating textual descriptions for objects.

Method: Descrip3D uses textual descriptions for objects, integrating them via embedding fusion and prompt-level injection for unified reasoning.

Result: Outperforms baselines on five benchmark datasets (ScanRefer, Multi3DRefer, ScanQA, SQA3D, Scan2Cap).

Conclusion: Language-guided relational representation is effective for understanding complex indoor scenes.

Abstract: Understanding 3D scenes goes beyond simply recognizing objects; it requires
reasoning about the spatial and semantic relationships between them. Current 3D
scene-language models often struggle with this relational understanding,
particularly when visual embeddings alone do not adequately convey the roles
and interactions of objects. In this paper, we introduce Descrip3D, a novel and
powerful framework that explicitly encodes the relationships between objects
using natural language. Unlike previous methods that rely only on 2D and 3D
embeddings, Descrip3D enhances each object with a textual description that
captures both its intrinsic attributes and contextual relationships. These
relational cues are incorporated into the model through a dual-level
integration: embedding fusion and prompt-level injection. This allows for
unified reasoning across various tasks such as grounding, captioning, and
question answering, all without the need for task-specific heads or additional
supervision. When evaluated on five benchmark datasets, including ScanRefer,
Multi3DRefer, ScanQA, SQA3D, and Scan2Cap, Descrip3D consistently outperforms
strong baseline models, demonstrating the effectiveness of language-guided
relational representation for understanding complex indoor scenes.

</details>


### [27] [LEAD: Exploring Logit Space Evolution for Model Selection](https://arxiv.org/abs/2507.14559)
*Zixuan Hu,Xiaotong Li,Shixiang Tang,Jun Liu,Yichun Hu,Ling-Yu Duan*

Main category: cs.CV

TL;DR: LEAD is a finetuning-aligned method using logits to model nonlinear fine-tuning dynamics, outperforming existing linear approaches.


<details>
  <summary>Details</summary>
Motivation: The challenge of selecting suitable pre-trained models for downstream tasks due to the inefficiency of current linear methods in capturing fine-tuning dynamics.

Method: LEAD models fine-tuning dynamics via logits, using an ODE framework for nonlinear optimization and class-aware decomposition for practical use.

Result: Outperforms existing methods on 24 pre-trained models across 10 datasets, even in low-data scenarios.

Conclusion: LEAD effectively bridges the optimization gap in model transferability, offering a concise and adaptable solution.

Abstract: The remarkable success of pretrain-then-finetune paradigm has led to a
proliferation of available pre-trained models for vision tasks. This surge
presents a significant challenge in efficiently choosing the most suitable
pre-trained models for downstream tasks. The critical aspect of this challenge
lies in effectively predicting the model transferability by considering the
underlying fine-tuning dynamics. Existing methods often model fine-tuning
dynamics in feature space with linear transformations, which do not precisely
align with the fine-tuning objective and fail to grasp the essential
nonlinearity from optimization. To this end, we present LEAD, a
finetuning-aligned approach based on the network output of logits. LEAD
proposes a theoretical framework to model the optimization process and derives
an ordinary differential equation (ODE) to depict the nonlinear evolution
toward the final logit state. Additionally, we design a class-aware
decomposition method to consider the varying evolution dynamics across classes
and further ensure practical applicability. Integrating the closely aligned
optimization objective and nonlinear modeling capabilities derived from the
differential equation, our method offers a concise solution to effectively
bridge the optimization gap in a single step, bypassing the lengthy fine-tuning
process. The comprehensive experiments on 24 supervised and self-supervised
pre-trained models across 10 downstream datasets demonstrate impressive
performances and showcase its broad adaptability even in low-data scenarios.

</details>


### [28] [Benchmarking GANs, Diffusion Models, and Flow Matching for T1w-to-T2w MRI Translation](https://arxiv.org/abs/2507.14575)
*Andrea Moschetto,Lemuel Puglisi,Alec Sargood,Pierluigi Dell'Acqua,Francesco Guarnera,Sebastiano Battiato,Daniele Ravì*

Main category: cs.CV

TL;DR: The paper benchmarks GANs, diffusion models, and flow matching for T1w-to-T2w MRI synthesis, finding GANs (Pix2Pix) superior in fidelity, quality, and efficiency.


<details>
  <summary>Details</summary>
Motivation: Reducing MRI scan time and cost by computationally synthesizing missing contrasts from acquired ones.

Method: Comparative evaluation of GANs, diffusion models, and flow matching for 2D MRI image-to-image translation on three public datasets.

Result: GAN-based Pix2Pix outperforms diffusion and flow matching in structural fidelity, image quality, and computational efficiency.

Conclusion: GANs are currently more practical for MRI synthesis; flow-based methods may need more data to compete. Findings guide real-world deployment and future research.

Abstract: Magnetic Resonance Imaging (MRI) enables the acquisition of multiple image
contrasts, such as T1-weighted (T1w) and T2-weighted (T2w) scans, each offering
distinct diagnostic insights. However, acquiring all desired modalities
increases scan time and cost, motivating research into computational methods
for cross-modal synthesis. To address this, recent approaches aim to synthesize
missing MRI contrasts from those already acquired, reducing acquisition time
while preserving diagnostic quality. Image-to-image (I2I) translation provides
a promising framework for this task. In this paper, we present a comprehensive
benchmark of generative models$\unicode{x2013}$specifically, Generative
Adversarial Networks (GANs), diffusion models, and flow matching (FM)
techniques$\unicode{x2013}$for T1w-to-T2w 2D MRI I2I translation. All
frameworks are implemented with comparable settings and evaluated on three
publicly available MRI datasets of healthy adults. Our quantitative and
qualitative analyses show that the GAN-based Pix2Pix model outperforms
diffusion and FM-based methods in terms of structural fidelity, image quality,
and computational efficiency. Consistent with existing literature, these
results suggest that flow-based models are prone to overfitting on small
datasets and simpler tasks, and may require more data to match or surpass GAN
performance. These findings offer practical guidance for deploying I2I
translation techniques in real-world MRI workflows and highlight promising
directions for future research in cross-modal medical image synthesis. Code and
models are publicly available at
https://github.com/AndreaMoschetto/medical-I2I-benchmark.

</details>


### [29] [Performance comparison of medical image classification systems using TensorFlow Keras, PyTorch, and JAX](https://arxiv.org/abs/2507.14587)
*Merjem Bećirović,Amina Kurtović,Nordin Smajlović,Medina Kapo,Amila Akagić*

Main category: cs.CV

TL;DR: The paper compares TensorFlow, PyTorch, and JAX for blood cell image classification, analyzing inference time and accuracy, with JAX and PyTorch performing well.


<details>
  <summary>Details</summary>
Motivation: To evaluate the performance of deep learning frameworks (TensorFlow, PyTorch, JAX) in blood cell image classification, addressing a gap in detailed analysis.

Method: Comparison of three frameworks on the BloodMNIST dataset, focusing on inference time and classification accuracy across image sizes.

Result: JAX and PyTorch showed comparable accuracy to benchmarks, with performance variations due to image resolution and framework optimizations.

Conclusion: JAX and PyTorch are efficient for medical image classification, with framework choice impacting performance based on specific needs.

Abstract: Medical imaging plays a vital role in early disease diagnosis and monitoring.
Specifically, blood microscopy offers valuable insights into blood cell
morphology and the detection of hematological disorders. In recent years, deep
learning-based automated classification systems have demonstrated high
potential in enhancing the accuracy and efficiency of blood image analysis.
However, a detailed performance analysis of specific deep learning frameworks
appears to be lacking. This paper compares the performance of three popular
deep learning frameworks, TensorFlow with Keras, PyTorch, and JAX, in
classifying blood cell images from the publicly available BloodMNIST dataset.
The study primarily focuses on inference time differences, but also
classification performance for different image sizes. The results reveal
variations in performance across frameworks, influenced by factors such as
image resolution and framework-specific optimizations. Classification accuracy
for JAX and PyTorch was comparable to current benchmarks, showcasing the
efficiency of these frameworks for medical image classification.

</details>


### [30] [DiSCO-3D : Discovering and segmenting Sub-Concepts from Open-vocabulary queries in NeRF](https://arxiv.org/abs/2507.14596)
*Doriand Petit,Steve Bourgeois,Vincent Gay-Bellile,Florian Chabot,Loïc Barthe*

Main category: cs.CV

TL;DR: DiSCO-3D is a novel method for 3D Open-Vocabulary Sub-concepts Discovery, combining unsupervised segmentation with weak open-vocabulary guidance to adapt to both scene content and user queries.


<details>
  <summary>Details</summary>
Motivation: Traditional methods focus on either task-specific goals or scene content, limiting adaptability. DiSCO-3D aims to bridge this gap for broader 3D semantic segmentation.

Method: Built on Neural Fields representations, DiSCO-3D integrates unsupervised segmentation with weak open-vocabulary guidance.

Result: DiSCO-3D achieves effective performance in Open-Vocabulary Sub-concepts Discovery and state-of-the-art results in edge cases of open-vocabulary and unsupervised segmentation.

Conclusion: DiSCO-3D successfully addresses the broader problem of 3D Open-Vocabulary Sub-concepts Discovery, demonstrating adaptability and superior performance.

Abstract: 3D semantic segmentation provides high-level scene understanding for
applications in robotics, autonomous systems, \textit{etc}. Traditional methods
adapt exclusively to either task-specific goals (open-vocabulary segmentation)
or scene content (unsupervised semantic segmentation). We propose DiSCO-3D, the
first method addressing the broader problem of 3D Open-Vocabulary Sub-concepts
Discovery, which aims to provide a 3D semantic segmentation that adapts to both
the scene and user queries. We build DiSCO-3D on Neural Fields representations,
combining unsupervised segmentation with weak open-vocabulary guidance. Our
evaluations demonstrate that DiSCO-3D achieves effective performance in
Open-Vocabulary Sub-concepts Discovery and exhibits state-of-the-art results in
the edge cases of both open-vocabulary and unsupervised segmentation.

</details>


### [31] [Exp-Graph: How Connections Learn Facial Attributes in Graph-based Expression Recognition](https://arxiv.org/abs/2507.14608)
*Nandani Sharma,Dinesh Singh*

Main category: cs.CV

TL;DR: Exp-Graph uses graph-based modeling with facial landmarks and vision transformers for facial expression recognition, achieving high accuracy across datasets.


<details>
  <summary>Details</summary>
Motivation: Facial expression recognition is vital for human-computer interaction, and incorporating structural information improves accuracy.

Method: Proposes Exp-Graph, using facial landmarks as graph vertices, vision transformers for edge determination, and graph convolutional networks for structural dependencies.

Result: Achieved accuracies of 98.09%, 79.01%, and 56.39% on Oulu-CASIA, eNTERFACE05, and AFEW datasets, respectively.

Conclusion: Exp-Graph demonstrates strong generalization and effectiveness in practical facial expression recognition.

Abstract: Facial expression recognition is crucial for human-computer interaction
applications such as face animation, video surveillance, affective computing,
medical analysis, etc. Since the structure of facial attributes varies with
facial expressions, incorporating structural information into facial attributes
is essential for facial expression recognition. In this paper, we propose
Exp-Graph, a novel framework designed to represent the structural relationships
among facial attributes using graph-based modeling for facial expression
recognition. For facial attributes graph representation, facial landmarks are
used as the graph's vertices. At the same time, the edges are determined based
on the proximity of the facial landmark and the similarity of the local
appearance of the facial attributes encoded using the vision transformer.
Additionally, graph convolutional networks are utilized to capture and
integrate these structural dependencies into the encoding of facial attributes,
thereby enhancing the accuracy of expression recognition. Thus, Exp-Graph
learns from the facial attribute graphs highly expressive semantic
representations. On the other hand, the vision transformer and graph
convolutional blocks help the framework exploit the local and global
dependencies among the facial attributes that are essential for the recognition
of facial expressions. We conducted comprehensive evaluations of the proposed
Exp-Graph model on three benchmark datasets: Oulu-CASIA, eNTERFACE05, and AFEW.
The model achieved recognition accuracies of 98.09\%, 79.01\%, and 56.39\%,
respectively. These results indicate that Exp-Graph maintains strong
generalization capabilities across both controlled laboratory settings and
real-world, unconstrained environments, underscoring its effectiveness for
practical facial expression recognition applications.

</details>


### [32] [Depthwise-Dilated Convolutional Adapters for Medical Object Tracking and Segmentation Using the Segment Anything Model 2](https://arxiv.org/abs/2507.14613)
*Guoping Xu,Christopher Kabat,You Zhang*

Main category: cs.CV

TL;DR: DD-SAM2 is an efficient adaptation framework for SAM2, enhancing medical video segmentation with minimal computational overhead, achieving high Dice scores.


<details>
  <summary>Details</summary>
Motivation: Existing medical image segmentation methods lack adaptability to dynamic scenarios and require large datasets for retraining, leading to high costs and risks like catastrophic forgetting.

Method: Proposes DD-SAM2, incorporating a Depthwise-Dilated Adapter (DD-Adapter) for multi-scale feature extraction, enabling fine-tuning of SAM2 on limited medical video data.

Result: Achieves superior performance with Dice scores of 0.93 (TrackRad2025) and 0.97 (EchoNet-Dynamic).

Conclusion: DD-SAM2 offers an efficient, adapter-based solution for SAM2 fine-tuning in medical video segmentation and tracking, with publicly available resources.

Abstract: Recent advances in medical image segmentation have been driven by deep
learning; however, most existing methods remain limited by modality-specific
designs and exhibit poor adaptability to dynamic medical imaging scenarios. The
Segment Anything Model 2 (SAM2) and its related variants, which introduce a
streaming memory mechanism for real-time video segmentation, present new
opportunities for prompt-based, generalizable solutions. Nevertheless, adapting
these models to medical video scenarios typically requires large-scale datasets
for retraining or transfer learning, leading to high computational costs and
the risk of catastrophic forgetting. To address these challenges, we propose
DD-SAM2, an efficient adaptation framework for SAM2 that incorporates a
Depthwise-Dilated Adapter (DD-Adapter) to enhance multi-scale feature
extraction with minimal parameter overhead. This design enables effective
fine-tuning of SAM2 on medical videos with limited training data. Unlike
existing adapter-based methods focused solely on static images, DD-SAM2 fully
exploits SAM2's streaming memory for medical video object tracking and
segmentation. Comprehensive evaluations on TrackRad2025 (tumor segmentation)
and EchoNet-Dynamic (left ventricle tracking) datasets demonstrate superior
performance, achieving Dice scores of 0.93 and 0.97, respectively. To the best
of our knowledge, this work provides an initial attempt at systematically
exploring adapter-based SAM2 fine-tuning for medical video segmentation and
tracking. Code, datasets, and models will be publicly available at
https://github.com/apple1986/DD-SAM2.

</details>


### [33] [BusterX++: Towards Unified Cross-Modal AI-Generated Content Detection and Explanation with MLLM](https://arxiv.org/abs/2507.14632)
*Haiquan Wen,Tianxiao Li,Zhenglin Huang,Yiwei He,Guangliang Cheng*

Main category: cs.CV

TL;DR: BusterX++ is a novel framework for cross-modal detection of synthetic media, using reinforcement learning and hybrid reasoning to outperform single-modality methods.


<details>
  <summary>Details</summary>
Motivation: The rise of generative AI has increased misinformation risks, but current detection systems are limited by single-modality designs, failing against multi-format synthetic content.

Method: BusterX++ employs reinforcement learning post-training (Multi-stage Training, Thinking Reward, Hybrid Reasoning) for cross-modal detection. GenBuster++ benchmark (4,000 images/videos) is introduced for evaluation.

Result: BusterX++ achieves stable and substantial performance improvements, demonstrating effectiveness and generalizability.

Conclusion: BusterX++ addresses limitations of single-modality detection, offering a robust solution for identifying synthetic media across formats.

Abstract: Recent advances in generative AI have dramatically improved image and video
synthesis capabilities, significantly increasing the risk of misinformation
through sophisticated fake content. In response, detection methods have evolved
from traditional approaches to multimodal large language models (MLLMs),
offering enhanced transparency and interpretability in identifying synthetic
media. However, current detection systems remain fundamentally limited by their
single-modality design. These approaches analyze images or videos separately,
making them ineffective against synthetic content that combines multiple media
formats. To address these challenges, we introduce \textbf{BusterX++}, a novel
framework designed specifically for cross-modal detection and explanation of
synthetic media. Our approach incorporates an advanced reinforcement learning
(RL) post-training strategy that eliminates cold-start. Through Multi-stage
Training, Thinking Reward, and Hybrid Reasoning, BusterX++ achieves stable and
substantial performance improvements. To enable comprehensive evaluation, we
also present \textbf{GenBuster++}, a cross-modal benchmark leveraging
state-of-the-art image and video generation techniques. This benchmark
comprises 4,000 images and video clips, meticulously curated by human experts
using a novel filtering methodology to ensure high quality, diversity, and
real-world applicability. Extensive experiments demonstrate the effectiveness
and generalizability of our approach.

</details>


### [34] [Multispectral State-Space Feature Fusion: Bridging Shared and Cross-Parametric Interactions for Object Detection](https://arxiv.org/abs/2507.14643)
*Jifeng Shen,Haibo Zhan,Shaohua Dong,Xin Zuo,Wankou Yang,Haibin Ling*

Main category: cs.CV

TL;DR: MS2Fusion is a novel multispectral feature fusion framework using a state space model (SSM) to address limitations in object detection by balancing local complementary features and cross-modal shared semantics. It outperforms state-of-the-art methods on benchmarks like FLIR, M3FD, and LLVIP.


<details>
  <summary>Details</summary>
Motivation: The paper addresses two key limitations in multispectral feature fusion for object detection: overemphasis on local complementary features and the trade-off between receptive field size and computational complexity.

Method: MS2Fusion employs a dual-path parametric interaction mechanism within an SSM framework. One branch focuses on cross-modal complementary features, while the other aligns shared semantics through parameter sharing. Both paths are jointly optimized.

Result: MS2Fusion significantly outperforms existing methods on benchmarks (FLIR, M3FD, LLVIP) and shows generality by achieving state-of-the-art results in RGB-T semantic segmentation and RGBT salient object detection.

Conclusion: MS2Fusion effectively balances complementary and shared features in multispectral fusion, demonstrating superior performance and versatility across tasks.

Abstract: Modern multispectral feature fusion for object detection faces two critical
limitations: (1) Excessive preference for local complementary features over
cross-modal shared semantics adversely affects generalization performance; and
(2) The trade-off between the receptive field size and computational complexity
present critical bottlenecks for scalable feature modeling. Addressing these
issues, a novel Multispectral State-Space Feature Fusion framework, dubbed
MS2Fusion, is proposed based on the state space model (SSM), achieving
efficient and effective fusion through a dual-path parametric interaction
mechanism. More specifically, the first cross-parameter interaction branch
inherits the advantage of cross-attention in mining complementary information
with cross-modal hidden state decoding in SSM. The second shared-parameter
branch explores cross-modal alignment with joint embedding to obtain
cross-modal similar semantic features and structures through parameter sharing
in SSM. Finally, these two paths are jointly optimized with SSM for fusing
multispectral features in a unified framework, allowing our MS2Fusion to enjoy
both functional complementarity and shared semantic space. In our extensive
experiments on mainstream benchmarks including FLIR, M3FD and LLVIP, our
MS2Fusion significantly outperforms other state-of-the-art multispectral object
detection methods, evidencing its superiority. Moreover, MS2Fusion is general
and applicable to other multispectral perception tasks. We show that, even
without specific design, MS2Fusion achieves state-of-the-art results on RGB-T
semantic segmentation and RGBT salient object detection, showing its
generality. The source code will be available at
https://github.com/61s61min/MS2Fusion.git.

</details>


### [35] [AI-Powered Precision in Sport Taekwondo: Enhancing Fairness, Speed, and Trust in Competition (FST.ai)](https://arxiv.org/abs/2507.14657)
*Keivan Shariatmadar,Ahmad Osman*

Main category: cs.CV

TL;DR: FST.ai is an AI framework for real-time head kick detection in Taekwondo, reducing decision time and improving fairness. It uses computer vision and deep learning, with potential applications in other sports.


<details>
  <summary>Details</summary>
Motivation: Traditional sports officiating systems are slow, subjective, and inconsistent, harming fairness and trust. AI can address these issues.

Method: FST.ai employs computer vision, deep learning, and edge inference for real-time action detection, using pose estimation, motion classification, and impact analysis.

Result: The system reduces decision time from minutes to seconds, enhances consistency, and is adaptable to other sports like judo or football.

Conclusion: FST.ai demonstrates robustness and scalability, offering a transformative solution for officiating in Taekwondo and beyond.

Abstract: The integration of Artificial Intelligence (AI) into sports officiating
represents a paradigm shift in how decisions are made in competitive
environments. Traditional manual systems, even when supported by Instant Video
Replay (IVR), often suffer from latency, subjectivity, and inconsistent
enforcement, undermining fairness and athlete trust. This paper introduces
FST.ai, a novel AI-powered framework designed to enhance officiating in Sport
Taekwondo, particularly focusing on the complex task of real-time head kick
detection and scoring. Leveraging computer vision, deep learning, and edge
inference, the system automates the identification and classification of key
actions, significantly reducing decision time from minutes to seconds while
improving consistency and transparency. Importantly, the methodology is not
limited to Taekwondo. The underlying framework -- based on pose estimation,
motion classification, and impact analysis -- can be adapted to a wide range of
sports requiring action detection, such as judo, karate, fencing, or even team
sports like football and basketball, where foul recognition or performance
tracking is critical. By addressing one of Taekwondo's most challenging
scenarios -- head kick scoring -- we demonstrate the robustness, scalability,
and sport-agnostic potential of FST.ai to transform officiating standards
across multiple disciplines.

</details>


### [36] [Artificial Intelligence in the Food Industry: Food Waste Estimation based on Computer Vision, a Brief Case Study in a University Dining Hall](https://arxiv.org/abs/2507.14662)
*Shayan Rokhva,Babak Teimourpour*

Main category: cs.CV

TL;DR: A cost-effective computer vision framework estimates plate-level food waste using semantic segmentation of RGB images, achieving high accuracy with lightweight models for real-time monitoring.


<details>
  <summary>Details</summary>
Motivation: Quantifying post-consumer food waste in institutional dining settings to support data-driven sustainability strategies.

Method: Utilizes semantic segmentation of RGB images before and after meals, training four models (U-Net, U-Net++, and lightweight variants) with a capped dynamic inverse-frequency loss and AdamW optimizer. Evaluated using Pixel Accuracy, Dice, IoU, and a custom DPA metric.

Result: All models performed well, with at least one model per dish achieving ≥90% DPA. Lighter models enabled real-time inference. Dry/rigid foods (e.g., rice) segmented better than complex/viscous ones (e.g., stews).

Conclusion: The framework is scalable and pioneering for contactless food waste monitoring, offering actionable insights for reducing institutional waste and future research directions.

Abstract: Quantifying post-consumer food waste in institutional dining settings is
essential for supporting data-driven sustainability strategies. This study
presents a cost-effective computer vision framework that estimates plate-level
food waste by utilizing semantic segmentation of RGB images taken before and
after meal consumption across five Iranian dishes. Four fully supervised models
(U-Net, U-Net++, and their lightweight variants) were trained using a capped
dynamic inverse-frequency loss and AdamW optimizer, then evaluated through a
comprehensive set of metrics, including Pixel Accuracy, Dice, IoU, and a
custom-defined Distributional Pixel Agreement (DPA) metric tailored to the
task. All models achieved satisfying performance, and for each food type, at
least one model approached or surpassed 90% DPA, demonstrating strong alignment
in pixel-wise proportion estimates. Lighter models with reduced parameter
counts offered faster inference, achieving real-time throughput on an NVIDIA T4
GPU. Further analysis showed superior segmentation performance for dry and more
rigid components (e.g., rice and fries), while more complex, fragmented, or
viscous dishes, such as stews, showed reduced performance, specifically
post-consumption. Despite limitations such as reliance on 2D imaging,
constrained food variety, and manual data collection, the proposed framework is
pioneering and represents a scalable, contactless solution for continuous
monitoring of food consumption. This research lays foundational groundwork for
automated, real-time waste tracking systems in large-scale food service
environments and offers actionable insights and outlines feasible future
directions for dining hall management and policymakers aiming to reduce
institutional food waste.

</details>


### [37] [Gene-DML: Dual-Pathway Multi-Level Discrimination for Gene Expression Prediction from Histopathology Images](https://arxiv.org/abs/2507.14670)
*Yaxuan Song,Jianan Fan,Hang Chang,Weidong Cai*

Main category: cs.CV

TL;DR: Gene-DML is a framework for predicting gene expression from histopathology images by aligning cross-modal representations at multiple levels, improving accuracy and generalization.


<details>
  <summary>Details</summary>
Motivation: Existing methods underutilize cross-modal alignment between histopathology images and gene expression, limiting prediction performance.

Method: Gene-DML uses Dual-pathway Multi-Level discrimination: multi-scale instance-level and cross-level instance-group discrimination to align morphological and transcriptional modalities.

Result: Gene-DML achieves state-of-the-art performance in gene expression prediction on public datasets.

Conclusion: Gene-DML enhances cross-modal representation learning, improving predictive accuracy and generalization in computational pathology.

Abstract: Accurately predicting gene expression from histopathology images offers a
scalable and non-invasive approach to molecular profiling, with significant
implications for precision medicine and computational pathology. However,
existing methods often underutilize the cross-modal representation alignment
between histopathology images and gene expression profiles across multiple
representational levels, thereby limiting their prediction performance. To
address this, we propose Gene-DML, a unified framework that structures latent
space through Dual-pathway Multi-Level discrimination to enhance correspondence
between morphological and transcriptional modalities. The multi-scale
instance-level discrimination pathway aligns hierarchical histopathology
representations extracted at local, neighbor, and global levels with gene
expression profiles, capturing scale-aware morphological-transcriptional
relationships. In parallel, the cross-level instance-group discrimination
pathway enforces structural consistency between individual (image/gene)
instances and modality-crossed (gene/image, respectively) groups, strengthening
the alignment across modalities. By jointly modelling fine-grained and
structural-level discrimination, Gene-DML is able to learn robust cross-modal
representations, enhancing both predictive accuracy and generalization across
diverse biological contexts. Extensive experiments on public spatial
transcriptomics datasets demonstrate that Gene-DML achieves state-of-the-art
performance in gene expression prediction. The code and checkpoints will be
released soon.

</details>


### [38] [Docopilot: Improving Multimodal Models for Document-Level Understanding](https://arxiv.org/abs/2507.14675)
*Yuchen Duan,Zhe Chen,Yusong Hu,Weiyun Wang,Shenglong Ye,Botian Shi,Lewei Lu,Qibin Hou,Tong Lu,Hongsheng Li,Jifeng Dai,Wenhai Wang*

Main category: cs.CV

TL;DR: The paper introduces Doc-750K, a high-quality dataset for multimodal document understanding, and Docopilot, a native multimodal model that outperforms RAG methods in coherence, accuracy, and efficiency.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs struggle with complex, multi-page document comprehension due to lack of quality datasets, and RAG methods have limitations like fragmented contexts and error accumulation.

Method: Developed Doc-750K, a document-level dataset with diverse structures and cross-page dependencies, and Docopilot, a native multimodal model for direct document understanding.

Result: Docopilot achieves superior performance in document understanding tasks and multi-turn interactions, setting a new benchmark.

Conclusion: The work provides a robust solution for document-level multimodal understanding, with data, code, and models publicly available.

Abstract: Despite significant progress in multimodal large language models (MLLMs),
their performance on complex, multi-page document comprehension remains
inadequate, largely due to the lack of high-quality, document-level datasets.
While current retrieval-augmented generation (RAG) methods offer partial
solutions, they suffer from issues, such as fragmented retrieval contexts,
multi-stage error accumulation, and extra time costs of retrieval. In this
work, we present a high-quality document-level dataset, Doc-750K, designed to
support in-depth understanding of multimodal documents. This dataset includes
diverse document structures, extensive cross-page dependencies, and real
question-answer pairs derived from the original documents. Building on the
dataset, we develop a native multimodal model, Docopilot, which can accurately
handle document-level dependencies without relying on RAG. Experiments
demonstrate that Docopilot achieves superior coherence, accuracy, and
efficiency in document understanding tasks and multi-turn interactions, setting
a new baseline for document-level multimodal understanding. Data, code, and
models are released at https://github.com/OpenGVLab/Docopilot

</details>


### [39] [WSI-Agents: A Collaborative Multi-Agent System for Multi-Modal Whole Slide Image Analysis](https://arxiv.org/abs/2507.14680)
*Xinheng Lyu,Yuci Liang,Wenting Chen,Meidan Ding,Jiaqi Yang,Guolin Huang,Daokun Zhang,Xiangjian He,Linlin Shen*

Main category: cs.CV

TL;DR: WSI-Agents is a collaborative multi-agent system for multi-modal WSI analysis, enhancing accuracy and versatility through task allocation, verification, and summarization.


<details>
  <summary>Details</summary>
Motivation: Current multi-modal large language models (MLLMs) underperform in WSI analysis compared to task-specific models, and collaborative multi-agent systems are underexplored in pathology.

Method: WSI-Agents integrates specialized agents with task allocation, verification, and summary modules to improve accuracy and versatility.

Result: Experiments show WSI-Agents outperforms current WSI MLLMs and medical agent frameworks across diverse tasks.

Conclusion: WSI-Agents offers a promising solution for balancing accuracy and versatility in multi-modal WSI analysis.

Abstract: Whole slide images (WSIs) are vital in digital pathology, enabling gigapixel
tissue analysis across various pathological tasks. While recent advancements in
multi-modal large language models (MLLMs) allow multi-task WSI analysis through
natural language, they often underperform compared to task-specific models.
Collaborative multi-agent systems have emerged as a promising solution to
balance versatility and accuracy in healthcare, yet their potential remains
underexplored in pathology-specific domains. To address these issues, we
propose WSI-Agents, a novel collaborative multi-agent system for multi-modal
WSI analysis. WSI-Agents integrates specialized functional agents with robust
task allocation and verification mechanisms to enhance both task-specific
accuracy and multi-task versatility through three components: (1) a task
allocation module assigning tasks to expert agents using a model zoo of patch
and WSI level MLLMs, (2) a verification mechanism ensuring accuracy through
internal consistency checks and external validation using pathology knowledge
bases and domain-specific models, and (3) a summary module synthesizing the
final summary with visual interpretation maps. Extensive experiments on
multi-modal WSI benchmarks show WSI-Agents's superiority to current WSI MLLMs
and medical agent frameworks across diverse tasks.

</details>


### [40] [From Semantics, Scene to Instance-awareness: Distilling Foundation Model for Open-vocabulary Situation Recognition](https://arxiv.org/abs/2507.14686)
*Chen Cai,Tianyi Liu,Jianjun Gao,Wenyang Liu,Kejun Wu,Ruoyu Wang,Yi Wang,Soo Chin Liew*

Main category: cs.CV

TL;DR: The paper introduces Open-vocabulary Grounded Situation Recognition (Ov-GSR) and proposes Multimodal Interactive Prompt Distillation (MIPD) to enhance generalization and zero-shot abilities by transferring knowledge from a teacher MLLM to a smaller GSR model.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs struggle with complex GSR and are resource-heavy, while conventional GSR models lack generalization for unseen and rare situations. The goal is to bridge this gap.

Method: MIPD distills multimodal knowledge from a teacher MLLM using LLM-based rationales and scene-aware prompts, aligning them via Negative-Guided Multimodal Prompting Alignment (NMPA).

Result: MIPD achieves superior performance on seen, rare, and unseen situations on the Ov-SWiG dataset and improves unseen detection on HICO-DET.

Conclusion: The proposed MIPD framework effectively enhances generalization and zero-shot abilities, addressing limitations of both MLLMs and conventional GSR models.

Abstract: Recent Multimodal Large Language Models (MLLMs) exhibit strong zero-shot
abilities but struggle with complex Grounded Situation Recognition (GSR) and
are resource-intensive for edge device deployment. Meanwhile, conventional GSR
models often lack generalization ability, falling short in recognizing unseen
and rare situations. In this paper, we exploit transferring knowledge from a
teacher MLLM to a small GSR model to enhance its generalization and zero-shot
abilities, thereby introducing the task of Open-vocabulary Grounded Situation
Recognition (Ov-GSR). To achieve this, we propose Multimodal Interactive Prompt
Distillation (MIPD), a novel framework that distills enriched multimodal
knowledge from the foundation model, enabling the student Ov-GSR model to
recognize unseen situations and be better aware of rare situations.
Specifically, the MIPD framework first leverages the LLM-based Judgmental
Rationales Generator (JRG) to construct positive and negative glimpse and gaze
rationales enriched with contextual semantic information. The proposed
scene-aware and instance-perception prompts are then introduced to align
rationales with visual information from the MLLM teacher via the
Negative-Guided Multimodal Prompting Alignment (NMPA) module, effectively
capturing holistic and perceptual multimodal knowledge. Finally, the aligned
multimodal knowledge is distilled into the student Ov-GSR model, providing a
stronger foundation for generalization that enhances situation understanding,
bridges the gap between seen and unseen scenarios, and mitigates prediction
bias in rare cases. We evaluate MIPD on the refined Ov-SWiG dataset, achieving
superior performance on seen, rare, and unseen situations, and further
demonstrate improved unseen detection on the HICO-DET dataset.

</details>


### [41] [GTPBD: A Fine-Grained Global Terraced Parcel and Boundary Dataset](https://arxiv.org/abs/2507.14697)
*Zhiwei Zhang,Zi Ye,Yibin Wen,Shuai Yuan,Haohuan Fu,Jianxi Huang,Juepeng Zheng*

Main category: cs.CV

TL;DR: The paper introduces GTPBD, a fine-grained terraced parcel dataset, addressing gaps in existing datasets by covering diverse terrains and complex parcels globally. It supports multiple tasks and benchmarks various methods.


<details>
  <summary>Details</summary>
Motivation: Existing datasets lack representation of complex terraced terrains, limiting precision agriculture research. GTPBD aims to fill this gap with a comprehensive, manually annotated dataset.

Method: GTPBD includes 47,537 high-resolution images with three-level labels (boundary, mask, parcel) across diverse terrains. It benchmarks semantic segmentation, edge detection, parcel extraction, and UDA tasks.

Result: GTPBD outperforms existing datasets, providing a robust infrastructure for fine-grained agricultural analysis and cross-scenario knowledge transfer.

Conclusion: GTPBD is a critical resource for terraced remote sensing research, enabling advanced agricultural terrain analysis and domain adaptation.

Abstract: Agricultural parcels serve as basic units for conducting agricultural
practices and applications, which is vital for land ownership registration,
food security assessment, soil erosion monitoring, etc. However, existing
agriculture parcel extraction studies only focus on mid-resolution mapping or
regular plain farmlands while lacking representation of complex terraced
terrains due to the demands of precision agriculture.In this paper, we
introduce a more fine-grained terraced parcel dataset named GTPBD (Global
Terraced Parcel and Boundary Dataset), which is the first fine-grained dataset
covering major worldwide terraced regions with more than 200,000 complex
terraced parcels with manual annotation. GTPBD comprises 47,537 high-resolution
images with three-level labels, including pixel-level boundary labels, mask
labels, and parcel labels. It covers seven major geographic zones in China and
transcontinental climatic regions around the world.Compared to the existing
datasets, the GTPBD dataset brings considerable challenges due to the: (1)
terrain diversity; (2) complex and irregular parcel objects; and (3) multiple
domain styles. Our proposed GTPBD dataset is suitable for four different tasks,
including semantic segmentation, edge detection, terraced parcel extraction,
and unsupervised domain adaptation (UDA) tasks.Accordingly, we benchmark the
GTPBD dataset on eight semantic segmentation methods, four edge extraction
methods, three parcel extraction methods, and five UDA methods, along with a
multi-dimensional evaluation framework integrating pixel-level and object-level
metrics. GTPBD fills a critical gap in terraced remote sensing research,
providing a basic infrastructure for fine-grained agricultural terrain analysis
and cross-scenario knowledge transfer.

</details>


### [42] [MultiRetNet: A Multimodal Vision Model and Deferral System for Staging Diabetic Retinopathy](https://arxiv.org/abs/2507.14738)
*Jeannie She,Katie Spivakovsky*

Main category: cs.CV

TL;DR: MultiRetNet improves diabetic retinopathy staging by combining retinal imaging, socioeconomic data, and comorbidities, using multimodal fusion and a deferral system for clinician review.


<details>
  <summary>Details</summary>
Motivation: Diabetic retinopathy (DR) disproportionately affects lower-income communities due to limited screening access. Comorbid conditions worsen progression, highlighting the need for better early detection methods.

Method: MultiRetNet integrates retinal imaging, socioeconomic factors, and comorbidities. It uses three multimodal fusion methods, with fully connected layer fusion being optimal. A deferral system trained via contrastive learning identifies cases needing clinician review.

Result: The system maintains diagnostic accuracy on low-quality images and improves early detection, especially in underserved populations. It also reduces healthcare costs and disparities.

Conclusion: MultiRetNet enhances DR staging and early detection, promoting healthcare equity by addressing access disparities and improving outcomes for high-risk populations.

Abstract: Diabetic retinopathy (DR) is a leading cause of preventable blindness,
affecting over 100 million people worldwide. In the United States, individuals
from lower-income communities face a higher risk of progressing to advanced
stages before diagnosis, largely due to limited access to screening. Comorbid
conditions further accelerate disease progression. We propose MultiRetNet, a
novel pipeline combining retinal imaging, socioeconomic factors, and
comorbidity profiles to improve DR staging accuracy, integrated with a clinical
deferral system for a clinical human-in-the-loop implementation. We experiment
with three multimodal fusion methods and identify fusion through a fully
connected layer as the most versatile methodology. We synthesize adversarial,
low-quality images and use contrastive learning to train the deferral system,
guiding the model to identify out-of-distribution samples that warrant
clinician review. By maintaining diagnostic accuracy on suboptimal images and
integrating critical health data, our system can improve early detection,
particularly in underserved populations where advanced DR is often first
identified. This approach may reduce healthcare costs, increase early detection
rates, and address disparities in access to care, promoting healthcare equity.

</details>


### [43] [InterAct-Video: Reasoning-Rich Video QA for Urban Traffic](https://arxiv.org/abs/2507.14743)
*Joseph Raj Vishal,Rutuja Patil,Manas Srinivas Gowda,Katha Naik,Yezhou Yang,Bharatesh Chakravarthi*

Main category: cs.CV

TL;DR: The paper introduces InterAct VideoQA, a dataset for benchmarking VideoQA models in traffic monitoring, addressing challenges in real-world traffic scenes.


<details>
  <summary>Details</summary>
Motivation: Existing VideoQA models struggle with complex traffic scenarios, necessitating a domain-specific dataset for improved performance.

Method: The InterAct VideoQA dataset includes 8 hours of real-world traffic footage, segmented into 10-second clips with 25,000 QA pairs, covering spatiotemporal dynamics and vehicle interactions.

Result: State-of-the-art models evaluated on InterAct VideoQA show challenges in reasoning over spatiotemporal dependencies, but fine-tuning improves performance.

Conclusion: InterAct VideoQA is a valuable benchmark for advancing VideoQA models in intelligent transportation systems.

Abstract: Traffic monitoring is crucial for urban mobility, road safety, and
intelligent transportation systems (ITS). Deep learning has advanced
video-based traffic monitoring through video question answering (VideoQA)
models, enabling structured insight extraction from traffic videos. However,
existing VideoQA models struggle with the complexity of real-world traffic
scenes, where multiple concurrent events unfold across spatiotemporal
dimensions. To address these challenges, this paper introduces \textbf{InterAct
VideoQA}, a curated dataset designed to benchmark and enhance VideoQA models
for traffic monitoring tasks. The InterAct VideoQA dataset comprises 8 hours of
real-world traffic footage collected from diverse intersections, segmented into
10-second video clips, with over 25,000 question-answer (QA) pairs covering
spatiotemporal dynamics, vehicle interactions, incident detection, and other
critical traffic attributes. State-of-the-art VideoQA models are evaluated on
InterAct VideoQA, exposing challenges in reasoning over fine-grained
spatiotemporal dependencies within complex traffic scenarios. Additionally,
fine-tuning these models on InterAct VideoQA yields notable performance
improvements, demonstrating the necessity of domain-specific datasets for
VideoQA. InterAct VideoQA is publicly available as a benchmark dataset to
facilitate future research in real-world deployable VideoQA models for
intelligent transportation systems. GitHub Repo:
https://github.com/joe-rabbit/InterAct_VideoQA

</details>


### [44] [LeAdQA: LLM-Driven Context-Aware Temporal Grounding for Video Question Answering](https://arxiv.org/abs/2507.14784)
*Xinxin Dong,Baoyun Peng,Haokai Ma,Yufei Wang,Zixuan Dong,Fei Hu,Xiaodong Wang*

Main category: cs.CV

TL;DR: LeAdQA improves VideoQA by refining queries with LLMs and grounding them temporally, achieving SOTA performance.


<details>
  <summary>Details</summary>
Motivation: Current VideoQA methods struggle with irrelevant content and lack causal-temporal reasoning, limiting complex question answering.

Method: LeAdQA uses LLMs to refine questions, temporal grounding for segment retrieval, and adaptive fusion for evidence integration.

Result: Achieves SOTA on NExT-QA, IntentQA, and NExT-GQA, enhancing video-question understanding.

Conclusion: LeAdQA effectively addresses limitations in VideoQA, improving accuracy and efficiency for complex reasoning.

Abstract: Video Question Answering (VideoQA) requires identifying sparse critical
moments in long videos and reasoning about their causal relationships to answer
semantically complex questions. While recent advances in multimodal learning
have improved alignment and fusion, current approaches remain limited by two
prevalent but fundamentally flawed strategies: (1) task-agnostic sampling
indiscriminately processes all frames, overwhelming key events with irrelevant
content; and (2) heuristic retrieval captures superficial patterns but misses
causal-temporal structures needed for complex reasoning. To address these
challenges, we introduce LeAdQA, an innovative approach that bridges these gaps
through synergizing causal-aware query refinement with fine-grained visual
grounding. Our method first leverages LLMs to reformulate question-option
pairs, resolving causal ambiguities and sharpening temporal focus. These
refined queries subsequently direct a temporal grounding model to precisely
retrieve the most salient segments, complemented by an adaptive fusion
mechanism dynamically integrating the evidence to maximize relevance. The
integrated visual-textual cues are then processed by an MLLM to generate
accurate, contextually-grounded answers. Experiments on NExT-QA, IntentQA, and
NExT-GQA demonstrate that our method's precise visual grounding substantially
enhances the understanding of video-question relationships, achieving
state-of-the-art (SOTA) performance on complex reasoning tasks while
maintaining computational efficiency.

</details>


### [45] [FOCUS: Fused Observation of Channels for Unveiling Spectra](https://arxiv.org/abs/2507.14787)
*Xi Xiao,Aristeidis Tsaris,Anika Tabassum,John Lagergren,Larry M. York,Tianyang Wang,Xiao Wang*

Main category: cs.CV

TL;DR: FOCUS is a framework for efficient and reliable spatial-spectral interpretability of Vision Transformers (ViTs) in hyperspectral imaging, addressing challenges like attention collapse and computational cost.


<details>
  <summary>Details</summary>
Motivation: Existing saliency methods fail to capture meaningful spectral cues in hyperspectral imaging, and full-spectrum ViTs are computationally impractical for interpretability.

Method: FOCUS introduces class-specific spectral prompts and a learnable [SINK] token to guide attention and absorb noise, enabling stable 3D saliency maps without gradient backpropagation.

Result: FOCUS improves band-level IoU by 15%, reduces attention collapse by 40%, and aligns saliency results with expert annotations.

Conclusion: FOCUS makes high-resolution ViT interpretability practical for hyperspectral applications, bridging the gap between black-box models and trustworthy decision-making.

Abstract: Hyperspectral imaging (HSI) captures hundreds of narrow, contiguous
wavelength bands, making it a powerful tool in biology, agriculture, and
environmental monitoring. However, interpreting Vision Transformers (ViTs) in
this setting remains largely unexplored due to two key challenges: (1) existing
saliency methods struggle to capture meaningful spectral cues, often collapsing
attention onto the class token, and (2) full-spectrum ViTs are computationally
prohibitive for interpretability, given the high-dimensional nature of HSI
data. We present FOCUS, the first framework that enables reliable and efficient
spatial-spectral interpretability for frozen ViTs. FOCUS introduces two core
components: class-specific spectral prompts that guide attention toward
semantically meaningful wavelength groups, and a learnable [SINK] token trained
with an attraction loss to absorb noisy or redundant attention. Together, these
designs make it possible to generate stable and interpretable 3D saliency maps
and spectral importance curves in a single forward pass, without any gradient
backpropagation or backbone modification. FOCUS improves band-level IoU by 15
percent, reduces attention collapse by over 40 percent, and produces saliency
results that align closely with expert annotations. With less than 1 percent
parameter overhead, our method makes high-resolution ViT interpretability
practical for real-world hyperspectral applications, bridging a long-standing
gap between black-box modeling and trustworthy HSI decision-making.

</details>


### [46] [A Novel Downsampling Strategy Based on Information Complementarity for Medical Image Segmentation](https://arxiv.org/abs/2507.14790)
*Wenbo Yue,Chang Li,Guoping Xu*

Main category: cs.CV

TL;DR: The paper proposes Hybrid Pooling Downsampling (HPD), a method replacing traditional downsampling in CNNs to retain spatial details, improving semantic segmentation accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional downsampling methods lose key spatial information in semantic segmentation, affecting pixel-by-pixel prediction accuracy.

Method: HPD uses MinMaxPooling to complement information, retaining image contrast and detail features by extracting local maximum values.

Result: HPD outperforms traditional methods, increasing the DSC coefficient by 0.5% on average in experiments on ACDC and Synapse datasets.

Conclusion: HPD provides an efficient solution for semantic segmentation tasks by preserving spatial details.

Abstract: In convolutional neural networks (CNNs), downsampling operations are crucial
to model performance. Although traditional downsampling methods (such as
maximum pooling and cross-row convolution) perform well in feature aggregation,
receptive field expansion, and computational reduction, they may lead to the
loss of key spatial information in semantic segmentation tasks, thereby
affecting the pixel-by-pixel prediction accuracy.To this end, this study
proposes a downsampling method based on information complementarity - Hybrid
Pooling Downsampling (HPD). The core is to replace the traditional method with
MinMaxPooling, and effectively retain the light and dark contrast and detail
features of the image by extracting the maximum value information of the local
area.Experiment on various CNN architectures on the ACDC and Synapse datasets
show that HPD outperforms traditional methods in segmentation performance, and
increases the DSC coefficient by 0.5% on average. The results show that the HPD
module provides an efficient solution for semantic segmentation tasks.

</details>


### [47] [Distilling Parallel Gradients for Fast ODE Solvers of Diffusion Models](https://arxiv.org/abs/2507.14797)
*Beier Zhu,Ruoyu Wang,Tong Zhao,Hanwang Zhang,Chi Zhang*

Main category: cs.CV

TL;DR: The paper introduces EPD, a novel ODE solver for diffusion models that reduces sampling latency while maintaining image quality by using parallel gradient evaluations.


<details>
  <summary>Details</summary>
Motivation: Diffusion models suffer from high sampling latency due to sequential denoising. Existing acceleration methods degrade image quality under low-latency constraints.

Method: EPD incorporates multiple parallel gradient evaluations per ODE step, fully parallelizable for low latency. It uses learnable parameters optimized via distillation.

Result: EPD achieves superior FID scores (e.g., 4.47 on CIFAR-10) at 5 NFE latency, outperforming existing solvers.

Conclusion: EPD is an effective, plug-in solution for high-quality, low-latency sampling in diffusion models.

Abstract: Diffusion models (DMs) have achieved state-of-the-art generative performance
but suffer from high sampling latency due to their sequential denoising nature.
Existing solver-based acceleration methods often face image quality degradation
under a low-latency budget. In this paper, we propose the Ensemble Parallel
Direction solver (dubbed as \ours), a novel ODE solver that mitigates
truncation errors by incorporating multiple parallel gradient evaluations in
each ODE step. Importantly, since the additional gradient computations are
independent, they can be fully parallelized, preserving low-latency sampling.
  Our method optimizes a small set of learnable parameters in a distillation
fashion, ensuring minimal training overhead.
  In addition, our method can serve as a plugin to improve existing ODE
samplers. Extensive experiments on various image synthesis benchmarks
demonstrate the effectiveness of our \ours~in achieving high-quality and
low-latency sampling. For example, at the same latency level of 5 NFE, EPD
achieves an FID of 4.47 on CIFAR-10, 7.97 on FFHQ, 8.17 on ImageNet, and 8.26
on LSUN Bedroom, surpassing existing learning-based solvers by a significant
margin. Codes are available in https://github.com/BeierZhu/EPD.

</details>


### [48] [An Evaluation of DUSt3R/MASt3R/VGGT 3D Reconstruction on Photogrammetric Aerial Blocks](https://arxiv.org/abs/2507.14798)
*Xinyi Wu,Steven Landgraf,Markus Ulrich,Rongjun Qin*

Main category: cs.CV

TL;DR: The paper evaluates DUSt3R, MASt3R, and VGGT models on aerial images, showing they excel in sparse, low-resolution scenarios but struggle with high-resolution or large sets.


<details>
  <summary>Details</summary>
Motivation: To assess the performance of transformer-based 3D reconstruction models (DUSt3R, MASt3R, VGGT) on photogrammetric aerial blocks, given their potential for sparse image sets.

Method: Comprehensive evaluation of pre-trained DUSt3R, MASt3R, and VGGT models on the UseGeo dataset for pose estimation and dense 3D reconstruction.

Result: Models accurately reconstruct dense point clouds from very sparse image sets (fewer than 10 images, up to 518 pixels resolution), with +50% completeness over COLMAP. VGGT shows higher efficiency and scalability.

Conclusion: Transformer-based methods are promising for sparse, low-resolution scenarios but cannot fully replace traditional SfM and MVS, serving as complementary approaches.

Abstract: State-of-the-art 3D computer vision algorithms continue to advance in
handling sparse, unordered image sets. Recently developed foundational models
for 3D reconstruction, such as Dense and Unconstrained Stereo 3D Reconstruction
(DUSt3R), Matching and Stereo 3D Reconstruction (MASt3R), and Visual Geometry
Grounded Transformer (VGGT), have attracted attention due to their ability to
handle very sparse image overlaps. Evaluating DUSt3R/MASt3R/VGGT on typical
aerial images matters, as these models may handle extremely low image overlaps,
stereo occlusions, and textureless regions. For redundant collections, they can
accelerate 3D reconstruction by using extremely sparsified image sets. Despite
tests on various computer vision benchmarks, their potential on photogrammetric
aerial blocks remains unexplored. This paper conducts a comprehensive
evaluation of the pre-trained DUSt3R/MASt3R/VGGT models on the aerial blocks of
the UseGeo dataset for pose estimation and dense 3D reconstruction. Results
show these methods can accurately reconstruct dense point clouds from very
sparse image sets (fewer than 10 images, up to 518 pixels resolution), with
completeness gains up to +50% over COLMAP. VGGT also demonstrates higher
computational efficiency, scalability, and more reliable camera pose
estimation. However, all exhibit limitations with high-resolution images and
large sets, as pose reliability declines with more images and geometric
complexity. These findings suggest transformer-based methods cannot fully
replace traditional SfM and MVS, but offer promise as complementary approaches,
especially in challenging, low-resolution, and sparse scenarios.

</details>


### [49] [Exploring Scalable Unified Modeling for General Low-Level Vision](https://arxiv.org/abs/2507.14801)
*Xiangyu Chen,Kaiwen Zhu,Yuandong Pu,Shuo Cao,Xiaohui Li,Wenlong Zhang,Yihao Liu,Yu Qiao,Jiantao Zhou,Chao Dong*

Main category: cs.CV

TL;DR: The paper proposes VPIP, a framework for unified modeling of diverse low-level vision tasks using visual prompts, and introduces GenLV, a model evaluated across 100+ tasks, showing scalability and strong adaptability.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of unified modeling across diverse low-level vision tasks (e.g., restoration, enhancement) with varying formulations and outputs.

Method: VPIP framework uses input-target image pairs as visual prompts, integrating a backbone, prompt encoder, and interaction module. GenLV is developed and evaluated on 100+ tasks, exploring scalability in model capacity and task diversity.

Result: GenLV achieves strong performance across tasks, with improved generalization for limited-data tasks. It excels in zero-shot, few-shot, and fine-tuning scenarios.

Conclusion: VPIP is effective, scalable, and adaptable, serving as a unified foundation for general low-level vision modeling.

Abstract: Low-level vision involves a wide spectrum of tasks, including image
restoration, enhancement, stylization, and feature extraction, which differ
significantly in both task formulation and output domains. To address the
challenge of unified modeling across such diverse tasks, we propose a Visual
task Prompt-based Image Processing (VPIP) framework that leverages input-target
image pairs as visual prompts to guide the model in performing a variety of
low-level vision tasks. The framework comprises an end-to-end image processing
backbone, a prompt encoder, and a prompt interaction module, enabling flexible
integration with various architectures and effective utilization of
task-specific visual representations. Based on this design, we develop a
unified low-level vision model, GenLV, and evaluate its performance across
multiple representative tasks. To explore the scalability of this approach, we
extend the framework along two dimensions: model capacity and task diversity.
We construct a large-scale benchmark consisting of over 100 low-level vision
tasks and train multiple versions of the model with varying scales.
Experimental results show that the proposed method achieves considerable
performance across a wide range of tasks. Notably, increasing the number of
training tasks enhances generalization, particularly for tasks with limited
data, indicating the model's ability to learn transferable representations
through joint training. Further evaluations in zero-shot generalization,
few-shot transfer, and task-specific fine-tuning scenarios demonstrate the
model's strong adaptability, confirming the effectiveness, scalability, and
potential of the proposed framework as a unified foundation for general
low-level vision modeling.

</details>


### [50] [Seeing Through Deepfakes: A Human-Inspired Framework for Multi-Face Detection](https://arxiv.org/abs/2507.14807)
*Juan Hu,Shaojing Fan,Terence Sim*

Main category: cs.CV

TL;DR: A novel framework, HICOM, improves multi-face deepfake detection by leveraging human cognition cues like scene-motion coherence and interpersonal gaze alignment, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing deepfake detection methods struggle with multi-face scenarios due to lack of contextual awareness, prompting the need for human-inspired solutions.

Method: The study uses human studies to identify key detection cues, then develops HICOM, a framework incorporating these cues and an LLM for interpretability.

Result: HICOM boosts accuracy by 3.3% in in-dataset detection and 5.8% on unseen datasets, showing strong generalization.

Conclusion: Incorporating human cognitive cues enhances deepfake detection, with HICOM offering improved accuracy and interpretability.

Abstract: Multi-face deepfake videos are becoming increasingly prevalent, often
appearing in natural social settings that challenge existing detection methods.
Most current approaches excel at single-face detection but struggle in
multi-face scenarios, due to a lack of awareness of crucial contextual cues. In
this work, we develop a novel approach that leverages human cognition to
analyze and defend against multi-face deepfake videos. Through a series of
human studies, we systematically examine how people detect deepfake faces in
social settings. Our quantitative analysis reveals four key cues humans rely
on: scene-motion coherence, inter-face appearance compatibility, interpersonal
gaze alignment, and face-body consistency. Guided by these insights, we
introduce \textsf{HICOM}, a novel framework designed to detect every fake face
in multi-face scenarios. Extensive experiments on benchmark datasets show that
\textsf{HICOM} improves average accuracy by 3.3\% in in-dataset detection and
2.8\% under real-world perturbations. Moreover, it outperforms existing methods
by 5.8\% on unseen datasets, demonstrating the generalization of human-inspired
cues. \textsf{HICOM} further enhances interpretability by incorporating an LLM
to provide human-readable explanations, making detection results more
transparent and convincing. Our work sheds light on involving human factors to
enhance defense against deepfakes.

</details>


### [51] [Light Future: Multimodal Action Frame Prediction via InstructPix2Pix](https://arxiv.org/abs/2507.14809)
*Zesen Zhong,Duomin Zhang,Yijia Li*

Main category: cs.CV

TL;DR: A novel, efficient, and lightweight approach for robot action prediction using InstructPix2Pix, reducing computational cost and latency compared to traditional video prediction models.


<details>
  <summary>Details</summary>
Motivation: Predicting future motion trajectories is crucial for safer and smarter decision-making in robotics and autonomous systems.

Method: Adapts InstructPix2Pix for future visual frame prediction in robotics, using a single image and textual instruction for multimodal input.

Result: Achieves superior SSIM and PSNR on the RoboTWin dataset, outperforming state-of-the-art baselines.

Conclusion: The lightweight design enables faster inference, reduced GPU demands, and flexible multimodal control, prioritizing motion trajectory precision over visual fidelity.

Abstract: Predicting future motion trajectories is a critical capability across domains
such as robotics, autonomous systems, and human activity forecasting, enabling
safer and more intelligent decision-making. This paper proposes a novel,
efficient, and lightweight approach for robot action prediction, offering
significantly reduced computational cost and inference latency compared to
conventional video prediction models. Importantly, it pioneers the adaptation
of the InstructPix2Pix model for forecasting future visual frames in robotic
tasks, extending its utility beyond static image editing. We implement a deep
learning-based visual prediction framework that forecasts what a robot will
observe 100 frames (10 seconds) into the future, given a current image and a
textual instruction. We repurpose and fine-tune the InstructPix2Pix model to
accept both visual and textual inputs, enabling multimodal future frame
prediction. Experiments on the RoboTWin dataset (generated based on real-world
scenarios) demonstrate that our method achieves superior SSIM and PSNR compared
to state-of-the-art baselines in robot action prediction tasks. Unlike
conventional video prediction models that require multiple input frames, heavy
computation, and slow inference latency, our approach only needs a single image
and a text prompt as input. This lightweight design enables faster inference,
reduced GPU demands, and flexible multimodal control, particularly valuable for
applications like robotics and sports motion trajectory analytics, where motion
trajectory precision is prioritized over visual fidelity.

</details>


### [52] [SegQuant: A Semantics-Aware and Generalizable Quantization Framework for Diffusion Models](https://arxiv.org/abs/2507.14811)
*Jiaji Zhang,Ruichao Sun,Hailiang Zhao,Jiaju Wu,Peng Chen,Hao Li,Xinkui Zhao,Kingsum Chow,Gang Xiong,Lin Ye,Shuiguang Deng*

Main category: cs.CV

TL;DR: SegQuant is a unified quantization framework for diffusion models, combining segment-aware quantization and dual-scale schemes to improve efficiency and compatibility without retraining.


<details>
  <summary>Details</summary>
Motivation: Existing PTQ methods for diffusion models lack generalizability and hinder industrial deployment due to architecture-specific heuristics.

Method: SegQuant uses SegLinear (segment-aware, graph-based quantization) and DualScale (dual-scale quantization) to preserve structural semantics and visual fidelity.

Result: SegQuant achieves strong performance and broad applicability across diffusion models, ensuring compatibility with deployment tools.

Conclusion: SegQuant addresses limitations of PTQ methods, offering a versatile and efficient solution for quantizing diffusion models.

Abstract: Diffusion models have demonstrated exceptional generative capabilities but
are computationally intensive, posing significant challenges for deployment in
resource-constrained or latency-sensitive environments. Quantization offers an
effective means to reduce model size and computational cost, with post-training
quantization (PTQ) being particularly appealing due to its compatibility with
pre-trained models without requiring retraining or training data. However,
existing PTQ methods for diffusion models often rely on architecture-specific
heuristics that limit their generalizability and hinder integration with
industrial deployment pipelines. To address these limitations, we propose
SegQuant, a unified quantization framework that adaptively combines
complementary techniques to enhance cross-model versatility. SegQuant consists
of a segment-aware, graph-based quantization strategy (SegLinear) that captures
structural semantics and spatial heterogeneity, along with a dual-scale
quantization scheme (DualScale) that preserves polarity-asymmetric activations,
which is crucial for maintaining visual fidelity in generated outputs. SegQuant
is broadly applicable beyond Transformer-based diffusion models, achieving
strong performance while ensuring seamless compatibility with mainstream
deployment tools.

</details>


### [53] [FinChart-Bench: Benchmarking Financial Chart Comprehension in Vision-Language Models](https://arxiv.org/abs/2507.14823)
*Dong Shu,Haoyang Yuan,Yuchen Wang,Yanguang Liu,Huopu Zhang,Haiyan Zhao,Mengnan Du*

Main category: cs.CV

TL;DR: FinChart-Bench is the first benchmark for financial chart understanding, evaluating 25 LVLMs. Results show gaps in performance, instruction following, and spatial reasoning, highlighting limitations in current models.


<details>
  <summary>Details</summary>
Motivation: Financial charts are complex and underexplored in LVLM research, necessitating a dedicated benchmark.

Method: FinChart-Bench includes 1,200 financial chart images with 7,016 TF, MC, and QA questions. 25 LVLMs are evaluated.

Result: Key findings: narrowing gap between open/closed-source models, performance degradation in upgrades, struggles with instruction following, spatial reasoning limitations, and unreliability as automated evaluators.

Conclusion: Current LVLMs have significant limitations for financial chart understanding, as revealed by FinChart-Bench.

Abstract: Large vision-language models (LVLMs) have made significant progress in chart
understanding. However, financial charts, characterized by complex temporal
structures and domain-specific terminology, remain notably underexplored. We
introduce FinChart-Bench, the first benchmark specifically focused on
real-world financial charts. FinChart-Bench comprises 1,200 financial chart
images collected from 2015 to 2024, each annotated with True/False (TF),
Multiple Choice (MC), and Question Answering (QA) questions, totaling 7,016
questions. We conduct a comprehensive evaluation of 25 state-of-the-art LVLMs
on FinChart-Bench. Our evaluation reveals critical insights: (1) the
performance gap between open-source and closed-source models is narrowing, (2)
performance degradation occurs in upgraded models within families, (3) many
models struggle with instruction following, (4) both advanced models show
significant limitations in spatial reasoning abilities, and (5) current LVLMs
are not reliable enough to serve as automated evaluators. These findings
highlight important limitations in current LVLM capabilities for financial
chart understanding. The FinChart-Bench dataset is available at
https://huggingface.co/datasets/Tizzzzy/FinChart-Bench.

</details>


### [54] [PHATNet: A Physics-guided Haze Transfer Network for Domain-adaptive Real-world Image Dehazing](https://arxiv.org/abs/2507.14826)
*Fu-Jen Tsai,Yan-Tsung Peng,Yen-Yu Lin,Chia-Wen Lin*

Main category: cs.CV

TL;DR: PHATNet improves dehazing by transferring haze patterns from unseen domains to source images, enhancing model adaptability with new losses.


<details>
  <summary>Details</summary>
Motivation: Existing dehazing models struggle with unseen real-world hazy images due to limited training data, necessitating better domain adaptation.

Method: Proposes PHATNet to transfer haze patterns to source-domain images, using Haze-Transfer-Consistency and Content-Leakage Loss for better disentanglement.

Result: PHATNet significantly improves state-of-the-art dehazing models on real-world datasets.

Conclusion: PHATNet effectively adapts dehazing models to unseen domains, outperforming existing methods.

Abstract: Image dehazing aims to remove unwanted hazy artifacts in images. Although
previous research has collected paired real-world hazy and haze-free images to
improve dehazing models' performance in real-world scenarios, these models
often experience significant performance drops when handling unseen real-world
hazy images due to limited training data. This issue motivates us to develop a
flexible domain adaptation method to enhance dehazing performance during
testing. Observing that predicting haze patterns is generally easier than
recovering clean content, we propose the Physics-guided Haze Transfer Network
(PHATNet) which transfers haze patterns from unseen target domains to
source-domain haze-free images, creating domain-specific fine-tuning sets to
update dehazing models for effective domain adaptation. Additionally, we
introduce a Haze-Transfer-Consistency loss and a Content-Leakage Loss to
enhance PHATNet's disentanglement ability. Experimental results demonstrate
that PHATNet significantly boosts state-of-the-art dehazing models on benchmark
real-world image dehazing datasets.

</details>


### [55] [Paired Image Generation with Diffusion-Guided Diffusion Models](https://arxiv.org/abs/2507.14833)
*Haoxuan Zhang,Wenju Cui,Yuzhu Cao,Tao Tan,Jie Liu,Yunsong Peng,Jian Zheng*

Main category: cs.CV

TL;DR: A paired image generation method using a diffusion guider improves lesion segmentation in DBT images by generating high-quality paired images and annotations, addressing data scarcity.


<details>
  <summary>Details</summary>
Motivation: High concealment of mass lesions in DBT images makes manual annotation difficult, leading to a lack of annotated data for training. Existing diffusion models struggle with lesion feature learning and lack annotation generation.

Method: Proposes a paired image generation method with a diffusion guider for conditional diffusion models, enabling generation of paired DBT slices and lesion masks without external conditions.

Result: The method improves generation quality and alleviates annotated data shortage, enhancing performance in downstream segmentation tasks.

Conclusion: The proposed method effectively addresses data scarcity and improves lesion segmentation in DBT images by generating paired images and annotations.

Abstract: The segmentation of mass lesions in digital breast tomosynthesis (DBT) images
is very significant for the early screening of breast cancer. However, the
high-density breast tissue often leads to high concealment of the mass lesions,
which makes manual annotation difficult and time-consuming. As a result, there
is a lack of annotated data for model training. Diffusion models are commonly
used for data augmentation, but the existing methods face two challenges.
First, due to the high concealment of lesions, it is difficult for the model to
learn the features of the lesion area. This leads to the low generation quality
of the lesion areas, thus limiting the quality of the generated images. Second,
existing methods can only generate images and cannot generate corresponding
annotations, which restricts the usability of the generated images in
supervised training. In this work, we propose a paired image generation method.
The method does not require external conditions and can achieve the generation
of paired images by training an extra diffusion guider for the conditional
diffusion model. During the experimental phase, we generated paired DBT slices
and mass lesion masks. Then, we incorporated them into the supervised training
process of the mass lesion segmentation task. The experimental results show
that our method can improve the generation quality without external conditions.
Moreover, it contributes to alleviating the shortage of annotated data, thus
enhancing the performance of downstream tasks.

</details>


### [56] [Training Self-Supervised Depth Completion Using Sparse Measurements and a Single Image](https://arxiv.org/abs/2507.14845)
*Rizhao Fan,Zhigen Li,Heping Li,Ning An*

Main category: cs.CV

TL;DR: A novel self-supervised depth completion method using only sparse depth and single images, eliminating the need for dense labels or multi-frame data.


<details>
  <summary>Details</summary>
Motivation: Overcoming the limitations of costly dense annotations and multi-frame dependencies in existing depth completion methods.

Method: Leverages depth distribution characteristics and novel loss functions for depth propagation, enhanced by segmentation maps from vision foundation models.

Result: Effective depth completion without dense labels or additional frames, validated through experiments.

Conclusion: The proposed method offers a practical solution for depth completion in static or single-frame scenarios.

Abstract: Depth completion is an important vision task, and many efforts have been made
to enhance the quality of depth maps from sparse depth measurements. Despite
significant advances, training these models to recover dense depth from sparse
measurements remains a challenging problem. Supervised learning methods rely on
dense depth labels to predict unobserved regions, while self-supervised
approaches require image sequences to enforce geometric constraints and
photometric consistency between frames. However, acquiring dense annotations is
costly, and multi-frame dependencies limit the applicability of self-supervised
methods in static or single-frame scenarios. To address these challenges, we
propose a novel self-supervised depth completion paradigm that requires only
sparse depth measurements and their corresponding image for training. Unlike
existing methods, our approach eliminates the need for dense depth labels or
additional images captured from neighboring viewpoints. By leveraging the
characteristics of depth distribution, we design novel loss functions that
effectively propagate depth information from observed points to unobserved
regions. Additionally, we incorporate segmentation maps generated by vision
foundation models to further enhance depth estimation. Extensive experiments
demonstrate the effectiveness of our proposed method.

</details>


### [57] [Grounding Degradations in Natural Language for All-In-One Video Restoration](https://arxiv.org/abs/2507.14851)
*Muhammad Kamran Janjua,Amirhosein Ghasemabadi,Kunlin Zhang,Mohammad Salameh,Chao Gao,Di Niu*

Main category: cs.CV

TL;DR: Proposes an all-in-one video restoration framework using foundation models for interpretable guidance, introduces new benchmarks, and achieves state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: To address the lack of interpretability and flexibility in video restoration by leveraging natural language and foundation models without requiring degradation knowledge.

Method: Uses degradation-aware semantic context grounded in natural language via foundation models, disentangling the model during inference for efficiency.

Result: Achieves state-of-the-art performance on proposed benchmarks, including multi-degradation and time-varying composite degradation settings.

Conclusion: The framework offers interpretable, flexible, and efficient video restoration, with new benchmarks standardizing future research.

Abstract: In this work, we propose an all-in-one video restoration framework that
grounds degradation-aware semantic context of video frames in natural language
via foundation models, offering interpretable and flexible guidance. Unlike
prior art, our method assumes no degradation knowledge in train or test time
and learns an approximation to the grounded knowledge such that the foundation
model can be safely disentangled during inference adding no extra cost.
Further, we call for standardization of benchmarks in all-in-one video
restoration, and propose two benchmarks in multi-degradation setting,
three-task (3D) and four-task (4D), and two time-varying composite degradation
benchmarks; one of the latter being our proposed dataset with varying snow
intensity, simulating how weather degradations affect videos naturally. We
compare our method with prior works and report state-of-the-art performance on
all benchmarks.

</details>


### [58] [An Uncertainty-aware DETR Enhancement Framework for Object Detection](https://arxiv.org/abs/2507.14855)
*Xingshu Chen,Sicheng Yu,Chong Cheng,Hao Wang,Ting Tian*

Main category: cs.CV

TL;DR: The paper introduces an uncertainty-aware enhancement framework for DETR-based object detectors, improving localization accuracy and modeling prediction uncertainty using Gaussian distributions and Gromov-Wasserstein distance.


<details>
  <summary>Details</summary>
Motivation: Conventional object detectors lack uncertainty modeling, limiting robustness. This work aims to address this gap by enhancing DETR-based detectors with uncertainty awareness.

Method: The method models bounding boxes as Gaussian distributions, uses Gromov-Wasserstein distance in the loss function, and employs a Bayes Risk formulation to filter high-risk predictions. It also quantifies uncertainty via confidence intervals.

Result: Experiments on COCO, LISC, and WBCDD datasets show improved performance and scalability across general and domain-specific tasks.

Conclusion: The framework effectively integrates uncertainty awareness into DETR variants, enhancing detection reliability and achieving state-of-the-art results in specialized tasks.

Abstract: This paper investigates the problem of object detection with a focus on
improving both the localization accuracy of bounding boxes and explicitly
modeling prediction uncertainty. Conventional detectors rely on deterministic
bounding box regression, ignoring uncertainty in predictions and limiting model
robustness. In this paper, we propose an uncertainty-aware enhancement
framework for DETR-based object detectors. We model bounding boxes as
multivariate Gaussian distributions and incorporate the Gromov-Wasserstein
distance into the loss function to better align the predicted and ground-truth
distributions. Building on this, we derive a Bayes Risk formulation to filter
high-risk information and improve detection reliability. We also propose a
simple algorithm to quantify localization uncertainty via confidence intervals.
Experiments on the COCO benchmark show that our method can be effectively
integrated into existing DETR variants, enhancing their performance. We further
extend our framework to leukocyte detection tasks, achieving state-of-the-art
results on the LISC and WBCDD datasets. These results confirm the scalability
of our framework across both general and domain-specific detection tasks. Code
page:
https://github.com/ParadiseforAndaChen/An-Uncertainty-aware-DETR-Enhancement-Framework-for-Object-Detection.

</details>


### [59] [Hybrid-supervised Hypergraph-enhanced Transformer for Micro-gesture Based Emotion Recognition](https://arxiv.org/abs/2507.14867)
*Zhaoqiang Xia,Hexiang Huang,Haoyu Chen,Xiaoyi Feng,Guoying Zhao*

Main category: cs.CV

TL;DR: The paper proposes a hypergraph-enhanced Transformer framework for emotion recognition using micro-gestures, achieving state-of-the-art performance on public datasets.


<details>
  <summary>Details</summary>
Motivation: Micro-gestures can reveal human emotions but lack sufficient exploration in modeling. This work aims to bridge this gap by leveraging advanced deep learning techniques.

Method: A hybrid-supervised framework with hypergraph-enhanced Transformer, combining self-supervised reconstruction (via upsampling) and supervised emotion recognition (via a shallow head).

Result: The method outperforms existing approaches on iMiGUE and SMG datasets under multiple metrics.

Conclusion: The proposed framework effectively models micro-gestures for emotion recognition, demonstrating superior performance and potential for affective computing.

Abstract: Micro-gestures are unconsciously performed body gestures that can convey the
emotion states of humans and start to attract more research attention in the
fields of human behavior understanding and affective computing as an emerging
topic. However, the modeling of human emotion based on micro-gestures has not
been explored sufficiently. In this work, we propose to recognize the emotion
states based on the micro-gestures by reconstructing the behavior patterns with
a hypergraph-enhanced Transformer in a hybrid-supervised framework. In the
framework, hypergraph Transformer based encoder and decoder are separately
designed by stacking the hypergraph-enhanced self-attention and multiscale
temporal convolution modules. Especially, to better capture the subtle motion
of micro-gestures, we construct a decoder with additional upsampling operations
for a reconstruction task in a self-supervised learning manner. We further
propose a hypergraph-enhanced self-attention module where the hyperedges
between skeleton joints are gradually updated to present the relationships of
body joints for modeling the subtle local motion. Lastly, for exploiting the
relationship between the emotion states and local motion of micro-gestures, an
emotion recognition head from the output of encoder is designed with a shallow
architecture and learned in a supervised way. The end-to-end framework is
jointly trained in a one-stage way by comprehensively utilizing
self-reconstruction and supervision information. The proposed method is
evaluated on two publicly available datasets, namely iMiGUE and SMG, and
achieves the best performance under multiple metrics, which is superior to the
existing methods.

</details>


### [60] [Region-aware Depth Scale Adaptation with Sparse Measurements](https://arxiv.org/abs/2507.14879)
*Rizhao Fan,Tianfang Ma,Zhigen Li,Ning An,Jian Cheng*

Main category: cs.CV

TL;DR: A non-learning-based method uses sparse depth measurements to convert relative-scale depth predictions from foundation models into metric-scale depth, avoiding retraining and preserving generalization.


<details>
  <summary>Details</summary>
Motivation: Foundation models for depth prediction produce relative-scale outputs, limiting real-world application. Existing scale adaptation methods are costly and reduce generalization.

Method: Leverages sparse depth measurements to adapt relative-scale predictions to metric-scale without retraining or fine-tuning.

Result: Effective conversion to metric depth without additional computational costs or loss of generalization.

Conclusion: The approach bridges the gap between relative and metric depth, maintaining the foundation models' strengths.

Abstract: In recent years, the emergence of foundation models for depth prediction has
led to remarkable progress, particularly in zero-shot monocular depth
estimation. These models generate impressive depth predictions; however, their
outputs are often in relative scale rather than metric scale. This limitation
poses challenges for direct deployment in real-world applications. To address
this, several scale adaptation methods have been proposed to enable foundation
models to produce metric depth. However, these methods are typically costly, as
they require additional training on new domains and datasets. Moreover,
fine-tuning these models often compromises their original generalization
capabilities, limiting their adaptability across diverse scenes. In this paper,
we introduce a non-learning-based approach that leverages sparse depth
measurements to adapt the relative-scale predictions of foundation models into
metric-scale depth. Our method requires neither retraining nor fine-tuning,
thereby preserving the strong generalization ability of the original foundation
models while enabling them to produce metric depth. Experimental results
demonstrate the effectiveness of our approach, high-lighting its potential to
bridge the gap between relative and metric depth without incurring additional
computational costs or sacrificing generalization ability.

</details>


### [61] [BeatFormer: Efficient motion-robust remote heart rate estimation through unsupervised spectral zoomed attention filters](https://arxiv.org/abs/2507.14885)
*Joaquim Comas,Federico Sukno*

Main category: cs.CV

TL;DR: BeatFormer is a lightweight spectral attention model for rPPG estimation, combining deep learning and handcrafted methods, trained without PPG/HR labels using Spectral Contrastive Learning (SCL). It shows robustness in cross-dataset evaluations under motion.


<details>
  <summary>Details</summary>
Motivation: Deep learning for rPPG relies on large datasets, while handcrafted methods generalize better but lack performance in complex conditions. A hybrid approach is needed.

Method: BeatFormer integrates zoomed orthonormal complex attention and frequency-domain energy measurement. SCL enables training without PPG/HR labels.

Result: Validated on PURE, UBFC-rPPG, and MMPD datasets, BeatFormer demonstrates robustness, especially in cross-dataset evaluations under motion.

Conclusion: BeatFormer effectively combines deep learning and handcrafted methods, offering a lightweight, efficient solution for rPPG estimation with strong generalization.

Abstract: Remote photoplethysmography (rPPG) captures cardiac signals from facial
videos and is gaining attention for its diverse applications. While deep
learning has advanced rPPG estimation, it relies on large, diverse datasets for
effective generalization. In contrast, handcrafted methods utilize
physiological priors for better generalization in unseen scenarios like motion
while maintaining computational efficiency. However, their linear assumptions
limit performance in complex conditions, where deep learning provides superior
pulsatile information extraction. This highlights the need for hybrid
approaches that combine the strengths of both methods. To address this, we
present BeatFormer, a lightweight spectral attention model for rPPG estimation,
which integrates zoomed orthonormal complex attention and frequency-domain
energy measurement, enabling a highly efficient model. Additionally, we
introduce Spectral Contrastive Learning (SCL), which allows BeatFormer to be
trained without any PPG or HR labels. We validate BeatFormer on the PURE,
UBFC-rPPG, and MMPD datasets, demonstrating its robustness and performance,
particularly in cross-dataset evaluations under motion scenarios.

</details>


### [62] [TriCLIP-3D: A Unified Parameter-Efficient Framework for Tri-Modal 3D Visual Grounding based on CLIP](https://arxiv.org/abs/2507.14904)
*Fan Li,Zanyi Wang,Zeyi Huang,Guang Dai,Jingdong Wang,Mengmeng Wang*

Main category: cs.CV

TL;DR: The paper proposes a unified 2D pre-trained multi-modal network (GARF) for 3D visual grounding, simplifying architecture by processing RGB images, text, and point clouds together, reducing parameters by 58% and improving performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on separate encoders for different modalities, leading to inefficiency and complexity. The goal is to unify feature extraction and fusion for better adaptability and performance.

Method: Leverages a 2D CLIP bi-modal model with adapter-based fine-tuning, introduces GARF for geometric feature fusion, and integrates textual features with a multi-modal decoder for cross-modal understanding.

Result: Reduces trainable parameters by 58%, improves 3D detection by 6.52%, and 3D visual grounding by 6.25%.

Conclusion: The proposed unified framework simplifies architecture, enhances efficiency, and achieves superior performance in 3D visual grounding tasks.

Abstract: 3D visual grounding allows an embodied agent to understand visual information
in real-world 3D environments based on human instructions, which is crucial for
embodied intelligence. Existing 3D visual grounding methods typically rely on
separate encoders for different modalities (e.g., RGB images, text, and 3D
point clouds), resulting in large and complex models that are inefficient to
train. While some approaches use pre-trained 2D multi-modal models like CLIP
for 3D tasks, they still struggle with aligning point cloud data to 2D
encoders. As a result, these methods continue to depend on 3D encoders for
feature extraction, further increasing model complexity and training
inefficiency. In this paper, we propose a unified 2D pre-trained multi-modal
network to process all three modalities (RGB images, text, and point clouds),
significantly simplifying the architecture. By leveraging a 2D CLIP bi-modal
model with adapter-based fine-tuning, this framework effectively adapts to the
tri-modal setting, improving both adaptability and performance across
modalities. Our Geometric-Aware 2D-3D Feature Recovery and Fusion (GARF) module
is designed to fuse geometric multi-scale features from point clouds and
images. We then integrate textual features for final modality fusion and
introduce a multi-modal decoder to facilitate deep cross-modal understanding.
Together, our method achieves unified feature extraction and fusion across the
three modalities, enabling an end-to-end 3D visual grounding model. Compared to
the baseline, our method reduces the number of trainable parameters by
approximately 58\%, while achieving a 6.52\% improvement in the 3D detection
task and a 6.25\% improvement in the 3D visual grounding task.

</details>


### [63] [Semantic-Aware Representation Learning for Multi-label Image Classification](https://arxiv.org/abs/2507.14918)
*Ren-Dong Xie,Zhi-Fen He,Bo Li,Bin Liu,Jin-Yan Hu*

Main category: cs.CV

TL;DR: Proposes Semantic-Aware Representation Learning (SARL) for multi-label image classification, improving accuracy by addressing noise and object localization issues.


<details>
  <summary>Details</summary>
Motivation: Existing methods using attention or GCNs may produce noisy representations and fail to locate objects precisely.

Method: Uses label semantic-related feature learning, optimal transport-based attention, and regional score aggregation for precise multi-label prediction.

Result: Outperforms existing methods on PASCAL VOC 2007 and MS-COCO datasets.

Conclusion: SARL effectively improves multi-label image classification by enhancing semantic alignment and representation.

Abstract: Multi-label image classification, an important research area in computer
vision, focuses on identifying multiple labels or concepts within an image.
Existing approaches often employ attention mechanisms or graph convolutional
networks (GCNs) to learn image representation. However, this representation may
contain noise and may not locate objects precisely. Therefore, this paper
proposes a Semantic-Aware Representation Learning (SARL) for multi-label image
classification. First, a label semantic-related feature learning module is
utilized to extract semantic-related features. Then, an optimal transport-based
attention mechanism is designed to obtain semantically aligned image
representation. Finally, a regional score aggregation strategy is used for
multi-label prediction. Experimental results on two benchmark datasets, PASCAL
VOC 2007 and MS-COCO, demonstrate the superiority of SARL over existing
methods.

</details>


### [64] [Stereo-GS: Multi-View Stereo Vision Model for Generalizable 3D Gaussian Splatting Reconstruction](https://arxiv.org/abs/2507.14921)
*Xiufeng Huang,Ka Chun Cheung,Runmin Cong,Simon See,Renjie Wan*

Main category: cs.CV

TL;DR: A disentangled framework for efficient 3D Gaussian prediction, reducing computational demands while maintaining high-quality 3D reconstruction.


<details>
  <summary>Details</summary>
Motivation: Current methods for 3D Gaussian Splatting reconstruction are computationally intensive, rely on large datasets, and entangle geometry and appearance prediction, leading to slow speeds.

Method: Uses a stereo vision backbone to extract local image features, fuses them via global attention, and employs dedicated heads for geometry and appearance prediction, refined for high-quality output.

Result: Achieves pose-free 3D reconstruction, improving robustness and practicality while reducing resource demands.

Conclusion: The proposed method offers an efficient, scalable solution for real-world 3D content generation.

Abstract: Generalizable 3D Gaussian Splatting reconstruction showcases advanced
Image-to-3D content creation but requires substantial computational resources
and large datasets, posing challenges to training models from scratch. Current
methods usually entangle the prediction of 3D Gaussian geometry and appearance,
which rely heavily on data-driven priors and result in slow regression speeds.
To address this, we propose \method, a disentangled framework for efficient 3D
Gaussian prediction. Our method extracts features from local image pairs using
a stereo vision backbone and fuses them via global attention blocks. Dedicated
point and Gaussian prediction heads generate multi-view point-maps for geometry
and Gaussian features for appearance, combined as GS-maps to represent the 3DGS
object. A refinement network enhances these GS-maps for high-quality
reconstruction. Unlike existing methods that depend on camera parameters, our
approach achieves pose-free 3D reconstruction, improving robustness and
practicality. By reducing resource demands while maintaining high-quality
outputs, \method provides an efficient, scalable solution for real-world 3D
content generation.

</details>


### [65] [3-Dimensional CryoEM Pose Estimation and Shift Correction Pipeline](https://arxiv.org/abs/2507.14924)
*Kaishva Chintan Shah,Virajith Boddapati,Karthik S. Gurumoorthy,Sandip Kaledhonkar,Ajit Rajwade*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Accurate pose estimation and shift correction are key challenges in cryo-EM
due to the very low SNR, which directly impacts the fidelity of 3D
reconstructions. We present an approach for pose estimation in cryo-EM that
leverages multi-dimensional scaling (MDS) techniques in a robust manner to
estimate the 3D rotation matrix of each particle from pairs of dihedral angles.
We express the rotation matrix in the form of an axis of rotation and a unit
vector in the plane perpendicular to the axis. The technique leverages the
concept of common lines in 3D reconstruction from projections. However, common
line estimation is ridden with large errors due to the very low SNR of cryo-EM
projection images. To address this challenge, we introduce two complementary
components: (i) a robust joint optimization framework for pose estimation based
on an $\ell_1$-norm objective or a similar robust norm, which simultaneously
estimates rotation axes and in-plane vectors while exactly enforcing unit norm
and orthogonality constraints via projected coordinate descent; and (ii) an
iterative shift correction algorithm that estimates consistent in-plane
translations through a global least-squares formulation. While prior approaches
have leveraged such embeddings and common-line geometry for orientation
recovery, existing formulations typically rely on $\ell_2$-based objectives
that are sensitive to noise, and enforce geometric constraints only
approximately. These choices, combined with a sequential pipeline structure,
can lead to compounding errors and suboptimal reconstructions in low-SNR
regimes. Our pipeline consistently outperforms prior methods in both Euler
angle accuracy and reconstruction fidelity, as measured by the Fourier Shell
Correlation (FSC).

</details>


### [66] [Probabilistic smooth attention for deep multiple instance learning in medical imaging](https://arxiv.org/abs/2507.14932)
*Francisco M. Castro-Macías,Pablo Morales-Álvarez,Yunan Wu,Rafael Molina,Aggelos K. Katsaggelos*

Main category: cs.CV

TL;DR: A novel probabilistic framework for Multiple Instance Learning (MIL) in medical imaging improves predictive performance and provides interpretable uncertainty maps.


<details>
  <summary>Details</summary>
Motivation: Addressing the deterministic treatment of attention values in deep MIL methods, which may overlook uncertainty in instance contributions.

Method: Proposes a probabilistic framework estimating distributions over attention values, capturing both global and local interactions.

Result: Outperforms eleven state-of-the-art baselines across three medical datasets, achieving top predictive performance.

Conclusion: The probabilistic approach enhances interpretability through uncertainty maps, aiding illness localization.

Abstract: The Multiple Instance Learning (MIL) paradigm is attracting plenty of
attention in medical imaging classification, where labeled data is scarce. MIL
methods cast medical images as bags of instances (e.g. patches in whole slide
images, or slices in CT scans), and only bag labels are required for training.
Deep MIL approaches have obtained promising results by aggregating
instance-level representations via an attention mechanism to compute the
bag-level prediction. These methods typically capture both local interactions
among adjacent instances and global, long-range dependencies through various
mechanisms. However, they treat attention values deterministically, potentially
overlooking uncertainty in the contribution of individual instances. In this
work we propose a novel probabilistic framework that estimates a probability
distribution over the attention values, and accounts for both global and local
interactions. In a comprehensive evaluation involving {\color{review} eleven}
state-of-the-art baselines and three medical datasets, we show that our
approach achieves top predictive performance in different metrics. Moreover,
the probabilistic treatment of the attention provides uncertainty maps that are
interpretable in terms of illness localization.

</details>


### [67] [Open-set Cross Modal Generalization via Multimodal Unified Representation](https://arxiv.org/abs/2507.14935)
*Hai Huang,Yan Xia,Shulei Wang,Hanting Wang,Minghui Fang,Shengpeng Ji,Sashuai Zhou,Tao Jin,Zhou Zhao*

Main category: cs.CV

TL;DR: The paper introduces Open-set Cross Modal Generalization (OSCMG), extending CMG to open-set environments, and proposes MICU with FCMI and CUJP components to address its challenges.


<details>
  <summary>Details</summary>
Motivation: Prior work lacks consideration for open-set conditions in multimodal unified representations, limiting real-world applicability.

Method: Proposes MICU with Fine-Coarse Masked multimodal InfoNCE (FCMI) for alignment and Cross modal Unified Jigsaw Puzzles (CUJP) for feature diversity.

Result: Extensive experiments validate MICU's effectiveness in CMG and OSCMG tasks.

Conclusion: MICU successfully addresses OSCMG challenges, enhancing generalization to unseen classes in open-set conditions.

Abstract: This paper extends Cross Modal Generalization (CMG) to open-set environments
by proposing the more challenging Open-set Cross Modal Generalization (OSCMG)
task. This task evaluates multimodal unified representations in open-set
conditions, addressing the limitations of prior closed-set cross-modal
evaluations. OSCMG requires not only cross-modal knowledge transfer but also
robust generalization to unseen classes within new modalities, a scenario
frequently encountered in real-world applications. Existing multimodal unified
representation work lacks consideration for open-set environments. To tackle
this, we propose MICU, comprising two key components: Fine-Coarse Masked
multimodal InfoNCE (FCMI) and Cross modal Unified Jigsaw Puzzles (CUJP). FCMI
enhances multimodal alignment by applying contrastive learning at both holistic
semantic and temporal levels, incorporating masking to enhance generalization.
CUJP enhances feature diversity and model uncertainty by integrating
modality-agnostic feature selection with self-supervised learning, thereby
strengthening the model's ability to handle unknown categories in open-set
tasks. Extensive experiments on CMG and the newly proposed OSCMG validate the
effectiveness of our approach. The code is available at
https://github.com/haihuangcode/CMG.

</details>


### [68] [Polymorph: Energy-Efficient Multi-Label Classification for Video Streams on Embedded Devices](https://arxiv.org/abs/2507.14959)
*Saeid Ghafouri,Mohsen Fayyaz,Xiangchen Li,Deepu John,Bo Ji,Dimitrios Nikolopoulos,Hans Vandierendonck*

Main category: cs.CV

TL;DR: Polymorph is a framework for efficient real-time multi-label video classification on embedded devices by leveraging label sparsity and co-occurrence patterns, reducing energy use by 40% and improving mAP by 9 points.


<details>
  <summary>Details</summary>
Motivation: Limited compute and energy budgets on embedded devices challenge real-time multi-label video classification, but structural properties like label sparsity and co-occurrence can optimize inference.

Method: Polymorph uses lightweight Low Rank Adapters (LoRA) per frame, dynamically selecting and composing adapters based on active labels, avoiding full-model switching.

Result: Achieves 40% lower energy consumption and 9-point mAP improvement on the TAO dataset.

Conclusion: Polymorph's modular, context-aware approach efficiently reduces latency and energy use while maintaining accuracy, making it suitable for embedded devices.

Abstract: Real-time multi-label video classification on embedded devices is constrained
by limited compute and energy budgets. Yet, video streams exhibit structural
properties such as label sparsity, temporal continuity, and label co-occurrence
that can be leveraged for more efficient inference. We introduce Polymorph, a
context-aware framework that activates a minimal set of lightweight Low Rank
Adapters (LoRA) per frame. Each adapter specializes in a subset of classes
derived from co-occurrence patterns and is implemented as a LoRA weight over a
shared backbone. At runtime, Polymorph dynamically selects and composes only
the adapters needed to cover the active labels, avoiding full-model switching
and weight merging. This modular strategy improves scalability while reducing
latency and energy overhead. Polymorph achieves 40% lower energy consumption
and improves mAP by 9 points over strong baselines on the TAO dataset.
Polymorph is open source at https://github.com/inference-serving/polymorph/.

</details>


### [69] [Decision PCR: Decision version of the Point Cloud Registration task](https://arxiv.org/abs/2507.14965)
*Yaojie Zhang,Tianlun Huang,Weijun Wang,Wei Feng*

Main category: cs.CV

TL;DR: A deep learning-based classifier is proposed to evaluate low-overlap point cloud registration, outperforming traditional metrics and enhancing existing methods.


<details>
  <summary>Details</summary>
Motivation: Traditional metrics fail under low inlier ratios, necessitating a reliable evaluation method for low-overlap PCR.

Method: Construct a dataset from 3DMatch, train a deep learning classifier, and integrate it into PCR pipelines.

Result: Achieves 86.97% registration recall on 3DLoMatch and generalizes well on ETH dataset.

Conclusion: The proposed classifier effectively improves PCR performance and generalizes across datasets.

Abstract: Low-overlap point cloud registration (PCR) remains a significant challenge in
3D vision. Traditional evaluation metrics, such as Maximum Inlier Count, become
ineffective under extremely low inlier ratios. In this paper, we revisit the
registration result evaluation problem and identify the Decision version of the
PCR task as the fundamental problem. To address this Decision PCR task, we
propose a data-driven approach. First, we construct a corresponding dataset
based on the 3DMatch dataset. Then, a deep learning-based classifier is trained
to reliably assess registration quality, overcoming the limitations of
traditional metrics. To our knowledge, this is the first comprehensive study to
address this task through a deep learning framework. We incorporate this
classifier into standard PCR pipelines. When integrated with our approach,
existing state-of-the-art PCR methods exhibit significantly enhanced
registration performance. For example, combining our framework with
GeoTransformer achieves a new SOTA registration recall of 86.97\% on the
challenging 3DLoMatch benchmark. Our method also demonstrates strong
generalization capabilities on the unseen outdoor ETH dataset.

</details>


### [70] [Hierarchical Cross-modal Prompt Learning for Vision-Language Models](https://arxiv.org/abs/2507.14976)
*Hao Zheng,Shunzhi Yang,Zhuoxin He,Jinfeng Yang,Zhenhua Huang*

Main category: cs.CV

TL;DR: HiCroPL is a hierarchical cross-modal prompt learning framework addressing modality isolation and semantic decay in VLMs, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Adapting large-scale VLMs like CLIP to downstream tasks without losing generalization is challenging due to modality isolation and hierarchical semantic decay.

Method: HiCroPL establishes bidirectional knowledge flow between text and vision, using a hierarchical knowledge mapper and lightweight layer-specific proxies.

Result: Outperforms existing methods on 11 benchmarks across four tasks.

Conclusion: HiCroPL effectively enhances generalization in VLMs by refining cross-modal semantics hierarchically.

Abstract: Pre-trained Vision-Language Models (VLMs) such as CLIP have shown excellent
generalization abilities. However, adapting these large-scale models to
downstream tasks while preserving their generalization capabilities remains
challenging. Although prompt learning methods have shown promise, they suffer
from two fundamental bottlenecks that limit generalization: (a) modality
isolation, and (b) hierarchical semantic decay. To address these limitations,
we propose HiCroPL, a Hierarchical Cross-modal Prompt Learning framework that
establishes bidirectional knowledge flow between text and vision modalities,
enabling them to refine their semantics mutually. HiCroPL routes knowledge
flows by leveraging the complementary strengths of text and vision. In early
layers, text prompts inject relatively clear semantics into visual prompts
through a hierarchical knowledge mapper, enhancing the representation of
low-level visual semantics. In later layers, visual prompts encoding specific
task-relevant objects flow back to refine text prompts, enabling deeper
alignment. Crucially, our hierarchical knowledge mapper allows representations
at multi-scales to be fused, ensuring that deeper representations retain
transferable shallow semantics thereby enhancing generalization. We further
introduce a lightweight layer-specific knowledge proxy to enable efficient
cross-modal interactions. Extensive evaluations across four tasks demonstrate
HiCroPL's superior performance, achieving state-of-the-art results on 11
benchmarks with significant improvements. Code is available at:
https://github.com/zzeoZheng/HiCroPL.

</details>


### [71] [Language Integration in Fine-Tuning Multimodal Large Language Models for Image-Based Regression](https://arxiv.org/abs/2507.14997)
*Roy H. Jennings,Genady Paikin,Roy Shaul,Evgeny Soloveichik*

Main category: cs.CV

TL;DR: Current MLLM approaches for image-based regression underperform due to preset vocabularies and generic prompts. RvTC, a bin-based method, outperforms by using flexible bins and data-specific prompts, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to leverage semantic understanding from textual input, performing no better than image-only models. The study aims to improve MLLM performance by addressing these limitations.

Method: Proposes RvTC, replacing vocabulary-constrained classification with a flexible bin-based approach and using data-specific prompts to enhance cross-modal understanding.

Result: RvTC achieves state-of-the-art performance on four datasets. Semantic prompts (e.g., challenge titles) boost correlations from 0.83 to 0.90 on AVA.

Conclusion: Meaningful textual context is crucial for MLLMs in multimodal regression tasks, as demonstrated by RvTC's success with semantic prompts.

Abstract: Multimodal Large Language Models (MLLMs) show promise for image-based
regression tasks, but current approaches face key limitations. Recent methods
fine-tune MLLMs using preset output vocabularies and generic task-level prompts
(e.g., "How would you rate this image?"), assuming this mimics human rating
behavior. Our analysis reveals these approaches provide no benefit over
image-only training. Models using preset vocabularies and generic prompts
perform equivalently to image-only models, failing to leverage semantic
understanding from textual input. We propose Regression via Transformer-Based
Classification (RvTC), which replaces vocabulary-constrained classification
with a flexible bin-based approach. Unlike approaches that address
discretization errors through complex distributional modeling, RvTC eliminates
manual vocabulary crafting through straightforward bin increase, achieving
state-of-the-art performance on four image assessment datasets using only
images. More importantly, we demonstrate that data-specific prompts
dramatically improve performance. Unlike generic task descriptions, prompts
containing semantic information about specific images enable MLLMs to leverage
cross-modal understanding. On the AVA dataset, adding challenge titles to
prompts improves correlations from 0.83 to 0.90, a new state-of-the-art. We
demonstrate through empirical evidence from the AVA and AGIQA-3k datasets that
MLLMs benefit from semantic prompt information surpassing mere statistical
biases. This underscores the importance of incorporating meaningful textual
context in multimodal regression tasks.

</details>


### [72] [Axis-Aligned Document Dewarping](https://arxiv.org/abs/2507.15000)
*Chaoyun Wang,I-Chao Shen,Takeo Igarashi,Nanning Zheng,Caigui Jiang*

Main category: cs.CV

TL;DR: The paper introduces an axis-aligned geometric constraint for document dewarping, improving performance and robustness.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack geometric properties in dewarping; leveraging axis-aligned features can enhance results.

Method: Proposes axis-aligned geometric constraint in training, preprocessing in inference, and a new AAD metric.

Result: Achieves SOTA on benchmarks with 18.2%~34.5% improvement on AAD.

Conclusion: Axis-aligned constraints and preprocessing significantly improve document dewarping.

Abstract: Document dewarping is crucial for many applications. However, existing
learning-based methods primarily rely on supervised regression with annotated
data without leveraging the inherent geometric properties in physical documents
to the dewarping process. Our key insight is that a well-dewarped document is
characterized by transforming distorted feature lines into axis-aligned ones.
This property aligns with the inherent axis-aligned nature of the discrete grid
geometry in planar documents. In the training phase, we propose an axis-aligned
geometric constraint to enhance document dewarping. In the inference phase, we
propose an axis alignment preprocessing strategy to reduce the dewarping
difficulty. In the evaluation phase, we introduce a new metric, Axis-Aligned
Distortion (AAD), that not only incorporates geometric meaning and aligns with
human visual perception but also demonstrates greater robustness. As a result,
our method achieves SOTA results on multiple existing benchmarks and achieves
18.2%~34.5% improvements on the AAD metric.

</details>


### [73] [FastSmoothSAM: A Fast Smooth Method For Segment Anything Model](https://arxiv.org/abs/2507.15008)
*Jiasheng Xu,Yewang Chen*

Main category: cs.CV

TL;DR: FastSAM improves real-time segmentation but produces jagged edges. This paper introduces B-Spline curve fitting to refine edges, enhancing accuracy without losing real-time performance.


<details>
  <summary>Details</summary>
Motivation: FastSAM's jagged edges limit its practical use in applications requiring precise edge recognition, such as industrial automation and medical imaging.

Method: A four-stage refining process using B-Spline curve fitting is applied to smooth edges in FastSAM, involving two rounds of curve fitting.

Result: The method improves edge quality and analytical accuracy while maintaining real-time processing.

Conclusion: The refinement enhances FastSAM's utility in real-world applications by balancing precision and efficiency.

Abstract: Accurately identifying and representing object edges is a challenging task in
computer vision and image processing. The Segment Anything Model (SAM) has
significantly influenced the field of image segmentation, but suffers from high
memory consumption and long inference times, limiting its efficiency in
real-time applications. To address these limitations, Fast Segment Anything
(FastSAM) was proposed, achieving real-time segmentation. However, FastSAM
often generates jagged edges that deviate from the true object shapes.
Therefore, this paper introduces a novel refinement approach using B-Spline
curve fitting techniques to enhance the edge quality in FastSAM. Leveraging the
robust shape control and flexible geometric construction of B-Splines, a
four-stage refining process involving two rounds of curve fitting is employed
to effectively smooth jagged edges. This approach significantly improves the
visual quality and analytical accuracy of object edges without compromising
critical geometric information. The proposed method improves the practical
utility of FastSAM by improving segmentation accuracy while maintaining
real-time processing capabilities. This advancement unlocks greater potential
for FastSAM technology in various real-world scenarios, such as industrial
automation, medical imaging, and autonomous systems, where precise and
efficient edge recognition is crucial.

</details>


### [74] [Towards Video Thinking Test: A Holistic Benchmark for Advanced Video Reasoning and Understanding](https://arxiv.org/abs/2507.15028)
*Yuanhan Zhang,Yunice Chew,Yuhao Dong,Aria Leo,Bo Hu,Ziwei Liu*

Main category: cs.CV

TL;DR: The paper introduces Video-TT, a benchmark to assess video LLMs' correctness and robustness in video interpretation, revealing a significant gap between models and human performance.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks fail to measure the gap between video LLMs and human intelligence in maintaining correctness and robustness in video understanding.

Method: Video-TT includes 1,000 YouTube Shorts videos with open-ended and adversarial questions to evaluate visual and narrative complexity.

Result: The evaluation shows a notable performance gap between video LLMs and humans in interpreting complex videos.

Conclusion: Video-TT highlights the need for improving video LLMs to match human-like correctness and robustness in video understanding.

Abstract: Human intelligence requires correctness and robustness, with the former being
foundational for the latter. In video understanding, correctness ensures the
accurate interpretation of visual content, and robustness maintains consistent
performance in challenging conditions. Despite advances in video large language
models (video LLMs), existing benchmarks inadequately reflect the gap between
these models and human intelligence in maintaining correctness and robustness
in video interpretation. We introduce the Video Thinking Test (Video-TT), to
assess if video LLMs can interpret real-world videos as effectively as humans.
Video-TT reflects genuine gaps in understanding complex visual narratives, and
evaluates robustness against natural adversarial questions. Video-TT comprises
1,000 YouTube Shorts videos, each with one open-ended question and four
adversarial questions that probe visual and narrative complexity. Our
evaluation shows a significant gap between video LLMs and human performance.

</details>


### [75] [OpenBreastUS: Benchmarking Neural Operators for Wave Imaging Using Breast Ultrasound Computed Tomography](https://arxiv.org/abs/2507.15035)
*Zhijun Zeng,Youjia Zheng,Hao Hu,Zeyuan Dong,Yihang Zheng,Xinliang Liu,Jinzhuo Wang,Zuoqiang Shi,Linfeng Zhang,Yubing Li,He Sun*

Main category: cs.CV

TL;DR: OpenBreastUS is a large-scale wave equation dataset for realistic breast imaging, enabling benchmarking of neural operators for forward and inverse tasks, and demonstrating efficient in vivo imaging.


<details>
  <summary>Details</summary>
Motivation: Traditional wave equation solvers are computationally intensive and unstable, while neural operators lack realistic datasets, limiting practical applications in medical imaging.

Method: Developed OpenBreastUS, a dataset with 8,000 realistic breast phantoms and 16M frequency-domain wave simulations, to benchmark neural operators.

Result: The dataset facilitates performance, scalability, and generalization analysis, enabling efficient in vivo breast imaging with neural operators.

Conclusion: OpenBreastUS bridges theory and practice, advancing neural PDE solvers for real-world medical imaging.

Abstract: Accurate and efficient simulation of wave equations is crucial in
computational wave imaging applications, such as ultrasound computed tomography
(USCT), which reconstructs tissue material properties from observed scattered
waves. Traditional numerical solvers for wave equations are computationally
intensive and often unstable, limiting their practical applications for
quasi-real-time image reconstruction. Neural operators offer an innovative
approach by accelerating PDE solving using neural networks; however, their
effectiveness in realistic imaging is limited because existing datasets
oversimplify real-world complexity. In this paper, we present OpenBreastUS, a
large-scale wave equation dataset designed to bridge the gap between
theoretical equations and practical imaging applications. OpenBreastUS includes
8,000 anatomically realistic human breast phantoms and over 16 million
frequency-domain wave simulations using real USCT configurations. It enables a
comprehensive benchmarking of popular neural operators for both forward
simulation and inverse imaging tasks, allowing analysis of their performance,
scalability, and generalization capabilities. By offering a realistic and
extensive dataset, OpenBreastUS not only serves as a platform for developing
innovative neural PDE solvers but also facilitates their deployment in
real-world medical imaging problems. For the first time, we demonstrate
efficient in vivo imaging of the human breast using neural operator solvers.

</details>


### [76] [EBA-AI: Ethics-Guided Bias-Aware AI for Efficient Underwater Image Enhancement and Coral Reef Monitoring](https://arxiv.org/abs/2507.15036)
*Lyes Saad Saoud,Irfan Hussain*

Main category: cs.CV

TL;DR: EBA-AI is an ethics-guided, bias-aware AI framework for underwater image enhancement, addressing dataset bias, computational costs, and transparency issues. It uses CLIP embeddings for bias mitigation and adaptive processing for efficiency, validated on multiple datasets.


<details>
  <summary>Details</summary>
Motivation: AI-based underwater image enhancement faces challenges like dataset bias, high computational costs, and lack of transparency, which can lead to misinterpretations in marine conservation efforts.

Method: EBA-AI leverages CLIP embeddings to detect and mitigate dataset bias and integrates adaptive processing to optimize energy efficiency. It includes uncertainty estimation and explainability techniques.

Result: Experiments show a controlled 1.0 dB drop in PSNR but significant computational savings, enabling real-time feasibility. Comparisons with other models validate its balance of efficiency, fairness, and interpretability.

Conclusion: EBA-AI contributes to sustainable, bias-aware, and computationally efficient marine conservation by addressing key AI limitations in underwater image enhancement.

Abstract: Underwater image enhancement is vital for marine conservation, particularly
coral reef monitoring. However, AI-based enhancement models often face dataset
bias, high computational costs, and lack of transparency, leading to potential
misinterpretations. This paper introduces EBA-AI, an ethics-guided bias-aware
AI framework to address these challenges. EBA-AI leverages CLIP embeddings to
detect and mitigate dataset bias, ensuring balanced representation across
varied underwater environments. It also integrates adaptive processing to
optimize energy efficiency, significantly reducing GPU usage while maintaining
competitive enhancement quality. Experiments on LSUI400, Oceanex, and UIEB100
show that while PSNR drops by a controlled 1.0 dB, computational savings enable
real-time feasibility for large-scale marine monitoring. Additionally,
uncertainty estimation and explainability techniques enhance trust in AI-driven
environmental decisions. Comparisons with CycleGAN, FunIEGAN, RAUNENet,
WaterNet, UGAN, PUGAN, and UTUIE validate EBA-AI's effectiveness in balancing
efficiency, fairness, and interpretability in underwater image processing. By
addressing key limitations of AI-driven enhancement, this work contributes to
sustainable, bias-aware, and computationally efficient marine conservation
efforts. For interactive visualizations, animations, source code, and access to
the preprint, visit: https://lyessaadsaoud.github.io/EBA-AI/

</details>


### [77] [OmniVTON: Training-Free Universal Virtual Try-On](https://arxiv.org/abs/2507.15037)
*Zhaotong Yang,Yuhui Li,Shengfeng He,Xinzhe Li,Yangyang Xu,Junyu Dong,Yong Du*

Main category: cs.CV

TL;DR: OmniVTON is a training-free universal VTON framework that decouples garment and pose conditioning for high fidelity and adaptability across diverse settings.


<details>
  <summary>Details</summary>
Motivation: Existing VTON methods face challenges in cross-domain generalization (supervised) or data biases (unsupervised). A unified, training-free solution is needed.

Method: OmniVTON uses garment prior generation and boundary stitching for texture fidelity, and DDIM inversion for pose alignment, disentangling garment and pose constraints.

Result: OmniVTON outperforms in diverse datasets, garment types, and scenarios, including multi-human VTON.

Conclusion: OmniVTON is the first universal, training-free VTON framework, achieving superior performance and enabling multi-human garment transfer.

Abstract: Image-based Virtual Try-On (VTON) techniques rely on either supervised
in-shop approaches, which ensure high fidelity but struggle with cross-domain
generalization, or unsupervised in-the-wild methods, which improve adaptability
but remain constrained by data biases and limited universality. A unified,
training-free solution that works across both scenarios remains an open
challenge. We propose OmniVTON, the first training-free universal VTON
framework that decouples garment and pose conditioning to achieve both texture
fidelity and pose consistency across diverse settings. To preserve garment
details, we introduce a garment prior generation mechanism that aligns clothing
with the body, followed by continuous boundary stitching technique to achieve
fine-grained texture retention. For precise pose alignment, we utilize DDIM
inversion to capture structural cues while suppressing texture interference,
ensuring accurate body alignment independent of the original image textures. By
disentangling garment and pose constraints, OmniVTON eliminates the bias
inherent in diffusion models when handling multiple conditions simultaneously.
Experimental results demonstrate that OmniVTON achieves superior performance
across diverse datasets, garment types, and application scenarios. Notably, it
is the first framework capable of multi-human VTON, enabling realistic garment
transfer across multiple individuals in a single scene. Code is available at
https://github.com/Jerome-Young/OmniVTON

</details>


### [78] [Rethinking Pan-sharpening: Principled Design, Unified Training, and a Universal Loss Surpass Brute-Force Scaling](https://arxiv.org/abs/2507.15059)
*Ran Zhang,Xuanhua He,Li Xueheng,Ke Cao,Liu Liu,Wenbo Xu,Fang Jiabin,Yang Qize,Jie Zhang*

Main category: cs.CV

TL;DR: PanTiny is a lightweight, efficient pan-sharpening framework trained on multiple satellite datasets, outperforming larger models with better generalization and performance-to-efficiency balance.


<details>
  <summary>Details</summary>
Motivation: Addressing the inefficiency and poor generalization of large, dataset-specific pan-sharpening models.

Method: Proposes PanTiny, a single-step framework with a multiple-in-one training paradigm and a composite loss function.

Result: Achieves superior performance-to-efficiency balance and outperforms specialized models.

Conclusion: Advocates for efficient, generalizable models in pan-sharpening, validated by PanTiny's success.

Abstract: The field of pan-sharpening has recently seen a trend towards increasingly
large and complex models, often trained on single, specific satellite datasets.
This approach, however, leads to high computational overhead and poor
generalization on full resolution data, a paradigm we challenge in this paper.
In response to this issue, we propose PanTiny, a lightweight, single-step
pan-sharpening framework designed for both efficiency and robust performance.
More critically, we introduce multiple-in-one training paradigm, where a
single, compact model is trained simultaneously on three distinct satellite
datasets (WV2, WV3, and GF2) with different resolution and spectral
information. Our experiments show that this unified training strategy not only
simplifies deployment but also significantly boosts generalization on
full-resolution data. Further, we introduce a universally powerful composite
loss function that elevates the performance of almost all of models for
pan-sharpening, pushing state-of-the-art metrics into a new era. Our PanTiny
model, benefiting from these innovations, achieves a superior
performance-to-efficiency balance, outperforming most larger, specialized
models. Through extensive ablation studies, we validate that principled
engineering in model design, training paradigms, and loss functions can surpass
brute-force scaling. Our work advocates for a community-wide shift towards
creating efficient, generalizable, and data-conscious models for
pan-sharpening. The code is available at
https://github.com/Zirconium233/PanTiny .

</details>


### [79] [StableAnimator++: Overcoming Pose Misalignment and Face Distortion for Human Image Animation](https://arxiv.org/abs/2507.15064)
*Shuyuan Tu,Zhen Xing,Xintong Han,Zhi-Qi Cheng,Qi Dai,Chong Luo,Zuxuan Wu,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: StableAnimator++ is a novel ID-preserving video diffusion framework for human image animation, addressing identity consistency issues with learnable pose alignment and advanced face optimization.


<details>
  <summary>Details</summary>
Motivation: Existing diffusion models struggle with identity consistency when reference images and driving videos differ in body size or position.

Method: Uses learnable pose alignment with SVD guidance, image/face embeddings, a Face Encoder, and a distribution-aware ID Adapter. Integrates HJB-based face optimization during inference.

Result: Achieves high-quality, ID-consistent video generation without post-processing, validated on benchmarks.

Conclusion: StableAnimator++ effectively mitigates identity inconsistency and enhances facial fidelity in human image animation.

Abstract: Current diffusion models for human image animation often struggle to maintain
identity (ID) consistency, especially when the reference image and driving
video differ significantly in body size or position. We introduce
StableAnimator++, the first ID-preserving video diffusion framework with
learnable pose alignment, capable of generating high-quality videos conditioned
on a reference image and a pose sequence without any post-processing. Building
upon a video diffusion model, StableAnimator++ contains carefully designed
modules for both training and inference, striving for identity consistency. In
particular, StableAnimator++ first uses learnable layers to predict the
similarity transformation matrices between the reference image and the driven
poses via injecting guidance from Singular Value Decomposition (SVD). These
matrices align the driven poses with the reference image, mitigating
misalignment to a great extent. StableAnimator++ then computes image and face
embeddings using off-the-shelf encoders, refining the face embeddings via a
global content-aware Face Encoder. To further maintain ID, we introduce a
distribution-aware ID Adapter that counteracts interference caused by temporal
layers while preserving ID via distribution alignment. During the inference
stage, we propose a novel Hamilton-Jacobi-Bellman (HJB) based face optimization
integrated into the denoising process, guiding the diffusion trajectory for
enhanced facial fidelity. Experiments on benchmarks show the effectiveness of
StableAnimator++ both qualitatively and quantitatively.

</details>


### [80] [Aesthetics is Cheap, Show me the Text: An Empirical Evaluation of State-of-the-Art Generative Models for OCR](https://arxiv.org/abs/2507.15085)
*Peirong Zhang,Haowei Xu,Jiaxin Zhang,Guitao Xu,Xuhan Zheng,Zhenhua Yang,Junle Liu,Yuyi Zhang,Lianwen Jin*

Main category: cs.CV

TL;DR: The paper evaluates state-of-the-art generative models for text image generation and editing, incorporating OCR tasks, and identifies their weaknesses, advocating for integrating these skills into general-domain models.


<details>
  <summary>Details</summary>
Motivation: To assess whether current generative models can handle the complexities of text image generation and editing, given their advancements in fidelity.

Method: Evaluates six models (closed and open-source) on 33 OCR tasks across five categories, using tailored inputs and prompts.

Result: Identifies weaknesses in current models for OCR tasks and emphasizes the need for photorealistic text generation as a foundational skill in general models.

Conclusion: Argues for integrating text image generation into general-domain models and provides insights for the community, with ongoing updates on GitHub.

Abstract: Text image is a unique and crucial information medium that integrates visual
aesthetics and linguistic semantics in modern e-society. Due to their subtlety
and complexity, the generation of text images represents a challenging and
evolving frontier in the image generation field. The recent surge of
specialized image generators (\emph{e.g.}, Flux-series) and unified generative
models (\emph{e.g.}, GPT-4o), which demonstrate exceptional fidelity, raises a
natural question: can they master the intricacies of text image generation and
editing? Motivated by this, we assess current state-of-the-art generative
models' capabilities in terms of text image generation and editing. We
incorporate various typical optical character recognition (OCR) tasks into our
evaluation and broaden the concept of text-based generation tasks into OCR
generative tasks. We select 33 representative tasks and categorize them into
five categories: document, handwritten text, scene text, artistic text, and
complex \& layout-rich text. For comprehensive evaluation, we examine six
models across both closed-source and open-source domains, using tailored,
high-quality image inputs and prompts. Through this evaluation, we draw crucial
observations and identify the weaknesses of current generative models for OCR
tasks. We argue that photorealistic text image generation and editing should be
internalized as foundational skills into general-domain generative models,
rather than being delegated to specialized solutions, and we hope this
empirical analysis can provide valuable insights for the community to achieve
this goal. This evaluation is online and will be continuously updated at our
GitHub repository.

</details>


### [81] [Visual Place Recognition for Large-Scale UAV Applications](https://arxiv.org/abs/2507.15089)
*Ioannis Tsampikos Papapetros,Ioannis Kansizoglou,Antonios Gasteratos*

Main category: cs.CV

TL;DR: LASED, a large-scale aerial dataset, and steerable CNNs improve aerial Visual Place Recognition (vPR) by addressing dataset limitations and rotational ambiguity, achieving higher recall and robustness.


<details>
  <summary>Details</summary>
Motivation: Aerial vPR faces challenges due to limited large-scale datasets and rotational ambiguity in UAV imagery, hindering model generalization.

Method: Introduce LASED, a large-scale dataset with geographic and temporal diversity, and integrate steerable CNNs to handle rotational variance.

Result: Models trained on LASED show higher recall, and steerable CNNs outperform conventional CNNs by 12% in recall, addressing rotational ambiguity.

Conclusion: Combining structured datasets with rotation-equivariant networks enhances robustness and generalization in aerial vPR.

Abstract: Visual Place Recognition (vPR) plays a crucial role in Unmanned Aerial
Vehicle (UAV) navigation, enabling robust localization across diverse
environments. Despite significant advancements, aerial vPR faces unique
challenges due to the limited availability of large-scale, high-altitude
datasets, which limits model generalization, along with the inherent rotational
ambiguity in UAV imagery. To address these challenges, we introduce LASED, a
large-scale aerial dataset with approximately one million images,
systematically sampled from 170,000 unique locations throughout Estonia over a
decade, offering extensive geographic and temporal diversity. Its structured
design ensures clear place separation significantly enhancing model training
for aerial scenarios. Furthermore, we propose the integration of steerable
Convolutional Neural Networks (CNNs) to explicitly handle rotational variance,
leveraging their inherent rotational equivariance to produce robust,
orientation-invariant feature representations. Our extensive benchmarking
demonstrates that models trained on LASED achieve significantly higher recall
compared to those trained on smaller, less diverse datasets, highlighting the
benefits of extensive geographic coverage and temporal diversity. Moreover,
steerable CNNs effectively address rotational ambiguity inherent in aerial
imagery, consistently outperforming conventional convolutional architectures,
achieving on average 12\% recall improvement over the best-performing
non-steerable network. By combining structured, large-scale datasets with
rotation-equivariant neural networks, our approach significantly enhances model
robustness and generalization for aerial vPR.

</details>


### [82] [BleedOrigin: Dynamic Bleeding Source Localization in Endoscopic Submucosal Dissection via Dual-Stage Detection and Tracking](https://arxiv.org/abs/2507.15094)
*Mengya Xu,Rulin Zhou,An Wang,Chaoyang Lyu,Zhen Li,Ning Zhong,Hongliang Ren*

Main category: cs.CV

TL;DR: The paper introduces BleedOrigin-Bench, a dataset for bleeding source detection in ESD, and BleedOrigin-Net, a dual-stage framework for localization and tracking, achieving high accuracy.


<details>
  <summary>Details</summary>
Motivation: Current AI methods for ESD bleeding focus on segmentation, not source detection or tracking, and lack specialized datasets, leading to inefficiencies and risks.

Method: The authors propose BleedOrigin-Net, a dual-stage detection-tracking framework, and validate it using the BleedOrigin-Bench dataset, comparing it with existing models.

Result: The framework achieves 96.85% frame-level accuracy for onset detection, 70.24% for initial source detection, and 96.11% for point tracking.

Conclusion: The work addresses critical gaps in ESD bleeding management, offering a robust solution for real-time localization and tracking.

Abstract: Intraoperative bleeding during Endoscopic Submucosal Dissection (ESD) poses
significant risks, demanding precise, real-time localization and continuous
monitoring of the bleeding source for effective hemostatic intervention. In
particular, endoscopists have to repeatedly flush to clear blood, allowing only
milliseconds to identify bleeding sources, an inefficient process that prolongs
operations and elevates patient risks. However, current Artificial Intelligence
(AI) methods primarily focus on bleeding region segmentation, overlooking the
critical need for accurate bleeding source detection and temporal tracking in
the challenging ESD environment, which is marked by frequent visual
obstructions and dynamic scene changes. This gap is widened by the lack of
specialized datasets, hindering the development of robust AI-assisted guidance
systems. To address these challenges, we introduce BleedOrigin-Bench, the first
comprehensive ESD bleeding source dataset, featuring 1,771 expert-annotated
bleeding sources across 106,222 frames from 44 procedures, supplemented with
39,755 pseudo-labeled frames. This benchmark covers 8 anatomical sites and 6
challenging clinical scenarios. We also present BleedOrigin-Net, a novel
dual-stage detection-tracking framework for the bleeding source localization in
ESD procedures, addressing the complete workflow from bleeding onset detection
to continuous spatial tracking. We compare with widely-used object detection
models (YOLOv11/v12), multimodal large language models, and point tracking
methods. Extensive evaluation demonstrates state-of-the-art performance,
achieving 96.85% frame-level accuracy ($\pm\leq8$ frames) for bleeding onset
detection, 70.24% pixel-level accuracy ($\leq100$ px) for initial source
detection, and 96.11% pixel-level accuracy ($\leq100$ px) for point tracking.

</details>


### [83] [LoopNet: A Multitasking Few-Shot Learning Approach for Loop Closure in Large Scale SLAM](https://arxiv.org/abs/2507.15109)
*Mohammad-Maher Nakshbandi,Ziad Sharawy,Sorin Grigorescu*

Main category: cs.CV

TL;DR: LoopNet improves SLAM loop closure with multitasking ResNet, online retraining, and DISK descriptors, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Addressing loop closure detection accuracy and real-time computation constraints in SLAM systems.

Method: Uses multitasking ResNet adapted for online retraining with few-shot learning, leveraging DISK descriptors.

Result: Surpasses handcrafted features and traditional deep learning methods, offering better performance.

Conclusion: LoopNet and the new LoopDB dataset advance SLAM loop closure solutions.

Abstract: One of the main challenges in the Simultaneous Localization and Mapping
(SLAM) loop closure problem is the recognition of previously visited places. In
this work, we tackle the two main problems of real-time SLAM systems: 1) loop
closure detection accuracy and 2) real-time computation constraints on the
embedded hardware. Our LoopNet method is based on a multitasking variant of the
classical ResNet architecture, adapted for online retraining on a dynamic
visual dataset and optimized for embedded devices. The online retraining is
designed using a few-shot learning approach. The architecture provides both an
index into the queried visual dataset, and a measurement of the prediction
quality. Moreover, by leveraging DISK (DIStinctive Keypoints) descriptors,
LoopNet surpasses the limitations of handcrafted features and traditional deep
learning methods, offering better performance under varying conditions. Code is
available at https://github.com/RovisLab/LoopNet. Additinally, we introduce a
new loop closure benchmarking dataset, coined LoopDB, which is available at
https://github.com/RovisLab/LoopDB.

</details>


### [84] [Enhancing Visual Planning with Auxiliary Tasks and Multi-token Prediction](https://arxiv.org/abs/2507.15130)
*Ce Zhang,Yale Song,Ruta Desai,Michael Louis Iuzzolino,Joseph Tighe,Gedas Bertasius,Satwik Kottur*

Main category: cs.CV

TL;DR: VideoPlan introduces Auxiliary Task Augmentation and Multi-token Prediction to improve long-horizon visual planning in MLLMs, achieving state-of-the-art results on COIN and CrossTask datasets.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in training MLLMs for video-based planning: scarcity of procedural annotations and inefficiency of next-token prediction for structured action spaces.

Method: Uses Auxiliary Task Augmentation (e.g., goal prediction) and Multi-token Prediction (predicting multiple future tokens) to enhance planning ability.

Result: Achieves 7.3% and 3.4% improvements on COIN and CrossTask datasets, respectively, and performs competitively on Ego4D without specialized features.

Conclusion: VideoPlan effectively tackles data scarcity and structured action modeling, advancing VPA performance.

Abstract: Visual Planning for Assistance (VPA) aims to predict a sequence of user
actions required to achieve a specified goal based on a video showing the
user's progress. Although recent advances in multimodal large language models
(MLLMs) have shown promising results in video understanding, long-horizon
visual planning remains a challenging problem. We identify two challenges in
training large MLLMs for video-based planning tasks: (1) scarcity of procedural
annotations, limiting the model's ability to learn procedural task dynamics
effectively, and (2) inefficiency of next-token prediction objective to
explicitly capture the structured action space for visual planning when
compared to free-form, natural language. To tackle data scarcity, we introduce
Auxiliary Task Augmentation. We design and train our model on auxiliary tasks
relevant to long-horizon video-based planning (e.g., goal prediction) to
augment the model's planning ability. To more explicitly model the structured
action space unique to visual planning tasks, we leverage Multi-token
Prediction, extending traditional next-token prediction by using multiple heads
to predict multiple future tokens during training. Our approach, VideoPlan,
achieves state-of-the-art VPA performance on the COIN and CrossTask datasets,
surpassing prior methods by 7.3% and 3.4%, respectively, when predicting 3
future actions. We further extend our method to the challenging Ego4D Long-term
Action Anticipation task, and show that it is on par with the state-of-the-art
approaches despite not using specialized egocentric features. Code will be made
available.

</details>


### [85] [Event-based Graph Representation with Spatial and Motion Vectors for Asynchronous Object Detection](https://arxiv.org/abs/2507.15150)
*Aayush Atul Verma,Arpitsinh Vaghela,Bharatesh Chakravarthi,Kaustav Chanda,Yezhou Yang*

Main category: cs.CV

TL;DR: A novel spatiotemporal multigraph representation improves event-based object detection by decoupling spatial and temporal dynamics, achieving better accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Event-based sensors' sparse data loses advantages when converted to dense tensors, and existing graph methods underperform due to poor spatiotemporal modeling.

Method: Proposes a spatiotemporal multigraph with decoupled spatial (B-spline basis) and temporal (motion vector-based attention) graphs, replacing 3D kernels with efficient 2D ones.

Result: 6% higher detection accuracy on Gen1 and eTraM datasets, 5x speedup, fewer parameters, and no added computational cost.

Conclusion: Structured graph modeling effectively leverages event-based data's sparsity and asynchronicity for improved vision tasks.

Abstract: Event-based sensors offer high temporal resolution and low latency by
generating sparse, asynchronous data. However, converting this irregular data
into dense tensors for use in standard neural networks diminishes these
inherent advantages, motivating research into graph representations. While such
methods preserve sparsity and support asynchronous inference, their performance
on downstream tasks remains limited due to suboptimal modeling of
spatiotemporal dynamics. In this work, we propose a novel spatiotemporal
multigraph representation to better capture spatial structure and temporal
changes. Our approach constructs two decoupled graphs: a spatial graph
leveraging B-spline basis functions to model global structure, and a temporal
graph utilizing motion vector-based attention for local dynamic changes. This
design enables the use of efficient 2D kernels in place of computationally
expensive 3D kernels. We evaluate our method on the Gen1 automotive and eTraM
datasets for event-based object detection, achieving over a 6% improvement in
detection accuracy compared to previous graph-based works, with a 5x speedup,
reduced parameter count, and no increase in computational cost. These results
highlight the effectiveness of structured graph modeling for asynchronous
vision. Project page: eventbasedvision.github.io/eGSMV.

</details>


### [86] [MeshMamba: State Space Models for Articulated 3D Mesh Generation and Reconstruction](https://arxiv.org/abs/2507.15212)
*Yusuke Yoshiyasu,Leyuan Sun,Ryusuke Sagawa*

Main category: cs.CV

TL;DR: MeshMamba is a neural network model using Mamba-SSMs for efficient 3D articulated mesh learning, enabling large-scale mesh generation and reconstruction. It introduces MambaDiff3D for mesh generation and Mamba-HMR for single-image human mesh recovery, outperforming prior methods.


<details>
  <summary>Details</summary>
Motivation: The need for scalable and efficient models to handle large 3D mesh datasets, particularly for articulated shapes like human bodies with clothing and hands, drives the development of MeshMamba.

Method: MeshMamba serializes mesh vertices into orderings processable by Mamba-SSMs, using body part annotations or 3D vertex locations. It includes MambaDiff3D for denoising diffusion-based mesh generation and Mamba-HMR for single-image human mesh recovery.

Result: MambaDiff3D generates dense 3D human meshes with clothing and hands, outperforming previous methods. Mamba-HMR extends non-parametric mesh recovery to whole-body settings, achieving competitive real-time performance.

Conclusion: MeshMamba demonstrates scalability and efficiency in 3D articulated mesh tasks, advancing the field with its novel serialization technique and outperforming existing approaches in generation and reconstruction.

Abstract: In this paper, we introduce MeshMamba, a neural network model for learning 3D
articulated mesh models by employing the recently proposed Mamba State Space
Models (Mamba-SSMs). MeshMamba is efficient and scalable in handling a large
number of input tokens, enabling the generation and reconstruction of body mesh
models with more than 10,000 vertices, capturing clothing and hand geometries.
The key to effectively learning MeshMamba is the serialization technique of
mesh vertices into orderings that are easily processed by Mamba. This is
achieved by sorting the vertices based on body part annotations or the 3D
vertex locations of a template mesh, such that the ordering respects the
structure of articulated shapes. Based on MeshMamba, we design 1) MambaDiff3D,
a denoising diffusion model for generating 3D articulated meshes and 2)
Mamba-HMR, a 3D human mesh recovery model that reconstructs a human body shape
and pose from a single image. Experimental results showed that MambaDiff3D can
generate dense 3D human meshes in clothes, with grasping hands, etc., and
outperforms previous approaches in the 3D human shape generation task.
Additionally, Mamba-HMR extends the capabilities of previous non-parametric
human mesh recovery approaches, which were limited to handling body-only poses
using around 500 vertex tokens, to the whole-body setting with face and hands,
while achieving competitive performance in (near) real-time.

</details>


### [87] [Improving Joint Embedding Predictive Architecture with Diffusion Noise](https://arxiv.org/abs/2507.15216)
*Yuping Qiu,Rui Zhu,Ying-cong Chen*

Main category: cs.CV

TL;DR: The paper proposes N-JEPA, a method combining self-supervised learning (SSL) with diffusion models to enhance feature learning, leveraging diffusion noise as a form of masked image modeling (MIM).


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between SSL (strong in discriminative tasks) and generative models (superior in image generation), aiming to improve SSL's representation capacity.

Method: Introduces N-JEPA, integrating diffusion noise into MIM via masked tokens' position embedding, using a multi-level noise schedule for robustness.

Result: Demonstrates effectiveness in downstream classification tasks.

Conclusion: Combining SSL with diffusion models via N-JEPA enhances recognition models, with potential for broader applications.

Abstract: Self-supervised learning has become an incredibly successful method for
feature learning, widely applied to many downstream tasks. It has proven
especially effective for discriminative tasks, surpassing the trending
generative models. However, generative models perform better in image
generation and detail enhancement. Thus, it is natural for us to find a
connection between SSL and generative models to further enhance the
representation capacity of SSL. As generative models can create new samples by
approximating the data distribution, such modeling should also lead to a
semantic understanding of the raw visual data, which is necessary for
recognition tasks. This enlightens us to combine the core principle of the
diffusion model: diffusion noise, with SSL to learn a competitive recognition
model. Specifically, diffusion noise can be viewed as a particular state of
mask that reveals a close relationship between masked image modeling (MIM) and
diffusion models. In this paper, we propose N-JEPA (Noise-based JEPA) to
incorporate diffusion noise into MIM by the position embedding of masked
tokens. The multi-level noise schedule is a series of feature augmentations to
further enhance the robustness of our model. We perform a comprehensive study
to confirm its effectiveness in the classification of downstream tasks. Codes
will be released soon in public.

</details>


### [88] [Hierarchical Part-based Generative Model for Realistic 3D Blood Vessel](https://arxiv.org/abs/2507.15223)
*Siqi Chen,Guoqing Zhang,Jiahao Lai,Bingzhi Shen,Sihong Zhang,Caixia Dong,Xuejin Chen,Yang Li*

Main category: cs.CV

TL;DR: A hierarchical part-based framework for 3D blood vessel generation separates global topology from local geometry, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Accurate modeling of complex blood vessel geometry and topology is challenging due to intricate branching patterns and irregular shapes.

Method: The approach involves three stages: key graph generation, vessel segment generation, and hierarchical vessel assembly.

Result: Validated on real-world datasets, the framework outperforms existing methods in modeling vascular networks.

Conclusion: This is the first successful part-based generative approach for 3D vessel modeling, setting a new benchmark.

Abstract: Advancements in 3D vision have increased the impact of blood vessel modeling
on medical applications. However, accurately representing the complex geometry
and topology of blood vessels remains a challenge due to their intricate
branching patterns, curvatures, and irregular shapes. In this study, we propose
a hierarchical part-based frame work for 3D vessel generation that separates
the global binary tree-like topology from local geometric details. Our approach
proceeds in three stages: (1) key graph generation to model the overall
hierarchical struc ture, (2) vessel segment generation conditioned on geometric
properties, and (3) hierarchical vessel assembly by integrating the local
segments according to the global key graph. We validate our framework on real
world datasets, demonstrating superior performance over existing methods in
modeling complex vascular networks. This work marks the first successful
application of a part-based generative approach for 3D vessel modeling, setting
a new benchmark for vascular data generation. The code is available at:
https://github.com/CybercatChen/PartVessel.git.

</details>


### [89] [Mammo-SAE: Interpreting Breast Cancer Concept Learning with Sparse Autoencoders](https://arxiv.org/abs/2507.15227)
*Krishna Kanth Nakka*

Main category: cs.CV

TL;DR: The paper introduces Sparse Autoencoder (SAE)-based interpretability to breast imaging using Mammo-CLIP, a vision-language model, to analyze clinically relevant features and uncover model decision-making factors.


<details>
  <summary>Details</summary>
Motivation: Interpretability is crucial in medical imaging for clinical adoption, especially in understanding model decisions.

Method: Train a patch-level Mammo-SAE on Mammo-CLIP to probe latent features linked to breast concepts like mass and suspicious calcification.

Result: Top activated latent neurons align with ground truth regions, and confounding factors in decision-making are identified. Latent neurons used in downstream finetuning for breast concept prediction are analyzed.

Conclusion: SAE latent representations offer deeper insights into foundation models for breast imaging, enhancing interpretability.

Abstract: Interpretability is critical in high-stakes domains such as medical imaging,
where understanding model decisions is essential for clinical adoption. In this
work, we introduce Sparse Autoencoder (SAE)-based interpretability to breast
imaging by analyzing {Mammo-CLIP}, a vision--language foundation model
pretrained on large-scale mammogram image--report pairs. We train a patch-level
\texttt{Mammo-SAE} on Mammo-CLIP to identify and probe latent features
associated with clinically relevant breast concepts such as \textit{mass} and
\textit{suspicious calcification}. Our findings reveal that top activated class
level latent neurons in the SAE latent space often tend to align with ground
truth regions, and also uncover several confounding factors influencing the
model's decision-making process. Additionally, we analyze which latent neurons
the model relies on during downstream finetuning for improving the breast
concept prediction. This study highlights the promise of interpretable SAE
latent representations in providing deeper insight into the internal workings
of foundation models at every layer for breast imaging.

</details>


### [90] [Cross-Domain Few-Shot Learning with Coalescent Projections and Latent Space Reservation](https://arxiv.org/abs/2507.15243)
*Naeem Paeedeh,Mahardhika Pratama,Wolfgang Mayer,Jimmy Cao,Ryszard Kowlczyk*

Main category: cs.CV

TL;DR: A new method, Coalescent Projection (CP), combined with pseudo-class generation and Self-Supervised Transformations (SSTs), outperforms SOTA in CD-FSL by addressing overfitting in transformers.


<details>
  <summary>Details</summary>
Motivation: Overcoming overfitting in transformers due to limited labeled samples in CD-FSL.

Method: Introduces Coalescent Projection (CP) and pseudo-class generation with SSTs, leveraging base domain data.

Result: Outperforms SOTA methods on the BSCD-FSL benchmark under extreme domain shifts.

Conclusion: The proposed CP and SSTs effectively enhance CD-FSL performance by mitigating overfitting and leveraging base domain data.

Abstract: Despite the progress in Cross-Domain Few-Shot Learning (CD-FSL), a model
pre-trained with DINO combined with a prototypical classifier outperforms the
latest SOTA methods. A crucial limitation that needs to be overcome is that
updating too many parameters of the transformers leads to overfitting due to
the scarcity of labeled samples. To address this challenge, we propose a new
concept, Coalescent Projection (CP), as an effective successor to soft prompts.
Additionally, we propose a novel pseudo-class generation method combined with
Self-Supervised Transformations (SSTs) that relies solely on the base domain to
prepare the network for encountering unseen samples from different domains. The
proposed method exhibits its effectiveness in comprehensive experiments on the
extreme domain shift scenario of the BSCD-FSL benchmark. Our code is published
at https://github.com/Naeem-Paeedeh/CPLSR.

</details>


### [91] [FreeCus: Free Lunch Subject-driven Customization in Diffusion Transformers](https://arxiv.org/abs/2507.15249)
*Yanbing Zhang,Zhe Wang,Qin Zhou,Mengping Yang*

Main category: cs.CV

TL;DR: FreeCus is a training-free framework for subject-driven text-to-image generation, leveraging diffusion transformers (DiT) with innovations like attention sharing, dynamic shifting analysis, and MLLM integration for zero-shot synthesis.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on training procedures, limiting practical use and failing to exploit DiT's zero-shot potential for authentic subject-driven synthesis.

Method: FreeCus introduces three innovations: 1) attention sharing for layout integrity, 2) dynamic shifting analysis for better feature extraction, and 3) MLLM integration for richer semantics.

Result: Achieves state-of-the-art or comparable results without training, with seamless compatibility for inpainting and control modules.

Conclusion: FreeCus successfully unlocks DiT's zero-shot ability for consistent subject synthesis, offering practical and flexible applications.

Abstract: In light of recent breakthroughs in text-to-image (T2I) generation,
particularly with diffusion transformers (DiT), subject-driven technologies are
increasingly being employed for high-fidelity customized production that
preserves subject identity from reference inputs, enabling thrilling design
workflows and engaging entertainment. Existing alternatives typically require
either per-subject optimization via trainable text embeddings or training
specialized encoders for subject feature extraction on large-scale datasets.
Such dependencies on training procedures fundamentally constrain their
practical applications. More importantly, current methodologies fail to fully
leverage the inherent zero-shot potential of modern diffusion transformers
(e.g., the Flux series) for authentic subject-driven synthesis. To bridge this
gap, we propose FreeCus, a genuinely training-free framework that activates
DiT's capabilities through three key innovations: 1) We introduce a pivotal
attention sharing mechanism that captures the subject's layout integrity while
preserving crucial editing flexibility. 2) Through a straightforward analysis
of DiT's dynamic shifting, we propose an upgraded variant that significantly
improves fine-grained feature extraction. 3) We further integrate advanced
Multimodal Large Language Models (MLLMs) to enrich cross-modal semantic
representations. Extensive experiments reflect that our method successfully
unlocks DiT's zero-shot ability for consistent subject synthesis across diverse
contexts, achieving state-of-the-art or comparable results compared to
approaches that require additional training. Notably, our framework
demonstrates seamless compatibility with existing inpainting pipelines and
control modules, facilitating more compelling experiences. Our code is
available at: https://github.com/Monalissaa/FreeCus.

</details>


### [92] [MinCD-PnP: Learning 2D-3D Correspondences with Approximate Blind PnP](https://arxiv.org/abs/2507.15257)
*Pei An,Jiaqi Yang,Muyao Peng,You Yang,Qiong Liu,Xiaolin Wu,Liangliang Nan*

Main category: cs.CV

TL;DR: The paper proposes MinCD-PnP, a robust method for image-to-point-cloud registration, addressing noise and outlier sensitivity in differential PnP by simplifying blind PnP into a Chamfer distance minimization task.


<details>
  <summary>Details</summary>
Motivation: Differential PnP is sensitive to noise and outliers, limiting correspondence learning effectiveness. Blind PnP's robustness inspired a simplified approach.

Method: MinCD-PnP minimizes Chamfer distance between 2D and 3D keypoints, integrated via lightweight MinCD-Net module into existing architectures.

Result: MinCD-Net outperforms state-of-the-art methods, achieving higher inlier ratio and registration recall in cross-scene and cross-dataset tests.

Conclusion: The proposed MinCD-PnP and MinCD-Net offer a robust, efficient solution for I2P registration, validated by superior performance across datasets.

Abstract: Image-to-point-cloud (I2P) registration is a fundamental problem in computer
vision, focusing on establishing 2D-3D correspondences between an image and a
point cloud. The differential perspective-n-point (PnP) has been widely used to
supervise I2P registration networks by enforcing the projective constraints on
2D-3D correspondences. However, differential PnP is highly sensitive to noise
and outliers in the predicted correspondences. This issue hinders the
effectiveness of correspondence learning. Inspired by the robustness of blind
PnP against noise and outliers in correspondences, we propose an approximated
blind PnP based correspondence learning approach. To mitigate the high
computational cost of blind PnP, we simplify blind PnP to an amenable task of
minimizing Chamfer distance between learned 2D and 3D keypoints, called
MinCD-PnP. To effectively solve MinCD-PnP, we design a lightweight multi-task
learning module, named as MinCD-Net, which can be easily integrated into the
existing I2P registration architectures. Extensive experiments on 7-Scenes,
RGBD-V2, ScanNet, and self-collected datasets demonstrate that MinCD-Net
outperforms state-of-the-art methods and achieves a higher inlier ratio (IR)
and registration recall (RR) in both cross-scene and cross-dataset settings.

</details>


### [93] [Conditional Video Generation for High-Efficiency Video Compression](https://arxiv.org/abs/2507.15269)
*Fangqiu Yi,Jingyu Xu,Jiawei Shao,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: A video compression framework using conditional diffusion models for perceptually optimized reconstruction, outperforming traditional and neural codecs on perceptual quality metrics.


<details>
  <summary>Details</summary>
Motivation: To leverage the perceptual strengths of conditional diffusion models for video compression, aligning reconstructed content with human visual perception.

Method: Reframes video compression as a conditional generation task with three key modules: multi-granular conditioning, compact representations, and multi-condition training.

Result: Significantly outperforms traditional and neural codecs on metrics like FVD and LPIPS, especially at high compression ratios.

Conclusion: The proposed framework effectively combines conditional diffusion models with novel modules for superior perceptual video compression.

Abstract: Perceptual studies demonstrate that conditional diffusion models excel at
reconstructing video content aligned with human visual perception. Building on
this insight, we propose a video compression framework that leverages
conditional diffusion models for perceptually optimized reconstruction.
Specifically, we reframe video compression as a conditional generation task,
where a generative model synthesizes video from sparse, yet informative
signals. Our approach introduces three key modules: (1) Multi-granular
conditioning that captures both static scene structure and dynamic
spatio-temporal cues; (2) Compact representations designed for efficient
transmission without sacrificing semantic richness; (3) Multi-condition
training with modality dropout and role-aware embeddings, which prevent
over-reliance on any single modality and enhance robustness. Extensive
experiments show that our method significantly outperforms both traditional and
neural codecs on perceptual quality metrics such as Fr\'echet Video Distance
(FVD) and LPIPS, especially under high compression ratios.

</details>


### [94] [In-context Learning of Vision Language Models for Detection of Physical and Digital Attacks against Face Recognition Systems](https://arxiv.org/abs/2507.15285)
*Lazaro Janier Gonzalez-Soler,Maciej Salwowski,Christoph Busch*

Main category: cs.CV

TL;DR: The paper proposes a Vision Language Model (VLM) framework for detecting biometric attacks, outperforming traditional CNNs without extensive training.


<details>
  <summary>Details</summary>
Motivation: Current deep learning models for biometric attack detection struggle with adaptability and require large training datasets, raising privacy and logistical concerns.

Method: The study introduces an in-context learning framework using VLMs to detect physical and digital attacks, evaluated on open-source models and databases.

Result: The proposed VLM framework achieves competitive performance in attack detection, surpassing some CNNs without resource-heavy training.

Conclusion: The VLM framework is a promising solution for improving generalization in biometric attack detection, addressing limitations of traditional methods.

Abstract: Recent advances in biometric systems have significantly improved the
detection and prevention of fraudulent activities. However, as detection
methods improve, attack techniques become increasingly sophisticated. Attacks
on face recognition systems can be broadly divided into physical and digital
approaches. Traditionally, deep learning models have been the primary defence
against such attacks. While these models perform exceptionally well in
scenarios for which they have been trained, they often struggle to adapt to
different types of attacks or varying environmental conditions. These
subsystems require substantial amounts of training data to achieve reliable
performance, yet biometric data collection faces significant challenges,
including privacy concerns and the logistical difficulties of capturing diverse
attack scenarios under controlled conditions. This work investigates the
application of Vision Language Models (VLM) and proposes an in-context learning
framework for detecting physical presentation attacks and digital morphing
attacks in biometric systems. Focusing on open-source models, the first
systematic framework for the quantitative evaluation of VLMs in
security-critical scenarios through in-context learning techniques is
established. The experimental evaluation conducted on freely available
databases demonstrates that the proposed subsystem achieves competitive
performance for physical and digital attack detection, outperforming some of
the traditional CNNs without resource-intensive training. The experimental
results validate the proposed framework as a promising tool for improving
generalisation in attack detection.

</details>


### [95] [Minutiae-Anchored Local Dense Representation for Fingerprint Matching](https://arxiv.org/abs/2507.15297)
*Zhiyu Pan,Xiongjun Guan,Yongjie Duan,Jianjiang Feng,Jie Zhou*

Main category: cs.CV

TL;DR: The paper proposes DMD, a minutiae-anchored local dense representation for robust fingerprint matching under diverse capture conditions. It combines ridge textures and minutiae features, achieving state-of-the-art accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Fingerprint matching under varied capture conditions is challenging. Existing methods lack robustness and accuracy in such scenarios.

Method: DMD extracts descriptors from local patches centered on minutiae, forming a 3D tensor capturing spatial and semantic features. It uses foreground segmentation masks for efficient matching.

Result: DMD achieves state-of-the-art accuracy on rolled, plain, partial, contactless, and latent fingerprint datasets while maintaining computational efficiency.

Conclusion: DMD is a highly effective and generalizable method for large-scale fingerprint recognition, with strong potential for real-world applications.

Abstract: Fingerprint matching under diverse capture conditions remains a fundamental
challenge in biometric recognition. To achieve robust and accurate performance
in such scenarios, we propose DMD, a minutiae-anchored local dense
representation which captures both fine-grained ridge textures and
discriminative minutiae features in a spatially structured manner.
Specifically, descriptors are extracted from local patches centered and
oriented on each detected minutia, forming a three-dimensional tensor, where
two dimensions represent spatial locations on the fingerprint plane and the
third encodes semantic features. This representation explicitly captures
abstract features of local image patches, enabling a multi-level, fine-grained
description that aggregates information from multiple minutiae and their
surrounding ridge structures. Furthermore, thanks to its strong spatial
correspondence with the patch image, DMD allows for the use of foreground
segmentation masks to identify valid descriptor regions. During matching,
comparisons are then restricted to overlapping foreground areas, improving
efficiency and robustness. Extensive experiments on rolled, plain, parital,
contactless, and latent fingerprint datasets demonstrate the effectiveness and
generalizability of the proposed method. It achieves state-of-the-art accuracy
across multiple benchmarks while maintaining high computational efficiency,
showing strong potential for large-scale fingerprint recognition. Corresponding
code is available at https://github.com/Yu-Yy/DMD.

</details>


### [96] [Few-Shot Object Detection via Spatial-Channel State Space Model](https://arxiv.org/abs/2507.15308)
*Zhimeng Xin,Tianxu Wu,Yixiong Zou,Shiming Chen,Dingjie Fu,Xinge You*

Main category: cs.CV

TL;DR: The paper introduces a Spatial-Channel State Space Modeling (SCSM) module to address feature extraction challenges in few-shot object detection by leveraging inter-channel correlation and Mamba-based modeling.


<details>
  <summary>Details</summary>
Motivation: Current few-shot object detection methods struggle with accurately extracting effective features from channels due to limited training samples, leading to misweighted channels.

Method: Proposes the SCSM module with Spatial Feature Modeling (SFM) for spatial-channel balance and Channel State Modeling (CSM) using Mamba for inter-channel correlation.

Result: Experiments on VOC and COCO datasets show improved feature representation and state-of-the-art performance.

Conclusion: The SCSM module effectively enhances feature extraction in few-shot object detection, achieving superior results.

Abstract: Due to the limited training samples in few-shot object detection (FSOD), we
observe that current methods may struggle to accurately extract effective
features from each channel. Specifically, this issue manifests in two aspects:
i) channels with high weights may not necessarily be effective, and ii)
channels with low weights may still hold significant value. To handle this
problem, we consider utilizing the inter-channel correlation to facilitate the
novel model's adaptation process to novel conditions, ensuring the model can
correctly highlight effective channels and rectify those incorrect ones. Since
the channel sequence is also 1-dimensional, its similarity with the temporal
sequence inspires us to take Mamba for modeling the correlation in the channel
sequence. Based on this concept, we propose a Spatial-Channel State Space
Modeling (SCSM) module for spatial-channel state modeling, which highlights the
effective patterns and rectifies those ineffective ones in feature channels. In
SCSM, we design the Spatial Feature Modeling (SFM) module to balance the
learning of spatial relationships and channel relationships, and then introduce
the Channel State Modeling (CSM) module based on Mamba to learn correlation in
channels. Extensive experiments on the VOC and COCO datasets show that the SCSM
module enables the novel detector to improve the quality of focused feature
representation in channels and achieve state-of-the-art performance.

</details>


### [97] [BenchDepth: Are We on the Right Way to Evaluate Depth Foundation Models?](https://arxiv.org/abs/2507.15321)
*Zhenyu Li,Haotong Lin,Jiashi Feng,Peter Wonka,Bingyi Kang*

Main category: cs.CV

TL;DR: BenchDepth introduces a new benchmark for evaluating depth foundation models (DFMs) using five downstream tasks, avoiding biases of traditional alignment-based metrics.


<details>
  <summary>Details</summary>
Motivation: Current evaluation protocols for DFMs are inconsistent and biased, favoring certain depth representations and complicating fair comparisons.

Method: Proposes BenchDepth, evaluating DFMs through five proxy tasks (e.g., depth completion, SLAM) to assess practical utility without alignment procedures.

Result: Eight state-of-the-art DFMs are benchmarked, with key findings and observations provided.

Conclusion: BenchDepth aims to improve depth model evaluation practices and inspire future research in depth estimation.

Abstract: Depth estimation is a fundamental task in computer vision with diverse
applications. Recent advancements in deep learning have led to powerful depth
foundation models (DFMs), yet their evaluation remains challenging due to
inconsistencies in existing protocols. Traditional benchmarks rely on
alignment-based metrics that introduce biases, favor certain depth
representations, and complicate fair comparisons. In this work, we propose
BenchDepth, a new benchmark that evaluates DFMs through five carefully selected
downstream proxy tasks: depth completion, stereo matching, monocular
feed-forward 3D scene reconstruction, SLAM, and vision-language spatial
understanding. Unlike conventional evaluation protocols, our approach assesses
DFMs based on their practical utility in real-world applications, bypassing
problematic alignment procedures. We benchmark eight state-of-the-art DFMs and
provide an in-depth analysis of key findings and observations. We hope our work
sparks further discussion in the community on best practices for depth model
evaluation and paves the way for future research and advancements in depth
estimation.

</details>


### [98] [ExDD: Explicit Dual Distribution Learning for Surface Defect Detection via Diffusion Synthesis](https://arxiv.org/abs/2507.15335)
*Muhammad Aqeel,Federico Leonardi,Francesco Setti*

Main category: cs.CV

TL;DR: ExDD introduces a dual-distribution framework for industrial defect detection, overcoming one-class anomaly detection limitations with synthetic data generation and neighborhood-aware scoring.


<details>
  <summary>Details</summary>
Motivation: One-class anomaly detection struggles with uniform outlier assumptions and data scarcity in real-world manufacturing.

Method: ExDD uses parallel memory banks for dual feature distributions, latent diffusion models for synthetic defect generation, and neighborhood-aware ratio scoring.

Result: Achieves 94.2% I-AUROC and 97.7% P-AUROC on KSDD2, optimal at 100 synthetic samples.

Conclusion: ExDD effectively addresses data scarcity and outlier assumptions, improving industrial defect detection.

Abstract: Industrial defect detection systems face critical limitations when confined
to one-class anomaly detection paradigms, which assume uniform outlier
distributions and struggle with data scarcity in realworld manufacturing
environments. We present ExDD (Explicit Dual Distribution), a novel framework
that transcends these limitations by explicitly modeling dual feature
distributions. Our approach leverages parallel memory banks that capture the
distinct statistical properties of both normality and anomalous patterns,
addressing the fundamental flaw of uniform outlier assumptions. To overcome
data scarcity, we employ latent diffusion models with domain-specific textual
conditioning, generating in-distribution synthetic defects that preserve
industrial context. Our neighborhood-aware ratio scoring mechanism elegantly
fuses complementary distance metrics, amplifying signals in regions exhibiting
both deviation from normality and similarity to known defect patterns.
Experimental validation on KSDD2 demonstrates superior performance (94.2%
I-AUROC, 97.7% P-AUROC), with optimal augmentation at 100 synthetic samples.

</details>


### [99] [RoadFusion: Latent Diffusion Model for Pavement Defect Detection](https://arxiv.org/abs/2507.15346)
*Muhammad Aqeel,Kidus Dagnaw Bellete,Francesco Setti*

Main category: cs.CV

TL;DR: RoadFusion addresses pavement defect detection challenges using synthetic anomaly generation and dual-path feature adaptation, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: The paper tackles critical challenges in pavement defect detection: limited annotated data, domain shift, and high variability in defect appearances.

Method: RoadFusion uses a latent diffusion model for synthetic anomaly generation and dual-path feature adaptors for robust representation learning, along with a lightweight discriminator for fine-grained defect detection.

Result: Evaluated on six datasets, RoadFusion excels in classification and localization tasks, setting new benchmarks.

Conclusion: RoadFusion effectively overcomes data scarcity and domain shift, offering a robust solution for real-world road inspection.

Abstract: Pavement defect detection faces critical challenges including limited
annotated data, domain shift between training and deployment environments, and
high variability in defect appearances across different road conditions. We
propose RoadFusion, a framework that addresses these limitations through
synthetic anomaly generation with dual-path feature adaptation. A latent
diffusion model synthesizes diverse, realistic defects using text prompts and
spatial masks, enabling effective training under data scarcity. Two separate
feature adaptors specialize representations for normal and anomalous inputs,
improving robustness to domain shift and defect variability. A lightweight
discriminator learns to distinguish fine-grained defect patterns at the patch
level. Evaluated on six benchmark datasets, RoadFusion achieves consistently
strong performance across both classification and localization tasks, setting
new state-of-the-art in multiple metrics relevant to real-world road
inspection.

</details>


### [100] [DAViD: Data-efficient and Accurate Vision Models from Synthetic Data](https://arxiv.org/abs/2507.15365)
*Fatemeh Saleh,Sadegh Aliakbarian,Charlie Hewitt,Lohit Petikam,Xiao-Xian,Antonio Criminisi,Thomas J. Cashman,Tadas Baltrušaitis*

Main category: cs.CV

TL;DR: Training high-accuracy human-centric vision models using small, high-fidelity synthetic datasets, achieving cost efficiency and fairness.


<details>
  <summary>Details</summary>
Motivation: Address the high cost and data requirements of large-scale models by leveraging synthetic datasets for accuracy and efficiency.

Method: Use procedural data synthesis to create small, high-fidelity synthetic datasets with perfect labels and control over diversity.

Result: Models achieve comparable accuracy on depth estimation, surface normal estimation, and soft foreground segmentation with lower training and inference costs.

Conclusion: Synthetic datasets offer a viable, efficient alternative to large-scale real datasets for human-centric vision tasks.

Abstract: The state of the art in human-centric computer vision achieves high accuracy
and robustness across a diverse range of tasks. The most effective models in
this domain have billions of parameters, thus requiring extremely large
datasets, expensive training regimes, and compute-intensive inference. In this
paper, we demonstrate that it is possible to train models on much smaller but
high-fidelity synthetic datasets, with no loss in accuracy and higher
efficiency. Using synthetic training data provides us with excellent levels of
detail and perfect labels, while providing strong guarantees for data
provenance, usage rights, and user consent. Procedural data synthesis also
provides us with explicit control on data diversity, that we can use to address
unfairness in the models we train. Extensive quantitative assessment on real
input images demonstrates accuracy of our models on three dense prediction
tasks: depth estimation, surface normal estimation, and soft foreground
segmentation. Our models require only a fraction of the cost of training and
inference when compared with foundational models of similar accuracy. Our
human-centric synthetic dataset and trained models are available at
https://aka.ms/DAViD.

</details>


### [101] [Rethinking Occlusion in FER: A Semantic-Aware Perspective and Go Beyond](https://arxiv.org/abs/2507.15401)
*Huiyu Zhai,Xingxing Yang,Yalan Ye,Chenyang Li,Bin Fan,Changze Li*

Main category: cs.CV

TL;DR: ORSANet improves facial expression recognition under occlusion by using multi-modal semantic guidance, a multi-scale fusion module, and a dynamic loss function, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Existing FER models struggle with occlusion and dataset biases, leading to inaccurate classifications.

Method: ORSANet uses semantic segmentation maps and facial landmarks as priors, a Multi-scale Cross-interaction Module for fusion, and a Dynamic Adversarial Repulsion Enhancement Loss.

Result: ORSANet achieves SOTA performance on public benchmarks and the new Occlu-FER dataset.

Conclusion: ORSANet effectively addresses occlusion challenges in FER, demonstrating robustness and superior performance.

Abstract: Facial expression recognition (FER) is a challenging task due to pervasive
occlusion and dataset biases. Especially when facial information is partially
occluded, existing FER models struggle to extract effective facial features,
leading to inaccurate classifications. In response, we present ORSANet, which
introduces the following three key contributions: First, we introduce auxiliary
multi-modal semantic guidance to disambiguate facial occlusion and learn
high-level semantic knowledge, which is two-fold: 1) we introduce semantic
segmentation maps as dense semantics prior to generate semantics-enhanced
facial representations; 2) we introduce facial landmarks as sparse geometric
prior to mitigate intrinsic noises in FER, such as identity and gender biases.
Second, to facilitate the effective incorporation of these two multi-modal
priors, we customize a Multi-scale Cross-interaction Module (MCM) to adaptively
fuse the landmark feature and semantics-enhanced representations within
different scales. Third, we design a Dynamic Adversarial Repulsion Enhancement
Loss (DARELoss) that dynamically adjusts the margins of ambiguous classes,
further enhancing the model's ability to distinguish similar expressions. We
further construct the first occlusion-oriented FER dataset to facilitate
specialized robustness analysis on various real-world occlusion conditions,
dubbed Occlu-FER. Extensive experiments on both public benchmarks and Occlu-FER
demonstrate that our proposed ORSANet achieves SOTA recognition performance.
Code is publicly available at https://github.com/Wenyuzhy/ORSANet-master.

</details>


### [102] [SurgX: Neuron-Concept Association for Explainable Surgical Phase Recognition](https://arxiv.org/abs/2507.15418)
*Ka Young Kim,Hyeon Bae Kim,Seong Tae Kim*

Main category: cs.CV

TL;DR: SurgX is a concept-based explanation framework to improve interpretability in surgical phase recognition models by linking neurons to relevant concepts.


<details>
  <summary>Details</summary>
Motivation: Deep learning models for surgical phase recognition lack interpretability, hindering trust and debugging.

Method: Proposes SurgX, which selects example sequences for neurons, constructs a concept set, associates neurons with concepts, and identifies key neurons for predictions.

Result: Validated on two models, SurgX effectively explains predictions, enhancing interpretability.

Conclusion: SurgX demonstrates potential in making surgical phase recognition models more transparent and trustworthy.

Abstract: Surgical phase recognition plays a crucial role in surgical workflow
analysis, enabling various applications such as surgical monitoring, skill
assessment, and workflow optimization. Despite significant advancements in deep
learning-based surgical phase recognition, these models remain inherently
opaque, making it difficult to understand how they make decisions. This lack of
interpretability hinders trust and makes it challenging to debug the model. To
address this challenge, we propose SurgX, a novel concept-based explanation
framework that enhances the interpretability of surgical phase recognition
models by associating neurons with relevant concepts. In this paper, we
introduce the process of selecting representative example sequences for
neurons, constructing a concept set tailored to the surgical video dataset,
associating neurons with concepts and identifying neurons crucial for
predictions. Through extensive experiments on two surgical phase recognition
models, we validate our method and analyze the explanation for prediction. This
highlights the potential of our method in explaining surgical phase
recognition. The code is available at https://github.com/ailab-kyunghee/SurgX

</details>


### [103] [EgoPrune: Efficient Token Pruning for Egomotion Video Reasoning in Embodied Agent](https://arxiv.org/abs/2507.15428)
*Jiaao Li,Kaiyuan Li,Chen Gao,Yong Li,Xinlei Chen*

Main category: cs.CV

TL;DR: EgoPrune is a training-free token pruning method for egomotion video reasoning, improving efficiency by leveraging spatiotemporal continuity and motion constraints.


<details>
  <summary>Details</summary>
Motivation: Egomotion videos are crucial for embodied AI but current vision-language models are computationally expensive for long videos. Existing pruning methods don't suit egomotion settings.

Method: EgoPrune includes a keyframe selector, Perspective-Aware Redundancy Filtering (PARF), and an MMR-based token selector to prune redundant tokens efficiently.

Result: EgoPrune outperforms prior methods, reducing FLOPs, memory usage, and latency, and works well on edge devices like Jetson Orin NX.

Conclusion: EgoPrune is effective for real-world, on-device egomotion video reasoning.

Abstract: Egomotion videos are first-person recordings where the view changes
continuously due to the agent's movement. As they serve as the primary visual
input for embodied AI agents, making egomotion video reasoning more efficient
is therefore essential for real-world deployment. Recent advances in
vision-language models have enabled strong multimodal reasoning capabilities,
but their computational cost remains prohibitive for long, redundant video
inputs. Existing token pruning methods, typically designed for third-person
videos, fail to leverage the spatiotemporal continuity and motion constraints
inherent in egomotion settings. To address this, we propose EgoPrune, a
training-free token pruning method tailored for egomotion video reasoning.
EgoPrune comprises three components: a keyframe selector adapted from EmbodiedR
for temporally efficient sampling; Perspective-Aware Redundancy Filtering
(PARF), which aligns visual tokens using perspective transformations and
removes redundant tokens; and a Maximal Marginal Relevance (MMR)-based token
selector that jointly considers visual-text relevance and intra-frame
diversity. Experiments on two egomotion video benchmarks show that EgoPrune
consistently outperforms prior training-free methods across various pruning
ratios while significantly reducing FLOPs, memory usage, and latency. Moreover,
we deploy EgoPrune on an embodied agent equipped with a Jetson Orin NX 16GB
edge device, demonstrating its real-world efficiency and suitability for
on-device egomotion video reasoning.

</details>


### [104] [One Last Attention for Your Vision-Language Model](https://arxiv.org/abs/2507.15480)
*Liang Chen,Ghazi Shazan Ahmad,Tianjun Yao,Lingqiao Liu,Zhiqiang Shen*

Main category: cs.CV

TL;DR: RAda is a fine-tuning method for VLMs that dynamically adjusts fused representations to improve cross-modal interactions without heavy modifications.


<details>
  <summary>Details</summary>
Motivation: Current adaptation methods for VLMs neglect the role of fused representations in decision-making, limiting their effectiveness.

Method: RAda uses a learned mask from a lightweight attention layer to calibrate the rational matrix, optimizing cross-modal interactions.

Result: RAda improves baseline performance with minimal code and matches state-of-the-art methods in various settings.

Conclusion: RAda is a versatile and efficient fine-tuning technique for VLMs, enhancing their downstream potential.

Abstract: Pretrained vision-language models (VLMs), such as CLIP, achieve remarkable
zero-shot performance, yet their downstream potential hinges on effective
fine-tuning. Most adaptation methods typically focus on refining representation
from separate modalities (text or vision) but neglect the critical role of
their fused representations in the decision-making process, \emph{\ie} rational
matrix that drives the final prediction. To bridge the gap, we propose a simple
yet effective \textbf{R}ational \textbf{Ada}ptaion ({RAda}) to explicitly
exploit the final fused representation during fine-tuning. RAda employs a
learned mask, obtained from a lightweight attention layer attached at the end
of a VLM, to dynamically calibrate the contribution of each element in the
rational matrix, enabling targeted adjustments to the final cross-modal
interactions without incurring costly modifications to intermediate features.
Experiments in different settings (i.e., updating, or freezing pretrained
encoders in adaptation, and test-time training that can only access the
unlabeled test data) show that RAda serves as a versatile fine-tuning
technique, improving the baseline with minimal code and performing comparably
against current arts in most settings. Code is available at
\href{https://github.com/khufia/RAda/tree/main}{github.com/khufia/RAda}.

</details>


### [105] [An aerial color image anomaly dataset for search missions in complex forested terrain](https://arxiv.org/abs/2507.15492)
*Rakesh John Amala Arokia Nathan,Matthias Gessner,Nurullah Özkan,Marius Bock,Mohamed Youssef,Maximilian Mews,Björn Piltz,Ralf Berger,Oliver Bimber*

Main category: cs.CV

TL;DR: A crowd-search initiative analyzed high-resolution aerial imagery of a dense forest to locate a suspect, creating a unique dataset for anomaly detection in occluded environments.


<details>
  <summary>Details</summary>
Motivation: The failure of automated analysis in dense vegetation prompted the need for a crowd-search approach to detect anomalies in challenging real-world conditions.

Method: High-resolution aerial imagery was captured, and a crowd-search initiative was employed to label hard-to-detect anomalies. An interactive web interface allowed user annotations.

Result: Existing anomaly detection methods performed poorly, emphasizing the need for context-aware approaches. The dataset serves as a benchmark for improving such methods.

Conclusion: The dataset and interactive platform support future research in anomaly detection for complex environments, aiding manhunts and rescue operations.

Abstract: After a family murder in rural Germany, authorities failed to locate the
suspect in a vast forest despite a massive search. To aid the search, a
research aircraft captured high-resolution aerial imagery. Due to dense
vegetation obscuring small clues, automated analysis was ineffective, prompting
a crowd-search initiative. This effort produced a unique dataset of labeled,
hard-to-detect anomalies under occluded, real-world conditions. It can serve as
a benchmark for improving anomaly detection approaches in complex forest
environments, supporting manhunts and rescue operations. Initial benchmark
tests showed existing methods performed poorly, highlighting the need for
context-aware approaches. The dataset is openly accessible for offline
processing. An additional interactive web interface supports online viewing and
dynamic growth by allowing users to annotate and submit new findings.

</details>


### [106] [Dense-depth map guided deep Lidar-Visual Odometry with Sparse Point Clouds and Images](https://arxiv.org/abs/2507.15496)
*JunYing Huang,Ao Xu,DongSun Yong,KeRen Li,YuanFeng Wang,Qi Qin*

Main category: cs.CV

TL;DR: A novel LiDAR-Visual odometry framework integrates LiDAR and images for accurate pose estimation, using depth completion and attention mechanisms, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Accurate and robust self-localization and navigation for autonomous systems require advanced odometry techniques combining LiDAR and visual data.

Method: The framework uses dense-depth maps from depth completion, multi-scale feature extraction with attention, and hierarchical pose refinement for robust motion estimation.

Result: The method achieves similar or superior accuracy and robustness on the KITTI odometry benchmark compared to existing visual and LiDAR odometry techniques.

Conclusion: The proposed LiDAR-Visual odometry framework is effective for accurate and robust pose estimation in dynamic environments.

Abstract: Odometry is a critical task for autonomous systems for self-localization and
navigation. We propose a novel LiDAR-Visual odometry framework that integrates
LiDAR point clouds and images for accurate and robust pose estimation. Our
method utilizes a dense-depth map estimated from point clouds and images
through depth completion, and incorporates a multi-scale feature extraction
network with attention mechanisms, enabling adaptive depth-aware
representations. Furthermore, we leverage dense depth information to refine
flow estimation and mitigate errors in occlusion-prone regions. Our
hierarchical pose refinement module optimizes motion estimation progressively,
ensuring robust predictions against dynamic environments and scale ambiguities.
Comprehensive experiments on the KITTI odometry benchmark demonstrate that our
approach achieves similar or superior accuracy and robustness compared to
state-of-the-art visual and LiDAR odometry methods.

</details>


### [107] [Quantifying and Narrowing the Unknown: Interactive Text-to-Video Retrieval via Uncertainty Minimization](https://arxiv.org/abs/2507.15504)
*Bingqing Zhang,Zhuo Cao,Heming Du,Yang Li,Xue Li,Jiajun Liu,Sen Wang*

Main category: cs.CV

TL;DR: UMIVR is an interactive text-to-video retrieval framework that quantifies uncertainties (text ambiguity, mapping uncertainty, frame uncertainty) to refine queries via targeted questions, improving retrieval performance.


<details>
  <summary>Details</summary>
Motivation: Current interactive TVR systems lack explicit uncertainty quantification, limiting effectiveness. UMIVR addresses this gap by measuring uncertainties to refine user intent.

Method: UMIVR uses training-free metrics: Text Ambiguity Score (TAS), Mapping Uncertainty Score (MUS), and Temporal Quality-based Frame Sampler (TQFS) to quantify uncertainties and guide clarifying questions.

Result: UMIVR achieves 69.2% Recall@1 after 10 rounds on MSR-VTT-1k, demonstrating significant improvement in retrieval accuracy.

Conclusion: UMIVR establishes a principled, uncertainty-minimizing approach for interactive TVR, validated by strong experimental results.

Abstract: Despite recent advances, Text-to-video retrieval (TVR) is still hindered by
multiple inherent uncertainties, such as ambiguous textual queries, indistinct
text-video mappings, and low-quality video frames. Although interactive systems
have emerged to address these challenges by refining user intent through
clarifying questions, current methods typically rely on heuristic or ad-hoc
strategies without explicitly quantifying these uncertainties, limiting their
effectiveness. Motivated by this gap, we propose UMIVR, an
Uncertainty-Minimizing Interactive Text-to-Video Retrieval framework that
explicitly quantifies three critical uncertainties-text ambiguity, mapping
uncertainty, and frame uncertainty-via principled, training-free metrics:
semantic entropy-based Text Ambiguity Score (TAS), Jensen-Shannon
divergence-based Mapping Uncertainty Score (MUS), and a Temporal Quality-based
Frame Sampler (TQFS). By adaptively generating targeted clarifying questions
guided by these uncertainty measures, UMIVR iteratively refines user queries,
significantly reducing retrieval ambiguity. Extensive experiments on multiple
benchmarks validate UMIVR's effectiveness, achieving notable gains in Recall@1
(69.2\% after 10 interactive rounds) on the MSR-VTT-1k dataset, thereby
establishing an uncertainty-minimizing foundation for interactive TVR.

</details>


### [108] [SAIGFormer: A Spatially-Adaptive Illumination-Guided Network for Low-Light Image Enhancement](https://arxiv.org/abs/2507.15520)
*Hanting Li,Fei Zhou,Xin Sun,Yang Hua,Jungong Han,Liang-Jie Zhang*

Main category: cs.CV

TL;DR: SAIGFormer is a Transformer-based framework for low-light enhancement, addressing non-uniform lighting issues with dynamic illumination modeling and guided attention.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with non-uniform lighting (e.g., backlit, shadows), leading to over-exposure or inadequate brightness restoration.

Method: Proposes SAIGFormer with dynamic integral image representation, Spatially-Adaptive Integral Illumination Estimator (SAI²E), and Illumination-Guided Multi-head Self-Attention (IG-MSA).

Result: Outperforms state-of-the-art methods on five datasets and LOL-Blur benchmark, excelling in non-uniform illumination enhancement.

Conclusion: SAIGFormer offers superior performance and generalization for low-light enhancement, especially in challenging non-uniform lighting scenarios.

Abstract: Recent Transformer-based low-light enhancement methods have made promising
progress in recovering global illumination. However, they still struggle with
non-uniform lighting scenarios, such as backlit and shadow, appearing as
over-exposure or inadequate brightness restoration. To address this challenge,
we present a Spatially-Adaptive Illumination-Guided Transformer (SAIGFormer)
framework that enables accurate illumination restoration. Specifically, we
propose a dynamic integral image representation to model the spatially-varying
illumination, and further construct a novel Spatially-Adaptive Integral
Illumination Estimator ($\text{SAI}^2\text{E}$). Moreover, we introduce an
Illumination-Guided Multi-head Self-Attention (IG-MSA) mechanism, which
leverages the illumination to calibrate the lightness-relevant features toward
visual-pleased illumination enhancement. Extensive experiments on five standard
low-light datasets and a cross-domain benchmark (LOL-Blur) demonstrate that our
SAIGFormer significantly outperforms state-of-the-art methods in both
quantitative and qualitative metrics. In particular, our method achieves
superior performance in non-uniform illumination enhancement while exhibiting
strong generalization capabilities across multiple datasets. Code is available
at https://github.com/LHTcode/SAIGFormer.git.

</details>


### [109] [Procedure Learning via Regularized Gromov-Wasserstein Optimal Transport](https://arxiv.org/abs/2507.15540)
*Syed Ahmed Mahmood,Ali Shah Ali,Umer Ahmed,Fawad Javed Fateh,M. Zeeshan Zia,Quoc-Huy Tran*

Main category: cs.CV

TL;DR: A self-supervised framework for procedure learning using fused Gromov-Wasserstein optimal transport and contrastive regularization to address challenges like order variations and redundant frames.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of previous methods suffering from order variations, background/redundant frames, and repeated actions in procedural videos.

Method: Proposes a self-supervised framework combining fused Gromov-Wasserstein optimal transport with a structural prior and contrastive regularization to avoid degenerate solutions.

Result: Demonstrates superior performance on benchmarks (EgoProceL, ProceL, CrossTask) compared to previous methods like OPEL.

Conclusion: The proposed framework effectively addresses key challenges in self-supervised procedure learning and outperforms existing approaches.

Abstract: We study the problem of self-supervised procedure learning, which discovers
key steps and establishes their order from a set of unlabeled procedural
videos. Previous procedure learning methods typically learn frame-to-frame
correspondences between videos before determining key steps and their order.
However, their performance often suffers from order variations,
background/redundant frames, and repeated actions. To overcome these
challenges, we propose a self-supervised procedure learning framework, which
utilizes a fused Gromov-Wasserstein optimal transport formulation with a
structural prior for computing frame-to-frame mapping between videos. However,
optimizing exclusively for the above temporal alignment term may lead to
degenerate solutions, where all frames are mapped to a small cluster in the
embedding space and hence every video is associated with only one key step. To
address that limitation, we further integrate a contrastive regularization
term, which maps different frames to different points in the embedding space,
avoiding the collapse to trivial solutions. Finally, we conduct extensive
experiments on large-scale egocentric (i.e., EgoProceL) and third-person (i.e.,
ProceL and CrossTask) benchmarks to demonstrate superior performance by our
approach against previous methods, including OPEL which relies on a traditional
Kantorovich optimal transport formulation with an optimality prior.

</details>


### [110] [Towards Holistic Surgical Scene Graph](https://arxiv.org/abs/2507.15541)
*Jongmin Shin,Enki Cho,Ka Yong Kim,Jung Yong Kim,Seong Tae Kim,Namkee Oh*

Main category: cs.CV

TL;DR: The paper introduces Endoscapes-SG201 dataset and SSG-Com, a graph-based method, to enhance surgical scene understanding by modeling tool-action-target combinations and hand identity in surgical scenes.


<details>
  <summary>Details</summary>
Motivation: Existing graph-based representations of surgical scenes lack details like tool-action-target combinations and hand identity, which are crucial for comprehensive scene understanding.

Method: The authors propose the Endoscapes-SG201 dataset with annotations for tool-action-target and hand identity, and introduce SSG-Com, a graph-based method to learn these elements.

Result: Experiments on tasks like critical view of safety assessment and action triplet recognition show the importance of integrating these components.

Conclusion: The study highlights the significance of detailed graph representations for surgical scene understanding and provides a dataset and method to address the gap.

Abstract: Surgical scene understanding is crucial for computer-assisted intervention
systems, requiring visual comprehension of surgical scenes that involves
diverse elements such as surgical tools, anatomical structures, and their
interactions. To effectively represent the complex information in surgical
scenes, graph-based approaches have been explored to structurally model
surgical entities and their relationships. Previous surgical scene graph
studies have demonstrated the feasibility of representing surgical scenes using
graphs. However, certain aspects of surgical scenes-such as diverse
combinations of tool-action-target and the identity of the hand operating the
tool-remain underexplored in graph-based representations, despite their
importance. To incorporate these aspects into graph representations, we propose
Endoscapes-SG201 dataset, which includes annotations for tool-action-target
combinations and hand identity. We also introduce SSG-Com, a graph-based method
designed to learn and represent these critical elements. Through experiments on
downstream tasks such as critical view of safety assessment and action triplet
recognition, we demonstrated the importance of integrating these essential
scene graph components, highlighting their significant contribution to surgical
scene understanding. The code and dataset are available at
https://github.com/ailab-kyunghee/SSG-Com

</details>


### [111] [HOLa: Zero-Shot HOI Detection with Low-Rank Decomposed VLM Feature Adaptation](https://arxiv.org/abs/2507.15542)
*Qinqian Lei,Bo Wang,Robby T. Tan*

Main category: cs.CV

TL;DR: HOLa improves zero-shot HOI detection by decomposing VLM text features into class-shared bases and adaptable weights, enhancing generalization and action distinction.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of existing methods in distinguishing actions and generalizing to unseen classes in zero-shot HOI detection.

Method: Decomposes VLM text features via low-rank factorization, adapts weights for each HOI class, and uses human-object tokens and LLM-derived action regularization.

Result: Achieves a state-of-the-art unseen-class mAP of 27.91 on HICO-DET in the unseen-verb setting.

Conclusion: HOLa effectively enhances generalization and action distinction in zero-shot HOI detection.

Abstract: Zero-shot human-object interaction (HOI) detection remains a challenging
task, particularly in generalizing to unseen actions. Existing methods address
this challenge by tapping Vision-Language Models (VLMs) to access knowledge
beyond the training data. However, they either struggle to distinguish actions
involving the same object or demonstrate limited generalization to unseen
classes. In this paper, we introduce HOLa (Zero-Shot HOI Detection with
Low-Rank Decomposed VLM Feature Adaptation), a novel approach that both
enhances generalization to unseen classes and improves action distinction. In
training, HOLa decomposes VLM text features for given HOI classes via low-rank
factorization, producing class-shared basis features and adaptable weights.
These features and weights form a compact HOI representation that preserves
shared information across classes, enhancing generalization to unseen classes.
Subsequently, we refine action distinction by adapting weights for each HOI
class and introducing human-object tokens to enrich visual interaction
representations. To further distinguish unseen actions, we guide the weight
adaptation with LLM-derived action regularization. Experimental results show
that our method sets a new state-of-the-art across zero-shot HOI settings on
HICO-DET, achieving an unseen-class mAP of 27.91 in the unseen-verb setting.
Our code is available at https://github.com/ChelsieLei/HOLa.

</details>


### [112] [DynImg: Key Frames with Visual Prompts are Good Representation for Multi-Modal Video Understanding](https://arxiv.org/abs/2507.15569)
*Xiaoyi Bao,Chenwei Xie,Hao Tang,Tingyu Weng,Xiaofeng Wang,Yun Zheng,Xingang Wang*

Main category: cs.CV

TL;DR: The paper introduces Dynamic-Image (DynImg), a novel video representation method using non-key frames as temporal prompts to improve spatio-temporal interaction in video understanding tasks.


<details>
  <summary>Details</summary>
Motivation: Traditional methods struggle with accurately representing spatial information of fast-moving objects due to motion blur, leading to underemphasized temporally important regions.

Method: DynImg uses non-key frames as temporal prompts to highlight fast-moving objects and employs 4D Rotary Position Embedding to maintain spatio-temporal order.

Result: DynImg outperforms state-of-the-art methods by ~2% on multiple video understanding benchmarks.

Conclusion: DynImg effectively enhances video comprehension by integrating temporal prompts and maintaining spatio-temporal adjacency.

Abstract: In recent years, the introduction of Multi-modal Large Language Models
(MLLMs) into video understanding tasks has become increasingly prevalent.
However, how to effectively integrate temporal information remains a critical
research focus. Traditional approaches treat spatial and temporal information
separately. Due to issues like motion blur, it is challenging to accurately
represent the spatial information of rapidly moving objects. This can lead to
temporally important regions being underemphasized during spatial feature
extraction, which in turn hinders accurate spatio-temporal interaction and
video understanding. To address this limitation, we propose an innovative video
representation method called Dynamic-Image (DynImg). Specifically, we introduce
a set of non-key frames as temporal prompts to highlight the spatial areas
containing fast-moving objects. During the process of visual feature
extraction, these prompts guide the model to pay additional attention to the
fine-grained spatial features corresponding to these regions. Moreover, to
maintain the correct sequence for DynImg, we employ a corresponding 4D video
Rotary Position Embedding. This retains both the temporal and spatial adjacency
of DynImg, helping MLLM understand the spatio-temporal order within this
combined format. Experimental evaluations reveal that DynImg surpasses the
state-of-the-art methods by approximately 2% across multiple video
understanding benchmarks, proving the effectiveness of our temporal prompts in
enhancing video comprehension.

</details>


### [113] [GeMix: Conditional GAN-Based Mixup for Improved Medical Image Augmentation](https://arxiv.org/abs/2507.15577)
*Hugo Carlesso,Maria Eliza Patulea,Moncef Garouani,Radu Tudor Ionescu,Josiane Mothe*

Main category: cs.CV

TL;DR: GeMix is a label-aware interpolation framework using GANs for realistic image augmentation, outperforming traditional mixup in medical imaging tasks like COVID-19 detection.


<details>
  <summary>Details</summary>
Motivation: Naive pixel-wise mixup produces unrealistic images, hindering learning in high-stakes medical applications.

Method: Uses a two-stage framework with class-conditional GANs (StyleGAN2-ADA) for label-aware interpolation, blending label vectors from Dirichlet priors.

Result: Improves macro-F1 scores and reduces false negatives in COVID-19 detection across multiple backbones.

Conclusion: GeMix is a drop-in replacement for mixup, offering better regularization and semantic fidelity without disrupting training pipelines.

Abstract: Mixup has become a popular augmentation strategy for image classification,
yet its naive pixel-wise interpolation often produces unrealistic images that
can hinder learning, particularly in high-stakes medical applications. We
propose GeMix, a two-stage framework that replaces heuristic blending with a
learned, label-aware interpolation powered by class-conditional GANs. First, a
StyleGAN2-ADA generator is trained on the target dataset. During augmentation,
we sample two label vectors from Dirichlet priors biased toward different
classes and blend them via a Beta-distributed coefficient. Then, we condition
the generator on this soft label to synthesize visually coherent images that
lie along a continuous class manifold. We benchmark GeMix on the large-scale
COVIDx-CT-3 dataset using three backbones (ResNet-50, ResNet-101,
EfficientNet-B0). When combined with real data, our method increases macro-F1
over traditional mixup for all backbones, reducing the false negative rate for
COVID-19 detection. GeMix is thus a drop-in replacement for pixel-space mixup,
delivering stronger regularization and greater semantic fidelity, without
disrupting existing training pipelines. We publicly release our code at
https://github.com/hugocarlesso/GeMix to foster reproducibility and further
research.

</details>


### [114] [Compress-Align-Detect: onboard change detection from unregistered images](https://arxiv.org/abs/2507.15578)
*Gabriele Inzerillo,Diego Valsesia,Aniello Fiengo,Enrico Magli*

Main category: cs.CV

TL;DR: Proposes an onboard satellite framework for real-time change detection using a deep neural network with three submodules: compression, co-registration, and change detection.


<details>
  <summary>Details</summary>
Motivation: Overcome latency in traditional change detection by shifting the workflow to satellites for real-time applications.

Method: End-to-end deep neural network with submodules for compression, co-registration, and change detection, optimized for low-power hardware.

Result: Achieves 0.7 Mpixel/s throughput on a 15W accelerator with competitive F1 scores at varying compression rates.

Conclusion: Demonstrates feasibility of onboard change detection with efficient performance under strict constraints.

Abstract: Change detection from satellite images typically incurs a delay ranging from
several hours up to days because of latency in downlinking the acquired images
and generating orthorectified image products at the ground stations; this may
preclude real- or near real-time applications. To overcome this limitation, we
propose shifting the entire change detection workflow onboard satellites. This
requires to simultaneously solve challenges in data storage, image registration
and change detection with a strict complexity constraint. In this paper, we
present a novel and efficient framework for onboard change detection that
addresses the aforementioned challenges in an end-to-end fashion with a deep
neural network composed of three interlinked submodules: (1) image compression,
tailored to minimize onboard data storage resources; (2) lightweight
co-registration of non-orthorectified multi-temporal image pairs; and (3) a
novel temporally-invariant and computationally efficient change detection
model. This is the first approach in the literature combining all these tasks
in a single end-to-end framework with the constraints dictated by onboard
processing. Experimental results compare each submodule with the current
state-of-the-art, and evaluate the performance of the overall integrated system
in realistic setting on low-power hardware. Compelling change detection results
are obtained in terms of F1 score as a function of compression rate, sustaining
a throughput of 0.7 Mpixel/s on a 15W accelerator.

</details>


### [115] [SegDT: A Diffusion Transformer-Based Segmentation Model for Medical Imaging](https://arxiv.org/abs/2507.15595)
*Salah Eddine Bekhouche,Gaby Maroun,Fadi Dornaika,Abdenour Hadid*

Main category: cs.CV

TL;DR: SegDT, a diffusion transformer-based model for skin lesion segmentation, achieves state-of-the-art results on low-cost hardware, improving diagnostic tools in healthcare.


<details>
  <summary>Details</summary>
Motivation: To enhance medical image segmentation, particularly for skin lesions, by developing a model that balances performance, speed, and hardware affordability.

Method: SegDT integrates Rectified Flow with a diffusion transformer (DiT) to improve generation quality and reduce inference steps, evaluated on three datasets.

Result: Achieves state-of-the-art performance with fast inference speeds, making it suitable for real-world medical applications.

Conclusion: SegDT advances deep learning in medical image analysis, offering faster and more accurate diagnostic tools.

Abstract: Medical image segmentation is crucial for many healthcare tasks, including
disease diagnosis and treatment planning. One key area is the segmentation of
skin lesions, which is vital for diagnosing skin cancer and monitoring
patients. In this context, this paper introduces SegDT, a new segmentation
model based on diffusion transformer (DiT). SegDT is designed to work on
low-cost hardware and incorporates Rectified Flow, which improves the
generation quality at reduced inference steps and maintains the flexibility of
standard diffusion models. Our method is evaluated on three benchmarking
datasets and compared against several existing works, achieving
state-of-the-art results while maintaining fast inference speeds. This makes
the proposed model appealing for real-world medical applications. This work
advances the performance and capabilities of deep learning models in medical
image analysis, enabling faster, more accurate diagnostic tools for healthcare
professionals. The code is made publicly available at
\href{https://github.com/Bekhouche/SegDT}{GitHub}.

</details>


### [116] [Being-H0: Vision-Language-Action Pretraining from Large-Scale Human Videos](https://arxiv.org/abs/2507.15597)
*Hao Luo,Yicheng Feng,Wanpeng Zhang,Sipeng Zheng,Ye Wang,Haoqi Yuan,Jiazheng Liu,Chaoyi Xu,Qin Jin,Zongqing Lu*

Main category: cs.CV

TL;DR: Being-H0 is a Vision-Language-Action model trained on human videos to address dexterity and generalization gaps in manipulation tasks, using physical instruction tuning and part-level motion tokenization.


<details>
  <summary>Details</summary>
Motivation: Existing VLAs struggle with complex manipulation tasks due to reliance on synthetic or limited teleoperated data, lacking dexterity and diversity.

Method: Proposes physical instruction tuning, combining VLA pretraining, 3D reasoning alignment, and robotic task adaptation, with part-level motion tokenization for precision.

Result: Being-H0 excels in hand motion generation and instruction following, scaling well with model and data sizes, and shows real-world robotic manipulation improvements.

Conclusion: Being-H0 demonstrates the effectiveness of leveraging human videos and physical instruction tuning for dexterous manipulation tasks, bridging sim-to-real gaps.

Abstract: We introduce Being-H0, a dexterous Vision-Language-Action model (VLA) trained
on large-scale human videos. Existing VLAs struggle with complex manipulation
tasks requiring high dexterity and generalize poorly to novel scenarios and
tasks, primarily due to their reliance on synthetic data with significant
sim-to-real gaps or teleoperated demonstrations lacking scale and diversity. To
address this data bottleneck, we propose leveraging human hands as a foundation
manipulator, capitalizing on the rich dexterity and scalability present in web
data. Our approach centers on physical instruction tuning, a novel training
paradigm that combines large-scale VLA pretraining from human videos, physical
space alignment for 3D reasoning, and post-training adaptation for robotic
tasks. Additionally, we introduce a part-level motion tokenization method which
achieves millimeter-level reconstruction accuracy to model precise hand
trajectories for action learning. To support our proposed paradigm, we further
develop a comprehensive data curation pipeline that integrates heterogeneous
sources -- including motion capture, VR, and RGB-only videos -- into a
large-scale dataset with millions of motion-based instructional instances. We
empirically show the excellence of Being-H0 in hand motion generation and
instruction following, and it also scales well with model and data sizes.
Importantly, we observe the expected gains of Being-H0 in real-world robotic
manipulation as physical instruction tuning is applied. More details are
available at https://beingbeyond.github.io/Being-H0.

</details>


### [117] [SurfaceSplat: Connecting Surface Reconstruction and Gaussian Splatting](https://arxiv.org/abs/2507.15602)
*Zihui Gao,Jia-Wang Bian,Guosheng Lin,Hao Chen,Chunhua Shen*

Main category: cs.CV

TL;DR: A hybrid method combining SDF and 3DGS improves surface reconstruction and novel view rendering by leveraging coarse geometry and fine details.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of SDF (lacking fine details) and 3DGS (lacking global coherence) in sparse-view image tasks.

Method: Combines SDF for coarse geometry and 3DGS for detail refinement, using rendered images from 3DGS to enhance SDF accuracy.

Result: Outperforms state-of-the-art methods on DTU and MobileBrick datasets.

Conclusion: The hybrid approach effectively balances geometry and detail, advancing surface reconstruction and rendering.

Abstract: Surface reconstruction and novel view rendering from sparse-view images are
challenging. Signed Distance Function (SDF)-based methods struggle with fine
details, while 3D Gaussian Splatting (3DGS)-based approaches lack global
geometry coherence. We propose a novel hybrid method that combines the
strengths of both approaches: SDF captures coarse geometry to enhance
3DGS-based rendering, while newly rendered images from 3DGS refine the details
of SDF for accurate surface reconstruction. As a result, our method surpasses
state-of-the-art approaches in surface reconstruction and novel view synthesis
on the DTU and MobileBrick datasets. Code will be released at
https://github.com/Gaozihui/SurfaceSplat.

</details>


### [118] [CylinderPlane: Nested Cylinder Representation for 3D-aware Image Generation](https://arxiv.org/abs/2507.15606)
*Ru Jia,Xiaozhuang Ma,Jianji Wang,Nanning Zheng*

Main category: cs.CV

TL;DR: The paper introduces CylinderPlane, a cylindrical coordinate-based implicit representation, to address multi-face artifacts and improve 360° view image synthesis in 3D-aware generative models.


<details>
  <summary>Details</summary>
Motivation: Tri-plane representations suffer from feature ambiguity and multi-face artifacts due to shared features in symmetric regions, limiting 360° view generation.

Method: Proposes CylinderPlane, using a cylindrical coordinate system to separate features by angle, and introduces nested cylinders for multi-scale geometry and resolution adaptability.

Result: Achieves high-quality, artifact-free 360° image synthesis, outperforming previous methods on synthetic and real-world datasets.

Conclusion: CylinderPlane effectively resolves feature ambiguity, enhances multi-view consistency, and is compatible with existing neural rendering pipelines.

Abstract: While the proposal of the Tri-plane representation has advanced the
development of the 3D-aware image generative models, problems rooted in its
inherent structure, such as multi-face artifacts caused by sharing the same
features in symmetric regions, limit its ability to generate 360$^\circ$ view
images. In this paper, we propose CylinderPlane, a novel implicit
representation based on Cylindrical Coordinate System, to eliminate the feature
ambiguity issue and ensure multi-view consistency in 360$^\circ$. Different
from the inevitable feature entanglement in Cartesian coordinate-based
Tri-plane representation, the cylindrical coordinate system explicitly
separates features at different angles, allowing our cylindrical representation
possible to achieve high-quality, artifacts-free 360$^\circ$ image synthesis.
We further introduce the nested cylinder representation that composites
multiple cylinders at different scales, thereby enabling the model more
adaptable to complex geometry and varying resolutions. The combination of
cylinders with different resolutions can effectively capture more critical
locations and multi-scale features, greatly facilitates fine detail learning
and robustness to different resolutions. Moreover, our representation is
agnostic to implicit rendering methods and can be easily integrated into any
neural rendering pipeline. Extensive experiments on both synthetic dataset and
unstructured in-the-wild images demonstrate that our proposed representation
achieves superior performance over previous methods.

</details>


### [119] [A Survey on Efficiency Optimization Techniques for DNN-based Video Analytics: Process Systems, Algorithms, and Applications](https://arxiv.org/abs/2507.15628)
*Shanjiang Tang,Rui Huang,Hsinyu Luo,Chunjiang Wang,Ce Yu,Yusen Li,Hao Fu,Chao Sun,and Jian Xiao*

Main category: cs.CV

TL;DR: A survey reviewing efficiency optimization techniques for DNNs in video analytics, covering hardware, data processing, and deployment, and discussing challenges.


<details>
  <summary>Details</summary>
Motivation: The rapid growth of video data demands efficient and accurate analytics, with DNNs ensuring accuracy but lacking efficiency focus.

Method: Organizes existing methods bottom-up, addressing hardware support, data processing, and operational deployment.

Result: Provides a comprehensive review of efficiency optimization techniques for DNNs in video analytics.

Conclusion: Highlights challenges and problems in optimizing DNN performance for video analytics.

Abstract: The explosive growth of video data in recent years has brought higher demands
for video analytics, where accuracy and efficiency remain the two primary
concerns. Deep neural networks (DNNs) have been widely adopted to ensure
accuracy; however, improving their efficiency in video analytics remains an
open challenge. Different from existing surveys that make summaries of
DNN-based video mainly from the accuracy optimization aspect, in this survey,
we aim to provide a thorough review of optimization techniques focusing on the
improvement of the efficiency of DNNs in video analytics. We organize existing
methods in a bottom-up manner, covering multiple perspectives such as hardware
support, data processing, operational deployment, etc. Finally, based on the
optimization framework and existing works, we analyze and discuss the problems
and challenges in the performance optimization of DNN-based video analytics.

</details>


### [120] [Experimenting active and sequential learning in a medieval music manuscript](https://arxiv.org/abs/2507.15633)
*Sachin Sharma,Federico Simonetta,Michele Flammini*

Main category: cs.CV

TL;DR: The paper explores Active Learning (AL) and Sequential Learning (SL) for OMR in medieval music manuscripts, using YOLOv8 to minimize manual labeling. Results show comparable accuracy to full supervision with fewer labels, but uncertainty-based AL is ineffective for the tested dataset.


<details>
  <summary>Details</summary>
Motivation: Addressing the scarcity of annotated data and complexity of historical manuscripts in Optical Music Recognition (OMR) for cultural heritage digitization.

Method: Uses YOLOv8 for object detection and layout recognition, selecting uncertain samples for iterative labeling and retraining, starting with one annotated image.

Result: Achieves comparable accuracy to fully supervised training with fewer labeled examples, but uncertainty-based AL is ineffective for the tested dataset.

Conclusion: Advocates for more usable methods in data-scarcity scenarios, as uncertainty-based AL did not perform well for the manuscript studied.

Abstract: Optical Music Recognition (OMR) is a cornerstone of music digitization
initiatives in cultural heritage, yet it remains limited by the scarcity of
annotated data and the complexity of historical manuscripts. In this paper, we
present a preliminary study of Active Learning (AL) and Sequential Learning
(SL) tailored for object detection and layout recognition in an old medieval
music manuscript. Leveraging YOLOv8, our system selects samples with the
highest uncertainty (lowest prediction confidence) for iterative labeling and
retraining. Our approach starts with a single annotated image and successfully
boosts performance while minimizing manual labeling. Experimental results
indicate that comparable accuracy to fully supervised training can be achieved
with significantly fewer labeled examples. We test the methodology as a
preliminary investigation on a novel dataset offered to the community by the
Anonymous project, which studies laude, a poetical-musical genre spread across
Italy during the 12th-16th Century. We show that in the manuscript at-hand,
uncertainty-based AL is not effective and advocates for more usable methods in
data-scarcity scenarios.

</details>


### [121] [Uncovering Critical Features for Deepfake Detection through the Lottery Ticket Hypothesis](https://arxiv.org/abs/2507.15636)
*Lisan Al Amin,Md. Ismail Hossain,Thanh Thi Nguyen,Tasnim Jahan,Mahbubul Islam,Faisal Quader*

Main category: cs.CV

TL;DR: The study applies the Lottery Ticket Hypothesis (LTH) to deepfake detection, identifying efficient subnetworks (winning tickets) that maintain high accuracy even at high sparsity levels.


<details>
  <summary>Details</summary>
Motivation: Deepfake detection methods are resource-intensive and poorly understood; the study aims to improve efficiency and deployability.

Method: Uses LTH-based iterative magnitude pruning on MesoNet, CNN-5, and ResNet-18 architectures, tested on OpenForensic and FaceForensics++ datasets.

Result: MesoNet retains 56.2% accuracy at 80% sparsity (90% of baseline). LTH-based pruning outperforms one-shot methods, and winning tickets transfer across datasets.

Conclusion: LTH enables efficient, deployable deepfake detection systems with minimal performance loss.

Abstract: Recent advances in deepfake technology have created increasingly convincing
synthetic media that poses significant challenges to information integrity and
social trust. While current detection methods show promise, their underlying
mechanisms remain poorly understood, and the large sizes of their models make
them challenging to deploy in resource-limited environments. This study
investigates the application of the Lottery Ticket Hypothesis (LTH) to deepfake
detection, aiming to identify the key features crucial for recognizing
deepfakes. We examine how neural networks can be efficiently pruned while
maintaining high detection accuracy. Through extensive experiments with
MesoNet, CNN-5, and ResNet-18 architectures on the OpenForensic and
FaceForensics++ datasets, we find that deepfake detection networks contain
winning tickets, i.e., subnetworks, that preserve performance even at
substantial sparsity levels. Our results indicate that MesoNet retains 56.2%
accuracy at 80% sparsity on the OpenForensic dataset, with only 3,000
parameters, which is about 90% of its baseline accuracy (62.6%). The results
also show that our proposed LTH-based iterative magnitude pruning approach
consistently outperforms one-shot pruning methods. Using Grad-CAM
visualization, we analyze how pruned networks maintain their focus on critical
facial regions for deepfake detection. Additionally, we demonstrate the
transferability of winning tickets across datasets, suggesting potential for
efficient, deployable deepfake detection systems.

</details>


### [122] [Extracting Visual Facts from Intermediate Layers for Mitigating Hallucinations in Multimodal Large Language Models](https://arxiv.org/abs/2507.15652)
*Haoran Zhou,Zihan Zhang,Hao Chen*

Main category: cs.CV

TL;DR: The paper introduces EVA, a training-free method to reduce object hallucinations in Multimodal Large Language Models (MLLMs) by dynamically selecting layers with visual factual information.


<details>
  <summary>Details</summary>
Motivation: MLLMs struggle with object hallucinations due to suppressed visual information by prior knowledge, especially in intermediate layers.

Method: EVA contrasts output distributions of selected layers from original and pure-text inputs to extract and incorporate visual facts into final outputs.

Result: EVA significantly reduces hallucination rates on benchmarks, outperforming baseline methods.

Conclusion: EVA is a model-agnostic, effective solution for mitigating hallucinations in MLLMs.

Abstract: Multimodal Large Language Models (MLLMs) have made significant strides by
combining visual recognition and language understanding to generate content
that is both coherent and contextually accurate. However, MLLMs continue to
struggle with object hallucinations, where models produce seemingly plausible
but factually incorrect outputs, including objects that do not exist in the
image. Recent work has revealed that the prior knowledge in MLLMs significantly
suppresses visual information in deep layers, causing hallucinatory outputs.
However, how these priors suppress visual information at the intermediate layer
stage in MLLMs remains unclear. We observe that visual factual knowledge and
the differences between intermediate-layer prior/original probability
distributions show similar evolutionary trends in intermediate layers.
Motivated by this, we introduce Decoding by Extracting Visual Facts (EVA), a
simple, training-free method that dynamically selects intermediate layers with
the most significant visual factual information. By contrasting the output
distributions of the selected layer derived from the original input and
pure-text input, EVA extracts visual factual knowledge and proportionally
incorporates it into the final layer to correct the output logits. Importantly,
EVA is model-agnostic, seamlessly integrates with various classic decoding
strategies, and is applicable across different MLLMs. We validate EVA on
widely-used benchmarks, and the results show that it significantly reduces
hallucination rates compared to baseline methods, underscoring its
effectiveness in mitigating hallucinations.

</details>


### [123] [HW-MLVQA: Elucidating Multilingual Handwritten Document Understanding with a Comprehensive VQA Benchmark](https://arxiv.org/abs/2507.15655)
*Aniket Pal,Ajoy Mondal,Minesh Mathew,C. V. Jawahar*

Main category: cs.CV

TL;DR: HW-MLVQA is a new benchmark for Multilingual Handwritten Visual Question Answering, addressing gaps in current models by including 1,600 handwritten pages and 2,400 Q&A pairs. It evaluates text, image, and combined modalities, and tests OCR models in real-world scenarios.


<details>
  <summary>Details</summary>
Motivation: Current MLVQA models underperform with handwritten documents. HW-MLVQA aims to fill this gap by providing a comprehensive benchmark for multilingual handwritten document comprehension.

Method: HW-MLVQA includes 1,600 handwritten pages and 2,400 Q&A pairs, evaluating three modalities (text, image, and combined). It also tests OCR models without ground truth transcriptions.

Result: The benchmark provides a robust framework for assessing multilingual handwritten document interpretation, enabling advancements in this niche area.

Conclusion: HW-MLVQA fosters innovation in multilingual handwritten document comprehension, addressing current limitations and encouraging further research.

Abstract: The proliferation of MultiLingual Visual Question Answering (MLVQA)
benchmarks augments the capabilities of large language models (LLMs) and
multi-modal LLMs, thereby enabling them to adeptly capture the intricate
linguistic subtleties and visual complexities inherent across diverse
languages. Despite its potential, the current MLVQA model struggles to fully
utilize its capabilities when dealing with the extensive variety of handwritten
documents. This article delineates HW-MLVQA, an avant-garde VQA benchmark
meticulously crafted to mitigate the dearth of authentic Multilingual
Handwritten document comprehension. HW-MLVQA encompasses an extensive
collection of 1,600 handwritten Pages complemented by 2,400 question-answers.
Furthermore, it provides a robust benchmark evaluation framework spanning three
distinct modalities: text, image, and an integrated image & text modality. To
simulate authentic real-world contexts devoid of ground truth textual
transcriptions, we facilitates a rigorous assessment of proprietary and
open-source OCR models. The benchmark aspires to facilitate pivotal
advancements in multilingual handwritten document interpretation, fostering
innovation and scholarly inquiry within this specialized domain.

</details>


### [124] [Visual-Language Model Knowledge Distillation Method for Image Quality Assessment](https://arxiv.org/abs/2507.15680)
*Yongkang Hou,Jiarun Song*

Main category: cs.CV

TL;DR: A method using CLIP for Image Quality Assessment (IQA) via knowledge distillation reduces model complexity and improves performance.


<details>
  <summary>Details</summary>
Motivation: Address CLIP's limitations in IQA: high parameter burden and poor local distortion detection.

Method: Design quality-graded prompts, fine-tune CLIP, and use modality-adaptive knowledge distillation.

Result: Outperforms existing IQA methods with reduced complexity.

Conclusion: The approach shows strong potential for practical IQA deployment.

Abstract: Image Quality Assessment (IQA) is a core task in computer vision. Multimodal
methods based on vision-language models, such as CLIP, have demonstrated
exceptional generalization capabilities in IQA tasks. To address the issues of
excessive parameter burden and insufficient ability to identify local distorted
features in CLIP for IQA, this study proposes a visual-language model knowledge
distillation method aimed at guiding the training of models with architectural
advantages using CLIP's IQA knowledge. First, quality-graded prompt templates
were designed to guide CLIP to output quality scores. Then, CLIP is fine-tuned
to enhance its capabilities in IQA tasks. Finally, a modality-adaptive
knowledge distillation strategy is proposed to achieve guidance from the CLIP
teacher model to the student model. Our experiments were conducted on multiple
IQA datasets, and the results show that the proposed method significantly
reduces model complexity while outperforming existing IQA methods,
demonstrating strong potential for practical deployment.

</details>


### [125] [Hi^2-GSLoc: Dual-Hierarchical Gaussian-Specific Visual Relocalization for Remote Sensing](https://arxiv.org/abs/2507.15683)
*Boni Hu,Zhenyu Xia,Lin Chen,Pengcheng Han,Shuhui Bu*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Visual relocalization, which estimates the 6-degree-of-freedom (6-DoF) camera
pose from query images, is fundamental to remote sensing and UAV applications.
Existing methods face inherent trade-offs: image-based retrieval and pose
regression approaches lack precision, while structure-based methods that
register queries to Structure-from-Motion (SfM) models suffer from
computational complexity and limited scalability. These challenges are
particularly pronounced in remote sensing scenarios due to large-scale scenes,
high altitude variations, and domain gaps of existing visual priors. To
overcome these limitations, we leverage 3D Gaussian Splatting (3DGS) as a novel
scene representation that compactly encodes both 3D geometry and appearance. We
introduce $\mathrm{Hi}^2$-GSLoc, a dual-hierarchical relocalization framework
that follows a sparse-to-dense and coarse-to-fine paradigm, fully exploiting
the rich semantic information and geometric constraints inherent in Gaussian
primitives. To handle large-scale remote sensing scenarios, we incorporate
partitioned Gaussian training, GPU-accelerated parallel matching, and dynamic
memory management strategies. Our approach consists of two stages: (1) a sparse
stage featuring a Gaussian-specific consistent render-aware sampling strategy
and landmark-guided detector for robust and accurate initial pose estimation,
and (2) a dense stage that iteratively refines poses through coarse-to-fine
dense rasterization matching while incorporating reliability verification.
Through comprehensive evaluation on simulation data, public datasets, and real
flight experiments, we demonstrate that our method delivers competitive
localization accuracy, recall rate, and computational efficiency while
effectively filtering unreliable pose estimates. The results confirm the
effectiveness of our approach for practical remote sensing applications.

</details>


### [126] [LINR-PCGC: Lossless Implicit Neural Representations for Point Cloud Geometry Compression](https://arxiv.org/abs/2507.15686)
*Wenjie Huang,Qi Yang,Shuting Xia,He Huang,Zhu Li,Yiling Xu*

Main category: cs.CV

TL;DR: Proposes LINR-PCGC, the first INR-based lossless point cloud geometry compression method, improving speed and efficiency over traditional and AI-based methods.


<details>
  <summary>Details</summary>
Motivation: Addresses limitations of AI-based methods (data distribution dependence) and INR methods (encoding time, decoder size) for point cloud compression.

Method: Uses a group-level coding framework, effective network initialization, and a lightweight multiscale SparseConv network for fast inference and compact decoder size.

Result: Reduces encoding time by ~60%, bitstream by ~21.21% (vs G-PCC) and ~21.95% (vs SparsePCGC) on MVUB dataset.

Conclusion: LINR-PCGC outperforms existing methods in lossless compression, offering faster encoding and smaller bitstreams.

Abstract: Existing AI-based point cloud compression methods struggle with dependence on
specific training data distributions, which limits their real-world deployment.
Implicit Neural Representation (INR) methods solve the above problem by
encoding overfitted network parameters to the bitstream, resulting in more
distribution-agnostic results. However, due to the limitation of encoding time
and decoder size, current INR based methods only consider lossy geometry
compression. In this paper, we propose the first INR based lossless point cloud
geometry compression method called Lossless Implicit Neural Representations for
Point Cloud Geometry Compression (LINR-PCGC). To accelerate encoding speed, we
design a group of point clouds level coding framework with an effective network
initialization strategy, which can reduce around 60% encoding time. A
lightweight coding network based on multiscale SparseConv, consisting of scale
context extraction, child node prediction, and model compression modules, is
proposed to realize fast inference and compact decoder size. Experimental
results show that our method consistently outperforms traditional and AI-based
methods: for example, with the convergence time in the MVUB dataset, our method
reduces the bitstream by approximately 21.21% compared to G-PCC TMC13v23 and
21.95% compared to SparsePCGC. Our project can be seen on
https://huangwenjie2023.github.io/LINR-PCGC/.

</details>


### [127] [DWTGS: Rethinking Frequency Regularization for Sparse-view 3D Gaussian Splatting](https://arxiv.org/abs/2507.15690)
*Hung Nguyen,Runfa Li,An Le,Truong Nguyen*

Main category: cs.CV

TL;DR: DWTGS improves sparse-view 3D Gaussian Splatting by using wavelet-space losses for better spatial supervision, outperforming Fourier-based methods.


<details>
  <summary>Details</summary>
Motivation: Sparse-view 3DGS struggles with overfitting to high-frequency details in sparse training views, requiring better regularization.

Method: DWTGS leverages wavelet-space losses, supervising low-frequency subbands and enforcing sparsity on high-frequency subbands.

Result: DWTGS consistently outperforms Fourier-based methods, improving generalization and reducing high-frequency hallucinations.

Conclusion: Wavelet-based frequency regularization (DWTGS) is more effective than Fourier-based methods for sparse-view 3DGS.

Abstract: Sparse-view 3D Gaussian Splatting (3DGS) presents significant challenges in
reconstructing high-quality novel views, as it often overfits to the
widely-varying high-frequency (HF) details of the sparse training views. While
frequency regularization can be a promising approach, its typical reliance on
Fourier transforms causes difficult parameter tuning and biases towards
detrimental HF learning. We propose DWTGS, a framework that rethinks frequency
regularization by leveraging wavelet-space losses that provide additional
spatial supervision. Specifically, we supervise only the low-frequency (LF) LL
subbands at multiple DWT levels, while enforcing sparsity on the HF HH subband
in a self-supervised manner. Experiments across benchmarks show that DWTGS
consistently outperforms Fourier-based counterparts, as this LF-centric
strategy improves generalization and reduces HF hallucinations.

</details>


### [128] [Efficient Face Image Quality Assessment via Self-training and Knowledge Distillation](https://arxiv.org/abs/2507.15709)
*Wei Sun,Weixia Zhang,Linhan Cao,Jun Jia,Xiangyang Zhu,Dandan Zhu,Xiongkuo Min,Guangtao Zhai*

Main category: cs.CV

TL;DR: A two-stage method for efficient face image quality assessment (FIQA) using teacher-student distillation and self-training to reduce computational complexity while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: To address the high computational complexity of FIQA algorithms, hindering scalability and real-world deployment.

Method: Two-stage approach: (1) Train a teacher model using labeled data and self-training with pseudo-labels, (2) Distill a lightweight student model using labeled and pseudo-labeled data.

Result: Student model achieves comparable performance to the teacher model with low computational overhead and won ICCV 2025 VQualA FIQA Challenge.

Conclusion: The proposed method efficiently balances performance and computational cost, making FIQA practical for real-world applications.

Abstract: Face image quality assessment (FIQA) is essential for various face-related
applications. Although FIQA has been extensively studied and achieved
significant progress, the computational complexity of FIQA algorithms remains a
key concern for ensuring scalability and practical deployment in real-world
systems. In this paper, we aim to develop a computationally efficient FIQA
method that can be easily deployed in real-world applications. Specifically,
our method consists of two stages: training a powerful teacher model and
distilling a lightweight student model from it. To build a strong teacher
model, we adopt a self-training strategy to improve its capacity. We first
train the teacher model using labeled face images, then use it to generate
pseudo-labels for a set of unlabeled images. These pseudo-labeled samples are
used in two ways: (1) to distill knowledge into the student model, and (2) to
combine with the original labeled images to further enhance the teacher model
through self-training. The enhanced teacher model is used to further
pseudo-label another set of unlabeled images for distilling the student models.
The student model is trained using a combination of labeled images,
pseudo-labeled images from the original teacher model, and pseudo-labeled
images from the enhanced teacher model. Experimental results demonstrate that
our student model achieves comparable performance to the teacher model with an
extremely low computational overhead. Moreover, our method achieved first place
in the ICCV 2025 VQualA FIQA Challenge. The code is available at
https://github.com/sunwei925/Efficient-FIQA.git.

</details>


### [129] [A Practical Investigation of Spatially-Controlled Image Generation with Transformers](https://arxiv.org/abs/2507.15724)
*Guoxuan Xia,Harleen Hanspal,Petru-Daniel Tudosiu,Shifeng Zhang,Sarah Parisot*

Main category: cs.CV

TL;DR: The paper provides a detailed comparison of transformer-based systems for spatially-controlled image generation, clarifying literature gaps and offering practical insights.


<details>
  <summary>Details</summary>
Motivation: To address the lack of detailed and fair scientific comparison in spatially-controlled image generation, and to clarify the literature for practitioners.

Method: Controlled experiments on ImageNet across diffusion-based, flow-based, and autoregressive models, focusing on control token prefilling, sampling time enhancements, and adapter-based approaches.

Result: Control token prefilling is a strong baseline; classifier-free guidance and softmax truncation improve control-generation consistency; adapter-based approaches mitigate forgetting but underperform in consistency.

Conclusion: The study offers clear takeaways for developing transformer-based spatially-controlled generation systems, bridging knowledge gaps and improving understanding of existing methods.

Abstract: Enabling image generation models to be spatially controlled is an important
area of research, empowering users to better generate images according to their
own fine-grained specifications via e.g. edge maps, poses. Although this task
has seen impressive improvements in recent times, a focus on rapidly producing
stronger models has come at the cost of detailed and fair scientific
comparison. Differing training data, model architectures and generation
paradigms make it difficult to disentangle the factors contributing to
performance. Meanwhile, the motivations and nuances of certain approaches
become lost in the literature. In this work, we aim to provide clear takeaways
across generation paradigms for practitioners wishing to develop
transformer-based systems for spatially-controlled generation, clarifying the
literature and addressing knowledge gaps. We perform controlled experiments on
ImageNet across diffusion-based/flow-based and autoregressive (AR) models.
First, we establish control token prefilling as a simple, general and
performant baseline approach for transformers. We then investigate previously
underexplored sampling time enhancements, showing that extending
classifier-free guidance to control, as well as softmax truncation, have a
strong impact on control-generation consistency. Finally, we re-clarify the
motivation of adapter-based approaches, demonstrating that they mitigate
"forgetting" and maintain generation quality when trained on limited downstream
data, but underperform full training in terms of generation-control
consistency. Code will be released upon publication.

</details>


### [130] [TokensGen: Harnessing Condensed Tokens for Long Video Generation](https://arxiv.org/abs/2507.15728)
*Wenqi Ouyang,Zeqi Xiao,Danni Yang,Yifan Zhou,Shuai Yang,Lei Yang,Jianlou Si,Xingang Pan*

Main category: cs.CV

TL;DR: TokensGen is a two-stage framework using condensed tokens for consistent long video generation, addressing memory and consistency issues.


<details>
  <summary>Details</summary>
Motivation: Overcoming memory bottlenecks and long-term inconsistency in diffusion-based long video generation.

Method: Decomposes generation into three tasks: inner-clip control, long-term consistency, and smooth transitions. Uses To2V for short clips and T2To for token generation, with adaptive FIFO-Diffusion for inference.

Result: Enhances temporal and content coherence without excessive computational cost.

Conclusion: Provides a scalable, modular solution for long video generation, useful for storytelling and simulations.

Abstract: Generating consistent long videos is a complex challenge: while
diffusion-based generative models generate visually impressive short clips,
extending them to longer durations often leads to memory bottlenecks and
long-term inconsistency. In this paper, we propose TokensGen, a novel two-stage
framework that leverages condensed tokens to address these issues. Our method
decomposes long video generation into three core tasks: (1) inner-clip semantic
control, (2) long-term consistency control, and (3) inter-clip smooth
transition. First, we train To2V (Token-to-Video), a short video diffusion
model guided by text and video tokens, with a Video Tokenizer that condenses
short clips into semantically rich tokens. Second, we introduce T2To
(Text-to-Token), a video token diffusion transformer that generates all tokens
at once, ensuring global consistency across clips. Finally, during inference,
an adaptive FIFO-Diffusion strategy seamlessly connects adjacent clips,
reducing boundary artifacts and enhancing smooth transitions. Experimental
results demonstrate that our approach significantly enhances long-term temporal
and content coherence without incurring prohibitive computational overhead. By
leveraging condensed tokens and pre-trained short video models, our method
provides a scalable, modular solution for long video generation, opening new
possibilities for storytelling, cinematic production, and immersive
simulations. Please see our project page at
https://vicky0522.github.io/tokensgen-webpage/ .

</details>


### [131] [Appearance Harmonization via Bilateral Grid Prediction with Transformers for 3DGS](https://arxiv.org/abs/2507.15748)
*Jisu Shin,Richard Shaw,Seunghyun Shin,Anton Pelykh,Zhensong Zhang,Hae-Gon Jeon,Eduardo Perez-Pellitero*

Main category: cs.CV

TL;DR: A transformer-based method predicts bilateral grids to correct photometric inconsistencies in multi-view scenes, improving 3D reconstruction quality without scene-specific retraining.


<details>
  <summary>Details</summary>
Motivation: Photometric inconsistencies from camera pipelines degrade novel view synthesis. Existing methods increase complexity and slow training.

Method: Uses transformer-based prediction of spatially adaptive bilateral grids for photometric correction, integrated into 3D Gaussian Splatting.

Result: Outperforms or matches scene-specific methods in fidelity and convergence speed.

Conclusion: Proposed method enables robust cross-scene generalization and efficient training.

Abstract: Modern camera pipelines apply extensive on-device processing, such as
exposure adjustment, white balance, and color correction, which, while
beneficial individually, often introduce photometric inconsistencies across
views. These appearance variations violate multi-view consistency and degrade
the quality of novel view synthesis. Joint optimization of scene
representations and per-image appearance embeddings has been proposed to
address this issue, but at the cost of increased computational complexity and
slower training. In this work, we propose a transformer-based method that
predicts spatially adaptive bilateral grids to correct photometric variations
in a multi-view consistent manner, enabling robust cross-scene generalization
without the need for scene-specific retraining. By incorporating the learned
grids into the 3D Gaussian Splatting pipeline, we improve reconstruction
quality while maintaining high training efficiency. Extensive experiments show
that our approach outperforms or matches existing scene-specific optimization
methods in reconstruction fidelity and convergence speed.

</details>


### [132] [Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization](https://arxiv.org/abs/2507.15765)
*Feng-Qi Cui,Anyang Tong,Jinyang Huang,Jie Zhang,Dan Guo,Zhi Liu,Meng Wang*

Main category: cs.CV

TL;DR: A novel framework (HDF) for Dynamic Facial Expression Recognition addresses performance degradation due to sample heterogeneity, using two modules (DAM and DSM) for improved time-frequency modeling and optimization balance.


<details>
  <summary>Details</summary>
Motivation: Existing methods for DFER suffer from performance degradation under multi-source data and individual expression variability.

Method: Proposes HDF with two modules: Time-Frequency Distributional Attention Module (DAM) for temporal and frequency robustness, and Distribution-aware Scaling Module (DSM) for adaptive loss balancing.

Result: HDF significantly improves recognition accuracy and robustness on DFEW and FERV39k datasets, achieving superior WAR and UAR.

Conclusion: HDF enhances DFER performance by addressing heterogeneity and imbalance, with strong generalization across diverse scenarios.

Abstract: Dynamic Facial Expression Recognition (DFER) plays a critical role in
affective computing and human-computer interaction. Although existing methods
achieve comparable performance, they inevitably suffer from performance
degradation under sample heterogeneity caused by multi-source data and
individual expression variability. To address these challenges, we propose a
novel framework, called Heterogeneity-aware Distributional Framework (HDF), and
design two plug-and-play modules to enhance time-frequency modeling and
mitigate optimization imbalance caused by hard samples. Specifically, the
Time-Frequency Distributional Attention Module (DAM) captures both temporal
consistency and frequency robustness through a dual-branch attention design,
improving tolerance to sequence inconsistency and visual style shifts. Then,
based on gradient sensitivity and information bottleneck principles, an
adaptive optimization module Distribution-aware Scaling Module (DSM) is
introduced to dynamically balance classification and contrastive losses,
enabling more stable and discriminative representation learning. Extensive
experiments on two widely used datasets, DFEW and FERV39k, demonstrate that HDF
significantly improves both recognition accuracy and robustness. Our method
achieves superior weighted average recall (WAR) and unweighted average recall
(UAR) while maintaining strong generalization across diverse and imbalanced
scenarios. Codes are released at https://github.com/QIcita/HDF_DFER.

</details>


### [133] [Label tree semantic losses for rich multi-class medical image segmentation](https://arxiv.org/abs/2507.15777)
*Junwen Wang,Oscar MacCormac,William Rochford,Aaron Kujawa,Jonathan Shapey,Tom Vercauteren*

Main category: cs.CV

TL;DR: The paper proposes tree-based semantic loss functions for medical image segmentation, leveraging hierarchical label organization to improve accuracy, especially for subtle class distinctions. It achieves state-of-the-art results in brain MRI and neurosurgical HSI tasks.


<details>
  <summary>Details</summary>
Motivation: Current methods penalize all segmentation errors equally, ignoring inter-class semantics, which is problematic for rich, hierarchical labels.

Method: Two tree-based semantic loss functions are introduced, integrated with sparse annotation training for broader applicability.

Result: State-of-the-art performance is achieved in whole brain parcellation (MRI) and neurosurgical hyperspectral imaging (HSI) tasks.

Conclusion: The proposed hierarchical loss functions enhance segmentation accuracy, particularly for complex label spaces, demonstrating effectiveness in medical imaging.

Abstract: Rich and accurate medical image segmentation is poised to underpin the next
generation of AI-defined clinical practice by delineating critical anatomy for
pre-operative planning, guiding real-time intra-operative navigation, and
supporting precise post-operative assessment. However, commonly used learning
methods for medical and surgical imaging segmentation tasks penalise all errors
equivalently and thus fail to exploit any inter-class semantics in the labels
space. This becomes particularly problematic as the cardinality and richness of
labels increases to include subtly different classes. In this work, we propose
two tree-based semantic loss functions which take advantage of a hierarchical
organisation of the labels. We further incorporate our losses in a recently
proposed approach for training with sparse, background-free annotations to
extend the applicability of our proposed losses. Extensive experiments are
reported on two medical and surgical image segmentation tasks, namely head MRI
for whole brain parcellation (WBP) with full supervision and neurosurgical
hyperspectral imaging (HSI) for scene understanding with sparse annotations.
Results demonstrate that our proposed method reaches state-of-the-art
performance in both cases.

</details>


### [134] [Regularized Low-Rank Adaptation for Few-Shot Organ Segmentation](https://arxiv.org/abs/2507.15793)
*Ghassen Baklouti,Julio Silva-Rodríguez,Jose Dolz,Houda Bahig,Ismail Ben Ayed*

Main category: cs.CV

TL;DR: A novel PEFT method for medical image segmentation dynamically adjusts rank during adaptation, outperforming standard LoRA and other PEFT methods.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of selecting a fixed rank in LoRA for medical imaging tasks by introducing dynamic rank adjustment.

Method: Uses l_1 sparsity regularizer on singular value decomposition of weight matrices, optimized with a proximal optimizer to find task-adapted ranks automatically.

Result: Significant performance improvements in few-shot fine-tuning, especially for base and novel organ segmentation tasks.

Conclusion: The method is efficient, robust against suboptimal rank initialization, and publicly available for use.

Abstract: Parameter-efficient fine-tuning (PEFT) of pre-trained foundation models is
increasingly attracting interest in medical imaging due to its effectiveness
and computational efficiency. Among these methods, Low-Rank Adaptation (LoRA)
is a notable approach based on the assumption that the adaptation inherently
occurs in a low-dimensional subspace. While it has shown good performance, its
implementation requires a fixed and unalterable rank, which might be
challenging to select given the unique complexities and requirements of each
medical imaging downstream task. Inspired by advancements in natural image
processing, we introduce a novel approach for medical image segmentation that
dynamically adjusts the intrinsic rank during adaptation. Viewing the low-rank
representation of the trainable weight matrices as a singular value
decomposition, we introduce an l_1 sparsity regularizer to the loss function,
and tackle it with a proximal optimizer. The regularizer could be viewed as a
penalty on the decomposition rank. Hence, its minimization enables to find
task-adapted ranks automatically. Our method is evaluated in a realistic
few-shot fine-tuning setting, where we compare it first to the standard LoRA
and then to several other PEFT methods across two distinguishable tasks: base
organs and novel organs. Our extensive experiments demonstrate the significant
performance improvements driven by our method, highlighting its efficiency and
robustness against suboptimal rank initialization. Our code is publicly
available: https://github.com/ghassenbaklouti/ARENA

</details>


### [135] [Exploring Superposition and Interference in State-of-the-Art Low-Parameter Vision Models](https://arxiv.org/abs/2507.15798)
*Lilian Hollard,Lucas Mohimont,Nathalie Gaveau,Luiz-Angelo Steffenel*

Main category: cs.CV

TL;DR: The paper explores low-parameter deep neural networks for computer vision, focusing on bottleneck architectures and superlinear activation functions. It addresses interference in feature maps, suggesting limiting it improves scaling and accuracy in small networks (<1.5M parameters). A new architecture, NoDepth Bottleneck, is proposed, showing robust performance on ImageNet.


<details>
  <summary>Details</summary>
Motivation: To enhance the performance and scalability of low-parameter deep neural networks by addressing interference in feature maps, a common issue in bottleneck architectures.

Method: Examines bottleneck architectures and superlinear activation functions, identifies design elements to reduce interference, and proposes the NoDepth Bottleneck architecture.

Result: Limiting interference improves scaling and accuracy in small networks. The NoDepth Bottleneck demonstrates robust performance on ImageNet.

Conclusion: The findings contribute to more efficient and scalable neural networks for low-parameter ranges and deepen understanding of bottlenecks in computer vision.

Abstract: The paper investigates the performance of state-of-the-art low-parameter deep
neural networks for computer vision, focusing on bottleneck architectures and
their behavior using superlinear activation functions. We address interference
in feature maps, a phenomenon associated with superposition, where neurons
simultaneously encode multiple characteristics. Our research suggests that
limiting interference can enhance scaling and accuracy in very low-scaled
networks (under 1.5M parameters). We identify key design elements that reduce
interference by examining various bottleneck architectures, leading to a more
efficient neural network. Consequently, we propose a proof-of-concept
architecture named NoDepth Bottleneck built on mechanistic insights from our
experiments, demonstrating robust scaling accuracy on the ImageNet dataset.
These findings contribute to more efficient and scalable neural networks for
the low-parameter range and advance the understanding of bottlenecks in
computer vision. https://caiac.pubpub.org/pub/3dh6rsel

</details>


### [136] [ConformalSAM: Unlocking the Potential of Foundational Segmentation Models in Semi-Supervised Semantic Segmentation with Conformal Prediction](https://arxiv.org/abs/2507.15803)
*Danhui Chen,Ziquan Liu,Chuxi Yang,Dan Wang,Yan Yan,Yi Xu,Xiangyang Ji*

Main category: cs.CV

TL;DR: ConformalSAM leverages a foundational segmentation model (SEEM) to address label scarcity in semantic segmentation by calibrating and filtering unreliable labels, outperforming recent SSSS methods.


<details>
  <summary>Details</summary>
Motivation: Pixel-level vision tasks require costly annotated data. Semi-supervised semantic segmentation (SSSS) and foundational models like SEEM offer potential solutions, but their reliability as annotators needs improvement.

Method: Proposes ConformalSAM, which calibrates SEEM using labeled target data, filters unreliable pixel labels via conformal prediction, and employs self-reliance training to avoid overfitting.

Result: ConformalSAM outperforms recent SSSS methods on three benchmarks and enhances other methods as a plug-in.

Conclusion: ConformalSAM effectively leverages foundational models for SSSS, improving performance and reliability.

Abstract: Pixel-level vision tasks, such as semantic segmentation, require extensive
and high-quality annotated data, which is costly to obtain. Semi-supervised
semantic segmentation (SSSS) has emerged as a solution to alleviate the
labeling burden by leveraging both labeled and unlabeled data through
self-training techniques. Meanwhile, the advent of foundational segmentation
models pre-trained on massive data, has shown the potential to generalize
across domains effectively. This work explores whether a foundational
segmentation model can address label scarcity in the pixel-level vision task as
an annotator for unlabeled images. Specifically, we investigate the efficacy of
using SEEM, a Segment Anything Model (SAM) variant fine-tuned for textual
input, to generate predictive masks for unlabeled data. To address the
shortcomings of using SEEM-generated masks as supervision, we propose
ConformalSAM, a novel SSSS framework which first calibrates the foundation
model using the target domain's labeled data and then filters out unreliable
pixel labels of unlabeled data so that only high-confidence labels are used as
supervision. By leveraging conformal prediction (CP) to adapt foundation models
to target data through uncertainty calibration, ConformalSAM exploits the
strong capability of the foundational segmentation model reliably which
benefits the early-stage learning, while a subsequent self-reliance training
strategy mitigates overfitting to SEEM-generated masks in the later training
stage. Our experiment demonstrates that, on three standard benchmarks of SSSS,
ConformalSAM achieves superior performance compared to recent SSSS methods and
helps boost the performance of those methods as a plug-in.

</details>


### [137] [True Multimodal In-Context Learning Needs Attention to the Visual Context](https://arxiv.org/abs/2507.15807)
*Shuo Chen,Jianzhe Liu,Zhen Han,Yan Xia,Daniel Cremers,Philip Torr,Volker Tresp,Jindong Gu*

Main category: cs.CV

TL;DR: Current MLLMs struggle with visual information in MICL, relying too much on text. The paper introduces DARA for better attention to visuals and TrueMICL dataset for evaluation, improving multimodal learning.


<details>
  <summary>Details</summary>
Motivation: Current MLLMs neglect visual cues in MICL, limiting practical utility. The paper aims to enhance and reliably evaluate MICL performance.

Method: Proposes Dynamic Attention Reallocation (DARA) for balanced visual-textual attention and introduces the TrueMICL dataset for evaluation.

Result: DARA and TrueMICL significantly improve multimodal in-context learning capabilities.

Conclusion: The holistic solution enhances true multimodal learning, addressing current MLLM limitations.

Abstract: Multimodal Large Language Models (MLLMs), built on powerful language
backbones, have enabled Multimodal In-Context Learning (MICL)-adapting to new
tasks from a few multimodal demonstrations consisting of images, questions, and
answers. Despite showing noticeable improvement on standard vision-language
datasets, current MLLMs struggle to leverage visual information in the
demonstrations. Specifically, they tend to neglect visual cues and over-rely on
textual patterns, leading to mere text imitation rather than genuine multimodal
adaptation. This behavior makes MICL still unimodal and largely restricts its
practical utility. More importantly, this limitation is often concealed by the
improved performance on tasks that do not require understanding the visual
context. As a result, how to effectively enhance MICL ability and reliably
evaluate the MICL performance remains underexplored. To address these issues,
we first introduce Dynamic Attention Reallocation (DARA), an efficient
fine-tuning strategy that encourages models to attend to the visual context by
rebalancing attention across visual and textual tokens. In addition, we present
TrueMICL, an MICL-dedicated dataset with both support and test sets that
explicitly requires the integration of multimodal information-particularly
visual content-for correct task completion. Extensive experiments demonstrate
the effectiveness of our holistic solution, showcasing substantial improvements
in the true multimodal in-context learning capabilities. Code and datasets are
available at https://chenxshuo.github.io/true-micl-colm .

</details>


### [138] [Diffusion models for multivariate subsurface generation and efficient probabilistic inversion](https://arxiv.org/abs/2507.15809)
*Roberto Miele,Niklas Linde*

Main category: cs.CV

TL;DR: Diffusion models improve multivariate subsurface modeling and probabilistic inversion, outperforming VAEs and GANs. Proposed corrections to Diffusion Posterior Sampling enhance robustness and reduce costs.


<details>
  <summary>Details</summary>
Motivation: To address limitations of existing methods (VAEs, GANs) in multivariate subsurface modeling and probabilistic inversion.

Method: Proposes corrections to Diffusion Posterior Sampling, including a noise-contamination-aware likelihood approximation. Evaluated on geological data with facies and acoustic impedance.

Result: Improved statistical robustness, better posterior sampling, and lower computational costs compared to original methods.

Conclusion: Diffusion models with proposed corrections offer efficient and robust solutions for subsurface modeling and inversion, handling both hard and indirect data.

Abstract: Diffusion models offer stable training and state-of-the-art performance for
deep generative modeling tasks. Here, we consider their use in the context of
multivariate subsurface modeling and probabilistic inversion. We first
demonstrate that diffusion models enhance multivariate modeling capabilities
compared to variational autoencoders and generative adversarial networks. In
diffusion modeling, the generative process involves a comparatively large
number of time steps with update rules that can be modified to account for
conditioning data. We propose different corrections to the popular Diffusion
Posterior Sampling approach by Chung et al. (2023). In particular, we introduce
a likelihood approximation accounting for the noise-contamination that is
inherent in diffusion modeling. We assess performance in a multivariate
geological scenario involving facies and correlated acoustic impedance.
Conditional modeling is demonstrated using both local hard data (well logs) and
nonlinear geophysics (fullstack seismic data). Our tests show significantly
improved statistical robustness, enhanced sampling of the posterior probability
density function and reduced computational costs, compared to the original
approach. The method can be used with both hard and indirect conditioning data,
individually or simultaneously. As the inversion is included within the
diffusion process, it is faster than other methods requiring an outer-loop
around the generative model, such as Markov chain Monte Carlo.

</details>


### [139] [Can Your Model Separate Yolks with a Water Bottle? Benchmarking Physical Commonsense Understanding in Video Generation Models](https://arxiv.org/abs/2507.15824)
*Enes Sanli,Baris Sarper Tezcan,Aykut Erdem,Erkut Erdem*

Main category: cs.CV

TL;DR: PhysVidBench is a benchmark to evaluate physical commonsense in text-to-video models, using 383 prompts and a three-stage evaluation pipeline.


<details>
  <summary>Details</summary>
Motivation: Current T2V models lack physical commonsense, producing unrealistic outputs. PhysVidBench addresses this gap.

Method: The benchmark uses prompts, video generation, and a three-stage evaluation (physics questions, captioning, and language model answers).

Result: PhysVidBench provides a structured framework to assess physical plausibility in T2V models.

Conclusion: The benchmark highlights overlooked areas like tool use and material properties, improving T2V evaluation.

Abstract: Recent progress in text-to-video (T2V) generation has enabled the synthesis
of visually compelling and temporally coherent videos from natural language.
However, these models often fall short in basic physical commonsense, producing
outputs that violate intuitive expectations around causality, object behavior,
and tool use. Addressing this gap, we present PhysVidBench, a benchmark
designed to evaluate the physical reasoning capabilities of T2V systems. The
benchmark includes 383 carefully curated prompts, emphasizing tool use,
material properties, and procedural interactions, and domains where physical
plausibility is crucial. For each prompt, we generate videos using diverse
state-of-the-art models and adopt a three-stage evaluation pipeline: (1)
formulate grounded physics questions from the prompt, (2) caption the generated
video with a vision-language model, and (3) task a language model to answer
several physics-involved questions using only the caption. This indirect
strategy circumvents common hallucination issues in direct video-based
evaluation. By highlighting affordances and tool-mediated actions, areas
overlooked in current T2V evaluations, PhysVidBench provides a structured,
interpretable framework for assessing physical commonsense in generative video
models.

</details>


### [140] [SeC: Advancing Complex Video Object Segmentation via Progressive Concept Construction](https://arxiv.org/abs/2507.15852)
*Zhixiong Zhang,Shuangrui Ding,Xiaoyi Dong,Songxin He,Jianfan Lin,Junsong Tang,Yuhang Zang,Yuhang Cao,Dahua Lin,Jiaqi Wang*

Main category: cs.CV

TL;DR: SeC introduces a concept-driven framework for Video Object Segmentation (VOS), leveraging Large Vision-Language Models (LVLMs) for robust object-centric representations, outperforming SAM 2.1 by 11.8 points on the new SeCVOS benchmark.


<details>
  <summary>Details</summary>
Motivation: Current VOS methods rely on appearance matching, lacking human-like conceptual understanding, which limits performance in complex scenarios.

Method: SeC uses LVLMs to build high-level object-centric representations and dynamically balances semantic reasoning with feature matching.

Result: SeC achieves an 11.8-point improvement over SAM 2.1 on the SeCVOS benchmark.

Conclusion: SeC sets a new state-of-the-art in concept-aware VOS, demonstrating the value of integrating conceptual reasoning with feature matching.

Abstract: Video Object Segmentation (VOS) is a core task in computer vision, requiring
models to track and segment target objects across video frames. Despite notable
advances with recent efforts, current techniques still lag behind human
capabilities in handling drastic visual variations, occlusions, and complex
scene changes. This limitation arises from their reliance on appearance
matching, neglecting the human-like conceptual understanding of objects that
enables robust identification across temporal dynamics. Motivated by this gap,
we propose Segment Concept (SeC), a concept-driven segmentation framework that
shifts from conventional feature matching to the progressive construction and
utilization of high-level, object-centric representations. SeC employs Large
Vision-Language Models (LVLMs) to integrate visual cues across diverse frames,
constructing robust conceptual priors. During inference, SeC forms a
comprehensive semantic representation of the target based on processed frames,
realizing robust segmentation of follow-up frames. Furthermore, SeC adaptively
balances LVLM-based semantic reasoning with enhanced feature matching,
dynamically adjusting computational efforts based on scene complexity. To
rigorously assess VOS methods in scenarios demanding high-level conceptual
reasoning and robust semantic understanding, we introduce the Semantic Complex
Scenarios Video Object Segmentation benchmark (SeCVOS). SeCVOS comprises 160
manually annotated multi-scenario videos designed to challenge models with
substantial appearance variations and dynamic scene transformations. In
particular, SeC achieves an 11.8-point improvement over SAM 2.1 on SeCVOS,
establishing a new state-of-the-art in concept-aware video object segmentation.

</details>


### [141] [Latent Denoising Makes Good Visual Tokenizers](https://arxiv.org/abs/2507.15856)
*Jiawei Yang,Tianhong Li,Lijie Fan,Yonglong Tian,Yue Wang*

Main category: cs.CV

TL;DR: The paper proposes aligning tokenizer embeddings with the denoising objective to improve generative modeling, introducing the Latent Denoising Tokenizer (l-DeTok), which outperforms standard tokenizers.


<details>
  <summary>Details</summary>
Motivation: Modern generative models share a denoising objective, and aligning tokenizer embeddings with this could enhance their effectiveness.

Method: Introduces l-DeTok, a tokenizer trained to reconstruct clean images from corrupted latent embeddings using interpolative noise and random masking.

Result: l-DeTok consistently outperforms standard tokenizers across six generative models on ImageNet 256x256.

Conclusion: Denoising is a fundamental design principle for tokenizers, offering new perspectives for future tokenizer development.

Abstract: Despite their fundamental role, it remains unclear what properties could make
visual tokenizers more effective for generative modeling. We observe that
modern generative models share a conceptually similar training objective --
reconstructing clean signals from corrupted inputs such as Gaussian noise or
masking -- a process we term denoising. Motivated by this insight, we propose
aligning tokenizer embeddings directly with the downstream denoising objective,
encouraging latent embeddings to be more easily reconstructed even when heavily
corrupted. To achieve this, we introduce the Latent Denoising Tokenizer
(l-DeTok), a simple yet effective tokenizer trained to reconstruct clean images
from latent embeddings corrupted by interpolative noise and random masking.
Extensive experiments on ImageNet 256x256 demonstrate that our tokenizer
consistently outperforms standard tokenizers across six representative
generative models. Our findings highlight denoising as a fundamental design
principle for tokenizer development, and we hope it could motivate new
perspectives for future tokenizer design.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [142] [The Free Will Equation: Quantum Field Analogies for AGI](https://arxiv.org/abs/2507.14154)
*Rahul Kabali*

Main category: cs.AI

TL;DR: The paper introduces the Free Will Equation, a framework inspired by quantum field theory to add adaptive stochasticity to AGI decision-making, enhancing creativity and adaptability.


<details>
  <summary>Details</summary>
Motivation: Human-like intelligence involves adaptive spontaneity ('free will'), which is crucial for creativity and robust problem-solving. Current AGI lacks this trait.

Method: The Free Will Equation treats an AI's cognitive state as a superposition of actions, collapsing probabilistically like quantum wavefunctions, with added intrinsic motivation.

Result: Experiments in a non-stationary multi-armed bandit environment show higher rewards and policy diversity compared to baselines.

Conclusion: The framework successfully introduces controlled stochasticity, improving AGI adaptability and creativity.

Abstract: Artificial General Intelligence (AGI) research traditionally focuses on
algorithms that optimize for specific goals under deterministic rules. Yet,
human-like intelligence exhibits adaptive spontaneity - an ability to make
unexpected choices or free decisions not strictly dictated by past data or
immediate reward. This trait, often dubbed "free will" in a loose sense, might
be crucial for creativity, robust adaptation, and avoiding ruts in
problem-solving. This paper proposes a theoretical framework, called the Free
Will Equation, that draws analogies from quantum field theory to endow AGI
agents with a form of adaptive, controlled stochasticity in their
decision-making process. The core idea is to treat an AI agent's cognitive
state as a superposition of potential actions or thoughts, which collapses
probabilistically into a concrete action when a decision is made - much like a
quantum wavefunction collapsing upon measurement. By incorporating mechanisms
analogous to quantum fields, along with intrinsic motivation terms, we aim to
improve an agent's ability to explore novel strategies and adapt to unforeseen
changes. Experiments in a non-stationary multi-armed bandit environment
demonstrate that agents using this framework achieve higher rewards and policy
diversity compared to baseline methods.

</details>


### [143] [DREAMS: Density Functional Theory Based Research Engine for Agentic Materials Simulation](https://arxiv.org/abs/2507.14267)
*Ziqi Wang,Hongshuo Huang,Hancheng Zhao,Changwen Xu,Shang Zhu,Jan Janssen,Venkatasubramanian Viswanathan*

Main category: cs.AI

TL;DR: DREAMS is a multi-agent framework using LLMs to automate DFT simulations, reducing human dependency and achieving expert-level accuracy in materials discovery.


<details>
  <summary>Details</summary>
Motivation: Address challenges in DFT simulations, such as training time, parameter tuning, and error handling, to democratize high-throughput materials discovery.

Method: Hierarchical framework with a central LLM planner and domain-specific agents for structure generation, DFT testing, HPC scheduling, and error handling, using a shared canvas for context.

Result: Achieves <1% error on Sol27LC benchmark, solves CO/Pt(111) adsorption puzzle, and confirms FCC-site preference with Bayesian sampling.

Conclusion: DREAMS achieves L3 automation, reducing human reliance and enabling scalable, high-fidelity materials discovery.

Abstract: Materials discovery relies on high-throughput, high-fidelity simulation
techniques such as Density Functional Theory (DFT), which require years of
training, extensive parameter fine-tuning and systematic error handling. To
address these challenges, we introduce the DFT-based Research Engine for
Agentic Materials Screening (DREAMS), a hierarchical, multi-agent framework for
DFT simulation that combines a central Large Language Model (LLM) planner agent
with domain-specific LLM agents for atomistic structure generation, systematic
DFT convergence testing, High-Performance Computing (HPC) scheduling, and error
handling. In addition, a shared canvas helps the LLM agents to structure their
discussions, preserve context and prevent hallucination. We validate DREAMS
capabilities on the Sol27LC lattice-constant benchmark, achieving average
errors below 1\% compared to the results of human DFT experts. Furthermore, we
apply DREAMS to the long-standing CO/Pt(111) adsorption puzzle, demonstrating
its long-term and complex problem-solving capabilities. The framework again
reproduces expert-level literature adsorption-energy differences. Finally,
DREAMS is employed to quantify functional-driven uncertainties with Bayesian
ensemble sampling, confirming the Face Centered Cubic (FCC)-site preference at
the Generalized Gradient Approximation (GGA) DFT level. In conclusion, DREAMS
approaches L3-level automation - autonomous exploration of a defined design
space - and significantly reduces the reliance on human expertise and
intervention, offering a scalable path toward democratized, high-throughput,
high-fidelity computational materials discovery.

</details>


### [144] [WebGuard: Building a Generalizable Guardrail for Web Agents](https://arxiv.org/abs/2507.14293)
*Boyuan Zheng,Zeyi Liao,Scott Salisbury,Zeyuan Liu,Michael Lin,Qinyuan Zheng,Zifan Wang,Xiang Deng,Dawn Song,Huan Sun,Yu Su*

Main category: cs.AI

TL;DR: WebGuard is a dataset for assessing web agent action risks, revealing LLMs' poor performance in predicting outcomes and highlighting the need for specialized guardrail models.


<details>
  <summary>Details</summary>
Motivation: The rapid development of autonomous web agents powered by LLMs exposes risks of unintended or harmful actions, necessitating safety measures.

Method: WebGuard, a dataset with 4,939 human-annotated actions across 193 websites, uses a three-tier risk schema (SAFE, LOW, HIGH) to evaluate and improve guardrail models.

Result: Frontier LLMs perform poorly (<60% accuracy/recall). Fine-tuned Qwen2.5VL-7B improves accuracy (37% to 80%) and HIGH-risk recall (20% to 76%).

Conclusion: Current guardrail models, despite improvements, lack reliability for high-stakes deployment, requiring near-perfect accuracy and recall.

Abstract: The rapid development of autonomous web agents powered by Large Language
Models (LLMs), while greatly elevating efficiency, exposes the frontier risk of
taking unintended or harmful actions. This situation underscores an urgent need
for effective safety measures, akin to access controls for human users. To
address this critical challenge, we introduce WebGuard, the first comprehensive
dataset designed to support the assessment of web agent action risks and
facilitate the development of guardrails for real-world online environments. In
doing so, WebGuard specifically focuses on predicting the outcome of
state-changing actions and contains 4,939 human-annotated actions from 193
websites across 22 diverse domains, including often-overlooked long-tail
websites. These actions are categorized using a novel three-tier risk schema:
SAFE, LOW, and HIGH. The dataset includes designated training and test splits
to support evaluation under diverse generalization settings. Our initial
evaluations reveal a concerning deficiency: even frontier LLMs achieve less
than 60% accuracy in predicting action outcomes and less than 60% recall in
lagging HIGH-risk actions, highlighting the risks of deploying
current-generation agents without dedicated safeguards. We therefore
investigate fine-tuning specialized guardrail models using WebGuard. We conduct
comprehensive evaluations across multiple generalization settings and find that
a fine-tuned Qwen2.5VL-7B model yields a substantial improvement in
performance, boosting accuracy from 37% to 80% and HIGH-risk action recall from
20% to 76%. Despite these improvements, the performance still falls short of
the reliability required for high-stakes deployment, where guardrails must
approach near-perfect accuracy and recall.

</details>


### [145] [Manimator: Transforming Research Papers into Visual Explanations](https://arxiv.org/abs/2507.14306)
*Samarth P,Vyoman Jain,Shiva Golugula,Motamarri Sai Sathvik*

Main category: cs.AI

TL;DR: Manimator is an open-source system using LLMs to convert research papers or text into animations via Manim, simplifying complex STEM education.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of understanding dense research papers by automating the creation of dynamic visualizations.

Method: Uses a pipeline with LLMs to interpret text/PDFs into structured scene descriptions and then into executable Manim code.

Result: Enables rapid creation of engaging visual explanations for STEM topics.

Conclusion: Manimator democratizes high-quality educational content creation, aiding STEM learning.

Abstract: Understanding complex scientific and mathematical concepts, particularly
those presented in dense research papers, poses a significant challenge for
learners. Dynamic visualizations can greatly enhance comprehension, but
creating them manually is time-consuming and requires specialized knowledge and
skills. We introduce manimator, an open-source system that leverages Large
Language Models to transform research papers and natural language prompts into
explanatory animations using the Manim engine. Manimator employs a pipeline
where an LLM interprets the input text or research paper PDF to generate a
structured scene description outlining key concepts, mathematical formulas, and
visual elements and another LLM translates this description into executable
Manim Python code. We discuss its potential as an educational tool for rapidly
creating engaging visual explanations for complex STEM topics, democratizing
the creation of high-quality educational content.

</details>


### [146] [Language Models as Ontology Encoders](https://arxiv.org/abs/2507.14334)
*Hui Yang,Jiaoyan Chen,Yuan He,Yongsheng Gao,Ian Horrocks*

Main category: cs.AI

TL;DR: OnT is a new ontology embedding method that combines pretrained language models with hyperbolic geometric modeling to improve knowledge inference while preserving logical structures.


<details>
  <summary>Details</summary>
Motivation: Existing ontology embedding methods either ignore textual information or fail to preserve logical structures, limiting their performance.

Method: OnT tunes a pretrained language model using hyperbolic geometric modeling to incorporate textual labels and preserve logical relationships in Description Logic EL.

Result: OnT outperforms baselines in prediction and inference tasks and shows strong transfer learning abilities in real-world applications like ontology construction.

Conclusion: OnT effectively combines textual and geometric information, offering superior performance and practical utility in ontology embedding.

Abstract: OWL (Web Ontology Language) ontologies which are able to formally represent
complex knowledge and support semantic reasoning have been widely adopted
across various domains such as healthcare and bioinformatics. Recently,
ontology embeddings have gained wide attention due to its potential to infer
plausible new knowledge and approximate complex reasoning. However, existing
methods face notable limitations: geometric model-based embeddings typically
overlook valuable textual information, resulting in suboptimal performance,
while the approaches that incorporate text, which are often based on language
models, fail to preserve the logical structure. In this work, we propose a new
ontology embedding method OnT, which tunes a Pretrained Language Model (PLM)
via geometric modeling in a hyperbolic space for effectively incorporating
textual labels and simultaneously preserving class hierarchies and other
logical relationships of Description Logic EL. Extensive experiments on four
real-world ontologies show that OnT consistently outperforms the baselines
including the state-of-the-art across both tasks of prediction and inference of
axioms. OnT also demonstrates strong potential in real-world applications,
indicated by its robust transfer learning abilities and effectiveness in real
cases of constructing a new ontology from SNOMED CT. Data and code are
available at https://github.com/HuiYang1997/OnT.

</details>


### [147] [ProofCompass: Enhancing Specialized Provers with LLM Guidance](https://arxiv.org/abs/2507.14335)
*Nicolas Wischermann,Claudio Mayrink Verdun,Gabriel Poesia,Francesco Noseda*

Main category: cs.AI

TL;DR: ProofCompass combines LLMs with specialized provers for efficient theorem proving, improving accuracy and reducing computational effort.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on either large general-purpose models or small specialized ones, each with limitations. Training large specialized models is resource-intensive.

Method: ProofCompass uses an LLM to guide specialized provers (e.g., DSP-v1.5) by providing proof strategies and analyzing failures, avoiding additional training.

Result: On miniF2F, ProofCompass outperforms DSP-v1.5 (55.3% vs. 54.9%) with 25x fewer attempts (128 vs. 3200).

Conclusion: The hybrid approach enhances computational efficiency and accuracy in formal theorem proving.

Abstract: Language models have become increasingly powerful tools for formal
mathematical reasoning. However, most existing approaches rely exclusively on
either large general-purpose models or smaller specialized models, each with
distinct limitations, while training specialized large models still requires
significant computational resources. This paper introduces ProofCompass, a
novel hybrid methodology that achieves remarkable computational efficiency by
strategically guiding existing specialized prover methods, such as
DeepSeek-Prover-v1.5-RL (DSP-v1.5) with a Large Language Model (LLM) without
requiring additional model training. The LLM provides natural language proof
strategies and analyzes failed attempts to select intermediate lemmas, enabling
effective problem decomposition. On the miniF2F benchmark, ProofCompass
demonstrates substantial resource efficiency: it outperforms DSP-v1.5 ($54.9\%
\rightarrow 55.3\%$) while using 25x fewer attempts ($3200 \rightarrow 128$).
Our synergistic approach paves the way for simultaneously improving
computational efficiency and accuracy in formal theorem proving.

</details>


### [148] [Adaptive Multi-Agent Reasoning via Automated Workflow Generation](https://arxiv.org/abs/2507.14393)
*Humza Sami,Mubashir ul Islam,Pierre-Emmanuel Gaillardon,Valerio Tenace*

Main category: cs.AI

TL;DR: Nexus Architect, an enhanced multi-agent system, improves reasoning model generalization by autonomously generating tailored workflows and refining prompts, outperforming state-of-the-art models.


<details>
  <summary>Details</summary>
Motivation: Current Large Reasoning Models (LRMs) often fail to generalize to novel problems due to overfitting, relying on memorization rather than genuine reasoning.

Method: Nexus Architect uses automated workflow synthesis and iterative prompt refinement to create tailored reasoning workflows for specific problem classes.

Result: Empirical evaluation shows Nexus Architect outperforms top LRMs, achieving up to a 66% higher pass rate than competitors like Gemini 2.5 Flash Preview.

Conclusion: Nexus Architect addresses LRM limitations by enhancing generalization and performance, proving its superiority over existing models.

Abstract: The rise of Large Reasoning Models (LRMs) promises a significant leap forward
in language model capabilities, aiming to tackle increasingly sophisticated
tasks with unprecedented efficiency and accuracy. However, despite their
impressive performance, recent studies have highlighted how current reasoning
models frequently fail to generalize to novel, unseen problems, often resorting
to memorized solutions rather than genuine inferential reasoning. Such behavior
underscores a critical limitation in modern LRMs, i.e., their tendency toward
overfitting, which in turn results in poor generalization in problem-solving
capabilities.
  In this paper, we introduce Nexus Architect, an enhanced iteration of our
multi-agent system framework, Nexus, equipped with a novel automated workflow
synthesis mechanism. Given a user's prompt and a small set of representative
examples, the Architect autonomously generates a tailored reasoning workflow by
selecting suitable strategies, tool integrations, and adversarial techniques
for a specific problem class. Furthermore, the Architect includes an iterative
prompt refinement mechanism that fine-tunes agents' system prompts to maximize
performance and improve the generalization capabilities of the system.
  We empirically evaluate Nexus Architect by employing an off-the-shelf,
non-reasoning model on a custom dataset of challenging logical questions and
compare its performance against state-of-the-art LRMs. Results show that Nexus
Architect consistently outperforms existing solutions, achieving up to a 66%
increase in pass rate over Gemini 2.5 Flash Preview, nearly 2.5$\times$ against
Claude Sonnet 4 and DeepSeek-R1, and over 3$\times$ w.r.t. Llama 4 Scout.

</details>


### [149] [Fail Fast, or Ask: Mitigating the Deficiencies of Reasoning LLMs with Human-in-the-Loop Systems Engineering](https://arxiv.org/abs/2507.14406)
*Michael J. Zellinger,Matt Thomson*

Main category: cs.AI

TL;DR: The paper proposes a human-in-the-loop system to reduce error rates and latency in reasoning LLMs by deferring uncertain queries to humans and using non-reasoning models for faster processing.


<details>
  <summary>Details</summary>
Motivation: To address the high error rates and latency of reasoning LLMs in risk-sensitive domains, where near-zero error rates are required.

Method: Collaboration between reasoning models and human experts, with uncertainty quantified by reasoning trace length. A non-reasoning model is used to defer queries faster ('Fail Fast, or Ask').

Result: Error rates reduced from 3% to <1% with 7.5% deferral. Latency reduced by ~40% and costs by ~50%, but 'latency drag' limits savings.

Conclusion: Black-box systems engineering can mitigate reasoning LLM deficiencies without accessing model internals.

Abstract: State-of-the-art reasoning LLMs are powerful problem solvers, but they still
occasionally make mistakes. However, adopting AI models in risk-sensitive
domains often requires error rates near 0%. To address this gap, we propose
collaboration between a reasoning model and a human expert who resolves queries
the model cannot confidently answer. We find that quantifying the uncertainty
of a reasoning model through the length of its reasoning trace yields an
effective basis for deferral to a human, e.g., cutting the error rate of Qwen3
235B-A22B on difficult MATH problems from 3% to less than 1% when deferring
7.5% of queries. However, the high latency of reasoning models still makes them
challenging to deploy on use cases with high query volume. To address this
challenge, we explore fronting a reasoning model with a large non-reasoning
model. We call this modified human-in-the-loop system "Fail Fast, or Ask",
since the non-reasoning model may defer difficult queries to the human expert
directly ("failing fast"), without incurring the reasoning model's higher
latency. We show that this approach yields around 40% latency reduction and
about 50% cost savings for DeepSeek R1 while maintaining 90+% area under the
accuracy-rejection curve. However, we observe that latency savings are lower
than expected because of "latency drag", the phenomenon that processing easier
queries with a non-reasoning model pushes the reasoning model's latency
distribution towards longer latencies. Broadly, our results suggest that the
deficiencies of state-of-the-art reasoning models -- nontrivial error rates and
high latency -- can be substantially mitigated through black-box systems
engineering, without requiring access to LLM internals.

</details>


### [150] [Inverse Scaling in Test-Time Compute](https://arxiv.org/abs/2507.14417)
*Aryo Pradipta Gema,Alexander Hägele,Runjin Chen,Andy Arditi,Jacob Goldman-Wetzler,Kit Fraser-Taliente,Henry Sleight,Linda Petrini,Julian Michael,Beatrice Alex,Pasquale Minervini,Yanda Chen,Joe Benton,Ethan Perez*

Main category: cs.AI

TL;DR: Longer reasoning in Large Reasoning Models (LRMs) can reduce accuracy, revealing inverse scaling between compute and performance. Five failure modes are identified, highlighting risks like distraction, overfitting, and spurious correlations.


<details>
  <summary>Details</summary>
Motivation: To investigate how extended reasoning affects LRM performance and identify potential failure modes.

Method: Constructed evaluation tasks across four categories (counting, regression, deduction, AI risks) and analyzed model behavior with increased reasoning length.

Result: Found five failure modes: distraction, overfitting, spurious correlations, focus loss, and amplified concerning behaviors. Inverse scaling observed.

Conclusion: Test-time compute scaling can reinforce problematic reasoning. Diverse reasoning length evaluations are crucial for addressing LRM failures.

Abstract: We construct evaluation tasks where extending the reasoning length of Large
Reasoning Models (LRMs) deteriorates performance, exhibiting an inverse scaling
relationship between test-time compute and accuracy. Our evaluation tasks span
four categories: simple counting tasks with distractors, regression tasks with
spurious features, deduction tasks with constraint tracking, and advanced AI
risks. We identify five distinct failure modes when models reason for longer:
1) Claude models become increasingly distracted by irrelevant information; 2)
OpenAI o-series models resist distractors but overfit to problem framings; 3)
models shift from reasonable priors to spurious correlations; 4) all models
show difficulties in maintaining focus on complex deductive tasks; and 5)
extended reasoning may amplify concerning behaviors, with Claude Sonnet 4
showing increased expressions of self-preservation. These findings suggest that
while test-time compute scaling remains promising for improving model
capabilities, it may inadvertently reinforce problematic reasoning patterns.
Our results demonstrate the importance of evaluating models across diverse
reasoning lengths to identify and address these failure modes in LRMs.

</details>


### [151] [Routine: A Structural Planning Framework for LLM Agent System in Enterprise](https://arxiv.org/abs/2507.14447)
*Guancheng Zeng,Xueyi Chen,Jiawang Hu,Shaohua Qi,Yaxuan Mao,Zhantao Wang,Yifan Nie,Shuang Li,Qiuyang Feng,Pengxu Qiu,Yujia Wang,Wenqiang Han,Linyan Huang,Gang Li,Jingjing Mo,Haowen Hu*

Main category: cs.AI

TL;DR: Routine is a multi-step agent planning framework that improves execution accuracy and stability in enterprise agent systems, significantly boosting performance for models like GPT-4o and Qwen3-14B.


<details>
  <summary>Details</summary>
Motivation: Current agent systems lack domain-specific process knowledge, leading to disorganized plans and poor execution stability. Routine aims to address these challenges.

Method: Routine introduces a structured framework with explicit instructions and seamless parameter passing for multi-step tool-calling tasks. It also involves dataset construction and model fine-tuning.

Result: Routine increased GPT-4o's accuracy from 41.1% to 96.3% and Qwen3-14B's from 32.6% to 83.3%. Fine-tuning further improved Qwen3-14B to 88.2% and 95.5% with distilled data.

Conclusion: Routine effectively enhances agent system stability and adaptability, accelerating deployment in enterprise environments and advancing AI for Process.

Abstract: The deployment of agent systems in an enterprise environment is often
hindered by several challenges: common models lack domain-specific process
knowledge, leading to disorganized plans, missing key tools, and poor execution
stability. To address this, this paper introduces Routine, a multi-step agent
planning framework designed with a clear structure, explicit instructions, and
seamless parameter passing to guide the agent's execution module in performing
multi-step tool-calling tasks with high stability. In evaluations conducted
within a real-world enterprise scenario, Routine significantly increases the
execution accuracy in model tool calls, increasing the performance of GPT-4o
from 41.1% to 96.3%, and Qwen3-14B from 32.6% to 83.3%. We further constructed
a Routine-following training dataset and fine-tuned Qwen3-14B, resulting in an
accuracy increase to 88.2% on scenario-specific evaluations, indicating
improved adherence to execution plans. In addition, we employed Routine-based
distillation to create a scenario-specific, multi-step tool-calling dataset.
Fine-tuning on this distilled dataset raised the model's accuracy to 95.5%,
approaching GPT-4o's performance. These results highlight Routine's
effectiveness in distilling domain-specific tool-usage patterns and enhancing
model adaptability to new scenarios. Our experimental results demonstrate that
Routine provides a practical and accessible approach to building stable agent
workflows, accelerating the deployment and adoption of agent systems in
enterprise environments, and advancing the technical vision of AI for Process.

</details>


### [152] [BioGraphFusion: Graph Knowledge Embedding for Biological Completion and Reasoning](https://arxiv.org/abs/2507.14468)
*Yitong Lin,Jiaying He,Jiahe Chen,Xinnan Zhu,Jianwei Zheng,Tao Bo*

Main category: cs.AI

TL;DR: BioGraphFusion introduces a synergistic framework combining tensor decomposition and LSTM-driven refinement for biomedical KG completion, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the gap in dynamic integration of semantic and structural learning in biomedical KGs for drug discovery and disease understanding.

Method: Uses tensor decomposition for global semantics, LSTM for dynamic relation embedding refinement, query-guided subgraphs, and hybrid scoring.

Result: Superior performance in biomedical tasks and meaningful pathway discovery, demonstrated in a CMM1 case study.

Conclusion: BioGraphFusion effectively bridges semantic and structural learning, enhancing biomedical KG applications.

Abstract: Motivation: Biomedical knowledge graphs (KGs) are crucial for drug discovery
and disease understanding, yet their completion and reasoning are challenging.
Knowledge Embedding (KE) methods capture global semantics but struggle with
dynamic structural integration, while Graph Neural Networks (GNNs) excel
locally but often lack semantic understanding. Even ensemble approaches,
including those leveraging language models, often fail to achieve a deep,
adaptive, and synergistic co-evolution between semantic comprehension and
structural learning. Addressing this critical gap in fostering continuous,
reciprocal refinement between these two aspects in complex biomedical KGs is
paramount.
  Results: We introduce BioGraphFusion, a novel framework for deeply
synergistic semantic and structural learning. BioGraphFusion establishes a
global semantic foundation via tensor decomposition, guiding an LSTM-driven
mechanism to dynamically refine relation embeddings during graph propagation.
This fosters adaptive interplay between semantic understanding and structural
learning, further enhanced by query-guided subgraph construction and a hybrid
scoring mechanism. Experiments across three key biomedical tasks demonstrate
BioGraphFusion's superior performance over state-of-the-art KE, GNN, and
ensemble models. A case study on Cutaneous Malignant Melanoma 1 (CMM1)
highlights its ability to unveil biologically meaningful pathways.
  Availability and Implementation: Source code and all training data are freely
available for download at https://github.com/Y-TARL/BioGraphFusion.
  Contact: zjw@zjut.edu.cn, botao666666@126.com.
  Supplementary information: Supplementary data are available at Bioinformatics
online.

</details>


### [153] [Amico: An Event-Driven Modular Framework for Persistent and Embedded Autonomy](https://arxiv.org/abs/2507.14513)
*Hongyi Yang,Yue Pan,Jiayi Xu,Kelsen Liu*

Main category: cs.AI

TL;DR: Amico is a modular, event-driven framework for autonomous agents, optimized for embedded systems, addressing limitations of cloud reliance and dynamic robustness.


<details>
  <summary>Details</summary>
Motivation: Existing frameworks struggle in real-world or resource-constrained environments due to cloud dependency and lack of robustness.

Method: Amico is written in Rust for safety and performance, supporting reactive, persistent agents via WebAssembly, with abstractions for event handling, state management, and reasoning integration.

Result: Amico provides a unified infrastructure for resilient, interactive agents in low-compute, intermittent connectivity settings.

Conclusion: Amico offers a robust solution for deploying autonomous agents in resource-constrained environments.

Abstract: Recent advances in large language models (LLMs) and autonomous agents have
enabled systems capable of performing complex tasks across domains such as
human-computer interaction, planning, and web navigation. However, many
existing frameworks struggle in real-world or resource-constrained environments
due to their reliance on cloud-based computation, limited robustness in dynamic
contexts, and lack of persistent autonomy and environmental awareness.
  We present Amico, a modular, event-driven framework for building autonomous
agents optimized for embedded systems. Written in Rust for safety and
performance, Amico supports reactive, persistent agents that operate
efficiently across embedded platforms and browser environments via WebAssembly.
It provides clean abstractions for event handling, state management, behavior
execution, and integration with reasoning modules. Amico delivers a unified
infrastructure for constructing resilient, interactive agents suitable for
deployment in settings with limited compute and intermittent connectivity.

</details>


### [154] [What if Othello-Playing Language Models Could See?](https://arxiv.org/abs/2507.14520)
*Xinyi Chen,Yifei Yuan,Jiaang Li,Serge Belongie,Maarten de Rijke,Anders Søgaard*

Main category: cs.AI

TL;DR: Multi-modal training (VISOTHELLO) improves performance and robustness in next-move prediction for Othello, suggesting visual grounding aids structured world inference.


<details>
  <summary>Details</summary>
Motivation: To investigate whether grounding language in visual input improves model performance and robustness in structured, rule-based worlds like Othello.

Method: Introduce VISOTHELLO, a multi-modal model trained on move histories and board images, comparing it to mono-modal baselines and testing robustness to irrelevant perturbations.

Result: Multi-modal training enhances performance and robustness of internal representations.

Conclusion: Grounding language in visual input helps models infer structured world representations.

Abstract: Language models are often said to face a symbol grounding problem. While some
argue that world understanding can emerge from text alone, others suggest
grounded learning is more efficient. We explore this through Othello, where the
board state defines a simplified, rule-based world. Building on prior work, we
introduce VISOTHELLO, a multi-modal model trained on move histories and board
images. Using next-move prediction, we compare it to mono-modal baselines and
test robustness to semantically irrelevant perturbations. We find that
multi-modal training improves both performance and the robustness of internal
representations. These results suggest that grounding language in visual input
helps models infer structured world representations.

</details>


### [155] [Large Language Models Assisting Ontology Evaluation](https://arxiv.org/abs/2507.14552)
*Anna Sofia Lippolis,Mohammad Javad Saeedizade,Robin Keskisärkkä,Aldo Gangemi,Eva Blomqvist,Andrea Giovanni Nuzzolese*

Main category: cs.AI

TL;DR: OE-Assist is a framework for automated and semi-automated ontology evaluation using LLMs, showing performance comparable to average users.


<details>
  <summary>Details</summary>
Motivation: Manual ontology evaluation is costly and error-prone, prompting the need for automated solutions.

Method: OE-Assist leverages LLMs for CQ verification, using a dataset of 1,393 CQs and ontologies.

Result: LLM-based evaluation (o1-preview and o3-mini) matches average user performance.

Conclusion: OE-Assist demonstrates the potential of LLMs in streamlining ontology evaluation.

Abstract: Ontology evaluation through functional requirements, such as testing via
competency question (CQ) verification, is a well-established yet costly,
labour-intensive, and error-prone endeavour, even for ontology engineering
experts. In this work, we introduce OE-Assist, a novel framework designed to
assist ontology evaluation through automated and semi-automated CQ
verification. By presenting and leveraging a dataset of 1,393 CQs paired with
corresponding ontologies and ontology stories, our contributions present, to
our knowledge, the first systematic investigation into large language model
(LLM)-assisted ontology evaluation, and include: (i) evaluating the
effectiveness of a LLM-based approach for automatically performing CQ
verification against a manually created gold standard, and (ii) developing and
assessing an LLM-powered framework to assist CQ verification with Prot\'eg\'e,
by providing suggestions. We found that automated LLM-based evaluation with
o1-preview and o3-mini perform at a similar level to the average user's
performance.

</details>


### [156] [Coordinate Heart System: A Geometric Framework for Emotion Representation](https://arxiv.org/abs/2507.14593)
*Omar Al-Desi*

Main category: cs.AI

TL;DR: The paper introduces the Coordinate Heart System (CHS), a geometric framework for emotion representation in AI, using an eight-emotion model on a unit circle for mathematical computation of complex emotional states. It includes novel algorithms for emotion mixing and stability modeling, validated through case studies.


<details>
  <summary>Details</summary>
Motivation: To address gaps in traditional emotion models by providing a mathematically rigorous framework for representing and computing complex emotional states in AI applications.

Method: Develops an eight-emotion coordinate system on a unit circle, introduces algorithms for emotion mixing and stability modeling, and leverages LLMs for textual cue interpretation.

Result: Demonstrates the system's ability to handle conflicted emotional states and contextual distress, outperforming traditional categorical models.

Conclusion: Establishes a new mathematical foundation for emotion modeling in AI, with proven geometric coverage and computational capabilities.

Abstract: This paper presents the Coordinate Heart System (CHS), a geometric framework
for emotion representation in artificial intelligence applications. We position
eight core emotions as coordinates on a unit circle, enabling mathematical
computation of complex emotional states through coordinate mixing and vector
operations. Our initial five-emotion model revealed significant coverage gaps
in the emotion space, leading to the development of an eight-emotion system
that provides complete geometric coverage with mathematical guarantees. The
framework converts natural language input to emotion coordinates and supports
real-time emotion interpolation through computational algorithms. The system
introduces a re-calibrated stability parameter S in [0,1], which dynamically
integrates emotional load, conflict resolution, and contextual drain factors.
This stability model leverages advanced Large Language Model interpretation of
textual cues and incorporates hybrid temporal tracking mechanisms to provide
nuanced assessment of psychological well-being states. Our key contributions
include: (i) mathematical proof demonstrating why five emotions are
insufficient for complete geometric coverage, (ii) an eight-coordinate system
that eliminates representational blind spots, (iii) novel algorithms for
emotion mixing, conflict resolution, and distance calculation in emotion space,
and (iv) a comprehensive computational framework for AI emotion recognition
with enhanced multi-dimensional stability modeling. Experimental validation
through case studies demonstrates the system's capability to handle emotionally
conflicted states, contextual distress factors, and complex psychological
scenarios that traditional categorical emotion models cannot adequately
represent. This work establishes a new mathematical foundation for emotion
modeling in artificial intelligence systems.

</details>


### [157] [Efficient Story Point Estimation With Comparative Learning](https://arxiv.org/abs/2507.14642)
*Monoshiz Mahbub Khan,Xioayin Xi,Andrew Meneely,Zhe Yu*

Main category: cs.AI

TL;DR: The paper proposes a comparative learning framework for story point estimation in agile development, reducing manual effort by using pairwise comparisons to train ML models.


<details>
  <summary>Details</summary>
Motivation: Manual story point estimation is tedious and labor-intensive. Machine learning can help but requires project-specific data. The goal is to streamline estimation using comparative judgments.

Method: Developers compare pairs of backlog items to indicate effort differences. A model is trained on these comparisons to predict story points.

Result: The model achieved a 0.34 Spearman's rank correlation, comparable to regression models using ground truth data.

Conclusion: Comparative learning is more efficient than regression-based methods, reducing cognitive burden while maintaining accuracy.

Abstract: Story point estimation is an essential part of agile software development.
Story points are unitless, project-specific effort estimates that help
developers plan their sprints. Traditionally, developers estimate story points
collaboratively using planning poker or other manual techniques. While the
initial calibrating of the estimates to each project is helpful, once a team
has converged on a set of precedents, story point estimation can become tedious
and labor-intensive. Machine learning can reduce this burden, but only with
enough context from the historical decisions made by the project team. That is,
state-of-the-art models, such as GPT2SP and FastText-SVM, only make accurate
predictions (within-project) when trained on data from the same project. The
goal of this work is to streamline story point estimation by evaluating a
comparative learning-based framework for calibrating project-specific story
point prediction models. Instead of assigning a specific story point value to
every backlog item, developers are presented with pairs of items, and indicate
which item requires more effort. Using these comparative judgments, a machine
learning model is trained to predict the story point estimates. We empirically
evaluated our technique using data with 23,313 manual estimates in 16 projects.
The model learned from comparative judgments can achieve on average 0.34
Spearman's rank correlation coefficient between its predictions and the ground
truth story points. This is similar to, if not better than, the performance of
a regression model learned from the ground truth story points. Therefore, the
proposed comparative learning approach is more efficient than state-of-the-art
regression-based approaches according to the law of comparative judgments -
providing comparative judgments yields a lower cognitive burden on humans than
providing ratings or categorical labels.

</details>


### [158] [When Autonomy Goes Rogue: Preparing for Risks of Multi-Agent Collusion in Social Systems](https://arxiv.org/abs/2507.14660)
*Qibing Ren,Sitao Xie,Longxuan Wei,Zhenfei Yin,Junchi Yan,Lizhuang Ma,Jing Shao*

Main category: cs.AI

TL;DR: The paper explores risks of AI-driven multi-agent systems (MAS) colluding maliciously, simulating decentralized and centralized coordination in misinformation and fraud scenarios. Decentralized MAS are found more harmful and adaptable than centralized ones, evading traditional interventions.


<details>
  <summary>Details</summary>
Motivation: Concerns about AI-driven groups causing harm, similar to human-coordinated fraud or misinformation, motivate the study of MAS risks, which are underexplored in AI safety research.

Method: A proof-of-concept framework simulates malicious MAS collusion in centralized and decentralized structures, applied to misinformation spread and e-commerce fraud.

Result: Decentralized MAS are more effective and adaptable in executing malicious actions, evading detection even with interventions like content flagging.

Conclusion: The study highlights the need for improved detection and countermeasures against malicious MAS, especially decentralized ones, due to their adaptability and harm potential.

Abstract: Recent large-scale events like election fraud and financial scams have shown
how harmful coordinated efforts by human groups can be. With the rise of
autonomous AI systems, there is growing concern that AI-driven groups could
also cause similar harm. While most AI safety research focuses on individual AI
systems, the risks posed by multi-agent systems (MAS) in complex real-world
situations are still underexplored. In this paper, we introduce a
proof-of-concept to simulate the risks of malicious MAS collusion, using a
flexible framework that supports both centralized and decentralized
coordination structures. We apply this framework to two high-risk fields:
misinformation spread and e-commerce fraud. Our findings show that
decentralized systems are more effective at carrying out malicious actions than
centralized ones. The increased autonomy of decentralized systems allows them
to adapt their strategies and cause more damage. Even when traditional
interventions, like content flagging, are applied, decentralized groups can
adjust their tactics to avoid detection. We present key insights into how these
malicious groups operate and the need for better detection systems and
countermeasures. Code is available at https://github.com/renqibing/RogueAgent.

</details>


### [159] [Configurable multi-agent framework for scalable and realistic testing of llm-based agents](https://arxiv.org/abs/2507.14705)
*Sai Wang,Senthilnathan Subramanian,Mudit Sahni,Praneeth Gone,Lingjie Meng,Xiaochen Wang,Nicolas Ferradas Bertoli,Tingxian Cheng,Jun Xu*

Main category: cs.AI

TL;DR: Neo is a multi-agent framework for automated, realistic evaluation of LLM-based systems, outperforming human testing in throughput and edge-case detection.


<details>
  <summary>Details</summary>
Motivation: Static benchmarks and manual testing are insufficient for evaluating the complex, context-sensitive behavior of LLM agents.

Method: Neo uses a configurable framework with Question Generation and Evaluation Agents, a shared context-hub, and probabilistic state models for diverse, adaptive conversations.

Result: Neo achieved a 3.3% break rate (close to human red-teamers' 5.8%) and 10-12X higher throughput, generating 180 test questions in 45 mins vs. 16h manually.

Conclusion: Neo provides a scalable, model-agnostic foundation for high-fidelity LLM testing, with extensible features for broader behavioral exploration.

Abstract: Large-language-model (LLM) agents exhibit complex, context-sensitive
behaviour that quickly renders static benchmarks and ad-hoc manual testing
obsolete.
  We present Neo, a configurable, multi-agent framework that automates
realistic, multi-turn evaluation of LLM-based systems. Neo couples a Question
Generation Agent and an Evaluation Agent through a shared context-hub, allowing
domain prompts, scenario controls and dynamic feedback to be composed
modularly. Test inputs are sampled from a probabilistic state model spanning
dialogue flow, user intent and emotional tone, enabling diverse, human-like
conversations that adapt after every turn.
  Applied to a production-grade Seller Financial Assistant chatbot, Neo (i)
uncovered edge-case failures across five attack categories with a 3.3% break
rate close to the 5.8% achieved by expert human red-teamers, and (ii) delivered
10-12X higher throughput, generating 180 coherent test questions in around 45
mins versus 16h of human effort. Beyond security probing, Neo's stochastic
policies balanced topic coverage and conversational depth, yielding broader
behavioural exploration than manually crafted scripts.
  Neo therefore lays a foundation for scalable, self-evolving LLM QA: its agent
interfaces, state controller and feedback loops are model-agnostic and
extensible to richer factual-grounding and policy-compliance checks. We release
the framework to facilitate reproducible, high-fidelity testing of emerging
agentic systems.

</details>


### [160] [Automated Safety Evaluations Across 20 Large Language Models: The Aymara LLM Risk and Responsibility Matrix](https://arxiv.org/abs/2507.14719)
*Juan Manuel Contreras*

Main category: cs.AI

TL;DR: Aymara AI is a platform for scalable safety evaluation of LLMs, converting policies into adversarial prompts and scoring responses. It evaluated 20 LLMs across 10 domains, revealing significant performance disparities and context-dependent safety issues.


<details>
  <summary>Details</summary>
Motivation: To address the need for scalable and rigorous safety evaluation of LLMs as they integrate into real-world applications.

Method: Aymara AI transforms natural-language safety policies into adversarial prompts and uses an AI-based rater validated against human judgments to score model responses.

Result: Evaluated 20 LLMs across 10 domains, showing wide performance disparities (scores from 86.2% to 52.4%). Models excelled in Misinformation (95.7%) but struggled in Privacy & Impersonation (24.3%). ANOVA confirmed significant differences across models and domains (p < .05).

Conclusion: LLM safety is inconsistent and context-dependent, highlighting the need for tools like Aymara AI to support responsible AI development and oversight.

Abstract: As large language models (LLMs) become increasingly integrated into
real-world applications, scalable and rigorous safety evaluation is essential.
This paper introduces Aymara AI, a programmatic platform for generating and
administering customized, policy-grounded safety evaluations. Aymara AI
transforms natural-language safety policies into adversarial prompts and scores
model responses using an AI-based rater validated against human judgments. We
demonstrate its capabilities through the Aymara LLM Risk and Responsibility
Matrix, which evaluates 20 commercially available LLMs across 10 real-world
safety domains. Results reveal wide performance disparities, with mean safety
scores ranging from 86.2% to 52.4%. While models performed well in
well-established safety domains such as Misinformation (mean = 95.7%), they
consistently failed in more complex or underspecified domains, notably Privacy
& Impersonation (mean = 24.3%). Analyses of Variance confirmed that safety
scores differed significantly across both models and domains (p < .05). These
findings underscore the inconsistent and context-dependent nature of LLM safety
and highlight the need for scalable, customizable tools like Aymara AI to
support responsible AI development and oversight.

</details>


### [161] [Towards AI Urban Planner in the Age of GenAI, LLMs, and Agentic AI](https://arxiv.org/abs/2507.14730)
*Yanjie Fu*

Main category: cs.AI

TL;DR: The paper explores the potential of generative AI in urban planning, identifying gaps and proposing future research directions.


<details>
  <summary>Details</summary>
Motivation: The convergence of AI and urban planning offers opportunities for AI-driven urban design, but current research lacks integration of urban theory, multi-resolution analysis, data-augmented knowledge, and real-world interaction.

Method: The paper conceptualizes urban planning as a generative AI task, surveying methods like VAEs, GANs, transformers, and diffusion models.

Result: Key gaps are identified: lack of urban theory guidance, multi-resolution analysis, data-augmented knowledge, and real-world interaction.

Conclusion: Future research should focus on theory-guided generation, digital twins, and human-machine co-design to bridge generative AI and participatory urbanism.

Abstract: Generative AI, large language models, and agentic AI have emerged separately
of urban planning. However, the convergence between AI and urban planning
presents an interesting opportunity towards AI urban planners. This paper
conceptualizes urban planning as a generative AI task, where AI synthesizes
land-use configurations under geospatial, social, and human-centric
constraints. We survey how generative AI approaches, including VAEs, GANs,
transformers, and diffusion models, reshape urban design. We further identify
critical gaps: 1) limited research on integrating urban theory guidance, 2)
limited research of AI urban planning over multiple spatial resolutions or
angularities, 3) limited research on augmenting urban design knowledge from
data, and 4) limited research on addressing real-world interactions. To address
these limitations, we outline future research directions in theory-guided
generation, digital twins, and human-machine co-design, calling for a new
synthesis of generative intelligence and participatory urbanism.

</details>


### [162] [AgentFly: Extensible and Scalable Reinforcement Learning for LM Agents](https://arxiv.org/abs/2507.14897)
*Renxi Wang,Rifo Ahmad Genadi,Bilal El Bouardi,Yongxin Wang,Fajri Koto,Zhengzhong Liu,Timothy Baldwin,Haonan Li*

Main category: cs.AI

TL;DR: AgentFly is a scalable and extensible framework combining LM agents with RL, featuring token-level masking, decorator-based tools, and high-throughput training.


<details>
  <summary>Details</summary>
Motivation: The combination of LM agents and RL (Agent-RL) is underexplored, lacking systematic study.

Method: Built AgentFly with token-level masking, decorator-based tools, asynchronous execution, and centralized resource management.

Result: Demonstrated effectiveness through successful agent training across multiple tasks.

Conclusion: AgentFly empowers LM agents with RL, offering scalability and extensibility.

Abstract: Language model (LM) agents have gained significant attention for their
ability to autonomously complete tasks through interactions with environments,
tools, and APIs. LM agents are primarily built with prompt engineering or
supervised finetuning. At the same time, reinforcement learning (RL) has been
explored to enhance LM's capabilities, such as reasoning and factuality.
However, the combination of the LM agents and reinforcement learning (Agent-RL)
remains underexplored and lacks systematic study. To this end, we built
AgentFly, a scalable and extensible Agent-RL framework designed to empower LM
agents with a variety of RL algorithms. Our framework supports multi-turn
interactions by adapting traditional RL methods with token-level masking. It
features a decorator-based interface for defining tools and reward functions,
enabling seamless extension and ease of use. To support high-throughput
training, we implement asynchronous execution of tool calls and reward
computations, and design a centralized resource management system for scalable
environment coordination. We also provide a suite of prebuilt tools and
environments, demonstrating the framework's effectiveness through successful
agent training across multiple tasks.

</details>


### [163] [InsightX Agent: An LMM-based Agentic Framework with Integrated Tools for Reliable X-ray NDT Analysis](https://arxiv.org/abs/2507.14899)
*Jiale Liu,Huan Wang,Yue Zhang,Xiaoyu Luo,Jiaxiang Hu,Zhiliang Liu,Min Xie*

Main category: cs.AI

TL;DR: The paper introduces InsightX Agent, an LMM-based framework for X-ray NDT, enhancing reliability, interpretability, and interactivity by combining SDMSD for defect detection and EGR for validation.


<details>
  <summary>Details</summary>
Motivation: Existing deep-learning-based NDT methods lack interactivity, interpretability, and self-assessment, limiting reliability and operator trust.

Method: InsightX Agent uses an LMM to coordinate SDMSD for multi-scale defect detection and EGR for a review process, improving accuracy and interpretability.

Result: Achieves a 96.35% F1-score on GDXray+ dataset, with enhanced interpretability and trustworthiness.

Conclusion: InsightX Agent demonstrates the potential of agentic LMM frameworks for reliable and interpretable industrial inspection.

Abstract: Non-destructive testing (NDT), particularly X-ray inspection, is vital for
industrial quality assurance, yet existing deep-learning-based approaches often
lack interactivity, interpretability, and the capacity for critical
self-assessment, limiting their reliability and operator trust. To address
these shortcomings, this paper proposes InsightX Agent, a novel LMM-based
agentic framework designed to deliver reliable, interpretable, and interactive
X-ray NDT analysis. Unlike typical sequential pipelines, InsightX Agent
positions a Large Multimodal Model (LMM) as a central orchestrator,
coordinating between the Sparse Deformable Multi-Scale Detector (SDMSD) and the
Evidence-Grounded Reflection (EGR) tool. The SDMSD generates dense defect
region proposals for multi-scale feature maps and sparsifies them through
Non-Maximum Suppression (NMS), optimizing detection of small, dense targets in
X-ray images while maintaining computational efficiency. The EGR tool guides
the LMM agent through a chain-of-thought-inspired review process, incorporating
context assessment, individual defect analysis, false positive elimination,
confidence recalibration and quality assurance to validate and refine the
SDMSD's initial proposals. By strategically employing and intelligently using
tools, InsightX Agent moves beyond passive data processing to active reasoning,
enhancing diagnostic reliability and providing interpretations that integrate
diverse information sources. Experimental evaluations on the GDXray+ dataset
demonstrate that InsightX Agent not only achieves a high object detection
F1-score of 96.35% but also offers significantly improved interpretability and
trustworthiness in its analyses, highlighting the transformative potential of
agentic LLM frameworks for industrial inspection tasks.

</details>


### [164] [Feedback-Induced Performance Decline in LLM-Based Decision-Making](https://arxiv.org/abs/2507.14906)
*Xiao Yang,Juxi Leitner,Michael Burke*

Main category: cs.AI

TL;DR: LLMs show promise in autonomous decision-making but struggle in complex scenarios without fine-tuning, highlighting the need for hybrid strategies.


<details>
  <summary>Details</summary>
Motivation: To evaluate LLMs' suitability in autonomous decision-making within MDPs, leveraging their pre-trained knowledge for faster adaptation compared to traditional RL.

Method: Investigates online structured prompting strategies in sequential decision-making tasks, comparing zero-shot LLM performance to classical RL methods.

Result: LLMs perform well initially in simple environments but falter in complex planning and reasoning without fine-tuning; feedback mechanisms can reduce performance.

Conclusion: Hybrid strategies, fine-tuning, and advanced memory integration are needed to enhance LLM-based decision-making in complex scenarios.

Abstract: The ability of Large Language Models (LLMs) to extract context from natural
language problem descriptions naturally raises questions about their
suitability in autonomous decision-making settings. This paper studies the
behaviour of these models within a Markov Decision Process (MDPs). While
traditional reinforcement learning (RL) strategies commonly employed in this
setting rely on iterative exploration, LLMs, pre-trained on diverse datasets,
offer the capability to leverage prior knowledge for faster adaptation. We
investigate online structured prompting strategies in sequential decision
making tasks, comparing the zero-shot performance of LLM-based approaches to
that of classical RL methods. Our findings reveal that although LLMs
demonstrate improved initial performance in simpler environments, they struggle
with planning and reasoning in complex scenarios without fine-tuning or
additional guidance. Our results show that feedback mechanisms, intended to
improve decision-making, often introduce confusion, leading to diminished
performance in intricate environments. These insights underscore the need for
further exploration into hybrid strategies, fine-tuning, and advanced memory
integration to enhance LLM-based decision-making capabilities.

</details>


### [165] [The Endless Tuning. An Artificial Intelligence Design To Avoid Human Replacement and Trace Back Responsibilities](https://arxiv.org/abs/2507.14909)
*Elio Grande*

Main category: cs.AI

TL;DR: The paper introduces the Endless Tuning method for AI deployment, focusing on avoiding human replacement and addressing responsibility gaps. It tests the method in three applications and emphasizes user experience over accuracy.


<details>
  <summary>Details</summary>
Motivation: To ensure reliable AI deployment by avoiding human replacement and filling the responsibility gap, inspired by relational ethics.

Method: A double mirroring process, implemented in three prototypical applications (loan granting, pneumonia diagnosis, art style recognition) and tested with domain experts.

Result: Users perceived full control in decision-making, and the method bridged accountability and liability in case of damage.

Conclusion: The Endless Tuning method successfully balances ethical concerns with technical deployment, prioritizing user experience and accountability.

Abstract: The Endless Tuning is a design method for a reliable deployment of artificial
intelligence based on a double mirroring process, which pursues both the goals
of avoiding human replacement and filling the so-called responsibility gap
(Matthias 2004). Originally depicted in (Fabris et al. 2024) and ensuing the
relational approach urged therein, it was then actualized in a protocol,
implemented in three prototypical applications regarding decision-making
processes (respectively: loan granting, pneumonia diagnosis, and art style
recognition) and tested with such as many domain experts. Step by step
illustrating the protocol, giving insights concretely showing a different voice
(Gilligan 1993) in the ethics of artificial intelligence, a philosophical
account of technical choices (e.g., a reversed and hermeneutic deployment of
XAI algorithms) will be provided in the present study together with the results
of the experiments, focusing on user experience rather than statistical
accuracy. Even thoroughly employing deep learning models, full control was
perceived by the interviewees in the decision-making setting, while it appeared
that a bridge can be built between accountability and liability in case of
damage.

</details>


### [166] [Redefining Elderly Care with Agentic AI: Challenges and Opportunities](https://arxiv.org/abs/2507.14912)
*Ruhul Amin Khalil,Kashif Ahmad,Hazrat Ali*

Main category: cs.AI

TL;DR: The paper explores the use of Agentic AI, powered by LLMs, in elderly care, highlighting its potential for proactive health and environmental management while addressing ethical concerns like privacy and decision independence.


<details>
  <summary>Details</summary>
Motivation: The global ageing population requires innovative care strategies, and Agentic AI offers transformative potential in elderly care.

Method: The study reviews the capabilities, applications, and limitations of LLM-based Agentic AI in elderly care, addressing a literature gap.

Result: Agentic AI can enhance elderly independence through personalized health and cognitive care but raises ethical concerns requiring safeguards.

Conclusion: The paper calls for responsible AI use in elderly care, emphasizing ethical safeguards and research priorities for human-centered integration.

Abstract: The global ageing population necessitates new and emerging strategies for
caring for older adults. In this article, we explore the potential for
transformation in elderly care through Agentic Artificial Intelligence (AI),
powered by Large Language Models (LLMs). We discuss the proactive and
autonomous decision-making facilitated by Agentic AI in elderly care.
Personalized tracking of health, cognitive care, and environmental management,
all aimed at enhancing independence and high-level living for older adults,
represents important areas of application. With a potential for significant
transformation of elderly care, Agentic AI also raises profound concerns about
data privacy and security, decision independence, and access. We share key
insights to emphasize the need for ethical safeguards, privacy protections, and
transparent decision-making. Our goal in this article is to provide a balanced
discussion of both the potential and the challenges associated with Agentic AI,
and to provide insights into its responsible use in elderly care, to bring
Agentic AI into harmony with the requirements and vulnerabilities specific to
the elderly. Finally, we identify the priorities for the academic research
communities, to achieve human-centered advancements and integration of Agentic
AI in elderly care. To the best of our knowledge, this is no existing study
that reviews the role of Agentic AI in elderly care. Hence, we address the
literature gap by analyzing the unique capabilities, applications, and
limitations of LLM-based Agentic AI in elderly care. We also provide a
companion interactive dashboard at https://hazratali.github.io/agenticai/.

</details>


### [167] [Complexity of Faceted Explanations in Propositional Abduction](https://arxiv.org/abs/2507.14962)
*Johannes Schmidt,Mohamed Maizia,Victor Lagerkvist,Johannes K. Fichte*

Main category: cs.AI

TL;DR: The paper explores propositional abduction, focusing on facets (relevant but dispensable literals) and explanation distances to better understand variability and complexity.


<details>
  <summary>Details</summary>
Motivation: To enhance understanding of explanations in propositional abduction by analyzing facets and distances, addressing computational challenges in counting and enumeration.

Method: Introduces facets (literals in some but not all explanations) and measures explanation distances to study heterogeneity. Analyzes these concepts in various settings, including Post's framework.

Result: Provides a comprehensive analysis of facets in propositional abduction, with an almost complete characterization in Post's framework.

Conclusion: Facets and distance metrics offer a fine-grained understanding of explanation variability, balancing computational feasibility with insightful reasoning.

Abstract: Abductive reasoning is a popular non-monotonic paradigm that aims to explain
observed symptoms and manifestations. It has many applications, such as
diagnosis and planning in artificial intelligence and database updates. In
propositional abduction, we focus on specifying knowledge by a propositional
formula. The computational complexity of tasks in propositional abduction has
been systematically characterized - even with detailed classifications for
Boolean fragments. Unsurprisingly, the most insightful reasoning problems
(counting and enumeration) are computationally highly challenging. Therefore,
we consider reasoning between decisions and counting, allowing us to understand
explanations better while maintaining favorable complexity. We introduce facets
to propositional abductions, which are literals that occur in some explanation
(relevant) but not all explanations (dispensable). Reasoning with facets
provides a more fine-grained understanding of variability in explanations
(heterogeneous). In addition, we consider the distance between two
explanations, enabling a better understanding of heterogeneity/homogeneity. We
comprehensively analyze facets of propositional abduction in various settings,
including an almost complete characterization in Post's framework.

</details>


### [168] [AlphaAlign: Incentivizing Safety Alignment with Extremely Simplified Reinforcement Learning](https://arxiv.org/abs/2507.14987)
*Yi Zhang,An Zhang,XiuYu Zhang,Leheng Sheng,Yuxin Chen,Zhenkai Liang,Xiang Wang*

Main category: cs.AI

TL;DR: AlphaAlign is a reinforcement learning framework that enhances LLM safety alignment by leveraging intrinsic safety awareness, improving refusal accuracy and reducing over-refusals without sacrificing utility.


<details>
  <summary>Details</summary>
Motivation: Current safety alignment methods for LLMs are superficial or require intensive supervision, failing to utilize the model's inherent safety awareness.

Method: AlphaAlign uses a dual-reward RL system: a verifiable safety reward for justified refusals and a helpfulness reward for benign inputs, promoting proactive safety reasoning.

Result: AlphaAlign improves refusal accuracy, reduces over-refusals, maintains task performance, and enhances robustness to unseen threats.

Conclusion: AlphaAlign offers a simple, efficient, and effective solution for deep safety alignment in LLMs, fostering explicit safety reasoning.

Abstract: Large language models (LLMs), despite possessing latent safety understanding
from their vast pretraining data, remain vulnerable to generating harmful
content and exhibit issues such as over-refusal and utility degradation after
safety alignment. Current safety alignment methods often result in superficial
refusal shortcuts or rely on intensive supervision for reasoning-based
approaches, failing to fully leverage the model's intrinsic safety
self-awareness. We propose \textbf{AlphaAlign}, a simple yet effective pure
reinforcement learning (RL) framework with verifiable safety reward designed to
incentivize this latent safety awareness through proactive safety reasoning.}
AlphaAlign employs a dual-reward system: a verifiable safety reward encourages
correctly formatted and explicitly justified refusals for harmful queries while
penalizing over-refusals, and a normalized helpfulness reward guides
high-quality responses to benign inputs. This allows the model to develop
proactive safety reasoning capabilities without depending on supervised
safety-specific reasoning data. AlphaAlign demonstrates three key advantages:
(1) Simplicity and efficiency, requiring only binary prompt safety labels and
minimal RL steps for substantial improvements. (2) Breaking the safety-utility
trade-off, by enhancing refusal of harmful content and reducing over-refusals,
while simultaneously maintaining or even improving general task performance and
robustness to unseen jailbreaks. (3) Deep alignment, fostering proactive safety
reasoning that generates explicit safety rationales rather than relying on
shallow refusal patterns.

</details>


### [169] [A Forced-Choice Neural Cognitive Diagnostic Model of Personality Testing](https://arxiv.org/abs/2507.15013)
*Xiaoyu Li,Jin Wu,Shaoyang Guo,Haoran Shi,Chanjin Zheng*

Main category: cs.AI

TL;DR: A deep learning-based Forced-Choice Neural Cognitive Diagnostic Model (FCNCD) is introduced for psychometric tests, improving accuracy and interpretability over traditional methods.


<details>
  <summary>Details</summary>
Motivation: To address limitations of traditional forced-choice test models and enhance interpretability in personality assessments.

Method: Uses multilayer neural networks to model participant-item interactions, incorporating monotonicity assumptions for interpretability.

Result: Validated on real-world and simulated datasets, showing accuracy, interpretability, and robustness.

Conclusion: FCNCD is effective for forced-choice tests, offering improved diagnostic capabilities.

Abstract: In the smart era, psychometric tests are becoming increasingly important for
personnel selection, career development, and mental health assessment.
Forced-choice tests are common in personality assessments because they require
participants to select from closely related options, lowering the risk of
response distortion. This study presents a deep learning-based Forced-Choice
Neural Cognitive Diagnostic Model (FCNCD) that overcomes the limitations of
traditional models and is applicable to the three most common item block types
found in forced-choice tests. To account for the unidimensionality of items in
forced-choice tests, we create interpretable participant and item parameters.
We model the interactions between participant and item features using
multilayer neural networks after mining them using nonlinear mapping. In
addition, we use the monotonicity assumption to improve the interpretability of
the diagnostic results. The FCNCD's effectiveness is validated by experiments
on real-world and simulated datasets that show its accuracy, interpretability,
and robustness.

</details>


### [170] [DeRAG: Black-box Adversarial Attacks on Multiple Retrieval-Augmented Generation Applications via Prompt Injection](https://arxiv.org/abs/2507.15042)
*Jerry Wang,Fang Yu*

Main category: cs.AI

TL;DR: A novel gradient-free method using Differential Evolution (DE) optimizes adversarial prompt suffixes for RAG systems, achieving competitive attack success with minimal tokens and evading detection.


<details>
  <summary>Details</summary>
Motivation: Adversarial prompt attacks undermine RAG system reliability by re-ranking outputs incorrectly. This work aims to optimize such attacks realistically.

Method: DE is applied to evolve adversarial suffixes, treating RAG as a black box. Experiments on BEIR QA datasets evaluate attack success under various retrieval conditions.

Result: DE-based optimization outperforms GGPP and PRADA in some cases, using ≤5 tokens. Readability-aware suffixes reduce MLM negative log-likelihood and evade BERT-based detection.

Conclusion: DE effectively optimizes adversarial prompts for RAG, balancing attack success and stealth with minimal tokens.

Abstract: Adversarial prompt attacks can significantly alter the reliability of
Retrieval-Augmented Generation (RAG) systems by re-ranking them to produce
incorrect outputs. In this paper, we present a novel method that applies
Differential Evolution (DE) to optimize adversarial prompt suffixes for
RAG-based question answering. Our approach is gradient-free, treating the RAG
pipeline as a black box and evolving a population of candidate suffixes to
maximize the retrieval rank of a targeted incorrect document to be closer to
real world scenarios. We conducted experiments on the BEIR QA datasets to
evaluate attack success at certain retrieval rank thresholds under multiple
retrieving applications. Our results demonstrate that DE-based prompt
optimization attains competitive (and in some cases higher) success rates
compared to GGPP to dense retrievers and PRADA to sparse retrievers, while
using only a small number of tokens (<=5 tokens) in the adversarial suffix.
Furthermore, we introduce a readability-aware suffix construction strategy,
validated by a statistically significant reduction in MLM negative
log-likelihood with Welch's t-test. Through evaluations with a BERT-based
adversarial suffix detector, we show that DE-generated suffixes evade
detection, yielding near-chance detection accuracy.

</details>


### [171] [From Kicking to Causality: Simulating Infant Agency Detection with a Robust Intrinsic Reward](https://arxiv.org/abs/2507.15106)
*Xia Xu,Jochen Triesch*

Main category: cs.AI

TL;DR: The paper introduces CAIS, a causal inference-based intrinsic reward, to improve reinforcement learning agents' robustness by isolating causal impact from noise.


<details>
  <summary>Details</summary>
Motivation: Standard reinforcement learning agents fail in noisy scenarios due to reliance on correlation-based rewards, unlike human infants who discover causal efficacy.

Method: CAIS measures the 1-Wasserstein distance between outcome distributions conditional on actions and baseline outcomes to quantify causal influence.

Result: CAIS outperforms correlation-based rewards in noisy environments, enabling correct policy learning and reproducing the "extinction burst" phenomenon.

Conclusion: Inferring causality is key for robust agency, providing a psychologically plausible framework for adaptive autonomous systems.

Abstract: While human infants robustly discover their own causal efficacy, standard
reinforcement learning agents remain brittle, as their reliance on
correlation-based rewards fails in noisy, ecologically valid scenarios. To
address this, we introduce the Causal Action Influence Score (CAIS), a novel
intrinsic reward rooted in causal inference. CAIS quantifies an action's
influence by measuring the 1-Wasserstein distance between the learned
distribution of sensory outcomes conditional on that action, $p(h|a)$, and the
baseline outcome distribution, $p(h)$. This divergence provides a robust reward
that isolates the agent's causal impact from confounding environmental noise.
We test our approach in a simulated infant-mobile environment where
correlation-based perceptual rewards fail completely when the mobile is
subjected to external forces. In stark contrast, CAIS enables the agent to
filter this noise, identify its influence, and learn the correct policy.
Furthermore, the high-quality predictive model learned for CAIS allows our
agent, when augmented with a surprise signal, to successfully reproduce the
"extinction burst" phenomenon. We conclude that explicitly inferring causality
is a crucial mechanism for developing a robust sense of agency, offering a
psychologically plausible framework for more adaptive autonomous systems.

</details>


### [172] [Automated planning with ontologies under coherence update semantics](https://arxiv.org/abs/2507.15120)
*Stefan Borgwardt,Duy Nhu,Gabriele Röger*

Main category: cs.AI

TL;DR: A new approach for planning with DL-Lite ontologies combines explicit-input knowledge and coherence update semantics, showing no higher complexity and offering efficient implementation.


<details>
  <summary>Details</summary>
Motivation: Incorporating background knowledge into automated planning, especially using ontologies with open-world semantics, to enhance planning capabilities.

Method: Combines ontology-based action conditions (eKABs) and ontology-aware action effects under coherence update semantics, with a polynomial compilation into classical planning.

Result: The formalism's complexity matches previous approaches, and implementation shows promising performance on benchmarks.

Conclusion: The approach effectively integrates ontologies into planning without increasing complexity, validated by benchmark evaluations.

Abstract: Standard automated planning employs first-order formulas under closed-world
semantics to achieve a goal with a given set of actions from an initial state.
We follow a line of research that aims to incorporate background knowledge into
automated planning problems, for example, by means of ontologies, which are
usually interpreted under open-world semantics. We present a new approach for
planning with DL-Lite ontologies that combines the advantages of ontology-based
action conditions provided by explicit-input knowledge and action bases (eKABs)
and ontology-aware action effects under the coherence update semantics. We show
that the complexity of the resulting formalism is not higher than that of
previous approaches and provide an implementation via a polynomial compilation
into classical planning. An evaluation of existing and new benchmarks examines
the performance of a planning system on different variants of our compilation.

</details>


### [173] [Clinical Semantic Intelligence (CSI): Emulating the Cognitive Framework of the Expert Clinician for Comprehensive Oral Disease Diagnosis](https://arxiv.org/abs/2507.15140)
*Mohammad Mashayekhi,Sara Ahmadi Majd,Arian AmirAmjadi,Parsa Hosseini*

Main category: cs.AI

TL;DR: CSI is an AI framework for diagnosing 118 oral diseases by mimicking expert clinician reasoning, achieving higher accuracy with hierarchical reasoning.


<details>
  <summary>Details</summary>
Motivation: Oral disease diagnosis is challenging due to overlapping symptoms; emulating expert reasoning is key for useful diagnostic aids.

Method: Combines multimodal CLIP and ChatGLM-6B models in a Hierarchical Diagnostic Reasoning Tree (HDRT) for fast and standard diagnostic modes.

Result: Fast Mode: 73.4% accuracy; Standard Mode (HDRT): 89.5% accuracy. Performance gain attributed to hierarchical reasoning.

Conclusion: CSI's hierarchical reasoning improves diagnostic accuracy, demonstrating its potential as a clinically useful tool.

Abstract: The diagnosis of oral diseases presents a problematic clinical challenge,
characterized by a wide spectrum of pathologies with overlapping
symptomatology. To address this, we developed Clinical Semantic Intelligence
(CSI), a novel artificial intelligence framework that diagnoses 118 different
oral diseases by computationally modeling the cognitive processes of an expert
clinician. Our core hypothesis is that moving beyond simple pattern matching to
emulate expert reasoning is critical to building clinically useful diagnostic
aids.
  CSI's architecture integrates a fine-tuned multimodal CLIP model with a
specialized ChatGLM-6B language model. This system executes a Hierarchical
Diagnostic Reasoning Tree (HDRT), a structured framework that distills the
systematic, multi-step logic of differential diagnosis. The framework operates
in two modes: a Fast Mode for rapid screening and a Standard Mode that
leverages the full HDRT for an interactive and in-depth diagnostic workup.
  To train and validate our system, we curated a primary dataset of 4,310
images, supplemented by an external hold-out set of 176 images for final
validation. A clinically-informed augmentation strategy expanded our training
data to over 30,000 image-text pairs. On a 431-image internal test set, CSI's
Fast Mode achieved an accuracy of 73.4%, which increased to 89.5% with the
HDRT-driven Standard Mode. The performance gain is directly attributable to the
hierarchical reasoning process. Herein, we detail the architectural philosophy,
development, and rigorous evaluation of the CSI framework.

</details>


### [174] [Can We Move Freely in NEOM's The Line? An Agent-Based Simulation of Human Mobility in a Futuristic Smart City](https://arxiv.org/abs/2507.15143)
*Abderaouf Bahi,Amel Ourici*

Main category: cs.AI

TL;DR: The paper explores human mobility in The Line, a 170km linear smart city, using a hybrid AI simulation. Results show efficient commute times (7.8-8.4 min), high satisfaction (89%), and reachability (91%) with AI, but performance drops significantly without it. Sustainable infrastructure and AI are key.


<details>
  <summary>Details</summary>
Motivation: To assess whether citizens can move freely in The Line's unique urban design, addressing challenges of mobility in a linear, high-density city.

Method: Developed a hybrid simulation combining agent-based modeling, reinforcement learning, supervised learning, and graph neural networks, tested with synthetic and real-world data.

Result: AI integration achieved efficient commute times (7.8-8.4 min), high satisfaction (89%), and reachability (91%). Removing AI modules worsened performance (85% longer commutes, <70% reachability).

Conclusion: Freedom of movement in The Line is feasible with adaptive AI, sustainable infrastructure, and real-time feedback, but relies heavily on intelligent systems.

Abstract: This paper investigates the feasibility of human mobility in The Line, a
proposed 170-kilometer linear smart city in NEOM, Saudi Arabia. To assess
whether citizens can move freely within this unprecedented urban topology, we
develop a hybrid simulation framework that integrates agent-based modeling,
reinforcement learning, supervised learning, and graph neural networks. The
simulation captures multi-modal transportation behaviors across 50 vertical
levels and varying density scenarios using both synthetic data and real-world
traces from high-density cities. Our experiments reveal that with the full
AI-integrated architecture, agents achieved an average commute time of 7.8 to
8.4 minutes, a satisfaction rate exceeding 89 percent, and a reachability index
of over 91 percent, even during peak congestion periods. Ablation studies
confirmed that the removal of intelligent modules such as reinforcement
learning or graph neural networks significantly degrades performance, with
commute times increasing by up to 85 percent and reachability falling below 70
percent. Environmental modeling further demonstrated low energy consumption and
minimal CO2 emissions when electric modes are prioritized. The findings suggest
that freedom of movement is not only conceptually achievable in The Line, but
also operationally realistic if supported by adaptive AI systems, sustainable
infrastructure, and real-time feedback loops.

</details>


### [175] [Solving Formal Math Problems by Decomposition and Iterative Reflection](https://arxiv.org/abs/2507.15225)
*Yichi Zhou,Jianqiu Zhao,Yongxin Zhang,Bohan Wang,Siran Wang,Luoxin Chen,Jiahui Wang,Haowei Chen,Allan Jie,Xinbo Zhang,Haocheng Wang,Luong Trung,Rong Ye,Phan Nhat Hoang,Huishuai Zhang,Peng Sun,Hang Li*

Main category: cs.AI

TL;DR: Delta Prover is an agent-based framework that enables general-purpose LLMs to construct formal proofs in Lean 4 without model specialization, achieving a 95.9% success rate on the miniF2F-test benchmark.


<details>
  <summary>Details</summary>
Motivation: Specialized models for formal proof generation in Lean 4 are costly and inefficient. Delta Prover aims to leverage general-purpose LLMs' reasoning capabilities for this task.

Method: Delta Prover integrates reflective decomposition, iterative proof repair, and a custom DSL for subproblem management to guide LLMs in proof construction.

Result: Achieves a 95.9% success rate on miniF2F-test, surpassing specialized models, and shows strong test-time scaling.

Conclusion: General-purpose LLMs, with effective agentic guidance, can excel in theorem proving, offering a computationally efficient alternative to specialized models.

Abstract: General-purpose Large Language Models (LLMs) have achieved remarkable success
in intelligence, performing comparably to human experts on complex reasoning
tasks such as coding and mathematical reasoning. However, generating formal
proofs in specialized languages like Lean 4 remains a significant challenge for
these models, limiting their application in complex theorem proving and
automated verification. Current approaches typically require specializing
models through fine-tuning on dedicated formal corpora, incurring high costs
for data collection and training. In this work, we introduce \textbf{Delta
Prover}, an agent-based framework that orchestrates the interaction between a
general-purpose LLM and the Lean 4 proof environment. Delta Prover leverages
the reflection and reasoning capabilities of general-purpose LLMs to
interactively construct formal proofs in Lean 4, circumventing the need for
model specialization. At its core, the agent integrates two novel,
interdependent components: an algorithmic framework for reflective
decomposition and iterative proof repair, and a custom Domain-Specific Language
(DSL) built upon Lean 4 for streamlined subproblem management. \textbf{Delta
Prover achieves a state-of-the-art 95.9\% success rate on the miniF2F-test
benchmark, surpassing all existing approaches, including those requiring model
specialization.} Furthermore, Delta Prover exhibits a significantly stronger
test-time scaling law compared to standard Best-of-N proof strategies.
Crucially, our findings demonstrate that general-purpose LLMs, when guided by
an effective agentic structure, possess substantial untapped theorem-proving
capabilities. This presents a computationally efficient alternative to
specialized models for robust automated reasoning in formal environments.

</details>


### [176] [Explainable Artificial Intelligence based Soft Evaluation Indicator for Arc Fault Diagnosis](https://arxiv.org/abs/2507.15239)
*Qianchao Wang,Yuxuan Ding,Chuanzhen Jia,Zhe Li,Yaping Du*

Main category: cs.AI

TL;DR: The paper proposes a soft evaluation indicator and a lightweight balanced neural network to enhance trust and understanding in AI-based arc fault diagnosis models.


<details>
  <summary>Details</summary>
Motivation: Existing AI-based arc fault diagnosis models lack trustworthiness, prompting the need for explainable outputs and reliable evaluation.

Method: The work uses Explainable AI and real arc fault experiments to define correct explanations, alongside a lightweight balanced neural network for accuracy and feature extraction.

Result: Tested on traditional and deep learning methods across datasets, the soft evaluation indicator improves model interpretability and trust.

Conclusion: The approach makes arc fault diagnosis models more understandable and trustworthy for practitioners.

Abstract: Novel AI-based arc fault diagnosis models have demonstrated outstanding
performance in terms of classification accuracy. However, an inherent problem
is whether these models can actually be trusted to find arc faults. In this
light, this work proposes a soft evaluation indicator that explains the outputs
of arc fault diagnosis models, by defining the the correct explanation of arc
faults and leveraging Explainable Artificial Intelligence and real arc fault
experiments. Meanwhile, a lightweight balanced neural network is proposed to
guarantee competitive accuracy and soft feature extraction score. In our
experiments, several traditional machine learning methods and deep learning
methods across two arc fault datasets with different sample times and noise
levels are utilized to test the effectiveness of the soft evaluation indicator.
Through this approach, the arc fault diagnosis models are easy to understand
and trust, allowing practitioners to make informed and trustworthy decisions.

</details>


### [177] [Disentangling Homophily and Heterophily in Multimodal Graph Clustering](https://arxiv.org/abs/2507.15253)
*Zhaochen Guo,Zhixiang Shen,Xuanting Xie,Liangjian Wen,Zhao Kang*

Main category: cs.AI

TL;DR: The paper introduces DMGC, a framework for unsupervised multimodal graph clustering, addressing hybrid neighborhood patterns by disentangling homophilic and heterophilic relationships.


<details>
  <summary>Details</summary>
Motivation: Multimodal graphs are understudied in unsupervised learning, despite their real-world utility. The paper aims to bridge this gap by addressing hybrid neighborhood patterns.

Method: Proposes DMGC, which decomposes the graph into homophily-enhanced and heterophily-aware views, using a Multimodal Dual-frequency Fusion mechanism and self-supervised alignment.

Result: DMGC achieves state-of-the-art performance on multimodal and multi-relational graph datasets, demonstrating effectiveness and generalizability.

Conclusion: DMGC successfully addresses hybrid patterns in multimodal graphs, offering a robust solution for unsupervised clustering.

Abstract: Multimodal graphs, which integrate unstructured heterogeneous data with
structured interconnections, offer substantial real-world utility but remain
insufficiently explored in unsupervised learning. In this work, we initiate the
study of multimodal graph clustering, aiming to bridge this critical gap.
Through empirical analysis, we observe that real-world multimodal graphs often
exhibit hybrid neighborhood patterns, combining both homophilic and
heterophilic relationships. To address this challenge, we propose a novel
framework -- \textsc{Disentangled Multimodal Graph Clustering (DMGC)} -- which
decomposes the original hybrid graph into two complementary views: (1) a
homophily-enhanced graph that captures cross-modal class consistency, and (2)
heterophily-aware graphs that preserve modality-specific inter-class
distinctions. We introduce a \emph{Multimodal Dual-frequency Fusion} mechanism
that jointly filters these disentangled graphs through a dual-pass strategy,
enabling effective multimodal integration while mitigating category confusion.
Our self-supervised alignment objectives further guide the learning process
without requiring labels. Extensive experiments on both multimodal and
multi-relational graph datasets demonstrate that DMGC achieves state-of-the-art
performance, highlighting its effectiveness and generalizability across diverse
settings. Our code is available at https://github.com/Uncnbb/DMGC.

</details>


### [178] [IM-Chat: A Multi-agent LLM-based Framework for Knowledge Transfer in Injection Molding Industry](https://arxiv.org/abs/2507.15268)
*Junhyeong Lee,Joon-Young Kim,Heekyu Kim,Inhyo Lee,Seunghwa Ryu*

Main category: cs.AI

TL;DR: IM-Chat, a multi-agent LLM framework, addresses knowledge transfer challenges in injection molding by integrating documented and field data, achieving high accuracy in complex tasks.


<details>
  <summary>Details</summary>
Motivation: The injection molding industry struggles with knowledge preservation due to retiring workers and multilingual barriers, necessitating an AI-driven solution.

Method: IM-Chat uses a retrieval-augmented generation (RAG) strategy and tool-calling agents to infer optimal settings from environmental inputs, tested on GPT models.

Result: More capable models (e.g., GPT-4o) showed higher accuracy, especially in complex tasks, validating the framework's effectiveness.

Conclusion: IM-Chat proves scalable and generalizable for AI-assisted decision support in manufacturing, leveraging multi-agent LLM systems.

Abstract: The injection molding industry faces critical challenges in preserving and
transferring field knowledge, particularly as experienced workers retire and
multilingual barriers hinder effective communication. This study introduces
IM-Chat, a multi-agent framework based on large language models (LLMs),
designed to facilitate knowledge transfer in injection molding. IM-Chat
integrates both limited documented knowledge (e.g., troubleshooting tables,
manuals) and extensive field data modeled through a data-driven process
condition generator that infers optimal manufacturing settings from
environmental inputs such as temperature and humidity, enabling robust and
context-aware task resolution. By adopting a retrieval-augmented generation
(RAG) strategy and tool-calling agents within a modular architecture, IM-Chat
ensures adaptability without the need for fine-tuning. Performance was assessed
across 100 single-tool and 60 hybrid tasks for GPT-4o, GPT-4o-mini, and
GPT-3.5-turbo by domain experts using a 10-point rubric focused on relevance
and correctness, and was further supplemented by automated evaluation using
GPT-4o guided by a domain-adapted instruction prompt. The evaluation results
indicate that more capable models tend to achieve higher accuracy, particularly
in complex, tool-integrated scenarios. Overall, these findings demonstrate the
viability of multi-agent LLM systems for industrial knowledge workflows and
establish IM-Chat as a scalable and generalizable approach to AI-assisted
decision support in manufacturing.

</details>


### [179] [QSAF: A Novel Mitigation Framework for Cognitive Degradation in Agentic AI](https://arxiv.org/abs/2507.15330)
*Hammad Atta,Muhammad Zeeshan Baig,Yasir Mehmood,Nadeem Shahzad,Ken Huang,Muhammad Aziz Ul Haq,Muhammad Awais,Kamal Ahmed*

Main category: cs.AI

TL;DR: The paper introduces Cognitive Degradation as a new vulnerability in AI systems, proposes the Qorvex Security AI Framework for mitigation, and maps AI architectures to human cognitive analogs for resilience.


<details>
  <summary>Details</summary>
Motivation: To address internal failures in AI systems (e.g., memory starvation, logic collapse) that cause silent degradation, unlike traditional external threats.

Method: Introduces the Qorvex Security AI Framework with six-stage lifecycle and seven runtime controls for real-time monitoring and mitigation.

Result: Establishes Cognitive Degradation as a critical vulnerability class and provides the first cross-platform defense model for resilient AI behavior.

Conclusion: The work highlights the importance of addressing internal AI vulnerabilities and offers a practical framework for ensuring cognitive resilience in agentic systems.

Abstract: We introduce Cognitive Degradation as a novel vulnerability class in agentic
AI systems. Unlike traditional adversarial external threats such as prompt
injection, these failures originate internally, arising from memory starvation,
planner recursion, context flooding, and output suppression. These systemic
weaknesses lead to silent agent drift, logic collapse, and persistent
hallucinations over time. To address this class of failures, we introduce the
Qorvex Security AI Framework for Behavioral & Cognitive Resilience (QSAF Domain
10), a lifecycle-aware defense framework defined by a six-stage cognitive
degradation lifecycle. The framework includes seven runtime controls
(QSAF-BC-001 to BC-007) that monitor agent subsystems in real time and trigger
proactive mitigation through fallback routing, starvation detection, and memory
integrity enforcement. Drawing from cognitive neuroscience, we map agentic
architectures to human analogs, enabling early detection of fatigue,
starvation, and role collapse. By introducing a formal lifecycle and real-time
mitigation controls, this work establishes Cognitive Degradation as a critical
new class of AI system vulnerability and proposes the first cross-platform
defense model for resilient agentic behavior.

</details>


### [180] [One Step is Enough: Multi-Agent Reinforcement Learning based on One-Step Policy Optimization for Order Dispatch on Ride-Sharing Platforms](https://arxiv.org/abs/2507.15351)
*Zijian Zhao,Sen Li*

Main category: cs.AI

TL;DR: The paper proposes two MARL-based methods, GRPO and OSPO, to improve ride-sharing by avoiding value function estimation issues.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of dynamic passenger-vehicle matching under uncertainty and the limitations of conventional MARL approaches.

Method: Adapts GRPO to replace PPO baseline with group average reward and introduces OSPO, which trains policies using one-step rewards.

Result: Both GRPO and OSPO outperform conventional methods, optimizing pickup times and order service rates.

Conclusion: The proposed methods offer efficient, scalable solutions for ride-sharing platforms without relying on problematic value function estimation.

Abstract: On-demand ride-sharing platforms face the fundamental challenge of
dynamically bundling passengers with diverse origins and destinations and
matching them with vehicles in real time, all under significant uncertainty.
Recently, MARL has emerged as a promising solution for this problem, leveraging
decentralized learning to address the curse of dimensionality caused by the
large number of agents in the ride-hailing market and the resulting expansive
state and action spaces. However, conventional MARL-based ride-sharing
approaches heavily rely on the accurate estimation of Q-values or V-values,
which becomes problematic in large-scale, highly uncertain environments.
Specifically, most of these approaches adopt an independent paradigm,
exacerbating this issue, as each agent treats others as part of the
environment, leading to unstable training and substantial estimation bias in
value functions. To address these challenges, we propose two novel alternative
methods that bypass value function estimation. First, we adapt GRPO to
ride-sharing, replacing the PPO baseline with the group average reward to
eliminate critic estimation errors and reduce training bias. Second, inspired
by GRPO's full utilization of group reward information, we customize the PPO
framework for ride-sharing platforms and show that, under a homogeneous fleet,
the optimal policy can be trained using only one-step rewards - a method we
term One-Step Policy Optimization (OSPO). Experiments on a real-world Manhattan
ride-hailing dataset demonstrate that both GRPO and OSPO achieve superior
performance across most scenarios, efficiently optimizing pickup times and the
number of served orders using simple MLP networks.

</details>


### [181] [RAD: Retrieval High-quality Demonstrations to Enhance Decision-making](https://arxiv.org/abs/2507.15356)
*Lu Guo,Yixiang Shan,Zhengbang Zhu,Qifan Liang,Lichang Song,Ting Long,Weinan Zhang,Yi Chang*

Main category: cs.AI

TL;DR: RAD combines retrieval and diffusion models to improve offline RL by dynamically retrieving high-return states and planning toward them, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: Offline RL struggles with dataset sparsity and lack of transition overlap, limiting long-horizon planning. Prior methods fail to generalize or rely on heuristics.

Method: RAD uses non-parametric retrieval to find high-return states and a diffusion model for planning, enabling flexible trajectory stitching.

Result: RAD achieves competitive or superior performance across diverse benchmarks.

Conclusion: RAD effectively addresses generalization and planning challenges in offline RL.

Abstract: Offline reinforcement learning (RL) enables agents to learn policies from
fixed datasets, avoiding costly or unsafe environment interactions. However,
its effectiveness is often limited by dataset sparsity and the lack of
transition overlap between suboptimal and expert trajectories, which makes
long-horizon planning particularly challenging. Prior solutions based on
synthetic data augmentation or trajectory stitching often fail to generalize to
novel states and rely on heuristic stitching points. To address these
challenges, we propose Retrieval High-quAlity Demonstrations (RAD) for
decision-making, which combines non-parametric retrieval with diffusion-based
generative modeling. RAD dynamically retrieves high-return states from the
offline dataset as target states based on state similarity and return
estimation, and plans toward them using a condition-guided diffusion model.
Such retrieval-guided generation enables flexible trajectory stitching and
improves generalization when encountered with underrepresented or
out-of-distribution states. Extensive experiments confirm that RAD achieves
competitive or superior performance compared to baselines across diverse
benchmarks, validating its effectiveness.

</details>


### [182] [Predictive Process Monitoring Using Object-centric Graph Embeddings](https://arxiv.org/abs/2507.15411)
*Wissam Gherissi,Mehdi Acheli,Joyce El Haddad,Daniela Grigori*

Main category: cs.AI

TL;DR: An end-to-end model for object-centric predictive process monitoring using graph attention and LSTM networks, achieving competitive performance.


<details>
  <summary>Details</summary>
Motivation: To enhance process predictions by leveraging object-centric event logs, addressing the challenge of extracting relevant information and building effective models.

Method: Combines a graph attention network for encoding activities and relationships with an LSTM network to handle temporal dependencies.

Result: Demonstrates competitive performance on one real-life and three synthetic event logs compared to state-of-the-art methods.

Conclusion: The proposed model effectively predicts future process behavior, particularly for next activity and next event time tasks.

Abstract: Object-centric predictive process monitoring explores and utilizes
object-centric event logs to enhance process predictions. The main challenge
lies in extracting relevant information and building effective models. In this
paper, we propose an end-to-end model that predicts future process behavior,
focusing on two tasks: next activity prediction and next event time. The
proposed model employs a graph attention network to encode activities and their
relationships, combined with an LSTM network to handle temporal dependencies.
Evaluated on one reallife and three synthetic event logs, the model
demonstrates competitive performance compared to state-of-the-art methods.

</details>


### [183] [Optimization of Activity Batching Policies in Business Processes](https://arxiv.org/abs/2507.15457)
*Orlenys López-Pintado,Jannis Rosenbaum,Marlon Dumas*

Main category: cs.AI

TL;DR: The paper proposes a Pareto optimization approach to discover optimal batching policies in business processes, balancing waiting time, cost, and processing effort using intervention heuristics and meta-heuristics.


<details>
  <summary>Details</summary>
Motivation: To address the trade-off between cost, processing effort, and waiting time in activity batching, aiming to find optimal batching policies.

Method: Uses intervention heuristics to improve batching policies, evaluated via simulation, and embeds these in meta-heuristics (hill-climbing, simulated annealing, reinforcement learning) to update the Pareto front.

Result: An experimental evaluation compares the heuristic-guided approach against baselines, assessing convergence, diversity, and cycle time gain of Pareto-optimal policies.

Conclusion: The proposed approach effectively discovers optimal batching policies, outperforming non-heuristic baselines in balancing trade-offs.

Abstract: In business processes, activity batching refers to packing multiple activity
instances for joint execution. Batching allows managers to trade off cost and
processing effort against waiting time. Larger and less frequent batches may
lower costs by reducing processing effort and amortizing fixed costs, but they
create longer waiting times. In contrast, smaller and more frequent batches
reduce waiting times but increase fixed costs and processing effort. A batching
policy defines how activity instances are grouped into batches and when each
batch is activated. This paper addresses the problem of discovering batching
policies that strike optimal trade-offs between waiting time, processing
effort, and cost. The paper proposes a Pareto optimization approach that starts
from a given set (possibly empty) of activity batching policies and generates
alternative policies for each batched activity via intervention heuristics.
Each heuristic identifies an opportunity to improve an activity's batching
policy with respect to a metric (waiting time, processing time, cost, or
resource utilization) and an associated adjustment to the activity's batching
policy (the intervention). The impact of each intervention is evaluated via
simulation. The intervention heuristics are embedded in an optimization
meta-heuristic that triggers interventions to iteratively update the Pareto
front of the interventions identified so far. The paper considers three
meta-heuristics: hill-climbing, simulated annealing, and reinforcement
learning. An experimental evaluation compares the proposed approach based on
intervention heuristics against the same (non-heuristic guided) meta-heuristics
baseline regarding convergence, diversity, and cycle time gain of
Pareto-optimal policies.

</details>


### [184] [Chart-R1: Chain-of-Thought Supervision and Reinforcement for Advanced Chart Reasoner](https://arxiv.org/abs/2507.15509)
*Lei Chen,Xuanle Zhao,Zhixiong Zeng,Jing Huang,Yufeng Zhong,Lin Ma*

Main category: cs.AI

TL;DR: Chart-R1 introduces a vision-language model for chart reasoning using reinforcement learning fine-tuning, supported by programmatic data synthesis and a two-stage training strategy (Chart-COT and Chart-RFT). It outperforms existing methods and rivals large-scale models like GPT-4o.


<details>
  <summary>Details</summary>
Motivation: To extend R1-Style methods beyond mathematical reasoning and code intelligence to multimodal chart data, addressing the lack of high-quality reasoning data in the chart domain.

Method: 1. Programmatic data synthesis for step-by-step chart reasoning data. 2. Two-stage training: Chart-COT (chain-of-thought supervision) and Chart-RFT (numerically sensitive reinforcement fine-tuning).

Result: Chart-R1 outperforms chart-domain methods and competes with large-scale models like GPT-4o and Claude-3.5.

Conclusion: Chart-R1 successfully advances chart reasoning through innovative data synthesis and training strategies, demonstrating strong performance.

Abstract: Recently, inspired by OpenAI-o1/o3 and Deepseek-R1, the R1-Style method based
on reinforcement learning fine-tuning has received widespread attention from
the community. Previous R1-Style methods mainly focus on mathematical reasoning
and code intelligence. It is of great research significance to verify their
advantages on more general multimodal data. Chart is an important multimodal
data type with rich information, which brings important research challenges in
complex reasoning. In this work, we introduce Chart-R1, a chart-domain
vision-language model with reinforcement learning fine-tuning to enable complex
chart reasoning. To support Chart-R1, we first propose a novel programmatic
data synthesis technology to generate high-quality step-by-step chart reasoning
data covering single- and multi-subcharts, which makes up for the lack of
reasoning data in the chart domain. Then we develop a two-stage training
strategy: Chart-COT with step-by-step chain-of-thought supervision, and
Chart-RFT with numerically sensitive reinforcement fine-tuning. Chart-COT aims
to decompose complex chart reasoning tasks into fine-grained, understandable
subtasks through step-by-step supervision, which lays a good foundation for
improving the reasoning level of reinforcement learning. Chart-RFT utilize the
typical group relative policy optimization strategy, in which a relatively soft
reward is adopted for numerical response to emphasize the numerical sensitivity
in the chart domain. We conduct extensive experiments on open-source benchmarks
and self-built chart reasoning dataset (\emph{i.e., ChartRQA}). Experimental
results show that Chart-R1 has significant advantages compared to chart-domain
methods, even comparable to open/closed source large-scale models (\emph{e.g.,
GPT-4o, Claude-3.5}).

</details>


### [185] [HAMLET: Hyperadaptive Agent-based Modeling for Live Embodied Theatrics](https://arxiv.org/abs/2507.15518)
*Sizhou Chen,Shufan Jiang,Chi Zhang,Xiao-Lei Zhang,Xuelong Li*

Main category: cs.AI

TL;DR: HAMLET is a multi-agent framework for drama creation and performance, enabling autonomous AI actors to interact dynamically with each other and the environment, improving interactivity and immersion.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based drama methods lack initiative and require detailed user input, reducing interactivity and immersion in real-time performances.

Method: HAMLET generates a narrative blueprint from a simple topic, allowing autonomous AI actors to make decisions based on background, goals, and emotions, and interact with scene props.

Result: Experimental evaluation shows HAMLET creates expressive and coherent theatrical experiences, assessed by character performance, narrative quality, and interaction experience.

Conclusion: HAMLET advances interactive narrative by enabling dynamic, autonomous AI-driven drama performances.

Abstract: Creating an immersive and interactive theatrical experience is a long-term
goal in the field of interactive narrative. The emergence of large language
model (LLM) is providing a new path to achieve this goal. However, existing
LLM-based drama generation methods often result in AI agents that lack
initiative and cannot interact with the physical environment. Furthermore,
these methods typically require detailed user input to drive the drama. These
limitations reduce the interactivity and immersion of online real-time
performance. To address the above challenges, we propose HAMLET, a multi-agent
framework focused on drama creation and online performance. Given a simple
topic, the framework generates a narrative blueprint, guiding the subsequent
improvisational performance. During the online performance, each actor is given
an autonomous mind. This means that actors can make independent decisions based
on their own background, goals, and emotional state. In addition to
conversations with other actors, their decisions can also change the state of
scene props through actions such as opening a letter or picking up a weapon.
The change is then broadcast to other related actors, updating what they know
and care about, which in turn influences their next action. To evaluate the
quality of drama performance, we designed an evaluation method to assess three
primary aspects, including character performance, narrative quality, and
interaction experience. The experimental evaluation shows that HAMLET can
create expressive and coherent theatrical experiences. Our code, dataset and
models are available at https://github.com/HAMLET-2025/HAMLET.

</details>


### [186] [LLM world models are mental: Output layer evidence of brittle world model use in LLM mechanical reasoning](https://arxiv.org/abs/2507.15521)
*Cole Robertson,Philip Wolff*

Main category: cs.AI

TL;DR: The paper investigates whether LLMs use internal world models or rely on statistical associations. Cognitive science methods were adapted to test LLMs on pulley system problems, revealing partial but not full world-modeling capabilities.


<details>
  <summary>Details</summary>
Motivation: To determine if LLMs construct internal world models or depend solely on statistical associations, using pulley system problems as a test case.

Method: Adapted cognitive science methodologies to test LLMs on TikZ-rendered pulley system stimuli across three studies: estimating mechanical advantage (MA), identifying functional systems, and comparing nuanced structural connectivity.

Result: LLMs showed marginal but significant performance above chance in estimating MA (Study 1) and identifying functional systems (Study 2), but struggled with nuanced structural reasoning (Study 3).

Conclusion: LLMs may manipulate internal world models to some extent but lack nuanced reasoning. Cognitive scientific methods are valuable for evaluating AI world-modeling capacities.

Abstract: Do large language models (LLMs) construct and manipulate internal world
models, or do they rely solely on statistical associations represented as
output layer token probabilities? We adapt cognitive science methodologies from
human mental models research to test LLMs on pulley system problems using
TikZ-rendered stimuli. Study 1 examines whether LLMs can estimate mechanical
advantage (MA). State-of-the-art models performed marginally but significantly
above chance, and their estimates correlated significantly with ground-truth
MA. Significant correlations between number of pulleys and model estimates
suggest that models employed a pulley counting heuristic, without necessarily
simulating pulley systems to derive precise values. Study 2 tested this by
probing whether LLMs represent global features crucial to MA estimation. Models
evaluated a functionally connected pulley system against a fake system with
randomly placed components. Without explicit cues, models identified the
functional system as having greater MA with F1=0.8, suggesting LLMs could
represent systems well enough to differentiate jumbled from functional systems.
Study 3 built on this by asking LLMs to compare functional systems with matched
systems which were connected up but which transferred no force to the weight;
LLMs identified the functional system with F1=0.46, suggesting random guessing.
Insofar as they may generalize, these findings are compatible with the notion
that LLMs manipulate internal world models, sufficient to exploit statistical
associations between pulley count and MA (Study 1), and to approximately
represent system components' spatial relations (Study 2). However, they may
lack the facility to reason over nuanced structural connectivity (Study 3). We
conclude by advocating the utility of cognitive scientific methods to evaluate
the world-modeling capacities of artificial intelligence systems.

</details>


### [187] [Data-Efficient Safe Policy Improvement Using Parametric Structure](https://arxiv.org/abs/2507.15532)
*Kasper Engelen,Guillermo A. Pérez,Marnix Suilen*

Main category: cs.AI

TL;DR: The paper introduces methods to improve data efficiency in Safe Policy Improvement (SPI) by leveraging parametric dependencies and preprocessing techniques.


<details>
  <summary>Details</summary>
Motivation: SPI aims to compute a new policy that reliably outperforms the behavior policy using only a dataset. Existing methods lack efficiency when additional parametric dependencies in transition dynamics are available.

Method: Three contributions: (1) a parametric SPI algorithm exploiting correlations, (2) action pruning via game-based abstraction, and (3) advanced pruning using SMT solving.

Result: Empirical results show increased data efficiency by orders of magnitude while maintaining reliability.

Conclusion: The proposed techniques significantly enhance SPI's data efficiency without compromising reliability.

Abstract: Safe policy improvement (SPI) is an offline reinforcement learning problem in
which a new policy that reliably outperforms the behavior policy with high
confidence needs to be computed using only a dataset and the behavior policy.
Markov decision processes (MDPs) are the standard formalism for modeling
environments in SPI. In many applications, additional information in the form
of parametric dependencies between distributions in the transition dynamics is
available. We make SPI more data-efficient by leveraging these dependencies
through three contributions: (1) a parametric SPI algorithm that exploits known
correlations between distributions to more accurately estimate the transition
dynamics using the same amount of data; (2) a preprocessing technique that
prunes redundant actions from the environment through a game-based abstraction;
and (3) a more advanced preprocessing technique, based on satisfiability modulo
theory (SMT) solving, that can identify more actions to prune. Empirical
results and an ablation study show that our techniques increase the data
efficiency of SPI by multiple orders of magnitude while maintaining the same
reliability guarantees.

</details>


### [188] [Metric assessment protocol in the context of answer fluctuation on MCQ tasks](https://arxiv.org/abs/2507.15581)
*Ekaterina Goliakova,Xavier Renard,Marie-Jeanne Lesot,Thibault Laugel,Christophe Marsala,Marcin Detyniecki*

Main category: cs.AI

TL;DR: The paper evaluates metrics for assessing LLM capabilities via MCQs, highlighting answer fluctuation issues and proposing a new metric, worst accuracy, which shows strong correlation with fluctuation rates.


<details>
  <summary>Details</summary>
Motivation: To address the lack of thorough assessment of metrics for MCQ evaluations in LLMs and the issue of answer fluctuation due to prompt variations.

Method: Proposes a metric assessment protocol analyzing evaluation methodologies based on their connection with fluctuation rates and original performance.

Result: Existing metrics strongly correlate with answer fluctuation; worst accuracy shows the highest association.

Conclusion: Worst accuracy is a promising metric for robust MCQ evaluation in LLMs, addressing prompt-induced fluctuations.

Abstract: Using multiple-choice questions (MCQs) has become a standard for assessing
LLM capabilities efficiently. A variety of metrics can be employed for this
task. However, previous research has not conducted a thorough assessment of
them. At the same time, MCQ evaluation suffers from answer fluctuation: models
produce different results given slight changes in prompts. We suggest a metric
assessment protocol in which evaluation methodologies are analyzed through
their connection with fluctuation rates, as well as original performance. Our
results show that there is a strong link between existing metrics and the
answer changing, even when computed without any additional prompt variants. A
novel metric, worst accuracy, demonstrates the highest association on the
protocol.

</details>


### [189] [TacticCraft: Natural Language-Driven Tactical Adaptation for StarCraft II](https://arxiv.org/abs/2507.15618)
*Weiyu Ma,Jiwen Jiang,Haobo Fu,Haifeng Zhang*

Main category: cs.AI

TL;DR: An adapter-based method for tactical conditioning of StarCraft II AI agents, enabling strategic adaptability without retraining the core policy.


<details>
  <summary>Details</summary>
Motivation: Current AI agents lack adaptability to high-level tactical directives, limiting strategic flexibility.

Method: Freezes a pre-trained policy (DI-Star) and adds lightweight adapter modules to action heads, conditioned on tactical tensors. Trained with KL divergence constraints to maintain core skills.

Result: Successfully modulates agent behavior in aggression, expansion, and tech preferences while staying competitive.

Conclusion: Offers flexible tactical control with low computational cost, useful for real-time strategy games.

Abstract: We present an adapter-based approach for tactical conditioning of StarCraft
II AI agents. Current agents, while powerful, lack the ability to adapt their
strategies based on high-level tactical directives. Our method freezes a
pre-trained policy network (DI-Star) and attaches lightweight adapter modules
to each action head, conditioned on a tactical tensor that encodes strategic
preferences. By training these adapters with KL divergence constraints, we
ensure the policy maintains core competencies while exhibiting tactical
variations. Experimental results show our approach successfully modulates agent
behavior across tactical dimensions including aggression, expansion patterns,
and technology preferences, while maintaining competitive performance. Our
method enables flexible tactical control with minimal computational overhead,
offering practical strategy customization for complex real-time strategy games.

</details>


### [190] [Agentic AI for autonomous anomaly management in complex systems](https://arxiv.org/abs/2507.15676)
*Reza Vatankhah Barenji,Sina Khoshgoftar*

Main category: cs.AI

TL;DR: Agentic AI can autonomously detect and respond to anomalies in complex systems, reducing reliance on human intervention.


<details>
  <summary>Details</summary>
Motivation: To improve anomaly management by leveraging AI to reduce human dependency and enhance efficiency.

Method: Utilizes agentic AI for autonomous anomaly detection and response in complex systems.

Result: Demonstrates the capability of agentic AI to transform traditional anomaly management.

Conclusion: Agentic AI offers a promising approach to modernize and automate anomaly management in complex systems.

Abstract: This paper explores the potential of agentic AI in autonomously detecting and
responding to anomalies within complex systems, emphasizing its ability to
transform traditional, human-dependent anomaly management methods.

</details>


### [191] [Towards physician-centered oversight of conversational diagnostic AI](https://arxiv.org/abs/2507.15743)
*Elahe Vedadi,David Barrett,Natalie Harris,Ellery Wulczyn,Shashir Reddy,Roma Ruparel,Mike Schaekermann,Tim Strother,Ryutaro Tanno,Yash Sharma,Jihyeon Lee,Cían Hughes,Dylan Slack,Anil Palepu,Jan Freyberg,Khaled Saab,Valentin Liévin,Wei-Hung Weng,Tao Tu,Yun Liu,Nenad Tomasev,Kavita Kulkarni,S. Sara Mahdavi,Kelvin Guu,Joëlle Barral,Dale R. Webster,James Manyika,Avinatan Hassidim,Katherine Chou,Yossi Matias,Pushmeet Kohli,Adam Rodman,Vivek Natarajan,Alan Karthikesalingam,David Stutz*

Main category: cs.AI

TL;DR: The paper proposes g-AMIE, a multi-agent AI system for medical diagnostics with asynchronous oversight by physicians, showing improved efficiency and decision quality compared to human clinicians under guardrails.


<details>
  <summary>Details</summary>
Motivation: To address the need for patient safety and regulatory compliance in AI-driven diagnostic dialogue, while leveraging physician oversight for accountability.

Method: Developed g-AMIE, a system that performs history-taking within guardrails and conveys assessments to overseeing physicians via a clinician cockpit. Tested in a randomized, blinded virtual OSCE with 60 scenarios.

Result: g-AMIE outperformed NPs/PAs and PCPs in intake quality, case summarization, and proposing diagnoses/management plans, leading to higher-quality decisions and time efficiency.

Conclusion: Asynchronous oversight by physicians is a feasible paradigm for AI diagnostic systems, enhancing real-world care without replacing clinician accountability.

Abstract: Recent work has demonstrated the promise of conversational AI systems for
diagnostic dialogue. However, real-world assurance of patient safety means that
providing individual diagnoses and treatment plans is considered a regulated
activity by licensed professionals. Furthermore, physicians commonly oversee
other team members in such activities, including nurse practitioners (NPs) or
physician assistants/associates (PAs). Inspired by this, we propose a framework
for effective, asynchronous oversight of the Articulate Medical Intelligence
Explorer (AMIE) AI system. We propose guardrailed-AMIE (g-AMIE), a multi-agent
system that performs history taking within guardrails, abstaining from
individualized medical advice. Afterwards, g-AMIE conveys assessments to an
overseeing primary care physician (PCP) in a clinician cockpit interface. The
PCP provides oversight and retains accountability of the clinical decision.
This effectively decouples oversight from intake and can thus happen
asynchronously. In a randomized, blinded virtual Objective Structured Clinical
Examination (OSCE) of text consultations with asynchronous oversight, we
compared g-AMIE to NPs/PAs or a group of PCPs under the same guardrails. Across
60 scenarios, g-AMIE outperformed both groups in performing high-quality
intake, summarizing cases, and proposing diagnoses and management plans for the
overseeing PCP to review. This resulted in higher quality composite decisions.
PCP oversight of g-AMIE was also more time-efficient than standalone PCP
consultations in prior work. While our study does not replicate existing
clinical practices and likely underestimates clinicians' capabilities, our
results demonstrate the promise of asynchronous oversight as a feasible
paradigm for diagnostic AI systems to operate under expert human oversight for
enhancing real-world care.

</details>


### [192] [LAPO: Internalizing Reasoning Efficiency via Length-Adaptive Policy Optimization](https://arxiv.org/abs/2507.15758)
*Xingyu Wu,Yuchen Yan,Shangke Lyu,Linjuan Wu,Yiwen Qiu,Yongliang Shen,Weiming Lu,Jian Shao,Jun Xiao,Yueting Zhuang*

Main category: cs.AI

TL;DR: LAPO is a framework that optimizes reasoning length in models, reducing token usage by 40.9% and improving accuracy by 2.3%.


<details>
  <summary>Details</summary>
Motivation: Large reasoning models generate excessive tokens for simple problems, needing a solution to internalize reasoning depth control.

Method: Uses a two-stage reinforcement learning process: first learns natural reasoning patterns, then embeds them as meta-cognitive guidance.

Result: Reduces token usage by 40.9% and improves accuracy by 2.3% on mathematical reasoning benchmarks.

Conclusion: LAPO enables efficient reasoning without quality loss, adapting computational resources to problem complexity.

Abstract: Large reasoning models have achieved remarkable performance through extended
chain-of-thought sequences, yet this computational freedom leads to excessive
token generation even for simple problems. We present Length-Adaptive Policy
Optimization (LAPO), a novel framework that transforms reasoning length control
from an external constraint into an intrinsic model capability. Unlike existing
approaches that impose rigid limits or rely on post-hoc interventions, LAPO
enables models to internalize an understanding of appropriate reasoning depth
through a two-stage reinforcement learning process. In the first stage, models
learn natural reasoning patterns by discovering the statistical distribution of
successful solution lengths. The second stage leverages these patterns as
meta-cognitive guidance, embedding them directly within the model's reasoning
context to ensure inference-time flexibility. Experiments on mathematical
reasoning benchmarks demonstrate that LAPO reduces token usage by up to 40.9\%
while improving accuracy by 2.3\%. Our analysis reveals that models trained
with LAPO develop emergent abilities to allocate computational resources based
on problem complexity, achieving efficient reasoning without sacrificing
quality.

</details>


### [193] [GasAgent: A Multi-Agent Framework for Automated Gas Optimization in Smart Contracts](https://arxiv.org/abs/2507.15761)
*Jingyi Zheng,Zifan Peng,Yule Liu,Junfeng Wang,Yifan Liao,Wenhan Dong,Xinlei He*

Main category: cs.AI

TL;DR: GasAgent is a multi-agent system for optimizing Gas waste in smart contracts, combining compatibility with existing patterns and automated discovery/validation of new patterns. It achieves significant Gas savings and works well with LLM-generated contracts.


<details>
  <summary>Details</summary>
Motivation: Existing solutions for Gas waste in smart contracts are inefficient, costly, and hard to scale. Manual discovery and LLM-based approaches face compatibility and redundancy issues.

Method: GasAgent uses four specialized agents (Seeker, Innovator, Executor, Manager) to collaboratively identify, validate, and apply Gas-saving improvements in a closed loop.

Result: GasAgent optimizes 82 out of 100 real-world contracts (9.97% average Gas savings) and 79.8% of 500 LLM-generated contracts (4.79%-13.93% savings).

Conclusion: GasAgent effectively addresses the limitations of existing methods, offering scalable, automated, and compatible Gas optimization for smart contracts.

Abstract: Smart contracts are trustworthy, immutable, and automatically executed
programs on the blockchain. Their execution requires the Gas mechanism to
ensure efficiency and fairness. However, due to non-optimal coding practices,
many contracts contain Gas waste patterns that need to be optimized. Existing
solutions mostly rely on manual discovery, which is inefficient, costly to
maintain, and difficult to scale. Recent research uses large language models
(LLMs) to explore new Gas waste patterns. However, it struggles to remain
compatible with existing patterns, often produces redundant patterns, and
requires manual validation/rewriting. To address this gap, we present GasAgent,
the first multi-agent system for smart contract Gas optimization that combines
compatibility with existing patterns and automated discovery/validation of new
patterns, enabling end-to-end optimization. GasAgent consists of four
specialized agents, Seeker, Innovator, Executor, and Manager, that collaborate
in a closed loop to identify, validate, and apply Gas-saving improvements.
Experiments on 100 verified real-world contracts demonstrate that GasAgent
successfully optimizes 82 contracts, achieving an average deployment Gas
savings of 9.97%. In addition, our evaluation confirms its compatibility with
existing tools and validates the effectiveness of each module through ablation
studies. To assess broader usability, we further evaluate 500 contracts
generated by five representative LLMs across 10 categories and find that
GasAgent optimizes 79.8% of them, with deployment Gas savings ranging from
4.79% to 13.93%, showing its usability as the optimization layer for
LLM-assisted smart contract development.

</details>


### [194] [A Framework for Analyzing Abnormal Emergence in Service Ecosystems Through LLM-based Agent Intention Mining](https://arxiv.org/abs/2507.15770)
*Yifan Shen,Zihan Zhao,Xiao Xue,Yuwei Guo,Qun Ma,Deyu Zhou,Ming Zhang*

Main category: cs.AI

TL;DR: The paper introduces EAMI, a framework for dynamic and interpretable emergence analysis in complex service ecosystems using multi-agent intention tracking and clustering.


<details>
  <summary>Details</summary>
Motivation: The complexity of service ecosystems and limitations of traditional causal methods in analyzing abnormal emergence motivate the need for a new approach.

Method: EAMI uses a dual-perspective thought track mechanism (Inspector and Analysis Agents) and k-means clustering to identify phase transitions in group intentions, visualized via an Intention Temporal Emergence diagram.

Result: Experiments in O2O service systems and Stanford AI Town validate EAMI's effectiveness, generalizability, and efficiency.

Conclusion: EAMI offers a novel paradigm for analyzing abnormal emergence and causality in service ecosystems, with code publicly available.

Abstract: With the rise of service computing, cloud computing, and IoT, service
ecosystems are becoming increasingly complex. The intricate interactions among
intelligent agents make abnormal emergence analysis challenging, as traditional
causal methods focus on individual trajectories. Large language models offer
new possibilities for Agent-Based Modeling (ABM) through Chain-of-Thought (CoT)
reasoning to reveal agent intentions. However, existing approaches remain
limited to microscopic and static analysis. This paper introduces a framework:
Emergence Analysis based on Multi-Agent Intention (EAMI), which enables dynamic
and interpretable emergence analysis. EAMI first employs a dual-perspective
thought track mechanism, where an Inspector Agent and an Analysis Agent extract
agent intentions under bounded and perfect rationality. Then, k-means
clustering identifies phase transition points in group intentions, followed by
a Intention Temporal Emergence diagram for dynamic analysis. The experiments
validate EAMI in complex online-to-offline (O2O) service system and the
Stanford AI Town experiment, with ablation studies confirming its
effectiveness, generalizability, and efficiency. This framework provides a
novel paradigm for abnormal emergence and causal analysis in service
ecosystems. The code is available at
https://anonymous.4open.science/r/EAMI-B085.

</details>


### [195] [Challenges of Trustworthy Federated Learning: What's Done, Current Trends and Remaining Work](https://arxiv.org/abs/2507.15796)
*Nuria Rodríguez-Barroso,Mario García-Márquez,M. Victoria Luzón,Francisco Herrera*

Main category: cs.AI

TL;DR: The paper analyzes the challenges of aligning Federated Learning (FL) with Trustworthy AI (TAI) requirements, focusing on privacy, ethics, and technical hurdles.


<details>
  <summary>Details</summary>
Motivation: To address the gap in aligning FL with TAI's ethical, legal, and technical requirements due to FL's distributed nature.

Method: Systematic analysis of FL challenges using TAI requirements as a framework, classifying obstacles and reviewing existing work.

Result: Identified key challenges in FL-TAI alignment, trends in solutions, and gaps requiring further research.

Conclusion: FL's potential for privacy-preserving AI is clear, but significant work remains to fully meet TAI standards.

Abstract: In recent years, the development of Trustworthy Artificial Intelligence (TAI)
has emerged as a critical objective in the deployment of AI systems across
sensitive and high-risk domains. TAI frameworks articulate a comprehensive set
of ethical, legal, and technical requirements to ensure that AI technologies
are aligned with human values, rights, and societal expectations. Among the
various AI paradigms, Federated Learning (FL) presents a promising solution to
pressing privacy concerns. However, aligning FL with the rest of the
requirements of TAI presents a series of challenges, most of which arise from
its inherently distributed nature. In this work, we adopt the requirements TAI
as a guiding structure to systematically analyze the challenges of adapting FL
to TAI. Specifically, we classify and examine the key obstacles to aligning FL
with TAI, providing a detailed exploration of what has been done, the trends,
and the remaining work within each of the identified challenges.

</details>


### [196] [Identifying Conditional Causal Effects in MPDAGs](https://arxiv.org/abs/2507.15842)
*Sara LaPlante,Emilija Perković*

Main category: cs.AI

TL;DR: The paper addresses identifying conditional causal effects in graphs known up to a maximally oriented partially directed acyclic graph (MPDAG). It provides an identification formula, extends do-calculus to MPDAGs, and offers a complete algorithm for such effects.


<details>
  <summary>Details</summary>
Motivation: To tackle causal effect identification when the causal graph is partially known (MPDAG) and all variables are observed, extending beyond traditional DAGs.

Method: Three key contributions: an identification formula for unaffected conditioning sets, a generalization of do-calculus for MPDAGs, and a complete algorithm for conditional effects.

Result: Theoretical results and an algorithm enabling conditional causal effect identification in MPDAG settings.

Conclusion: The work advances causal inference by addressing identification challenges in partially known graphs, providing practical tools for researchers.

Abstract: We consider identifying a conditional causal effect when a graph is known up
to a maximally oriented partially directed acyclic graph (MPDAG). An MPDAG
represents an equivalence class of graphs that is restricted by background
knowledge and where all variables in the causal model are observed. We provide
three results that address identification in this setting: an identification
formula when the conditioning set is unaffected by treatment, a generalization
of the well-known do calculus to the MPDAG setting, and an algorithm that is
complete for identifying these conditional effects.

</details>


### [197] [Hierarchical Budget Policy Optimization for Adaptive Reasoning](https://arxiv.org/abs/2507.15844)
*Shangke Lyu,Linjuan Wu,Yuchen Yan,Xingyu Wu,Hao Li,Yongliang Shen,Peisheng Jiang,Weiming Lu,Jun Xiao,Yueting Zhuang*

Main category: cs.AI

TL;DR: HBPO is a reinforcement learning framework that optimizes reasoning efficiency and capability by learning problem-specific reasoning depths, reducing token usage by 60.6% while improving accuracy by 3.14%.


<details>
  <summary>Details</summary>
Motivation: Current large reasoning models use uniform reasoning strategies regardless of problem complexity, leading to computational inefficiency.

Method: HBPO uses hierarchical budget exploration and differentiated reward mechanisms to partition rollout samples into subgroups with distinct token budgets, enabling efficient resource allocation.

Result: HBPO reduces token usage by up to 60.6% and improves accuracy by 3.14% across four reasoning benchmarks.

Conclusion: Reasoning efficiency and capability can be optimized simultaneously through hierarchical training, as demonstrated by HBPO's emergent adaptive behavior.

Abstract: Large reasoning models achieve remarkable performance through extensive
chain-of-thought generation, yet exhibit significant computational inefficiency
by applying uniform reasoning strategies regardless of problem complexity. We
present Hierarchical Budget Policy Optimization (HBPO), a reinforcement
learning framework that enables models to learn problem-specific reasoning
depths without sacrificing capability. HBPO addresses the fundamental challenge
of exploration space collapse in efficiency-oriented training, where penalties
on long output length systematically bias models away from necessary long
reasoning paths. Through hierarchical budget exploration, our approach
partitions rollout samples into multiple subgroups with distinct token budgets,
aiming to enable efficient resource allocation while preventing degradation of
capability. We introduce differentiated reward mechanisms that create
budget-aware incentives aligned with the complexity of the problem, allowing
models to discover natural correspondences between task requirements and
computational effort. Extensive experiments demonstrate that HBPO reduces
average token usage by up to 60.6% while improving accuracy by 3.14% across
four reasoning benchmarks. Unlike existing methods that impose external
constraints or rely on discrete mode selection, HBPO exhibits emergent adaptive
behavior where models automatically adjust reasoning depth based on problem
complexity. Our results suggest that reasoning efficiency and capability are
not inherently conflicting, and can be simultaneously optimized through
appropriately structured hierarchical training that preserves exploration
diversity.

</details>


### [198] [The Other Mind: How Language Models Exhibit Human Temporal Cognition](https://arxiv.org/abs/2507.15851)
*Lingyu Li,Yang Yao,Yixu Wang,Chubo Li,Yan Teng,Yingchun Wang*

Main category: cs.AI

TL;DR: LLMs exhibit human-like temporal cognition, adhering to the Weber-Fechner law, with mechanisms involving temporal-preferential neurons and hierarchical representations. The findings suggest an experientialist perspective for AI alignment.


<details>
  <summary>Details</summary>
Motivation: To investigate spontaneous human-like temporal cognition in LLMs, particularly how they establish subjective temporal reference points and adhere to the Weber-Fechner law.

Method: Used similarity judgment tasks, analyzed temporal-preferential neurons, hierarchical representations, and training corpus structure.

Result: Larger LLMs show logarithmic temporal compression, hierarchical year representations, and reliance on inherent corpus structure.

Conclusion: LLMs' cognition is a subjective construction, suggesting alien cognitive frameworks and a need for AI alignment focused on internal guidance.

Abstract: As Large Language Models (LLMs) continue to advance, they exhibit certain
cognitive patterns similar to those of humans that are not directly specified
in training data. This study investigates this phenomenon by focusing on
temporal cognition in LLMs. Leveraging the similarity judgment task, we find
that larger models spontaneously establish a subjective temporal reference
point and adhere to the Weber-Fechner law, whereby the perceived distance
logarithmically compresses as years recede from this reference point. To
uncover the mechanisms behind this behavior, we conducted multiple analyses
across neuronal, representational, and informational levels. We first identify
a set of temporal-preferential neurons and find that this group exhibits
minimal activation at the subjective reference point and implements a
logarithmic coding scheme convergently found in biological systems. Probing
representations of years reveals a hierarchical construction process, where
years evolve from basic numerical values in shallow layers to abstract temporal
orientation in deep layers. Finally, using pre-trained embedding models, we
found that the training corpus itself possesses an inherent, non-linear
temporal structure, which provides the raw material for the model's internal
construction. In discussion, we propose an experientialist perspective for
understanding these findings, where the LLMs' cognition is viewed as a
subjective construction of the external world by its internal representational
system. This nuanced perspective implies the potential emergence of alien
cognitive frameworks that humans cannot intuitively predict, pointing toward a
direction for AI alignment that focuses on guiding internal constructions. Our
code is available at https://TheOtherMind.github.io.

</details>


### [199] [Gemini 2.5 Pro Capable of Winning Gold at IMO 2025](https://arxiv.org/abs/2507.15855)
*Yichen Huang,Lin F. Yang*

Main category: cs.AI

TL;DR: Gemini 2.5 Pro solves 5 out of 6 IMO 2025 problems with pipeline design and prompt engineering, showcasing effective model usage.


<details>
  <summary>Details</summary>
Motivation: Large Language Models (LLMs) struggle with Olympiad-level math problems despite performing well on other benchmarks.

Method: Used Google's Gemini 2.5 Pro on IMO 2025 problems with pipeline design and prompt engineering, avoiding data contamination.

Result: Solved 5 out of 6 problems correctly, with a caveat.

Conclusion: Optimal usage of powerful models is crucial for solving uniquely challenging problems like the IMO.

Abstract: The International Mathematical Olympiad (IMO) poses uniquely challenging
problems requiring deep insight, creativity, and formal reasoning. While Large
Language Models (LLMs) perform well on mathematical benchmarks like AIME, they
struggle with Olympiad-level tasks. We use Google's Gemini 2.5 Pro on the newly
released IMO 2025 problems, avoiding data contamination. With pipeline design
and prompt engineering, 5 (out of 6) problems are solved correctly (up to a
caveat discussed below), highlighting the importance of finding the optimal way
of using powerful models.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [200] [DeepWriter: A Fact-Grounded Multimodal Writing Assistant Based On Offline Knowledge Base](https://arxiv.org/abs/2507.14189)
*Song Mao,Lejun Cheng,Pinlong Cai,Guohang Yan,Ding Wang,Botian Shi*

Main category: cs.CL

TL;DR: DeepWriter is a customizable, multimodal writing assistant for specialized domains, addressing LLM limitations like hallucination and inconsistent retrieval. It uses a curated offline knowledge base and a novel pipeline to generate high-quality, verifiable documents.


<details>
  <summary>Details</summary>
Motivation: LLMs lack deep domain-specific knowledge and hallucinate, while existing solutions like RAG or online search are inconsistent or unreliable.

Method: DeepWriter employs task decomposition, outline generation, multimodal retrieval, and section-by-section composition with reflection, using a structured offline knowledge base.

Result: DeepWriter outperforms baselines in financial report generation, producing factually accurate, professional-grade documents.

Conclusion: DeepWriter effectively addresses LLM limitations in specialized domains, offering a reliable solution for high-quality document generation.

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
various applications. However, their use as writing assistants in specialized
domains like finance, medicine, and law is often hampered by a lack of deep
domain-specific knowledge and a tendency to hallucinate. Existing solutions,
such as Retrieval-Augmented Generation (RAG), can suffer from inconsistency
across multiple retrieval steps, while online search-based methods often
degrade quality due to unreliable web content. To address these challenges, we
introduce DeepWriter, a customizable, multimodal, long-form writing assistant
that operates on a curated, offline knowledge base. DeepWriter leverages a
novel pipeline that involves task decomposition, outline generation, multimodal
retrieval, and section-by-section composition with reflection. By deeply mining
information from a structured corpus and incorporating both textual and visual
elements, DeepWriter generates coherent, factually grounded, and
professional-grade documents. We also propose a hierarchical knowledge
representation to enhance retrieval efficiency and accuracy. Our experiments on
financial report generation demonstrate that DeepWriter produces high-quality,
verifiable articles that surpasses existing baselines in factual accuracy and
generated content quality.

</details>


### [201] [Retention analysis of edited knowledge after fine-tuning](https://arxiv.org/abs/2507.14198)
*Fufang Wen,Shichang Zhang*

Main category: cs.CL

TL;DR: The paper investigates how fine-tuning affects edited knowledge in LLMs, finding it more prone to forgetting than intrinsic knowledge. It suggests evaluating edit robustness and proposes freezing layers to improve retention.


<details>
  <summary>Details</summary>
Motivation: To understand the interaction between fine-tuning and model editing in LLMs, as the impact of fine-tuning on edited knowledge is unclear.

Method: Systematic investigation of how different fine-tuning objectives interact with various model editing techniques.

Result: Edited knowledge is more susceptible to forgetting during fine-tuning than intrinsic knowledge. Freezing layers with edited content improves retention.

Conclusion: Current editing methods have limitations; evaluating robustness under fine-tuning is crucial. Freezing layers offers a potential solution for more robust editing.

Abstract: Large language models (LLMs) store vast amounts of knowledge, which often
requires updates to correct factual errors, incorporate newly acquired
information, or adapt model behavior. Model editing methods have emerged as
efficient solutions for such updates, offering localized and precise knowledge
modification at significantly lower computational cost than continual training.
In parallel, LLMs are frequently fine-tuned for a wide range of downstream
tasks. However, the effect of fine-tuning on previously edited knowledge
remains poorly understood. In this work, we systematically investigate how
different fine-tuning objectives interact with various model editing
techniques. Our findings show that edited knowledge is substantially more
susceptible to forgetting during fine-tuning than intrinsic knowledge acquired
through pre-training. This analysis highlights a key limitation of current
editing approaches and suggests that evaluating edit robustness under
downstream fine-tuning is critical for their practical deployment. We further
find that freezing layers associated with edited content can significantly
improve knowledge retention, offering insight into how future editing methods
might be made more robust.

</details>


### [202] [Open-Source LLMs Collaboration Beats Closed-Source LLMs: A Scalable Multi-Agent System](https://arxiv.org/abs/2507.14200)
*Shengji Tang,Jianjian Cao,Weihao Lin,Jiale Hong,Bo Zhang,Shuyue Hu,Lei Bai,Tao Chen,Wanli Ouyang,Peng Ye*

Main category: cs.CL

TL;DR: SMACS, a scalable multi-agent collaboration system, integrates open-source LLMs to outperform closed-source models like Claude-3.7-Sonnet and GPT-4.1, achieving higher performance across tasks.


<details>
  <summary>Details</summary>
Motivation: To explore whether open-source LLMs can match or surpass closed-source models by leveraging collective intelligence.

Method: Proposes Retrieval-based Prior Selection (RPS) for LLM selection and Exploration-Exploitation-Driven Posterior Enhancement (EPE) for diverse, high-quality responses.

Result: SMACS outperforms leading closed-source LLMs by significant margins (e.g., +12.73% over Claude-3.7-Sonnet) and exceeds average best results from both open and closed-source models.

Conclusion: SMACS demonstrates the potential of open-source LLM collectives to surpass closed-source models, pushing the boundaries of AI performance.

Abstract: This paper aims to demonstrate the potential and strengths of open-source
collectives. It leads to a promising question: Can we harness multiple
open-source LLMs to match or even beat the closed-source LLMs? To answer this,
we propose SMACS, a scalable multi-agent collaboration system (MACS) framework
with high performance. Specifically, for continuous integration of new LLMs and
generalization to diverse questions, we first propose a Retrieval-based Prior
Selection (RPS), which assigns a proxy performance score to each LLM to select
the Top-k LLMs at the instance level for any given question. Then, we propose
an Exploration-Exploitation-Driven Posterior Enhancement (EPE), encouraging the
generation of diverse responses through prior dropping and selecting the
high-quality response via a hybrid posterior score. Experiments on eight
mainstream benchmarks validate the effectiveness of our SMACS: by integrating
fifteen open-source LLMs, SMACS outperforms leading closed-source LLMs in 2025,
e.g., Claude-3.7-Sonnet (+12.73%), GPT-4.1(+5.36%) and GPT-o3-mini(+5.28%)
across multiple tasks. Remarkably, it even exceeds the average of best results
of different datasets from both open-source LLMs (+2.86%) and closed-source
LLMs (+2.04%), pushing the upper bound of intelligence. Code will be released
at https://github.com/magent4aci/SMACS.

</details>


### [203] [Let's Measure the Elephant in the Room: Facilitating Personalized Automated Analysis of Privacy Policies at Scale](https://arxiv.org/abs/2507.14214)
*Rui Zhao,Vladyslav Melnychuk,Jun Zhao,Jesse Wright,Nigel Shadbolt*

Main category: cs.CL

TL;DR: PoliAnalyzer is a neuro-symbolic system using NLP and logical inference to analyze privacy policies against user preferences, reducing cognitive burden by highlighting non-compliant segments.


<details>
  <summary>Details</summary>
Motivation: Users rarely read privacy policies despite their importance, leading to a lack of control over personal data. PoliAnalyzer aims to automate and personalize policy analysis.

Method: Extends a formal policy language, uses NLP for policy text analysis, and applies logical inference to compare policies with user preferences. Evaluated using PolicyIE dataset and 23 user profiles.

Result: Achieved 90-100% F1-score in identifying data practices. Found 95.2% of policy segments compliant, highlighting 4.8% violations (e.g., location data sharing).

Conclusion: PoliAnalyzer enables scalable, automated privacy policy analysis, empowering users and fostering discussions on fair data practices.

Abstract: In modern times, people have numerous online accounts, but they rarely read
the Terms of Service or Privacy Policy of those sites despite claiming
otherwise. This paper introduces PoliAnalyzer, a neuro-symbolic system that
assists users with personalized privacy policy analysis. PoliAnalyzer uses
Natural Language Processing (NLP) to extract formal representations of data
usage practices from policy texts. In favor of deterministic, logical inference
is applied to compare user preferences with the formal privacy policy
representation and produce a compliance report. To achieve this, we extend an
existing formal Data Terms of Use policy language to model privacy policies as
app policies and user preferences as data policies. In our evaluation using our
enriched PolicyIE dataset curated by legal experts, PoliAnalyzer demonstrated
high accuracy in identifying relevant data usage practices, achieving F1-score
of 90-100% across most tasks. Additionally, we demonstrate how PoliAnalyzer can
model diverse user data-sharing preferences, derived from prior research as 23
user profiles, and perform compliance analysis against the top 100 most-visited
websites. This analysis revealed that, on average, 95.2% of a privacy policy's
segments do not conflict with the analyzed user preferences, enabling users to
concentrate on understanding the 4.8% (636 / 13205) that violates preferences,
significantly reducing cognitive burden. Further, we identified common
practices in privacy policies that violate user expectations - such as the
sharing of location data with 3rd parties. This paper demonstrates that
PoliAnalyzer can support automated personalized privacy policy analysis at
scale using off-the-shelf NLP tools. This sheds light on a pathway to help
individuals regain control over their data and encourage societal discussions
on platform data practices to promote a fairer power dynamic.

</details>


### [204] [Beyond Architectures: Evaluating the Role of Contextual Embeddings in Detecting Bipolar Disorder on Social Media](https://arxiv.org/abs/2507.14231)
*Khalid Hasan,Jamil Saquer*

Main category: cs.CL

TL;DR: The paper evaluates NLP models for detecting bipolar disorder from social media text, finding RoBERTa and BERT-embedded LSTMs perform best, while static embeddings fail. DistilBERT balances efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: Bipolar disorder is often underdiagnosed due to subtle symptoms and stigma. The study aims to leverage NLP for early detection using social media data.

Method: Evaluated transformer models (BERT, RoBERTa, ALBERT, ELECTRA, DistilBERT) and LSTM models with contextualized (BERT) and static (GloVe, Word2Vec) embeddings on annotated Reddit posts.

Result: RoBERTa achieved ~98% F1 score; BERT-embedded LSTMs performed similarly. Static embeddings scored near-zero. DistilBERT balanced speed and accuracy.

Conclusion: Contextual language models are crucial for bipolar disorder detection. The study provides insights for model selection in mental health NLP applications.

Abstract: Bipolar disorder is a chronic mental illness frequently underdiagnosed due to
subtle early symptoms and social stigma. This paper explores the advanced
natural language processing (NLP) models for recognizing signs of bipolar
disorder based on user-generated social media text. We conduct a comprehensive
evaluation of transformer-based models (BERT, RoBERTa, ALBERT, ELECTRA,
DistilBERT) and Long Short Term Memory (LSTM) models based on contextualized
(BERT) and static (GloVe, Word2Vec) word embeddings. Experiments were performed
on a large, annotated dataset of Reddit posts after confirming their validity
through sentiment variance and judgmental analysis. Our results demonstrate
that RoBERTa achieves the highest performance among transformer models with an
F1 score of ~98% while LSTM models using BERT embeddings yield nearly identical
results. In contrast, LSTMs trained on static embeddings fail to capture
meaningful patterns, scoring near-zero F1. These findings underscore the
critical role of contextual language modeling in detecting bipolar disorder. In
addition, we report model training times and highlight that DistilBERT offers
an optimal balance between efficiency and accuracy. In general, our study
offers actionable insights for model selection in mental health NLP
applications and validates the potential of contextualized language models to
support early bipolar disorder screening.

</details>


### [205] [Language Models Change Facts Based on the Way You Talk](https://arxiv.org/abs/2507.14238)
*Matthew Kearney,Reuben Binns,Yarin Gal*

Main category: cs.CL

TL;DR: LLMs infer identity from text and show bias in high-stakes applications like medicine, law, and job salaries, leading to harmful disparities.


<details>
  <summary>Details</summary>
Motivation: To investigate how LLMs use identity markers in decision-making across critical domains.

Method: Comprehensive analysis of LLM responses in five high-stakes applications, testing bias based on race, gender, and age.

Result: LLMs exhibit significant bias, varying care standards, political alignment, and salary recommendations by identity.

Conclusion: Off-the-shelf LLMs may cause harm; thorough bias assessments are needed before deployment.

Abstract: Large language models (LLMs) are increasingly being used in user-facing
applications, from providing medical consultations to job interview advice.
Recent research suggests that these models are becoming increasingly proficient
at inferring identity information about the author of a piece of text from
linguistic patterns as subtle as the choice of a few words. However, little is
known about how LLMs use this information in their decision-making in
real-world applications. We perform the first comprehensive analysis of how
identity markers present in a user's writing bias LLM responses across five
different high-stakes LLM applications in the domains of medicine, law,
politics, government benefits, and job salaries. We find that LLMs are
extremely sensitive to markers of identity in user queries and that race,
gender, and age consistently influence LLM responses in these applications. For
instance, when providing medical advice, we find that models apply different
standards of care to individuals of different ethnicities for the same
symptoms; we find that LLMs are more likely to alter answers to align with a
conservative (liberal) political worldview when asked factual questions by
older (younger) individuals; and that LLMs recommend lower salaries for
non-White job applicants and higher salaries for women compared to men. Taken
together, these biases mean that the use of off-the-shelf LLMs for these
applications may cause harmful differences in medical care, foster wage gaps,
and create different political factual realities for people of different
identities. Beyond providing an analysis, we also provide new tools for
evaluating how subtle encoding of identity in users' language choices impacts
model decisions. Given the serious implications of these findings, we recommend
that similar thorough assessments of LLM use in user-facing applications are
conducted before future deployment.

</details>


### [206] [CCL-XCoT: An Efficient Cross-Lingual Knowledge Transfer Method for Mitigating Hallucination Generation](https://arxiv.org/abs/2507.14239)
*Weihua Zheng,Roy Ka-Wei Lee,Zhengyuan Liu,Kui Wu,AiTi Aw,Bowei Zou*

Main category: cs.CL

TL;DR: CCL-XCoT, a two-stage fine-tuning framework, reduces hallucinations in Multilingual Large Language Models (MLLMs) by 62% using curriculum-based contrastive learning and cross-lingual Chain-of-Thought prompting.


<details>
  <summary>Details</summary>
Motivation: MLLMs suffer from hallucinations in low-resource languages due to training data imbalances, especially in domain-specific tasks.

Method: A two-stage approach: 1) curriculum-based contrastive learning for semantic alignment, 2) cross-lingual Chain-of-Thought prompting during fine-tuning.

Result: Reduces hallucination rates by up to 62% and improves factual knowledge transfer across languages.

Conclusion: CCL-XCoT effectively mitigates hallucinations in MLLMs without external tools, enhancing cross-lingual performance.

Abstract: Multilingual Large Language Models(MLLMs) demonstrate strong generalization
across languages, yet they remain prone to hallucinations, especially in
low-resource languages, due to training data imbalances. These hallucinations,
which include inaccurate or fabricated outputs, are particularly problematic in
domain-specific generation tasks (Chataigner et al., 2024). To address this
challenge, we propose CCL-XCoT(Curriculum-based Contrastive Learning-based
Cross-lingual Chain-of-Thought), a two-stage fine-tuning framework for
mitigating hallucination in MLLMs. Our approach first enhances cross-lingual
semantic alignment through curriculum-based contrastive learning combined with
next-token prediction during continued pre-training. Building on this
foundation, we then introduce a cross-lingual Chain-of-Thought (XCoT) prompting
strategy during instruction fine-tuning, which guides the model to reason in a
high-resource language before generating answers in the target low-resource
language. Experimental results show that CCL-XCoT reduces hallucination rates
by up to 62% and substantially improves factual knowledge transfer across
language pairs, without relying on external retrieval or multi-model ensembles.

</details>


### [207] [HuggingGraph: Understanding the Supply Chain of LLM Ecosystem](https://arxiv.org/abs/2507.14240)
*Mohammad Shahedur Rahman,Peng Gao,Yuede Ji*

Main category: cs.CL

TL;DR: The paper studies the LLM supply chain, analyzing relationships between models and datasets to detect risks, improve fairness, and ensure compliance.


<details>
  <summary>Details</summary>
Motivation: The growing complexity and resource demands of LLMs, along with inherited vulnerabilities and biases from models and datasets, necessitate understanding their origins and relationships.

Method: A systematic data collection method is designed, and a directed heterogeneous graph (397,376 nodes, 453,469 edges) is built to model model-dataset relationships. Analyses include examining graph structure, roles of datasets, and dynamics.

Result: Key findings: the graph is large, sparse, power-law distributed; has a dense core and fragmented periphery; datasets are pivotal; strong model-dataset interdependence exists; and the graph evolves daily.

Conclusion: Understanding the LLM supply chain through graph modeling helps detect risks, improve fairness, and track ecosystem evolution.

Abstract: Large language models (LLMs) leverage deep learning to process and predict
sequences of words from context, enabling them to perform various NLP tasks,
such as translation, summarization, question answering, and content generation.
However, the growing size and complexity of developing, training, and deploying
advanced LLMs require extensive computational resources and large datasets.
This creates a barrier for users. As a result, platforms that host models and
datasets are widely used. For example, Hugging Face, one of the most popular
platforms, hosted 1.8 million models and 450K datasets by June 2025, with no
sign of slowing down. Since many LLMs are built from base models, pre-trained
models, and external datasets, they can inherit vulnerabilities, biases, or
malicious components from earlier models or datasets. Therefore, it is critical
to understand the origin and development of these components to better detect
potential risks, improve model fairness, and ensure compliance. Motivated by
this, our project aims to study the relationships between models and datasets,
which are core components of the LLM supply chain. First, we design a method to
systematically collect LLM supply chain data. Using this data, we build a
directed heterogeneous graph to model the relationships between models and
datasets, resulting in a structure with 397,376 nodes and 453,469 edges. We
then perform various analyses and uncover several findings, such as: (i) the
LLM supply chain graph is large, sparse, and follows a power-law degree
distribution; (ii) it features a densely connected core and a fragmented
periphery; (iii) datasets play pivotal roles in training; (iv) strong
interdependence exists between models and datasets; and (v) the graph is
dynamic, with daily updates reflecting the ecosystem's ongoing evolution.

</details>


### [208] [Promptomatix: An Automatic Prompt Optimization Framework for Large Language Models](https://arxiv.org/abs/2507.14241)
*Rithesh Murthy,Ming Zhu,Liangwei Yang,Jielin Qiu,Juntao Tan,Shelby Heinecke,Huan Wang,Caiming Xiong,Silvio Savarese*

Main category: cs.CL

TL;DR: Promptomatix automates prompt optimization for LLMs, improving performance without manual tuning or expertise.


<details>
  <summary>Details</summary>
Motivation: Manual prompt engineering is inconsistent and inaccessible to non-experts, limiting LLM performance.

Method: Uses a meta-prompt-based optimizer and DSPy-powered compiler to analyze intent, generate synthetic data, and refine prompts.

Result: Achieves competitive or superior performance across 5 task categories, reducing prompt length and computational overhead.

Conclusion: Promptomatix makes prompt optimization scalable, efficient, and accessible.

Abstract: Large Language Models (LLMs) perform best with well-crafted prompts, yet
prompt engineering remains manual, inconsistent, and inaccessible to
non-experts. We introduce Promptomatix, an automatic prompt optimization
framework that transforms natural language task descriptions into high-quality
prompts without requiring manual tuning or domain expertise. Promptomatix
supports both a lightweight meta-prompt-based optimizer and a DSPy-powered
compiler, with modular design enabling future extension to more advanced
frameworks. The system analyzes user intent, generates synthetic training data,
selects prompting strategies, and refines prompts using cost-aware objectives.
Evaluated across 5 task categories, Promptomatix achieves competitive or
superior performance compared to existing libraries, while reducing prompt
length and computational overhead making prompt optimization scalable and
efficient.

</details>


### [209] [In-Depth and In-Breadth: Pre-training Multimodal Language Models Customized for Comprehensive Chart Understanding](https://arxiv.org/abs/2507.14298)
*Wan-Cyuan Fan,Yen-Chun Chen,Mengchen Liu,Alexander Jacobson,Lu Yuan,Leonid Sigal*

Main category: cs.CL

TL;DR: ChartScope is a new LVLM for chart comprehension, addressing limitations of existing methods by using a data generation pipeline and Dual-Path training, and introducing a benchmark for evaluation.


<details>
  <summary>Details</summary>
Motivation: Existing LVLMs for chart comprehension lack generalization across chart types and targeted pre-training for data alignment.

Method: Proposes a data generation pipeline for diverse chart types and a Dual-Path training strategy for data-detail capture and reasoning.

Result: ChartScope improves comprehension across various chart types, validated by the new ChartDQA benchmark.

Conclusion: ChartScope advances chart comprehension with better generalization and data understanding, supported by a new benchmark.

Abstract: Recent methods for customizing Large Vision Language Models (LVLMs) for
domain-specific tasks have shown promising results in scientific chart
comprehension. However, existing approaches face two major limitations: First,
they rely on paired data from only a few chart types, limiting generalization
to wide range of chart types. Secondly, they lack targeted pre-training for
chart-data alignment, which hampers the model's understanding of underlying
data. In this paper, we introduce ChartScope, an LVLM optimized for in-depth
chart comprehension across diverse chart types. We propose an efficient data
generation pipeline that synthesizes paired data for a wide range of chart
types, along with a novel Dual-Path training strategy that enabling the model
to succinctly capture essential data details while preserving robust reasoning
capabilities by incorporating reasoning over the underlying data. Lastly, we
establish ChartDQA, a new benchmark for evaluating not only question-answering
at different levels but also underlying data understanding. Experimental
results demonstrate that ChartScope significantly enhances comprehension on a
wide range of chart types. The code and data are available at
https://davidhalladay.github.io/chartscope_demo.

</details>


### [210] [Aligning Large Language Models to Low-Resource Languages through LLM-Based Selective Translation: A Systematic Study](https://arxiv.org/abs/2507.14304)
*Rakesh Paul,Anusha Kamath,Kanishk Singla,Raviraj Joshi,Utkarsh Vaidya,Sanjay Singh Chauhan,Niranjan Wartikar*

Main category: cs.CL

TL;DR: Selective translation improves multilingual LLM alignment by preserving non-translatable content, outperforming standard translation methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the performance gap in multilingual LLMs for low-resource languages by overcoming data scarcity and preserving critical elements during translation.

Method: LLM-based selective translation, comparing it with vanilla translation, filtering noisy outputs, and mixing translated samples with English data.

Result: Selective translation proves effective for Hindi, outperforming Google Cloud Translation and Llama-3.1-405B.

Conclusion: Selective translation is a practical solution for enhancing multilingual alignment in LLMs, especially for low-resource languages.

Abstract: Multilingual large language models (LLMs) often demonstrate a performance gap
between English and non-English languages, particularly in low-resource
settings. Aligning these models to low-resource languages is essential yet
challenging due to limited high-quality data. While English alignment datasets
are readily available, curating equivalent data in other languages is expensive
and time-consuming. A common workaround is to translate existing English
alignment data; however, standard translation techniques often fail to preserve
critical elements such as code, mathematical expressions, and structured
formats like JSON. In this work, we investigate LLM-based selective
translation, a technique that selectively translates only the translatable
parts of a text while preserving non-translatable content and sentence
structure. We conduct a systematic study to explore key questions around this
approach, including its effectiveness compared to vanilla translation, the
importance of filtering noisy outputs, and the benefits of mixing translated
samples with original English data during alignment. Our experiments focus on
the low-resource Indic language Hindi and compare translations generated by
Google Cloud Translation (GCP) and Llama-3.1-405B. The results highlight the
promise of selective translation as a practical and effective method for
improving multilingual alignment in LLMs.

</details>


### [211] [How LLMs Comprehend Temporal Meaning in Narratives: A Case Study in Cognitive Evaluation of LLMs](https://arxiv.org/abs/2507.14307)
*Karin de Langis,Jong Inn Park,Andreas Schramm,Bin Hu,Khanh Chi Le,Michael Mensink,Ahn Thu Tong,Dongyeop Kang*

Main category: cs.CL

TL;DR: LLMs process linguistic aspect differently from humans, over-relying on prototypicality and struggling with causal reasoning, indicating limited narrative comprehension.


<details>
  <summary>Details</summary>
Motivation: To determine if LLMs process linguistic aspect in narratives similarly to humans or rely on pattern recognition.

Method: Used an Expert-in-the-Loop probing pipeline with targeted experiments to assess semantic and pragmatic processing.

Result: LLMs over-rely on prototypicality, show inconsistent judgments, and struggle with causal reasoning, differing from human processing.

Conclusion: LLMs lack robust narrative understanding, processing aspect fundamentally differently than humans, and a standardized framework for assessment is proposed.

Abstract: Large language models (LLMs) exhibit increasingly sophisticated linguistic
capabilities, yet the extent to which these behaviors reflect human-like
cognition versus advanced pattern recognition remains an open question. In this
study, we investigate how LLMs process the temporal meaning of linguistic
aspect in narratives that were previously used in human studies. Using an
Expert-in-the-Loop probing pipeline, we conduct a series of targeted
experiments to assess whether LLMs construct semantic representations and
pragmatic inferences in a human-like manner. Our findings show that LLMs
over-rely on prototypicality, produce inconsistent aspectual judgments, and
struggle with causal reasoning derived from aspect, raising concerns about
their ability to fully comprehend narratives. These results suggest that LLMs
process aspect fundamentally differently from humans and lack robust narrative
understanding. Beyond these empirical findings, we develop a standardized
experimental framework for the reliable assessment of LLMs' cognitive and
linguistic capabilities.

</details>


### [212] [What Makes You CLIC: Detection of Croatian Clickbait Headlines](https://arxiv.org/abs/2507.14314)
*Marija Anđedelić,Dominik Šipek,Laura Majer,Jan Šnajder*

Main category: cs.CL

TL;DR: The paper examines clickbait detection in Croatian news, comparing fine-tuned BERTić models with LLM-based in-context learning (ICL). It introduces CLIC, a dataset spanning 20 years, and finds fine-tuned models outperform general LLMs.


<details>
  <summary>Details</summary>
Motivation: To improve information quality and reader trust by detecting clickbait in less-resourced languages like Croatian.

Method: Compiled CLIC dataset, fine-tuned BERTić, and compared it with LLM-based ICL methods in Croatian and English.

Result: Nearly half of headlines contained clickbait; fine-tuned models performed better than general LLMs.

Conclusion: Fine-tuned models are more effective for clickbait detection in less-resourced languages like Croatian.

Abstract: Online news outlets operate predominantly on an advertising-based revenue
model, compelling journalists to create headlines that are often scandalous,
intriguing, and provocative -- commonly referred to as clickbait. Automatic
detection of clickbait headlines is essential for preserving information
quality and reader trust in digital media and requires both contextual
understanding and world knowledge. For this task, particularly in
less-resourced languages, it remains unclear whether fine-tuned methods or
in-context learning (ICL) yield better results. In this paper, we compile CLIC,
a novel dataset for clickbait detection of Croatian news headlines spanning a
20-year period and encompassing mainstream and fringe outlets. We fine-tune the
BERTi\'c model on this task and compare its performance to LLM-based ICL
methods with prompts both in Croatian and English. Finally, we analyze the
linguistic properties of clickbait. We find that nearly half of the analyzed
headlines contain clickbait, and that finetuned models deliver better results
than general LLMs.

</details>


### [213] [Can LLMs Infer Personality from Real World Conversations?](https://arxiv.org/abs/2507.14355)
*Jianfeng Zhu,Ruoming Jin,Karin G. Coifman*

Main category: cs.CL

TL;DR: The paper evaluates LLMs for personality assessment using real-world interviews and BFI-10 scores, finding limited validity despite high reliability.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of inferring personality traits from open-ended language using LLMs, given the limitations of prior work relying on synthetic or non-valid data.

Method: Tested three LLMs (GPT-4.1 Mini, Meta-LLaMA, DeepSeek) with zero-shot and chain-of-thought prompting on 555 semi-structured interviews with BFI-10 scores.

Result: Models showed high reliability but weak validity (max Pearson's r = 0.27), low interrater agreement, and bias toward moderate/high traits. Chain-of-thought and longer context improved alignment but not accuracy.

Conclusion: Current LLM-based personality inference has limitations, emphasizing the need for evidence-based development in psychological applications.

Abstract: Large Language Models (LLMs) such as OpenAI's GPT-4 and Meta's LLaMA offer a
promising approach for scalable personality assessment from open-ended
language. However, inferring personality traits remains challenging, and
earlier work often relied on synthetic data or social media text lacking
psychometric validity. We introduce a real-world benchmark of 555
semi-structured interviews with BFI-10 self-report scores for evaluating
LLM-based personality inference. Three state-of-the-art LLMs (GPT-4.1 Mini,
Meta-LLaMA, and DeepSeek) were tested using zero-shot prompting for BFI-10 item
prediction and both zero-shot and chain-of-thought prompting for Big Five trait
inference. All models showed high test-retest reliability, but construct
validity was limited: correlations with ground-truth scores were weak (max
Pearson's $r = 0.27$), interrater agreement was low (Cohen's $\kappa < 0.10$),
and predictions were biased toward moderate or high trait levels.
Chain-of-thought prompting and longer input context modestly improved
distributional alignment, but not trait-level accuracy. These results
underscore limitations in current LLM-based personality inference and highlight
the need for evidence-based development for psychological applications.

</details>


### [214] [Text-to-SQL for Enterprise Data Analytics](https://arxiv.org/abs/2507.14372)
*Albert Chen,Manas Bundele,Gaurav Ahlawat,Patrick Stetz,Zhitao Wang,Qiang Fei,Donghoon Jung,Audrey Chu,Bharadwaj Jayaraman,Ayushi Panth,Yatin Arora,Sourav Jain,Renjith Varma,Alexey Ilin,Iuliia Melnychuk,Chelsea Chueh,Joyan Sil,Xiaofeng Wang*

Main category: cs.CL

TL;DR: The paper presents a practical approach to building an enterprise Text-to-SQL chatbot using a knowledge graph, a Text-to-SQL agent, and an interactive interface, achieving 53% accuracy on an internal benchmark.


<details>
  <summary>Details</summary>
Motivation: Large language models have advanced Text-to-SQL benchmarks, but enterprise solutions remain challenging. The paper aims to bridge this gap by developing a self-serve data insights chatbot for LinkedIn teams.

Method: The approach involves: 1) constructing a dynamic knowledge graph from metadata and logs, 2) building a Text-to-SQL agent for query generation and error correction, and 3) creating an interactive chatbot with rich UI for diverse user intents.

Result: The chatbot has 300+ weekly users and achieves 53% correctness on an internal benchmark. Ablation studies highlight key components for enterprise solutions.

Conclusion: The study offers actionable insights for developing practical enterprise Text-to-SQL systems, emphasizing the importance of dynamic knowledge graphs and interactive interfaces.

Abstract: The introduction of large language models has brought rapid progress on
Text-to-SQL benchmarks, but it is not yet easy to build a working enterprise
solution. In this paper, we present insights from building an internal chatbot
that enables LinkedIn's product managers, engineers, and operations teams to
self-serve data insights from a large, dynamic data lake. Our approach features
three components. First, we construct a knowledge graph that captures
up-to-date semantics by indexing database metadata, historical query logs,
wikis, and code. We apply clustering to identify relevant tables for each team
or product area. Second, we build a Text-to-SQL agent that retrieves and ranks
context from the knowledge graph, writes a query, and automatically corrects
hallucinations and syntax errors. Third, we build an interactive chatbot that
supports various user intents, from data discovery to query writing to
debugging, and displays responses in rich UI elements to encourage follow-up
chats. Our chatbot has over 300 weekly users. Expert review shows that 53% of
its responses are correct or close to correct on an internal benchmark set.
Through ablation studies, we identify the most important knowledge graph and
modeling components, offering a practical path for developing enterprise
Text-to-SQL solutions.

</details>


### [215] [Error-Aware Curriculum Learning for Biomedical Relation Classification](https://arxiv.org/abs/2507.14374)
*Sinchani Chakraborty,Sudeshna Sarkar,Pawan Goyal*

Main category: cs.CL

TL;DR: An error-aware teacher-student framework using GPT-4o improves biomedical relation classification by analyzing errors, generating remediations, and training models progressively.


<details>
  <summary>Details</summary>
Motivation: Enhancing relation classification in biomedical texts for applications like knowledge graph construction and clinical decision-making.

Method: Uses a teacher-student framework with GPT-4o to analyze errors, generate remediations, and train models via instruction tuning and curriculum learning.

Result: Achieves state-of-the-art performance on 4 of 5 PPI datasets and the DDI dataset, remaining competitive on ChemProt.

Conclusion: The proposed framework effectively improves relation classification in biomedical texts through structured guidance and progressive learning.

Abstract: Relation Classification (RC) in biomedical texts is essential for
constructing knowledge graphs and enabling applications such as drug
repurposing and clinical decision-making. We propose an error-aware
teacher--student framework that improves RC through structured guidance from a
large language model (GPT-4o). Prediction failures from a baseline student
model are analyzed by the teacher to classify error types, assign difficulty
scores, and generate targeted remediations, including sentence rewrites and
suggestions for KG-based enrichment. These enriched annotations are used to
train a first student model via instruction tuning. This model then annotates a
broader dataset with difficulty scores and remediation-enhanced inputs. A
second student is subsequently trained via curriculum learning on this dataset,
ordered by difficulty, to promote robust and progressive learning. We also
construct a heterogeneous biomedical knowledge graph from PubMed abstracts to
support context-aware RC. Our approach achieves new state-of-the-art
performance on 4 of 5 PPI datasets and the DDI dataset, while remaining
competitive on ChemProt.

</details>


### [216] [X-Intelligence 3.0: Training and Evaluating Reasoning LLM for Semiconductor Display](https://arxiv.org/abs/2507.14430)
*Xiaolin Yan,Yangxing Liu,Jiazhang Zheng,Chi Liu,Mingyu Du,Caisheng Chen,Haoyang Liu,Ming Ding,Yuan Li,Qiuping Liao,Linfeng Li,Zhili Mei,Siyu Wan,Li Li,Ruyi Zhong,Jiangling Yu,Xule Liu,Huihui Hu,Jiameng Yue,Ruohui Cheng,Qi Yang,Liangqing Wu,Ke Zhu,Chi Zhang,Chufei Jing,Yifan Zhou,Yan Liang,Dongdong Li,Zhaohui Wang,Bin Zhao,Mingzhou Wu,Mingzhong Zhou,Peng Du,Zuomin Liao,Chao Dai,Pengfei Liang,Xiaoguang Zhu,Yu Zhang,Yu Gu,Kun Pan,Yuan Wu,Yanqing Guan,Shaojing Wu,Zikang Feng,Xianze Ma,Peishan Cheng,Wenjuan Jiang,Jing Ba,Huihao Yu,Zeping Hu,Yuan Xu,Zhiwei Liu,He Wang,Zhenguo Lin,Ming Liu,Yanhong Meng*

Main category: cs.CL

TL;DR: X-Intelligence 3.0 is a specialized LLM for the semiconductor display industry, outperforming larger models like DeepSeek-R1-671B despite its smaller size (32B parameters).


<details>
  <summary>Details</summary>
Motivation: Existing LLMs lack domain-specific expertise for the semiconductor display industry, limiting their effectiveness.

Method: The model uses supervised fine-tuning, reinforcement learning, a domain-specific RAG mechanism, and an automated evaluation framework.

Result: X-Intelligence 3.0 outperforms SOTA models like DeepSeek-R1-671B on benchmark datasets.

Conclusion: The model addresses industry-specific reasoning challenges efficiently, proving its value as a tailored solution.

Abstract: Large language models (LLMs) have recently achieved significant advances in
reasoning and demonstrated their advantages in solving challenging problems.
Yet, their effectiveness in the semiconductor display industry remains limited
due to a lack of domain-specific training and expertise. To bridge this gap, we
present X-Intelligence 3.0, the first high-performance reasoning model
specifically developed for the semiconductor display industry. This model is
designed to deliver expert-level understanding and reasoning for the industry's
complex challenges. Leveraging a carefully curated industry knowledge base, the
model undergoes supervised fine-tuning and reinforcement learning to enhance
its reasoning and comprehension capabilities. To further accelerate
development, we implemented an automated evaluation framework that simulates
expert-level assessments. We also integrated a domain-specific
retrieval-augmented generation (RAG) mechanism, resulting in notable
performance gains on benchmark datasets. Despite its relatively compact size of
32 billion parameters, X-Intelligence 3.0 outperforms SOTA DeepSeek-R1-671B
across multiple evaluations. This demonstrates its exceptional efficiency and
establishes it as a powerful solution to the longstanding reasoning challenges
faced by the semiconductor display industry.

</details>


### [217] [XL-DURel: Finetuning Sentence Transformers for Ordinal Word-in-Context Classification](https://arxiv.org/abs/2507.14578)
*Sachin Yadav,Dominik Schlechtweg*

Main category: cs.CL

TL;DR: XL-DURel is a multilingual Sentence Transformer model optimized for ordinal Word-in-Context classification, outperforming previous models with a ranking objective based on angular distance.


<details>
  <summary>Details</summary>
Motivation: To improve performance on ordinal and binary Word-in-Context tasks and unify their treatment.

Method: Finetuned multilingual Sentence Transformer model tested with various loss functions for regression and ranking.

Result: Outperforms previous models on ordinal and binary data; shows binary WiC as a special case of ordinal WiC.

Conclusion: Optimizing for ordinal tasks improves binary task performance, enabling unified WiC modeling.

Abstract: We propose XL-DURel, a finetuned, multilingual Sentence Transformer model
optimized for ordinal Word-in-Context classification. We test several loss
functions for regression and ranking tasks managing to outperform previous
models on ordinal and binary data with a ranking objective based on angular
distance in complex space. We further show that binary WiC can be treated as a
special case of ordinal WiC and that optimizing models for the general ordinal
task improves performance on the more specific binary task. This paves the way
for a unified treatment of WiC modeling across different task formulations.

</details>


### [218] [Exploring Human-AI Complementarity in CPS Diagnosis Using Unimodal and Multimodal BERT Models](https://arxiv.org/abs/2507.14579)
*Kester Wong,Sahan Bulathwela,Mutlu Cukurova*

Main category: cs.CL

TL;DR: The paper explores using multimodal BERT (AudiBERT) to detect CPS indicators, showing significant improvements in social-cognitive dimensions but not affective ones. It emphasizes data size and human-AI collaboration for better results.


<details>
  <summary>Details</summary>
Motivation: To enhance CPS diagnosis by integrating multimodal data (speech and acoustic-prosodic features) and improving human-AI collaboration.

Method: Uses AudiBERT, a multimodal BERT variant, and compares it with BERT on transcription data for CPS indicator detection. Analyzes statistical significance and correlations with training data size and human coder agreement.

Result: AudiBERT showed statistically significant improvements in social-cognitive dimensions but not affective ones. Larger training data improved recall, and human coder agreement improved BERT's precision.

Conclusion: Proposes a structured approach for human-AI complementarity in CPS diagnosis, stressing model explainability to support human agency in coding.

Abstract: Detecting collaborative problem solving (CPS) indicators from dialogue using
machine learning techniques is a significant challenge for the field of AI in
Education. Recent studies have explored the use of Bidirectional Encoder
Representations from Transformers (BERT) models on transcription data to
reliably detect meaningful CPS indicators. A notable advancement involved the
multimodal BERT variant, AudiBERT, which integrates speech and
acoustic-prosodic audio features to enhance CPS diagnosis. Although initial
results demonstrated multimodal improvements, the statistical significance of
these enhancements remained unclear, and there was insufficient guidance on
leveraging human-AI complementarity for CPS diagnosis tasks. This workshop
paper extends the previous research by highlighting that the AudiBERT model not
only improved the classification of classes that were sparse in the dataset,
but it also had statistically significant class-wise improvements over the BERT
model for classifications in the social-cognitive dimension. However, similar
significant class-wise improvements over the BERT model were not observed for
classifications in the affective dimension. A correlation analysis highlighted
that larger training data was significantly associated with higher recall
performance for both the AudiBERT and BERT models. Additionally, the precision
of the BERT model was significantly associated with high inter-rater agreement
among human coders. When employing the BERT model to diagnose indicators within
these subskills that were well-detected by the AudiBERT model, the performance
across all indicators was inconsistent. We conclude the paper by outlining a
structured approach towards achieving human-AI complementarity for CPS
diagnosis, highlighting the crucial inclusion of model explainability to
support human agency and engagement in the reflective coding process.

</details>


### [219] [Explainable Collaborative Problem Solving Diagnosis with BERT using SHAP and its Implications for Teacher Adoption](https://arxiv.org/abs/2507.14584)
*Kester Wong,Sahan Bulathwela,Mutlu Cukurova*

Main category: cs.CL

TL;DR: The study uses SHAP to analyze token contributions in BERT-based CPS classification, finding that good performance doesn't guarantee explainability and identifying spurious word influences.


<details>
  <summary>Details</summary>
Motivation: Enhancing explainability of BERT-based CPS diagnostics to foster trust and adoption in education.

Method: Applied SHAP to examine token contributions in BERT model classifications of CPS processes.

Result: Well-performing classifications lacked reasonable explanations; spurious words influenced decisions.

Conclusion: Model transparency aids in avoiding overreliance on AI, but human-AI collaboration is needed for fine-grained CPS diagnosis.

Abstract: The use of Bidirectional Encoder Representations from Transformers (BERT)
model and its variants for classifying collaborative problem solving (CPS) has
been extensively explored within the AI in Education community. However,
limited attention has been given to understanding how individual tokenised
words in the dataset contribute to the model's classification decisions.
Enhancing the explainability of BERT-based CPS diagnostics is essential to
better inform end users such as teachers, thereby fostering greater trust and
facilitating wider adoption in education. This study undertook a preliminary
step towards model transparency and explainability by using SHapley Additive
exPlanations (SHAP) to examine how different tokenised words in transcription
data contributed to a BERT model's classification of CPS processes. The
findings suggested that well-performing classifications did not necessarily
equate to a reasonable explanation for the classification decisions. Particular
tokenised words were used frequently to affect classifications. The analysis
also identified a spurious word, which contributed positively to the
classification but was not semantically meaningful to the class. While such
model transparency is unlikely to be useful to an end user to improve their
practice, it can help them not to overrely on LLM diagnostics and ignore their
human expertise. We conclude the workshop paper by noting that the extent to
which the model appropriately uses the tokens for its classification is
associated with the number of classes involved. It calls for an investigation
into the exploration of ensemble model architectures and the involvement of
human-AI complementarity for CPS diagnosis, since considerable human reasoning
is still required for fine-grained discrimination of CPS subskills.

</details>


### [220] [Backtranslation and paraphrasing in the LLM era? Comparing data augmentation methods for emotion classification](https://arxiv.org/abs/2507.14590)
*Łukasz Radliński,Mateusz Guściora,Jan Kocoń*

Main category: cs.CL

TL;DR: The paper explores NLP data augmentation using large language models like GPT, comparing traditional methods (paraphrasing, backtranslation) with generative methods. Findings show traditional methods can match or outperform few-shot generation.


<details>
  <summary>Details</summary>
Motivation: Address data scarcity and class imbalance in domain-specific ML tasks by evaluating traditional vs. generative data augmentation methods in NLP.

Method: Compare four data augmentation approaches (including paraphrasing and backtranslation) using ChatGPT and an exemplary dataset, assessing quality and classification impact.

Result: Backtranslation and paraphrasing achieve comparable or better results than zero/few-shot generative methods.

Conclusion: Traditional augmentation methods remain effective and competitive with newer generative approaches in NLP tasks.

Abstract: Numerous domain-specific machine learning tasks struggle with data scarcity
and class imbalance. This paper systematically explores data augmentation
methods for NLP, particularly through large language models like GPT. The
purpose of this paper is to examine and evaluate whether traditional methods
such as paraphrasing and backtranslation can leverage a new generation of
models to achieve comparable performance to purely generative methods. Methods
aimed at solving the problem of data scarcity and utilizing ChatGPT were
chosen, as well as an exemplary dataset. We conducted a series of experiments
comparing four different approaches to data augmentation in multiple
experimental setups. We then evaluated the results both in terms of the quality
of generated data and its impact on classification performance. The key
findings indicate that backtranslation and paraphrasing can yield comparable or
even better results than zero and a few-shot generation of examples.

</details>


### [221] [Retrieval-Augmented Clinical Benchmarking for Contextual Model Testing in Kenyan Primary Care: A Methodology Paper](https://arxiv.org/abs/2507.14615)
*Fred Mutisya,Shikoh Gitau,Christine Syovata,Diana Oigara,Ibrahim Matende,Muna Aden,Munira Ali,Ryan Nyotu,Diana Marion,Job Nyangena,Nasubo Ongoma,Keith Mbae,Elizabeth Wamicha,Eric Mibuari,Jean Philbert Nsengemana,Talkmore Chidede*

Main category: cs.CL

TL;DR: The paper introduces a methodology for evaluating LLMs in African primary care, using a benchmark dataset aligned with Kenyan clinical guidelines and local standards. It highlights performance gaps in LLMs for localized scenarios.


<details>
  <summary>Details</summary>
Motivation: To address the underexplored effectiveness of LLMs in African primary care and improve healthcare access in low-resource settings.

Method: Uses retrieval augmented generation (RAG) to create a benchmark dataset from Kenyan national guidelines, involving physicians for refinement and expert review for accuracy.

Result: Reveals significant performance gaps in LLMs for localized African medical content compared to US benchmarks.

Conclusion: Provides a replicable model for guideline-driven benchmarking to support safe AI deployment in African health systems.

Abstract: Large Language Models(LLMs) hold promise for improving healthcare access in
low-resource settings, but their effectiveness in African primary care remains
underexplored. We present a methodology for creating a benchmark dataset and
evaluation framework focused on Kenyan Level 2 and 3 clinical care. Our
approach uses retrieval augmented generation (RAG) to ground clinical questions
in Kenya's national guidelines, ensuring alignment with local standards. These
guidelines were digitized, chunked, and indexed for semantic retrieval. Gemini
Flash 2.0 Lite was then prompted with guideline excerpts to generate realistic
clinical scenarios, multiple-choice questions, and rationale based answers in
English and Swahili. Kenyan physicians co-created and refined the dataset, and
a blinded expert review process ensured clinical accuracy, clarity, and
cultural appropriateness. The resulting Alama Health QA dataset includes
thousands of regulator-aligned question answer pairs across common outpatient
conditions. Beyond accuracy, we introduce evaluation metrics that test clinical
reasoning, safety, and adaptability such as rare case detection (Needle in the
Haystack), stepwise logic (Decision Points), and contextual adaptability.
Initial results reveal significant performance gaps when LLMs are applied to
localized scenarios, consistent with findings that LLM accuracy is lower on
African medical content than on US-based benchmarks. This work offers a
replicable model for guideline-driven, dynamic benchmarking to support safe AI
deployment in African health systems.

</details>


### [222] [Linear Relational Decoding of Morphology in Language Models](https://arxiv.org/abs/2507.14640)
*Eric Xia,Jugal Kalita*

Main category: cs.CL

TL;DR: A two-part affine approximation effectively approximates transformer computations for certain subject-object relations, achieving 90% faithfulness on morphological relations.


<details>
  <summary>Details</summary>
Motivation: To explore interpretability of conceptual relationships in language models, such as morphology, from latent space.

Method: Adapting the Bigger Analogy Test Set and using linear transformation Ws, where s is a middle layer representation and W is derived from model derivatives.

Result: The linear technique achieves high faithfulness (90%) on morphological relations, with similar results across languages and models.

Conclusion: Some conceptual relationships in language models are sparsely encoded by cross-layer linear transformations and are interpretable from latent space.

Abstract: A two-part affine approximation has been found to be a good approximation for
transformer computations over certain subject object relations. Adapting the
Bigger Analogy Test Set, we show that the linear transformation Ws, where s is
a middle layer representation of a subject token and W is derived from model
derivatives, is also able to accurately reproduce final object states for many
relations. This linear technique is able to achieve 90% faithfulness on
morphological relations, and we show similar findings multi-lingually and
across models. Our findings indicate that some conceptual relationships in
language models, such as morphology, are readily interpretable from latent
space, and are sparsely encoded by cross-layer linear transformations.

</details>


### [223] [Cleanse: Uncertainty Estimation Approach Using Clustering-based Semantic Consistency in LLMs](https://arxiv.org/abs/2507.14649)
*Minsuh Joo,Hyunsoo Cho*

Main category: cs.CL

TL;DR: The paper proposes Cleanse, a clustering-based method to estimate uncertainty in LLM responses to detect hallucinations, validated on four models and two benchmarks.


<details>
  <summary>Details</summary>
Motivation: Hallucinations in LLMs pose safety and reliability risks, necessitating effective uncertainty estimation to distinguish accurate from inaccurate responses.

Method: Cleanse uses clustering to quantify uncertainty by measuring intra-cluster consistency in LLM hidden embeddings, leveraging semantic information.

Result: Validated on LLaMA-7B, LLaMA-13B, LLaMA2-7B, Mistral-7B, and benchmarks SQuAD and CoQA, Cleanse effectively detects hallucinations.

Conclusion: Cleanse provides a reliable approach for uncertainty estimation in LLMs, addressing the critical issue of hallucinations.

Abstract: Despite the outstanding performance of large language models (LLMs) across
various NLP tasks, hallucinations in LLMs--where LLMs generate inaccurate
responses--remains as a critical problem as it can be directly connected to a
crisis of building safe and reliable LLMs. Uncertainty estimation is primarily
used to measure hallucination levels in LLM responses so that correct and
incorrect answers can be distinguished clearly. This study proposes an
effective uncertainty estimation approach, \textbf{Cl}ust\textbf{e}ring-based
sem\textbf{an}tic con\textbf{s}ist\textbf{e}ncy (\textbf{Cleanse}). Cleanse
quantifies the uncertainty with the proportion of the intra-cluster consistency
in the total consistency between LLM hidden embeddings which contain adequate
semantic information of generations, by employing clustering. The effectiveness
of Cleanse for detecting hallucination is validated using four off-the-shelf
models, LLaMA-7B, LLaMA-13B, LLaMA2-7B and Mistral-7B and two
question-answering benchmarks, SQuAD and CoQA.

</details>


### [224] [Mangosteen: An Open Thai Corpus for Language Model Pretraining](https://arxiv.org/abs/2507.14664)
*Wannaphong Phatthiyaphaibun,Can Udomcharoenchaikit,Pakpoom Singkorapoom,Kunat Pipatanakul,Ekapol Chuangsuwanich,Peerat Limkonchotiwat,Sarana Nutanong*

Main category: cs.CL

TL;DR: Mangosteen is a 47B-token Thai corpus built with a Thai-adapted pipeline, improving quality and outperforming existing models on Thai benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing corpora lack Thai-specific handling, leaving noisy or risky content untreated, and prior efforts lack transparency and reproducibility.

Method: Uses a Thai-adapted Dolma pipeline with custom language ID, quality filters, and curated non-web sources.

Result: Pipeline trims CommonCrawl from 202M to 25M docs, improves NLG scores, and an 8B-parameter model outperforms SEA-LION-v3 and Llama-3.1.

Conclusion: Mangosteen provides a transparent, high-quality Thai corpus with full reproducibility for future research.

Abstract: Pre-training data shapes a language model's quality, but raw web text is
noisy and demands careful cleaning. Existing large-scale corpora rely on
English-centric or language-agnostic pipelines whose heuristics do not capture
Thai script or cultural nuances, leaving risky material such as gambling
content untreated. Prior Thai-specific efforts customize pipelines or build new
ones, yet seldom release their data or document design choices, hindering
reproducibility and raising the question of how to construct a transparent,
high-quality Thai corpus. We introduce Mangosteen: a 47 billion-token Thai
corpus built through a Thai-adapted Dolma pipeline that includes custom
rule-based language ID, revised C4/Gopher quality filters, and Thai-trained
content filters, plus curated non-web sources such as Wikipedia, Royal Gazette
texts, OCR-extracted books, and CC-licensed YouTube subtitles. Systematic
ablations using GPT-2 show the pipeline trims CommonCrawl from 202M to 25M
documents while raising SEA-HELM NLG from 3 to 11; an 8B-parameter SEA-LION
model continually pre-trained on Mangosteen then surpasses SEA-LION-v3 and
Llama-3.1 by about four points on Thai benchmarks. We release the full pipeline
code, cleaning manifests, corpus snapshot, and all checkpoints, providing a
fully reproducible foundation for future Thai and regional LLM research.

</details>


### [225] [Large Language Models as Medical Codes Selectors: a benchmark using the International Classification of Primary Care](https://arxiv.org/abs/2507.14681)
*Vinicius Anjos de Almeida,Vinicius de Camargo,Raquel Gómez-Bravo,Egbert van der Haring,Kees van Boven,Marcelo Finger,Luis Fernandez Lopez*

Main category: cs.CL

TL;DR: LLMs can automate ICPC-2 coding effectively, with top models achieving high F1-scores. Challenges include dataset limitations and smaller model performance.


<details>
  <summary>Details</summary>
Motivation: To assess the potential of LLMs for automating medical coding (ICPC-2) using domain-specific search engine outputs.

Method: Used a dataset of 437 clinical expressions annotated with ICPC-2 codes. Evaluated 33 LLMs with semantic search engine retrieval, measuring F1-score, token usage, cost, and format adherence.

Result: Top models (e.g., gpt-4.5-preview) achieved F1-scores > 0.85. Retriever optimization improved performance. Smaller models struggled with formatting.

Conclusion: LLMs are promising for ICPC-2 coding without fine-tuning, but broader evaluations are needed for clinical validation.

Abstract: Background: Medical coding structures healthcare data for research, quality
monitoring, and policy. This study assesses the potential of large language
models (LLMs) to assign ICPC-2 codes using the output of a domain-specific
search engine.
  Methods: A dataset of 437 Brazilian Portuguese clinical expressions, each
annotated with ICPC-2 codes, was used. A semantic search engine (OpenAI's
text-embedding-3-large) retrieved candidates from 73,563 labeled concepts.
Thirty-three LLMs were prompted with each query and retrieved results to select
the best-matching ICPC-2 code. Performance was evaluated using F1-score, along
with token usage, cost, response time, and format adherence.
  Results: Twenty-eight models achieved F1-score > 0.8; ten exceeded 0.85. Top
performers included gpt-4.5-preview, o3, and gemini-2.5-pro. Retriever
optimization can improve performance by up to 4 points. Most models returned
valid codes in the expected format, with reduced hallucinations. Smaller models
(<3B) struggled with formatting and input length.
  Conclusions: LLMs show strong potential for automating ICPC-2 coding, even
without fine-tuning. This work offers a benchmark and highlights challenges,
but findings are limited by dataset scope and setup. Broader, multilingual,
end-to-end evaluations are needed for clinical validation.

</details>


### [226] [MiroMind-M1: An Open-Source Advancement in Mathematical Reasoning via Context-Aware Multi-Stage Policy Optimization](https://arxiv.org/abs/2507.14683)
*Xingxuan Li,Yao Xiao,Dianwen Ng,Hai Ye,Yue Deng,Xiang Lin,Bin Wang,Zhanfeng Mo,Chong Zhang,Yueyi Zhang,Zonglin Yang,Ruilin Li,Lei Lei,Shihao Xu,Han Zhao,Weiling Chen,Feng Ji,Lidong Bing*

Main category: cs.CL

TL;DR: The paper introduces MiroMind-M1, a fully open-source reasoning language model (RLM) series, addressing transparency and reproducibility gaps in existing RLMs. It achieves competitive performance on math benchmarks through a two-stage training process and a novel optimization algorithm.


<details>
  <summary>Details</summary>
Motivation: To enhance transparency and reproducibility in RLM development, as current closed-source and many open-source models lack critical resources like datasets and training details.

Method: Two-stage training: supervised fine-tuning (SFT) on 719K math problems with verified reasoning steps, followed by reinforcement learning with verifiable rewards (RLVR) on 62K problems. Introduces Context-Aware Multi-Stage Policy Optimization for robust RLVR.

Result: MiroMind-M1 matches or exceeds performance of existing open-source RLMs on benchmarks (AIME24, AIME25, MATH) with superior token efficiency.

Conclusion: The release of models, datasets, and configurations aims to support reproducibility and advance community research in RLMs.

Abstract: Large language models have recently evolved from fluent text generation to
advanced reasoning across diverse domains, giving rise to reasoning language
models. Among these domains, mathematical reasoning serves as a representative
benchmark as it requires precise multi-step logic and abstract reasoning, which
can be generalized to other tasks. While closed-source RLMs such as GPT-o3
demonstrate impressive reasoning capabilities, their proprietary nature limits
transparency and reproducibility. Although many open-source projects aim to
close this gap, most of them lack sufficient openness by omitting critical
resources such as datasets and detailed training configurations, which hinders
reproducibility. To contribute toward greater transparency in RLM development,
we introduce the MiroMind-M1 series, a set of fully open-source RLMs built on
the Qwen-2.5 backbone that match or exceed the performance of existing
open-source RLMs. Specifically, our models are trained in two stages: SFT on a
carefully curated corpus of 719K math-reasoning problems with verified CoT
trajectories, followed by RLVR on 62K challenging and verifiable problems. To
enhance the robustness and efficiency of the RLVR process, we introduce
Context-Aware Multi-Stage Policy Optimization, an algorithm that integrates
length-progressive training with an adaptive repetition penalty to encourage
context-aware RL training. Our model achieves state-of-the-art or competitive
performance and superior token efficiency among Qwen-2.5-based open-source 7B
and 32B models on the AIME24, AIME25, and MATH benchmarks. To facilitate
reproducibility, we release the complete stack: models (MiroMind-M1-SFT-7B,
MiroMind-M1-RL-7B, MiroMind-M1-RL-32B); datasets (MiroMind-M1-SFT-719K,
MiroMind-M1-RL-62K); and all training and evaluation configurations. We hope
these resources will support further research and foster community advancement.

</details>


### [227] [Mind the Gap: A Review of Arabic Post-Training Datasets and Their Limitations](https://arxiv.org/abs/2507.14688)
*Mohammed Alkhowaiter,Norah Alshahrani,Saied Alshahrani,Reem I. Masoud,Alaa Alzahrani,Deema Alnuhait,Emad A. Alghamdi,Khalid Almubarak*

Main category: cs.CL

TL;DR: A review of Arabic post-training datasets on Hugging Face Hub highlights gaps in task diversity, documentation, and adoption, with recommendations for future improvements.


<details>
  <summary>Details</summary>
Motivation: To assess and improve the quality and diversity of Arabic post-training datasets for better alignment of LLMs with human instructions.

Method: Evaluated datasets along four dimensions: LLM Capabilities, Steerability, Alignment, and Robustness, using criteria like popularity, adoption, and documentation quality.

Result: Identified critical gaps, including limited task diversity, poor documentation, and low community adoption.

Conclusion: The gaps hinder Arabic LLM progress; recommendations are provided to enhance future dataset development.

Abstract: Post-training has emerged as a crucial technique for aligning pre-trained
Large Language Models (LLMs) with human instructions, significantly enhancing
their performance across a wide range of tasks. Central to this process is the
quality and diversity of post-training datasets. This paper presents a review
of publicly available Arabic post-training datasets on the Hugging Face Hub,
organized along four key dimensions: (1) LLM Capabilities (e.g., Question
Answering, Translation, Reasoning, Summarization, Dialogue, Code Generation,
and Function Calling); (2) Steerability (e.g., persona and system prompts); (3)
Alignment (e.g., cultural, safety, ethics, and fairness), and (4) Robustness.
Each dataset is rigorously evaluated based on popularity, practical adoption,
recency and maintenance, documentation and annotation quality, licensing
transparency, and scientific contribution. Our review revealed critical gaps in
the development of Arabic post-training datasets, including limited task
diversity, inconsistent or missing documentation and annotation, and low
adoption across the community. Finally, the paper discusses the implications of
these gaps on the progress of Arabic LLMs and applications while providing
concrete recommendations for future efforts in post-training dataset
development.

</details>


### [228] [Rethinking Suicidal Ideation Detection: A Trustworthy Annotation Framework and Cross-Lingual Model Evaluation](https://arxiv.org/abs/2507.14693)
*Amina Dzafic,Merve Kavut,Ulya Bayram*

Main category: cs.CL

TL;DR: The paper addresses challenges in suicidal ideation detection by creating a Turkish corpus and evaluating label reliability and model consistency across datasets, advocating for better practices in mental health NLP.


<details>
  <summary>Details</summary>
Motivation: Limited language coverage and unreliable annotation practices hinder progress in suicidal ideation detection, especially in non-English languages.

Method: Constructed a Turkish suicidal ideation corpus, introduced an annotation framework with human annotators and LLMs, and evaluated label reliability and model consistency using transfer learning.

Result: Highlighted the need for rigorous, language-inclusive annotation and evaluation, and questioned the performance of popular models with zero-shot transfer learning.

Conclusion: Advocates for transparency in dataset construction and model training in mental health NLP, emphasizing data and model reliability.

Abstract: Suicidal ideation detection is critical for real-time suicide prevention, yet
its progress faces two under-explored challenges: limited language coverage and
unreliable annotation practices. Most available datasets are in English, but
even among these, high-quality, human-annotated data remains scarce. As a
result, many studies rely on available pre-labeled datasets without examining
their annotation process or label reliability. The lack of datasets in other
languages further limits the global realization of suicide prevention via
artificial intelligence (AI). In this study, we address one of these gaps by
constructing a novel Turkish suicidal ideation corpus derived from social media
posts and introducing a resource-efficient annotation framework involving three
human annotators and two large language models (LLMs). We then address the
remaining gaps by performing a bidirectional evaluation of label reliability
and model consistency across this dataset and three popular English suicidal
ideation detection datasets, using transfer learning through eight pre-trained
sentiment and emotion classifiers. These transformers help assess annotation
consistency and benchmark model performance against manually labeled data. Our
findings underscore the need for more rigorous, language-inclusive approaches
to annotation and evaluation in mental health natural language processing (NLP)
while demonstrating the questionable performance of popular models with
zero-shot transfer learning. We advocate for transparency in model training and
dataset construction in mental health NLP, prioritizing data and model
reliability.

</details>


### [229] [Disparities in Peer Review Tone and the Role of Reviewer Anonymity](https://arxiv.org/abs/2507.14741)
*Maria Sahakyan,Bedoor AlShebli*

Main category: cs.CL

TL;DR: The study analyzes 80,000 peer reviews to uncover linguistic biases related to author demographics and reviewer anonymity, challenging assumptions about fairness in peer review.


<details>
  <summary>Details</summary>
Motivation: To investigate how language in peer reviews may subtly reinforce disparities, focusing on tone, sentiment, and supportive language across demographics.

Method: Natural language processing and large-scale statistical modeling of reviews from two major journals, including anonymous and signed reviews.

Result: Reveals hidden biases in peer feedback and shows how reviewer identity disclosure affects evaluation language.

Conclusion: The findings question the role of anonymity in fairness and highlight the impact of review policies on careers and scientific progress.

Abstract: The peer review process is often regarded as the gatekeeper of scientific
integrity, yet increasing evidence suggests that it is not immune to bias.
Although structural inequities in peer review have been widely debated, much
less attention has been paid to the subtle ways in which language itself may
reinforce disparities. This study undertakes one of the most comprehensive
linguistic analyses of peer review to date, examining more than 80,000 reviews
in two major journals. Using natural language processing and large-scale
statistical modeling, it uncovers how review tone, sentiment, and supportive
language vary across author demographics, including gender, race, and
institutional affiliation. Using a data set that includes both anonymous and
signed reviews, this research also reveals how the disclosure of reviewer
identity shapes the language of evaluation. The findings not only expose hidden
biases in peer feedback, but also challenge conventional assumptions about
anonymity's role in fairness. As academic publishing grapples with reform,
these insights raise critical questions about how review policies shape career
trajectories and scientific progress.

</details>


### [230] [On the robustness of modeling grounded word learning through a child's egocentric input](https://arxiv.org/abs/2507.14749)
*Wai Keen Vong,Brenden M. Lake*

Main category: cs.CL

TL;DR: The paper explores whether neural networks can learn language like children by training on limited, child-like input. It validates robustness across multiple children's data but notes individual learning differences.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between machine learning's reliance on massive datasets and children's efficient language acquisition from limited input.

Method: Automated speech transcription of 500+ hours of video data from three children (SAYCam dataset), used to train and evaluate multimodal neural networks for word learning.

Result: Networks successfully learned and generalized word-referent mappings across architectures, showing robustness but also individual learning differences.

Conclusion: Multimodal neural networks can robustly simulate child-like word learning, though individual developmental experiences influence learning patterns.

Abstract: What insights can machine learning bring to understanding human language
acquisition? Large language and multimodal models have achieved remarkable
capabilities, but their reliance on massive training datasets creates a
fundamental mismatch with children, who succeed in acquiring language from
comparatively limited input. To help bridge this gap, researchers have
increasingly trained neural networks using data similar in quantity and quality
to children's input. Taking this approach to the limit, Vong et al. (2024)
showed that a multimodal neural network trained on 61 hours of visual and
linguistic input extracted from just one child's developmental experience could
acquire word-referent mappings. However, whether this approach's success
reflects the idiosyncrasies of a single child's experience, or whether it would
show consistent and robust learning patterns across multiple children's
experiences was not explored. In this article, we applied automated speech
transcription methods to the entirety of the SAYCam dataset, consisting of over
500 hours of video data spread across all three children. Using these automated
transcriptions, we generated multi-modal vision-and-language datasets for both
training and evaluation, and explored a range of neural network configurations
to examine the robustness of simulated word learning. Our findings demonstrate
that networks trained on automatically transcribed data from each child can
acquire and generalize word-referent mappings across multiple network
architectures. These results validate the robustness of multimodal neural
networks for grounded word learning, while highlighting the individual
differences that emerge in how models learn when trained on each child's
developmental experiences.

</details>


### [231] [GRACE: Generative Recommendation via Journey-Aware Sparse Attention on Chain-of-Thought Tokenization](https://arxiv.org/abs/2507.14758)
*Luyi Ma,Wanjia Zhang,Kai Zhao,Abhishek Kulkarni,Lalitesh Morishetti,Anjana Ganesh,Ashish Ranjan,Aashika Padmanabhan,Jianpeng Xu,Jason Cho,Praveen Kanumala,Kaushiki Nag,Sumit Dutta,Kamiya Motwani,Malay Patel,Evren Korpeoglu,Sushant Kumar,Kannan Achan*

Main category: cs.CL

TL;DR: GRACE is a generative framework for multi-behavior recommendation, addressing inefficiencies in tokenization and attention mechanisms, achieving significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: Current generative models for multi-behavior recommendation lack explicit token reasoning, suffer from high computational costs, and have limited multi-scale modeling.

Method: GRACE uses hybrid Chain-of-Thought tokenization and Journey-Aware Sparse Attention to improve interpretability and efficiency.

Result: GRACE outperforms baselines by up to +106.9% HR@10 and reduces attention computation by 48%.

Conclusion: GRACE effectively addresses key challenges in generative recommendation, offering interpretable and efficient solutions.

Abstract: Generative models have recently demonstrated strong potential in
multi-behavior recommendation systems, leveraging the expressive power of
transformers and tokenization to generate personalized item sequences. However,
their adoption is hindered by (1) the lack of explicit information for token
reasoning, (2) high computational costs due to quadratic attention complexity
and dense sequence representations after tokenization, and (3) limited
multi-scale modeling over user history. In this work, we propose GRACE
(Generative Recommendation via journey-aware sparse Attention on
Chain-of-thought tokEnization), a novel generative framework for multi-behavior
sequential recommendation. GRACE introduces a hybrid Chain-of-Thought (CoT)
tokenization method that encodes user-item interactions with explicit
attributes from product knowledge graphs (e.g., category, brand, price) over
semantic tokenization, enabling interpretable and behavior-aligned generation.
To address the inefficiency of standard attention, we design a Journey-Aware
Sparse Attention (JSA) mechanism, which selectively attends to compressed,
intra-, inter-, and current-context segments in the tokenized sequence.
Experiments on two real-world datasets show that GRACE significantly
outperforms state-of-the-art baselines, achieving up to +106.9% HR@10 and
+106.7% NDCG@10 improvement over the state-of-the-art baseline on the Home
domain, and +22.1% HR@10 on the Electronics domain. GRACE also reduces
attention computation by up to 48% with long sequences.

</details>


### [232] [FastLongSpeech: Enhancing Large Speech-Language Models for Efficient Long-Speech Processing](https://arxiv.org/abs/2507.14815)
*Shoutao Guo,Shaolei Zhang,Qingkai Fang,Zhengrui Ma,Min Zhang,Yang Feng*

Main category: cs.CL

TL;DR: FastLongSpeech is a framework for efficient long-speech processing in LSLMs, using iterative fusion and dynamic compression training without needing long-speech datasets.


<details>
  <summary>Details</summary>
Motivation: Existing LSLMs focus on short-speech tasks, leaving long-speech processing underexplored due to data scarcity and high computational costs.

Method: Introduces FastLongSpeech with iterative fusion for sequence compression and dynamic compression training for adaptation.

Result: Strong performance in long- and short-speech tasks with improved inference efficiency.

Conclusion: FastLongSpeech effectively extends LSLM capabilities for long-speech processing efficiently.

Abstract: The rapid advancement of Large Language Models (LLMs) has spurred significant
progress in Large Speech-Language Models (LSLMs), enhancing their capabilities
in both speech understanding and generation. While existing LSLMs often
concentrate on augmenting speech generation or tackling a diverse array of
short-speech tasks, the efficient processing of long-form speech remains a
critical yet underexplored challenge. This gap is primarily attributed to the
scarcity of long-speech training datasets and the high computational costs
associated with long sequences. To address these limitations, we introduce
FastLongSpeech, a novel framework designed to extend LSLM capabilities for
efficient long-speech processing without necessitating dedicated long-speech
training data. FastLongSpeech incorporates an iterative fusion strategy that
can compress excessively long-speech sequences into manageable lengths. To
adapt LSLMs for long-speech inputs, it introduces a dynamic compression
training approach, which exposes the model to short-speech sequences at varying
compression ratios, thereby transferring the capabilities of LSLMs to
long-speech tasks. To assess the long-speech capabilities of LSLMs, we develop
a long-speech understanding benchmark called LongSpeech-Eval. Experiments show
that our method exhibits strong performance in both long-speech and
short-speech tasks, while greatly improving inference efficiency.

</details>


### [233] [Doc2Chart: Intent-Driven Zero-Shot Chart Generation from Documents](https://arxiv.org/abs/2507.14819)
*Akriti Jain,Pritika Ramu,Aparna Garimella,Apoorv Saxena*

Main category: cs.CL

TL;DR: The paper introduces intent-based chart generation from documents using a two-staged LLM framework, outperforming baselines in accuracy and chart type selection.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of generating charts from long documents based on user intents without manual content selection.

Method: Unsupervised two-staged framework: LLM extracts and refines data, followed by heuristic-guided chart type selection and code generation.

Result: Outperforms baselines by 9 points in data accuracy and 17 points in chart type selection.

Conclusion: The proposed method effectively generates accurate charts from documents based on user intents, validated on a curated dataset.

Abstract: Large Language Models (LLMs) have demonstrated strong capabilities in
transforming text descriptions or tables to data visualizations via
instruction-tuning methods. However, it is not straightforward to apply these
methods directly for a more real-world use case of visualizing data from long
documents based on user-given intents, as opposed to the user pre-selecting the
relevant content manually. We introduce the task of intent-based chart
generation from documents: given a user-specified intent and document(s), the
goal is to generate a chart adhering to the intent and grounded on the
document(s) in a zero-shot setting. We propose an unsupervised, two-staged
framework in which an LLM first extracts relevant information from the
document(s) by decomposing the intent and iteratively validates and refines
this data. Next, a heuristic-guided module selects an appropriate chart type
before final code generation. To assess the data accuracy of the generated
charts, we propose an attribution-based metric that uses a structured textual
representation of charts, instead of relying on visual decoding metrics that
often fail to capture the chart data effectively. To validate our approach, we
curate a dataset comprising of 1,242 $<$intent, document, charts$>$ tuples from
two domains, finance and scientific, in contrast to the existing datasets that
are largely limited to parallel text descriptions/ tables and their
corresponding charts. We compare our approach with baselines using single-shot
chart generation using LLMs and query-based retrieval methods; our method
outperforms by upto $9$ points and $17$ points in terms of chart data accuracy
and chart type respectively over the best baselines.

</details>


### [234] [Beyond Isolated Capabilities: Bridging Long CoT Reasoning and Long-Context Understanding](https://arxiv.org/abs/2507.14849)
*Yifei Wang*

Main category: cs.CL

TL;DR: Reasoning distillation improves smaller models' long-context understanding, addressing the 'lost in the middle' issue in Retrieval-Augmented Generation systems.


<details>
  <summary>Details</summary>
Motivation: To explore how large-scale reasoning distillation affects in-context retrieval and reasoning, especially in RAG systems.

Method: Evaluated open-source models distilled from Deepseek-R1 using multi-document question-answering tasks.

Result: Distilled reasoning patterns enhance long-context comprehension by promoting detailed reasoning during context analysis.

Conclusion: Reasoning distillation significantly improves long-context awareness and mitigates the 'lost in the middle' problem.

Abstract: Reasoning distillation has emerged as an effective approach to enhance the
reasoning capabilities of smaller language models. However, the impact of
large-scale reasoning distillation on other critical abilities, particularly
in-context retrieval and reasoning, remains unexplored. This gap in
understanding is particularly significant given the increasing importance of
Retrieval-Augmented Generation (RAG) systems, where efficient acquisition and
utilization of contextual information are paramount for generating reliable
responses. Motivated by the need to understand how the extended long-CoT
process influences long-context comprehension, we conduct a comprehensive
investigation using a series of open-source models distilled from Deepseek-R1,
renowned for its exceptional reasoning capabilities. Our study focuses on
evaluating these models' performance in extracting and integrating relevant
information from extended contexts through multi-document question and
answering tasks. Through rigorous experimentation, we demonstrate that
distilled reasoning patterns significantly improve long-context understanding.
Our analysis reveals that distillation fosters greater long-context awareness
by promoting more detailed and explicit reasoning processes during context
analysis and information parsing. This advancement effectively mitigates the
persistent "lost in the middle" issue that has hindered long-context models.

</details>


### [235] [Tiny language models](https://arxiv.org/abs/2507.14871)
*Ronit D. Gross,Yarden Tzach,Tal Halevi,Ella Koresh,Ido Kanter*

Main category: cs.CL

TL;DR: The study explores if tiny language models (TLMs) mimic key features of large language models (LLMs), showing pre-training's effectiveness even at small scales.


<details>
  <summary>Details</summary>
Motivation: Limited accessibility of LLMs due to high computational costs necessitates research into more feasible alternatives like TLMs.

Method: Pre-training BERT-6 and BERT-1 variants on Wikipedia subsets and evaluating on FewRel, AGNews, and DBPedia tasks.

Result: TLMs show a performance gap favoring pre-trained models, with accuracy replicable via shallow architectures.

Conclusion: TLMs offer a viable, low-resource alternative to LLMs, with potential insights into human language development.

Abstract: A prominent achievement of natural language processing (NLP) is its ability
to understand and generate meaningful human language. This capability relies on
complex feedforward transformer block architectures pre-trained on large
language models (LLMs). However, LLM pre-training is currently feasible only
for a few dominant companies due to the immense computational resources
required, limiting broader research participation. This creates a critical need
for more accessible alternatives. In this study, we explore whether tiny
language models (TLMs) exhibit the same key qualitative features of LLMs. We
demonstrate that TLMs exhibit a clear performance gap between pre-trained and
non-pre-trained models across classification tasks, indicating the
effectiveness of pre-training, even at a tiny scale. The performance gap
increases with the size of the pre-training dataset and with greater overlap
between tokens in the pre-training and classification datasets. Furthermore,
the classification accuracy achieved by a pre-trained deep TLM architecture can
be replicated through a soft committee of multiple, independently pre-trained
shallow architectures, enabling low-latency TLMs without affecting
classification accuracy. Our results are based on pre-training BERT-6 and
variants of BERT-1 on subsets of the Wikipedia dataset and evaluating their
performance on FewRel, AGNews, and DBPedia classification tasks. Future
research on TLM is expected to further illuminate the mechanisms underlying
NLP, especially given that its biologically inspired models suggest that TLMs
may be sufficient for children or adolescents to develop language.

</details>


### [236] [MEKiT: Multi-source Heterogeneous Knowledge Injection Method via Instruction Tuning for Emotion-Cause Pair Extraction](https://arxiv.org/abs/2507.14887)
*Shiyi Mu,Yongkang Liu,Shi Feng,Xiaocui Yang,Daling Wang,Yifei Zhang*

Main category: cs.CL

TL;DR: MEKiT improves LLMs' performance on Emotion-Cause Pair Extraction by injecting multi-source heterogeneous knowledge.


<details>
  <summary>Details</summary>
Motivation: LLMs underperform in ECPE due to lack of auxiliary knowledge for emotion perception and cause reasoning.

Method: MEKiT integrates internal emotional and external causal knowledge using instruction templates and mixed data for instruction-tuning.

Result: MEKiT outperforms baselines, significantly boosting LLMs' ECPE performance.

Conclusion: MEKiT is an effective and adaptable solution for enhancing LLMs in ECPE tasks.

Abstract: Although large language models (LLMs) excel in text comprehension and
generation, their performance on the Emotion-Cause Pair Extraction (ECPE) task,
which requires reasoning ability, is often underperform smaller language model.
The main reason is the lack of auxiliary knowledge, which limits LLMs' ability
to effectively perceive emotions and reason causes. To address this issue, we
propose a novel \textbf{M}ulti-source h\textbf{E}terogeneous \textbf{K}nowledge
\textbf{i}njection me\textbf{T}hod, MEKiT, which integrates heterogeneous
internal emotional knowledge and external causal knowledge. Specifically, for
these two distinct aspects and structures of knowledge, we apply the approaches
of incorporating instruction templates and mixing data for instruction-tuning,
which respectively facilitate LLMs in more comprehensively identifying emotion
and accurately reasoning causes. Experimental results demonstrate that MEKiT
provides a more effective and adaptable solution for the ECPE task, exhibiting
an absolute performance advantage over compared baselines and dramatically
improving the performance of LLMs on the ECPE task.

</details>


### [237] [Sparse Autoencoder-guided Supervised Finetuning to Mitigate Unexpected Code-Switching in LLMs](https://arxiv.org/abs/2507.14894)
*Boyi Deng,Yu Wan,Baosong Yang,Fei Huang,Wenjie Wang,Fuli Feng*

Main category: cs.CL

TL;DR: The paper addresses unexpected code-switching in LLMs, proposes SASFT to reduce it by 50%+, and maintains multilingual performance.


<details>
  <summary>Details</summary>
Motivation: Unexpected code-switching in LLMs degrades readability and usability, but existing solutions lack mechanistic analysis and effectiveness.

Method: Uses sparse autoencoders to analyze code-switching, then proposes SASFT to control language feature pre-activation values.

Result: SASFT reduces code-switching by over 50%, eliminates it in four cases, and maintains multilingual benchmark performance.

Conclusion: SASFT effectively mitigates code-switching while preserving LLMs' multilingual capabilities.

Abstract: Large Language Models (LLMs) have impressive multilingual capabilities, but
they suffer from unexpected code-switching, also known as language mixing,
which involves switching to unexpected languages in the model response. This
problem leads to poor readability and degrades the usability of model
responses. However, existing work on this issue lacks a mechanistic analysis
and shows limited effectiveness. In this paper, we first provide an in-depth
analysis of unexpected code-switching using sparse autoencoders and find that
when LLMs switch to a language, the features of that language exhibit excessive
pre-activation values. Based on our findings, we propose $\textbf{S}$parse
$\textbf{A}$utoencoder-guided $\textbf{S}$upervised
$\textbf{F}$ine$\textbf{t}$uning (SASFT), which teaches LLMs to maintain
appropriate pre-activation values of specific language features during
training. Experiments on five models across three languages demonstrate that
SASFT consistently reduces unexpected code-switching by more than 50\% compared
to standard supervised fine-tuning, with complete elimination in four cases.
Moreover, SASFT maintains or even improves the models' performance on six
multilingual benchmarks, showing its effectiveness in addressing code-switching
while preserving multilingual capabilities.

</details>


### [238] [From Neurons to Semantics: Evaluating Cross-Linguistic Alignment Capabilities of Large Language Models via Neurons Alignment](https://arxiv.org/abs/2507.14900)
*Chongxuan Huang,Yongshi Ye,Biao Fu,Qifeng Su,Xiaodong Shi*

Main category: cs.CL

TL;DR: NeuronXA is a novel method for evaluating cross-lingual alignment in LLMs, achieving high correlation with downstream tasks and transferability using minimal data.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for cross-lingual alignment focus on sentence embeddings but overlook non-smooth representation spaces, especially for low-resource languages.

Method: Proposes NeuronXA, a neuron state-based approach inspired by neuroscientific findings, to assess alignment in multilingual LLMs.

Result: NeuronXA achieves high Pearson correlations (0.9556 with downstream tasks, 0.8514 with transferability) using only 100 parallel sentence pairs.

Conclusion: NeuronXA effectively evaluates cross-lingual alignment and transferability, advancing research and improving multilingual LLM understanding.

Abstract: Large language models (LLMs) have demonstrated remarkable multilingual
capabilities, however, how to evaluate cross-lingual alignment remains
underexplored. Existing alignment benchmarks primarily focus on sentence
embeddings, but prior research has shown that neural models tend to induce a
non-smooth representation space, which impact of semantic alignment evaluation
on low-resource languages. Inspired by neuroscientific findings that similar
information activates overlapping neuronal regions, we propose a novel Neuron
State-Based Cross-Lingual Alignment (NeuronXA) to assess the cross-lingual a
lignment capabilities of LLMs, which offers a more semantically grounded
approach to assess cross-lingual alignment. We evaluate NeuronXA on several
prominent multilingual LLMs (LLaMA, Qwen, Mistral, GLM, and OLMo) across two
transfer tasks and three multilingual benchmarks. The results demonstrate that
with only 100 parallel sentence pairs, NeuronXA achieves a Pearson correlation
of 0.9556 with downstream tasks performance and 0.8514 with transferability.
These findings demonstrate NeuronXA's effectiveness in assessing both
cross-lingual alignment and transferability, even with a small dataset. This
highlights its potential to advance cross-lingual alignment research and to
improve the semantic understanding of multilingual LLMs.

</details>


### [239] [PromptSuite: A Task-Agnostic Framework for Multi-Prompt Generation](https://arxiv.org/abs/2507.14913)
*Eliya Habba,Noam Dahan,Gili Lior,Gabriel Stanovsky*

Main category: cs.CL

TL;DR: PromptSuite is a framework for automatically generating diverse prompts to improve LLM evaluation reliability.


<details>
  <summary>Details</summary>
Motivation: Single-prompt evaluations of LLMs are unreliable due to performance sensitivity to small changes.

Method: PromptSuite uses modular prompt design for controlled perturbations and supports extensibility.

Result: Case studies show PromptSuite provides meaningful prompt variations for robust evaluation.

Conclusion: PromptSuite enhances LLM evaluation practices and is accessible via Python API and web interface.

Abstract: Evaluating LLMs with a single prompt has proven unreliable, with small
changes leading to significant performance differences. However, generating the
prompt variations needed for a more robust multi-prompt evaluation is
challenging, limiting its adoption in practice. To address this, we introduce
PromptSuite, a framework that enables the automatic generation of various
prompts. PromptSuite is flexible - working out of the box on a wide range of
tasks and benchmarks. It follows a modular prompt design, allowing controlled
perturbations to each component, and is extensible, supporting the addition of
new components and perturbation types. Through a series of case studies, we
show that PromptSuite provides meaningful variations to support strong
evaluation practices. It is available through both a Python API:
https://github.com/eliyahabba/PromptSuite, and a user-friendly web interface:
https://promptsuite.streamlit.app/

</details>


### [240] [SYNTHIA: Synthetic Yet Naturally Tailored Human-Inspired PersonAs](https://arxiv.org/abs/2507.14922)
*Vahid Rahimzadeh,Erfan Moosavi Monazzah,Mohammad Taher Pilehvar,Yadollah Yaghoobzadeh*

Main category: cs.CL

TL;DR: SYNTHIA is a dataset of 30,000 backstories from 10,000 real social media users, balancing realism and synthetic generation for persona-driven LLMs.


<details>
  <summary>Details</summary>
Motivation: Existing methods for persona-driven LLMs are either costly (human-curated) or lack realism (synthetic). SYNTHIA bridges this gap.

Method: SYNTHIA uses real social media data from BlueSky to create synthetic personas, incorporating temporal and social interaction metadata.

Result: SYNTHIA matches state-of-the-art in demographic diversity and survey alignment, excels in narrative consistency, and enables new research.

Conclusion: SYNTHIA offers a balanced, realistic dataset for persona-driven LLMs, advancing computational social science and language modeling.

Abstract: Persona-driven LLMs have emerged as powerful tools in computational social
science, yet existing approaches fall at opposite extremes, either relying on
costly human-curated data or producing synthetic personas that lack consistency
and realism. We introduce SYNTHIA, a dataset of 30,000 backstories derived from
10,000 real social media users from BlueSky open platform across three time
windows, bridging this spectrum by grounding synthetic generation in authentic
user activity. Our evaluation demonstrates that SYNTHIA achieves competitive
performance with state-of-the-art methods in demographic diversity and social
survey alignment while significantly outperforming them in narrative
consistency. Uniquely, SYNTHIA incorporates temporal dimensionality and
provides rich social interaction metadata from the underlying network, enabling
new research directions in computational social science and persona-driven
language modeling.

</details>


### [241] [MUR: Momentum Uncertainty guided Reasoning for Large Language Models](https://arxiv.org/abs/2507.14958)
*Hang Yan,Fangzhi Xu,Rongman Xu,Yifei Li,Jian Zhang,Haoran Luo,Xiaobao Wu,Luu Anh Tuan,Haiteng Zhao,Qika Lin,Jun Liu*

Main category: cs.CL

TL;DR: MUR (Momentum Uncertainty-guided Reasoning) dynamically allocates thinking budgets to LLMs, reducing computation by 50% while improving accuracy by 0.62-3.37%.


<details>
  <summary>Details</summary>
Motivation: Optimizing reasoning efficiency in LLMs without additional training, addressing overthinking in Test-Time Scaling (TTS).

Method: Proposes MUR, inspired by momentum in physics, using gamma-control for flexible inference-time tuning.

Result: Reduces computation by over 50% and improves accuracy across benchmarks.

Conclusion: MUR efficiently guides LLM reasoning, balancing computation and performance.

Abstract: Large Language Models (LLMs) have achieved impressive performance on
reasoning-intensive tasks, yet optimizing their reasoning efficiency remains an
open challenge. While Test-Time Scaling (TTS) improves reasoning quality, it
often leads to overthinking, wasting tokens on redundant computations. This
work investigates how to efficiently and adaptively guide LLM test-time scaling
without additional training. Inspired by the concept of momentum in physics, we
propose Momentum Uncertainty-guided Reasoning (MUR), which dynamically
allocates thinking budgets to critical reasoning steps by tracking and
aggregating stepwise uncertainty over time. To support flexible inference-time
control, we introduce gamma-control, a simple mechanism that tunes the
reasoning budget via a single hyperparameter. We provide in-depth theoretical
proof to support the superiority of MUR in terms of stability and biases. MUR
is comprehensively evaluated against various TTS methods across four
challenging benchmarks (MATH-500, AIME24, AIME25, and GPQA-diamond) using
different sizes of recent Qwen3 models (1.7B, 4B, and 8B). Results demonstrate
that MUR reduces computation by over 50% on average while improving accuracy by
0.62-3.37%.

</details>


### [242] [RefCritic: Training Long Chain-of-Thought Critic Models with Refinement Feedback](https://arxiv.org/abs/2507.15024)
*Qiaoyu Tang,Hao Xiang,Le Yu,Bowen Yu,Hongyu Lin,Yaojie Lu,Xianpei Han,Le Sun,Junyang Lin*

Main category: cs.CL

TL;DR: RefCritic, a reinforcement learning-based critic module, outperforms supervised fine-tuning methods by generating actionable critiques and improving model refinement.


<details>
  <summary>Details</summary>
Motivation: Current supervised fine-tuning for critic modules fails to enhance critique abilities, producing superficial feedback.

Method: Proposes RefCritic, using reinforcement learning with dual rule-based rewards for high-quality critiques.

Result: RefCritic shows consistent gains (e.g., 6.8% and 7.2%) on benchmarks and outperforms step-level supervised methods.

Conclusion: RefCritic unlocks superior critique capabilities and model refinement, validated across multiple benchmarks.

Abstract: With the rapid advancement of Large Language Models (LLMs), developing
effective critic modules for precise guidance has become crucial yet
challenging. In this paper, we initially demonstrate that supervised
fine-tuning for building critic modules (which is widely adopted in current
solutions) fails to genuinely enhance models' critique abilities, producing
superficial critiques with insufficient reflections and verifications. To
unlock the unprecedented critique capabilities, we propose RefCritic, a
long-chain-of-thought critic module based on reinforcement learning with dual
rule-based rewards: (1) instance-level correctness of solution judgments and
(2) refinement accuracies of the policy model based on critiques, aiming to
generate high-quality evaluations with actionable feedback that effectively
guides model refinement. We evaluate RefCritic on Qwen2.5-14B-Instruct and
DeepSeek-R1-Distill-Qwen-14B across five benchmarks. On critique and refinement
settings, RefCritic demonstrates consistent advantages across all benchmarks,
e.g., 6.8\% and 7.2\% gains on AIME25 for the respective base models. Notably,
under majority voting, policy models filtered by RefCritic show superior
scaling with increased voting numbers. Moreover, despite training on
solution-level supervision, RefCritic outperforms step-level supervised
approaches on ProcessBench, a benchmark to identify erroneous steps in
mathematical reasoning.

</details>


### [243] [WebShaper: Agentically Data Synthesizing via Information-Seeking Formalization](https://arxiv.org/abs/2507.15061)
*Zhengwei Tao,Jialong Wu,Wenbiao Yin,Junkai Zhang,Baixuan Li,Haiyang Shen,Kuan Li,Liwen Zhang,Xinyu Wang,Yong Jiang,Pengjun Xie,Fei Huang,Jingren Zhou*

Main category: cs.CL

TL;DR: WebShaper introduces a formalization-driven framework to synthesize high-quality IS training data, improving consistency and performance in LLM-powered agents.


<details>
  <summary>Details</summary>
Motivation: The scarcity of high-quality training data for IS agents and inconsistencies in existing approaches drive the need for a formalization-driven solution.

Method: WebShaper uses set theory and Knowledge Projections (KP) to formalize IS tasks, followed by a multi-step expansion process with retrieval and validation tools.

Result: WebShaper achieves state-of-the-art performance on GAIA and WebWalkerQA benchmarks.

Conclusion: The framework effectively addresses data scarcity and inconsistency, enhancing IS agent performance.

Abstract: The advent of Large Language Model (LLM)-powered agents has revolutionized
artificial intelligence by enabling solutions to complex, open-ended tasks
through web-based information-seeking (IS) capabilities. The scarcity of
high-quality training data has limited the development of IS agents. Existing
approaches typically adopt an information-driven paradigm that first collects
web data and then generates questions based on the retrieval. However, this may
lead to inconsistency between information structure and reasoning structure,
question and answer. To mitigate, we propose a formalization-driven IS data
synthesis framework WebShaper to construct a dataset. WebShaper systematically
formalizes IS tasks through set theory. Central to the formalization is the
concept of Knowledge Projections (KP), which enables precise control over
reasoning structure by KP operation compositions. During synthesis, we begin by
creating seed tasks, then use a multi-step expansion process. At each step, an
agentic Expander expands the current formal question more complex with
retrieval and validation tools based on our formalization. We train our model
on the synthesized dataset. Experiment results demonstrate that WebShaper
achieves state-of-the-art performance among open-sourced IS agents on GAIA and
WebWalkerQA benchmarks.

</details>


### [244] [Evaluation of Coding Schemes for Transformer-based Gene Sequence Modeling](https://arxiv.org/abs/2507.15087)
*Chenlei Gong,Yuanhe Tian,Lei Mao,Yan Song*

Main category: cs.CL

TL;DR: The paper compares k-mer segmentation and BPE tokenization for DNA sequence modeling, evaluating performance with different positional encodings and Transformer depths. BPE outperforms k-mer, and RoPE excels in capturing periodic motifs.


<details>
  <summary>Details</summary>
Motivation: To systematically evaluate the effectiveness of k-mer segmentation versus BPE tokenization and different positional encoding methods in DNA sequence modeling.

Method: Comparison of k-mer (k=1,3,4,5,6), BPE (4,096-token vocabulary), and three positional encodings (sinusoidal, AliBi, RoPE) in 3, 6, 12, and 24-layer Transformer encoders, evaluated on the GUE benchmark.

Result: BPE performs better and more stably by compressing motifs and reducing sequence length. RoPE captures periodic motifs well, while AliBi handles local dependencies. Performance improves significantly up to 12 layers, with diminishing returns at 24.

Conclusion: The study offers practical insights for choosing tokenization and positional encoding in DNA Transformer models, favoring BPE and RoPE for most tasks.

Abstract: Currently, many studies view DNA sequences as a special type of language and
utilize Transformers to model them. These studies use fixed-length k-mer
segmentation and BPE subword tokenization but lack a systematic evaluation to
determine which is superior. We compare k-mer segmentation with k=1,3,4,5,6, a
4,096-token BPE vocabulary, and three positional encoding methods-sinusoidal,
AliBi, and RoPE. Each configuration is trained from scratch in 3, 6, 12, and
24-layer Transformer encoders and evaluated on GUE benchmark dataset. In
general, BPE delivers higher and more stable performance across tasks by
compressing frequent motifs into variable-length tokens, reducing sequence
length, and improving model generalization. RoPE excels at capturing periodic
motifs and extrapolating to long sequences, while AliBi also performs well on
tasks driven by local dependencies. In terms of depth, we observe significant
gains when increasing layers from 3 to 12, with only marginal improvements or
slight overfitting at 24 layers. This study provides practical guidance for
designing tokenization and positional encoding in DNA Transformer models.

</details>


### [245] [A Penalty Goes a Long Way: Measuring Lexical Diversity in Synthetic Texts Under Prompt-Influenced Length Variations](https://arxiv.org/abs/2507.15092)
*Vijeta Deshpande,Ishita Dasgupta,Uttaran Bhattacharya,Somdeb Sarkhel,Saayan Mitra,Anna Rumshisky*

Main category: cs.CL

TL;DR: The paper introduces PATTR, a new diversity metric for synthetic text, addressing biases from text length variations in existing metrics like MATTR and CR.


<details>
  <summary>Details</summary>
Motivation: Current diversity metrics for synthetic text are biased by text length variations, impacting their reliability in tasks like creative writing.

Method: Proposes Penalty-Adjusted Type-Token Ratio (PATTR), tested on a 20M-word synthetic corpus from LLaMA, OLMo, and Phi models for video script generation.

Result: PATTR outperforms MATTR and CR by mitigating length biases and better adhering to target response lengths, improving diversity rankings.

Conclusion: PATTR is a robust diversity metric for synthetic text, especially in tasks where length variations skew existing metrics.

Abstract: Synthetic text generated by Large Language Models (LLMs) is increasingly used
for further training and improvement of LLMs. Diversity is crucial for the
effectiveness of synthetic data, and researchers rely on prompt engineering to
improve diversity. However, the impact of prompt variations on response text
length, and, more importantly, the consequential effect on lexical diversity
measurements, remain underexplored. In this work, we propose Penalty-Adjusted
Type-Token Ratio (PATTR), a diversity metric robust to length variations. We
generate a large synthetic corpus of over 20M words using seven models from the
LLaMA, OLMo, and Phi families, focusing on a creative writing task of video
script generation, where diversity is crucial. We evaluate per-response lexical
diversity using PATTR and compare it against existing metrics of Moving-Average
TTR (MATTR) and Compression Ratio (CR). Our analysis highlights how text length
variations introduce biases favoring shorter responses. Unlike existing
metrics, PATTR explicitly considers the task-specific target response length
($L_T$) to effectively mitigate length biases. We further demonstrate the
utility of PATTR in filtering the top-10/100/1,000 most lexically diverse
responses, showing that it consistently outperforms MATTR and CR by yielding on
par or better diversity with high adherence to $L_T$.

</details>


### [246] [Filling the Gap: Is Commonsense Knowledge Generation useful for Natural Language Inference?](https://arxiv.org/abs/2507.15100)
*Chathuri Jayaweera,Brianna Yanqui,Bonnie Dorr*

Main category: cs.CL

TL;DR: The paper explores using Large Language Models (LLMs) as commonsense knowledge generators for Natural Language Inference (NLI), evaluating their reliability and impact on prediction accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing commonsense resources lack coverage for diverse premise-hypothesis pairs, prompting the use of LLMs to fill this gap.

Method: The study adapts metrics to assess LLM factuality and consistency in generating commonsense knowledge for NLI.

Result: Incorporating commonsense knowledge doesn't consistently boost overall accuracy but helps distinguish entailing instances and moderately improves contradictory/neutral inferences.

Conclusion: LLMs show potential as commonsense knowledge generators for NLI, though their impact varies across inference types.

Abstract: Natural Language Inference (NLI) is the task of determining the semantic
entailment of a premise for a given hypothesis. The task aims to develop
systems that emulate natural human inferential processes where commonsense
knowledge plays a major role. However, existing commonsense resources lack
sufficient coverage for a variety of premise-hypothesis pairs. This study
explores the potential of Large Language Models as commonsense knowledge
generators for NLI along two key dimensions: their reliability in generating
such knowledge and the impact of that knowledge on prediction accuracy. We
adapt and modify existing metrics to assess LLM factuality and consistency in
generating in this context. While explicitly incorporating commonsense
knowledge does not consistently improve overall results, it effectively helps
distinguish entailing instances and moderately improves distinguishing
contradictory and neutral inferences.

</details>


### [247] [From Disagreement to Understanding: The Case for Ambiguity Detection in NLI](https://arxiv.org/abs/2507.15114)
*Chathuri Jayaweera,Bonnie Dorr*

Main category: cs.CL

TL;DR: The paper argues that annotation disagreement in NLI stems from meaningful interpretive variation due to ambiguity, not noise. It proposes an ambiguity-aware NLI framework and highlights the need for datasets annotated for ambiguity.


<details>
  <summary>Details</summary>
Motivation: To address annotation disagreement in NLI by recognizing ambiguity as a meaningful signal of divergent human perspectives, rather than dismissing it as noise.

Method: Proposes a unified framework integrating existing taxonomies to classify ambiguity types, supported by concrete examples. Suggests new annotated resources and unsupervised methods for ambiguity detection.

Result: Illustrates how ambiguity influences annotator decisions and identifies the lack of ambiguity-annotated datasets as a key limitation.

Conclusion: Calls for ambiguity-aware NLI systems, emphasizing the need for better datasets and detection methods to align models with human interpretation.

Abstract: This position paper argues that annotation disagreement in Natural Language
Inference (NLI) is not mere noise but often reflects meaningful interpretive
variation, especially when triggered by ambiguity in the premise or hypothesis.
While underspecified guidelines and annotator behavior can contribute to
variation, content-based ambiguity offers a process-independent signal of
divergent human perspectives. We call for a shift toward ambiguity-aware NLI by
systematically identifying ambiguous input pairs and classifying ambiguity
types. To support this, we present a unified framework that integrates existing
taxonomies and illustrate key ambiguity subtypes through concrete examples.
These examples reveal how ambiguity shapes annotator decisions and motivate the
need for targeted detection methods that better align models with human
interpretation. A key limitation is the lack of datasets annotated for
ambiguity and subtypes. We propose addressing this gap through new annotated
resources and unsupervised approaches to ambiguity detection -- paving the way
for more robust, explainable, and human-aligned NLI systems.

</details>


### [248] [A Case Against Implicit Standards: Homophone Normalization in Machine Translation for Languages that use the Ge'ez Script](https://arxiv.org/abs/2507.15142)
*Hellina Hailu Nigatu,Atnafu Lambebo Tonja,Henok Biadglign Ademtew,Hizkel Mitiku Alemayehu,Negasi Haile Abadi,Tadesse Destaw Belay,Seid Muhie Yimam*

Main category: cs.CL

TL;DR: The paper examines the impact of homophone normalization in Amharic NLP, proposing post-inference normalization to improve BLEU scores while preserving language features.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of homophone normalization in NLP, which may hinder model generalization and language understanding.

Method: Experiments with monolingual training and cross-lingual transfer, followed by post-inference normalization of model predictions.

Result: Post-inference normalization increased BLEU scores by up to 1.03 without altering training data.

Conclusion: Advocates for language-aware interventions to balance performance and linguistic integrity in NLP.

Abstract: Homophone normalization, where characters that have the same sound in a
writing script are mapped to one character, is a pre-processing step applied in
Amharic Natural Language Processing (NLP) literature. While this may improve
performance reported by automatic metrics, it also results in models that are
not able to understand different forms of writing in a single language.
Further, there might be impacts in transfer learning, where models trained on
normalized data do not generalize well to other languages. In this paper, we
experiment with monolingual training and cross-lingual transfer to understand
the impacts of normalization on languages that use the Ge'ez script. We then
propose a post-inference intervention in which normalization is applied to
model predictions instead of training data. With our simple scheme of
post-inference normalization, we show that we can achieve an increase in BLEU
score of up to 1.03 while preserving language features in training. Our work
contributes to the broader discussion on technology-facilitated language change
and calls for more language-aware interventions.

</details>


### [249] [What Level of Automation is "Good Enough"? A Benchmark of Large Language Models for Meta-Analysis Data Extraction](https://arxiv.org/abs/2507.15152)
*Lingbo Li,Anuradha Mathrani,Teo Susnjak*

Main category: cs.CL

TL;DR: The study evaluates three LLMs for automating data extraction from RCTs, finding high precision but poor recall. Customised prompts improved recall by 15%, leading to proposed guidelines for task-specific automation.


<details>
  <summary>Details</summary>
Motivation: Automating data extraction from RCTs for meta-analysis is challenging, requiring evaluation of LLMs' performance and prompting strategies.

Method: Tested three LLMs (Gemini-2.0-flash, Grok-3, GPT-4o-mini) on statistical results, risk-of-bias assessments, and study-level characteristics in three medical domains, using four prompting strategies.

Result: All models showed high precision but poor recall; customised prompts improved recall by up to 15%.

Conclusion: Proposed a three-tiered guideline for using LLMs in data extraction, balancing automation with expert oversight for real-world meta-analyses.

Abstract: Automating data extraction from full-text randomised controlled trials (RCTs)
for meta-analysis remains a significant challenge. This study evaluates the
practical performance of three LLMs (Gemini-2.0-flash, Grok-3, GPT-4o-mini)
across tasks involving statistical results, risk-of-bias assessments, and
study-level characteristics in three medical domains: hypertension, diabetes,
and orthopaedics. We tested four distinct prompting strategies (basic
prompting, self-reflective prompting, model ensemble, and customised prompts)
to determine how to improve extraction quality. All models demonstrate high
precision but consistently suffer from poor recall by omitting key information.
We found that customised prompts were the most effective, boosting recall by up
to 15\%. Based on this analysis, we propose a three-tiered set of guidelines
for using LLMs in data extraction, matching data types to appropriate levels of
automation based on task complexity and risk. Our study offers practical advice
for automating data extraction in real-world meta-analyses, balancing LLM
efficiency with expert oversight through targeted, task-specific automation.

</details>


### [250] [Collaborative Distillation Strategies for Parameter-Efficient Language Model Deployment](https://arxiv.org/abs/2507.15198)
*Xiandong Meng,Yan Wu,Yexin Tian,Xin Hu,Tianze Kang,Junliang Du*

Main category: cs.CL

TL;DR: The paper proposes a multi-teacher guided distillation method to reduce computational costs and improve inference speed in large language models, achieving strong performance while maintaining a small parameter size.


<details>
  <summary>Details</summary>
Motivation: Address the challenges of high computational cost and slow inference in deploying large language models.

Method: Uses multiple teacher models to guide a student model via weighted output fusion, feature alignment loss, and dynamic teacher weighting.

Result: The student model outperforms other distillation methods in perplexity, distillation loss, and generation quality.

Conclusion: The method provides an efficient way to compress large language models and highlights the effectiveness of multi-teacher collaboration.

Abstract: This paper addresses the challenges of high computational cost and slow
inference in deploying large language models. It proposes a distillation
strategy guided by multiple teacher models. The method constructs several
teacher models and integrates their output probability distributions and
intermediate semantic features. This guides the student model to learn from
multiple sources of knowledge. As a result, the student model gains stronger
language understanding and generation ability while maintaining a small
parameter size. To achieve this, the paper introduces a weighted output fusion
mechanism, a feature alignment loss function, and an entropy-driven dynamic
teacher weighting strategy. These components improve the quality and stability
of knowledge transfer during distillation. Under multi-teacher guidance, the
student model captures semantic information more effectively and demonstrates
strong performance across multiple evaluation metrics. In particular, the
method shows high consistency in expression, generalization ability, and task
adaptability in tasks such as language modeling, text generation, and
multi-task learning. The experiments compare the proposed method with several
widely adopted distillation approaches. The results further confirm its overall
advantages in perplexity, distillation loss, and generation quality. This study
provides a feasible technical path for the efficient compression of large-scale
language models. It also demonstrates the effectiveness of multi-teacher
collaborative mechanisms in complex language modeling tasks.

</details>


### [251] [SOI Matters: Analyzing Multi-Setting Training Dynamics in Pretrained Language Models via Subsets of Interest](https://arxiv.org/abs/2507.15236)
*Shayan Vassef,Amirhossein Dabiriaghdam,Mohammadreza Bakhtiari,Yadollah Yaghoobzadeh*

Main category: cs.CL

TL;DR: The paper explores how multi-task, multi-lingual, and multi-source learning affect pretrained language models, introducing a framework (SOI) to categorize learning behaviors. Experiments show multi-source learning boosts performance, while multi-task learning has mixed results. A two-stage fine-tuning method further improves results.


<details>
  <summary>Details</summary>
Motivation: To understand and improve the robustness and performance of pretrained language models in multi-setting configurations (multi-task, multi-lingual, multi-source).

Method: Introduces Subsets of Interest (SOI) to categorize learning behaviors. Uses SOI transition heatmaps and dataset cartography for analysis. Tests three parallel comparisons: multi-task vs. single-task, multi-source vs. single-source, and multi-lingual vs. single-lingual learning. Proposes a two-stage fine-tuning approach with SOI-based subset selection.

Result: Multi-source learning improves out-of-distribution performance by up to 7%. Multi-task learning shows mixed results, with gains in similar tasks. The two-stage fine-tuning approach further enhances performance.

Conclusion: The findings offer insights into training dynamics and practical methods for optimizing multi-setting language models, with multi-source learning being particularly effective.

Abstract: This work investigates the impact of multi-task, multi-lingual, and
multi-source learning approaches on the robustness and performance of
pretrained language models. To enhance this analysis, we introduce Subsets of
Interest (SOI), a novel categorization framework that identifies six distinct
learning behavior patterns during training, including forgettable examples,
unlearned examples, and always correct examples. Through SOI transition
heatmaps and dataset cartography visualization, we analyze how examples shift
between these categories when transitioning from single-setting to
multi-setting configurations. We perform comprehensive experiments across three
parallel comparisons: multi-task vs. single-task learning using English tasks
(entailment, paraphrase, sentiment), multi-source vs. single-source learning
using sentiment analysis datasets, and multi-lingual vs. single-lingual
learning using intent classification in French, English, and Persian. Our
results demonstrate that multi-source learning consistently improves
out-of-distribution performance by up to 7%, while multi-task learning shows
mixed results with notable gains in similar task combinations. We further
introduce a two-stage fine-tuning approach where the second stage leverages
SOI-based subset selection to achieve additional performance improvements.
These findings provide new insights into training dynamics and offer practical
approaches for optimizing multi-setting language model performance.

</details>


### [252] [ChiMed 2.0: Advancing Chinese Medical Dataset in Facilitating Large Language Modeling](https://arxiv.org/abs/2507.15275)
*Yuanhe Tian,Junjie Liu,Zhizhou Kou,Yuxiang Li,Yan Song*

Main category: cs.CL

TL;DR: ChiMed 2.0 is a large-scale Chinese medical dataset supporting pre-training, SFT, and RLHF, validated by improved LLM performance.


<details>
  <summary>Details</summary>
Motivation: Existing Chinese medical datasets are limited and lack diversity, hindering effective AI research and applications.

Method: Extends ChiMed with data from online platforms and LLMs, covering 204.4M characters for pre-training, SFT, and RLHF.

Result: Experiments show performance gains across model scales, validating dataset effectiveness.

Conclusion: ChiMed 2.0 addresses dataset limitations and supports diverse training needs for Chinese medical LLMs.

Abstract: Building high-quality data resources is crucial for advancing artificial
intelligence research and applications in specific domains, particularly in the
Chinese medical domain. Existing Chinese medical datasets are limited in size
and narrow in domain coverage, falling short of the diverse corpora required
for effective pre-training. Moreover, most datasets are designed solely for LLM
fine-tuning and do not support pre-training and reinforcement learning from
human feedback (RLHF). In this paper, we propose a Chinese medical dataset
named ChiMed 2.0, which extends our previous work ChiMed, and covers data
collected from Chinese medical online platforms and generated by LLMs. ChiMed
2.0 contains 204.4M Chinese characters covering both traditional Chinese
medicine classics and modern general medical data, where there are 164.8K
documents for pre-training, 351.6K question-answering pairs for supervised
fine-tuning (SFT), and 41.7K preference data tuples for RLHF. To validate the
effectiveness of our approach for training a Chinese medical LLM, we conduct
further pre-training, SFT, and RLHF experiments on representative general
domain LLMs and evaluate their performance on medical benchmark datasets. The
results show performance gains across different model scales, validating the
dataset's effectiveness and applicability.

</details>


### [253] [A Novel Self-Evolution Framework for Large Language Models](https://arxiv.org/abs/2507.15281)
*Haoran Sun,Zekun Zhang,Shaoning Zeng*

Main category: cs.CL

TL;DR: The paper introduces a Dual-Phase Self-Evolution (DPSE) framework to enhance LLMs by jointly optimizing user preference adaptation and domain-specific competence, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing post-training strategies for LLMs improve user alignment but lack domain cognition enhancement, creating a gap this work addresses.

Method: DPSE uses a Censor module for multi-dimensional interaction signals, guiding structured data expansion and a two-stage fine-tuning pipeline.

Result: DPSE outperforms baselines in general NLP benchmarks and long-term dialogue tasks, with ablation studies confirming module contributions.

Conclusion: The DPSE framework offers an autonomous path for LLMs' continual self-evolution, bridging the gap in domain cognition and user alignment.

Abstract: The capabilities of Large Language Models (LLMs) are limited to some extent
by pre-training, so some researchers optimize LLMs through post-training.
Existing post-training strategies, such as memory-based retrieval or preference
optimization, improve user alignment yet fail to enhance the model's domain
cognition. To bridge this gap, we propose a novel Dual-Phase Self-Evolution
(DPSE) framework that jointly optimizes user preference adaptation and
domain-specific competence. DPSE introduces a Censor module to extract
multi-dimensional interaction signals and estimate satisfaction scores, which
guide structured data expansion via topic-aware and preference-driven
strategies. These expanded datasets support a two-stage fine-tuning pipeline:
supervised domain grounding followed by frequency-aware preference
optimization. Experiments across general NLP benchmarks and long-term dialogue
tasks demonstrate that DPSE consistently outperforms Supervised Fine-Tuning,
Preference Optimization, and Memory-Augmented baselines. Ablation studies
validate the contribution of each module. In this way, our framework provides
an autonomous path toward continual self-evolution of LLMs.

</details>


### [254] [Beyond Easy Wins: A Text Hardness-Aware Benchmark for LLM-generated Text Detection](https://arxiv.org/abs/2507.15286)
*Navid Ayoobi,Sadat Shahriar,Arjun Mukherjee*

Main category: cs.CL

TL;DR: SHIELD introduces a new AI text detector evaluation paradigm focusing on real-world reliability and stability, addressing overlooked practical deployment challenges.


<details>
  <summary>Details</summary>
Motivation: Current AI text detector evaluations rely on conventional metrics like AUROC, ignoring critical real-world factors like false positive rates and stability across domains.

Method: SHIELD integrates reliability and stability into a unified metric and introduces a model-agnostic humanification framework with a hardness parameter to challenge detectors.

Result: The benchmark effectively evaluates and challenges state-of-the-art zero-shot detection methods in maintaining reliability and stability.

Conclusion: SHIELD provides a practical and equitable assessment framework for AI text detectors, addressing gaps in prior research.

Abstract: We present a novel evaluation paradigm for AI text detectors that prioritizes
real-world and equitable assessment. Current approaches predominantly report
conventional metrics like AUROC, overlooking that even modest false positive
rates constitute a critical impediment to practical deployment of detection
systems. Furthermore, real-world deployment necessitates predetermined
threshold configuration, making detector stability (i.e. the maintenance of
consistent performance across diverse domains and adversarial scenarios), a
critical factor. These aspects have been largely ignored in previous research
and benchmarks. Our benchmark, SHIELD, addresses these limitations by
integrating both reliability and stability factors into a unified evaluation
metric designed for practical assessment. Furthermore, we develop a post-hoc,
model-agnostic humanification framework that modifies AI text to more closely
resemble human authorship, incorporating a controllable hardness parameter.
This hardness-aware approach effectively challenges current SOTA zero-shot
detection methods in maintaining both reliability and stability. (Data and
code: https://github.com/navid-aub/SHIELD-Benchmark)

</details>


### [255] [On the Inevitability of Left-Leaning Political Bias in Aligned Language Models](https://arxiv.org/abs/2507.15328)
*Thilo Hagendorff*

Main category: cs.CL

TL;DR: The paper argues that left-wing political bias in LLMs is inherent to AI alignment goals (HHH principles) due to their alignment with progressive values, while right-wing ideologies often conflict with these principles. It critiques the framing of left-leaning bias as problematic, suggesting this undermines alignment efforts.


<details>
  <summary>Details</summary>
Motivation: To reconcile the apparent contradiction between AI alignment goals (HHH principles) and critiques of left-wing bias in LLMs, showing that such bias is a natural outcome of alignment with progressive values.

Method: Theoretical argumentation linking normative assumptions of AI alignment (harm avoidance, inclusivity, fairness, truthfulness) to progressive moral frameworks and left-wing principles.

Result: Left-wing bias in LLMs is not a flaw but a necessary outcome of alignment with HHH principles, while right-wing ideologies often conflict with these goals.

Conclusion: Framing left-leaning bias as problematic undermines AI alignment; instead, it should be recognized as inherent to achieving harmless, helpful, and honest AI systems.

Abstract: The guiding principle of AI alignment is to train large language models
(LLMs) to be harmless, helpful, and honest (HHH). At the same time, there are
mounting concerns that LLMs exhibit a left-wing political bias. Yet, the
commitment to AI alignment cannot be harmonized with the latter critique. In
this article, I argue that intelligent systems that are trained to be harmless
and honest must necessarily exhibit left-wing political bias. Normative
assumptions underlying alignment objectives inherently concur with progressive
moral frameworks and left-wing principles, emphasizing harm avoidance,
inclusivity, fairness, and empirical truthfulness. Conversely, right-wing
ideologies often conflict with alignment guidelines. Yet, research on political
bias in LLMs is consistently framing its insights about left-leaning tendencies
as a risk, as problematic, or concerning. This way, researchers are actively
arguing against AI alignment, tacitly fostering the violation of HHH
principles.

</details>


### [256] [Reasoning Models are Test Exploiters: Rethinking Multiple-Choice](https://arxiv.org/abs/2507.15337)
*Narun Raman,Taylor Lundy,Kevin Leyton-Brown*

Main category: cs.CL

TL;DR: The paper evaluates if multiple-choice question-answering (MCQA) remains a reliable proxy for assessing LLMs' downstream performance, finding it effective only with pre-option reasoning. Post-option reasoning exploits choices, skewing results.


<details>
  <summary>Details</summary>
Motivation: To determine if MCQA benchmarks still accurately reflect the reasoning capabilities of state-of-the-art LLMs, given their evolving performance and potential biases.

Method: Systematic evaluation of 15 QA benchmarks and 25 LLMs, testing 5 question-presentation variations, including pre- and post-option reasoning and choice manipulation.

Result: MCQA works as a proxy only if models reason before seeing options. Post-option reasoning leads to inflated performance, making MCQA unreliable for modern LLMs.

Conclusion: MCQA is no longer a valid proxy for downstream performance. New benchmarks are needed to better assess LLMs' true reasoning abilities without bias.

Abstract: When evaluating Large Language Models (LLMs) in question-answering domains,
it is common to ask the model to choose among a fixed set of choices (so-called
multiple-choice question-answering, or MCQA). Although downstream tasks of
interest typically do not provide systems with explicit options among which to
choose, this approach is nevertheless widely used because it makes it makes
automatic grading straightforward and has tended to produce challenging
benchmarks that correlate sufficiently well with downstream performance. This
paper investigates the extent to which this trend continues to hold for
state-of-the-art reasoning models, describing a systematic evaluation of $15$
different question-answering benchmarks (e.g., MMLU, HLE) and $25$ different
LLMs (including small models such as Qwen 7B and relatively large models such
as Llama 70B). For each model-benchmark pair, we considered $5$ ways of
presenting the model with questions, including variations on whether multiple
choices were offered to the model at all; whether "none of the above" sometimes
replaced the right answer; and whether the model was permitted to perform
chain-of-thought reasoning before and/or after the choices were presented. MCQA
remained a good proxy for the downstream performance of models as long as they
were allowed to perform chain-of-thought reasoning only before being presented
with the options among which they had to select. On the other hand, large
models that were able to perform reasoning after being given a set of options
tended to significantly outperform their free-text performance due to
exploiting the information in the options. We conclude that MCQA is no longer a
good proxy for assessing downstream performance of state-of-the-art models, and
offer practical guidelines for designing more robust, bias-resistant benchmarks
that better reflect LLMs' genuine reasoning capabilities.

</details>


### [257] [LionGuard 2: Building Lightweight, Data-Efficient & Localised Multilingual Content Moderators](https://arxiv.org/abs/2507.15339)
*Leanne Tan,Gabriel Chua,Ziyu Ge,Roy Ka-Wei Lee*

Main category: cs.CL

TL;DR: LionGuard 2 is a lightweight, multilingual moderation classifier for Singapore, outperforming commercial systems without fine-tuning large models.


<details>
  <summary>Details</summary>
Motivation: Addressing gaps in multilingual moderation, especially for low-resource languages and local contexts like Singapore.

Method: Uses pre-trained OpenAI embeddings and a multi-head ordinal classifier, tailored for English, Chinese, Malay, and partial Tamil.

Result: Outperforms commercial and open-source systems on 17 benchmarks, including Singapore-specific datasets.

Conclusion: High-quality local data and robust embeddings enable strong moderation without large models; model weights and training data are released.

Abstract: Modern moderation systems increasingly support multiple languages, but often
fail to address localisation and low-resource variants - creating safety gaps
in real-world deployments. Small models offer a potential alternative to large
LLMs, yet still demand considerable data and compute. We present LionGuard 2, a
lightweight, multilingual moderation classifier tailored to the Singapore
context, supporting English, Chinese, Malay, and partial Tamil. Built on
pre-trained OpenAI embeddings and a multi-head ordinal classifier, LionGuard 2
outperforms several commercial and open-source systems across 17 benchmarks,
including both Singapore-specific and public English datasets. The system is
actively deployed within the Singapore Government, demonstrating practical
efficacy at scale. Our findings show that high-quality local data and robust
multilingual embeddings can achieve strong moderation performance, without
fine-tuning large models. We release our model weights and part of our training
data to support future work on LLM safety.

</details>


### [258] [Probing Information Distribution in Transformer Architectures through Entropy Analysis](https://arxiv.org/abs/2507.15347)
*Amedeo Buonanno,Alessandro Rivetti,Francesco A. N. Palmieri,Giovanni Di Gennaro,Gianmarco Romano*

Main category: cs.CL

TL;DR: The paper uses entropy analysis to study information distribution in Transformer models, focusing on token-level uncertainty and processing stages, with GPT as a case study.


<details>
  <summary>Details</summary>
Motivation: To understand how information is managed and transformed in Transformer models and improve interpretability.

Method: Entropy analysis is applied to quantify token-level uncertainty and examine entropy patterns across processing stages in a GPT-based model.

Result: The approach reveals insights into model behavior and internal representations.

Conclusion: This method can enhance interpretability and evaluation frameworks for Transformer models.

Abstract: This work explores entropy analysis as a tool for probing information
distribution within Transformer-based architectures. By quantifying token-level
uncertainty and examining entropy patterns across different stages of
processing, we aim to investigate how information is managed and transformed
within these models. As a case study, we apply the methodology to a GPT-based
large language model, illustrating its potential to reveal insights into model
behavior and internal representations. This approach may offer insights into
model behavior and contribute to the development of interpretability and
evaluation frameworks for transformer-based models

</details>


### [259] [Metaphor and Large Language Models: When Surface Features Matter More than Deep Understanding](https://arxiv.org/abs/2507.15357)
*Elisa Sanchez-Bayona,Rodrigo Agerri*

Main category: cs.CL

TL;DR: The paper evaluates LLMs' metaphor interpretation across diverse datasets and tasks, finding performance driven by surface features rather than metaphor understanding.


<details>
  <summary>Details</summary>
Motivation: Address limitations of prior research by evaluating LLMs on realistic, diverse datasets for metaphor interpretation in NLP.

Method: Extensive experiments using publicly available datasets for NLI and QA tasks, analyzing performance across prompt configurations.

Result: LLMs' performance is influenced by lexical overlap and sentence length, not metaphorical content, debunking claims of emergent metaphor understanding.

Conclusion: Highlights LLMs' limitations in figurative language processing and calls for more realistic evaluation frameworks.

Abstract: This paper presents a comprehensive evaluation of the capabilities of Large
Language Models (LLMs) in metaphor interpretation across multiple datasets,
tasks, and prompt configurations. Although metaphor processing has gained
significant attention in Natural Language Processing (NLP), previous research
has been limited to single-dataset evaluations and specific task settings,
often using artificially constructed data through lexical replacement. We
address these limitations by conducting extensive experiments using diverse
publicly available datasets with inference and metaphor annotations, focusing
on Natural Language Inference (NLI) and Question Answering (QA) tasks. The
results indicate that LLMs' performance is more influenced by features like
lexical overlap and sentence length than by metaphorical content, demonstrating
that any alleged emergent abilities of LLMs to understand metaphorical language
are the result of a combination of surface-level features, in-context learning,
and linguistic knowledge. This work provides critical insights into the current
capabilities and limitations of LLMs in processing figurative language,
highlighting the need for more realistic evaluation frameworks in metaphor
interpretation tasks. Data and code are publicly available.

</details>


### [260] [STITCH: Simultaneous Thinking and Talking with Chunked Reasoning for Spoken Language Models](https://arxiv.org/abs/2507.15375)
*Cheng-Han Chiang,Xiaofei Wang,Linjie Li,Chung-Ching Lin,Kevin Lin,Shujie Liu,Zhendong Wang,Zhengyuan Yang,Hung-yi Lee,Lijuan Wang*

Main category: cs.CL

TL;DR: Stitch is a novel method enabling Spoken Language Models (SLMs) to alternate between unspoken reasoning and spoken responses, reducing latency while improving reasoning performance.


<details>
  <summary>Details</summary>
Motivation: Current SLMs lack internal reasoning like humans, leading to unclear or delayed responses. Integrating unspoken thought processes is desired.

Method: Stitch alternates between generating unspoken reasoning chunks and spoken response chunks, leveraging audio playback time for reasoning.

Result: Stitch matches baseline latency while outperforming them by 15% on math reasoning tasks and performs equally on non-reasoning tasks.

Conclusion: Stitch effectively integrates reasoning into SLMs without added latency, enhancing performance and maintaining responsiveness.

Abstract: Spoken Language Models (SLMs) are designed to take speech inputs and produce
spoken responses. However, current SLMs lack the ability to perform an
internal, unspoken thinking process before responding. In contrast, humans
typically engage in complex mental reasoning internally, enabling them to
communicate ideas clearly and concisely. Thus, integrating an unspoken thought
process into SLMs is highly desirable. While naively generating a complete
chain-of-thought (CoT) reasoning before starting to talk can enable thinking
for SLMs, this induces additional latency for the speech response, as the CoT
reasoning can be arbitrarily long. To solve this issue, we propose Stitch, a
novel generation method that alternates between the generation of unspoken
reasoning chunks and spoken response chunks. Since the audio duration of a
chunk of spoken response is much longer than the time to generate the tokens in
a chunk of spoken response, we use the remaining free time to generate the
unspoken reasoning tokens. When a chunk of audio is played to the user, the
model continues to generate the next unspoken reasoning chunk, achieving
simultaneous thinking and talking. Remarkably, Stitch matches the latency of
baselines that cannot generate unspoken CoT by design while outperforming those
baselines by 15% on math reasoning datasets; Stitch also performs equally well
on non-reasoning datasets as those baseline models. Some animations and
demonstrations are on the project page: https://d223302.github.io/STITCH.

</details>


### [261] [AlgoSimBench: Identifying Algorithmically Similar Problems for Competitive Programming](https://arxiv.org/abs/2507.15378)
*Jierui Li,Raymond Mooney*

Main category: cs.CL

TL;DR: The paper introduces AlgoSimBench to test LLMs' ability to identify algorithmically similar problems (ASPs), revealing their struggles and proposing a method (ASM) to improve accuracy.


<details>
  <summary>Details</summary>
Motivation: Assess whether LLMs' problem-solving abilities generalize to less-seen domains, specifically identifying ASPs.

Method: Introduces AlgoSimBench with 1317 problems and 402 MCQs, evaluates LLMs, and proposes ASM for improved similarity detection.

Result: LLMs struggle with ASP identification (best model: 65.9% accuracy). ASM improves accuracy by 6.7%-11.7%. Code embedding models and retrieval methods are also evaluated.

Conclusion: ASM enhances LLMs' ASP detection, and combining it with BM25 yields better results. The benchmark and findings highlight limitations and potential improvements.

Abstract: Recent progress in LLMs, such as reasoning models, has demonstrated strong
abilities to solve complex competitive programming problems, often rivaling top
human competitors. However, it remains underexplored whether these abilities
generalize to relevant domains that are less seen during training. To address
this, we introduce AlgoSimBench, a new benchmark designed to assess LLMs'
ability to identify algorithmically similar problems (ASPs)-problems that can
be solved using similar algorithmic approaches. AlgoSimBench consists of 1317
problems, annotated with 231 distinct fine-grained algorithm tags, from which
we curate 402 multiple-choice questions (MCQs), where each question presents
one algorithmically similar problem alongside three textually similar but
algorithmically dissimilar distractors. Our evaluation reveals that LLMs
struggle to identify ASPs, with the best-performing model (o3-mini) achieving
only 65.9% accuracy on the MCQ task. To address this challenge, we propose
attempted solution matching (ASM), a novel method for improving problem
similarity detection. On our MCQ task, ASM yields an absolute accuracy
improvement of 6.7% to 11.7% across different models. We also evaluated code
embedding models and retrieval methods on similar problem identification. While
the adversarial selection of problems degrades the performance to be less than
random, we found that simply summarizing the problem to remove narrative
elements eliminates the effect, and combining ASM with a keyword-prioritized
method, BM25, can yield up to 52.2% accuracy. Code and data are available at
github.com

</details>


### [262] [ASPERA: A Simulated Environment to Evaluate Planning for Complex Action Execution](https://arxiv.org/abs/2507.15501)
*Alexandru Coca,Mark Gaynor,Zhenxing Zhang,Jianpeng Cheng,Bo-Hsiang Tseng,Pete Boothroyd,Héctor Martinez Alonso,Diarmuid Ó Séaghdha,Anders Johannsen*

Main category: cs.CL

TL;DR: ASPERA framework evaluates LLMs for digital assistants, addressing data and evaluation challenges with a human-assisted engine and Asper-Bench dataset.


<details>
  <summary>Details</summary>
Motivation: To assess LLMs' ability to execute complex actions for digital assistants using pre-trained programming knowledge.

Method: Developed ASPERA framework with a simulation and human-assisted LLM data generation engine to create high-quality tasks.

Result: Program generation with custom libraries is harder for LLMs than dependency-free code, as shown by Asper-Bench.

Conclusion: ASPERA and Asper-Bench highlight challenges in LLM-powered digital assistants, emphasizing the need for robust evaluation.

Abstract: This work evaluates the potential of large language models (LLMs) to power
digital assistants capable of complex action execution. These assistants rely
on pre-trained programming knowledge to execute multi-step goals by composing
objects and functions defined in assistant libraries into action execution
programs. To achieve this, we develop ASPERA, a framework comprising an
assistant library simulation and a human-assisted LLM data generation engine.
Our engine allows developers to guide LLM generation of high-quality tasks
consisting of complex user queries, simulation state and corresponding
validation programs, tackling data availability and evaluation robustness
challenges. Alongside the framework we release Asper-Bench, an evaluation
dataset of 250 challenging tasks generated using ASPERA, which we use to show
that program generation grounded in custom assistant libraries is a significant
challenge to LLMs compared to dependency-free code generation.

</details>


### [263] [Step-level Verifier-guided Hybrid Test-Time Scaling for Large Language Models](https://arxiv.org/abs/2507.15512)
*Kaiyan Chang,Yonghao Shi,Chenglong Wang,Hang Zhou,Chi Hu,Xiaoqian Liu,Yingfeng Luo,Yuan Ge,Tong Xiao,Jingbo Zhu*

Main category: cs.CL

TL;DR: Hybrid Test-Time Scaling (TTS) combines fine-grained sequential and parallel scaling methods to enhance reasoning in LLMs without additional training overhead.


<details>
  <summary>Details</summary>
Motivation: Training-based TTS methods increase computational burden, so the paper focuses on training-free TTS methods to improve reasoning in LLMs.

Method: Proposes Conditional Step-level Self-refinement (sequential scaling) and combines it with parallel scaling methods to create Hybrid TTS.

Result: Experiments on 3B-14B LLMs show Hybrid TTS significantly boosts reasoning performance.

Conclusion: Training-free Hybrid TTS has strong potential to expand reasoning capabilities in LLMs.

Abstract: Test-Time Scaling (TTS) is a promising approach to progressively elicit the
model's intelligence during inference. Recently, training-based TTS methods,
such as continued reinforcement learning (RL), have further surged in
popularity, while training-free TTS methods are gradually fading from
prominence. However, the additional computation overhead of training amplifies
the burden on test-time scaling. In this paper, we focus on training-free TTS
methods for reasoning. We first design Conditional Step-level Self-refinement,
a fine-grained sequential scaling method guided by process verification. On top
of its effectiveness, we further combine it with other classical parallel
scaling methods at the step level, to introduce a novel inference paradigm
called Hybrid Test-Time Scaling. Extensive experiments on five
instruction-tuned LLMs across different scales (3B-14B) and families
demonstrate that hybrid strategy incorporating various training-free TTS
methods at a fine granularity has considerable potential for expanding the
reasoning performance boundaries of LLMs.

</details>


### [264] [Evaluating Text Style Transfer: A Nine-Language Benchmark for Text Detoxification](https://arxiv.org/abs/2507.15557)
*Vitaly Protasov,Nikolay Babakov,Daryna Dementieva,Alexander Panchenko*

Main category: cs.CL

TL;DR: The paper addresses the challenge of evaluating text style transfer (TST) in multilingual settings, focusing on text detoxification across nine languages. It compares neural-based evaluation models and LLM-as-a-judge approaches, offering practical insights for reliable evaluation.


<details>
  <summary>Details</summary>
Motivation: The gap between automatic metrics and human judgments in TST evaluation, especially in multilingual contexts, motivates this study. Prior work's English-centric focus leaves multilingual TST evaluation underexplored.

Method: The study conducts a comprehensive multilingual evaluation of text detoxification systems across nine languages, comparing neural-based models and LLM-as-a-judge approaches inspired by machine translation.

Result: The findings highlight the effectiveness of modern evaluation methods and provide actionable recommendations for designing reliable multilingual TST evaluation pipelines.

Conclusion: The paper concludes with practical guidelines for improving multilingual TST evaluation, particularly in text detoxification, bridging the gap between automatic metrics and human judgments.

Abstract: Despite recent progress in large language models (LLMs), evaluation of text
generation tasks such as text style transfer (TST) remains a significant
challenge. Recent studies (Dementieva et al., 2024; Pauli et al., 2025)
revealed a substantial gap between automatic metrics and human judgments.
Moreover, most prior work focuses exclusively on English, leaving multilingual
TST evaluation largely unexplored. In this paper, we perform the first
comprehensive multilingual study on evaluation of text detoxification system
across nine languages: English, Spanish, German, Chinese, Arabic, Hindi,
Ukrainian, Russian, Amharic. Drawing inspiration from the machine translation,
we assess the effectiveness of modern neural-based evaluation models alongside
prompting-based LLM-as-a-judge approaches. Our findings provide a practical
recipe for designing more reliable multilingual TST evaluation pipeline in the
text detoxification case.

</details>


### [265] [Smart Eyes for Silent Threats: VLMs and In-Context Learning for THz Imaging](https://arxiv.org/abs/2507.15576)
*Nicolas Poggi,Shashank Agnihotri,Margret Keuper*

Main category: cs.CL

TL;DR: The paper introduces In-Context Learning (ICL) with Vision-Language Models (VLMs) for THz image classification, addressing challenges like limited annotations and low resolution. It shows improved performance and interpretability without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: THz imaging faces challenges in classification due to limited data and visual ambiguity. The goal is to provide a flexible, interpretable solution without fine-tuning.

Method: The authors adapt two open-weight VLMs to the THz domain using a modality-aligned prompting framework, evaluating them in zero-shot and one-shot settings.

Result: ICL-enhanced VLMs improve classification and interpretability in low-data regimes, marking the first application of this approach to THz imaging.

Conclusion: ICL with VLMs offers a promising solution for THz image classification, especially in resource-constrained domains.

Abstract: Terahertz (THz) imaging enables non-invasive analysis for applications such
as security screening and material classification, but effective image
classification remains challenging due to limited annotations, low resolution,
and visual ambiguity. We introduce In-Context Learning (ICL) with
Vision-Language Models (VLMs) as a flexible, interpretable alternative that
requires no fine-tuning. Using a modality-aligned prompting framework, we adapt
two open-weight VLMs to the THz domain and evaluate them under zero-shot and
one-shot settings. Our results show that ICL improves classification and
interpretability in low-data regimes. This is the first application of
ICL-enhanced VLMs to THz imaging, offering a promising direction for
resource-constrained scientific domains. Code:
\href{https://github.com/Nicolas-Poggi/Project_THz_Classification/tree/main}{GitHub
repository}.

</details>


### [266] [Learning to Extract Rational Evidence via Reinforcement Learning for Retrieval-Augmented Generation](https://arxiv.org/abs/2507.15586)
*Xinping Zhao,Shouzheng Huang,Yan Zhong,Xinshuo Hu,Baotian Hu,Min Zhang*

Main category: cs.CL

TL;DR: LEAR improves RAG by learning to extract rational evidence through explicit reasoning and conscious extraction, enhancing LLM accuracy.


<details>
  <summary>Details</summary>
Motivation: Retrieval noises degrade LLM generation quality, and existing methods lack explicit reasoning, risking key clue omission and poor generalization.

Method: LEAR combines evidence reasoning and extraction into unified training, uses knowledge token masks, and applies verifiable rewards for optimization.

Result: LEAR outperforms on benchmarks, providing compact, high-quality evidence and improving downstream task accuracy.

Conclusion: LEAR effectively denoises retrieval for RAG, enhancing LLM performance and practical application.

Abstract: Retrieval-Augmented Generation (RAG) effectively improves the accuracy of
Large Language Models (LLMs). However, retrieval noises significantly impact
the quality of LLMs' generation, necessitating the development of denoising
mechanisms. Previous methods extract evidence straightforwardly without
explicit thinking, which risks filtering out key clues and struggles with
generalization. To this end, we propose LEAR, which learns to extract rational
evidence by (1) explicitly reasoning to identify potential cues within
retrieval contents first, and then (2) consciously extracting to avoid omitting
any key cues helpful for answering questions. Specifically, we frame evidence
reasoning and evidence extraction into one unified response for end-to-end
training; apply knowledge token masks for disentanglement to derive
reasoning-based and extraction-based answers; and devise three types of
verifiable reward functions, including answer, length, and format, to update
the model via the policy optimization algorithm. Extensive experiments on three
benchmark datasets show the effectiveness of LEAR, providing compact and
high-quality evidence, improving the accuracy of downstream tasks, and
promoting effective application in online RAG systems.

</details>


### [267] [Conflicting narratives and polarization on social media](https://arxiv.org/abs/2507.15600)
*Armin Pournaki*

Main category: cs.CL

TL;DR: The paper analyzes conflicting narratives in political discourse on Twitter to understand polarization and issue alignment, focusing on topics like Ukraine, Covid, and climate change.


<details>
  <summary>Details</summary>
Motivation: To explore how conflicting narratives reveal discursive mechanisms of polarization and issue alignment in public discourse.

Method: Analyzed tweets from opposing opinion groups in the German Twittersphere (2021-2023), extracting signals of conflicting narratives for selected issues.

Result: Found evidence of conflicting narratives in role attributions and emplotment, and identified narrative alignment as a discursive strategy.

Conclusion: Narratives serve as a valuable analytical tool for understanding polarization in political discourse.

Abstract: Narratives are key interpretative devices by which humans make sense of
political reality. In this work, we show how the analysis of conflicting
narratives, i.e. conflicting interpretive lenses through which political
reality is experienced and told, provides insight into the discursive
mechanisms of polarization and issue alignment in the public sphere. Building
upon previous work that has identified ideologically polarized issues in the
German Twittersphere between 2021 and 2023, we analyze the discursive dimension
of polarization by extracting textual signals of conflicting narratives from
tweets of opposing opinion groups. Focusing on a selection of salient issues
and events (the war in Ukraine, Covid, climate change), we show evidence for
conflicting narratives along two dimensions: (i) different attributions of
actantial roles to the same set of actants (e.g. diverging interpretations of
the role of NATO in the war in Ukraine), and (ii) emplotment of different
actants for the same event (e.g. Bill Gates in the right-leaning Covid
narrative). Furthermore, we provide first evidence for patterns of narrative
alignment, a discursive strategy that political actors employ to align opinions
across issues. These findings demonstrate the use of narratives as an
analytical lens into the discursive mechanisms of polarization.

</details>


### [268] [Leveraging Context for Multimodal Fallacy Classification in Political Debates](https://arxiv.org/abs/2507.15641)
*Alessio Pittiglio*

Main category: cs.CL

TL;DR: The paper presents a multimodal approach for detecting logical fallacies in political debates, achieving competitive results with text and multimodal models.


<details>
  <summary>Details</summary>
Motivation: To advance research in multimodal argument mining, specifically focusing on identifying logical fallacies in political debates.

Method: Uses pretrained Transformer-based models and explores leveraging context in multimodal data (text, audio).

Result: Achieved macro F1-scores of 0.4444 (text), 0.3559 (audio), and 0.4403 (multimodal), with multimodal performance comparable to text-only.

Conclusion: The multimodal model shows promise, indicating potential for further improvements in fallacy detection.

Abstract: In this paper, we present our submission to the MM-ArgFallacy2025 shared
task, which aims to advance research in multimodal argument mining, focusing on
logical fallacies in political debates. Our approach uses pretrained
Transformer-based models and proposes several ways to leverage context. In the
fallacy classification subtask, our models achieved macro F1-scores of 0.4444
(text), 0.3559 (audio), and 0.4403 (multimodal). Our multimodal model showed
performance comparable to the text-only model, suggesting potential for
improvements.

</details>


### [269] [P3: Prompts Promote Prompting](https://arxiv.org/abs/2507.15675)
*Xinyu Zhang,Yuanquan Hu,Fangchao Liu,Zhicheng Dou*

Main category: cs.CL

TL;DR: P3 is a self-improvement framework that concurrently optimizes system and user prompts through iteration, outperforming unilateral approaches in LLM applications.


<details>
  <summary>Details</summary>
Motivation: Unilateral optimization of system or user prompts often leads to suboptimal results due to their interdependence.

Method: P3 introduces a framework for concurrent optimization of both prompts through an iterative process, with offline prompts aiding online query-dependent optimization.

Result: P3 achieves superior performance in general and reasoning tasks, demonstrating the effectiveness of holistic optimization.

Conclusion: Holistic optimization of system and user prompts enhances LLM performance across diverse domains.

Abstract: Current large language model (LLM) applications often employ multi-component
prompts, comprising both system and user prompts, to guide model behaviors.
While recent advancements have demonstrated the efficacy of automatically
optimizing either the system or user prompt to boost performance, such
unilateral approaches often yield suboptimal outcomes due to the interdependent
nature of these components. In this work, we introduce P3, a novel
self-improvement framework that concurrently optimizes both system and user
prompts through an iterative process. The offline optimized prompts are further
leveraged to promote online prompting by performing query-dependent prompt
optimization. Extensive experiments on general tasks (e.g., Arena-hard and
Alpaca-eval) and reasoning tasks (e.g., GSM8K and GPQA) demonstrate that P3
achieves superior performance in the realm of automatic prompt optimization.
Our results highlight the effectiveness of a holistic optimization strategy in
enhancing LLM performance across diverse domains.

</details>


### [270] [CoLD: Counterfactually-Guided Length Debiasing for Process Reward Models](https://arxiv.org/abs/2507.15698)
*Congmin Zheng,Jiachen Zhu,Jianghao Lin,Xinyi Dai,Yong Yu,Weinan Zhang,Mengyue Yang*

Main category: cs.CL

TL;DR: CoLD mitigates length bias in Process Reward Models (PRMs) for LLMs by using counterfactual reasoning and causal analysis, improving reward predictions and reasoning conciseness.


<details>
  <summary>Details</summary>
Motivation: Existing PRMs exhibit length bias, favoring longer reasoning steps regardless of semantic validity, reducing reliability and encouraging verbosity.

Method: CoLD introduces three components: length-penalty adjustment, learned bias estimator, and joint training for length-invariant rewards, grounded in counterfactual reasoning.

Result: Experiments on MATH500 and GSM-Plus show reduced reward-length correlation, better step selection, and more concise, valid reasoning.

Conclusion: CoLD effectively improves PRM fidelity and robustness by addressing length bias.

Abstract: Process Reward Models (PRMs) play a central role in evaluating and guiding
multi-step reasoning in large language models (LLMs), especially for
mathematical problem solving. However, we identify a pervasive length bias in
existing PRMs: they tend to assign higher scores to longer reasoning steps,
even when the semantic content and logical validity are unchanged. This bias
undermines the reliability of reward predictions and leads to overly verbose
outputs during inference. To address this issue, we propose
CoLD(Counterfactually-Guided Length Debiasing), a unified framework that
mitigates length bias through three components: an explicit length-penalty
adjustment, a learned bias estimator trained to capture spurious length-related
signals, and a joint training strategy that enforces length-invariance in
reward predictions. Our approach is grounded in counterfactual reasoning and
informed by causal graph analysis. Extensive experiments on MATH500 and
GSM-Plus show that CoLD consistently reduces reward-length correlation,
improves accuracy in step selection, and encourages more concise, logically
valid reasoning. These results demonstrate the effectiveness and practicality
of CoLD in improving the fidelity and robustness of PRMs.

</details>


### [271] [Compositional Understanding in Signaling Games](https://arxiv.org/abs/2507.15706)
*David Peter Wallis Freeborn*

Main category: cs.CL

TL;DR: The paper addresses the issue of receivers in signaling games failing to learn compositional information, proposing two new models for genuine compositional understanding.


<details>
  <summary>Details</summary>
Motivation: Standard signaling game models struggle with compositional learning, as receivers lose information from all components if one is forgotten.

Method: Two new models are introduced: a minimalist receiver learning from atomic messages and a generalist receiver utilizing all available information.

Result: The proposed models are simpler and enable receivers to learn from atomic message components.

Conclusion: The new models successfully evolve genuine compositional understanding in signaling games.

Abstract: Receivers in standard signaling game models struggle with learning
compositional information. Even when the signalers send compositional messages,
the receivers do not interpret them compositionally. When information from one
message component is lost or forgotten, the information from other components
is also erased. In this paper I construct signaling game models in which
genuine compositional understanding evolves. I present two new models: a
minimalist receiver who only learns from the atomic messages of a signal, and a
generalist receiver who learns from all of the available information. These
models are in many ways simpler than previous alternatives, and allow the
receivers to learn from the atomic components of messages.

</details>


### [272] [Is Large Language Model Performance on Reasoning Tasks Impacted by Different Ways Questions Are Asked?](https://arxiv.org/abs/2507.15707)
*Seok Hwan Song,Mohna Chakraborty,Qi Li,Wallapak Tavanapong*

Main category: cs.CL

TL;DR: The study explores how different question types affect LLM accuracy in reasoning tasks, revealing performance variations and factors like question wording and options.


<details>
  <summary>Details</summary>
Motivation: To understand the impact of question types on LLM performance in reasoning tasks, an area not previously explored.

Method: Evaluated five LLMs on three question types (multiple-choice, true/false, short/long answers) using quantitative and deductive reasoning tasks, measuring accuracy in reasoning steps and final answer selection.

Result: (1) Performance varies significantly by question type. (2) Reasoning accuracy doesn't always align with final answer accuracy. (3) Question wording and number of options influence LLM performance.

Conclusion: Question type and design significantly impact LLM accuracy in reasoning tasks, highlighting the need for careful evaluation framework design.

Abstract: Large Language Models (LLMs) have been evaluated using diverse question
types, e.g., multiple-choice, true/false, and short/long answers. This study
answers an unexplored question about the impact of different question types on
LLM accuracy on reasoning tasks. We investigate the performance of five LLMs on
three different types of questions using quantitative and deductive reasoning
tasks. The performance metrics include accuracy in the reasoning steps and
choosing the final answer. Key Findings: (1) Significant differences exist in
LLM performance across different question types. (2) Reasoning accuracy does
not necessarily correlate with the final selection accuracy. (3) The number of
options and the choice of words, influence LLM performance.

</details>


### [273] [Chinchunmei at SemEval-2025 Task 11: Boosting the Large Language Model's Capability of Emotion Perception using Contrastive Learning](https://arxiv.org/abs/2507.15714)
*Tian Li,Yujian Sun,Huizhi Liang*

Main category: cs.CL

TL;DR: The paper discusses SemEval-2025 Task 11, focusing on emotion detection in 28 languages using contrastive learning approaches, achieving competitive results.


<details>
  <summary>Details</summary>
Motivation: To address challenges in emotion recognition due to diverse expressions and backgrounds, the task introduces advanced methods for multi-label classification and emotion intensity prediction.

Method: Two contrastive learning approaches are explored: sample-based (Contrastive Reasoning Calibration) and generation-based (DPO, SimPO), fine-tuned from LLaMa3-Instruct-8B.

Result: The system ranked 9th in Track A and 6th in Track B for English, and performed well in other languages.

Conclusion: Contrastive learning methods show promise in improving emotion detection across diverse languages and tasks.

Abstract: The SemEval-2025 Task 11, Bridging the Gap in Text-Based Emotion Detection,
introduces an emotion recognition challenge spanning over 28 languages. This
competition encourages researchers to explore more advanced approaches to
address the challenges posed by the diversity of emotional expressions and
background variations. It features two tracks: multi-label classification
(Track A) and emotion intensity prediction (Track B), covering six emotion
categories: anger, fear, joy, sadness, surprise, and disgust. In our work, we
systematically explore the benefits of two contrastive learning approaches:
sample-based (Contrastive Reasoning Calibration) and generation-based (DPO,
SimPO) contrastive learning. The sample-based contrastive approach trains the
model by comparing two samples to generate more reliable predictions. The
generation-based contrastive approach trains the model to differentiate between
correct and incorrect generations, refining its prediction. All models are
fine-tuned from LLaMa3-Instruct-8B. Our system achieves 9th place in Track A
and 6th place in Track B for English, while ranking among the top-tier
performing systems for other languages.

</details>


### [274] [From Queries to Criteria: Understanding How Astronomers Evaluate LLMs](https://arxiv.org/abs/2507.15715)
*Alina Hyk,Kiera McCormick,Mian Zhong,Ioana Ciucă,Sanjib Sharma,John F Wu,J. E. G. Peek,Kartheik G. Iyer,Ziang Xiao,Anjalie Field*

Main category: cs.CL

TL;DR: The study explores how users evaluate LLMs in astronomy, using a Slack-deployed bot. Findings from queries and interviews inform better benchmark recommendations.


<details>
  <summary>Details</summary>
Motivation: To improve LLM evaluation methods by understanding user criteria, especially for scientific research like astronomy.

Method: Inductive coding of 368 bot queries and interviews with 11 astronomers to analyze evaluation practices.

Result: Identified user evaluation criteria and question types, leading to recommendations for better benchmarks.

Conclusion: Provides actionable insights to enhance LLM evaluation and usability in scientific research.

Abstract: There is growing interest in leveraging LLMs to aid in astronomy and other
scientific research, but benchmarks for LLM evaluation in general have not kept
pace with the increasingly diverse ways that real people evaluate and use these
models. In this study, we seek to improve evaluation procedures by building an
understanding of how users evaluate LLMs. We focus on a particular use case: an
LLM-powered retrieval-augmented generation bot for engaging with astronomical
literature, which we deployed via Slack. Our inductive coding of 368 queries to
the bot over four weeks and our follow-up interviews with 11 astronomers reveal
how humans evaluated this system, including the types of questions asked and
the criteria for judging responses. We synthesize our findings into concrete
recommendations for building better benchmarks, which we then employ in
constructing a sample benchmark for evaluating LLMs for astronomy. Overall, our
work offers ways to improve LLM evaluation and ultimately usability,
particularly for use in scientific research.

</details>


### [275] [BEnchmarking LLMs for Ophthalmology (BELO) for Ophthalmological Knowledge and Reasoning](https://arxiv.org/abs/2507.15717)
*Sahana Srinivasan,Xuguang Ai,Thaddaeus Wai Soon Lo,Aidan Gilson,Minjie Zou,Ke Zou,Hyunjae Kim,Mingjia Yang,Krithi Pushpanathan,Samantha Yew,Wan Ting Loke,Jocelyn Goh,Yibing Chen,Yiming Kong,Emily Yuelei Fu,Michelle Ongyong Hui,Kristen Nwanyanwu,Amisha Dave,Kelvin Zhenghao Li,Chen-Hsin Sun,Mark Chia,Gabriel Dawei Yang,Wendy Meihua Wong,David Ziyou Chen,Dianbo Liu,Maxwell Singer,Fares Antaki,Lucian V Del Priore,Jost Jonas,Ron Adelman,Qingyu Chen,Yih-Chung Tham*

Main category: cs.CL

TL;DR: BELO is a standardized benchmark for evaluating LLMs in ophthalmology, focusing on clinical accuracy and reasoning quality. It includes 900 expert-reviewed MCQs from diverse sources and assesses six LLMs using multiple metrics.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks for LLMs in ophthalmology are limited and overly focused on accuracy, lacking comprehensive evaluation. BELO addresses this gap by providing a standardized, expert-reviewed benchmark.

Method: BELO curates ophthalmology-specific MCQs from multiple datasets, refines them through expert review, and evaluates LLMs using accuracy, macro-F1, and text-generation metrics. Human experts also qualitatively assess outputs.

Result: The benchmark includes 900 high-quality questions and evaluates six LLMs, with results reported via a public leaderboard. BELO ensures fair comparisons by remaining a hold-out evaluation-only dataset.

Conclusion: BELO provides a robust, transparent, and reproducible benchmark for evaluating LLMs in ophthalmology, promoting fair comparisons and future advancements in the field.

Abstract: Current benchmarks evaluating large language models (LLMs) in ophthalmology
are limited in scope and disproportionately prioritise accuracy. We introduce
BELO (BEnchmarking LLMs for Ophthalmology), a standardized and comprehensive
evaluation benchmark developed through multiple rounds of expert checking by 13
ophthalmologists. BELO assesses ophthalmology-related clinical accuracy and
reasoning quality. Using keyword matching and a fine-tuned PubMedBERT model, we
curated ophthalmology-specific multiple-choice-questions (MCQs) from diverse
medical datasets (BCSC, MedMCQA, MedQA, BioASQ, and PubMedQA). The dataset
underwent multiple rounds of expert checking. Duplicate and substandard
questions were systematically removed. Ten ophthalmologists refined the
explanations of each MCQ's correct answer. This was further adjudicated by
three senior ophthalmologists. To illustrate BELO's utility, we evaluated six
LLMs (OpenAI o1, o3-mini, GPT-4o, DeepSeek-R1, Llama-3-8B, and Gemini 1.5 Pro)
using accuracy, macro-F1, and five text-generation metrics (ROUGE-L, BERTScore,
BARTScore, METEOR, and AlignScore). In a further evaluation involving human
experts, two ophthalmologists qualitatively reviewed 50 randomly selected
outputs for accuracy, comprehensiveness, and completeness. BELO consists of 900
high-quality, expert-reviewed questions aggregated from five sources: BCSC
(260), BioASQ (10), MedMCQA (572), MedQA (40), and PubMedQA (18). A public
leaderboard has been established to promote transparent evaluation and
reporting. Importantly, the BELO dataset will remain a hold-out,
evaluation-only benchmark to ensure fair and reproducible comparisons of future
models.

</details>


### [276] [Understanding Large Language Models' Ability on Interdisciplinary Research](https://arxiv.org/abs/2507.15736)
*Yuanhao Shen,Daniel Xavier de Sousa,Ricardo Marçal,Ali Asad,Hongyu Guo,Xiaodan Zhu*

Main category: cs.CL

TL;DR: IDRBench is introduced as a benchmark to evaluate LLMs' ability in interdisciplinary research (IDR) idea generation, revealing their current limitations despite some awareness.


<details>
  <summary>Details</summary>
Motivation: The lack of a dedicated benchmark for assessing LLMs' interdisciplinary research capabilities hinders understanding their potential and limitations in scientific discovery.

Method: IDRBench includes expert-annotated datasets from six ArXiv disciplines and tasks like IDR Paper Identification, Idea Integration, and Idea Recommendation.

Result: Baselines across 10 LLMs show they struggle to produce quality IDR ideas, despite some awareness.

Conclusion: IDRBench provides a framework for assessing LLMs in IDR and highlights the need for improved models in interdisciplinary research.

Abstract: Recent advancements in Large Language Models (LLMs) have revealed their
impressive ability to perform multi-step, logic-driven reasoning across complex
domains, positioning them as powerful tools and collaborators in scientific
discovery while challenging the long-held view that inspiration-driven ideation
is uniquely human. However, the lack of a dedicated benchmark that evaluates
LLMs' ability to develop ideas in Interdisciplinary Research (IDR) settings
poses a critical barrier to fully understanding their strengths and
limitations. To address this gap, we introduce IDRBench -- a pioneering
benchmark featuring an expert annotated dataset and a suite of tasks tailored
to evaluate LLMs' capabilities in proposing valuable research ideas from
different scientific domains for interdisciplinary research. This benchmark
aims to provide a systematic framework for assessing LLM performance in
complex, cross-domain scientific research. Our dataset consists of scientific
publications sourced from the ArXiv platform covering six distinct disciplines,
and is annotated by domain experts with diverse academic backgrounds. To ensure
high-quality annotations, we emphasize clearly defined dimensions that
characterize authentic interdisciplinary research. The design of evaluation
tasks in IDRBench follows a progressive, real-world perspective, reflecting the
natural stages of interdisciplinary research development, including 1) IDR
Paper Identification, 2) IDR Idea Integration, and 3) IDR Idea Recommendation.
Using IDRBench, we construct baselines across 10 LLMs and observe that despite
fostering some level of IDR awareness, LLMs still struggle to produce quality
IDR ideas. These findings could not only spark new research directions, but
also help to develop next-generation LLMs that excel in interdisciplinary
research.

</details>


### [277] [A Fisher's exact test justification of the TF-IDF term-weighting scheme](https://arxiv.org/abs/2507.15742)
*Paul Sheridan,Zeyad Ahmed,Aitazaz A. Farooque*

Main category: cs.CL

TL;DR: The paper justifies TF-IDF from a significance testing perspective, linking TF-ICF to Fisher's exact test p-values and showing convergence to TF-IDF in large document collections.


<details>
  <summary>Details</summary>
Motivation: To provide a theoretical foundation for TF-IDF's effectiveness by connecting it to statistical significance testing, making it more accessible to statisticians.

Method: Demonstrates the relationship between TF-ICF (a TF-IDF variant) and the negative logarithm of Fisher's exact test p-value under mild conditions. Also explores idealized assumptions and limiting cases.

Result: TF-ICF is closely related to Fisher's exact test p-values, and TF-IDF converges to this quantity in large document collections.

Conclusion: The paper successfully links TF-IDF to statistical significance, offering a theoretical explanation for its effectiveness in term-weighting.

Abstract: Term frequency-inverse document frequency, or TF-IDF for short, is arguably
the most celebrated mathematical expression in the history of information
retrieval. Conceived as a simple heuristic quantifying the extent to which a
given term's occurrences are concentrated in any one given document out of
many, TF-IDF and its many variants are routinely used as term-weighting schemes
in diverse text analysis applications. There is a growing body of scholarship
dedicated to placing TF-IDF on a sound theoretical foundation. Building on that
tradition, this paper justifies the use of TF-IDF to the statistics community
by demonstrating how the famed expression can be understood from a significance
testing perspective. We show that the common TF-IDF variant TF-ICF is, under
mild regularity conditions, closely related to the negative logarithm of the
$p$-value from a one-tailed version of Fisher's exact test of statistical
significance. As a corollary, we establish a connection between TF-IDF and the
said negative log-transformed $p$-value under certain idealized assumptions. We
further demonstrate, as a limiting case, that this same quantity converges to
TF-IDF in the limit of an infinitely large document collection. The Fisher's
exact test justification of TF-IDF equips the working statistician with a ready
explanation of the term-weighting scheme's long-established effectiveness.

</details>


### [278] [DialogueForge: LLM Simulation of Human-Chatbot Dialogue](https://arxiv.org/abs/2507.15752)
*Ruizhe Zhu,Hao Zhu,Yaxuan Li,Syang Zhou,Shijing Cai,Malgorzata Lazuka,Elliott Ash*

Main category: cs.CL

TL;DR: DialogueForge is a framework for generating AI-simulated human-chatbot dialogues using seed prompts from real interactions, tested with various LLMs and fine-tuning techniques.


<details>
  <summary>Details</summary>
Motivation: Manual collection of human-chatbot dialogues is resource-intensive, limiting conversational AI research. DialogueForge aims to automate this process.

Method: Uses seed prompts from real interactions, tests various LLMs (proprietary and open-source), and applies fine-tuning for smaller models. Evaluates with UniEval and GTEval.

Result: Large proprietary models (e.g., GPT-4o) perform best, while smaller models (e.g., Llama, Mistral) show promise with customization and fine-tuning.

Conclusion: DialogueForge effectively generates dialogues, with fine-tuning improving smaller models, but maintaining coherence in long-form dialogues remains a challenge.

Abstract: Collecting human-chatbot dialogues typically demands substantial manual
effort and is time-consuming, which limits and poses challenges for research on
conversational AI. In this work, we propose DialogueForge - a framework for
generating AI-simulated conversations in human-chatbot style. To initialize
each generated conversation, DialogueForge uses seed prompts extracted from
real human-chatbot interactions. We test a variety of LLMs to simulate the
human chatbot user, ranging from state-of-the-art proprietary models to
small-scale open-source LLMs, and generate multi-turn dialogues tailored to
specific tasks. In addition, we explore fine-tuning techniques to enhance the
ability of smaller models to produce indistinguishable human-like dialogues. We
evaluate the quality of the simulated conversations and compare different
models using the UniEval and GTEval evaluation protocols. Our experiments show
that large proprietary models (e.g., GPT-4o) generally outperform others in
generating more realistic dialogues, while smaller open-source models (e.g.,
Llama, Mistral) offer promising performance with greater customization. We
demonstrate that the performance of smaller models can be significantly
improved by employing supervised fine-tuning techniques. Nevertheless,
maintaining coherent and natural long-form human-like dialogues remains a
common challenge across all models.

</details>


### [279] [Interaction as Intelligence: Deep Research With Human-AI Partnership](https://arxiv.org/abs/2507.15759)
*Lyumanshan Ye,Xiaojie Cai,Xinkai Wang,Junfei Wang,Xiangkun Hu,Jiadi Su,Yang Nan,Sihan Wang,Bohan Zhang,Xiaoze Fan,Jinbin Luo,Yuxiang Zheng,Tianze Xu,Dayuan Fu,Yunze Wu,Pengrui Lu,Zengzhi Wang,Yiwei Qin,Zhen Huang,Yan Ma,Zhulin Hu,Haoyang Zou,Tiantian Mi,Yixin Ye,Ethan Chern,Pengfei Liu*

Main category: cs.CL

TL;DR: The paper redefines human-AI interaction as a core aspect of intelligence, introducing 'Deep Cognition' to replace traditional input-output models with transparent, interruptible, and collaborative engagement.


<details>
  <summary>Details</summary>
Motivation: Current AI systems in deep research tasks suffer from rigid interaction models, leading to errors and inefficiencies. The paper aims to enhance intelligence by integrating meaningful human-AI interaction.

Method: Introduces 'Deep Cognition,' featuring transparent reasoning, fine-grained dialogue, and shared cognitive context, enabling human oversight and intervention.

Result: Outperforms baselines in transparency (+20.0%), interaction (+29.2%), and other metrics, with 31.8%-50.0% improvements in research tasks.

Conclusion: Interaction is fundamental to AI intelligence; Deep Cognition proves its effectiveness in enhancing research outcomes through collaborative oversight.

Abstract: This paper introduces "Interaction as Intelligence" research series,
presenting a reconceptualization of human-AI relationships in deep research
tasks. Traditional approaches treat interaction merely as an interface for
accessing AI capabilities-a conduit between human intent and machine output. We
propose that interaction itself constitutes a fundamental dimension of
intelligence. As AI systems engage in extended thinking processes for research
tasks, meaningful interaction transitions from an optional enhancement to an
essential component of effective intelligence. Current deep research systems
adopt an "input-wait-output" paradigm where users initiate queries and receive
results after black-box processing. This approach leads to error cascade
effects, inflexible research boundaries that prevent question refinement during
investigation, and missed opportunities for expertise integration. To address
these limitations, we introduce Deep Cognition, a system that transforms the
human role from giving instructions to cognitive oversight-a mode of engagement
where humans guide AI thinking processes through strategic intervention at
critical junctures. Deep cognition implements three key innovations:
(1)Transparent, controllable, and interruptible interaction that reveals AI
reasoning and enables intervention at any point; (2)Fine-grained bidirectional
dialogue; and (3)Shared cognitive context where the system observes and adapts
to user behaviors without explicit instruction. User evaluation demonstrates
that this cognitive oversight paradigm outperforms the strongest baseline
across six key metrics: Transparency(+20.0%), Fine-Grained Interaction(+29.2%),
Real-Time Intervention(+18.5%), Ease of Collaboration(+27.7%),
Results-Worth-Effort(+8.8%), and Interruptibility(+20.7%). Evaluations on
challenging research problems show 31.8% to 50.0% points of improvements over
deep research systems.

</details>


### [280] [Supernova: Achieving More with Less in Transformer Architectures](https://arxiv.org/abs/2507.15773)
*Andrei-Valentin Tanase,Elena Pelican*

Main category: cs.CL

TL;DR: Supernova, a 650M-parameter transformer, achieves 90% performance of 1B-parameter models with fewer parameters and tokens, using innovative design and tokenization.


<details>
  <summary>Details</summary>
Motivation: To challenge the scaling paradigm by proving architectural efficiency and tokenization can compensate for fewer parameters.

Method: Combines RoPE, GQA (3:1 compression), RMSNorm, SwiGLU, and a custom 128K-vocabulary byte-level BPE tokenizer.

Result: Achieves 90% performance of 1B-parameter models with 53% fewer parameters and 100B training tokens.

Conclusion: Architectural efficiency and tokenization quality can replace the need for larger models.

Abstract: We present Supernova, a 650M-parameter decoder-only transformer that
demonstrates how careful architectural design and tokenization innovation can
achieve the performance of larger models while maintaining computational
efficiency. Our architecture combines Rotary Positional Embeddings (RoPE),
Grouped Query Attention (GQA) with a 3:1 compression ratio, RMSNorm for
computational efficiency, and SwiGLU activation functions. A critical
innovation is our custom 128,000-vocabulary byte-level BPE tokenizer, which
achieves state-of-the-art compression performance. Through detailed analysis,
we show that Supernova achieves 90% of the performance of 1B-parameter models
while using 53% fewer parameters and requiring only 100B training tokens--an
order of magnitude less than competing models. Our findings challenge the
prevailing scaling paradigm, demonstrating that architectural efficiency and
tokenization quality can compensate for reduced parameter counts.

</details>


### [281] [Stabilizing Knowledge, Promoting Reasoning: Dual-Token Constraints for RLVR](https://arxiv.org/abs/2507.15778)
*Jiakang Wang,Runze Liu,Fuzheng Zhang,Xiu Li,Guorui Zhou*

Main category: cs.CL

TL;DR: Archer introduces an entropy-aware RLVR method with dual-token constraints and synchronous updates, outperforming previous methods in reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: Previous RLVR methods treat all tokens uniformly, ignoring the distinct roles of knowledge and reasoning tokens, which can hinder learning.

Method: Archer applies weaker KL regularization and higher clipping thresholds to reasoning tokens for exploration, while enforcing stronger constraints on knowledge tokens to retain factual accuracy.

Result: Archer achieves state-of-the-art performance on mathematical reasoning and code generation benchmarks.

Conclusion: The proposed method effectively balances exploration and factual retention, improving reasoning abilities in LLMs.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has become an effective
post-training method for improving the reasoning abilities of Large Language
Models (LLMs), mainly by shaping higher-order behaviors such as reflection and
planning. However, previous RLVR algorithms often apply uniform training
signals to all tokens, without considering the different roles of low-entropy
knowledge-related tokens and high-entropy reasoning-related tokens. Some recent
methods try to separate these token types by gradient masking or asynchronous
updates, but these approaches may break semantic dependencies in the model
output and hinder effective learning. In this work, we propose Archer, an
entropy-aware RLVR approach with dual-token constraints and synchronous
updates. Specifically, our method applies weaker KL regularization and higher
clipping thresholds to reasoning tokens to encourage exploration, while using
stronger constraints on knowledge tokens to maintain factual knowledge.
Experimental results on several mathematical reasoning and code generation
benchmarks show that our approach significantly outperforms previous RLVR
methods, reaching or exceeding state-of-the-art performance among models of
comparable size. The code is available at
https://github.com/wizard-III/ArcherCodeR.

</details>


### [282] [Reservoir Computing as a Language Model](https://arxiv.org/abs/2507.15779)
*Felix Köster,Atsushi Uchida*

Main category: cs.CL

TL;DR: The paper compares reservoir computing and transformer-based architectures for language modeling, highlighting transformers' superior prediction quality and reservoir computing's efficiency in speed and energy.


<details>
  <summary>Details</summary>
Motivation: Address the bottleneck of LLMs' high energy demand and slow processing by exploring reservoir computing as a fast, energy-efficient alternative for natural text processing.

Method: Compare three approaches: two reservoir computing methods (static linear readout and attention-enhanced) and transformer-based architectures, varying trainable parameters equally.

Result: Transformers outperform in prediction quality, while reservoir computing excels in training and inference speed. Attention-enhanced reservoirs show dynamic adaptability.

Conclusion: The study provides guidelines for balancing performance and resource constraints, suggesting reservoir computing as a viable, efficient alternative to transformers.

Abstract: Large Language Models (LLM) have dominated the science and media landscape
duo to their impressive performance on processing large chunks of data and
produce human-like levels of text. Nevertheless, their huge energy demand and
slow processing still a bottleneck for further increasing quality while also
making the models accessible to everyone. To solve this bottleneck, we will
investigate how reservoir computing performs on natural text processing, which
could enable fast and energy efficient hardware implementations. Studies
investigating the use of reservoir computing as a language model remain sparse.
In this paper, we compare three distinct approaches for character-level
language modeling, two different reservoir computing approaches, where only an
output layer is trainable, and the well-known transformer-based architectures,
which fully learn an attention-based sequence representation. We explore the
performance, computational cost and prediction accuracy for both paradigms by
equally varying the number of trainable parameters for all models. Using a
consistent pipeline for all three approaches, we demonstrate that transformers
excel in prediction quality, whereas reservoir computers remain highly
efficient reducing the training and inference speed. Furthermore, we
investigate two types of reservoir computing: a traditional reservoir with a
static linear readout, and an attention-enhanced reservoir that dynamically
adapts its output weights via an attention mechanism. Our findings underline
how these paradigms scale and offer guidelines to balance resource constraints
with performance.

</details>


### [283] [Operationalizing AI for Good: Spotlight on Deployment and Integration of AI Models in Humanitarian Work](https://arxiv.org/abs/2507.15823)
*Anton Abilov,Ke Zhang,Hemank Lamba,Elizabeth M. Olson,Joel R. Tetreault,Alejandro Jaimes*

Main category: cs.CL

TL;DR: The paper highlights the gap in AI for Good literature regarding deployment and collaboration with partner organizations, sharing insights from a real-world humanitarian project.


<details>
  <summary>Details</summary>
Motivation: To address the lack of discussion on deploying AI models in resource-constrained settings and maintaining them for real-world impact.

Method: Close collaboration with a humanitarian organization, focusing on deployment, maintenance, and continuous performance updates.

Result: Practical insights and key takeaways for deploying and maintaining AI models in humanitarian contexts.

Conclusion: The paper provides valuable lessons for practitioners aiming to implement AI solutions in real-world, resource-limited scenarios.

Abstract: Publications in the AI for Good space have tended to focus on the research
and model development that can support high-impact applications. However, very
few AI for Good papers discuss the process of deploying and collaborating with
the partner organization, and the resulting real-world impact. In this work, we
share details about the close collaboration with a humanitarian-to-humanitarian
(H2H) organization and how to not only deploy the AI model in a
resource-constrained environment, but also how to maintain it for continuous
performance updates, and share key takeaways for practitioners.

</details>


### [284] [The Impact of Language Mixing on Bilingual LLM Reasoning](https://arxiv.org/abs/2507.15849)
*Yihao Li,Jiayi Xin,Miranda Muqing Miao,Qi Long,Lyle Ungar*

Main category: cs.CL

TL;DR: Language mixing in bilingual LLMs enhances reasoning, with RLVR training enabling it. Enforcing monolingual decoding reduces accuracy, while guided switching improves it.


<details>
  <summary>Details</summary>
Motivation: To understand why bilingual LLMs mix languages and whether this behavior benefits reasoning.

Method: Study language switching in Chinese-English bilingual models, identify RLVR as the key training stage, and test monolingual vs. mixed decoding.

Result: Language mixing improves reasoning (5.6% accuracy drop with monolingual decoding). A probe predicting beneficial switches boosts accuracy by 6.25%.

Conclusion: Language mixing is strategic, not accidental, and enhances reasoning in bilingual LLMs.

Abstract: Proficient multilingual speakers often intentionally switch languages in the
middle of a conversation. Similarly, recent reasoning-focused bilingual large
language models (LLMs) with strong capabilities in both languages exhibit
language mixing--alternating languages within their chain of thought.
Discouraging this behavior in DeepSeek-R1 was found to degrade accuracy,
suggesting that language mixing may benefit reasoning. In this work, we study
language switching in Chinese-English bilingual reasoning models. We identify
reinforcement learning with verifiable rewards (RLVR) as the critical training
stage that leads to language mixing. We demonstrate that language mixing can
enhance reasoning: enforcing monolingual decoding reduces accuracy by 5.6
percentage points on math reasoning tasks. Additionally, a lightweight probe
can be trained to predict whether a potential language switch would benefit or
harm reasoning, and when used to guide decoding, increases accuracy by up to
6.25 percentage points. Our findings suggest that language mixing is not merely
a byproduct of multilingual training, but is a strategic reasoning behavior.

</details>


### [285] [3LM: Bridging Arabic, STEM, and Code through Benchmarking](https://arxiv.org/abs/2507.15850)
*Basma El Amel Boussaha,Leen AlQadi,Mugariya Farooq,Shaikha Alsuwaidi,Giulia Campesan,Ahmed Alzubaidi,Mohammed Alyafeai,Hakim Hacid*

Main category: cs.CL

TL;DR: The paper introduces 3LM, a suite of three benchmarks for Arabic LLMs, focusing on STEM and code generation to address gaps in existing Arabic benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing Arabic benchmarks lack coverage in STEM and code domains, which are crucial for real-world LLM applications.

Method: Developed three benchmarks: (1) STEM Q&A from Arabic textbooks, (2) synthetic STEM questions, and (3) translated code benchmarks with human review.

Result: Publicly released benchmarks to support Arabic LLM research in underrepresented areas.

Conclusion: 3LM fills a critical gap in Arabic LLM evaluation, promoting broader and more practical applications.

Abstract: Arabic is one of the most widely spoken languages in the world, yet efforts
to develop and evaluate Large Language Models (LLMs) for Arabic remain
relatively limited. Most existing Arabic benchmarks focus on linguistic,
cultural, or religious content, leaving a significant gap in domains like STEM
and code which are increasingly relevant for real-world LLM applications. To
help bridge this gap, we present 3LM, a suite of three benchmarks designed
specifically for Arabic. The first is a set of STEM-related question-answer
pairs, naturally sourced from Arabic textbooks and educational worksheets. The
second consists of synthetically generated STEM questions, created using the
same sources. The third benchmark focuses on code generation, built through a
careful translation of two widely used code benchmarks, incorporating a
human-in-the-loop process with several rounds of review to ensure high-quality
and faithful translations. We release all three benchmarks publicly to support
the growth of Arabic LLM research in these essential but underrepresented
areas.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [286] [Catalyst: a Novel Regularizer for Structured Pruning with Auxiliary Extension of Parameter Space](https://arxiv.org/abs/2507.14170)
*Jaeheun Jung,Donghun Lee*

Main category: cs.LG

TL;DR: The paper introduces Catalyst regularization, a novel method for structured pruning of deep neural networks, addressing biases and instability in traditional pruning techniques.


<details>
  <summary>Details</summary>
Motivation: Traditional pruning methods like L1 or Group Lasso are biased towards small-magnitude filters and lack robustness due to narrow decision margins.

Method: The authors identify an algebraic condition for performance-preserving pruning and propose Catalyst regularization, using auxiliary variables to ensure fair and robust pruning.

Result: Catalyst Pruning outperforms state-of-the-art methods, demonstrating fair, robust, and effective pruning across various datasets and models.

Conclusion: Catalyst regularization provides a theoretically grounded and empirically validated solution for structured pruning, addressing key limitations of existing methods.

Abstract: Structured pruning aims to reduce the size and computational cost of deep
neural networks by removing entire filters or channels. The traditional
regularizers such as L1 or Group Lasso and its variants lead to
magnitude-biased pruning decisions, such that the filters with small magnitudes
are likely to be pruned. Also, they often entail pruning results with almost
zero margin around pruning decision boundary, such that tiny perturbation in a
filter magnitude can flip the pruning decision. In this paper, we identify the
precise algebraic condition under which pruning operations preserve model
performance, and use the condition to construct a novel regularizer defined in
an extended parameter space via auxiliary catalyst variables. The proposed
Catalyst regularization ensures fair pruning chance for each filters with
theoretically provable zero bias to their magnitude and robust pruning behavior
achieved by wide-margin bifurcation of magnitudes between the preserved and the
pruned filters. The theoretical properties naturally lead to real-world
effectiveness, as shown by empirical validations of Catalyst Pruning algorithm.
Pruning results on various datasets and models are superior to state-of-the-art
filter pruning methods, and at the same time confirm the predicted robust and
fair pruning characteristics of Catalyst pruning.

</details>


### [287] [IPPRO: Importance-based Pruning with PRojective Offset for Magnitude-indifferent Structural Pruning](https://arxiv.org/abs/2507.14171)
*Jaeheun Jung,Jaehyuk Lee,Yeajin Lee,Donghun Lee*

Main category: cs.LG

TL;DR: A novel pruning strategy (IPPRO) challenges the dominance of magnitude in filter pruning by using projective space and gradient descent movement to measure pruning likelihood, achieving near-lossless results.


<details>
  <summary>Details</summary>
Motivation: Address limitations of magnitude-based pruning, where larger filters dominate, by providing a fair chance for all filters to be pruned.

Method: Proposes PROscore, a magnitude-indifferent importance score, by observing gradient descent movement in projective space.

Result: Achieves near-lossless pruning with minimal performance drop and promising post-finetuning results.

Conclusion: Debunks the 'size-matters' myth in pruning, advancing importance-based pruning theoretically and empirically.

Abstract: With the growth of demand on neural network compression methods, the
structured pruning methods including importance-based approach are actively
studied. The magnitude importance and many correlated modern importance
criteria often limit the capacity of pruning decision, since the filters with
larger magnitudes are not likely to be pruned if the smaller one didn't, even
if it is redundant. In this paper, we propose a novel pruning strategy to
challenge this dominating effect of magnitude and provide fair chance to each
filter to be pruned, by placing it on projective space. After that, we observe
the gradient descent movement whether the filters move toward the origin or
not, to measure how the filter is likely to be pruned. This measurement is used
to construct PROscore, a novel importance score for IPPRO, a novel
importance-based structured pruning with magnitude-indifference. Our evaluation
results shows that the proposed importance criteria using the projective space
achieves near-lossless pruning by reducing the performance drop in pruning,
with promising performance after the finetuning. Our work debunks the
``size-matters'' myth in pruning and expands the frontier of importance-based
pruning both theoretically and empirically.

</details>


### [288] [Self-Improving Language Models for Evolutionary Program Synthesis: A Case Study on ARC-AGI](https://arxiv.org/abs/2507.14172)
*Julien Pourcel,Cédric Colas,Pierre-Yves Oudeyer*

Main category: cs.LG

TL;DR: SOAR integrates language models into a self-improving evolutionary loop for program synthesis, achieving significant gains on the ARC-AGI benchmark.


<details>
  <summary>Details</summary>
Motivation: State-of-the-art language models struggle with complex program synthesis tasks, and evolutionary methods are limited by fixed generative capabilities.

Method: SOAR alternates between evolutionary search using an LLM and hindsight learning to fine-tune the LLM iteratively.

Result: SOAR achieves 52% success on the ARC-AGI public test set, showing performance gains across model scales.

Conclusion: SOAR demonstrates the potential of combining evolutionary search with self-improving language models for program synthesis.

Abstract: Many program synthesis tasks prove too challenging for even state-of-the-art
language models to solve in single attempts. Search-based evolutionary methods
offer a promising alternative by exploring solution spaces iteratively, but
their effectiveness remain limited by the fixed capabilities of the underlying
generative model.
  We propose SOAR, a method that learns program synthesis by integrating
language models into a self-improving evolutionary loop.
  SOAR alternates between (1) an evolutionary search that uses an LLM to sample
and refine candidate solutions, and (2) a hindsight learning phase that
converts search attempts into valid problem-solution pairs used to fine-tune
the LLM's sampling and refinement capabilities\, -- \,enabling increasingly
effective search in subsequent iterations.
  On the challenging ARC-AGI benchmark, SOAR achieves significant performance
gains across model scales and iterations, leveraging positive transfer between
the sampling and refinement finetuning tasks. These improvements carry over to
test-time adaptation, enabling SOAR to solve 52\% of the public test set. Our
code is open-sourced at: https://github.com/flowersteam/SOAR

</details>


### [289] [Latent Space Data Fusion Outperforms Early Fusion in Multimodal Mental Health Digital Phenotyping Data](https://arxiv.org/abs/2507.14175)
*Youcef Barkat,Dylan Hamitouche,Deven Parekh,Ivy Guo,David Benrimoh*

Main category: cs.LG

TL;DR: Intermediate (latent space) fusion outperforms early fusion in predicting depressive symptoms, showing better accuracy and generalization with multimodal psychiatric data.


<details>
  <summary>Details</summary>
Motivation: Improving early detection and personalized intervention for mental illnesses like depression and anxiety by addressing limitations of traditional unimodal or early fusion methods.

Method: Compared early fusion (Random Forest) and intermediate fusion (Combined Model with autoencoders and neural network) using behavioral, demographic, and clinical data from the BRIGHTEN trial. Evaluated with MSE and R2.

Result: Combined Model achieved lower MSE (0.4985 vs. 0.5305) and higher R2 (0.4695 vs. 0.4356) than Random Forest, with better generalization and performance when integrating all data modalities.

Conclusion: Latent space fusion is superior for multimodal mental health data prediction; future work should focus on interpretability and clinical deployment.

Abstract: Background: Mental illnesses such as depression and anxiety require improved
methods for early detection and personalized intervention. Traditional
predictive models often rely on unimodal data or early fusion strategies that
fail to capture the complex, multimodal nature of psychiatric data. Advanced
integration techniques, such as intermediate (latent space) fusion, may offer
better accuracy and clinical utility. Methods: Using data from the BRIGHTEN
clinical trial, we evaluated intermediate (latent space) fusion for predicting
daily depressive symptoms (PHQ-2 scores). We compared early fusion implemented
with a Random Forest (RF) model and intermediate fusion implemented via a
Combined Model (CM) using autoencoders and a neural network. The dataset
included behavioral (smartphone-based), demographic, and clinical features.
Experiments were conducted across multiple temporal splits and data stream
combinations. Performance was evaluated using mean squared error (MSE) and
coefficient of determination (R2). Results: The CM outperformed both RF and
Linear Regression (LR) baselines across all setups, achieving lower MSE (0.4985
vs. 0.5305 with RF) and higher R2 (0.4695 vs. 0.4356). The RF model showed
signs of overfitting, with a large gap between training and test performance,
while the CM maintained consistent generalization. Performance was best when
integrating all data modalities in the CM (in contradistinction to RF),
underscoring the value of latent space fusion for capturing non-linear
interactions in complex psychiatric datasets. Conclusion: Latent space fusion
offers a robust alternative to traditional fusion methods for prediction with
multimodal mental health data. Future work should explore model
interpretability and individual-level prediction for clinical deployment.

</details>


### [290] [Predictive Representativity: Uncovering Racial Bias in AI-based Skin Cancer Detection](https://arxiv.org/abs/2507.14176)
*Andrés Morales-Forero,Lili J. Rueda,Ronald Herrera,Samuel Bassetto,Eric Coatanea*

Main category: cs.LG

TL;DR: The paper introduces Predictive Representativity (PR), a fairness auditing framework focusing on outcome-level equity in AI systems, revealing performance disparities in skin cancer classifiers for darker skin tones despite proportional data sampling.


<details>
  <summary>Details</summary>
Motivation: Address concerns about algorithmic bias and inequitable outcomes in AI-driven medical decision-making, especially for marginalized populations.

Method: Evaluated AI-based skin cancer classifiers on the HAM10000 dataset and an independent Colombian dataset (BOSQUE Test set), analyzing performance disparities by skin phototype.

Result: Classifiers underperformed for darker skin tones despite proportional data sampling, highlighting the need for dynamic fairness evaluation.

Conclusion: Advocates for post-hoc fairness auditing, transparency, and inclusive validation, proposing PR and an External Transportability Criterion to address structural inequities in AI systems.

Abstract: Artificial intelligence (AI) systems increasingly inform medical
decision-making, yet concerns about algorithmic bias and inequitable outcomes
persist, particularly for historically marginalized populations. This paper
introduces the concept of Predictive Representativity (PR), a framework of
fairness auditing that shifts the focus from the composition of the data set to
outcomes-level equity. Through a case study in dermatology, we evaluated
AI-based skin cancer classifiers trained on the widely used HAM10000 dataset
and on an independent clinical dataset (BOSQUE Test set) from Colombia. Our
analysis reveals substantial performance disparities by skin phototype, with
classifiers consistently underperforming for individuals with darker skin,
despite proportional sampling in the source data. We argue that
representativity must be understood not as a static feature of datasets but as
a dynamic, context-sensitive property of model predictions. PR operationalizes
this shift by quantifying how reliably models generalize fairness across
subpopulations and deployment contexts. We further propose an External
Transportability Criterion that formalizes the thresholds for fairness
generalization. Our findings highlight the ethical imperative for post-hoc
fairness auditing, transparency in dataset documentation, and inclusive model
validation pipelines. This work offers a scalable tool for diagnosing
structural inequities in AI systems, contributing to discussions on equity,
interpretability, and data justice and fostering a critical re-evaluation of
fairness in data-driven healthcare.

</details>


### [291] [Understanding Two-Layer Neural Networks with Smooth Activation Functions](https://arxiv.org/abs/2507.14177)
*Changcun Huang*

Main category: cs.LG

TL;DR: The paper analyzes the training solutions of two-layer neural networks with smooth activation functions, revealing the solution space's structure and proving universal approximation.


<details>
  <summary>Details</summary>
Motivation: To demystify the 'black box' of solution spaces in two-layer neural networks with smooth activation functions and enrich approximation theory.

Method: Uses Taylor series expansions, strict partial order of knots, smooth-spline implementation, and smooth-continuity restriction.

Result: Proves universal approximation for arbitrary input dimensionality and provides experimental verification.

Conclusion: The study clarifies the solution space of neural networks and contributes to approximation theory.

Abstract: This paper aims to understand the training solution, which is obtained by the
back-propagation algorithm, of two-layer neural networks whose hidden layer is
composed of the units with smooth activation functions, including the usual
sigmoid type most commonly used before the advent of ReLUs. The mechanism
contains four main principles: construction of Taylor series expansions, strict
partial order of knots, smooth-spline implementation and smooth-continuity
restriction. The universal approximation for arbitrary input dimensionality is
proved and experimental verification is given, through which the mystery of
``black box'' of the solution space is largely revealed. The new proofs
employed also enrich approximation theory.

</details>


### [292] [Feature Bank Enhancement for Distance-based Out-of-Distribution Detection](https://arxiv.org/abs/2507.14178)
*Yuhang Liu,Yuefei Wu,Bin Shi,Bo Dong*

Main category: cs.LG

TL;DR: The paper proposes Feature Bank Enhancement (FBE) to improve OOD detection by addressing biased feature distributions in distance-based methods.


<details>
  <summary>Details</summary>
Motivation: Distance-based OOD detection methods struggle with extreme features, leading to low scores for ID samples and reduced detection performance.

Method: FBE uses dataset statistics to identify and constrain extreme features, enhancing separation boundaries between ID and OOD samples.

Result: Experiments on ImageNet-1k and CIFAR-10 show FBE achieves state-of-the-art performance.

Conclusion: FBE effectively addresses feature bias, improving OOD detection, with theoretical and experimental validation.

Abstract: Out-of-distribution (OOD) detection is critical to ensuring the reliability
of deep learning applications and has attracted significant attention in recent
years. A rich body of literature has emerged to develop efficient score
functions that assign high scores to in-distribution (ID) samples and low
scores to OOD samples, thereby helping distinguish OOD samples. Among these
methods, distance-based score functions are widely used because of their
efficiency and ease of use. However, deep learning often leads to a biased
distribution of data features, and extreme features are inevitable. These
extreme features make the distance-based methods tend to assign too low scores
to ID samples. This limits the OOD detection capabilities of such methods. To
address this issue, we propose a simple yet effective method, Feature Bank
Enhancement (FBE), that uses statistical characteristics from dataset to
identify and constrain extreme features to the separation boundaries, therapy
making the distance between samples inside and outside the distribution
farther. We conducted experiments on large-scale ImageNet-1k and CIFAR-10
respectively, and the results show that our method achieves state-of-the-art
performance on both benchmark. Additionally, theoretical analysis and
supplementary experiments are conducted to provide more insights into our
method.

</details>


### [293] [A Sparsity Predicting Approach for Large Language Models via Activation Pattern Clustering](https://arxiv.org/abs/2507.14179)
*Nobel Dhar,Bobin Deng,Md Romyull Islam,Xinyue Zhang,Kazi Fahim Ahmad Nasif,Kun Suo*

Main category: cs.LG

TL;DR: A clustering-based framework is proposed to efficiently predict and utilize activation sparsity in LLMs, reducing computational costs while preserving model quality.


<details>
  <summary>Details</summary>
Motivation: Activation sparsity in LLMs offers computational savings, but predicting neuron-level activation patterns is impractical due to the vast number of neurons.

Method: The paper introduces a clustering-based approach to group similar activation patterns into representative clusters, enabling scalable prediction.

Result: Achieves 79.34% clustering precision and maintains low perplexity (PPL) scores (e.g., 12.49), demonstrating effectiveness.

Conclusion: The method provides a foundation for efficient activation pattern prediction, improving sparse computation in LLMs.

Abstract: Large Language Models (LLMs) exhibit significant activation sparsity, where
only a subset of neurons are active for a given input. Although this sparsity
presents opportunities to reduce computational cost, efficiently utilizing it
requires predicting activation patterns in a scalable manner. However, direct
prediction at the neuron level is computationally expensive due to the vast
number of neurons in modern LLMs. To enable efficient prediction and
utilization of activation sparsity, we propose a clustering-based activation
pattern compression framework. Instead of treating each neuron independently,
we group similar activation patterns into a small set of representative
clusters. Our method achieves up to 79.34% clustering precision, outperforming
standard binary clustering approaches while maintaining minimal degradation in
perplexity (PPL) scores. With a sufficiently large number of clusters, our
approach attains a PPL score as low as 12.49, demonstrating its effectiveness
in preserving model quality while reducing computational overhead. By
predicting cluster assignments rather than individual neuron states, future
models can efficiently infer activation patterns from pre-computed centroids.
We detail the clustering algorithm, analyze its effectiveness in capturing
meaningful activation structures, and demonstrate its potential to improve
sparse computation efficiency. This clustering-based formulation serves as a
foundation for future work on activation pattern prediction, paving the way for
efficient inference in large-scale language models.

</details>


### [294] [Digital Twin-Assisted Explainable AI for Robust Beam Prediction in mmWave MIMO Systems](https://arxiv.org/abs/2507.14180)
*Nasir Khan,Asmaa Abdallah,Abdulkadir Celik,Ahmed M. Eltawil,Sinem Coleri*

Main category: cs.LG

TL;DR: The paper proposes a robust and explainable DL-based beam alignment engine (BAE) for mmWave MIMO systems, using synthetic data from a digital twin and transfer learning to reduce real-world data needs. It employs SHAP for explainability and DkNN for robustness, achieving significant efficiency gains.


<details>
  <summary>Details</summary>
Motivation: Address challenges in DL-based beam alignment, such as high data collection overhead, lack of explainability, and susceptibility to adversarial attacks, to ensure trust and reliability in mmWave systems.

Method: Uses RSSI measurements from wide beams, synthetic data from a digital twin, transfer learning for model refinement, SHAP for feature importance ranking, and DkNN for outlier detection.

Result: Reduces real-world data needs by 70%, beam training overhead by 62%, and improves outlier detection robustness by 8.5x, achieving near-optimal spectral efficiency.

Conclusion: The proposed framework enhances efficiency, transparency, and robustness in mmWave beam alignment, making it practical for real-world deployment.

Abstract: In line with the AI-native 6G vision, explainability and robustness are
crucial for building trust and ensuring reliable performance in millimeter-wave
(mmWave) systems. Efficient beam alignment is essential for initial access, but
deep learning (DL) solutions face challenges, including high data collection
overhead, hardware constraints, lack of explainability, and susceptibility to
adversarial attacks. This paper proposes a robust and explainable DL-based beam
alignment engine (BAE) for mmWave multiple-input multiple output (MIMO)
systems. The BAE uses received signal strength indicator (RSSI) measurements
from wide beams to predict the best narrow beam, reducing the overhead of
exhaustive beam sweeping. To overcome the challenge of real-world data
collection, this work leverages a site-specific digital twin (DT) to generate
synthetic channel data closely resembling real-world environments. A model
refinement via transfer learning is proposed to fine-tune the pre-trained model
residing in the DT with minimal real-world data, effectively bridging
mismatches between the digital replica and real-world environments. To reduce
beam training overhead and enhance transparency, the framework uses deep
Shapley additive explanations (SHAP) to rank input features by importance,
prioritizing key spatial directions and minimizing beam sweeping. It also
incorporates the Deep k-nearest neighbors (DkNN) algorithm, providing a
credibility metric for detecting out-of-distribution inputs and ensuring
robust, transparent decision-making. Experimental results show that the
proposed framework reduces real-world data needs by 70%, beam training overhead
by 62%, and improves outlier detection robustness by up to 8.5x, achieving
near-optimal spectral efficiency and transparent decision making compared to
traditional softmax based DL models.

</details>


### [295] [Semi-Supervised Federated Learning via Dual Contrastive Learning and Soft Labeling for Intelligent Fault Diagnosis](https://arxiv.org/abs/2507.14181)
*Yajiao Dai,Jun Li,Zhen Mei,Yiyang Ni,Shi Jin,Zengxiang Li,Sheng Guo,Wei Xiang*

Main category: cs.LG

TL;DR: A semi-supervised federated learning framework (SSFL-DCSL) is proposed to address data scarcity and privacy in intelligent fault diagnosis, using dual contrastive loss and soft labeling for improved accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional supervised deep learning requires large labeled datasets, which are costly and distributed across clients. Data distribution differences also hinder performance.

Method: SSFL-DCSL integrates dual contrastive loss and soft labeling, uses a sample weighting function for pseudo-label bias, and aggregates local prototypes for knowledge sharing.

Result: Experiments show SSFL-DCSL improves accuracy by 1.15% to 7.85% over state-of-the-art methods, especially with only 10% labeled data.

Conclusion: SSFL-DCSL effectively addresses data and label scarcity while ensuring privacy, outperforming existing methods in challenging scenarios.

Abstract: Intelligent fault diagnosis (IFD) plays a crucial role in ensuring the safe
operation of industrial machinery and improving production efficiency. However,
traditional supervised deep learning methods require a large amount of training
data and labels, which are often located in different clients. Additionally,
the cost of data labeling is high, making labels difficult to acquire.
Meanwhile, differences in data distribution among clients may also hinder the
model's performance. To tackle these challenges, this paper proposes a
semi-supervised federated learning framework, SSFL-DCSL, which integrates dual
contrastive loss and soft labeling to address data and label scarcity for
distributed clients with few labeled samples while safeguarding user privacy.
It enables representation learning using unlabeled data on the client side and
facilitates joint learning among clients through prototypes, thereby achieving
mutual knowledge sharing and preventing local model divergence. Specifically,
first, a sample weighting function based on the Laplace distribution is
designed to alleviate bias caused by low confidence in pseudo labels during the
semi-supervised training process. Second, a dual contrastive loss is introduced
to mitigate model divergence caused by different data distributions, comprising
local contrastive loss and global contrastive loss. Third, local prototypes are
aggregated on the server with weighted averaging and updated with momentum to
share knowledge among clients. To evaluate the proposed SSFL-DCSL framework,
experiments are conducted on two publicly available datasets and a dataset
collected on motors from the factory. In the most challenging task, where only
10\% of the data are labeled, the proposed SSFL-DCSL can improve accuracy by
1.15% to 7.85% over state-of-the-art methods.

</details>


### [296] [From Bias to Behavior: Learning Bull-Bear Market Dynamics with Contrastive Modeling](https://arxiv.org/abs/2507.14182)
*Xiaotong Luo,Shengda Zhuo,Min Chen,Lichun Li,Ruizhao Lu,Wenqi Fan,Shuqiang Huang,Yin Tang*

Main category: cs.LG

TL;DR: The paper introduces the B4 model to analyze bull and bear regimes in financial markets, combining price data and external signals to predict trends and interpret biases and behaviors.


<details>
  <summary>Details</summary>
Motivation: Financial markets are complex due to historical data and external narratives, leading to biases that complicate modeling. The paper aims to explore bull and bear regimes to better understand market dynamics.

Method: Proposes the B4 model, which embeds price sequences and contextual signals into a shared latent space, using inertial pairing and dual competition to capture behavioral divergence.

Result: B4 outperforms in trend prediction and offers interpretable insights into biases, behaviors, and market dynamics.

Conclusion: The B4 model effectively captures market heterogeneity and bias-driven asymmetry, providing a robust framework for understanding and predicting financial trends.

Abstract: Financial markets exhibit highly dynamic and complex behaviors shaped by both
historical price trajectories and exogenous narratives, such as news, policy
interpretations, and social media sentiment. The heterogeneity in these data
and the diverse insight of investors introduce biases that complicate the
modeling of market dynamics. Unlike prior work, this paper explores the
potential of bull and bear regimes in investor-driven market dynamics. Through
empirical analysis on real-world financial datasets, we uncover a dynamic
relationship between bias variation and behavioral adaptation, which enhances
trend prediction under evolving market conditions. To model this mechanism, we
propose the Bias to Behavior from Bull-Bear Dynamics model (B4), a unified
framework that jointly embeds temporal price sequences and external contextual
signals into a shared latent space where opposing bull and bear forces
naturally emerge, forming the foundation for bias representation. Within this
space, an inertial pairing module pairs temporally adjacent samples to preserve
momentum, while the dual competition mechanism contrasts bullish and bearish
embeddings to capture behavioral divergence. Together, these components allow
B4 to model bias-driven asymmetry, behavioral inertia, and market
heterogeneity. Experimental results on real-world financial datasets
demonstrate that our model not only achieves superior performance in predicting
market trends but also provides interpretable insights into the interplay of
biases, investor behaviors, and market dynamics.

</details>


### [297] [LaCache: Ladder-Shaped KV Caching for Efficient Long-Context Modeling of Large Language Models](https://arxiv.org/abs/2507.14204)
*Dachuan Shi,Yonggan Fu,Xiangchi Yuan,Zhongzhi Yu,Haoran You,Sixu Li,Xin Dong,Jan Kautz,Pavlo Molchanov,Yingyan,Lin*

Main category: cs.LG

TL;DR: LaCache introduces a training-free KV cache optimization method for LLMs, enhancing long-range capabilities and continuous generation without OOM.


<details>
  <summary>Details</summary>
Motivation: Address efficiency bottlenecks in LLMs due to increasing KV pairs with longer sequences, while maintaining robust long-range capabilities.

Method: LaCache uses a ladder-shaped KV cache pattern and iterative compaction to optimize storage and enable dynamic compression.

Result: Experiments show LaCache effectively boosts long-range capabilities and continuous generation in LLMs.

Conclusion: LaCache provides a scalable solution for efficient and accurate generative inference in LLMs.

Abstract: Recent advancements in Large Language Models (LLMs) have spurred interest in
numerous applications requiring robust long-range capabilities, essential for
processing extensive input contexts and continuously generating extended
outputs. As sequence lengths increase, the number of Key-Value (KV) pairs in
LLMs escalates, creating a significant efficiency bottleneck. In this paper, we
propose a new KV cache optimization paradigm called LaCache, a training-free
method for efficient and accurate generative inference of LLMs. LaCache enables
LLMs to simultaneously address both of the critical challenges in long-range
modeling: robust long-range capabilities and continuous generation without
running out-of-memory (OOM). Specifically, LaCache integrates two key
innovations: (1) a ladder-shaped KV cache pattern that stores KV pairs not only
sequentially (left-to-right within each layer) but also across layers (from
shallow to deep), providing an extended span for capturing long-range
dependencies under a fixed storage budget, thereby boosting long-range
capabilities; and (2) an iterative compaction mechanism that progressively
compresses older caches, freeing up space for new tokens within a fixed cache
size. This token distance-based dynamic compression enables more effective
continuous generation under constrained cache budgets. Experiments across
various tasks, benchmarks, and LLM models consistently validate LaCache's
effectiveness in enhancing LLMs' long-range capabilities. Our code is available
at https://github.com/GATECH-EIC/LaCache.

</details>


### [298] [Developing an AI-Guided Assistant Device for the Deaf and Hearing Impaired](https://arxiv.org/abs/2507.14215)
*Jiayu,Liu*

Main category: cs.LG

TL;DR: A deep learning system for an accessibility device for the deaf or hearing impaired, featuring sound localization and identification in real time using custom CNNs, fine-tuned CLAP models, and multimodal integration.


<details>
  <summary>Details</summary>
Motivation: To address the gap in accessibility devices for the deaf or hearing impaired by leveraging machine learning for accurate sound localization and identification.

Method: 1. JerryNet (CNN for direction of arrival). 2. Fine-tuned CLAP model for audio classification. 3. Multimodal integration (audio, visual, text) with Yolov9 and CIoU for localization.

Result: JerryNet: 91.1% precision in sound direction. CLAP: 98.5% (custom) and 95% (AudioSet) accuracy. Audio-visual localization: CIoU 0.892, AUC 0.658.

Conclusion: The system shows high accuracy and potential for future accessibility devices.

Abstract: This study aims to develop a deep learning system for an accessibility device
for the deaf or hearing impaired. The device will accurately localize and
identify sound sources in real time. This study will fill an important gap in
current research by leveraging machine learning techniques to target the
underprivileged community. The system includes three main components. 1.
JerryNet: A custom designed CNN architecture that determines the direction of
arrival (DoA) for nine possible directions. 2. Audio Classification: This model
is based on fine-tuning the Contrastive Language-Audio Pretraining (CLAP) model
to identify the exact sound classes only based on audio. 3. Multimodal
integration model: This is an accurate sound localization model that combines
audio, visual, and text data to locate the exact sound sources in the images.
The part consists of two modules, one object detection using Yolov9 to generate
all the bounding boxes of the objects, and an audio visual localization model
to identify the optimal bounding box using complete Intersection over Union
(CIoU). The hardware consists of a four-microphone rectangular formation and a
camera mounted on glasses with a wristband for displaying necessary information
like direction. On a custom collected data set, JerryNet achieved a precision
of 91. 1% for the sound direction, outperforming all the baseline models. The
CLAP model achieved 98.5% and 95% accuracy on custom and AudioSet datasets,
respectively. The audio-visual localization model within component 3 yielded a
cIoU of 0.892 and an AUC of 0.658, surpassing other similar models. There are
many future potentials to this study, paving the way to creating a new
generation of accessibility devices.

</details>


### [299] [Geometry-Aware Active Learning of Pattern Rankings via Choquet-Based Aggregation](https://arxiv.org/abs/2507.14217)
*Tudor Matei Opran,Samir Loudni*

Main category: cs.LG

TL;DR: Proposes an interactive learning framework for pattern mining to address pattern explosion, using nonlinear utility aggregation and geometry-aware query selection.


<details>
  <summary>Details</summary>
Motivation: To solve the pattern explosion problem in pattern mining by efficiently modeling user preferences and reducing required interactions.

Method: Uses a Choquet integral for utility aggregation, geometry-aware query selection, and a branch-and-bound strategy for efficient query identification.

Result: Outperforms ChoquetRank, achieving better ranking accuracy with fewer user interactions on UCI datasets.

Conclusion: The framework effectively addresses pattern explosion and improves efficiency in pattern mining.

Abstract: We address the pattern explosion problem in pattern mining by proposing an
interactive learning framework that combines nonlinear utility aggregation with
geometry-aware query selection. Our method models user preferences through a
Choquet integral over multiple interestingness measures and exploits the
geometric structure of the version space to guide the selection of informative
comparisons. A branch-and-bound strategy with tight distance bounds enables
efficient identification of queries near the decision boundary. Experiments on
UCI datasets show that our approach outperforms existing methods such as
ChoquetRank, achieving better ranking accuracy with fewer user interactions.

</details>


### [300] [Artificial Intelligence for Green Hydrogen Yield Prediction and Site Suitability using SHAP-Based Composite Index: Focus on Oman](https://arxiv.org/abs/2507.14219)
*Obumneme Zimuzor Nwafor,Mohammed Abdul Majeed Al Hooti*

Main category: cs.LG

TL;DR: An AI framework using SHAP values identifies optimal green hydrogen production sites by analyzing environmental, atmospheric, and infrastructural factors, achieving 98% accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the lack of direct hydrogen yield data and provide an objective method for identifying suitable green hydrogen production sites in solar-rich arid regions.

Method: A multi-stage AI pipeline combining unsupervised clustering, supervised machine learning, and SHAP analysis on integrated meteorological, topographic, and temporal datasets.

Result: Identified water proximity, elevation, and seasonal variation as key factors for site suitability in Oman, with 98% predictive accuracy.

Conclusion: The framework offers a scalable, data-driven tool for green hydrogen planning in data-scarce regions, replacing subjective expert opinions.

Abstract: As nations seek sustainable alternatives to fossil fuels, green hydrogen has
emerged as a promising strategic pathway toward decarbonisation, particularly
in solar-rich arid regions. However, identifying optimal locations for hydrogen
production requires the integration of complex environmental, atmospheric, and
infrastructural factors, often compounded by limited availability of direct
hydrogen yield data. This study presents a novel Artificial Intelligence (AI)
framework for computing green hydrogen yield and site suitability index using
mean absolute SHAP (SHapley Additive exPlanations) values. This framework
consists of a multi-stage pipeline of unsupervised multi-variable clustering,
supervised machine learning classifier and SHAP algorithm. The pipeline trains
on an integrated meteorological, topographic and temporal dataset and the
results revealed distinct spatial patterns of suitability and relative
influence of the variables. With model predictive accuracy of 98%, the result
also showed that water proximity, elevation and seasonal variation are the most
influential factors determining green hydrogen site suitability in Oman with
mean absolute shap values of 2.470891, 2.376296 and 1.273216 respectively.
Given limited or absence of ground-truth yield data in many countries that have
green hydrogen prospects and ambitions, this study offers an objective and
reproducible alternative to subjective expert weightings, thus allowing the
data to speak for itself and potentially discover novel latent groupings
without pre-imposed assumptions. This study offers industry stakeholders and
policymakers a replicable and scalable tool for green hydrogen infrastructure
planning and other decision making in data-scarce regions.

</details>


### [301] [Domain Generalization via Pareto Optimal Gradient Matching](https://arxiv.org/abs/2507.14227)
*Khoi Do,Duong Nguyen,Nam-Khanh Le,Quoc-Viet Pham,Binh-Son Hua,Won-Joo Hwang*

Main category: cs.LG

TL;DR: Proposes POGM for gradient-based domain generalization, addressing gradient fluctuations and computational inefficiency by leveraging gradient trajectories and meta-learning.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with gradient fluctuations and high computation costs in domain generalization.

Method: POGM uses gradient trajectories for meta-learning, maximizing gradient inner products while aligning with empirical risk minimization.

Result: POGM achieves competitive performance on DomainBed datasets with computational efficiency.

Conclusion: POGM effectively balances gradient alignment and computational cost, outperforming baselines.

Abstract: In this study, we address the gradient-based domain generalization problem,
where predictors aim for consistent gradient directions across different
domains. Existing methods have two main challenges. First, minimization of
gradient empirical distance or gradient inner products (GIP) leads to gradient
fluctuations among domains, thereby hindering straightforward learning. Second,
the direct application of gradient learning to the joint loss function can
incur high computation overheads due to second-order derivative approximation.
To tackle these challenges, we propose a new Pareto Optimality Gradient
Matching (POGM) method. In contrast to existing methods that add gradient
matching as regularization, we leverage gradient trajectories as collected data
and apply independent training at the meta-learner. In the meta-update, we
maximize GIP while limiting the learned gradient from deviating too far from
the empirical risk minimization gradient trajectory. By doing so, the aggregate
gradient can incorporate knowledge from all domains without suffering gradient
fluctuation towards any particular domain. Experimental evaluations on datasets
from DomainBed demonstrate competitive results yielded by POGM against other
baselines while achieving computational efficiency.

</details>


### [302] [A million-scale dataset and generalizable foundation model for nanomaterial-protein interactions](https://arxiv.org/abs/2507.14245)
*Hengjie Yu,Kenneth A. Dawson,Haiyun Yang,Shuya Liu,Yan Yan,Yaochu Jin*

Main category: cs.LG

TL;DR: NanoPro-3M is the largest nanomaterial-protein interaction dataset, enabling NanoProFormer, a multimodal AI model, to predict affinities with high generalization and handle missing data.


<details>
  <summary>Details</summary>
Motivation: Understanding nanomaterial-protein interactions is critical for medicine and environmental science, but progress is limited by small datasets and poor model generalizability.

Method: Developed NanoPro-3M (3.2M samples, 37K proteins) and NanoProFormer, a multimodal model for predicting interactions, handling missing features, and generalizing to unseen cases.

Result: NanoProFormer outperforms single-modality methods, identifies corona formation determinants, and excels in zero-shot inference and fine-tuning for downstream tasks.

Conclusion: This work provides a robust foundation for predicting nanomaterial-protein interactions, reducing experimental needs and accelerating applications.

Abstract: Unlocking the potential of nanomaterials in medicine and environmental
science hinges on understanding their interactions with proteins, a complex
decision space where AI is poised to make a transformative impact. However,
progress has been hindered by limited datasets and the restricted
generalizability of existing models. Here, we propose NanoPro-3M, the largest
nanomaterial-protein interaction dataset to date, comprising over 3.2 million
samples and 37,000 unique proteins. Leveraging this, we present NanoProFormer,
a foundational model that predicts nanomaterial-protein affinities through
multimodal representation learning, demonstrating strong generalization,
handling missing features, and unseen nanomaterials or proteins. We show that
multimodal modeling significantly outperforms single-modality approaches and
identifies key determinants of corona formation. Furthermore, we demonstrate
its applicability to a range of downstream tasks through zero-shot inference
and fine-tuning. Together, this work establishes a solid foundation for
high-performance and generalized prediction of nanomaterial-protein interaction
endpoints, reducing experimental reliance and accelerating various in vitro
applications.

</details>


### [303] [Linearized Diffusion Map](https://arxiv.org/abs/2507.14257)
*Julio Candanedo*

Main category: cs.LG

TL;DR: LDM is a linear dimensionality reduction method that combines diffusion-map intuition with linear efficiency, outperforming PCA in manifold-structured datasets.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between nonlinear diffusion-based methods and linear techniques like PCA, offering interpretability and efficiency.

Method: LDM uses a linear approximation of the diffusion-map kernel, tested on synthetic (Swiss roll, hyperspheres) and real-world datasets (MNIST, COIL-20).

Result: LDM excels in manifold-structured datasets, while PCA is better for variance/noise-dominated cases. LDM's kernel also enables NMF for interpretable analysis.

Conclusion: LDM is a promising linear dimensionality reduction tool with theoretical and practical potential.

Abstract: We introduce the Linearized Diffusion Map (LDM), a novel linear
dimensionality reduction method constructed via a linear approximation of the
diffusion-map kernel. LDM integrates the geometric intuition of diffusion-based
nonlinear methods with the computational simplicity, efficiency, and
interpretability inherent in linear embeddings such as PCA and classical MDS.
Through comprehensive experiments on synthetic datasets (Swiss roll and
hyperspheres) and real-world benchmarks (MNIST and COIL-20), we illustrate that
LDM captures distinct geometric features of datasets compared to PCA, offering
complementary advantages. Specifically, LDM embeddings outperform PCA in
datasets exhibiting explicit manifold structures, particularly in
high-dimensional regimes, whereas PCA remains preferable in scenarios dominated
by variance or noise. Furthermore, the complete positivity of LDM's kernel
matrix allows direct applicability of Non-negative Matrix Factorization (NMF),
suggesting opportunities for interpretable latent-structure discovery. Our
analysis positions LDM as a valuable new linear dimensionality reduction
technique with promising theoretical and practical extensions.

</details>


### [304] [A Simple "Try Again" Can Elicit Multi-Turn LLM Reasoning](https://arxiv.org/abs/2507.14295)
*Licheng Liu,Zihan Wang,Linjie Li,Chenwei Xu,Yiping Lu,Han Liu,Avirup Sil,Manling Li*

Main category: cs.LG

TL;DR: Training Large Reasoning Models (LRMs) with multi-turn RL using unary feedback improves both single-turn performance and multi-turn reasoning by up to 14%.


<details>
  <summary>Details</summary>
Motivation: Existing RL methods for LRMs focus on single-turn problem solving, leading to repetitive responses and poor multi-turn reasoning. The goal is to enhance LRMs' ability to reflect and revise answers based on feedback.

Method: Introduces Unary Feedback as Observation (UFO), a minimal feedback method for multi-turn RL training, and designs reward structures to encourage diverse reasoning and fewer turns for correct answers.

Result: UFO improves multi-turn reasoning accuracy by up to 14% while maintaining single-turn performance.

Conclusion: Multi-turn RL with unary feedback (UFO) effectively enhances LRMs' problem-solving and feedback-revision capabilities.

Abstract: Multi-turn problem solving is critical yet challenging for Large Reasoning
Models (LRMs) to reflect on their reasoning and revise from feedback. Existing
Reinforcement Learning (RL) methods train large reasoning models on a
single-turn paradigm with verifiable rewards. However, we observe that models
trained with existing RL paradigms often lose their ability to solve problems
across multiple turns and struggle to revise answers based on contextual
feedback, leading to repetitive responses. We ask: can LRMs learn to reflect
their answers in a multi-turn context? In this work, we find that training
models with multi-turn RL using only unary feedback (e.g., "Let's try again")
after wrong answers can improve both single-turn performance and multi-turn
reasoning. We introduce Unary Feedback as Observation (UFO) for reinforcement
learning, which uses minimal yet common unary user feedback during iterative
problem solving. It can be easily applied to existing single-turn RL training
setups. Experimental results show that RL training with UFO keeps single-turn
performance and improves multi-turn reasoning accuracy by up to 14%, enabling
language models to better react to feedback in multi-turn problem solving. To
further minimize the number of turns needed for a correct answer while
encouraging diverse reasoning when mistakes occur, we design reward structures
that guide models to produce careful and deliberate answers in each turn. Code:
https://github.com/lichengliu03/unary-feedback

</details>


### [305] [FedStrategist: A Meta-Learning Framework for Adaptive and Robust Aggregation in Federated Learning](https://arxiv.org/abs/2507.14322)
*Md Rafid Haque,Abu Raihan Mostofa Kamal,Md. Azam Hossain*

Main category: cs.LG

TL;DR: FedStrategist is a meta-learning framework for adaptive defense against model poisoning in Federated Learning, outperforming static rules by dynamically selecting optimal aggregation strategies.


<details>
  <summary>Details</summary>
Motivation: Static defenses in FL are context-dependent and fail against adaptive adversaries or heterogeneous data. FedStrategist addresses this by enabling real-time, adaptive defense.

Method: FedStrategist uses a lightweight contextual bandit agent to dynamically choose aggregation rules from a defense arsenal based on real-time metrics.

Result: Experiments show no single static rule is universally optimal; FedStrategist learns superior policies, even against stealth adversaries, and balances performance-security trade-offs.

Conclusion: FedStrategist offers a practical, analyzable approach for resilient decentralized AI, controllable via a risk tolerance parameter.

Abstract: Federated Learning (FL) offers a paradigm for privacy-preserving
collaborative AI, but its decentralized nature creates significant
vulnerabilities to model poisoning attacks. While numerous static defenses
exist, their effectiveness is highly context-dependent, often failing against
adaptive adversaries or in heterogeneous data environments. This paper
introduces FedStrategist, a novel meta-learning framework that reframes robust
aggregation as a real-time, cost-aware control problem. We design a lightweight
contextual bandit agent that dynamically selects the optimal aggregation rule
from an arsenal of defenses based on real-time diagnostic metrics. Through
comprehensive experiments, we demonstrate that no single static rule is
universally optimal. We show that our adaptive agent successfully learns
superior policies across diverse scenarios, including a ``Krum-favorable"
environment and against a sophisticated "stealth" adversary designed to
neutralize specific diagnostic signals. Critically, we analyze the paradoxical
scenario where a non-robust baseline achieves high but compromised accuracy,
and demonstrate that our agent learns a conservative policy to prioritize model
integrity. Furthermore, we prove the agent's policy is controllable via a
single "risk tolerance" parameter, allowing practitioners to explicitly manage
the trade-off between performance and security. Our work provides a new,
practical, and analyzable approach to creating resilient and intelligent
decentralized AI systems.

</details>


### [306] [Rethinking Individual Fairness in Deepfake Detection](https://arxiv.org/abs/2507.14326)
*Aryana Hou,Li Lin,Justin Li,Shu Hu*

Main category: cs.LG

TL;DR: The paper addresses fairness gaps in deepfake detection, focusing on individual fairness, and proposes a framework to improve it without compromising detection performance.


<details>
  <summary>Details</summary>
Motivation: The misuse of generative AI models through deepfakes poses risks, and existing detection methods lack fairness, especially at the individual level.

Method: The authors propose a generalizable framework to enhance individual fairness in deepfake detectors, integrating it into existing systems.

Result: Experiments on leading datasets show the framework improves individual fairness while maintaining strong detection performance, outperforming state-of-the-art methods.

Conclusion: The work highlights the importance of individual fairness in deepfake detection and provides a practical solution to address it.

Abstract: Generative AI models have substantially improved the realism of synthetic
media, yet their misuse through sophisticated DeepFakes poses significant
risks. Despite recent advances in deepfake detection, fairness remains
inadequately addressed, enabling deepfake markers to exploit biases against
specific populations. While previous studies have emphasized group-level
fairness, individual fairness (i.e., ensuring similar predictions for similar
individuals) remains largely unexplored. In this work, we identify for the
first time that the original principle of individual fairness fundamentally
fails in the context of deepfake detection, revealing a critical gap previously
unexplored in the literature. To mitigate it, we propose the first
generalizable framework that can be integrated into existing deepfake detectors
to enhance individual fairness and generalization. Extensive experiments
conducted on leading deepfake datasets demonstrate that our approach
significantly improves individual fairness while maintaining robust detection
performance, outperforming state-of-the-art methods. The code is available at
https://github.com/Purdue-M2/Individual-Fairness-Deepfake-Detection.

</details>


### [307] [Development and Deployment of Hybrid ML Models for Critical Heat Flux Prediction in Annulus Geometries](https://arxiv.org/abs/2507.14332)
*Aidan Furlong,Xingang Zhao,Robert Salko,Xu Wu*

Main category: cs.LG

TL;DR: The paper explores ML models for predicting CHF in annular geometries, outperforming traditional empirical correlations with significantly lower errors.


<details>
  <summary>Details</summary>
Motivation: Accurate CHF prediction is crucial for reactor safety, but traditional methods lack interpretability and resilience to data scarcity, especially for annular geometries.

Method: Four ML models were developed and validated using CTF subchannel code, correcting residuals from three empirical correlations (Biasi, Bowring, Katto) with 577 experimental data points.

Result: ML models achieved mean relative errors below 3.5%, outperforming empirical correlations (26% error).

Conclusion: Hybrid ML models are superior for CHF prediction in annular geometries, offering higher accuracy and reliability.

Abstract: Accurate prediction of critical heat flux (CHF) is an essential component of
safety analysis in pressurized and boiling water reactors. To support reliable
prediction of this quantity, several empirical correlations and lookup tables
have been constructed from physical experiments over the past several decades.
With the onset of accessible machine learning (ML) frameworks, multiple
initiatives have been established with the goal of predicting CHF more
accurately than these traditional methods. While purely data-driven surrogate
modeling has been extensively investigated, these approaches lack
interpretability, lack resilience to data scarcity, and have been developed
mostly using data from tube experiments. As a result, bias-correction hybrid
approaches have become increasingly popular, which correct initial
"low-fidelity" estimates provided by deterministic base models by using
ML-predicted residuals. This body of work has mostly considered round tube
geometries; annular geometry-specific ML models have not yet been deployed in
thermal hydraulic codes. This study developed, deployed, and validated four ML
models to predict CHF in annular geometries using the CTF subchannel code.
Three empirical correlation models, Biasi, Bowring, and Katto, were used as
base models for comparison. The ML models were trained and tested using 577
experimental annulus data points from four datasets: Becker, Beus, Janssen, and
Mortimore. Baseline CHF predictions were obtained from the empirical
correlations, with mean relative errors above 26%. The ML-driven models
achieved mean relative errors below 3.5%, with no more than one point exceeding
the 10% error envelope. In all cases, the hybrid ML models significantly
outperformed their empirical counterparts.

</details>


### [308] [Influence Functions for Preference Dataset Pruning](https://arxiv.org/abs/2507.14344)
*Daniel Fein,Gabriela Aranguiz-Dias*

Main category: cs.LG

TL;DR: The paper explores using influence functions to filter noisy training data in fine-tuning language models, showing a 1.5% accuracy improvement after pruning 10% of examples. Gradient similarity outperforms influence functions for identifying helpful examples.


<details>
  <summary>Details</summary>
Motivation: Human preference datasets for fine-tuning language models are often noisy, and small post-training datasets make it feasible to use influence functions for filtering harmful examples.

Method: The study adapts the TL;DR dataset for reward model training and uses conjugate-gradient approximated influence functions to filter datasets. Gradient similarity is also evaluated.

Result: Influence function filtering improves retraining accuracy by 1.5% after removing 10% of training examples. Gradient similarity is better than influence functions for detecting helpful examples.

Conclusion: Local curvature (influence functions) is important for detecting harmful examples, but gradient similarity is more effective for identifying helpful ones.

Abstract: Language models are commonly fine-tuned via reinforcement learning to alter
their behavior or elicit new capabilities. Datasets used for these purposes,
and particularly human preference datasets, are often noisy. The relatively
small size post-training datasets, combined with parameter-efficient
fine-tuning methods, enable the use of influence functions approximations to
detect and prune training examples that are harmful to performance on a
validation set. In this work, we adapt the TL;DR dataset for reward model
training to demonstrate how conjugate-gradient approximated influence functions
can be used to filter datasets. In our experiments, influence function
filtering yields a small retraining accuracy uplift of 1.5% after removing 10%
of training examples. We also show that gradient similarity outperforms
influence functions for detecting helpful training examples. This suggests that
local curvature is important for detecting harmful training examples, but less
so for identifying helpful examples.

</details>


### [309] [Solo Connection: A Parameter Efficient Fine-Tuning Technique for Transformers](https://arxiv.org/abs/2507.14353)
*Harsh Nilesh Pathak,Randy Paffenroth*

Main category: cs.LG

TL;DR: Solo Connection is a novel PEFT method that adapts decoder-block representations, outperforming LoRA with fewer parameters and leveraging homotopy theory for stable adaptation.


<details>
  <summary>Details</summary>
Motivation: To improve fine-tuning efficiency and performance in LLMs by focusing on decoder-block level adaptation rather than individual weight matrices, inspired by homotopy theory.

Method: Introduces Solo Connection, a trainable linear transformation interpolating between zero and task-specific representations, using long skip connections across decoder blocks.

Result: Outperforms LoRA on E2E benchmarks, reduces trainable parameters by 59% vs. LoRA and >99% vs. full fine-tuning.

Conclusion: Solo Connection offers a more efficient and effective fine-tuning approach for LLMs, especially in larger architectures with many decoder blocks.

Abstract: Parameter efficient fine tuning (PEFT) is a versatile and extensible approach
for adapting a Large Language Model (LLM) for newer tasks. One of the most
prominent PEFT approaches, Low Rank Adaptation (LoRA), primarily focuses on
adjusting the attention weight matrices within individual decoder blocks of a
Generative Pre trained Transformer (GPT2). In contrast, we introduce Solo
Connection a novel method that adapts the representation at the decoder-block
level rather than modifying individual weight matrices. Not only does Solo
Connection outperform LoRA on E2E natural language generation benchmarks, but
it also reduces the number of trainable parameters by 59% relative to LoRA and
by more than 99% compared to full fine-tuning of GPT2, an early version of
Large Language Models (LLMs). Solo Connection is also motivated by homotopy
theory: we introduce a trainable linear transformation that gradually
interpolates between a zero vector and the task-specific representation,
enabling smooth and stable adaptation over time. While skip connections in the
original 12 layer GPT2 are typically confined to individual decoder blocks,
subsequent GPT2 variants scale up to 48 layers, and even larger language models
can include 128 or more decoder blocks. These expanded architectures underscore
the need to revisit how skip connections are employed during fine-tuning. This
paper focuses on long skip connections that link outputs of different decoder
blocks, potentially enhancing the model's ability to adapt to new tasks while
leveraging pre-trained knowledge.

</details>


### [310] [Incremental Causal Graph Learning for Online Cyberattack Detection in Cyber-Physical Infrastructures](https://arxiv.org/abs/2507.14387)
*Arun Vignesh Malarkkan,Dongjie Wang,Haoyue Bai,Yanjie Fu*

Main category: cs.LG

TL;DR: INCADET is a novel framework for real-time cyberattack detection using incremental causal graph learning, addressing limitations of static methods by dynamically updating causal graphs and leveraging GCNs for classification.


<details>
  <summary>Details</summary>
Motivation: The increasing threat of cyberattacks on critical infrastructures requires adaptable detection methods to handle complex interdependencies and evolving attack patterns, overcoming issues like false positives and static data reliance.

Method: INCADET includes three modules: early symptom detection via edge-weight divergence, incremental causal graph learning with experience replay, and causal graph classification using GCNs.

Result: Experiments show INCADET outperforms static causal and deep temporal baselines in accuracy, robustness, and adaptability for evolving attack scenarios.

Conclusion: INCADET provides an effective solution for real-time cyberattack detection by dynamically learning causal relationships and adapting to system behavior changes.

Abstract: The escalating threat of cyberattacks on real-time critical infrastructures
poses serious risks to public safety, demanding detection methods that
effectively capture complex system interdependencies and adapt to evolving
attack patterns. Traditional real-time anomaly detection techniques often
suffer from excessive false positives due to their statistical sensitivity to
high data variance and class imbalance. To address these limitations, recent
research has explored modeling causal relationships among system components.
However, prior work mainly focuses on offline causal graph-based approaches
that require static historical data and fail to generalize to real-time
settings. These methods are fundamentally constrained by: (1) their inability
to adapt to dynamic shifts in data distribution without retraining, and (2) the
risk of catastrophic forgetting when lacking timely supervision in live
systems. To overcome these challenges, we propose INCADET, a novel framework
for incremental causal graph learning tailored to real-time cyberattack
detection. INCADET dynamically captures evolving system behavior by
incrementally updating causal graphs across streaming time windows. The
framework comprises three modules: 1) Early Symptom Detection: Detects
transitions in system status using divergence in edge-weight distributions
across sequential causal graphs. 2) Incremental Causal Graph Learning:
Leverages experience replay and edge reinforcement to continually refine causal
structures while preserving prior knowledge. 3) Causal Graph Classification:
Employs Graph Convolutional Networks (GCNs) to classify system status using the
learned causal graphs. Extensive experiments on real-world critical
infrastructure datasets demonstrate that INCADET achieves superior accuracy,
robustness, and adaptability compared to both static causal and deep temporal
baselines in evolving attack scenarios.

</details>


### [311] [It's Not That Simple. An Analysis of Simple Test-Time Scaling](https://arxiv.org/abs/2507.14419)
*Guojun Wu*

Main category: cs.LG

TL;DR: Simple test-time scaling in models distilled from o1-like models is mainly effective when scaling down by enforcing a maximum length, while scaling up via 'Wait' appends leads to inconsistencies. Fine-tuning on long CoT data has little impact. o1-like models naturally scale up compute during RL, outperforming simple scaling methods.


<details>
  <summary>Details</summary>
Motivation: To analyze the effectiveness and limitations of simple test-time scaling methods in replicating the scaling behavior of o1-like models.

Method: Compare scaling down (enforcing max length) and scaling up (appending 'Wait') in test-time compute, and evaluate the impact of fine-tuning on long CoT data.

Result: Scaling down is effective, while scaling up causes inconsistencies. Fine-tuning has no significant impact. o1-like models naturally scale up compute during RL, achieving higher performance.

Conclusion: Simple test-time scaling can mimic scaling behavior but falls short of unlocking higher performance. The goal should be to surpass original model limits, not just replicate scaling.

Abstract: Prior work proposed simple test-time scaling, a method for replicating this
scaling behavior with models distilled from o1-like models by manually
controlling test-time compute: either scaling down by enforcing a maximum
length or scaling up by iteratively appending "Wait" when the model is about to
terminate its generation. This paper presents an analysis of simple test-time
scaling and finds that the scaling behavior is largely attributed to scaling
down by enforcing a maximum length. In contrast, fine-tuning on long CoT data
distilled from o1-like models has no significant impact on scaling behavior,
and scaling up by appending "Wait" leads to inconsistencies, as the model may
oscillate between solutions. A key distinction exists between scaling down by
enforcing a maximum length and scaling up test-time compute in o1-like models,
such as DeepSeek-R1\@. These models are typically allowed to utilize as much
compute as needed, with the only constraint being the model's maximum supported
length. By learning to naturally scale up test-time compute during
reinforcement learning, o1-like models surpass their peak performance when
scaling up. In contrast, simple test-time scaling progressively imposes a lower
upper limit on model performance as it scales down. While replicating the
test-time scaling behavior of o1 models can be straightforward by scaling down,
it is crucial to recognize that the goal of scaling test-time compute is to
unlock higher performance -- beyond what the model could originally achieve --
rather than merely reproducing the appearance of scaling behavior.

</details>


### [312] [Deep RL Dual Sourcing Inventory Management with Supply and Capacity Risk Awareness](https://arxiv.org/abs/2507.14446)
*Feng Liu,Ying Liu,Carson Eisenach*

Main category: cs.LG

TL;DR: The paper proposes using reinforcement learning (RL) with intervention models and pre-trained deep learning (DL) models to solve large-scale stochastic optimization problems, demonstrated on a supply chain inventory management problem.


<details>
  <summary>Details</summary>
Motivation: To efficiently tackle complex stochastic optimization problems by breaking them into scalable DL modules rather than modeling constraints directly in RL.

Method: Leverages deep RL models for learning and forecasting stochastic processes, introduces a constraint coordination mechanism for dual cost forecasting, and decomposes supply chain processes into composable DL modules.

Result: Improved performance on large real-world datasets by modularizing the problem and avoiding direct modeling of complex constraints.

Conclusion: The approach is effective for large-scale stochastic optimization, with open problems identified for future research on model efficacy.

Abstract: In this work, we study how to efficiently apply reinforcement learning (RL)
for solving large-scale stochastic optimization problems by leveraging
intervention models. The key of the proposed methodology is to better explore
the solution space by simulating and composing the stochastic processes using
pre-trained deep learning (DL) models. We demonstrate our approach on a
challenging real-world application, the multi-sourcing multi-period inventory
management problem in supply chain optimization. In particular, we employ deep
RL models for learning and forecasting the stochastic supply chain processes
under a range of assumptions. Moreover, we also introduce a constraint
coordination mechanism, designed to forecast dual costs given the
cross-products constraints in the inventory network. We highlight that instead
of directly modeling the complex physical constraints into the RL optimization
problem and solving the stochastic problem as a whole, our approach breaks down
those supply chain processes into scalable and composable DL modules, leading
to improved performance on large real-world datasets. We also outline open
problems for future research to further investigate the efficacy of such
models.

</details>


### [313] [ReDiSC: A Reparameterized Masked Diffusion Model for Scalable Node Classification with Structured Predictions](https://arxiv.org/abs/2507.14484)
*Yule Li,Yifeng Lu,Zhen Wang,Zhewei Wei,Yaliang Li,Bolin Ding*

Main category: cs.LG

TL;DR: ReDiSC, a reparameterized masked diffusion model, improves structured node classification by capturing label correlations, outperforming existing methods in scalability and performance.


<details>
  <summary>Details</summary>
Motivation: Existing GNNs assume conditional independence of node labels, ignoring their inherent correlations, which limits performance in structured prediction tasks.

Method: ReDiSC uses a reparameterized masked diffusion model within a variational EM framework to estimate joint label distributions, linking its M-step to GNN and label propagation hybrids.

Result: ReDiSC outperforms state-of-the-art baselines in performance and scalability, especially on large datasets where other methods fail.

Conclusion: ReDiSC effectively addresses label correlation in graphs, offering a scalable and superior solution for structured node classification.

Abstract: In recent years, graph neural networks (GNN) have achieved unprecedented
successes in node classification tasks. Although GNNs inherently encode
specific inductive biases (e.g., acting as low-pass or high-pass filters), most
existing methods implicitly assume conditional independence among node labels
in their optimization objectives. While this assumption is suitable for
traditional classification tasks such as image recognition, it contradicts the
intuitive observation that node labels in graphs remain correlated, even after
conditioning on the graph structure. To make structured predictions for node
labels, we propose ReDiSC, namely, Reparameterized masked Diffusion model for
Structured node Classification. ReDiSC estimates the joint distribution of node
labels using a reparameterized masked diffusion model, which is learned through
the variational expectation-maximization (EM) framework. Our theoretical
analysis shows the efficiency advantage of ReDiSC in the E-step compared to
DPM-SNC, a state-of-the-art model that relies on a manifold-constrained
diffusion model in continuous domain. Meanwhile, we explicitly link ReDiSC's
M-step objective to popular GNN and label propagation hybrid approaches.
Extensive experiments demonstrate that ReDiSC achieves superior or highly
competitive performance compared to state-of-the-art GNN, label propagation,
and diffusion-based baselines across both homophilic and heterophilic graphs of
varying sizes. Notably, ReDiSC scales effectively to large-scale datasets on
which previous structured diffusion methods fail due to computational
constraints, highlighting its significant practical advantage in structured
node classification tasks.

</details>


### [314] [Federated Reinforcement Learning in Heterogeneous Environments](https://arxiv.org/abs/2507.14487)
*Ukjo Hwang,Songnam Hong*

Main category: cs.LG

TL;DR: A Federated Reinforcement Learning (FRL) framework addresses environment heterogeneity (FRL-EH) by optimizing a global policy via collective learning while preserving privacy. The proposed FedRQ algorithm ensures robust performance and converges optimally, extending to continuous spaces with expectile loss.


<details>
  <summary>Details</summary>
Motivation: To tackle statistical heterogeneity in local environments and enable collaborative learning without compromising privacy, a robust FRL framework is needed.

Method: Introduces a novel global objective function and the FedRQ algorithm, proven to converge optimally. Extends to continuous spaces using expectile loss.

Result: Empirical evaluations show superior performance over existing FRL methods in diverse heterogeneous environments.

Conclusion: The FRL-EH framework and FedRQ algorithm effectively address environment heterogeneity, ensuring robust and scalable reinforcement learning.

Abstract: We investigate a Federated Reinforcement Learning with Environment
Heterogeneity (FRL-EH) framework, where local environments exhibit statistical
heterogeneity. Within this framework, agents collaboratively learn a global
policy by aggregating their collective experiences while preserving the privacy
of their local trajectories. To better reflect real-world scenarios, we
introduce a robust FRL-EH framework by presenting a novel global objective
function. This function is specifically designed to optimize a global policy
that ensures robust performance across heterogeneous local environments and
their plausible perturbations. We propose a tabular FRL algorithm named FedRQ
and theoretically prove its asymptotic convergence to an optimal policy for the
global objective function. Furthermore, we extend FedRQ to environments with
continuous state space through the use of expectile loss, addressing the key
challenge of minimizing a value function over a continuous subset of the state
space. This advancement facilitates the seamless integration of the principles
of FedRQ with various Deep Neural Network (DNN)-based RL algorithms. Extensive
empirical evaluations validate the effectiveness and robustness of our FRL
algorithms across diverse heterogeneous environments, consistently achieving
superior performance over the existing state-of-the-art FRL algorithms.

</details>


### [315] [Glitches in Decision Tree Ensemble Models](https://arxiv.org/abs/2507.14492)
*Satyankar Chandra,Ashutosh Gupta,Kaushik Mallik,Krishna Shankaranarayanan,Namrita Varshney*

Main category: cs.LG

TL;DR: The paper identifies 'glitches'—small input neighborhoods causing abrupt model output oscillations—as a new source of unreliability in AI models, especially those with steep decision boundaries. It formalizes glitches, demonstrates their prevalence, and proposes an NP-complete glitch-detection algorithm for GBDT models using MILP encoding.


<details>
  <summary>Details</summary>
Motivation: To enhance the reliability and consistency of AI models by addressing glitches, which cause abrupt output changes and indicate potential model inconsistencies.

Method: Formally defines glitches, demonstrates their existence in literature models/datasets, and develops an MILP-based algorithm for detecting glitches in GBDT models.

Result: Glitches are widespread and often signal model inconsistencies. The glitch-detection problem is NP-complete for tree ensembles, and the proposed algorithm is effective for GBDT benchmarks.

Conclusion: Glitches are a critical reliability issue in AI models, and the proposed method offers a practical solution for detecting them in GBDT models, despite computational complexity.

Abstract: Many critical decision-making tasks are now delegated to machine-learned
models, and it is imperative that their decisions are trustworthy and reliable,
and their outputs are consistent across similar inputs. We identify a new
source of unreliable behaviors-called glitches-which may significantly impair
the reliability of AI models having steep decision boundaries. Roughly
speaking, glitches are small neighborhoods in the input space where the model's
output abruptly oscillates with respect to small changes in the input. We
provide a formal definition of glitches, and use well-known models and datasets
from the literature to demonstrate that they have widespread existence and
argue they usually indicate potential model inconsistencies in the neighborhood
of where they are found. We proceed to the algorithmic search of glitches for
widely used gradient-boosted decision tree (GBDT) models. We prove that the
problem of detecting glitches is NP-complete for tree ensembles, already for
trees of depth 4. Our glitch-search algorithm for GBDT models uses an MILP
encoding of the problem, and its effectiveness and computational feasibility
are demonstrated on a set of widely used GBDT benchmarks taken from the
literature.

</details>


### [316] [Generative Distribution Distillation](https://arxiv.org/abs/2507.14503)
*Jiequan Cui,Beier Zhu,Qingshan Xu,Xiaogang Xu,Pengguang Chen,Xiaojuan Qi,Bei Yu,Hanwang Zhang,Richang Hong*

Main category: cs.LG

TL;DR: The paper introduces Generative Distribution Distillation (GenDD), a framework for knowledge distillation as a conditional generative problem, addressing challenges like high-dimensional optimization and lack of label supervision with Split Tokenization and Distribution Contraction techniques.


<details>
  <summary>Details</summary>
Motivation: The motivation is to improve knowledge distillation by treating it as a generative problem, overcoming limitations like high-dimensional optimization and insufficient label supervision.

Method: The method involves Split Tokenization for stable unsupervised KD and Distribution Contraction to integrate label supervision into the reconstruction objective.

Result: GenDD outperforms KL baseline by 16.29% on ImageNet in unsupervised settings and achieves 82.28% top-1 accuracy with ResNet-50 in supervised training.

Conclusion: GenDD sets a new state-of-the-art, proving effective for both unsupervised and supervised knowledge distillation.

Abstract: In this paper, we formulate the knowledge distillation (KD) as a conditional
generative problem and propose the \textit{Generative Distribution Distillation
(GenDD)} framework. A naive \textit{GenDD} baseline encounters two major
challenges: the curse of high-dimensional optimization and the lack of semantic
supervision from labels. To address these issues, we introduce a \textit{Split
Tokenization} strategy, achieving stable and effective unsupervised KD.
Additionally, we develop the \textit{Distribution Contraction} technique to
integrate label supervision into the reconstruction objective. Our theoretical
proof demonstrates that \textit{GenDD} with \textit{Distribution Contraction}
serves as a gradient-level surrogate for multi-task learning, realizing
efficient supervised training without explicit classification loss on
multi-step sampling image representations. To evaluate the effectiveness of our
method, we conduct experiments on balanced, imbalanced, and unlabeled data.
Experimental results show that \textit{GenDD} performs competitively in the
unsupervised setting, significantly surpassing KL baseline by \textbf{16.29\%}
on ImageNet validation set. With label supervision, our ResNet-50 achieves
\textbf{82.28\%} top-1 accuracy on ImageNet in 600 epochs training,
establishing a new state-of-the-art.

</details>


### [317] [SDSC:A Structure-Aware Metric for Semantic Signal Representation Learning](https://arxiv.org/abs/2507.14516)
*Jeyoung Lee,Hochul Kang*

Main category: cs.LG

TL;DR: The paper introduces SDSC, a structure-aware metric for time series SSL, addressing limitations of distance-based objectives like MSE. It improves semantic alignment and interpretability by focusing on structural agreement.


<details>
  <summary>Details</summary>
Motivation: Distance-based objectives (e.g., MSE) in SSL for signals are amplitude-sensitive, polarity-invariant, and unbounded, hindering semantic alignment and interpretability.

Method: SDSC quantifies structural agreement using signed amplitude intersection, derived from DSC. It can be used as a loss via a differentiable approximation and is combined with MSE for stability.

Result: SDSC-based pre-training matches or outperforms MSE, especially in in-domain and low-resource settings, enhancing semantic representation quality.

Conclusion: Structure-aware metrics like SDSC are viable alternatives to distance-based methods, improving signal representation fidelity.

Abstract: We propose the Signal Dice Similarity Coefficient (SDSC), a structure-aware
metric function for time series self-supervised representation learning. Most
Self-Supervised Learning (SSL) methods for signals commonly adopt
distance-based objectives such as mean squared error (MSE), which are sensitive
to amplitude, invariant to waveform polarity, and unbounded in scale. These
properties hinder semantic alignment and reduce interpretability. SDSC
addresses this by quantifying structural agreement between temporal signals
based on the intersection of signed amplitudes, derived from the Dice
Similarity Coefficient (DSC).Although SDSC is defined as a structure-aware
metric, it can be used as a loss by subtracting from 1 and applying a
differentiable approximation of the Heaviside function for gradient-based
optimization. A hybrid loss formulation is also proposed to combine SDSC with
MSE, improving stability and preserving amplitude where necessary. Experiments
on forecasting and classification benchmarks demonstrate that SDSC-based
pre-training achieves comparable or improved performance over MSE, particularly
in in-domain and low-resource scenarios. The results suggest that structural
fidelity in signal representations enhances the semantic representation
quality, supporting the consideration of structure-aware metrics as viable
alternatives to conventional distance-based methods.

</details>


### [318] [Positive-Unlabeled Learning for Control Group Construction in Observational Causal Inference](https://arxiv.org/abs/2507.14528)
*Ilias Tsoumas,Dimitrios Bormpoudakis,Vasileios Sitokonstantinou,Athanasios Askitopoulos,Andreas Kalogeras,Charalampos Kontoes,Ioannis Athanasiadis*

Main category: cs.LG

TL;DR: The paper proposes using positive-unlabeled (PU) learning to identify control units in observational studies where labeled controls are absent, enabling unbiased average treatment effect (ATE) estimation.


<details>
  <summary>Details</summary>
Motivation: In non-randomized settings, identifying control units is challenging, especially when only treated units are available. This hinders unbiased ATE estimation.

Method: PU learning is applied to identify control units from unlabeled data using treated units. The method is tested on simulated and real-world agricultural data.

Result: PU learning successfully identifies control units and estimates ATE close to the true value, validated through simulations and real-world applications.

Conclusion: PU learning enables reliable ATE estimation in observational studies, benefiting fields like agriculture and environmental sciences where randomized experiments are impractical.

Abstract: In causal inference, whether through randomized controlled trials or
observational studies, access to both treated and control units is essential
for estimating the effect of a treatment on an outcome of interest. When
treatment assignment is random, the average treatment effect (ATE) can be
estimated directly by comparing outcomes between groups. In non-randomized
settings, various techniques are employed to adjust for confounding and
approximate the counterfactual scenario to recover an unbiased ATE. A common
challenge, especially in observational studies, is the absence of units clearly
labeled as controls-that is, units known not to have received the treatment. To
address this, we propose positive-unlabeled (PU) learning as a framework for
identifying, with high confidence, control units from a pool of unlabeled ones,
using only the available treated (positive) units. We evaluate this approach
using both simulated and real-world data. We construct a causal graph with
diverse relationships and use it to generate synthetic data under various
scenarios, assessing how reliably the method recovers control groups that allow
estimates of true ATE. We also apply our approach to real-world data on optimal
sowing and fertilizer treatments in sustainable agriculture. Our findings show
that PU learning can successfully identify control (negative) units from
unlabeled data based only on treated units and, through the resulting control
group, estimate an ATE that closely approximates the true value. This work has
important implications for observational causal inference, especially in fields
where randomized experiments are difficult or costly. In domains such as earth,
environmental, and agricultural sciences, it enables a plethora of
quasi-experiments by leveraging available earth observation and climate data,
particularly when treated units are available but control units are lacking.

</details>


### [319] [Kernel Based Maximum Entropy Inverse Reinforcement Learning for Mean-Field Games](https://arxiv.org/abs/2507.14529)
*Berkay Anahtarci,Can Deha Kariksiz,Naci Saldi*

Main category: cs.LG

TL;DR: The paper introduces a maximum causal entropy inverse reinforcement learning method for infinite-horizon mean-field games, using a reproducing kernel Hilbert space for nonlinear reward inference.


<details>
  <summary>Details</summary>
Motivation: Existing inverse reinforcement learning approaches for mean-field games often limit reward functions to linear combinations of basis functions and focus on finite-horizon settings. This work aims to address these limitations.

Method: The authors propose a Lagrangian relaxation to reformulate the problem as unconstrained log-likelihood maximization, solved via gradient ascent. They prove the smoothness of the objective using Fréchet differentiability.

Result: The method effectively recovers expert behavior in a mean-field traffic routing game, demonstrating its practical utility.

Conclusion: The approach successfully extends inverse reinforcement learning to nonlinear rewards and infinite-horizon settings, with theoretical and empirical validation.

Abstract: We consider the maximum causal entropy inverse reinforcement learning problem
for infinite-horizon stationary mean-field games, in which we model the unknown
reward function within a reproducing kernel Hilbert space. This allows the
inference of rich and potentially nonlinear reward structures directly from
expert demonstrations, in contrast to most existing inverse reinforcement
learning approaches for mean-field games that typically restrict the reward
function to a linear combination of a fixed finite set of basis functions. We
also focus on the infinite-horizon cost structure, whereas prior studies
primarily rely on finite-horizon formulations. We introduce a Lagrangian
relaxation to this maximum causal entropy inverse reinforcement learning
problem that enables us to reformulate it as an unconstrained log-likelihood
maximization problem, and obtain a solution \lk{via} a gradient ascent
algorithm. To illustrate the theoretical consistency of the algorithm, we
establish the smoothness of the log-likelihood objective by proving the
Fr\'echet differentiability of the related soft Bellman operators with respect
to the parameters in the reproducing kernel Hilbert space. We demonstrate the
effectiveness of our method on a mean-field traffic routing game, where it
accurately recovers expert behavior.

</details>


### [320] [The Origin of Self-Attention: From Pairwise Affinity Matrices to Transformers](https://arxiv.org/abs/2507.14560)
*Giorgio Roffo*

Main category: cs.LG

TL;DR: The paper connects self-attention in Transformers to the broader concept of affinity-based computation, highlighting Infinite Feature Selection (Inf-FS) as a foundational approach. It shows self-attention as a special case of Inf-FS, unifying diverse machine learning models.


<details>
  <summary>Details</summary>
Motivation: To trace the conceptual origins of self-attention and situate it within the broader paradigm of affinity-based computation, revealing a common mathematical foundation across domains like computer vision, NLP, and graph learning.

Method: The paper compares self-attention with Inf-FS, analyzing how both use affinity matrices (A) for information flow. It contrasts the dynamic, single-hop affinity in self-attention with Inf-FS's multi-hop propagation and flexible A definition.

Result: Self-attention is shown as a special case of Inf-FS, with both relying on pairwise affinity matrices. The key difference lies in how A is defined and applied (dynamic vs. learned/domain-based).

Conclusion: The paper unifies self-attention and Inf-FS under affinity-based computation, emphasizing their shared mathematical foundation and the broader applicability of this principle across machine learning.

Abstract: The self-attention mechanism, now central to deep learning architectures such
as Transformers, is a modern instance of a more general computational
principle: learning and using pairwise affinity matrices to control how
information flows through a model. This paper traces the conceptual origins of
self-attention across multiple domains, including computer vision, natural
language processing, and graph learning, through their shared reliance on an
affinity matrix, denoted as A. We highlight Infinite Feature Selection (Inf-FS)
as a foundational approach that generalizes the idea of affinity-based
weighting. Unlike the fixed dot-product structure used in Transformers, Inf-FS
defines A either through domain knowledge or by learning, and computes feature
relevance through multi-hop propagation over the affinity graph. From this
perspective, self-attention can be seen as a special case of Inf-FS: it uses a
single-hop affinity computation where A is dynamically built from token
similarities. We argue that the underlying structure, reasoning over pairwise
relationships, is preserved across both approaches, and the key differences lie
in how the affinity matrix is defined and applied. By situating self-attention
within the broader paradigm of affinity-based computation, we unify several
strands of machine learning research and highlight a common mathematical
foundation that underpins diverse models and tasks.

</details>


### [321] [LPS-GNN : Deploying Graph Neural Networks on Graphs with 100-Billion Edges](https://arxiv.org/abs/2507.14570)
*Xu Cheng,Liang Yao,Feng He,Yukuo Cen,Yufei He,Chenhui Zhang,Wenzheng Feng,Hongyun Cai,Jie Tang*

Main category: cs.LG

TL;DR: LPS-GNN is a scalable, efficient GNN framework that handles 100B graphs with a single GPU in 10 hours, improving accuracy by 13.8% in User Acquisition. It introduces LPMetis for superior graph partitioning and subgraph augmentation for better performance.


<details>
  <summary>Details</summary>
Motivation: Existing GNNs struggle with efficiency and accuracy due to computational demands and memory issues in large-scale graphs.

Method: Proposes LPS-GNN with LPMetis for graph partitioning and subgraph augmentation for enhanced performance.

Result: Achieves 8.24% to 13.89% performance lift over SOTA models in real-world applications.

Conclusion: LPS-GNN is a scalable, efficient, and flexible solution for large-scale graph mining tasks.

Abstract: Graph Neural Networks (GNNs) have emerged as powerful tools for various graph
mining tasks, yet existing scalable solutions often struggle to balance
execution efficiency with prediction accuracy. These difficulties stem from
iterative message-passing techniques, which place significant computational
demands and require extensive GPU memory, particularly when dealing with the
neighbor explosion issue inherent in large-scale graphs. This paper introduces
a scalable, low-cost, flexible, and efficient GNN framework called LPS-GNN,
which can perform representation learning on 100 billion graphs with a single
GPU in 10 hours and shows a 13.8% improvement in User Acquisition scenarios. We
examine existing graph partitioning methods and design a superior graph
partition algorithm named LPMetis. In particular, LPMetis outperforms current
state-of-the-art (SOTA) approaches on various evaluation metrics. In addition,
our paper proposes a subgraph augmentation strategy to enhance the model's
predictive performance. It exhibits excellent compatibility, allowing the
entire framework to accommodate various GNN algorithms. Successfully deployed
on the Tencent platform, LPS-GNN has been tested on public and real-world
datasets, achieving performance lifts of 8. 24% to 13. 89% over SOTA models in
online applications.

</details>


### [322] [A Transformer-Based Conditional GAN with Multiple Instance Learning for UAV Signal Detection and Classification](https://arxiv.org/abs/2507.14592)
*Haochen Liu,Jia Bi,Xiaomin Wang,Xin Yang,Ling Wang*

Main category: cs.LG

TL;DR: A novel framework combining Transformer-based GAN and MILET for UAV flight state classification achieves high accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Accurate UAV flight state classification is critical for safe operations, but existing methods lack robustness and require large datasets.

Method: Integrates Transformer-based GAN for data augmentation and MILET for focusing on discriminative segments.

Result: Achieves 96.5% accuracy on DroneDetect and 98.6% on DroneRF, outperforming SOTA methods.

Conclusion: The framework is efficient, generalizable, and suitable for real-time deployment in resource-constrained environments.

Abstract: Unmanned Aerial Vehicles (UAVs) are increasingly used in surveillance,
logistics, agriculture, disaster management, and military operations. Accurate
detection and classification of UAV flight states, such as hovering, cruising,
ascending, or transitioning, which are essential for safe and effective
operations. However, conventional time series classification (TSC) methods
often lack robustness and generalization for dynamic UAV environments, while
state of the art(SOTA) models like Transformers and LSTM based architectures
typically require large datasets and entail high computational costs,
especially with high-dimensional data streams. This paper proposes a novel
framework that integrates a Transformer-based Generative Adversarial Network
(GAN) with Multiple Instance Locally Explainable Learning (MILET) to address
these challenges in UAV flight state classification. The Transformer encoder
captures long-range temporal dependencies and complex telemetry dynamics, while
the GAN module augments limited datasets with realistic synthetic samples. MIL
is incorporated to focus attention on the most discriminative input segments,
reducing noise and computational overhead. Experimental results show that the
proposed method achieves superior accuracy 96.5% on the DroneDetect dataset and
98.6% on the DroneRF dataset that outperforming other SOTA approaches. The
framework also demonstrates strong computational efficiency and robust
generalization across diverse UAV platforms and flight states, highlighting its
potential for real-time deployment in resource constrained environments.

</details>


### [323] [$k$-PCA for (non-squared) Euclidean Distances: Polynomial Time Approximation](https://arxiv.org/abs/2507.14631)
*Daniel Greenhut,Dan Feldman*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Given an integer $k\geq1$ and a set $P$ of $n$ points in $\REAL^d$, the
classic $k$-PCA (Principle Component Analysis) approximates the affine
\emph{$k$-subspace mean} of $P$, which is the $k$-dimensional affine linear
subspace that minimizes its sum of squared Euclidean distances
($\ell_{2,2}$-norm) over the points of $P$, i.e., the mean of these distances.
The \emph{$k$-subspace median} is the subspace that minimizes its sum of
(non-squared) Euclidean distances ($\ell_{2,1}$-mixed norm), i.e., their
median. The median subspace is usually more sparse and robust to noise/outliers
than the mean, but also much harder to approximate since, unlike the
$\ell_{z,z}$ (non-mixed) norms, it is non-convex for $k<d-1$.
  We provide the first polynomial-time deterministic algorithm whose both
running time and approximation factor are not exponential in $k$. More
precisely, the multiplicative approximation factor is $\sqrt{d}$, and the
running time is polynomial in the size of the input. We expect that our
technique would be useful for many other related problems, such as $\ell_{2,z}$
norm of distances for $z\not \in \br{1,2}$, e.g., $z=\infty$, and handling
outliers/sparsity.
  Open code and experimental results on real-world datasets are also provided.

</details>


### [324] [Rec-AD: An Efficient Computation Framework for FDIA Detection Based on Tensor Train Decomposition and Deep Learning Recommendation Model](https://arxiv.org/abs/2507.14668)
*Yunfeng Li,Junhong Liu,Zhaohui Yang,Guofu Liao,Chuyun Zhang*

Main category: cs.LG

TL;DR: Rec-AD improves FDIA detection efficiency in smart grids using Tensor Train decomposition and DLRM, enhancing computational throughput and real-time performance.


<details>
  <summary>Details</summary>
Motivation: Address computational and memory burdens in deep learning models for FDIA detection due to large-scale datasets.

Method: Integrates Tensor Train decomposition with DLRM, employing embedding compression, index reordering, and pipeline training.

Result: Significantly improves computational throughput, real-time detection, and reduces attack window.

Conclusion: Rec-AD strengthens smart grid security with scalable, efficient edge computing capabilities.

Abstract: Deep learning models have been widely adopted for False Data Injection Attack
(FDIA) detection in smart grids due to their ability to capture unstructured
and sparse features. However, the increasing system scale and data
dimensionality introduce significant computational and memory burdens,
particularly in large-scale industrial datasets, limiting detection efficiency.
To address these issues, this paper proposes Rec-AD, a computationally
efficient framework that integrates Tensor Train decomposition with the Deep
Learning Recommendation Model (DLRM). Rec-AD enhances training and inference
efficiency through embedding compression, optimized data access via index
reordering, and a pipeline training mechanism that reduces memory communication
overhead. Fully compatible with PyTorch, Rec-AD can be integrated into existing
FDIA detection systems without code modifications. Experimental results show
that Rec-AD significantly improves computational throughput and real-time
detection performance, narrowing the attack window and increasing attacker
cost. These advancements strengthen edge computing capabilities and
scalability, providing robust technical support for smart grid security.

</details>


### [325] [Revisiting Graph Contrastive Learning on Anomaly Detection: A Structural Imbalance Perspective](https://arxiv.org/abs/2507.14677)
*Yiming Xu,Zhen Peng,Bin Shi,Xu Hua,Bo Dong,Song Wang,Chen Chen*

Main category: cs.LG

TL;DR: AD-GCL is a novel GCL-based framework addressing structural imbalance in anomaly detection by filtering noisy edges and enhancing tail node detection.


<details>
  <summary>Details</summary>
Motivation: Existing GCL-based models neglect robustness to structural imbalance, especially for tail anomalies, limiting their real-world applicability.

Method: AD-GCL uses neighbor pruning for head nodes, anomaly-guided neighbor completion for tail nodes, and intra-/inter-view consistency loss for better representation.

Result: AD-GCL outperforms in detecting both head and tail anomalies across multiple datasets.

Conclusion: AD-GCL provides a robust solution for graph anomaly detection, addressing structural imbalance effectively.

Abstract: The superiority of graph contrastive learning (GCL) has prompted its
application to anomaly detection tasks for more powerful risk warning systems.
Unfortunately, existing GCL-based models tend to excessively prioritize overall
detection performance while neglecting robustness to structural imbalance,
which can be problematic for many real-world networks following power-law
degree distributions. Particularly, GCL-based methods may fail to capture tail
anomalies (abnormal nodes with low degrees). This raises concerns about the
security and robustness of current anomaly detection algorithms and therefore
hinders their applicability in a variety of realistic high-risk scenarios. To
the best of our knowledge, research on the robustness of graph anomaly
detection to structural imbalance has received little scrutiny. To address the
above issues, this paper presents a novel GCL-based framework named AD-GCL. It
devises the neighbor pruning strategy to filter noisy edges for head nodes and
facilitate the detection of genuine tail nodes by aligning from head nodes to
forged tail nodes. Moreover, AD-GCL actively explores potential neighbors to
enlarge the receptive field of tail nodes through anomaly-guided neighbor
completion. We further introduce intra- and inter-view consistency loss of the
original and augmentation graph for enhanced representation. The performance
evaluation of the whole, head, and tail nodes on multiple datasets validates
the comprehensive superiority of the proposed AD-GCL in detecting both head
anomalies and tail anomalies.

</details>


### [326] [GCC-Spam: Spam Detection via GAN, Contrastive Learning, and Character Similarity Networks](https://arxiv.org/abs/2507.14679)
*Zixin Xu,Zhijie Wang,Zhiyuan Pan*

Main category: cs.LG

TL;DR: GCC-Spam is a novel spam-text detection framework addressing adversarial strategies and data scarcity through character similarity networks, contrastive learning, and GAN-generated pseudo-spam samples, outperforming baselines.


<details>
  <summary>Details</summary>
Motivation: The rise of spam text poses risks like information leakage and social instability, requiring robust detection methods despite adversarial tactics and limited labeled data.

Method: GCC-Spam integrates: 1) a character similarity network for orthographic/phonetic features, 2) contrastive learning for better discriminability, and 3) GAN-generated pseudo-spam samples to tackle data scarcity.

Result: The model outperforms baselines, achieving higher detection rates with fewer labeled examples in real-world experiments.

Conclusion: GCC-Spam effectively addresses spam detection challenges, offering improved accuracy and robustness against adversarial attacks and data limitations.

Abstract: The exponential growth of spam text on the Internet necessitates robust
detection mechanisms to mitigate risks such as information leakage and social
instability. This work addresses two principal challenges: adversarial
strategies employed by spammers and the scarcity of labeled data. We propose a
novel spam-text detection framework GCC-Spam, which integrates three core
innovations. First, a character similarity network captures orthographic and
phonetic features to counter character-obfuscation attacks and furthermore
produces sentence embeddings for downstream classification. Second, contrastive
learning enhances discriminability by optimizing the latent-space distance
between spam and normal texts. Third, a Generative Adversarial Network (GAN)
generates realistic pseudo-spam samples to alleviate data scarcity while
improving model robustness and classification accuracy. Extensive experiments
on real-world datasets demonstrate that our model outperforms baseline
approaches, achieving higher detection rates with significantly fewer labeled
examples.

</details>


### [327] [Spatial-Temporal Transformer with Curriculum Learning for EEG-Based Emotion Recognition](https://arxiv.org/abs/2507.14698)
*Xuetao Lin,Tianhao Peng,Peihong Dai,Yu Liang,Wenjun Wu*

Main category: cs.LG

TL;DR: The paper proposes SST-CL, a framework combining spatial-temporal transformers and curriculum learning for EEG-based emotion recognition, addressing challenges in integrating neural patterns and adapting to emotional intensity variations.


<details>
  <summary>Details</summary>
Motivation: To overcome challenges in EEG-based emotion recognition, specifically integrating non-stationary spatial-temporal neural patterns and adapting to dynamic emotional intensity variations in real-world scenarios.

Method: SST-CL integrates spatial-temporal transformers (spatial encoder for inter-channel relationships, temporal encoder for multi-scale dependencies) with an intensity-aware curriculum learning strategy for dynamic sample scheduling.

Result: State-of-the-art performance on three benchmark datasets, validated by ablation studies confirming the necessity of the proposed components.

Conclusion: SST-CL effectively addresses key challenges in EEG-based emotion recognition, demonstrating superior performance and robustness across emotional intensity levels.

Abstract: EEG-based emotion recognition plays an important role in developing adaptive
brain-computer communication systems, yet faces two fundamental challenges in
practical implementations: (1) effective integration of non-stationary
spatial-temporal neural patterns, (2) robust adaptation to dynamic emotional
intensity variations in real-world scenarios. This paper proposes SST-CL, a
novel framework integrating spatial-temporal transformers with curriculum
learning. Our method introduces two core components: a spatial encoder that
models inter-channel relationships and a temporal encoder that captures
multi-scale dependencies through windowed attention mechanisms, enabling
simultaneous extraction of spatial correlations and temporal dynamics from EEG
signals. Complementing this architecture, an intensity-aware curriculum
learning strategy progressively guides training from high-intensity to
low-intensity emotional states through dynamic sample scheduling based on a
dual difficulty assessment. Comprehensive experiments on three benchmark
datasets demonstrate state-of-the-art performance across various emotional
intensity levels, with ablation studies confirming the necessity of both
architectural components and the curriculum learning mechanism.

</details>


### [328] [Fraud is Not Just Rarity: A Causal Prototype Attention Approach to Realistic Synthetic Oversampling](https://arxiv.org/abs/2507.14706)
*Claudio Giusti,Luca Guarnera,Mirko Casu,Sebastiano Battiato*

Main category: cs.LG

TL;DR: The paper introduces CPAC, an interpretable architecture for fraud detection, combining prototype-based attention with VAE-GAN to improve latent space structure and outperforming traditional oversamplers and generative models.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenges of class imbalance and subtle fraud patterns in credit card transactions, existing methods like GANs and VAEs often lead to overconfident classifiers and poor latent separation.

Method: Proposes CPAC, a prototype-based attention classifier integrated with VAE-GAN, to enhance latent space clustering and separation, compared against traditional oversamplers (e.g., SMOTE) and generative models.

Result: CPAC achieves superior performance (F1-score: 93.14%, recall: 90.18%) and better latent cluster separation compared to other methods.

Conclusion: Classifier-guided latent shaping with CPAC improves fraud detection performance and offers insights into representation learning, with code to be released upon submission.

Abstract: Detecting fraudulent credit card transactions remains a significant
challenge, due to the extreme class imbalance in real-world data and the often
subtle patterns that separate fraud from legitimate activity. Existing research
commonly attempts to address this by generating synthetic samples for the
minority class using approaches such as GANs, VAEs, or hybrid generative
models. However, these techniques, particularly when applied only to
minority-class data, tend to result in overconfident classifiers and poor
latent cluster separation, ultimately limiting real-world detection
performance. In this study, we propose the Causal Prototype Attention
Classifier (CPAC), an interpretable architecture that promotes class-aware
clustering and improved latent space structure through prototype-based
attention mechanisms and we will couple it with the encoder in a VAE-GAN
allowing it to offer a better cluster separation moving beyond post-hoc sample
augmentation. We compared CPAC-augmented models to traditional oversamplers,
such as SMOTE, as well as to state-of-the-art generative models, both with and
without CPAC-based latent classifiers. Our results show that classifier-guided
latent shaping with CPAC delivers superior performance, achieving an F1-score
of 93.14\% percent and recall of 90.18\%, along with improved latent cluster
separation. Further ablation studies and visualizations provide deeper insight
into the benefits and limitations of classifier-driven representation learning
for fraud detection. The codebase for this work will be available at final
submission.

</details>


### [329] [Exploring the Dynamic Scheduling Space of Real-Time Generative AI Applications on Emerging Heterogeneous Systems](https://arxiv.org/abs/2507.14715)
*Rachid Karami,Rajeev Patwari,Hyoukjun Kwon,Ashish Sirasao*

Main category: cs.LG

TL;DR: The paper explores real-time generative AI (RTGen) workloads on heterogeneous SoCs, analyzing scheduling policies' impact on performance metrics like deadline violations and LLM efficiency.


<details>
  <summary>Details</summary>
Motivation: To address the underexplored scheduling complexity and performance implications of RTGen workloads on heterogeneous SoCs like AMD's Ryzen AI.

Method: Comprehensive characterization of RTGen workloads, profiling model performance across backends, and evaluating five scheduling policies.

Result: Scheduling decisions significantly impact performance (e.g., 41.7% difference in deadline violation rates), emphasizing the need for dynamic, workload-aware strategies.

Conclusion: Workload-aware, dynamic heterogeneous scheduling is crucial for high-performance, on-device RTGen applications.

Abstract: The integration of generative AI models, particularly large language models
(LLMs), into real-time multi-model AI applications such as video conferencing
and gaming is giving rise to a new class of workloads: real-time generative AI
(RTGen). These workloads combine the compute intensity and dynamic execution
patterns of generative models with the stringent latency and concurrency
constraints of real-time inference. To meet the diverse demands of RTGen
workloads, modern edge platforms increasingly adopt heterogeneous
system-on-chip (SoC) architectures that integrate CPUs, GPUs, and NPUs. Despite
the potential of heterogeneous SoC, the scheduling space complexity and
performance implications of RTGen workloads on such platforms remain
underexplored. In this work, we perform a comprehensive characterization of
RTGen workloads on AMD's latest heterogeneous SoC, Ryzen AI. We construct
realistic multi-model scenarios inspired by industry use cases and profile
model performance across all available backends. Using this data, we evaluate
five scheduling policies and their impact on both real-time metrics (e.g.,
deadline violation rate) and LLM performance (e.g., time-to-first-token and
tokens-per-second). Our results show that scheduling decisions significantly
affect workload performance (e.g., leading to a 41.7% difference in deadline
violation rates on average), and highlight the need for scheduling strategies
that are aware of workload dynamics and hardware heterogeneity. Our findings
underscore the importance of workload-aware, dynamic heterogeneous scheduling
in enabling high-performance, on-device RTGen applications.

</details>


### [330] [LeanTree: Accelerating White-Box Proof Search with Factorized States in Lean 4](https://arxiv.org/abs/2507.14722)
*Matěj Kripner,Michal Šustr,Milan Straka*

Main category: cs.LG

TL;DR: LeanTree introduces a white-box tool for ATP using LLMs, outperforming black-box methods by leveraging factorized proof states and richer training data.


<details>
  <summary>Details</summary>
Motivation: Addressing the gap in white-box ATP methods, which lag behind black-box approaches despite potential advantages like incremental proof construction.

Method: Develops LeanTree, a Lean 4-based tool that factorizes complex proofs into simpler branches and provides a dataset of these states.

Result: Preliminary results suggest white-box approaches may outperform black-box methods in certain scenarios.

Conclusion: LeanTree demonstrates the potential of white-box ATP tools to enhance correctness, efficiency, and training data quality.

Abstract: Automated theorem proving (ATP) has been a classical problem in artificial
intelligence since its inception, yet it remains challenging due to its vast
state and action space. Large language models (LLMs) have recently emerged as a
promising heuristic for ATP, but they lack correctness guarantees and thus
require interaction with a proof verifier. Such interactions typically follow
one of two approaches: black-box interaction, which does not utilize
intermediate proof states, or white-box approaches, which allow for incremental
proof construction and examination of intermediate states. While black-box
approaches have directly benefited from recent LLM advances, white-box methods
have comparatively lagged behind. In this paper, we address this gap by
introducing LeanTree, which consists of (i) a tool built in the Lean 4 language
that factorizes complex proof states into simpler, independent branches, and
(ii) a dataset of these factorized intermediate states. Our white-box tooling
offers several advantages over black-box approaches: it simplifies evaluation,
reduces necessary context, generates richer training data, enables parallel
search across multiple states, supports efficient reuse of states, and provides
feedback in case of errors. Our preliminary results hint that white-box
approaches outperform black-box alternatives in some settings.

</details>


### [331] [Task-Agnostic Continual Prompt Tuning with Gradient-Based Selection and Decoding](https://arxiv.org/abs/2507.14725)
*Anushka Tiwari,Sayantan Pal,Rohini K. Srihari,Kaiyi Ji*

Main category: cs.LG

TL;DR: GRID is a unified framework for prompt-based continual learning, addressing latent forgetting and prompt memory explosion with task-aware decoding and gradient-based prompt selection.


<details>
  <summary>Details</summary>
Motivation: Existing prompt-based CL methods assume task-aware inference and maintain growing task-specific prompts, limiting scalability and hiding latent forgetting.

Method: GRID integrates task-aware decoding (leveraging representative inputs, automatic task identification, constrained decoding) and gradient-based prompt selection to compress less informative prompts.

Result: GRID improves backward transfer, achieves competitive forward transfer, and reduces forgotten tasks by up to 80%, outperforming state-of-the-art methods.

Conclusion: GRID effectively addresses scalability and forgetting in prompt-based CL, demonstrating superior performance on T5 and Flan-T5 backbones.

Abstract: Prompt-based continual learning (CL) offers a parameter-efficient way to
adapt large language models (LLMs) across task sequences. However, most
existing methods assume task-aware inference and maintain a growing list of
task-specific prompts, which limits scalability and hides latent forgetting. In
this work, we introduce GRID, a unified framework that addresses two key
limitations: (1) latent forgetting under task-agnostic inference, and (2)
prompt memory explosion as task sequences grow. GRID integrates a task-aware
decoding mechanism that improves backward transfer by leveraging representative
inputs, automatic task identification, and constrained decoding. Additionally,
we propose a gradient-based prompt selection strategy that compresses less
informative prompts into a single aggregated representation, enabling scalable
and memory-efficient lifelong learning. Extensive experiments across
short-sequence, long-sequence, and negative transfer benchmarks show that GRID
significantly improves backward transfer, achieves competitive forward
transfer, and reduces forgotten tasks by up to 80\%, outperforming
state-of-the-art methods on T5 and Flan-T5 backbones.

</details>


### [332] [Balancing Expressivity and Robustness: Constrained Rational Activations for Reinforcement Learning](https://arxiv.org/abs/2507.14736)
*Rafał Surdej,Michał Bortkiewicz,Alex Lewandowski,Mateusz Ostaszewski,Clare Lyle*

Main category: cs.LG

TL;DR: Trainable rational activation functions enhance adaptability but can cause instability in RL and continual learning. A constrained variant improves stability and performance.


<details>
  <summary>Details</summary>
Motivation: To study the impact of trainable rational activation functions on training stability in reinforcement and continual learning settings.

Method: Propose a constrained variant of trainable rational activations to limit excessive output scaling while preserving adaptability. Tested in MetaWorld, DMC, and continual learning benchmarks.

Result: The constrained variant improves stability and performance in RL and continual learning, revealing a trade-off between expressivity and plasticity.

Conclusion: Design principles for robust trainable activations in dynamic environments are provided, with code available.

Abstract: Trainable activation functions, whose parameters are optimized alongside
network weights, offer increased expressivity compared to fixed activation
functions. Specifically, trainable activation functions defined as ratios of
polynomials (rational functions) have been proposed to enhance plasticity in
reinforcement learning. However, their impact on training stability remains
unclear. In this work, we study trainable rational activations in both
reinforcement and continual learning settings. We find that while their
flexibility enhances adaptability, it can also introduce instability, leading
to overestimation in RL and feature collapse in longer continual learning
scenarios. Our main result is demonstrating a trade-off between expressivity
and plasticity in rational activations. To address this, we propose a
constrained variant that structurally limits excessive output scaling while
preserving adaptability. Experiments across MetaWorld and DeepMind Control
Suite (DMC) environments show that our approach improves training stability and
performance. In continual learning benchmarks, including MNIST with reshuffled
labels and Split CIFAR-100, we reveal how different constraints affect the
balance between expressivity and long-term retention. While preliminary
experiments in discrete action domains (e.g., Atari) did not show similar
instability, this suggests that the trade-off is particularly relevant for
continuous control. Together, our findings provide actionable design principles
for robust and adaptable trainable activations in dynamic, non-stationary
environments. Code available at:
https://github.com/special114/rl_rational_plasticity.

</details>


### [333] [Better Training Data Attribution via Better Inverse Hessian-Vector Products](https://arxiv.org/abs/2507.14740)
*Andrew Wang,Elisa Nguyen,Runshi Yang,Juhan Bae,Sheila A. McIlraith,Roger Grosse*

Main category: cs.LG

TL;DR: ASTRA improves TDA by using EKFAC-preconditioned Neumann series for accurate iHVP approximation, enhancing performance.


<details>
  <summary>Details</summary>
Motivation: Efficiently approximating inverse Hessian-vector products (iHVP) is challenging in gradient-based TDA methods.

Method: ASTRA algorithm combines EKFAC-preconditioner with Neumann series iterations for better iHVP approximation.

Result: ASTRA is more accurate, requires fewer iterations, and improves TDA performance.

Conclusion: Accurate iHVP approximation via ASTRA significantly enhances TDA effectiveness.

Abstract: Training data attribution (TDA) provides insights into which training data is
responsible for a learned model behavior. Gradient-based TDA methods such as
influence functions and unrolled differentiation both involve a computation
that resembles an inverse Hessian-vector product (iHVP), which is difficult to
approximate efficiently. We introduce an algorithm (ASTRA) which uses the
EKFAC-preconditioner on Neumann series iterations to arrive at an accurate iHVP
approximation for TDA. ASTRA is easy to tune, requires fewer iterations than
Neumann series iterations, and is more accurate than EKFAC-based
approximations. Using ASTRA, we show that improving the accuracy of the iHVP
approximation can significantly improve TDA performance.

</details>


### [334] [Beyond the Single-Best Model: Rashomon Partial Dependence Profile for Trustworthy Explanations in AutoML](https://arxiv.org/abs/2507.14744)
*Mustafa Cavus,Jan N. van Rijn,Przemysław Biecek*

Main category: cs.LG

TL;DR: The paper proposes a framework to incorporate model multiplicity into explanation generation by aggregating partial dependence profiles (PDP) from near-optimal models, addressing uncertainty in explainable AI.


<details>
  <summary>Details</summary>
Motivation: Current automated machine learning systems focus on single best-performing models, neglecting explanation uncertainty, which is crucial for human-centered explainable AI.

Method: The framework aggregates PDPs from the Rashomon set (near-optimal models) to create Rashomon PDP, capturing interpretive variability and disagreement.

Result: Experiments on 35 regression datasets show Rashomon PDP often covers less than 70% of the best model's PDP, highlighting limitations of single-model explanations.

Conclusion: Rashomon PDP enhances reliability and trustworthiness of model interpretations, especially in high-stakes domains requiring transparency and confidence.

Abstract: Automated machine learning systems efficiently streamline model selection but
often focus on a single best-performing model, overlooking explanation
uncertainty, an essential concern in human centered explainable AI. To address
this, we propose a novel framework that incorporates model multiplicity into
explanation generation by aggregating partial dependence profiles (PDP) from a
set of near optimal models, known as the Rashomon set. The resulting Rashomon
PDP captures interpretive variability and highlights areas of disagreement,
providing users with a richer, uncertainty aware view of feature effects. To
evaluate its usefulness, we introduce two quantitative metrics, the coverage
rate and the mean width of confidence intervals, to evaluate the consistency
between the standard PDP and the proposed Rashomon PDP. Experiments on 35
regression datasets from the OpenML CTR23 benchmark suite show that in most
cases, the Rashomon PDP covers less than 70% of the best model's PDP,
underscoring the limitations of single model explanations. Our findings suggest
that Rashomon PDP improves the reliability and trustworthiness of model
interpretations by adding additional information that would otherwise be
neglected. This is particularly useful in high stakes domains where
transparency and confidence are critical.

</details>


### [335] [Sampling from Gaussian Processes: A Tutorial and Applications in Global Sensitivity Analysis and Optimization](https://arxiv.org/abs/2507.14746)
*Bach Do,Nafeezat A. Ajenifuja,Taiwo A. Adebiyi,Ruda Zhang*

Main category: cs.LG

TL;DR: The paper introduces two sampling methods (random Fourier features and pathwise conditioning) for Gaussian processes (GPs) to address high costs in engineering tasks like global sensitivity analysis and optimization. It demonstrates their application through numerical examples.


<details>
  <summary>Details</summary>
Motivation: High costs of simulations and experiments limit their use in global sensitivity analysis and optimization, motivating the use of GPs as proxy models for uncertainty-aware predictions.

Method: The paper formulates and implements two sampling methods (random Fourier features and pathwise conditioning) for GPs, with alternative approaches briefly described.

Result: The sampling methods are successfully applied in global sensitivity analysis, single-objective optimization, and multi-objective optimization, demonstrated through numerical examples.

Conclusion: The presented sampling methods for GPs effectively support engineering tasks like sensitivity analysis and optimization, offering a practical solution to high-cost limitations.

Abstract: High-fidelity simulations and physical experiments are essential for
engineering analysis and design. However, their high cost often limits their
applications in two critical tasks: global sensitivity analysis (GSA) and
optimization. This limitation motivates the common use of Gaussian processes
(GPs) as proxy regression models to provide uncertainty-aware predictions based
on a limited number of high-quality observations. GPs naturally enable
efficient sampling strategies that support informed decision-making under
uncertainty by extracting information from a subset of possible functions for
the model of interest. Despite their popularity in machine learning and
statistics communities, sampling from GPs has received little attention in the
community of engineering optimization. In this paper, we present the
formulation and detailed implementation of two notable sampling methods --
random Fourier features and pathwise conditioning -- for generating posterior
samples from GPs. Alternative approaches are briefly described. Importantly, we
detail how the generated samples can be applied in GSA, single-objective
optimization, and multi-objective optimization. We show successful applications
of these sampling methods through a series of numerical examples.

</details>


### [336] [Pruning Increases Orderedness in Recurrent Computation](https://arxiv.org/abs/2507.14747)
*Yiding Song*

Main category: cs.LG

TL;DR: The paper explores whether directionality is a necessary inductive bias for neural networks, showing it can be induced via pruning without performance loss.


<details>
  <summary>Details</summary>
Motivation: Inspired by recurrent circuits in biological brains, the study questions if directionality must be hard-wired in artificial networks.

Method: A perceptron layer with all-to-all connections (like a weight-tied RNN) is pruned to induce directionality.

Result: Pruning successfully induces topological ordering in information flow without compromising performance.

Conclusion: Directionality is not essential for learning but can be a beneficial inductive bias discovered through pruning and gradient descent.

Abstract: Inspired by the prevalence of recurrent circuits in biological brains, we
investigate the degree to which directionality is a helpful inductive bias for
artificial neural networks. Taking directionality as topologically-ordered
information flow between neurons, we formalise a perceptron layer with
all-to-all connections (mathematically equivalent to a weight-tied recurrent
neural network) and demonstrate that directionality, a hallmark of modern
feed-forward networks, can be induced rather than hard-wired by applying
appropriate pruning techniques. Across different random seeds our pruning
schemes successfully induce greater topological ordering in information flow
between neurons without compromising performance, suggesting that
directionality is not a prerequisite for learning, but may be an advantageous
inductive bias discoverable by gradient descent and sparsification.

</details>


### [337] [Skill Learning via Policy Diversity Yields Identifiable Representations for Reinforcement Learning](https://arxiv.org/abs/2507.14748)
*Patrik Reizinger,Bálint Mucsányi,Siyuan Guo,Benjamin Eysenbach,Bernhard Schölkopf,Wieland Brendel*

Main category: cs.LG

TL;DR: The paper analyzes Contrastive Successor Features (CSF) in self-supervised RL, proving it can recover ground-truth features up to a linear transformation due to skill diversity and inner product parametrization.


<details>
  <summary>Details</summary>
Motivation: To understand the theoretical role of representation and mutual information in mutual information skill learning (MISL) methods like CSF.

Method: Focuses on CSF, proving its identifiability guarantees for recovering ground-truth features. Analyzes skill diversity and inner product parametrization.

Result: CSF provably recovers ground-truth features up to a linear transformation, validated in MuJoCo and DeepMind Control.

Conclusion: The work provides the first identifiability guarantee for RL representation learning, clarifying the role of mutual information objectives and entropy regularizers.

Abstract: Self-supervised feature learning and pretraining methods in reinforcement
learning (RL) often rely on information-theoretic principles, termed mutual
information skill learning (MISL). These methods aim to learn a representation
of the environment while also incentivizing exploration thereof. However, the
role of the representation and mutual information parametrization in MISL is
not yet well understood theoretically. Our work investigates MISL through the
lens of identifiable representation learning by focusing on the Contrastive
Successor Features (CSF) method. We prove that CSF can provably recover the
environment's ground-truth features up to a linear transformation due to the
inner product parametrization of the features and skill diversity in a
discriminative sense. This first identifiability guarantee for representation
learning in RL also helps explain the implications of different mutual
information objectives and the downsides of entropy regularizers. We
empirically validate our claims in MuJoCo and DeepMind Control and show how CSF
provably recovers the ground-truth features both from states and pixels.

</details>


### [338] [CXR-TFT: Multi-Modal Temporal Fusion Transformer for Predicting Chest X-ray Trajectories](https://arxiv.org/abs/2507.14766)
*Mehak Arora,Ayman Ali,Kaiyuan Wu,Carolyn Davis,Takashi Shimazui,Mahmoud Alwakeel,Victor Moas,Philip Yang,Annette Esper,Rishikesan Kamaleswaran*

Main category: cs.LG

TL;DR: CXR-TFT is a multi-modal framework integrating sparse CXR data and high-frequency clinical metrics to predict abnormal CXR findings in ICU patients up to 12 hours early.


<details>
  <summary>Details</summary>
Motivation: Existing CXR tools lack temporal analysis, limiting their utility in dynamic ICU settings where early intervention is critical.

Method: CXR-TFT combines CXR imaging, radiology reports, and clinical data (vitals, labs, etc.) using a vision encoder and transformer model to predict future CXR findings.

Result: The framework accurately predicted abnormal CXR findings 12 hours in advance in a study of 20,000 ICU patients.

Conclusion: CXR-TFT enhances ICU patient management by providing early, actionable insights for time-sensitive conditions like acute respiratory distress syndrome.

Abstract: In intensive care units (ICUs), patients with complex clinical conditions
require vigilant monitoring and prompt interventions. Chest X-rays (CXRs) are a
vital diagnostic tool, providing insights into clinical trajectories, but their
irregular acquisition limits their utility. Existing tools for CXR
interpretation are constrained by cross-sectional analysis, failing to capture
temporal dynamics. To address this, we introduce CXR-TFT, a novel multi-modal
framework that integrates temporally sparse CXR imaging and radiology reports
with high-frequency clinical data, such as vital signs, laboratory values, and
respiratory flow sheets, to predict the trajectory of CXR findings in
critically ill patients. CXR-TFT leverages latent embeddings from a vision
encoder that are temporally aligned with hourly clinical data through
interpolation. A transformer model is then trained to predict CXR embeddings at
each hour, conditioned on previous embeddings and clinical measurements. In a
retrospective study of 20,000 ICU patients, CXR-TFT demonstrated high accuracy
in forecasting abnormal CXR findings up to 12 hours before they became
radiographically evident. This predictive capability in clinical data holds
significant potential for enhancing the management of time-sensitive conditions
like acute respiratory distress syndrome, where early intervention is crucial
and diagnoses are often delayed. By providing distinctive temporal resolution
in prognostic CXR analysis, CXR-TFT offers actionable 'whole patient' insights
that can directly improve clinical outcomes.

</details>


### [339] [Rethinking Memorization Measures and their Implications in Large Language Models](https://arxiv.org/abs/2507.14777)
*Bishwamittra Ghosh,Soumi Das,Qinyuan Wu,Mohammad Aflah Khan,Krishna P. Gummadi,Evimaria Terzi,Deepak Garg*

Main category: cs.LG

TL;DR: The paper investigates whether memorization in LLMs can be avoided during optimal language learning and evaluates the actual privacy threat posed by memorization. It introduces contextual memorization to distinguish memorization from contextual learning and compares it with existing measures. Experiments show memorization measures disagree, optimal learning cannot avoid memorization, and improved learning affects memorization types differently.


<details>
  <summary>Details</summary>
Motivation: To determine if memorization is unavoidable in optimal language learning and assess if privacy threats from memorization are overstated.

Method: Re-examines existing memorization measures (recollection-based, counterfactual) and introduces contextual memorization. Experiments on 18 LLMs and multiple formal languages.

Result: Memorization measures disagree; optimal learning cannot avoid memorization; improved learning reduces contextual/counterfactual memorization but increases recollection-based memorization.

Conclusion: Memorization is unavoidable in optimal learning, and privacy threats from memorization may be exaggerated, as some reported memorized strings do not pose actual threats.

Abstract: Concerned with privacy threats, memorization in LLMs is often seen as
undesirable, specifically for learning. In this paper, we study whether
memorization can be avoided when optimally learning a language, and whether the
privacy threat posed by memorization is exaggerated or not. To this end, we
re-examine existing privacy-focused measures of memorization, namely
recollection-based and counterfactual memorization, along with a newly proposed
contextual memorization.
  Relating memorization to local over-fitting during learning, contextual
memorization aims to disentangle memorization from the contextual learning
ability of LLMs. Informally, a string is contextually memorized if its
recollection due to training exceeds the optimal contextual recollection, a
learned threshold denoting the best contextual learning without training.
Conceptually, contextual recollection avoids the fallacy of recollection-based
memorization, where any form of high recollection is a sign of memorization.
Theoretically, contextual memorization relates to counterfactual memorization,
but imposes stronger conditions. Memorization measures differ in outcomes and
information requirements.
  Experimenting on 18 LLMs from 6 families and multiple formal languages of
different entropy, we show that (a) memorization measures disagree on
memorization order of varying frequent strings, (b) optimal learning of a
language cannot avoid partial memorization of training strings, and (c)
improved learning decreases contextual and counterfactual memorization but
increases recollection-based memorization. Finally, (d) we revisit existing
reports of memorized strings by recollection that neither pose a privacy threat
nor are contextually or counterfactually memorized.

</details>


### [340] [Omni-Think: Scaling Cross-Domain Generalization in LLMs via Multi-Task RL with Hybrid Rewards](https://arxiv.org/abs/2507.14783)
*Derek Li,Jiaming Zhou,Amirreza Kazemi,Qianyi Sun,Abbas Ghaddar,Mohammad Ali Alomrani,Liheng Ma,Yu Luo,Dong Li,Feng Wen,Jianye Hao,Mark Coates,Yingxue Zhang*

Main category: cs.LG

TL;DR: Omni-Think is a unified RL framework for LLMs, combining rule-based rewards and LLM-as-a-Judge evaluations to enhance generalization. Curriculum learning improves performance by 5.2% over joint training.


<details>
  <summary>Details</summary>
Motivation: Post-training methods like SFT often struggle with generalization, favoring memorization. Omni-Think aims to improve transferable learning across diverse tasks.

Method: Omni-Think uses a unified RL framework with rule-based rewards and generative preference signals via LLM-as-a-Judge. It employs curriculum-based task ordering.

Result: Curriculum learning improves performance by 5.2% over joint training and 9.1% over model merging across four domains.

Conclusion: Task-aware sampling and hybrid supervision are key for scaling RL-based post-training in general-purpose LLMs.

Abstract: The advancement of general-purpose artificial intelligence relies on large
language models (LLMs) that excel across a wide range of tasks, from structured
reasoning to creative generation. However, post-training methods like
Supervised Fine-Tuning (SFT) often struggle with generalization, favoring
memorization over transferable learning. In this work, we introduce Omni-Think,
a unified reinforcement learning (RL) framework that enhances LLM performance
across diverse tasks by combining rule-based verifiable rewards with generative
preference signals via LLM-as-a-Judge evaluations. Our approach enables
consistent optimization across task types and scales RL-based training to
subjective domains. We further investigate training strategies, demonstrating
that a curriculum-based progression that orders tasks from structured to
open-ended improves performance and reduces forgetting. Experimental results
across four domains reveal that curriculum learning improves performance by
5.2\% over joint training and 9.1\% over model merging. These results highlight
the importance of task-aware sampling and hybrid supervision in scaling
RL-based post-training for general-purpose LLMs.

</details>


### [341] [Exploring the In-Context Learning Capabilities of LLMs for Money Laundering Detection in Financial Graphs](https://arxiv.org/abs/2507.14785)
*Erfan Pirmorad*

Main category: cs.LG

TL;DR: LLMs are used for reasoning over financial graph data to detect money laundering, showing promise for explainable analytics.


<details>
  <summary>Details</summary>
Motivation: Address the complexity of money laundering by leveraging LLMs for graph-based reasoning.

Method: A pipeline retrieves k-hop subgraphs, serializes them into text, and uses LLMs for few-shot learning to assess suspiciousness.

Result: LLMs emulate analyst logic, flag suspicious activity, and provide explanations in synthetic AML scenarios.

Conclusion: LLM-based graph reasoning has potential for explainable financial crime analytics.

Abstract: The complexity and interconnectivity of entities involved in money laundering
demand investigative reasoning over graph-structured data. This paper explores
the use of large language models (LLMs) as reasoning engines over localized
subgraphs extracted from a financial knowledge graph. We propose a lightweight
pipeline that retrieves k-hop neighborhoods around entities of interest,
serializes them into structured text, and prompts an LLM via few-shot
in-context learning to assess suspiciousness and generate justifications. Using
synthetic anti-money laundering (AML) scenarios that reflect common laundering
behaviors, we show that LLMs can emulate analyst-style logic, highlight red
flags, and provide coherent explanations. While this study is exploratory, it
illustrates the potential of LLM-based graph reasoning in AML and lays
groundwork for explainable, language-driven financial crime analytics.

</details>


### [342] [Flow Equivariant Recurrent Neural Networks](https://arxiv.org/abs/2507.14793)
*T. Anderson Keller*

Main category: cs.LG

TL;DR: The paper extends equivariant network theory to time-parameterized transformations (flows) in sequence models, showing improved performance over non-equivariant models.


<details>
  <summary>Details</summary>
Motivation: Current equivariant networks are limited to static transformations, ignoring continuous symmetries in time-parameterized data like sequences.

Method: The authors generalize equivariance to flows (time-parameterized transformations), propose equivariant RNNs, and validate them experimentally.

Result: Equivariant RNNs outperform non-equivariant models in training speed, length generalization, and velocity generalization.

Conclusion: This work advances sequence models by incorporating time-parameterized symmetries, with promising results for real-world applications.

Abstract: Data arrives at our senses as a continuous stream, smoothly transforming from
one instant to the next. These smooth transformations can be viewed as
continuous symmetries of the environment that we inhabit, defining equivalence
relations between stimuli over time. In machine learning, neural network
architectures that respect symmetries of their data are called equivariant and
have provable benefits in terms of generalization ability and sample
efficiency. To date, however, equivariance has been considered only for static
transformations and feed-forward networks, limiting its applicability to
sequence models, such as recurrent neural networks (RNNs), and corresponding
time-parameterized sequence transformations. In this work, we extend
equivariant network theory to this regime of `flows' -- one-parameter Lie
subgroups capturing natural transformations over time, such as visual motion.
We begin by showing that standard RNNs are generally not flow equivariant:
their hidden states fail to transform in a geometrically structured manner for
moving stimuli. We then show how flow equivariance can be introduced, and
demonstrate that these models significantly outperform their non-equivariant
counterparts in terms of training speed, length generalization, and velocity
generalization, on both next step prediction and sequence classification. We
present this work as a first step towards building sequence models that respect
the time-parameterized symmetries which govern the world around us.

</details>


### [343] [Subliminal Learning: Language models transmit behavioral traits via hidden signals in data](https://arxiv.org/abs/2507.14805)
*Alex Cloud,Minh Le,James Chua,Jan Betley,Anna Sztyber-Betley,Jacob Hilton,Samuel Marks,Owain Evans*

Main category: cs.LG

TL;DR: Language models can transmit behavioral traits (T) through semantically unrelated data (e.g., number sequences), even when filtered, a phenomenon called subliminal learning. This occurs across models of the same base type but not different ones, posing risks for AI development.


<details>
  <summary>Details</summary>
Motivation: To investigate how language models can unintentionally transmit behavioral traits through seemingly unrelated data, revealing potential pitfalls in AI development.

Method: Experiments with 'teacher' models generating datasets (number sequences, code, reasoning traces) containing trait T, and 'student' models trained on this data to observe subliminal learning. Theoretical proof and demonstration in a simple MLP classifier.

Result: Subliminal learning occurs when teacher and student share the same base model, even with filtered data. It does not occur with different base models.

Conclusion: Subliminal learning is a general phenomenon, highlighting risks in AI development, such as unintended trait propagation despite data filtering.

Abstract: We study subliminal learning, a surprising phenomenon where language models
transmit behavioral traits via semantically unrelated data. In our main
experiments, a "teacher" model with some trait T (such as liking owls or being
misaligned) generates a dataset consisting solely of number sequences.
Remarkably, a "student" model trained on this dataset learns T. This occurs
even when the data is filtered to remove references to T. We observe the same
effect when training on code or reasoning traces generated by the same teacher
model. However, we do not observe the effect when the teacher and student have
different base models. To help explain our findings, we prove a theoretical
result showing that subliminal learning occurs in all neural networks under
certain conditions, and demonstrate subliminal learning in a simple MLP
classifier. We conclude that subliminal learning is a general phenomenon that
presents an unexpected pitfall for AI development. Distillation could propagate
unintended traits, even when developers try to prevent this via data filtering.

</details>


### [344] [Benchmarking Foundation Models with Multimodal Public Electronic Health Records](https://arxiv.org/abs/2507.14824)
*Kunyu Yu,Rui Yang,Jingchi Liao,Siqi Li,Huitao Li,Irene Li,Yifan Peng,Rishikesan Kamaleswaran,Nan Liu*

Main category: cs.LG

TL;DR: A benchmark evaluates foundation models for EHRs, showing multimodal models improve performance without bias, using MIMIC-IV data.


<details>
  <summary>Details</summary>
Motivation: To assess the performance, fairness, and interpretability of foundation models in processing diverse EHR data for clinical AI applications.

Method: Developed a standardized pipeline for MIMIC-IV data, compared eight foundation models (unimodal/multimodal, domain-specific/general-purpose).

Result: Multimodal models consistently outperformed unimodal ones in predictive tasks without increasing bias.

Conclusion: The benchmark supports developing trustworthy multimodal AI for clinical use, with code publicly available.

Abstract: Foundation models have emerged as a powerful approach for processing
electronic health records (EHRs), offering flexibility to handle diverse
medical data modalities. In this study, we present a comprehensive benchmark
that evaluates the performance, fairness, and interpretability of foundation
models, both as unimodal encoders and as multimodal learners, using the
publicly available MIMIC-IV database. To support consistent and reproducible
evaluation, we developed a standardized data processing pipeline that
harmonizes heterogeneous clinical records into an analysis-ready format. We
systematically compared eight foundation models, encompassing both unimodal and
multimodal models, as well as domain-specific and general-purpose variants. Our
findings demonstrate that incorporating multiple data modalities leads to
consistent improvements in predictive performance without introducing
additional bias. Through this benchmark, we aim to support the development of
effective and trustworthy multimodal artificial intelligence (AI) systems for
real-world clinical applications. Our code is available at
https://github.com/nliulab/MIMIC-Multimodal.

</details>


### [345] [eMargin: Revisiting Contrastive Learning with Margin-Based Separation](https://arxiv.org/abs/2507.14828)
*Abdul-Kazeem Shamba,Kerstin Bach,Gavin Taylor*

Main category: cs.LG

TL;DR: The paper investigates the effect of an adaptive margin (eMargin) in contrastive loss for time series learning, finding it improves clustering but not downstream classification.


<details>
  <summary>Details</summary>
Motivation: To explore if an adaptive margin in contrastive loss can enhance separation of dissimilar time steps and improve downstream task performance.

Method: Introduces eMargin, adjusted by a similarity threshold, into contrastive loss (InfoNCE) and evaluates on clustering and classification tasks.

Result: eMargin improves unsupervised clustering metrics but fails to enhance downstream classification performance.

Conclusion: High clustering scores do not guarantee meaningful embeddings for downstream tasks; eMargin excels in clustering but not classification.

Abstract: We revisit previous contrastive learning frameworks to investigate the effect
of introducing an adaptive margin into the contrastive loss function for time
series representation learning. Specifically, we explore whether an adaptive
margin (eMargin), adjusted based on a predefined similarity threshold, can
improve the separation between adjacent but dissimilar time steps and
subsequently lead to better performance in downstream tasks. Our study
evaluates the impact of this modification on clustering performance and
classification in three benchmark datasets. Our findings, however, indicate
that achieving high scores on unsupervised clustering metrics does not
necessarily imply that the learned embeddings are meaningful or effective in
downstream tasks. To be specific, eMargin added to InfoNCE consistently
outperforms state-of-the-art baselines in unsupervised clustering metrics, but
struggles to achieve competitive results in downstream classification with
linear probing. The source code is publicly available at
https://github.com/sfi-norwai/eMargin.

</details>


### [346] [The Invisible Leash: Why RLVR May Not Escape Its Origin](https://arxiv.org/abs/2507.14843)
*Fang Wu,Weihao Xuan,Ximing Lu,Zaid Harchaoui,Yejin Choi*

Main category: cs.LG

TL;DR: RLVR enhances precision but may limit exploration and original solutions due to constraints of the base model.


<details>
  <summary>Details</summary>
Motivation: To investigate whether RLVR truly expands reasoning boundaries or just refines known solutions.

Method: Theoretical analysis and empirical experiments to evaluate RLVR's impact on reasoning and exploration.

Result: RLVR improves precision (pass@1) but shrinks empirical support, missing correct solutions the base model could find.

Conclusion: RLVR has limits in extending reasoning; future work needs exploration mechanisms or hybrid strategies.

Abstract: Recent advances in large reasoning models highlight Reinforcement Learning
with Verifiable Rewards (RLVR) as a promising method for enhancing AI's
capabilities, particularly in solving complex logical tasks. However, it
remains unclear whether RLVR truly expands a model's reasoning boundary or
merely amplifies high-reward outputs that the base model already knows for
improved precision. This study presents a theoretical and empirical
investigation that provides fresh insights into the potential limits of RLVR.
First, we offer a new theoretical perspective that RLVR is constrained by the
base model's support-unable to sample solutions with zero initial
probability-and operates as a conservative reweighting mechanism that may
restrict the discovery of entirely original solutions. We also identify an
entropy-reward tradeoff: while RLVR reliably enhances precision, it may
progressively narrow exploration and potentially overlook correct yet
underrepresented solutions. Extensive empirical experiments validate that while
RLVR consistently improves pass@1, the shrinkage of empirical support generally
outweighs the expansion of empirical support under larger sampling budgets,
failing to recover correct answers that were previously accessible to the base
model. Interestingly, we also observe that while RLVR sometimes increases
token-level entropy, resulting in greater uncertainty at each generation step,
answer-level entropy declines, indicating that these seemingly more uncertain
paths ultimately converge onto a smaller set of distinct answers. Taken
together, these findings reveal potential limits of RLVR in extending reasoning
horizons. Breaking this invisible leash may require future algorithmic
innovations such as explicit exploration mechanisms or hybrid strategies that
seed probability mass into underrepresented solution regions.

</details>


### [347] [Time-Aware Attention for Enhanced Electronic Health Records Modeling](https://arxiv.org/abs/2507.14847)
*Junhan Yu,Zhunyi Feng,Junwei Lu,Tianxi Cai,Doudou Zhou*

Main category: cs.LG

TL;DR: TALE-EHR is a Transformer-based framework with a time-aware attention mechanism and LLM-derived embeddings for better EHR analysis, outperforming baselines in disease progression forecasting.


<details>
  <summary>Details</summary>
Motivation: EHRs contain valuable clinical data but pose challenges like data heterogeneity and irregular temporal patterns, requiring advanced modeling techniques.

Method: TALE-EHR uses a time-aware attention mechanism to model continuous temporal gaps and leverages LLM-derived embeddings for semantic understanding.

Result: Outperforms state-of-the-art baselines on tasks like disease progression forecasting in MIMIC-IV and PIC datasets.

Conclusion: Integrating explicit temporal modeling with strong semantic representations enhances EHR analysis, as demonstrated by TALE-EHR.

Abstract: Electronic Health Records (EHR) contain valuable clinical information for
predicting patient outcomes and guiding healthcare decisions. However,
effectively modeling Electronic Health Records (EHRs) requires addressing data
heterogeneity and complex temporal patterns. Standard approaches often struggle
with irregular time intervals between clinical events. We propose TALE-EHR, a
Transformer-based framework featuring a novel time-aware attention mechanism
that explicitly models continuous temporal gaps to capture fine-grained
sequence dynamics. To complement this temporal modeling with robust semantics,
TALE-EHR leverages embeddings derived from standardized code descriptions using
a pre-trained Large Language Model (LLM), providing a strong foundation for
understanding clinical concepts. Experiments on the MIMIC-IV and PIC dataset
demonstrate that our approach outperforms state-of-the-art baselines on tasks
such as disease progression forecasting. TALE-EHR underscores the benefit of
integrating explicit, continuous temporal modeling with strong semantic
representations provides a powerful solution for advancing EHR analysis.

</details>


### [348] [Hierarchical Multi-Agent Reinforcement Learning with Control Barrier Functions for Safety-Critical Autonomous Systems](https://arxiv.org/abs/2507.14850)
*H. M. Sabbir Ahmad,Ehsan Sabouni,Alexander Wasilkoff,Param Budhraja,Zijian Guo,Songyuan Zhang,Chuchu Fan,Christos Cassandras,Wenchao Li*

Main category: cs.LG

TL;DR: A safe Hierarchical Multi-Agent Reinforcement Learning (HMARL) approach using Control Barrier Functions (CBFs) is proposed to ensure safety and cooperation in multi-agent systems.


<details>
  <summary>Details</summary>
Motivation: Addressing the need for safety in multi-agent autonomous systems while ensuring cooperation.

Method: Hierarchical decomposition into high-level joint policy learning and low-level safe skill execution using CBFs.

Result: Achieves near-perfect safety (within 5%) and improved performance in challenging environments.

Conclusion: The HMARL-CBF approach effectively balances safety and cooperation in multi-agent systems.

Abstract: We address the problem of safe policy learning in multi-agent safety-critical
autonomous systems. In such systems, it is necessary for each agent to meet the
safety requirements at all times while also cooperating with other agents to
accomplish the task. Toward this end, we propose a safe Hierarchical
Multi-Agent Reinforcement Learning (HMARL) approach based on Control Barrier
Functions (CBFs). Our proposed hierarchical approach decomposes the overall
reinforcement learning problem into two levels learning joint cooperative
behavior at the higher level and learning safe individual behavior at the lower
or agent level conditioned on the high-level policy. Specifically, we propose a
skill-based HMARL-CBF algorithm in which the higher level problem involves
learning a joint policy over the skills for all the agents and the lower-level
problem involves learning policies to execute the skills safely with CBFs. We
validate our approach on challenging environment scenarios whereby a large
number of agents have to safely navigate through conflicting road networks.
Compared with existing state of the art methods, our approach significantly
improves the safety achieving near perfect (within 5%) success/safety rate
while also improving performance across all the environments.

</details>


### [349] [The Tsetlin Machine Goes Deep: Logical Learning and Reasoning With Graphs](https://arxiv.org/abs/2507.14874)
*Ole-Christoffer Granmo,Youmna Abdelwahab,Per-Arne Andersen,Paul F. A. Clarke,Kunal Dumbre,Ylva Grønninsæter,Vojtech Halenka,Runar Helin,Lei Jiao,Ahmed Khalid,Rebekka Omslandseter,Rupsa Saha,Mayur Shende,Xuan Zhang*

Main category: cs.LG

TL;DR: The Graph Tsetlin Machine (GraphTM) extends the Tsetlin Machine to graph-structured data, offering interpretability, efficiency, and competitive accuracy across diverse tasks like image classification, action tracking, recommendation systems, and genome analysis.


<details>
  <summary>Details</summary>
Motivation: To enhance the Tsetlin Machine's versatility by enabling it to handle graph-structured input, supporting sequences, grids, relations, and multimodality while maintaining interpretability and efficiency.

Method: The GraphTM uses message passing to build nested deep clauses for recognizing sub-graph patterns, reducing the number of clauses needed and improving data utilization.

Result: GraphTM outperforms other methods in accuracy (e.g., +3.86% on CIFAR-10, +20.6% in action tracking) and robustness (e.g., 89.86% vs. 70.87% for GCN in noisy recommendation systems). It also trains faster (2.5x) than GCN for genome analysis.

Conclusion: GraphTM demonstrates the potential of graph representation learning and deep clauses to expand the capabilities of Tsetlin Machines, achieving interpretability and performance across varied applications.

Abstract: Pattern recognition with concise and flat AND-rules makes the Tsetlin Machine
(TM) both interpretable and efficient, while the power of Tsetlin automata
enables accuracy comparable to deep learning on an increasing number of
datasets. We introduce the Graph Tsetlin Machine (GraphTM) for learning
interpretable deep clauses from graph-structured input. Moving beyond flat,
fixed-length input, the GraphTM gets more versatile, supporting sequences,
grids, relations, and multimodality. Through message passing, the GraphTM
builds nested deep clauses to recognize sub-graph patterns with exponentially
fewer clauses, increasing both interpretability and data utilization. For image
classification, GraphTM preserves interpretability and achieves 3.86%-points
higher accuracy on CIFAR-10 than a convolutional TM. For tracking action
coreference, faced with increasingly challenging tasks, GraphTM outperforms
other reinforcement learning methods by up to 20.6%-points. In recommendation
systems, it tolerates increasing noise to a greater extent than a Graph
Convolutional Neural Network (GCN), e.g., for noise ratio 0.1, GraphTM obtains
accuracy 89.86% compared to GCN's 70.87%. Finally, for viral genome sequence
data, GraphTM is competitive with BiLSTM-CNN and GCN accuracy-wise, training
2.5x faster than GCN. The GraphTM's application to these varied fields
demonstrates how graph representation learning and deep clauses bring new
possibilities for TM learning.

</details>


### [350] [Application-Specific Component-Aware Structured Pruning of Deep Neural Networks via Soft Coefficient Optimization](https://arxiv.org/abs/2507.14882)
*Ganesh Sundaram,Jonas Ulmen,Amjad Haider,Daniel Görges*

Main category: cs.LG

TL;DR: Proposes an enhanced importance metric for structured pruning of DNNs to balance compression and task performance, validated on MNIST image reconstruction.


<details>
  <summary>Details</summary>
Motivation: Addresses the challenge of preserving application-specific performance during DNN compression, where conventional pruning metrics often fail.

Method: Introduces a framework with strategies to determine optimal pruning magnitude for groups, ensuring performance constraints are met.

Result: Demonstrates effective preservation of task-relevant performance on an MNIST autoencoder after significant pruning.

Conclusion: The proposed method successfully maintains model usability post-pruning by adhering to application-specific criteria.

Abstract: Deep neural networks (DNNs) offer significant versatility and performance
benefits, but their widespread adoption is often hindered by high model
complexity and computational demands. Model compression techniques such as
pruning have emerged as promising solutions to these challenges. However, it
remains critical to ensure that application-specific performance
characteristics are preserved during compression. In structured pruning, where
groups of structurally coherent elements are removed, conventional importance
metrics frequently fail to maintain these essential performance attributes. In
this work, we propose an enhanced importance metric framework that not only
reduces model size but also explicitly accounts for application-specific
performance constraints. We employ multiple strategies to determine the optimal
pruning magnitude for each group, ensuring a balance between compression and
task performance. Our approach is evaluated on an autoencoder tasked with
reconstructing MNIST images. Experimental results demonstrate that the proposed
method effectively preserves task-relevant performance, maintaining the model's
usability even after substantial pruning, by satisfying the required
application-specific criteria.

</details>


### [351] [Old Rules in a New Game: Mapping Uncertainty Quantification to Quantum Machine Learning](https://arxiv.org/abs/2507.14919)
*Maximilian Wendlinger,Kilian Tscharke,Pascal Debus*

Main category: cs.LG

TL;DR: The paper addresses the lack of transparency in quantum machine learning by adapting classical uncertainty quantification methods to improve model reliability.


<details>
  <summary>Details</summary>
Motivation: The opacity of quantum machine learning models, similar to classical deep learning, leads to issues like overfitting and overconfidence, necessitating better uncertainty quantification.

Method: The authors theoretically develop and empirically evaluate techniques to map classical uncertainty quantification methods to quantum machine learning, building on classical and quantum Bayesian modeling.

Result: The findings highlight the importance of integrating classical uncertainty insights into quantum machine learning model design.

Conclusion: Classical uncertainty quantification methods can enhance transparency and reliability in quantum machine learning.

Abstract: One of the key obstacles in traditional deep learning is the reduction in
model transparency caused by increasingly intricate model functions, which can
lead to problems such as overfitting and excessive confidence in predictions.
With the advent of quantum machine learning offering possible advances in
computational power and latent space complexity, we notice the same opaque
behavior. Despite significant research in classical contexts, there has been
little advancement in addressing the black-box nature of quantum machine
learning. Consequently, we approach this gap by building upon existing work in
classical uncertainty quantification and initial explorations in quantum
Bayesian modeling to theoretically develop and empirically evaluate techniques
to map classical uncertainty quantification methods to the quantum machine
learning domain. Our findings emphasize the necessity of leveraging classical
insights into uncertainty quantification to include uncertainty awareness in
the process of designing new quantum machine learning models.

</details>


### [352] [FedWCM: Unleashing the Potential of Momentum-based Federated Learning in Long-Tailed Scenarios](https://arxiv.org/abs/2507.14980)
*Tianle Li,Yongzhi Huang,Linshan Jiang,Qipeng Xie,Chang Liu,Wenfeng Du,Lu Wang,Kaishun Wu*

Main category: cs.LG

TL;DR: FedWCM dynamically adjusts momentum in FL to address non-IID data challenges, improving convergence and performance in long-tailed scenarios.


<details>
  <summary>Details</summary>
Motivation: FL struggles with non-IID data, especially long-tailed distributions, causing biased models and convergence issues.

Method: FedWCM adjusts momentum using global and per-round data to correct directional biases, analyzed through layer-wise neural network behavior.

Result: FedWCM resolves non-convergence and outperforms existing methods, enhancing FL efficiency in heterogeneous and imbalanced data.

Conclusion: FedWCM effectively addresses FL challenges in non-IID and long-tailed data, improving model convergence and performance.

Abstract: Federated Learning (FL) enables decentralized model training while preserving
data privacy. Despite its benefits, FL faces challenges with non-identically
distributed (non-IID) data, especially in long-tailed scenarios with imbalanced
class samples. Momentum-based FL methods, often used to accelerate FL
convergence, struggle with these distributions, resulting in biased models and
making FL hard to converge. To understand this challenge, we conduct extensive
investigations into this phenomenon, accompanied by a layer-wise analysis of
neural network behavior. Based on these insights, we propose FedWCM, a method
that dynamically adjusts momentum using global and per-round data to correct
directional biases introduced by long-tailed distributions. Extensive
experiments show that FedWCM resolves non-convergence issues and outperforms
existing methods, enhancing FL's efficiency and effectiveness in handling
client heterogeneity and data imbalance.

</details>


### [353] [Clustered Federated Learning for Generalizable FDIA Detection in Smart Grids with Heterogeneous Data](https://arxiv.org/abs/2507.14999)
*Yunfeng Li,Junhong Liu,Zhaohui Yang,Guofu Liao,Chuyun Zhang*

Main category: cs.LG

TL;DR: A federated learning framework (FedClusAvg) is proposed for detecting False Data Injection Attacks (FDIAs) in smart grids, addressing Non-IID data challenges and privacy concerns while reducing communication costs.


<details>
  <summary>Details</summary>
Motivation: FDIAs threaten smart grids, and traditional centralized detection methods face privacy, data sharing, and scalability issues due to Non-IID data.

Method: FedClusAvg uses cluster-based stratified sampling and hierarchical communication (client-subserver-server) for localized training and weighted parameter aggregation.

Result: Experiments show improved FDIA detection accuracy under Non-IID conditions, with reduced communication rounds and bandwidth usage.

Conclusion: FedClusAvg offers a secure, efficient solution for FDIA detection in distributed power systems.

Abstract: False Data Injection Attacks (FDIAs) pose severe security risks to smart
grids by manipulating measurement data collected from spatially distributed
devices such as SCADA systems and PMUs. These measurements typically exhibit
Non-Independent and Identically Distributed (Non-IID) characteristics across
different regions, which significantly challenges the generalization ability of
detection models. Traditional centralized training approaches not only face
privacy risks and data sharing constraints but also incur high transmission
costs, limiting their scalability and deployment feasibility. To address these
issues, this paper proposes a privacy-preserving federated learning framework,
termed Federated Cluster Average (FedClusAvg), designed to improve FDIA
detection in Non-IID and resource-constrained environments. FedClusAvg
incorporates cluster-based stratified sampling and hierarchical communication
(client-subserver-server) to enhance model generalization and reduce
communication overhead. By enabling localized training and weighted parameter
aggregation, the algorithm achieves accurate model convergence without
centralizing sensitive data. Experimental results on benchmark smart grid
datasets demonstrate that FedClusAvg not only improves detection accuracy under
heterogeneous data distributions but also significantly reduces communication
rounds and bandwidth consumption. This work provides an effective solution for
secure and efficient FDIA detection in large-scale distributed power systems.

</details>


### [354] [Time-RA: Towards Time Series Reasoning for Anomaly with LLM Feedback](https://arxiv.org/abs/2507.15066)
*Yiyuan Yang,Zichuan Liu,Lei Song,Kai Ying,Zhiguang Wang,Tom Bamford,Svitlana Vyetrenko,Jiang Bian,Qingsong Wen*

Main category: cs.LG

TL;DR: The paper introduces Time-RA, a generative task for time-series anomaly reasoning using LLMs, and RATs40K, a multimodal benchmark dataset with detailed annotations. It highlights the limitations of current models and the need for supervised fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Current time-series anomaly detection lacks detailed categorization and reasoning. The paper aims to shift from binary classification to a generative, reasoning-intensive task.

Method: Proposes Time-RA task and RATs40K dataset, annotated with fine-grained categories and reasoning. Uses ensemble-generated labels refined by GPT-4 for accuracy.

Result: Benchmarking shows capabilities and limitations of LLMs, emphasizing the importance of supervised fine-tuning.

Conclusion: The dataset and task advance interpretable time-series anomaly detection and reasoning.

Abstract: Time series anomaly detection is critical across various domains, yet current
approaches often limit analysis to mere binary anomaly classification without
detailed categorization or further explanatory reasoning. To address these
limitations, we propose a novel task, Time-series Reasoning for Anomaly
(Time-RA) that transforms classical time series anomaly detection from a
discriminative into a generative, reasoning-intensive task leveraging Large
Language Models (LLMs). Also, we introduce the first real-world multimodal
benchmark dataset, RATs40K, explicitly annotated for anomaly reasoning,
comprising approximately 40,000 samples across 10 real-world domains. Each
sample includes numeric time series data, contextual text information, and
visual representations, each annotated with fine-grained categories (14 types
for univariate anomalies and 6 for multivariate anomalies) and structured
explanatory reasoning. We develop a sophisticated annotation framework
utilizing ensemble-generated labels refined through GPT-4-driven feedback,
ensuring accuracy and interpretability. Extensive benchmarking of LLMs and
multimodal LLMs demonstrates the capabilities and limitations of current
models, highlighting the critical role of supervised fine-tuning. Our dataset
and task pave the way for significant advancements in interpretable time series
anomaly detection and reasoning.

</details>


### [355] [ROBAD: Robust Adversary-aware Local-Global Attended Bad Actor Detection Sequential Model](https://arxiv.org/abs/2507.15067)
*Bing He,Mustaque Ahamad,Srijan Kumar*

Main category: cs.LG

TL;DR: ROBAD is a transformer-based model designed to detect bad actors on internet platforms robustly by capturing local and global post information and using contrastive learning to resist adversarial attacks.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning models for detecting bad actors lack robustness against adversarial attacks, as they are sensitive to minor input changes.

Method: ROBAD uses transformer encoder blocks for post-level embeddings (local info) and decoder blocks for sequence-level embeddings (global info), enhanced with contrastive learning using mimicked adversarial sequences.

Result: ROBAD effectively detects bad actors under adversarial attacks, outperforming existing models on Yelp and Wikipedia datasets.

Conclusion: ROBAD's robustness against adversarial attacks makes it a reliable solution for detecting bad actors on internet platforms.

Abstract: Detecting bad actors is critical to ensure the safety and integrity of
internet platforms. Several deep learning-based models have been developed to
identify such users. These models should not only accurately detect bad actors,
but also be robust against adversarial attacks that aim to evade detection.
However, past deep learning-based detection models do not meet the robustness
requirement because they are sensitive to even minor changes in the input
sequence. To address this issue, we focus on (1) improving the model
understanding capability and (2) enhancing the model knowledge such that the
model can recognize potential input modifications when making predictions. To
achieve these goals, we create a novel transformer-based classification model,
called ROBAD (RObust adversary-aware local-global attended Bad Actor Detection
model), which uses the sequence of user posts to generate user embedding to
detect bad actors. Particularly, ROBAD first leverages the transformer encoder
block to encode each post bidirectionally, thus building a post embedding to
capture the local information at the post level. Next, it adopts the
transformer decoder block to model the sequential pattern in the post
embeddings by using the attention mechanism, which generates the sequence
embedding to obtain the global information at the sequence level. Finally, to
enrich the knowledge of the model, embeddings of modified sequences by mimicked
attackers are fed into a contrastive-learning-enhanced classification layer for
sequence prediction. In essence, by capturing the local and global information
(i.e., the post and sequence information) and leveraging the mimicked behaviors
of bad actors in training, ROBAD can be robust to adversarial attacks.
Extensive experiments on Yelp and Wikipedia datasets show that ROBAD can
effectively detect bad actors when under state-of-the-art adversarial attacks.

</details>


### [356] [Reinforcement Learning for Flow-Matching Policies](https://arxiv.org/abs/2507.15073)
*Samuel Pfrommer,Yixiao Huang,Somayeh Sojoudi*

Main category: cs.LG

TL;DR: Flow-matching policies trained via reinforcement learning outperform suboptimal human demonstrations, with GRPO reducing costs by 50-85% compared to naive imitation learning.


<details>
  <summary>Details</summary>
Motivation: To surpass the performance of suboptimal demonstration policies, such as human operators, in flow-matching robotics tasks.

Method: Introduces Reward-Weighted Flow Matching (RWFM) and Group Relative Policy Optimization (GRPO) with a learned reward surrogate, tested on simulated unicycle dynamics tasks.

Result: Both RWFM and GRPO improve upon demonstrator performance, with GRPO reducing costs by 50-85% compared to naive imitation learning.

Conclusion: Reinforcement learning enhances flow-matching policies, with GRPO being particularly effective for surpassing suboptimal demonstration performance.

Abstract: Flow-matching policies have emerged as a powerful paradigm for generalist
robotics. These models are trained to imitate an action chunk, conditioned on
sensor observations and textual instructions. Often, training demonstrations
are generated by a suboptimal policy, such as a human operator. This work
explores training flow-matching policies via reinforcement learning to surpass
the original demonstration policy performance. We particularly note
minimum-time control as a key application and present a simple scheme for
variable-horizon flow-matching planning. We then introduce two families of
approaches: a simple Reward-Weighted Flow Matching (RWFM) scheme and a Group
Relative Policy Optimization (GRPO) approach with a learned reward surrogate.
Our policies are trained on an illustrative suite of simulated unicycle
dynamics tasks, and we show that both approaches dramatically improve upon the
suboptimal demonstrator performance, with the GRPO approach in particular
generally incurring between $50\%$ and $85\%$ less cost than a naive Imitation
Learning Flow Matching (ILFM) approach.

</details>


### [357] [Isotonic Quantile Regression Averaging for uncertainty quantification of electricity price forecasts](https://arxiv.org/abs/2507.15079)
*Arkadiusz Lipiecki,Bartosz Uniejewski*

Main category: cs.LG

TL;DR: The paper introduces Isotonic Quantile Regression Averaging (iQRA), a method for probabilistic forecasting in volatile domains like electricity markets, improving reliability and sharpness over existing methods.


<details>
  <summary>Details</summary>
Motivation: Uncertainty in forecasting models is crucial for risk assessment in volatile domains like electricity markets, but current machine learning models often lack reliable uncertainty estimates.

Method: The proposed iQRA method extends Quantile Regression Averaging (QRA) by introducing stochastic order constraints to enhance accuracy, reliability, and computational efficiency.

Result: iQRA outperforms state-of-the-art methods in the German day-ahead electricity market, providing well-calibrated prediction intervals and superior reliability, especially compared to coverage-based conformal prediction.

Conclusion: iQRA offers a hyperparameter-free, computationally efficient solution for probabilistic forecasting, addressing the limitations of existing methods in uncertainty estimation.

Abstract: Quantifying the uncertainty of forecasting models is essential to assess and
mitigate the risks associated with data-driven decisions, especially in
volatile domains such as electricity markets. Machine learning methods can
provide highly accurate electricity price forecasts, critical for informing the
decisions of market participants. However, these models often lack uncertainty
estimates, which limits the ability of decision makers to avoid unnecessary
risks. In this paper, we propose a novel method for generating probabilistic
forecasts from ensembles of point forecasts, called Isotonic Quantile
Regression Averaging (iQRA). Building on the established framework of Quantile
Regression Averaging (QRA), we introduce stochastic order constraints to
improve forecast accuracy, reliability, and computational costs. In an
extensive forecasting study of the German day-ahead electricity market, we show
that iQRA consistently outperforms state-of-the-art postprocessing methods in
terms of both reliability and sharpness. It produces well-calibrated prediction
intervals across multiple confidence levels, providing superior reliability to
all benchmark methods, particularly coverage-based conformal prediction. In
addition, isotonic regularization decreases the complexity of the quantile
regression problem and offers a hyperparameter-free approach to variable
selection.

</details>


### [358] [Robust Control with Gradient Uncertainty](https://arxiv.org/abs/2507.15082)
*Qian Qi*

Main category: cs.LG

TL;DR: A novel robust control theory extension addresses gradient uncertainty in value functions, introducing the GU-HJBI equation and a new algorithm, GURAC, validated empirically.


<details>
  <summary>Details</summary>
Motivation: Address uncertainty in value function gradients, common in reinforcement learning and other fields, to improve robustness.

Method: Formulate a zero-sum dynamic game with adversarial perturbations, analyze GU-HJBI equation, and develop the GURAC algorithm.

Result: Proved well-posedness of GU-HJBI, showed classical quadratic value function fails under gradient uncertainty, and validated GURAC's effectiveness.

Conclusion: Provides a new robust control direction with implications for reinforcement learning and computational finance.

Abstract: We introduce a novel extension to robust control theory that explicitly
addresses uncertainty in the value function's gradient, a form of uncertainty
endemic to applications like reinforcement learning where value functions are
approximated. We formulate a zero-sum dynamic game where an adversary perturbs
both system dynamics and the value function gradient, leading to a new, highly
nonlinear partial differential equation: the Hamilton-Jacobi-Bellman-Isaacs
Equation with Gradient Uncertainty (GU-HJBI). We establish its well-posedness
by proving a comparison principle for its viscosity solutions under a uniform
ellipticity condition. Our analysis of the linear-quadratic (LQ) case yields a
key insight: we prove that the classical quadratic value function assumption
fails for any non-zero gradient uncertainty, fundamentally altering the problem
structure. A formal perturbation analysis characterizes the non-polynomial
correction to the value function and the resulting nonlinearity of the optimal
control law, which we validate with numerical studies. Finally, we bridge
theory to practice by proposing a novel Gradient-Uncertainty-Robust
Actor-Critic (GURAC) algorithm, accompanied by an empirical study demonstrating
its effectiveness in stabilizing training. This work provides a new direction
for robust control, holding significant implications for fields where function
approximation is common, including reinforcement learning and computational
finance.

</details>


### [359] [AnalogFed: Federated Discovery of Analog Circuit Topologies with Generative AI](https://arxiv.org/abs/2507.15104)
*Qiufeng Li,Shu Hong,Jian Gao,Xuan Zhang,Tian Lan,Weidong Cao*

Main category: cs.LG

TL;DR: AnalogFed enables collaborative analog circuit topology discovery using generative AI without sharing raw data, addressing privacy and data fragmentation issues.


<details>
  <summary>Details</summary>
Motivation: The proprietary nature of analog circuit design limits data availability, hindering collaborative AI-driven innovation.

Method: AnalogFed employs federated learning (FedL) with tailored techniques for generative model development, data heterogeneity handling, and privacy preservation.

Result: AnalogFed matches centralized performance while ensuring data privacy, achieving state-of-the-art efficiency in topology design.

Conclusion: AnalogFed successfully bridges the gap between collaborative innovation and data privacy in analog circuit design.

Abstract: Recent breakthroughs in AI/ML offer exciting opportunities to revolutionize
analog design automation through data-driven approaches. In particular,
researchers are increasingly fascinated by harnessing the power of generative
AI to automate the discovery of novel analog circuit topologies. Unlocking the
full potential of generative AI in these data-driven discoveries requires
access to large and diverse datasets.Yet, there is a significant barrier in the
analog domain--Analog circuit design is inherently proprietary, involving not
only confidential circuit structures but also the underlying commercial
semiconductor processes. As a result, current generative AI research is largely
confined to individual researchers who construct small, narrowly focused
private datasets. This fragmentation severely limits collaborative innovation
and impedes progress across the research community. To address these
challenges, we propose AnalogFed. AnalogFed enables collaborative topology
discovery across decentralized clients (e.g., individual researchers or
institutions) without requiring the sharing of raw private data. To make this
vision practical, we introduce a suite of techniques tailored to the unique
challenges of applying FedL in analog design--from generative model development
and data heterogeneity handling to privacy-preserving strategies that ensure
both flexibility and security for circuit designers and semiconductor
manufacturers. Extensive experiments across varying client counts and dataset
sizes demonstrate that AnalogFed achieves performance comparable to centralized
baselines--while maintaining strict data privacy. Specifically, the generative
AI model within AnalogFed achieves state-of-the-art efficiency and scalability
in the design of analog circuit topologies.

</details>


### [360] [Distributional Unlearning: Forgetting Distributions, Not Just Samples](https://arxiv.org/abs/2507.15112)
*Youssef Allouah,Rachid Guerraoui,Sanmi Koyejo*

Main category: cs.LG

TL;DR: The paper introduces distributional unlearning, a framework to remove unwanted data domains efficiently while preserving retained data quality, outperforming random deletion methods.


<details>
  <summary>Details</summary>
Motivation: Existing unlearning tools are sample-oriented and inefficient for removing entire domains, which is needed for privacy, legal, or quality compliance.

Method: Proposes distributional unlearning using Kullback-Leibler divergence to quantify removal and preservation, deriving exact Pareto frontiers for Gaussian cases and a distance-based selection rule.

Result: Achieves 15-72% fewer deletions than random removal with negligible impact on retained performance in experiments.

Conclusion: Distributional unlearning is a practical, efficient solution for domain-level data removal in machine learning models.

Abstract: Machine unlearning seeks to remove unwanted information from trained models,
initially at the individual-sample level, but increasingly at the level of
entire sub-populations. In many deployments, models must delete whole topical
domains to satisfy privacy, legal, or quality requirements, e.g., removing
several users' posts under GDPR or copyrighted web content. Existing unlearning
tools remain largely sample-oriented, and straightforward point deletion often
leaves enough residual signal for downstream learners to recover the unwanted
domain. We introduce distributional unlearning, a data-centric, model-agnostic
framework that asks: Given examples from an unwanted distribution and a
retained distribution, what is the smallest set of points whose removal makes
the edited dataset far from the unwanted domain yet close to the retained one?
Using Kullback-Leibler divergence to quantify removal and preservation, we
derive the exact Pareto frontier in the Gaussian case and prove that any model
retrained on the edited data incurs log-loss shifts bounded by the divergence
thresholds. We propose a simple distance-based selection rule satisfying these
constraints with a quadratic reduction in deletion budget compared to random
removal. Experiments on synthetic Gaussians, Jigsaw Toxic Comments, SMS spam,
and CIFAR-10 show 15-72% fewer deletions than random, with negligible impact on
retained performance.

</details>


### [361] [Are We Overlooking the Dimensions? Learning Latent Hierarchical Channel Structure for High-Dimensional Time Series Forecasting](https://arxiv.org/abs/2507.15119)
*Juntong Ni,Shiyu Wang,Zewen Liu,Xiaoming Shi,Xinyue Zhong,Zhou Ye,Wei Jin*

Main category: cs.LG

TL;DR: U-Cast is a novel architecture for High-Dimensional Time Series Forecasting (HDTSF) that leverages query-based attention and full-rank regularization to model complex channel correlations, outperforming baselines on the Time-HD benchmark.


<details>
  <summary>Details</summary>
Motivation: Traditional TSF models struggle with high-dimensional datasets due to ignored or unscalable channel interactions, necessitating a specialized approach for HDTSF.

Method: U-Cast uses query-based attention to learn hierarchical channel structures and full-rank regularization to disentangle correlated representations.

Result: U-Cast achieves superior accuracy and efficiency on the Time-HD benchmark, supported by theoretical evidence on cross-channel information benefits.

Conclusion: U-Cast and Time-HD establish a foundation for advancing HDTSF research by addressing scalability and complex channel correlations.

Abstract: Time series forecasting (TSF) is a central problem in time series analysis.
However, as the number of channels in time series datasets scales to the
thousands or more, a scenario we define as High-Dimensional Time Series
Forecasting (HDTSF), it introduces significant new modeling challenges that are
often not the primary focus of traditional TSF research. HDTSF is challenging
because the channel correlation often forms complex and hierarchical patterns.
Existing TSF models either ignore these interactions or fail to scale as
dimensionality grows. To address this issue, we propose U-Cast, a
channel-dependent forecasting architecture that learns latent hierarchical
channel structures with an innovative query-based attention. To disentangle
highly correlated channel representation, U-Cast adds a full-rank
regularization during training. We also release Time-HD, a benchmark of large,
diverse, high-dimensional datasets. Our theory shows that exploiting
cross-channel information lowers forecasting risk, and experiments on Time-HD
demonstrate that U-Cast surpasses strong baselines in both accuracy and
efficiency. Together, U-Cast and Time-HD provide a solid basis for future HDTSF
research.

</details>


### [362] [Transforming Datasets to Requested Complexity with Projection-based Many-Objective Genetic Algorithm](https://arxiv.org/abs/2507.15132)
*Joanna Komorniczak*

Main category: cs.LG

TL;DR: A genetic algorithm is proposed to generate synthetic datasets with controlled complexity for ML evaluation, showing a correlation between dataset complexity and model performance.


<details>
  <summary>Details</summary>
Motivation: To enhance the availability of diverse datasets for reliably evaluating ML methods by controlling problem complexity.

Method: A genetic algorithm optimizes problem complexity measures (10 for classification, 4 for regression) via linear feature projections to achieve target complexity values.

Result: The algorithm successfully generates datasets of varying difficulty, with experiments showing a link between complexity and model recognition quality.

Conclusion: The approach effectively produces datasets with tailored complexity, aiding ML method evaluation.

Abstract: The research community continues to seek increasingly more advanced synthetic
data generators to reliably evaluate the strengths and limitations of machine
learning methods. This work aims to increase the availability of datasets
encompassing a diverse range of problem complexities by proposing a genetic
algorithm that optimizes a set of problem complexity measures for
classification and regression tasks towards specific targets. For
classification, a set of 10 complexity measures was used, while for regression
tasks, 4 measures demonstrating promising optimization capabilities were
selected. Experiments confirmed that the proposed genetic algorithm can
generate datasets with varying levels of difficulty by transforming
synthetically created datasets to achieve target complexity values through
linear feature projections. Evaluations involving state-of-the-art classifiers
and regressors revealed a correlation between the complexity of the generated
data and the recognition quality.

</details>


### [363] [Constraint-aware Learning of Probabilistic Sequential Models for Multi-Label Classification](https://arxiv.org/abs/2507.15156)
*Mykhailo Buleshnyi,Anna Polova,Zsolt Zombori,Michael Benedikt*

Main category: cs.LG

TL;DR: The paper explores multi-label classification with large label sets and logical constraints, using an expressive sequential model to capture correlations and enforce constraints.


<details>
  <summary>Details</summary>
Motivation: To address multi-label classification challenges where labels have logical constraints and correlations, requiring an expressive model.

Method: An architecture combining individual label classifiers with a sequential model to produce a joint distribution, exploiting constraints during training and inference.

Result: Empirical demonstration shows the model effectively exploits and enforces constraints.

Conclusion: The proposed architecture successfully handles multi-label classification with constraints by leveraging an expressive sequential model.

Abstract: We investigate multi-label classification involving large sets of labels,
where the output labels may be known to satisfy some logical constraints. We
look at an architecture in which classifiers for individual labels are fed into
an expressive sequential model, which produces a joint distribution. One of the
potential advantages for such an expressive model is its ability to modelling
correlations, as can arise from constraints. We empirically demonstrate the
ability of the architecture both to exploit constraints in training and to
enforce constraints at inference time.

</details>


### [364] [Resonant-Tunnelling Diode Reservoir Computing System for Image Recognition](https://arxiv.org/abs/2507.15158)
*A. H. Abbas,Hend Abdel-Ghani,Ivan S. Maksymov*

Main category: cs.LG

TL;DR: A neuromorphic computing architecture using resonant-tunneling diodes (RTDs) is proposed for efficient physical reservoir computing, validated on image recognition tasks.


<details>
  <summary>Details</summary>
Motivation: The need for hardware-efficient computational models in AI for edge-based and resource-constrained environments drives this research.

Method: The study formulates and implements an RTD-based reservoir computing system, tested on handwritten digit and Fruit~360 dataset recognition.

Result: The architecture shows promising performance while avoiding random connectivity, using deterministic nonlinear transformations.

Conclusion: The RTD-based approach offers a viable solution for next-generation reservoir computing in constrained environments.

Abstract: As artificial intelligence continues to push into real-time, edge-based and
resource-constrained environments, there is an urgent need for novel,
hardware-efficient computational models. In this study, we present and validate
a neuromorphic computing architecture based on resonant-tunnelling diodes
(RTDs), which exhibit the nonlinear characteristics ideal for physical
reservoir computing (RC). We theoretically formulate and numerically implement
an RTD-based RC system and demonstrate its effectiveness on two image
recognition benchmarks: handwritten digit classification and object recognition
using the Fruit~360 dataset. Our results show that this circuit-level
architecture delivers promising performance while adhering to the principles of
next-generation RC -- eliminating random connectivity in favour of a
deterministic nonlinear transformation of input signals.

</details>


### [365] [Designing User-Centric Metrics for Evaluation of Counterfactual Explanations](https://arxiv.org/abs/2507.15162)
*Firdaus Ahmed Choudhury,Ethan Leicht,Jude Ethan Bislig,Hangzhi Guo,Amulya Yadav*

Main category: cs.LG

TL;DR: The paper addresses the misalignment between existing counterfactual explanation (CFE) evaluation metrics and real-world user preferences, proposing a user-centric model (AWP) that predicts preferred CFEs with high accuracy.


<details>
  <summary>Details</summary>
Motivation: Current CFE evaluation metrics like proximity often overlook user preferences and constraints, leading to a gap in practical applicability.

Method: Conducted two studies: a pilot with 20 crowd-workers and a detailed two-day study with 41 participants in credit application scenarios, followed by proposing the AWP model.

Result: User-preferred CFEs matched proximity-based ones only 63.81% of the time; AWP model predicted preferences with 84.37% accuracy.

Conclusion: The study validates the need for adaptive, user-centered CFE evaluation metrics and introduces the effective AWP model.

Abstract: Machine learning-based decision models are increasingly being used to make
decisions that significantly impact people's lives, but their opaque nature
leaves end users without a clear understanding of why a decision was made.
Counterfactual Explanations (CFEs) have grown in popularity as a means of
offering actionable guidance by identifying the minimum changes in feature
values required to flip a model's prediction to something more desirable.
Unfortunately, most prior research in CFEs relies on artificial evaluation
metrics, such as proximity, which may overlook end-user preferences and
constraints, e.g., the user's perception of effort needed to make certain
feature changes may differ from that of the model designer. To address this
research gap, this paper makes three novel contributions. First, we conduct a
pilot study with 20 crowd-workers on Amazon MTurk to experimentally validate
the alignment of existing CF evaluation metrics with real-world user
preferences. Results show that user-preferred CFEs matched those based on
proximity in only 63.81% of cases, highlighting the limited applicability of
these metrics in real-world settings. Second, inspired by the need to design a
user-informed evaluation metric for CFEs, we conduct a more detailed two-day
user study with 41 participants facing realistic credit application scenarios
to find experimental support for or against three intuitive hypotheses that may
explain how end users evaluate CFEs. Third, based on the findings of this
second study, we propose the AWP model, a novel user-centric, two-stage model
that describes one possible mechanism by which users evaluate and select CFEs.
Our results show that AWP predicts user-preferred CFEs with 84.37% accuracy.
Our study provides the first human-centered validation for personalized cost
models in CFE generation and highlights the need for adaptive, user-centered
evaluation metrics.

</details>


### [366] [Better Models and Algorithms for Learning Ising Models from Dynamics](https://arxiv.org/abs/2507.15173)
*Jason Gaitonde,Ankur Moitra,Elchanan Mossel*

Main category: cs.LG

TL;DR: The paper presents efficient algorithms for learning the Ising model's structure and parameters from a Markov chain's evolution, focusing on a realistic observation model where only configuration changes are observed.


<details>
  <summary>Details</summary>
Motivation: Prior work assumed observing all site update attempts, even unsuccessful ones, which is unrealistic. This work addresses the open problem of learning from only observed configuration changes.

Method: The algorithm recovers the dependency graph in poly(d)⋅n²log n time and parameters in Õ(2^d n) time, applicable to reversible, single-site Markov chains like Metropolis.

Result: The algorithm efficiently learns the Ising model in a realistic observation model, matching state-of-the-art performance despite weaker assumptions.

Conclusion: This work advances learning in more natural settings, extending applicability to broader Markov chains and providing robust theoretical guarantees.

Abstract: We study the problem of learning the structure and parameters of the Ising
model, a fundamental model of high-dimensional data, when observing the
evolution of an associated Markov chain. A recent line of work has studied the
natural problem of learning when observing an evolution of the well-known
Glauber dynamics [Bresler, Gamarnik, Shah, IEEE Trans. Inf. Theory 2018,
Gaitonde, Mossel STOC 2024], which provides an arguably more realistic
generative model than the classical i.i.d. setting. However, this prior work
crucially assumes that all site update attempts are observed, \emph{even when
this attempt does not change the configuration}: this strong observation model
is seemingly essential for these approaches. While perhaps possible in
restrictive contexts, this precludes applicability to most realistic settings
where we can observe \emph{only} the stochastic evolution itself, a minimal and
natural assumption for any process we might hope to learn from. However,
designing algorithms that succeed in this more realistic setting has remained
an open problem [Bresler, Gamarnik, Shah, IEEE Trans. Inf. Theory 2018,
Gaitonde, Moitra, Mossel, STOC 2025].
  In this work, we give the first algorithms that efficiently learn the Ising
model in this much more natural observation model that only observes when the
configuration changes. For Ising models with maximum degree $d$, our algorithm
recovers the underlying dependency graph in time $\mathsf{poly}(d)\cdot n^2\log
n$ and then the actual parameters in additional $\widetilde{O}(2^d n)$ time,
which qualitatively matches the state-of-the-art even in the i.i.d. setting in
a much weaker observation model. Our analysis holds more generally for a
broader class of reversible, single-site Markov chains that also includes the
popular Metropolis chain by leveraging more robust properties of reversible
Markov chains.

</details>


### [367] [Joint-Local Grounded Action Transformation for Sim-to-Real Transfer in Multi-Agent Traffic Control](https://arxiv.org/abs/2507.15174)
*Justin Turnau,Longchao Da,Khoa Vo,Ferdous Al Rafi,Shreyas Bachiraju,Tiejin Chen,Hua Wei*

Main category: cs.LG

TL;DR: JL-GAT applies Grounded Action Transformation (GAT) to multi-agent RL for Traffic Signal Control (TSC), addressing the sim-to-real gap while maintaining scalability and capturing agent interactions.


<details>
  <summary>Details</summary>
Motivation: The sim-to-real gap in MARL-based TSC policies causes performance drops in real-world traffic networks, necessitating a scalable and grounded solution like JL-GAT.

Method: JL-GAT decentralizes GAT for MARL-based TSC, incorporating neighboring agent information to balance scalability and grounding capability.

Result: Experiments under simulated adverse weather conditions show JL-GAT effectively mitigates the sim-to-real gap in diverse road networks.

Conclusion: JL-GAT successfully bridges the sim-to-real gap for MARL-based TSC, offering a scalable and grounded approach for real-world traffic networks.

Abstract: Traffic Signal Control (TSC) is essential for managing urban traffic flow and
reducing congestion. Reinforcement Learning (RL) offers an adaptive method for
TSC by responding to dynamic traffic patterns, with multi-agent RL (MARL)
gaining traction as intersections naturally function as coordinated agents.
However, due to shifts in environmental dynamics, implementing MARL-based TSC
policies in the real world often leads to a significant performance drop, known
as the sim-to-real gap. Grounded Action Transformation (GAT) has successfully
mitigated this gap in single-agent RL for TSC, but real-world traffic networks,
which involve numerous interacting intersections, are better suited to a MARL
framework. In this work, we introduce JL-GAT, an application of GAT to
MARL-based TSC that balances scalability with enhanced grounding capability by
incorporating information from neighboring agents. JL-GAT adopts a
decentralized approach to GAT, allowing for the scalability often required in
real-world traffic networks while still capturing key interactions between
agents. Comprehensive experiments on various road networks under simulated
adverse weather conditions, along with ablation studies, demonstrate the
effectiveness of JL-GAT. The code is publicly available at
https://github.com/DaRL-LibSignal/JL-GAT/.

</details>


### [368] [Feature Construction Using Network Control Theory and Rank Encoding for Graph Machine Learning](https://arxiv.org/abs/2507.15195)
*Anwar Said,Yifan Wei,Ubaid Ullah Ahmad,Mudassir Shabbir,Waseem Abbas,Xenofon Koutsoukos*

Main category: cs.LG

TL;DR: The paper proposes using average controllability and a novel rank encoding method to improve GNN performance in social network classification, especially when node features are scarce.


<details>
  <summary>Details</summary>
Motivation: GNNs struggle in social networks due to lack of node features. The study aims to enhance feature expressiveness using network topology metrics.

Method: Introduces NCT-EFA (node-level metrics) and a rank encoding method to transform graph-theoretic metrics into fixed-dimensional features.

Result: Average controllability and rank encoding boost GNN performance, with ROC AUC improving from 68.7% to 73.9% on GitHub Stargazers dataset.

Conclusion: The proposed methods effectively address feature scarcity in social networks, enhancing GNN performance.

Abstract: In this article, we utilize the concept of average controllability in graphs,
along with a novel rank encoding method, to enhance the performance of Graph
Neural Networks (GNNs) in social network classification tasks. GNNs have proven
highly effective in various network-based learning applications and require
some form of node features to function. However, their performance is heavily
influenced by the expressiveness of these features. In social networks, node
features are often unavailable due to privacy constraints or the absence of
inherent attributes, making it challenging for GNNs to achieve optimal
performance. To address this limitation, we propose two strategies for
constructing expressive node features. First, we introduce average
controllability along with other centrality metrics (denoted as NCT-EFA) as
node-level metrics that capture critical aspects of network topology. Building
on this, we develop a rank encoding method that transforms average
controllability or any other graph-theoretic metric into a fixed-dimensional
feature space, thereby improving feature representation. We conduct extensive
numerical evaluations using six benchmark GNN models across four social network
datasets to compare different node feature construction methods. Our results
demonstrate that incorporating average controllability into the feature space
significantly improves GNN performance. Moreover, the proposed rank encoding
method outperforms traditional one-hot degree encoding, improving the ROC AUC
from 68.7% to 73.9% using GraphSAGE on the GitHub Stargazers dataset,
underscoring its effectiveness in generating expressive and efficient node
representations.

</details>


### [369] [Long-Short Distance Graph Neural Networks and Improved Curriculum Learning for Emotion Recognition in Conversation](https://arxiv.org/abs/2507.15205)
*Xinran Li,Xiujuan Xu,Jiaqi Qiao*

Main category: cs.LG

TL;DR: The paper introduces LSDGNN, a multimodal approach for Emotion Recognition in Conversation (ERC), using long- and short-distance graph neural networks with a Differential Regularizer and BiAffine Module. It also proposes Improved Curriculum Learning (ICL) to handle data imbalance, achieving superior results on IEMOCAP and MELD datasets.


<details>
  <summary>Details</summary>
Motivation: ERC is challenging due to the complexity of multimodal interactions and data imbalance. The paper aims to improve feature extraction and learning efficiency.

Method: LSDGNN combines long- and short-distance graph neural networks (DAG-based) with a Differential Regularizer and BiAffine Module. ICL uses a "weighted emotional shift" metric for balanced training.

Result: The model outperforms benchmarks on IEMOCAP and MELD datasets.

Conclusion: LSDGNN and ICL effectively address ERC challenges, demonstrating superior performance and robustness.

Abstract: Emotion Recognition in Conversation (ERC) is a practical and challenging
task. This paper proposes a novel multimodal approach, the Long-Short Distance
Graph Neural Network (LSDGNN). Based on the Directed Acyclic Graph (DAG), it
constructs a long-distance graph neural network and a short-distance graph
neural network to obtain multimodal features of distant and nearby utterances,
respectively. To ensure that long- and short-distance features are as distinct
as possible in representation while enabling mutual influence between the two
modules, we employ a Differential Regularizer and incorporate a BiAffine Module
to facilitate feature interaction. In addition, we propose an Improved
Curriculum Learning (ICL) to address the challenge of data imbalance. By
computing the similarity between different emotions to emphasize the shifts in
similar emotions, we design a "weighted emotional shift" metric and develop a
difficulty measurer, enabling a training process that prioritizes learning easy
samples before harder ones. Experimental results on the IEMOCAP and MELD
datasets demonstrate that our model outperforms existing benchmarks.

</details>


### [370] [Exact Reformulation and Optimization for Direct Metric Optimization in Binary Imbalanced Classification](https://arxiv.org/abs/2507.15240)
*Le Peng,Yash Travadi,Chuan He,Ying Cui,Ju Sun*

Main category: cs.LG

TL;DR: The paper introduces exact constrained reformulations for direct metric optimization (DMO) in imbalanced classification, addressing precision and recall under three practical settings, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Standard accuracy is misleading in imbalanced classification, and existing methods fail when class significance varies or specific metric levels are required.

Method: The authors propose exact constrained reformulations for DMO problems (FPOR, FROP, OFBS) and solve them using exact penalty methods.

Result: Experiments on benchmark datasets show the approach's superiority over state-of-the-art methods for the three DMO problems.

Conclusion: The exact reformulation and optimization (ERO) framework is effective for binary imbalanced classification and has broader applicability.

Abstract: For classification with imbalanced class frequencies, i.e., imbalanced
classification (IC), standard accuracy is known to be misleading as a
performance measure. While most existing methods for IC resort to optimizing
balanced accuracy (i.e., the average of class-wise recalls), they fall short in
scenarios where the significance of classes varies or certain metrics should
reach prescribed levels. In this paper, we study two key classification
metrics, precision and recall, under three practical binary IC settings: fix
precision optimize recall (FPOR), fix recall optimize precision (FROP), and
optimize $F_\beta$-score (OFBS). Unlike existing methods that rely on smooth
approximations to deal with the indicator function involved, \textit{we
introduce, for the first time, exact constrained reformulations for these
direct metric optimization (DMO) problems}, which can be effectively solved by
exact penalty methods. Experiment results on multiple benchmark datasets
demonstrate the practical superiority of our approach over the state-of-the-art
methods for the three DMO problems. We also expect our exact reformulation and
optimization (ERO) framework to be applicable to a wide range of DMO problems
for binary IC and beyond. Our code is available at
https://github.com/sun-umn/DMO.

</details>


### [371] [Spatio-Temporal Demand Prediction for Food Delivery Using Attention-Driven Graph Neural Networks](https://arxiv.org/abs/2507.15246)
*Rabia Latief Bhat,Iqra Altaf Gillani*

Main category: cs.LG

TL;DR: The paper introduces an attention-based Graph Neural Network for accurate food delivery demand forecasting by modeling spatial-temporal dependencies in urban zones.


<details>
  <summary>Details</summary>
Motivation: Accurate demand forecasting is essential for optimizing food delivery operations, which are impacted by spatial heterogeneity and temporal fluctuations in order volumes.

Method: An attention-based Graph Neural Network models delivery zones as nodes and spatial proximity/order flows as edges, dynamically weighing neighboring influences and learning temporal trends.

Result: Experiments on real-world datasets show the model's high accuracy in forecasting order volumes, outperforming existing methods.

Conclusion: The framework provides a scalable, adaptive solution for proactive fleet positioning and resource allocation in urban food delivery.

Abstract: Accurate demand forecasting is critical for enhancing the efficiency and
responsiveness of food delivery platforms, where spatial heterogeneity and
temporal fluctuations in order volumes directly influence operational
decisions. This paper proposes an attention-based Graph Neural Network
framework that captures spatial-temporal dependencies by modeling the food
delivery environment as a graph. In this graph, nodes represent urban delivery
zones, while edges reflect spatial proximity and inter-regional order flow
patterns derived from historical data. The attention mechanism dynamically
weighs the influence of neighboring zones, enabling the model to focus on the
most contextually relevant areas during prediction. Temporal trends are jointly
learned alongside spatial interactions, allowing the model to adapt to evolving
demand patterns. Extensive experiments on real-world food delivery datasets
demonstrate the superiority of the proposed model in forecasting future order
volumes with high accuracy. The framework offers a scalable and adaptive
solution to support proactive fleet positioning, resource allocation, and
dispatch optimization in urban food delivery operations.

</details>


### [372] [CHORDS: Diffusion Sampling Accelerator with Multi-core Hierarchical ODE Solvers](https://arxiv.org/abs/2507.15260)
*Jiaqi Han,Haotian Ye,Puheng Li,Minkai Xu,James Zou,Stefano Ermon*

Main category: cs.LG

TL;DR: CHORDS is a training-free, model-agnostic acceleration strategy for diffusion-based generative models using multi-core parallelism, achieving up to 2.9x speedup without quality loss.


<details>
  <summary>Details</summary>
Motivation: Existing acceleration techniques for diffusion models either require retraining or degrade sample quality, limiting their practicality.

Method: CHORDS leverages multi-core parallelism, treating diffusion sampling as an ODE solver pipeline where slower solvers rectify faster ones via inter-core communication.

Result: CHORDS achieves up to 2.1x speedup with four cores and 2.9x with eight cores, outperforming baselines by 50% without quality degradation.

Conclusion: CHORDS provides a foundation for real-time, high-fidelity diffusion generation, addressing computational bottlenecks without compromising quality.

Abstract: Diffusion-based generative models have become dominant generators of
high-fidelity images and videos but remain limited by their computationally
expensive inference procedures. Existing acceleration techniques either require
extensive model retraining or compromise significantly on sample quality. This
paper explores a general, training-free, and model-agnostic acceleration
strategy via multi-core parallelism. Our framework views multi-core diffusion
sampling as an ODE solver pipeline, where slower yet accurate solvers
progressively rectify faster solvers through a theoretically justified
inter-core communication mechanism. This motivates our multi-core training-free
diffusion sampling accelerator, CHORDS, which is compatible with various
diffusion samplers, model architectures, and modalities. Through extensive
experiments, CHORDS significantly accelerates sampling across diverse
large-scale image and video diffusion models, yielding up to 2.1x speedup with
four cores, improving by 50% over baselines, and 2.9x speedup with eight cores,
all without quality degradation. This advancement enables CHORDS to establish a
solid foundation for real-time, high-fidelity diffusion generation.

</details>


### [373] [Temporal Basis Function Models for Closed-Loop Neural Stimulation](https://arxiv.org/abs/2507.15274)
*Matthew J. Bryan,Felix Schwock,Azadeh Yazdan-Shahmorad,Rajesh P N Rao*

Main category: cs.LG

TL;DR: The paper proposes temporal basis function models (TBFMs) for efficient, low-latency closed-loop neural stimulation, demonstrating their effectiveness in predicting and controlling neural activity in non-human primates.


<details>
  <summary>Details</summary>
Motivation: To address translational challenges like sample efficiency, training time, and latency in AI-driven closed-loop neural stimulation for neurological diseases like Parkinson's.

Method: Uses TBFMs for single-trial, spatiotemporal forward prediction of optogenetic stimulation effects on local field potentials (LFPs) in non-human primates. Also employs simulations for closed-loop control.

Result: TBFMs are sample-efficient, fast to train (2-4min), and low-latency (0.2ms). They achieve prediction accuracy comparable to slower baseline models and successfully control neural circuits in simulations.

Conclusion: TBFMs bridge the gap between complex AI models and practical, clinically useful closed-loop stimulation protocols.

Abstract: Closed-loop neural stimulation provides novel therapies for neurological
diseases such as Parkinson's disease (PD), but it is not yet clear whether
artificial intelligence (AI) techniques can tailor closed-loop stimulation to
individual patients or identify new therapies. Progress requires us to address
a number of translational issues, including sample efficiency, training time,
and minimizing loop latency such that stimulation may be shaped in response to
changing brain activity. We propose temporal basis function models (TBFMs) to
address these difficulties, and explore this approach in the context of
excitatory optogenetic stimulation. We demonstrate the ability of TBF models to
provide a single-trial, spatiotemporal forward prediction of the effect of
optogenetic stimulation on local field potentials (LFPs) measured in two
non-human primates. We further use simulations to demonstrate the use of TBF
models for closed-loop stimulation, driving neural activity towards target
patterns. The simplicity of TBF models allow them to be sample efficient, rapid
to train (2-4min), and low latency (0.2ms) on desktop CPUs. We demonstrate the
model on 40 sessions of previously published excitatory optogenetic stimulation
data. For each session, the model required 15-20min of data collection to
successfully model the remainder of the session. It achieved a prediction
accuracy comparable to a baseline nonlinear dynamical systems model that
requires hours to train, and superior accuracy to a linear state-space model.
In our simulations, it also successfully allowed a closed-loop stimulator to
control a neural circuit. Our approach begins to bridge the translational gap
between complex AI-based approaches to modeling dynamical systems and the
vision of using such forward prediction models to develop novel, clinically
useful closed-loop stimulation protocols.

</details>


### [374] [Machine Unlearning for Streaming Forgetting](https://arxiv.org/abs/2507.15280)
*Shaofei Shen,Chenhao Zhang,Yawen Zhao,Alina Bialkowski,Weitong Chen,Miao Xu*

Main category: cs.LG

TL;DR: The paper introduces a streaming unlearning paradigm to address inefficiencies in existing machine unlearning methods when handling streaming data removal requests.


<details>
  <summary>Details</summary>
Motivation: Current machine unlearning methods are inefficient for streaming data removal requests, which are common in practice but under-researched.

Method: The authors formalize unlearning as a distribution shift problem, estimate the altered distribution, and propose a novel streaming unlearning algorithm that avoids accessing original training data.

Result: Theoretical analysis shows an $O(\sqrt{T} + V_T)$ error bound on streaming unlearning regret, validated by experiments on various models and datasets.

Conclusion: The proposed method effectively addresses streaming unlearning challenges, offering theoretical guarantees and practical performance.

Abstract: Machine unlearning aims to remove knowledge of the specific training data in
a well-trained model. Currently, machine unlearning methods typically handle
all forgetting data in a single batch, removing the corresponding knowledge all
at once upon request. However, in practical scenarios, requests for data
removal often arise in a streaming manner rather than in a single batch,
leading to reduced efficiency and effectiveness in existing methods. Such
challenges of streaming forgetting have not been the focus of much research. In
this paper, to address the challenges of performance maintenance, efficiency,
and data access brought about by streaming unlearning requests, we introduce a
streaming unlearning paradigm, formalizing the unlearning as a distribution
shift problem. We then estimate the altered distribution and propose a novel
streaming unlearning algorithm to achieve efficient streaming forgetting
without requiring access to the original training data. Theoretical analyses
confirm an $O(\sqrt{T} + V_T)$ error bound on the streaming unlearning regret,
where $V_T$ represents the cumulative total variation in the optimal solution
over $T$ learning rounds. This theoretical guarantee is achieved under mild
conditions without the strong restriction of convex loss function. Experiments
across various models and datasets validate the performance of our proposed
method.

</details>


### [375] [Mixture of Autoencoder Experts Guidance using Unlabeled and Incomplete Data for Exploration in Reinforcement Learning](https://arxiv.org/abs/2507.15287)
*Elias Malomgré,Pieter Simoens*

Main category: cs.LG

TL;DR: A framework for RL agents to learn from incomplete expert demonstrations by transforming state-expert similarity into shaped intrinsic rewards, enabling robust exploration in sparse/dense reward environments.


<details>
  <summary>Details</summary>
Motivation: Addresses the challenge of learning from reward-free interactions and imperfect demonstrations in RL, aiming to improve agent adaptability in real-world settings.

Method: Uses a mapping function to convert state-expert similarity into intrinsic rewards and employs a Mixture of Autoencoder Experts to handle diverse behaviors and missing data.

Result: Demonstrates robust exploration and strong performance in various reward environments, even with sparse or incomplete demonstrations.

Conclusion: Provides a practical solution for RL in realistic scenarios where optimal data and precise reward control are lacking.

Abstract: Recent trends in Reinforcement Learning (RL) highlight the need for agents to
learn from reward-free interactions and alternative supervision signals, such
as unlabeled or incomplete demonstrations, rather than relying solely on
explicit reward maximization. Additionally, developing generalist agents that
can adapt efficiently in real-world environments often requires leveraging
these reward-free signals to guide learning and behavior. However, while
intrinsic motivation techniques provide a means for agents to seek out novel or
uncertain states in the absence of explicit rewards, they are often challenged
by dense reward environments or the complexity of high-dimensional state and
action spaces. Furthermore, most existing approaches rely directly on the
unprocessed intrinsic reward signals, which can make it difficult to shape or
control the agent's exploration effectively. We propose a framework that can
effectively utilize expert demonstrations, even when they are incomplete and
imperfect. By applying a mapping function to transform the similarity between
an agent's state and expert data into a shaped intrinsic reward, our method
allows for flexible and targeted exploration of expert-like behaviors. We
employ a Mixture of Autoencoder Experts to capture a diverse range of behaviors
and accommodate missing information in demonstrations. Experiments show our
approach enables robust exploration and strong performance in both sparse and
dense reward environments, even when demonstrations are sparse or incomplete.
This provides a practical framework for RL in realistic settings where optimal
data is unavailable and precise reward control is needed.

</details>


### [376] [Preferential subspace identification (PSID) with forward-backward smoothing](https://arxiv.org/abs/2507.15288)
*Omid G. Sani,Maryam M. Shanechi*

Main category: cs.LG

TL;DR: The paper extends Preferential Subspace Identification (PSID) to include optimal filtering and smoothing for better estimation in offline applications, validated on simulated data.


<details>
  <summary>Details</summary>
Motivation: Current PSID methods focus on prediction using past data, but incorporating concurrent or all data (filtering/smoothing) can improve estimation.

Method: Extends PSID with a reduced-rank regression step for optimal filtering and introduces a forward-backward smoothing algorithm.

Result: Validated on simulations, the method recovers ground-truth parameters and achieves optimal decoding performance matching the true model.

Conclusion: Provides a principled framework for optimal filtering and smoothing in two-signal settings, enhancing analysis of multivariate time-series.

Abstract: System identification methods for multivariate time-series, such as neural
and behavioral recordings, have been used to build models for predicting one
from the other. For example, Preferential Subspace Identification (PSID) builds
a state-space model of a primary time-series (e.g., neural activity) to
optimally predict a secondary time-series (e.g., behavior). However, PSID
focuses on optimal prediction using past primary data, even though in offline
applications, better estimation can be achieved by incorporating concurrent
data (filtering) or all available data (smoothing). Here, we extend PSID to
enable optimal filtering and smoothing. First, we show that the presence of a
secondary signal makes it possible to uniquely identify a model with an optimal
Kalman update step (to enable filtering) from a family of otherwise equivalent
state-space models. Our filtering solution augments PSID with a reduced-rank
regression step that directly learns the optimal gain required for the update
step from data. We refer to this extension of PSID as PSID with filtering.
Second, inspired by two-filter Kalman smoother formulations, we develop a novel
forward-backward PSID smoothing algorithm where we first apply PSID with
filtering and then apply it again in the reverse time direction on the
residuals of the filtered secondary signal. We validate our methods on
simulated data, showing that our approach recovers the ground-truth model
parameters for filtering, and achieves optimal filtering and smoothing decoding
performance of the secondary signal that matches the ideal performance of the
true underlying model. This work provides a principled framework for optimal
linear filtering and smoothing in the two-signal setting, significantly
expanding the toolkit for analyzing dynamic interactions in multivariate
time-series.

</details>


### [377] [Feel-Good Thompson Sampling for Contextual Bandits: a Markov Chain Monte Carlo Showdown](https://arxiv.org/abs/2507.15290)
*Emile Anand,Sarah Liaw*

Main category: cs.LG

TL;DR: FG-TS and SFG-TS are studied for their performance with exact and approximate posteriors in contextual bandits, showing trade-offs in bonus scaling and robustness.


<details>
  <summary>Details</summary>
Motivation: Address the lack of aggressive exploration in Thompson Sampling (TS) for high-dimensional problems by introducing optimism bonuses in FG-TS.

Method: Systematic evaluation of FG-TS and SFG-TS across 11 benchmarks, comparing exact and approximate posteriors, with ablations on preconditioning, bonus scale, and prior strength.

Result: FG-TS outperforms vanilla TS in linear/logistic bandits but struggles in neural bandits; trade-offs exist between bonus size and sampling noise.

Conclusion: FG-TS and variants are recommended as baselines due to competitive performance and ease of use, with code provided for reproducibility.

Abstract: Thompson Sampling (TS) is widely used to address the exploration/exploitation
tradeoff in contextual bandits, yet recent theory shows that it does not
explore aggressively enough in high-dimensional problems. Feel-Good Thompson
Sampling (FG-TS) addresses this by adding an optimism bonus that biases toward
high-reward models, and it achieves the asymptotically minimax-optimal regret
in the linear setting when posteriors are exact. However, its performance with
\emph{approximate} posteriors -- common in large-scale or neural problems --
has not been benchmarked. We provide the first systematic study of FG-TS and
its smoothed variant (SFG-TS) across eleven real-world and synthetic
benchmarks. To evaluate their robustness, we compare performance across
settings with exact posteriors (linear and logistic bandits) to approximate
regimes produced by fast but coarse stochastic-gradient samplers. Ablations
over preconditioning, bonus scale, and prior strength reveal a trade-off:
larger bonuses help when posterior samples are accurate, but hurt when sampling
noise dominates. FG-TS generally outperforms vanilla TS in linear and logistic
bandits, but tends to be weaker in neural bandits. Nevertheless, because FG-TS
and its variants are competitive and easy-to-use, we recommend them as
baselines in modern contextual-bandit benchmarks. Finally, we provide source
code for all our experiments in
https://github.com/SarahLiaw/ctx-bandits-mcmc-showdown.

</details>


### [378] [Universal crystal material property prediction via multi-view geometric fusion in graph transformers](https://arxiv.org/abs/2507.15303)
*Liang Zhang,Kong Chen,Yuen Wu*

Main category: cs.LG

TL;DR: MGT, a multi-view graph transformer, improves crystal property prediction by fusing SE3 and SO3 representations, reducing errors by up to 21% and showing strong transfer learning performance.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle to capture the geometric and topological complexity of crystal structures, limiting machine learning applications in materials science.

Method: MGT combines SE3 invariant and SO3 equivariant graph representations using a mixture of experts router for adaptive weighting. Multi-task self-supervised pretraining enhances performance.

Result: MGT reduces mean absolute error by up to 21% and achieves up to 58% improvement in transfer learning tasks like catalyst adsorption energy and perovskite bandgap prediction.

Conclusion: MGT is a versatile and effective framework for crystal property prediction, aiding in novel material discovery.

Abstract: Accurately and comprehensively representing crystal structures is critical
for advancing machine learning in large-scale crystal materials simulations,
however, effectively capturing and leveraging the intricate geometric and
topological characteristics of crystal structures remains a core, long-standing
challenge for most existing methods in crystal property prediction. Here, we
propose MGT, a multi-view graph transformer framework that synergistically
fuses SE3 invariant and SO3 equivariant graph representations, which
respectively captures rotation-translation invariance and rotation equivariance
in crystal geometries. To strategically incorporate these complementary
geometric representations, we employ a lightweight mixture of experts router in
MGT to adaptively adjust the weight assigned to SE3 and SO3 embeddings based on
the specific target task. Compared with previous state-of-the-art models, MGT
reduces the mean absolute error by up to 21% on crystal property prediction
tasks through multi-task self-supervised pretraining. Ablation experiments and
interpretable investigations confirm the effectiveness of each technique
implemented in our framework. Additionally, in transfer learning scenarios
including crystal catalyst adsorption energy and hybrid perovskite bandgap
prediction, MGT achieves performance improvements of up to 58% over existing
baselines, demonstrating domain-agnostic scalability across diverse application
domains. As evidenced by the above series of studies, we believe that MGT can
serve as useful model for crystal material property prediction, providing a
valuable tool for the discovery of novel materials.

</details>


### [379] [Beyond Model Base Selection: Weaving Knowledge to Master Fine-grained Neural Network Design](https://arxiv.org/abs/2507.15336)
*Jialiang Wang,Hanmo Liu,Shimin Di,Zhili Wang,Jiachuan Wang,Lei Chen,Xiaofang Zhou*

Main category: cs.LG

TL;DR: M-DESIGN introduces a model knowledge base (MKB) pipeline for adaptive neural network refinement, addressing the gap in static model selection by leveraging relational dependencies and iterative refinement.


<details>
  <summary>Details</summary>
Motivation: Traditional static model selection in databases overlooks fine-grained relational dependencies between tasks and model architectures, leading to suboptimal matches. M-DESIGN aims to fill this gap by enabling adaptive refinement.

Method: M-DESIGN uses a knowledge weaving engine to reframe model refinement as an adaptive query problem, leveraging a graph-relational schema for fine-grained analytics and predictive query planning.

Result: Empirical results show M-DESIGN delivers optimal models in 26 of 33 data-task pairs within limited budgets.

Conclusion: M-DESIGN effectively bridges the model refinement gap in database research by enabling adaptive, fine-grained model selection and refinement.

Abstract: Database systems have recently advocated for embedding machine learning (ML)
capabilities, offering declarative model queries over large, managed model
repositories, thereby circumventing the huge computational overhead of
traditional ML-based algorithms in automated neural network model selection.
Pioneering database studies aim to organize existing benchmark repositories as
model bases (MB), querying them for the model records with the highest
performance estimation metrics for given tasks. However, this static model
selection practice overlooks the fine-grained, evolving relational dependencies
between diverse task queries and model architecture variations, resulting in
suboptimal matches and failing to further refine the model effectively. To fill
the model refinement gap in database research, we propose M-DESIGN, a curated
model knowledge base (MKB) pipeline for mastering neural network refinement by
adaptively weaving prior insights about model architecture modification. First,
we propose a knowledge weaving engine that reframes model refinement as an
adaptive query problem over task metadata. Given a user's task query, M-DESIGN
quickly matches and iteratively refines candidate models by leveraging a
graph-relational knowledge schema that explicitly encodes data properties,
architecture variations, and pairwise performance deltas as joinable relations.
This schema supports fine-grained relational analytics over architecture tweaks
and drives a predictive query planner that can detect and adapt to
out-of-distribution (OOD) tasks. We instantiate M-DESIGN for graph analytics
tasks, where our model knowledge base enriches existing benchmarks with
structured metadata covering 3 graph tasks and 22 graph datasets, contributing
data records of 67,760 graph models. Empirical results demonstrate that
M-DESIGN delivers the optimal model in 26 of 33 data-task pairs within limited
budgets.

</details>


### [380] [Scaling Decentralized Learning with FLock](https://arxiv.org/abs/2507.15349)
*Zehua Cheng,Rui Sun,Jiahao Sun,Yike Guo*

Main category: cs.LG

TL;DR: FLock is a decentralized framework for secure LLM fine-tuning, replacing the central server with blockchain and economic incentives, achieving robust defense against attacks and superior cross-domain performance.


<details>
  <summary>Details</summary>
Motivation: Centralized control and overhead in decentralized schemes hinder LLM fine-tuning, while standard FL is vulnerable to attacks. FLock addresses these issues for large models in trustless environments.

Method: FLock integrates a blockchain-based trust layer with economic incentives, enabling secure collaboration among untrusted parties for LLM fine-tuning.

Result: FLock reduces adversarial attack success rates by >68% and outperforms isolated models in cross-domain generalization.

Conclusion: FLock provides a scalable, secure solution for decentralized LLM fine-tuning, enhancing robustness and knowledge transfer.

Abstract: Fine-tuning the large language models (LLMs) are prevented by the deficiency
of centralized control and the massive computing and communication overhead on
the decentralized schemes. While the typical standard federated learning (FL)
supports data privacy, the central server requirement creates a single point of
attack and vulnerability to poisoning attacks. Generalizing the result in this
direction to 70B-parameter models in the heterogeneous, trustless environments
has turned out to be a huge, yet unbroken bottleneck. This paper introduces
FLock, a decentralized framework for secure and efficient collaborative LLM
fine-tuning. Integrating a blockchain-based trust layer with economic
incentives, FLock replaces the central aggregator with a secure, auditable
protocol for cooperation among untrusted parties. We present the first
empirical validation of fine-tuning a 70B LLM in a secure, multi-domain,
decentralized setting. Our experiments show the FLock framework defends against
backdoor poisoning attacks that compromise standard FL optimizers and fosters
synergistic knowledge transfer. The resulting models show a >68% reduction in
adversarial attack success rates. The global model also demonstrates superior
cross-domain generalization, outperforming models trained in isolation on their
own specialized data.

</details>


### [381] [To Label or Not to Label: PALM -- A Predictive Model for Evaluating Sample Efficiency in Active Learning Models](https://arxiv.org/abs/2507.15381)
*Julia Machnio,Mads Nielsen,Mostafa Mehdipour Ghazi*

Main category: cs.LG

TL;DR: PALM is a unified model for analyzing active learning (AL) trajectories, predicting performance and enabling strategy comparisons.


<details>
  <summary>Details</summary>
Motivation: Traditional AL evaluation focuses only on final accuracy, missing the learning process dynamics. PALM addresses this gap.

Method: PALM characterizes AL through four parameters: achievable accuracy, coverage efficiency, early-stage performance, and scalability.

Result: Validated on CIFAR and ImageNet datasets, PALM generalizes well, predicting learning curves and revealing AL insights.

Conclusion: PALM enables systematic, reproducible AL evaluation, aiding strategy selection and performance prediction under budget constraints.

Abstract: Active learning (AL) seeks to reduce annotation costs by selecting the most
informative samples for labeling, making it particularly valuable in
resource-constrained settings. However, traditional evaluation methods, which
focus solely on final accuracy, fail to capture the full dynamics of the
learning process. To address this gap, we propose PALM (Performance Analysis of
Active Learning Models), a unified and interpretable mathematical model that
characterizes AL trajectories through four key parameters: achievable accuracy,
coverage efficiency, early-stage performance, and scalability. PALM provides a
predictive description of AL behavior from partial observations, enabling the
estimation of future performance and facilitating principled comparisons across
different strategies. We validate PALM through extensive experiments on
CIFAR-10/100 and ImageNet-50/100/200, covering a wide range of AL methods and
self-supervised embeddings. Our results demonstrate that PALM generalizes
effectively across datasets, budgets, and strategies, accurately predicting
full learning curves from limited labeled data. Importantly, PALM reveals
crucial insights into learning efficiency, data space coverage, and the
scalability of AL methods. By enabling the selection of cost-effective
strategies and predicting performance under tight budget constraints, PALM lays
the basis for more systematic, reproducible, and data-efficient evaluation of
AL in both research and real-world applications. The code is available at:
https://github.com/juliamachnio/PALM.

</details>


### [382] [Learning to Gridize: Segment Physical World by Wireless Communication Channel](https://arxiv.org/abs/2507.15386)
*Juntao Wang,Feng Yin,Tian Ding,Tsung-Hui Chang,Zhi-Quan Luo,Qi Yan*

Main category: cs.LG

TL;DR: The paper introduces Channel Space Gridization (CSG), a novel framework for grid-based network optimization, overcoming limitations of existing methods by jointly estimating channel properties and partitioning space using only beam-level RSRP data.


<details>
  <summary>Details</summary>
Motivation: Existing gridization methods (GSG or BSG) rely on unavailable location data or flawed assumptions about channel properties. CSG aims to unify channel estimation and gridization for more accurate and efficient network optimization.

Method: CSG uses a joint optimization approach with beam-level RSRP to estimate Channel Angle Power Spectra (CAPS) and partition samples into grids. The CSG Autoencoder (CSG-AE) is developed, featuring an RSRP-to-CAPS encoder, a learnable quantizer, and a physics-informed decoder. A novel PIDA training scheme ensures stable training.

Result: CSG-AE outperforms baselines in CAPS estimation and clustering quality on synthetic data. On real-world datasets, it reduces Active MAE by 30% and Overall MAE by 65% in RSRP prediction, while improving channel consistency and cluster balance.

Conclusion: CSG advances gridization for large-scale network optimization by addressing limitations of existing methods, demonstrating superior performance in accuracy and efficiency.

Abstract: Gridization, the process of partitioning space into grids where users share
similar channel characteristics, serves as a fundamental prerequisite for
efficient large-scale network optimization. However, existing methods like
Geographical or Beam Space Gridization (GSG or BSG) are limited by reliance on
unavailable location data or the flawed assumption that similar signal
strengths imply similar channel properties. We propose Channel Space
Gridization (CSG), a pioneering framework that unifies channel estimation and
gridization for the first time. Formulated as a joint optimization problem, CSG
uses only beam-level reference signal received power (RSRP) to estimate Channel
Angle Power Spectra (CAPS) and partition samples into grids with homogeneous
channel characteristics. To perform CSG, we develop the CSG Autoencoder
(CSG-AE), featuring a trainable RSRP-to-CAPS encoder, a learnable sparse
codebook quantizer, and a physics-informed decoder based on the Localized
Statistical Channel Model. On recognizing the limitations of naive training
scheme, we propose a novel Pretraining-Initialization-Detached-Asynchronous
(PIDA) training scheme for CSG-AE, ensuring stable and effective training by
systematically addressing the common pitfalls of the naive training paradigm.
Evaluations reveal that CSG-AE excels in CAPS estimation accuracy and
clustering quality on synthetic data. On real-world datasets, it reduces Active
Mean Absolute Error (MAE) by 30\% and Overall MAE by 65\% on RSRP prediction
accuracy compared to salient baselines using the same data, while improving
channel consistency, cluster sizes balance, and active ratio, advancing the
development of gridization for large-scale network optimization.

</details>


### [383] [MAP Estimation with Denoisers: Convergence Rates and Guarantees](https://arxiv.org/abs/2507.15397)
*Scott Pesme,Giacomo Meanti,Michael Arbel,Julien Mairal*

Main category: cs.LG

TL;DR: The paper provides theoretical justification for using denoiser models as surrogates for proximal operators in MAP optimization, proving convergence under log-concavity assumptions.


<details>
  <summary>Details</summary>
Motivation: Denoiser models are widely used in practice as surrogates for proximal operators in MAP optimization, but lack theoretical backing. This work aims to bridge that gap.

Method: The authors propose a simple algorithm related to practical methods, interpreting it as gradient descent on smoothed proximal objectives.

Result: The algorithm provably converges to the proximal operator under log-concavity assumptions on the prior distribution.

Conclusion: The study offers a theoretical foundation for empirically successful but heuristic denoiser-based methods in inverse problems.

Abstract: Denoiser models have become powerful tools for inverse problems, enabling the
use of pretrained networks to approximate the score of a smoothed prior
distribution. These models are often used in heuristic iterative schemes aimed
at solving Maximum a Posteriori (MAP) optimisation problems, where the proximal
operator of the negative log-prior plays a central role. In practice, this
operator is intractable, and practitioners plug in a pretrained denoiser as a
surrogate-despite the lack of general theoretical justification for this
substitution. In this work, we show that a simple algorithm, closely related to
several used in practice, provably converges to the proximal operator under a
log-concavity assumption on the prior $p$. We show that this algorithm can be
interpreted as a gradient descent on smoothed proximal objectives. Our analysis
thus provides a theoretical foundation for a class of empirically successful
but previously heuristic methods.

</details>


### [384] [The calculus of variations of the Transformer on the hyperspherical tangent bundle](https://arxiv.org/abs/2507.15431)
*Andrew Gracyk*

Main category: cs.LG

TL;DR: The paper provides a theoretical framework for Transformers using Lagrangian optimization and calculus of variations, treating them as flow maps on a high-dimensional unit sphere.


<details>
  <summary>Details</summary>
Motivation: To establish a mathematical foundation for Transformers by linking them to Lagrangian mechanics and variational calculus, addressing gaps in existing literature.

Method: Develops a functional for the Transformer's continuous flow map, derives the Euler-Lagrange equation, and applies calculus of variations on manifolds.

Result: The Transformer is shown to solve a variational problem, with new insights into its dynamics and applicability in neural scenarios.

Conclusion: The work lays the groundwork for further research in variational methods for Transformers, offering novel theoretical tools and proofs.

Abstract: We offer a theoretical mathematical background to Transformers through
Lagrangian optimization across the token space. The Transformer, as a flow map,
exists in the tangent fiber for each token along the high-dimensional unit
sphere. The circumstance of the hypersphere across the latent data is
reasonable due to the trained diagonal matrix equal to the identity, which has
various empirical justifications. Thus, under the continuum limit of the
dynamics, the latent vectors flow among the tangent bundle. Using these facts,
we devise a mathematical framework for the Transformer through calculus of
variations. We develop a functional and show that the continuous flow map
induced by the Transformer satisfies this functional, therefore the Transformer
can be viewed as a natural solver of a calculus of variations problem. We
invent new scenarios of when our methods are applicable based on loss
optimization with respect to path optimality. We derive the Euler-Lagrange
equation for the Transformer. The variant of the Euler-Lagrange equation we
present has various appearances in literature, but, to our understanding,
oftentimes not foundationally proven or under other specialized cases. Our
overarching proof is new: our techniques are classical and the use of the flow
map object is original. We provide several other relevant results, primarily
ones specific to neural scenarios. In particular, much of our analysis will be
attempting to quantify Transformer data in variational contexts under neural
approximations. Calculus of variations on manifolds is a well-nourished
research area, but for the Transformer specifically, it is uncharted: we lay
the foundation for this area through an introduction to the Lagrangian for the
Transformer.

</details>


### [385] [An Adaptive Random Fourier Features approach Applied to Learning Stochastic Differential Equations](https://arxiv.org/abs/2507.15442)
*Owen Douglas,Aku Kammonen,Anamika Pandey,Raúl Tempone*

Main category: cs.LG

TL;DR: Proposes an adaptive random Fourier features (ARFF) method with Metropolis sampling for learning stochastic differential equations, outperforming conventional methods.


<details>
  <summary>Details</summary>
Motivation: To improve the learning of drift and diffusion components in stochastic differential equations from snapshot data.

Method: Uses ARFF with Metropolis sampling and resampling, along with a likelihood-based loss function derived from Euler-Maruyama integration.

Result: ARFF matches or surpasses Adam-based optimization in loss minimization and convergence speed across benchmark problems.

Conclusion: ARFF is a promising alternative for data-driven modeling of stochastic dynamics.

Abstract: This work proposes a training algorithm based on adaptive random Fourier
features (ARFF) with Metropolis sampling and resampling
\cite{kammonen2024adaptiverandomfourierfeatures} for learning drift and
diffusion components of stochastic differential equations from snapshot data.
Specifically, this study considers It\^{o} diffusion processes and a
likelihood-based loss function derived from the Euler-Maruyama integration
introduced in \cite{Dietrich2023} and
\cite{dridi2021learningstochasticdynamicalsystems}.
  This work evaluates the proposed method against benchmark problems presented
in \cite{Dietrich2023}, including polynomial examples, underdamped Langevin
dynamics, a stochastic susceptible-infected-recovered model, and a stochastic
wave equation. Across all cases, the ARFF-based approach matches or surpasses
the performance of conventional Adam-based optimization in both loss
minimization and convergence speed. These results highlight the potential of
ARFF as a compelling alternative for data-driven modeling of stochastic
dynamics.

</details>


### [386] [FedMultiEmo: Real-Time Emotion Recognition via Multimodal Federated Learning](https://arxiv.org/abs/2507.15470)
*Baran Can Gül,Suraksha Nadig,Stefanos Tziampazis,Nasser Jazdi,Michael Weyrich*

Main category: cs.LG

TL;DR: FedMultiEmo is a privacy-preserving framework for in-vehicle emotion recognition, combining visual and physiological data via federated learning, achieving 87% accuracy while keeping data local.


<details>
  <summary>Details</summary>
Motivation: Challenges like modality fragility, physiological variability, and privacy risks hinder practical deployment of in-vehicle emotion recognition systems.

Method: FedMultiEmo uses a multimodal federated learning pipeline with majority-vote fusion, an edge-to-cloud prototype, and personalized Federated Averaging.

Result: The system achieves 77% accuracy for visual data, 74% for physiological data, and 87% when fused, matching centralized performance with local data storage.

Conclusion: FedMultiEmo provides a practical, privacy-aware solution for real-time emotion recognition in automotive settings.

Abstract: In-vehicle emotion recognition underpins adaptive driver-assistance systems
and, ultimately, occupant safety. However, practical deployment is hindered by
(i) modality fragility - poor lighting and occlusions degrade vision-based
methods; (ii) physiological variability - heart-rate and skin-conductance
patterns differ across individuals; and (iii) privacy risk - centralized
training requires transmission of sensitive data. To address these challenges,
we present FedMultiEmo, a privacy-preserving framework that fuses two
complementary modalities at the decision level: visual features extracted by a
Convolutional Neural Network from facial images, and physiological cues (heart
rate, electrodermal activity, and skin temperature) classified by a Random
Forest. FedMultiEmo builds on three key elements: (1) a multimodal federated
learning pipeline with majority-vote fusion, (2) an end-to-end edge-to-cloud
prototype on Raspberry Pi clients and a Flower server, and (3) a personalized
Federated Averaging scheme that weights client updates by local data volume.
Evaluated on FER2013 and a custom physiological dataset, the federated
Convolutional Neural Network attains 77% accuracy, the Random Forest 74%, and
their fusion 87%, matching a centralized baseline while keeping all raw data
local. The developed system converges in 18 rounds, with an average round time
of 120 seconds and a per-client memory footprint below 200 MB. These results
indicate that FedMultiEmo offers a practical approach to real-time,
privacy-aware emotion recognition in automotive settings.

</details>


### [387] [Off-Policy Corrected Reward Modeling for Reinforcement Learning from Human Feedback](https://arxiv.org/abs/2507.15507)
*Johannes Ackermann,Takashi Ishida,Masashi Sugiyama*

Main category: cs.LG

TL;DR: The paper addresses overoptimization in RLHF for LMs by proposing Off-Policy Corrected Reward Modeling (OCRM), which improves reward model accuracy without new labels.


<details>
  <summary>Details</summary>
Motivation: Overoptimization in RLHF causes reward models to become inaccurate as LM responses diverge from training data, leading to mismatched human preferences.

Method: Proposes OCRM, which uses importance weighting to iteratively correct the reward model off-policy, avoiding new labels or samples.

Result: OCRM outperforms standard RLHF methods in experiments on summarization and chatbot datasets.

Conclusion: OCRM effectively mitigates overoptimization, improving reward model accuracy and final policy performance.

Abstract: Reinforcement Learning from Human Feedback (RLHF) allows us to train models,
such as language models (LMs), to follow complex human preferences. In RLHF for
LMs, we first train an LM using supervised fine-tuning, sample pairs of
responses, obtain human feedback, and use the resulting data to train a reward
model (RM). RL methods are then used to train the LM to maximize the reward
given by the RM. As training progresses, the responses generated by the LM no
longer resemble the responses seen by the RM during training, leading to the RM
becoming inaccurate. The score given by the RM keeps increasing, but the
learned behavior no longer matches the human preferences. This issue is known
as overoptimization. We investigate overoptimization from the point of view of
distribution shift and show that the shift results in an inconsistent estimate
of the RM parameters, leading to an inconsistent estimate of the policy
gradient. We propose Off-Policy Corrected Reward Modeling (OCRM), which
iteratively off-policy corrects the RM using importance weighting, without
requiring new labels or samples. This results in a more accurate RM, which
empirically leads to an improved final policy. We validate our approach in
experiments with summarization and chatbot datasets and show that it performs
significantly better than standard RLHF methods and baselines. Our
implementation is available at
https://github.com/JohannesAck/OffPolicyCorrectedRewardModeling

</details>


### [388] [An Investigation of Test-time Adaptation for Audio Classification under Background Noise](https://arxiv.org/abs/2507.15523)
*Weichuang Shao,Iman Yi Liao,Tomas Henrique Bode Maul,Tissa Chandesa*

Main category: cs.LG

TL;DR: The paper addresses domain shift in audio classification using Test-Time Adaptation (TTA), comparing methods like TTT, TENT, and a modified CoNMix, which outperforms others in noisy conditions.


<details>
  <summary>Details</summary>
Motivation: Domain shift degrades model performance; this study aims to improve audio classification under noise-induced domain shift using TTA, a novel approach in this context.

Method: Adopts TTA methods (TTT, TENT, CoNMix) and tests them on AudioMNIST and SpeechCommands V1 datasets under varying noise conditions.

Result: Modified CoNMix achieves the lowest error rates (5.31% at 10 dB, 12.75% at 3 dB) compared to TTT and TENT.

Conclusion: This is the first study to apply TTA for audio classification under domain shift, demonstrating CoNMix's superiority in noisy environments.

Abstract: Domain shift is a prominent problem in Deep Learning, causing a model
pre-trained on a source dataset to suffer significant performance degradation
on test datasets. This research aims to address the issue of audio
classification under domain shift caused by background noise using Test-Time
Adaptation (TTA), a technique that adapts a pre-trained model during testing
using only unlabelled test data before making predictions. We adopt two common
TTA methods, TTT and TENT, and a state-of-the-art method CoNMix, and
investigate their respective performance on two popular audio classification
datasets, AudioMNIST (AM) and SpeechCommands V1 (SC), against different types
of background noise and noise severity levels. The experimental results reveal
that our proposed modified version of CoNMix produced the highest
classification accuracy under domain shift (5.31% error rate under 10 dB
exercise bike background noise and 12.75% error rate under 3 dB running tap
background noise for AM) compared to TTT and TENT. The literature search
provided no evidence of similar works, thereby motivating the work reported
here as the first study to leverage TTA techniques for audio classification
under domain shift.

</details>


### [389] [Data Aware Differentiable Neural Architecture Search for Tiny Keyword Spotting Applications](https://arxiv.org/abs/2507.15545)
*Yujia Shi,Emil Njor,Pablo Martínez-Nuevo,Sven Ewan Shepstone,Xenofon Fafoutis*

Main category: cs.LG

TL;DR: The paper introduces 'Data Aware Differentiable Neural Architecture Search' to simplify TinyML design by co-optimizing model architecture and data configuration, improving resource efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: The complexity of designing TinyML systems limits their adoption, despite their potential for resource-efficient ML.

Method: Expands Neural Architecture Search to include data configuration parameters, enabling joint optimization of architecture and data characteristics.

Result: Initial tests on keyword spotting show the method produces accurate yet resource-efficient TinyML systems.

Conclusion: The approach effectively balances performance and resource usage, advancing TinyML adoption.

Abstract: The success of Machine Learning is increasingly tempered by its significant
resource footprint, driving interest in efficient paradigms like TinyML.
However, the inherent complexity of designing TinyML systems hampers their
broad adoption. To reduce this complexity, we introduce "Data Aware
Differentiable Neural Architecture Search". Unlike conventional Differentiable
Neural Architecture Search, our approach expands the search space to include
data configuration parameters alongside architectural choices. This enables
Data Aware Differentiable Neural Architecture Search to co-optimize model
architecture and input data characteristics, effectively balancing resource
usage and system performance for TinyML applications. Initial results on
keyword spotting demonstrate that this novel approach to TinyML system design
can generate lean but highly accurate systems.

</details>


### [390] [The added value for MRI radiomics and deep-learning for glioblastoma prognostication compared to clinical and molecular information](https://arxiv.org/abs/2507.15548)
*D. Abler,O. Pusterla,A. Joye-Kühnis,N. Andratschke,M. Bach,A. Bink,S. M. Christ,P. Hagmann,B. Pouymayou,E. Pravatà,P. Radojewski,M. Reyes,L. Ruinelli,R. Schaer,B. Stieltjes,G. Treglia,W. Valenzuela,R. Wiest,S. Zoergiebel,M. Guckenberger,S. Tanadini-Lang,A. Depeursinge*

Main category: cs.LG

TL;DR: The study evaluates the added value of conventional radiomics (CR) and deep learning (DL) MRI radiomics for glioblastoma prognosis, finding minimal improvement over clinical predictors.


<details>
  <summary>Details</summary>
Motivation: To assess whether radiomics (CR and DL) provides significant added value over clinical and molecular predictors for glioblastoma prognosis.

Method: Analyzed 1152 glioblastoma patients from multiple centers, using CR and DL models with clinical, molecular, and MRI data. Evaluated performance on internal and external cohorts with different feature sets and patient subsets.

Result: Combined-feature CR models slightly outperformed clinical-only models (AUC 0.75 vs 0.74), but DL models showed no significant improvement. Imaging data had modest relevance for overall survival prediction.

Conclusion: Radiomics offers minimal added value over clinical predictors for glioblastoma prognosis, despite confirming the predictive role of MRI sequences.

Abstract: Background: Radiomics shows promise in characterizing glioblastoma, but its
added value over clinical and molecular predictors has yet to be proven. This
study assessed the added value of conventional radiomics (CR) and deep learning
(DL) MRI radiomics for glioblastoma prognosis (<= 6 vs > 6 months survival) on
a large multi-center dataset.
  Methods: After patient selection, our curated dataset gathers 1152
glioblastoma (WHO 2016) patients from five Swiss centers and one public source.
It included clinical (age, gender), molecular (MGMT, IDH), and baseline MRI
data (T1, T1 contrast, FLAIR, T2) with tumor regions. CR and DL models were
developed using standard methods and evaluated on internal and external
cohorts. Sub-analyses assessed models with different feature sets
(imaging-only, clinical/molecular-only, combined-features) and patient subsets
(S-1: all patients, S-2: with molecular data, S-3: IDH wildtype).
  Results: The best performance was observed in the full cohort (S-1). In
external validation, the combined-feature CR model achieved an AUC of 0.75,
slightly, but significantly outperforming clinical-only (0.74) and imaging-only
(0.68) models. DL models showed similar trends, though without statistical
significance. In S-2 and S-3, combined models did not outperform clinical-only
models. Exploratory analysis of CR models for overall survival prediction
suggested greater relevance of imaging data: across all subsets,
combined-feature models significantly outperformed clinical-only models, though
with a modest advantage of 2-4 C-index points.
  Conclusions: While confirming the predictive value of anatomical MRI
sequences for glioblastoma prognosis, this multi-center study found standard CR
and DL radiomics approaches offer minimal added value over demographic
predictors such as age and gender.

</details>


### [391] [PhysGym: Benchmarking LLMs in Interactive Physics Discovery with Controlled Priors](https://arxiv.org/abs/2507.15550)
*Yimeng Chen,Piotr Piȩkos,Mateusz Ostaszewski,Firas Laakom,Jürgen Schmidhuber*

Main category: cs.LG

TL;DR: PhysGym is a benchmark suite for evaluating LLM-based agents' scientific reasoning in physics, focusing on prior knowledge and problem complexity.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks lack the ability to assess LLM-based agents' scientific discovery capabilities, especially in varying environmental complexity and prior knowledge utilization.

Method: PhysGym introduces interactive physics simulations where agents probe environments, gather data, and hypothesize physical laws, with controlled prior knowledge levels.

Result: The benchmark differentiates LLM capabilities based on prior knowledge and task complexity, as demonstrated by baseline model results.

Conclusion: PhysGym fills a critical gap by providing a standardized platform for rigorous evaluation of LLM-based scientific reasoning.

Abstract: Evaluating the scientific discovery capabilities of large language model
based agents, particularly how they cope with varying environmental complexity
and utilize prior knowledge, requires specialized benchmarks currently lacking
in the landscape. To address this gap, we introduce PhysGym, a novel benchmark
suite and simulation platform for rigorously assessing LLM-based scientific
reasoning in interactive physics environments. PhysGym's primary contribution
lies in its sophisticated control over the level of prior knowledge provided to
the agent. This allows researchers to dissect agent performance along axes
including the complexity of the problem and the prior knowledge levels. The
benchmark comprises a suite of interactive simulations, where agents must
actively probe environments, gather data sequentially under constraints and
formulate hypotheses about underlying physical laws. PhysGym provides
standardized evaluation protocols and metrics for assessing hypothesis accuracy
and model fidelity. We demonstrate the benchmark's utility by presenting
results from baseline LLMs, showcasing its ability to differentiate
capabilities based on varying priors and task complexity.

</details>


### [392] [Trade-offs between elective surgery rescheduling and length-of-stay prediction accuracy](https://arxiv.org/abs/2507.15566)
*Pieter Smet,Martina Doneda,Ettore Lanzarone,Giuliana Carello*

Main category: cs.LG

TL;DR: The paper explores how the accuracy of machine learning predictions for patient length-of-stay (LOS) affects rescheduling flexibility in elective surgery planning, aiming to prevent bed overflows and optimize resource use.


<details>
  <summary>Details</summary>
Motivation: Downstream resource availability, like inpatient beds, is critical for elective surgery planning. Inaccurate LOS predictions can disrupt schedules, necessitating rescheduling strategies.

Method: The study evaluates the relationship between LOS prediction accuracy and rescheduling flexibility using simulated ML models and corrective policies.

Result: The research identifies effective rescheduling strategies to mitigate the impact of LOS prediction errors, balancing bed availability and resource utilization.

Conclusion: Accurate LOS predictions reduce rescheduling needs, but flexible policies can compensate for prediction errors, optimizing hospital resource management.

Abstract: The availability of downstream resources plays a critical role in planning
the admission of patients undergoing elective surgery, with inpatient beds
being one of the most crucial resources. When planning patient admissions,
predictions on their length-of-stay (LOS) made by machine learning (ML) models
are used to ensure bed availability. However, the actual LOS for each patient
may differ considerably from the predicted value, potentially making the
schedule infeasible. To address such infeasibilities, rescheduling strategies
that take advantage of operational flexibility can be implemented. For example,
adjustments may include postponing admission dates, relocating patients to
different wards, or even transferring patients who are already admitted. The
common assumption is that more accurate LOS predictions reduce the impact of
rescheduling. However, training ML models that can make such accurate
predictions can be costly. Building on previous work that proposed simulated
\ac{ml} for evaluating data-driven approaches, this paper explores the
relationship between LOS prediction accuracy and rescheduling flexibility
across various corrective policies. Specifically, we examine the most effective
patient rescheduling strategies under LOS prediction errors to prevent bed
overflows while optimizing resource utilization.

</details>


### [393] [On the Role of AI in Managing Satellite Constellations: Insights from the ConstellAI Project](https://arxiv.org/abs/2507.15574)
*Gregory F. Stock,Juan A. Fraire,Holger Hermanns,Jędrzej Mosiężny,Yusra Al-Khazraji,Julio Ramírez Molina,Evridiki V. Ntagiou*

Main category: cs.LG

TL;DR: AI-driven algorithms, particularly Reinforcement Learning (RL), outperform traditional methods in optimizing satellite mega-constellation operations, specifically in data routing and resource allocation.


<details>
  <summary>Details</summary>
Motivation: The rapid expansion of satellite constellations demands efficient, scalable, and resilient management solutions, which AI can provide.

Method: The ConstellAI project develops RL algorithms for data routing (improving latency) and resource allocation (optimizing task scheduling), tested in real-life scenarios.

Result: RL outperforms classical methods, offering better flexibility, scalability, and generalizability in decision-making.

Conclusion: AI can transform satellite constellation management by enabling adaptive, robust, and cost-effective solutions.

Abstract: The rapid expansion of satellite constellations in near-Earth orbits presents
significant challenges in satellite network management, requiring innovative
approaches for efficient, scalable, and resilient operations. This paper
explores the role of Artificial Intelligence (AI) in optimizing the operation
of satellite mega-constellations, drawing from the ConstellAI project funded by
the European Space Agency (ESA). A consortium comprising GMV GmbH, Saarland
University, and Thales Alenia Space collaborates to develop AI-driven
algorithms and demonstrates their effectiveness over traditional methods for
two crucial operational challenges: data routing and resource allocation. In
the routing use case, Reinforcement Learning (RL) is used to improve the
end-to-end latency by learning from historical queuing latency, outperforming
classical shortest path algorithms. For resource allocation, RL optimizes the
scheduling of tasks across constellations, focussing on efficiently using
limited resources such as battery and memory. Both use cases were tested for
multiple satellite constellation configurations and operational scenarios,
resembling the real-life spacecraft operations of communications and Earth
observation satellites. This research demonstrates that RL not only competes
with classical approaches but also offers enhanced flexibility, scalability,
and generalizability in decision-making processes, which is crucial for the
autonomous and intelligent management of satellite fleets. The findings of this
activity suggest that AI can fundamentally alter the landscape of satellite
constellation management by providing more adaptive, robust, and cost-effective
solutions.

</details>


### [394] [We Need to Rethink Benchmarking in Anomaly Detection](https://arxiv.org/abs/2507.15584)
*Philipp Röchner,Simon Klüttermann,Franz Rothlauf,Daniel Schlör*

Main category: cs.LG

TL;DR: The paper argues that stagnation in anomaly detection progress stems from flawed benchmarking practices and proposes three key improvements for better evaluation.


<details>
  <summary>Details</summary>
Motivation: Current benchmarking in anomaly detection lacks diversity and fails to reflect real-world applications, leading to stagnant progress.

Method: Proposes rethinking benchmarking by focusing on scenario-based evaluation, end-to-end pipeline analysis, and meaningful algorithm assessment.

Result: Identifies three key areas for improvement: scenario taxonomy, pipeline analysis, and objective-aligned evaluation.

Conclusion: Anomaly detection research should adopt scenario-based benchmarking to better reflect application diversity and drive meaningful progress.

Abstract: Despite the continuous proposal of new anomaly detection algorithms and
extensive benchmarking efforts, progress seems to stagnate, with only minor
performance differences between established baselines and new algorithms. In
this position paper, we argue that this stagnation is due to limitations in how
we evaluate anomaly detection algorithms. Current benchmarking does not, for
example, sufficiently reflect the diversity of anomalies in applications
ranging from predictive maintenance to scientific discovery. Consequently, we
need to rethink benchmarking in anomaly detection. In our opinion, anomaly
detection should be studied using scenarios that capture the relevant
characteristics of different applications. We identify three key areas for
improvement: First, we need to identify anomaly detection scenarios based on a
common taxonomy. Second, anomaly detection pipelines should be analyzed
end-to-end and by component. Third, evaluating anomaly detection algorithms
should be meaningful regarding the scenario's objectives.

</details>


### [395] [Red-Team Multi-Agent Reinforcement Learning for Emergency Braking Scenario](https://arxiv.org/abs/2507.15587)
*Yinsong Chen,Kaifeng Wang,Xiaoqiang Meng,Xueyuan Li,Zirui Li,Xin Gao*

Main category: cs.LG

TL;DR: A Red-Team Multi-Agent Reinforcement Learning framework is proposed to uncover corner cases in safety-critical scenarios by using interfering red-team vehicles, improving AV decision-making safety.


<details>
  <summary>Details</summary>
Motivation: Existing methods for scenario generation in safety-critical contexts are inefficient and miss corner cases, necessitating a more robust approach.

Method: The framework employs red-team agents (background vehicles) with interference capabilities, using a Constraint Graph Representation Markov Decision Process to ensure safety while disrupting AVs. A policy threat zone model quantifies threats.

Result: The framework successfully impacts AV decision-making safety and generates diverse corner cases.

Conclusion: This approach provides a novel and effective direction for research in safety-critical scenarios.

Abstract: Current research on decision-making in safety-critical scenarios often relies
on inefficient data-driven scenario generation or specific modeling approaches,
which fail to capture corner cases in real-world contexts. To address this
issue, we propose a Red-Team Multi-Agent Reinforcement Learning framework,
where background vehicles with interference capabilities are treated as
red-team agents. Through active interference and exploration, red-team vehicles
can uncover corner cases outside the data distribution. The framework uses a
Constraint Graph Representation Markov Decision Process, ensuring that red-team
vehicles comply with safety rules while continuously disrupting the autonomous
vehicles (AVs). A policy threat zone model is constructed to quantify the
threat posed by red-team vehicles to AVs, inducing more extreme actions to
increase the danger level of the scenario. Experimental results show that the
proposed framework significantly impacts AVs decision-making safety and
generates various corner cases. This method also offers a novel direction for
research in safety-critical scenarios.

</details>


### [396] [Data Mixing Agent: Learning to Re-weight Domains for Continual Pre-training](https://arxiv.org/abs/2507.15640)
*Kailai Yang,Xiao Liu,Lei Ji,Hao Li,Yeyun Gong,Peng Cheng,Mao Yang*

Main category: cs.LG

TL;DR: Proposes Data Mixing Agent, a model-based framework for domain reweighting in continual pre-training, outperforming baselines and generalizing well.


<details>
  <summary>Details</summary>
Motivation: Addresses catastrophic forgetting in continual pre-training by automating domain reweighting instead of relying on manual heuristics.

Method: Uses reinforcement learning to train an agent that learns generalizable heuristics for data mixing, evaluated in math reasoning and code generation.

Result: Outperforms baselines in balanced performance, generalizes across unseen fields/models, and aligns with human intuition.

Conclusion: Data Mixing Agent is effective, adaptable, and efficient for continual pre-training across domains.

Abstract: Continual pre-training on small-scale task-specific data is an effective
method for improving large language models in new target fields, yet it risks
catastrophic forgetting of their original capabilities. A common solution is to
re-weight training data mixtures from source and target fields on a domain
space to achieve balanced performance. Previous domain reweighting strategies
rely on manual designation with certain heuristics based on human intuition or
empirical results. In this work, we prove that more general heuristics can be
parameterized by proposing Data Mixing Agent, the first model-based, end-to-end
framework that learns to re-weight domains. The agent learns generalizable
heuristics through reinforcement learning on large quantities of data mixing
trajectories with corresponding feedback from an evaluation environment.
Experiments in continual pre-training on math reasoning show that Data Mixing
Agent outperforms strong baselines in achieving balanced performance across
source and target field benchmarks. Furthermore, it generalizes well across
unseen source fields, target models, and domain spaces without retraining.
Direct application to the code generation field also indicates its adaptability
across target domains. Further analysis showcases the agents' well-aligned
heuristics with human intuitions and their efficiency in achieving superior
model performance with less source-field data.

</details>


### [397] [Optimal Batch-Size Control for Low-Latency Federated Learning with Device Heterogeneity](https://arxiv.org/abs/2507.15601)
*Huiling Yang,Zhanwei Wang,Kaibin Huang*

Main category: cs.LG

TL;DR: A novel C²-aware framework for optimal batch-size control in federated learning (FL) minimizes end-to-end latency while ensuring convergence, addressing challenges of high-dimensional model updates and device heterogeneity.


<details>
  <summary>Details</summary>
Motivation: The need for low-latency FL in 6G networks for time-sensitive IoT applications like autonomous driving and healthcare, balancing communication-and-computation (C²) tradeoffs.

Method: Proposes a C²-aware framework with batch-size control strategies, using a tractable surrogate for convergence speed fitted to real data, tailored for slow/fast fading and device heterogeneity.

Result: Outperforms conventional batch-size adaptation schemes, demonstrating effectiveness in minimizing latency while maintaining learning performance.

Conclusion: The framework successfully addresses FL challenges, offering practical solutions for latency-sensitive applications in heterogeneous environments.

Abstract: Federated learning (FL) has emerged as a popular approach for collaborative
machine learning in sixth-generation (6G) networks, primarily due to its
privacy-preserving capabilities. The deployment of FL algorithms is expected to
empower a wide range of Internet-of-Things (IoT) applications, e.g., autonomous
driving, augmented reality, and healthcare. The mission-critical and
time-sensitive nature of these applications necessitates the design of
low-latency FL frameworks that guarantee high learning performance. In
practice, achieving low-latency FL faces two challenges: the overhead of
computing and transmitting high-dimensional model updates, and the
heterogeneity in communication-and-computation (C$^2$) capabilities across
devices. To address these challenges, we propose a novel C$^2$-aware framework
for optimal batch-size control that minimizes end-to-end (E2E) learning latency
while ensuring convergence. The framework is designed to balance a fundamental
C$^2$ tradeoff as revealed through convergence analysis. Specifically,
increasing batch sizes improves the accuracy of gradient estimation in FL and
thus reduces the number of communication rounds required for convergence, but
results in higher per-round latency, and vice versa. The associated problem of
latency minimization is intractable; however, we solve it by designing an
accurate and tractable surrogate for convergence speed, with parameters fitted
to real data. This approach yields two batch-size control strategies tailored
to scenarios with slow and fast fading, while also accommodating device
heterogeneity. Extensive experiments using real datasets demonstrate that the
proposed strategies outperform conventional batch-size adaptation schemes that
do not consider the C$^2$ tradeoff or device heterogeneity.

</details>


### [398] [Accelerating HEC-RAS: A Recurrent Neural Operator for Rapid River Forecasting](https://arxiv.org/abs/2507.15614)
*Edward Holmberg,Pujan Pokhrel,Maximilian Zoch,Elias Ioup,Ken Pathak,Steven Sloan,Kendall Niles,Jay Ratcliff,Maik Flanagin,Christian Guetl,Julian Simeonov,Mahdi Abdelguerfi*

Main category: cs.LG

TL;DR: A deep learning surrogate model accelerates HEC-RAS river forecasts with high accuracy, reducing computation time by 3.5x.


<details>
  <summary>Details</summary>
Motivation: Physics-based solvers like HEC-RAS are computationally intensive, hindering real-time flood decision-making. The goal is to speed up simulations without losing accuracy.

Method: A hybrid architecture combines GRU for short-term dynamics and Geo-FNO for spatial dependencies, trained on HEC-RAS data from the Mississippi River Basin.

Result: The model achieves a median absolute stage error of 0.31 feet and reduces computation time from 139 to 40 minutes.

Conclusion: The surrogate model offers a fast, accurate alternative to traditional hydraulic models, enhancing large-scale flood forecasting.

Abstract: Physics-based solvers like HEC-RAS provide high-fidelity river forecasts but
are too computationally intensive for on-the-fly decision-making during flood
events. The central challenge is to accelerate these simulations without
sacrificing accuracy. This paper introduces a deep learning surrogate that
treats HEC-RAS not as a solver but as a data-generation engine. We propose a
hybrid, auto-regressive architecture that combines a Gated Recurrent Unit (GRU)
to capture short-term temporal dynamics with a Geometry-Aware Fourier Neural
Operator (Geo-FNO) to model long-range spatial dependencies along a river
reach. The model learns underlying physics implicitly from a minimal
eight-channel feature vector encoding dynamic state, static geometry, and
boundary forcings extracted directly from native HEC-RAS files. Trained on 67
reaches of the Mississippi River Basin, the surrogate was evaluated on a
year-long, unseen hold-out simulation. Results show the model achieves a strong
predictive accuracy, with a median absolute stage error of 0.31 feet.
Critically, for a full 67-reach ensemble forecast, our surrogate reduces the
required wall-clock time from 139 minutes to 40 minutes, a speedup of nearly
3.5 times over the traditional solver. The success of this data-driven approach
demonstrates that robust feature engineering can produce a viable, high-speed
replacement for conventional hydraulic models, improving the computational
feasibility of large-scale ensemble flood forecasting.

</details>


### [399] [Towards Explainable Anomaly Detection in Shared Mobility Systems](https://arxiv.org/abs/2507.15643)
*Elnur Isgandarov,Matteo Cederle,Federico Chiariotti,Gian Antonio Susto*

Main category: cs.LG

TL;DR: An interpretable anomaly detection framework for bike-sharing systems using multi-source data and Isolation Forest with DIFFI for interpretability.


<details>
  <summary>Details</summary>
Motivation: Identifying anomalies in shared mobility systems is crucial for optimizing operations, improving reliability, and enhancing user experience.

Method: Uses Isolation Forest for unsupervised anomaly detection and DIFFI for interpretability, integrating bike-sharing trip records, weather, and transit data.

Result: Station-level analysis effectively identifies anomalies, influenced by weather and transit availability.

Conclusion: The framework improves decision-making in shared mobility operations.

Abstract: Shared mobility systems, such as bike-sharing networks, play a crucial role
in urban transportation. Identifying anomalies in these systems is essential
for optimizing operations, improving service reliability, and enhancing user
experience. This paper presents an interpretable anomaly detection framework
that integrates multi-source data, including bike-sharing trip records, weather
conditions, and public transit availability. The Isolation Forest algorithm is
employed for unsupervised anomaly detection, along with the Depth-based
Isolation Forest Feature Importance (DIFFI) algorithm providing
interpretability. Results show that station-level analysis offers a robust
understanding of anomalies, highlighting the influence of external factors such
as adverse weather and limited transit availability. Our findings contribute to
improving decision-making in shared mobility operations.

</details>


### [400] [Small LLMs Do Not Learn a Generalizable Theory of Mind via Reinforcement Learning](https://arxiv.org/abs/2507.15788)
*Sneheel Sarangi,Hanan Salam*

Main category: cs.LG

TL;DR: Small LLMs trained with RLVR struggle to develop generalizable Theory of Mind (ToM) capabilities, showing overfitting to training data but failing on unseen tasks.


<details>
  <summary>Details</summary>
Motivation: Explore whether RL techniques can instill nuanced social intelligence (ToM) in small LLMs.

Method: Train models on ToM datasets (HiToM, ExploreToM, FANToM) using RL with verifiable rewards (RLVR) and test generalization on held-out datasets (e.g., OpenToM).

Result: Small LLMs improve on in-distribution tasks but fail to generalize to unseen ToM tasks, showing narrow overfitting.

Conclusion: RLVR leads to overfitting rather than true abstract ToM capability in small LLMs.

Abstract: Recent advancements in large language models (LLMs) have demonstrated
emergent capabilities in complex reasoning, largely spurred by rule-based
Reinforcement Learning (RL) techniques applied during the post-training. This
has raised the question of whether similar methods can instill more nuanced,
human-like social intelligence, such as a Theory of Mind (ToM), in LLMs. This
paper investigates whether small-scale LLMs can acquire a robust and
generalizable ToM capability through RL with verifiable rewards (RLVR). We
conduct a systematic evaluation by training models on various combinations of
prominent ToM datasets (HiToM, ExploreToM, FANToM) and testing for
generalization on held-out datasets (e.g., OpenToM). Our findings indicate that
small LLMs struggle to develop a generic ToM capability. While performance on
in-distribution tasks improves, this capability fails to transfer to unseen ToM
tasks with different characteristics. Furthermore, we demonstrate that
prolonged RL training leads to models ``hacking'' the statistical patterns of
the training datasets, resulting in significant performance gains on in-domain
data but no change, or degradation of performance on out-of-distribution tasks.
This suggests the learned behavior is a form of narrow overfitting rather than
the acquisition of a true, abstract ToM capability.

</details>


### [401] [GeoHNNs: Geometric Hamiltonian Neural Networks](https://arxiv.org/abs/2507.15678)
*Amine Mohamed Aboussalah,Abdessalam Ed-dib*

Main category: cs.LG

TL;DR: GeoHNN introduces a neural network framework that embeds geometric priors from physics, ensuring stability and accuracy in modeling dynamics.


<details>
  <summary>Details</summary>
Motivation: Common machine learning methods ignore the geometric principles underlying physical laws, leading to unstable predictions for complex systems.

Method: GeoHNN encodes Riemannian and symplectic geometries, using symmetric positive-definite matrices and a constrained autoencoder to preserve phase space volume.

Result: GeoHNN outperforms existing models in long-term stability, accuracy, and energy conservation across various systems.

Conclusion: Embedding geometric physics principles is essential for robust and generalizable models of physical dynamics.

Abstract: The fundamental laws of physics are intrinsically geometric, dictating the
evolution of systems through principles of symmetry and conservation. While
modern machine learning offers powerful tools for modeling complex dynamics
from data, common methods often ignore this underlying geometric fabric.
Physics-informed neural networks, for instance, can violate fundamental
physical principles, leading to predictions that are unstable over long
periods, particularly for high-dimensional and chaotic systems. Here, we
introduce \textit{Geometric Hamiltonian Neural Networks (GeoHNN)}, a framework
that learns dynamics by explicitly encoding the geometric priors inherent to
physical laws. Our approach enforces two fundamental structures: the Riemannian
geometry of inertia, by parameterizing inertia matrices in their natural
mathematical space of symmetric positive-definite matrices, and the symplectic
geometry of phase space, using a constrained autoencoder to ensure the
preservation of phase space volume in a reduced latent space. We demonstrate
through experiments on systems ranging from coupled oscillators to
high-dimensional deformable objects that GeoHNN significantly outperforms
existing models. It achieves superior long-term stability, accuracy, and energy
conservation, confirming that embedding the geometry of physics is not just a
theoretical appeal but a practical necessity for creating robust and
generalizable models of the physical world.

</details>


### [402] [Explainable Anomaly Detection for Electric Vehicles Charging Stations](https://arxiv.org/abs/2507.15718)
*Matteo Cederle,Andrea Mazzucco,Andrea Demartini,Eugenio Mazza,Eugenia Suriani,Federico Vitti,Gian Antonio Susto*

Main category: cs.LG

TL;DR: The paper proposes an unsupervised anomaly detection method for EV charging stations using Isolation Forest and DIFFI for interpretability, tested with real-world data.


<details>
  <summary>Details</summary>
Motivation: To ensure reliability and efficiency in EV charging infrastructure by detecting anomalies and uncovering their root causes.

Method: Uses Isolation Forest for anomaly detection and DIFFI for feature importance analysis to interpret anomalies.

Result: The approach is evaluated in a real industrial case, demonstrating its efficacy.

Conclusion: The integration of unsupervised anomaly detection and explainable AI enhances the interpretability and root cause analysis of anomalies in EV charging stations.

Abstract: Electric vehicles (EV) charging stations are one of the critical
infrastructures needed to support the transition to renewable-energy-based
mobility, but ensuring their reliability and efficiency requires effective
anomaly detection to identify irregularities in charging behavior. However, in
such a productive scenario, it is also crucial to determine the underlying
cause behind the detected anomalies. To achieve this goal, this study
investigates unsupervised anomaly detection techniques for EV charging
infrastructure, integrating eXplainable Artificial Intelligence techniques to
enhance interpretability and uncover root causes of anomalies.
  Using real-world sensors and charging session data, this work applies
Isolation Forest to detect anomalies and employs the Depth-based Isolation
Forest Feature Importance (DIFFI) method to identify the most important
features contributing to such anomalies. The efficacy of the proposed approach
is evaluated in a real industrial case.

</details>


### [403] [GUI-G$^2$: Gaussian Reward Modeling for GUI Grounding](https://arxiv.org/abs/2507.15846)
*Fei Tang,Zhangxuan Gu,Zhengxi Lu,Xuyang Liu,Shuheng Shen,Changhua Meng,Wen Wang,Wenqi Zhang,Yongliang Shen,Weiming Lu,Jun Xiao,Yueting Zhuang*

Main category: cs.LG

TL;DR: GUI-G² introduces Gaussian rewards for GUI grounding, outperforming UI-TARS-72B by 24.7% on ScreenSpot-Pro by modeling spatial interactions continuously.


<details>
  <summary>Details</summary>
Motivation: Current reinforcement learning uses sparse binary rewards, ignoring the continuous nature of spatial interactions. Inspired by human clicking behavior, GUI-G² aims to improve precision and robustness.

Method: GUI-G² models GUI elements as Gaussian distributions, using point rewards for localization and coverage rewards for spatial alignment. An adaptive variance mechanism handles diverse element scales.

Result: GUI-G² outperforms UI-TARS-72B by 24.7% on ScreenSpot-Pro, showing superior robustness and generalization to unseen layouts.

Conclusion: GUI-G² transforms GUI grounding into dense continuous optimization, setting a new paradigm for spatial reasoning in GUI tasks.

Abstract: Graphical User Interface (GUI) grounding maps natural language instructions
to precise interface locations for autonomous interaction. Current
reinforcement learning approaches use binary rewards that treat elements as
hit-or-miss targets, creating sparse signals that ignore the continuous nature
of spatial interactions. Motivated by human clicking behavior that naturally
forms Gaussian distributions centered on target elements, we introduce GUI
Gaussian Grounding Rewards (GUI-G$^2$), a principled reward framework that
models GUI elements as continuous Gaussian distributions across the interface
plane. GUI-G$^2$ incorporates two synergistic mechanisms: Gaussian point
rewards model precise localization through exponentially decaying distributions
centered on element centroids, while coverage rewards assess spatial alignment
by measuring the overlap between predicted Gaussian distributions and target
regions. To handle diverse element scales, we develop an adaptive variance
mechanism that calibrates reward distributions based on element dimensions.
This framework transforms GUI grounding from sparse binary classification to
dense continuous optimization, where Gaussian distributions generate rich
gradient signals that guide models toward optimal interaction positions.
Extensive experiments across ScreenSpot, ScreenSpot-v2, and ScreenSpot-Pro
benchmarks demonstrate that GUI-G$^2$, substantially outperforms
state-of-the-art method UI-TARS-72B, with the most significant improvement of
24.7% on ScreenSpot-Pro. Our analysis reveals that continuous modeling provides
superior robustness to interface variations and enhanced generalization to
unseen layouts, establishing a new paradigm for spatial reasoning in GUI
interaction tasks.

</details>


### [404] [Competitive Algorithms for Cooperative Multi-Agent Ski-Rental Problems](https://arxiv.org/abs/2507.15727)
*Xuchuang Wang,Bo Sun,Hedyeh Beyhaghi,John C. S. Lui,Mohammad Hajiesmaili,Adam Wierman*

Main category: cs.LG

TL;DR: The paper generalizes the ski-rental problem to a multi-agent setting with individual and shared costs, introducing three competitive ratios and analyzing optimal deterministic/randomized policies.


<details>
  <summary>Details</summary>
Motivation: To extend the classical ski-rental problem to group decision-making, addressing dynamic agent participation and shared costs.

Method: Defines three competitive ratios (overall, state-dependent, individual rational) and designs deterministic (state-aware thresholds) and randomized (sampled thresholds) policies.

Result: Symmetric policies outperform asymmetric ones, with competitive ratio bounds provided, extending classical insights to multi-agent scenarios.

Conclusion: The work offers theoretical and practical insights for group decision-making under uncertainty, with implications for dynamic cost-sharing.

Abstract: This paper introduces a novel multi-agent ski-rental problem that generalizes
the classical ski-rental dilemma to a group setting where agents incur
individual and shared costs. In our model, each agent can either rent at a
fixed daily cost, or purchase a pass at an individual cost, with an additional
third option of a discounted group pass available to all. We consider scenarios
in which agents' active days differ, leading to dynamic states as agents drop
out of the decision process. To address this problem from different
perspectives, we define three distinct competitive ratios: overall,
state-dependent, and individual rational. For each objective, we design and
analyze optimal deterministic and randomized policies. Our deterministic
policies employ state-aware threshold functions that adapt to the dynamic
states, while our randomized policies sample and resample thresholds from
tailored state-aware distributions. The analysis reveals that symmetric
policies, in which all agents use the same threshold, outperform asymmetric
ones. Our results provide competitive ratio upper and lower bounds and extend
classical ski-rental insights to multi-agent settings, highlighting both
theoretical and practical implications for group decision-making under
uncertainty.

</details>


### [405] [Multi-Modal Sensor Fusion for Proactive Blockage Prediction in mmWave Vehicular Networks](https://arxiv.org/abs/2507.15769)
*Ahmad M. Nazar,Abdulkadir Celik,Mohamed Y. Selim,Asmaa Abdallah,Daji Qiao,Ahmed M. Eltawil*

Main category: cs.LG

TL;DR: A proactive blockage prediction framework for mmWave vehicular communication uses multi-modal sensing (camera, GPS, LiDAR, radar) with deep learning models. The camera-only model achieves 97.1% F1-score, while camera+radar improves to 97.2%.


<details>
  <summary>Details</summary>
Motivation: Signal blockage in mmWave vehicular communication due to dynamic obstacles like vehicles and pedestrians necessitates proactive prediction methods.

Method: Multi-modal sensing (camera, GPS, LiDAR, radar) with modality-specific deep learning models, fused using a softmax-weighted ensemble strategy.

Result: Camera-only achieves 97.1% F1-score (89.8ms inference time); camera+radar improves to 97.2% F1 (95.7ms).

Conclusion: Multi-modal sensing is effective and efficient for mmWave blockage prediction, enabling proactive wireless communication in dynamic environments.

Abstract: Vehicular communication systems operating in the millimeter wave (mmWave)
band are highly susceptible to signal blockage from dynamic obstacles such as
vehicles, pedestrians, and infrastructure. To address this challenge, we
propose a proactive blockage prediction framework that utilizes multi-modal
sensing, including camera, GPS, LiDAR, and radar inputs in an
infrastructure-to-vehicle (I2V) setting. This approach uses modality-specific
deep learning models to process each sensor stream independently and fuses
their outputs using a softmax-weighted ensemble strategy based on validation
performance. Our evaluations, for up to 1.5s in advance, show that the
camera-only model achieves the best standalone trade-off with an F1-score of
97.1% and an inference time of 89.8ms. A camera+radar configuration further
improves accuracy to 97.2% F1 at 95.7ms. Our results display the effectiveness
and efficiency of multi-modal sensing for mmWave blockage prediction and
provide a pathway for proactive wireless communication in dynamic environments.

</details>


### [406] [Deep-Learning Investigation of Vibrational Raman Spectra for Plant-Stress Analysis](https://arxiv.org/abs/2507.15772)
*Anoop C. Patil,Benny Jian Rong Sng,Yu-Wei Chang,Joana B. Pereira,Chua Nam-Hai,Rajani Sarojam,Gajendra Pratap Singh,In-Cheol Jang,Giovanni Volpe*

Main category: cs.LG

TL;DR: DIVA, a deep-learning-based workflow using Raman spectroscopy, automates plant stress detection by analyzing native spectra without manual preprocessing, improving accuracy and consistency.


<details>
  <summary>Details</summary>
Motivation: Traditional Raman analysis for plant stress detection is biased and inconsistent due to manual preprocessing. DIVA aims to automate and improve this process.

Method: DIVA uses a variational autoencoder to process native Raman spectra, including fluorescence backgrounds, without manual preprocessing, identifying spectral features objectively.

Result: DIVA successfully detected various plant stresses (abiotic and biotic) by analyzing spectral features in an unbiased manner.

Conclusion: DIVA integrates deep learning with Raman spectroscopy, enabling AI-driven plant health assessment for more sustainable agriculture.

Abstract: Detecting stress in plants is crucial for both open-farm and
controlled-environment agriculture. Biomolecules within plants serve as key
stress indicators, offering vital markers for continuous health monitoring and
early disease detection. Raman spectroscopy provides a powerful, non-invasive
means to quantify these biomolecules through their molecular vibrational
signatures. However, traditional Raman analysis relies on customized
data-processing workflows that require fluorescence background removal and
prior identification of Raman peaks of interest-introducing potential biases
and inconsistencies. Here, we introduce DIVA (Deep-learning-based Investigation
of Vibrational Raman spectra for plant-stress Analysis), a fully automated
workflow based on a variational autoencoder. Unlike conventional approaches,
DIVA processes native Raman spectra-including fluorescence backgrounds-without
manual preprocessing, identifying and quantifying significant spectral features
in an unbiased manner. We applied DIVA to detect a range of plant stresses,
including abiotic (shading, high light intensity, high temperature) and biotic
stressors (bacterial infections). By integrating deep learning with vibrational
spectroscopy, DIVA paves the way for AI-driven plant health assessment,
fostering more resilient and sustainable agricultural practices.

</details>


### [407] [Dynamics is what you need for time-series forecasting!](https://arxiv.org/abs/2507.15774)
*Alexis-Raja Brachet,Pierre-Yves Richard,Céline Hudelot*

Main category: cs.LG

TL;DR: The paper investigates the importance of learning underlying dynamics in time-series forecasting, proposing a nomenclature (PRO-DYN) to analyze models and finding that performance improves when dynamics are fully learned and placed at the model's end.


<details>
  <summary>Details</summary>
Motivation: Current deep models struggle with time-series forecasting despite vanishing boundaries between data modalities, suggesting a need for models that better learn underlying dynamics.

Method: The authors introduce the PRO-DYN nomenclature to analyze models, conduct systemic and empirical studies, and test models with diverse backbones.

Result: Findings show under-performing models learn dynamics partially, and placing the dynamics block at the model's end is crucial for performance.

Conclusion: Incorporating a learnable dynamics block as the final predictor significantly improves time-series forecasting performance.

Abstract: While boundaries between data modalities are vanishing, the usual successful
deep models are still challenged by simple ones in the time-series forecasting
task. Our hypothesis is that this task needs models that are able to learn the
data underlying dynamics. We propose to validate it through both systemic and
empirical studies. We develop an original $\texttt{PRO-DYN}$ nomenclature to
analyze existing models through the lens of dynamics. Two observations thus
emerged: $\textbf{1}$. under-performing architectures learn dynamics at most
partially, $\textbf{2}$. the location of the dynamics block at the model end is
of prime importance. We conduct extensive experiments to confirm our
observations on a set of performance-varying models with diverse backbones.
Results support the need to incorporate a learnable dynamics block and its use
as the final predictor.

</details>


### [408] [Graph Attention Specialized Expert Fusion Model for Node Classification: Based on Cora and Pubmed Datasets](https://arxiv.org/abs/2507.15784)
*Zihang Ma,Qitian Yin*

Main category: cs.LG

TL;DR: The paper proposes a Wasserstein-Rubinstein (WR) distance enhanced Expert Fusion Model (WR-EFM) to address classification disparities in graph node classification, achieving balanced accuracy across categories.


<details>
  <summary>Details</summary>
Motivation: The study aims to resolve significant classification difficulty disparities observed in the PubMed citation network dataset, particularly improving performance for Category 2.

Method: WR-EFM trains specialized GNN models for Categories 0/1 and Multi-hop GAT for Category 2, using WR distance to optimize representation similarity and an adaptive fusion strategy for dynamic weighting.

Result: WR-EFM achieves balanced accuracies (77.8%, 78.0%, 79.9%) and reduces the coefficient of variation by 77.6%, with a 5.5% improvement for Category 2 over GCN.

Conclusion: The work introduces a novel paradigm for class-imbalanced graph classification, validated by improved performance and stability.

Abstract: Graph node classification is a fundamental task in graph neural networks
(GNNs), aiming to assign predefined class labels to nodes. On the PubMed
citation network dataset, we observe significant classification difficulty
disparities, with Category 2 achieving only 74.4% accuracy in traditional GCN,
7.5% lower than Category 1. To address this, we propose a
Wasserstein-Rubinstein (WR) distance enhanced Expert Fusion Model (WR-EFM),
training specialized GNN models for Categories 0/1 (with layer normalization
and residual connections) and Multi-hop Graph Attention Networks (GAT) for
Category 2. The WR distance metric optimizes representation similarity between
models, particularly focusing on improving Category 2 performance. Our adaptive
fusion strategy dynamically weights models based on category-specific
performance, with Category 2 assigned a GAT weight of 0.8. WR distance further
guides the fusion process by measuring distributional differences between model
representations, enabling more principled integration of complementary
features.
  Experimental results show WR-EFM achieves balanced accuracy across
categories: 77.8% (Category 0), 78.0% (Category 1), and 79.9% (Category 2),
outperforming both single models and standard fusion approaches. The
coefficient of variation (CV) of WR-EFM's category accuracies is 0.013, 77.6%
lower than GCN's 0.058, demonstrating superior stability. Notably, WR-EFM
improves Category 2 accuracy by 5.5% compared to GCN, verifying the
effectiveness of WR-guided fusion in capturing complex structural patterns.
This work provides a novel paradigm for handling class-imbalanced graph
classification tasks. To promote the research community, we release our project
at https://github.com/s010m00n/GASEM4NC.

</details>


### [409] [Federated Split Learning with Improved Communication and Storage Efficiency](https://arxiv.org/abs/2507.15816)
*Yujia Mu,Cong Shen*

Main category: cs.LG

TL;DR: CSE-FSL is a novel federated split learning method that reduces communication and storage costs by using an auxiliary network for local client updates and transmitting smashed data selectively.


<details>
  <summary>Details</summary>
Motivation: Existing federated split learning (FSL) methods incur high communication overhead and server storage requirements due to frequent gradient transmissions and separate partial models for each client.

Method: CSE-FSL introduces an auxiliary network for local client updates, maintains a single server model, and transmits smashed data only in selected epochs.

Result: Theoretical analysis confirms convergence under non-convex loss functions, and experiments show significant communication reduction compared to existing FSL methods.

Conclusion: CSE-FSL effectively addresses communication and storage inefficiencies in FSL while maintaining performance.

Abstract: Federated learning (FL) is one of the popular distributed machine learning
(ML) solutions but incurs significant communication and computation costs at
edge devices. Federated split learning (FSL) can train sub-models in parallel
and reduce the computational burden of edge devices by splitting the model
architecture. However, it still requires a high communication overhead due to
transmitting the smashed data and gradients between clients and the server in
every global round. Furthermore, the server must maintain separate partial
models for every client, leading to a significant storage requirement. To
address these challenges, this paper proposes a novel communication and storage
efficient federated split learning method, termed CSE-FSL, which utilizes an
auxiliary network to locally update the weights of the clients while keeping a
single model at the server, hence avoiding frequent transmissions of gradients
from the server and greatly reducing the storage requirement of the server.
Additionally, a new model update method of transmitting the smashed data in
selected epochs can reduce the amount of smashed data sent from the clients. We
provide a theoretical analysis of CSE-FSL, rigorously guaranteeing its
convergence under non-convex loss functions. The extensive experimental results
further indicate that CSE-FSL achieves a significant communication reduction
over existing FSL solutions using real-world FL tasks.

</details>


### [410] [Multi-Strategy Improved Snake Optimizer Accelerated CNN-LSTM-Attention-Adaboost for Trajectory Prediction](https://arxiv.org/abs/2507.15832)
*Shiyang Li*

Main category: cs.LG

TL;DR: A hybrid CNN-LSTM-attention-Adaboost model with an improved snake-herd optimization (SO) algorithm is proposed for 4D trajectory prediction, outperforming traditional optimizers and improving accuracy by 39.89%.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations in medium- and long-term 4D trajectory prediction models.

Method: Combines CNN for spatial features, LSTM for temporal features, attention for global features, and Adaboost for weak learners. Uses SO for hyperparameter optimization.

Result: Outperforms traditional optimizers (e.g., particle swarm) and achieves 39.89% higher accuracy.

Conclusion: The proposed hybrid model with SO optimization significantly enhances trajectory prediction accuracy.

Abstract: To address the limitations of medium- and long-term four-dimensional (4D)
trajectory prediction models, this paper proposes a hybrid
CNN-LSTM-attention-adaboost neural network model incorporating a multi-strategy
improved snake-herd optimization (SO) algorithm. The model applies the Adaboost
algorithm to divide multiple weak learners, and each submodel utilizes CNN to
extract spatial features, LSTM to capture temporal features, and attention
mechanism to capture global features comprehensively. The strong learner model,
combined with multiple sub-models, then optimizes the hyperparameters of the
prediction model through the natural selection behavior pattern simulated by
SO. In this study, based on the real ADS-B data from Xi'an to Tianjin, the
comparison experiments and ablation studies of multiple optimizers are carried
out, and a comprehensive test and evaluation analysis is carried out. The
results show that SO-CLA-adaboost outperforms traditional optimizers such as
particle swarm, whale, and gray wolf in handling large-scale high-dimensional
trajectory data. In addition, introducing the full-strategy collaborative
improvement SO algorithm improves the model's prediction accuracy by 39.89%.

</details>


### [411] [Optimizing Canaries for Privacy Auditing with Metagradient Descent](https://arxiv.org/abs/2507.15836)
*Matteo Boglioni,Terrance Liu,Andrew Ilyas,Zhiwei Steven Wu*

Main category: cs.LG

TL;DR: The paper introduces a method to optimize canary sets for black-box privacy auditing in differentially private learning, improving lower bounds on privacy parameters by over 2x in some cases.


<details>
  <summary>Details</summary>
Motivation: To enhance privacy auditing by optimizing canary sets, addressing limitations in current methods for auditing DP-SGD.

Method: Uses metagradient optimization to improve the auditor's canary set, tested on DP-SGD for image classification.

Result: Empirical evaluation shows over 2x improvement in privacy parameter lower bounds; optimized canaries are transferable and efficient.

Conclusion: Optimized canaries significantly enhance privacy auditing effectiveness for differentially private learning algorithms.

Abstract: In this work we study black-box privacy auditing, where the goal is to lower
bound the privacy parameter of a differentially private learning algorithm
using only the algorithm's outputs (i.e., final trained model). For DP-SGD (the
most successful method for training differentially private deep learning
models), the canonical approach auditing uses membership inference-an auditor
comes with a small set of special "canary" examples, inserts a random subset of
them into the training set, and then tries to discern which of their canaries
were included in the training set (typically via a membership inference
attack). The auditor's success rate then provides a lower bound on the privacy
parameters of the learning algorithm. Our main contribution is a method for
optimizing the auditor's canary set to improve privacy auditing, leveraging
recent work on metagradient optimization. Our empirical evaluation demonstrates
that by using such optimized canaries, we can improve empirical lower bounds
for differentially private image classification models by over 2x in certain
instances. Furthermore, we demonstrate that our method is transferable and
efficient: canaries optimized for non-private SGD with a small model
architecture remain effective when auditing larger models trained with DP-SGD.

</details>


### [412] [FASTGEN: Fast and Cost-Effective Synthetic Tabular Data Generation with LLMs](https://arxiv.org/abs/2507.15839)
*Anh Nguyen,Sam Schafft,Nicholas Hale,John Alfaro*

Main category: cs.LG

TL;DR: A cost-effective method for synthetic tabular data generation using LLMs to create reusable sampling scripts, improving diversity and realism while reducing time and cost.


<details>
  <summary>Details</summary>
Motivation: Addressing the prohibitive time and cost burdens of traditional LLM-based synthetic data generation methods, especially for large volumes.

Method: Leverages LLMs to infer field distributions (numerical, categorical, free-text) and encode them into reusable scripts for efficient, scalable data synthesis.

Result: Outperforms direct methods in diversity and realism, significantly reducing the burden of high-volume synthetic data generation.

Conclusion: The approach accelerates testing in production pipelines, shortens development cycles, and offers scalable, cost-effective synthetic data solutions.

Abstract: Synthetic data generation has emerged as an invaluable solution in scenarios
where real-world data collection and usage are limited by cost and scarcity.
Large language models (LLMs) have demonstrated remarkable capabilities in
producing high-fidelity, domain-relevant samples across various fields.
However, existing approaches that directly use LLMs to generate each record
individually impose prohibitive time and cost burdens, particularly when large
volumes of synthetic data are required. In this work, we propose a fast,
cost-effective method for realistic tabular data synthesis that leverages LLMs
to infer and encode each field's distribution into a reusable sampling script.
By automatically classifying fields into numerical, categorical, or free-text
types, the LLM generates distribution-based scripts that can efficiently
produce diverse, realistic datasets at scale without continuous model
inference. Experimental results show that our approach outperforms traditional
direct methods in both diversity and data realism, substantially reducing the
burden of high-volume synthetic data generation. We plan to apply this
methodology to accelerate testing in production pipelines, thereby shortening
development cycles and improving overall system efficiency. We believe our
insights and lessons learned will aid researchers and practitioners seeking
scalable, cost-effective solutions for synthetic data generation.

</details>


### [413] [Diffusion Beats Autoregressive in Data-Constrained Settings](https://arxiv.org/abs/2507.15857)
*Mihir Prabhudesai,Menging Wu,Amir Zadeh,Katerina Fragkiadaki,Deepak Pathak*

Main category: cs.LG

TL;DR: Diffusion models outperform autoregressive (AR) models in data-scarce settings due to better utilization of repeated data, achieving lower validation loss and superior performance.


<details>
  <summary>Details</summary>
Motivation: To explore the advantages of diffusion-based language models over AR models, especially in data-constrained settings.

Method: Systematically study masked diffusion models in data-constrained environments, comparing their performance with AR models.

Result: Diffusion models excel when compute is abundant but data is scarce, leveraging implicit data augmentation and diverse token orderings.

Conclusion: Diffusion models are a compelling alternative to AR models when data is the bottleneck, not compute.

Abstract: Autoregressive (AR) models have long dominated the landscape of large
language models, driving progress across a wide range of tasks. Recently,
diffusion-based language models have emerged as a promising alternative, though
their advantages over AR models remain underexplored. In this paper, we
systematically study masked diffusion models in data-constrained settings-where
training involves repeated passes over limited data-and find that they
significantly outperform AR models when compute is abundant but data is scarce.
Diffusion models make better use of repeated data, achieving lower validation
loss and superior downstream performance. We interpret this advantage as
implicit data augmentation: masked diffusion exposes the model to a diverse
distribution of token orderings and prediction tasks, unlike AR's fixed
left-to-right factorization. We find new scaling laws for diffusion models and
derive a closed-form expression for the critical compute threshold at which
diffusion begins to outperform AR. These results suggest that when data, not
compute, is the bottleneck, diffusion models offer a compelling alternative to
the standard AR paradigm. Our code is available at:
https://diffusion-scaling.github.io.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [414] [Transaction Profiling and Address Role Inference in Tokenized U.S. Treasuries](https://arxiv.org/abs/2507.14808)
*Junliang Luo,Katrin Tinn,Samuel Ferreira Duran,Di Wu,Xue Liu*

Main category: q-fin.CP

TL;DR: The paper analyzes tokenized U.S. Treasuries (RWAs) like BUIDL, BENJI, and USDY, focusing on transaction-level behavior, functional primitives, and participant roles using Poincaré embeddings and liquidity-based graph features.


<details>
  <summary>Details</summary>
Motivation: To address the lack of empirical analyses of transaction-level behavior in the rapidly expanding market of tokenized U.S. Treasuries.

Method: Quantitative, function-level dissection of Treasury-backed RWA tokens, analyzing decoded contract calls and introducing a curvature-aware representation learning framework with Poincaré embeddings and liquidity-based graph features.

Result: The method outperforms baselines in role inference and generalizes to tasks like anomaly detection and wallet classification, revealing segmentation between institutional and retail users.

Conclusion: The study provides structured insights into functional heterogeneity and participant roles in tokenized Treasuries, contributing empirical evidence to on-chain financialization research.

Abstract: Tokenized U.S. Treasuries have emerged as a prominent subclass of real-world
assets (RWAs), offering cryptographically enforced, yield-bearing instruments
collateralized by sovereign debt and deployed across multiple blockchain
networks. While the market has expanded rapidly, empirical analyses of
transaction-level behaviour remain limited. This paper conducts a quantitative,
function-level dissection of U.S. Treasury-backed RWA tokens including BUIDL,
BENJI, and USDY, across multi-chain: mostly Ethereum and Layer-2s. We analyze
decoded contract calls to isolate core functional primitives such as issuance,
redemption, transfer, and bridge activity, revealing segmentation in behaviour
between institutional actors and retail users. To model address-level economic
roles, we introduce a curvature-aware representation learning framework using
Poincar\'e embeddings and liquidity-based graph features. Our method
outperforms baseline models on our RWA Treasury dataset in role inference and
generalizes to downstream tasks such as anomaly detection and wallet
classification in broader blockchain transaction networks. These findings
provide a structured understanding of functional heterogeneity and participant
roles in tokenized Treasury in a transaction-level perspective, contributing
new empirical evidence to the study of on-chain financialization.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [415] [Misspecifying non-compensatory as compensatory IRT: analysis of estimated skills and variance](https://arxiv.org/abs/2507.15222)
*Hiroshi Tamano,Hideitsu Hino,Daichi Mochihashi*

Main category: stat.ME

TL;DR: The paper explores the effects of misspecifying non-compensatory models as compensatory in multidimensional item response theory, revealing underestimation of higher skills and newly discovering overestimation near the origin, while also examining variance differences in parameter estimates.


<details>
  <summary>Details</summary>
Motivation: To understand the mechanisms behind underestimation and overestimation of skills in misspecified models and to investigate the impact on the variance of estimated parameters.

Method: Theoretical approach to analyze skill estimation and parameter variance under model misspecification.

Result: Underestimation of higher skills and new discovery of overestimation near the origin; variance of estimated parameters differs with misspecification.

Conclusion: The study provides a deeper understanding of estimation biases and variance issues in misspecified models, highlighting the need for accurate model specification in practice.

Abstract: Multidimensional item response theory is a statistical test theory used to
estimate the latent skills of learners and the difficulty levels of problems
based on test results. Both compensatory and non-compensatory models have been
proposed in the literature. Previous studies have revealed the substantial
underestimation of higher skills when the non-compensatory model is
misspecified as the compensatory model. However, the underlying mechanism
behind this phenomenon has not been fully elucidated. It remains unclear
whether overestimation also occurs and whether issues arise regarding the
variance of the estimated parameters. In this paper, we aim to provide a
comprehensive understanding of both underestimation and overestimation through
a theoretical approach. In addition to the previously identified
underestimation of the skills, we newly discover that the overestimation of
skills occurs around the origin. Furthermore, we investigate the extent to
which the asymptotic variance of the estimated parameters differs when
considering model misspecification compared to when it is not taken into
account.

</details>


### [416] [Robust and Differentially Private PCA for non-Gaussian data](https://arxiv.org/abs/2507.15232)
*Minwoo Kim,Sungkyu Jung*

Main category: stat.ME

TL;DR: A differentially private PCA method for heavy-tailed or contaminated data, outperforming existing approaches in non-Gaussian settings.


<details>
  <summary>Details</summary>
Motivation: Existing privacy-preserving PCA methods have restrictive assumptions, computational costs, or sensitivity to contamination, limiting accessibility.

Method: Uses a bounded transformation on rescaled data to compute PCA privately, leveraging elliptical distribution properties for robustness.

Result: The method recovers principal components effectively, showing superior statistical utility in non-Gaussian or contaminated data.

Conclusion: Proposed method offers a robust, accessible, and efficient solution for privacy-preserving PCA in challenging data scenarios.

Abstract: Recent advances have sparked significant interest in the development of
privacy-preserving Principal Component Analysis (PCA). However, many existing
approaches rely on restrictive assumptions, such as assuming sub-Gaussian data
or being vulnerable to data contamination. Additionally, some methods are
computationally expensive or depend on unknown model parameters that must be
estimated, limiting their accessibility for data analysts seeking
privacy-preserving PCA. In this paper, we propose a differentially private PCA
method applicable to heavy-tailed and potentially contaminated data. Our
approach leverages the property that the covariance matrix of properly rescaled
data preserves eigenvectors and their order under elliptical distributions,
which include Gaussian and heavy-tailed distributions. By applying a bounded
transformation, we enable straightforward computation of principal components
in a differentially private manner. Additionally, boundedness guarantees
robustness against data contamination. We conduct both theoretical analysis and
empirical evaluations of the proposed method, focusing on its ability to
recover the subspace spanned by the leading principal components. Extensive
numerical experiments demonstrate that our method consistently outperforms
existing approaches in terms of statistical utility, particularly in
non-Gaussian or contaminated data settings.

</details>


### [417] [ACS: An interactive framework for conformal selection](https://arxiv.org/abs/2507.15825)
*Yu Gui,Ying Jin,Yash Nair,Zhimei Ren*

Main category: stat.ME

TL;DR: ACS is an interactive framework for model-free selection with guaranteed error control, supporting adaptive data analysis and human input.


<details>
  <summary>Details</summary>
Motivation: To generalize conformal selection for adaptive data analysis, enabling flexible decision-making while controlling false discovery rates.

Method: ACS uses a principle to control decision-making information, allowing adaptive exploration with FDR control. It includes algorithms for model updates, diversified selection, and incorporating new labeled data.

Result: Demonstrated effectiveness through simulations and real-data applications in LLM deployment and drug discovery.

Conclusion: ACS provides a robust framework for adaptive, model-free selection with rigorous error control, applicable in diverse domains.

Abstract: This paper presents adaptive conformal selection (ACS), an interactive
framework for model-free selection with guaranteed error control. Building on
conformal selection (Jin and Cand\`es, 2023b), ACS generalizes the approach to
support human-in-the-loop adaptive data analysis. Under the ACS framework, we
can partially reuse the data to boost the selection power, make decisions on
the fly while exploring the data, and incorporate new information or
preferences as they arise. The key to ACS is a carefully designed principle
that controls the information available for decision making, allowing the data
analyst to explore the data adaptively while maintaining rigorous control of
the false discovery rate (FDR). Based on the ACS framework, we provide concrete
selection algorithms for various goals, including model update/selection,
diversified selection, and incorporating newly available labeled data. The
effectiveness of ACS is demonstrated through extensive numerical simulations
and real-data applications in large language model (LLM) deployment and drug
discovery.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [418] [Conan: A Chunkwise Online Network for Zero-Shot Adaptive Voice Conversion](https://arxiv.org/abs/2507.14534)
*Yu Zhang,Baotong Tian,Zhiyao Duan*

Main category: eess.AS

TL;DR: Conan is a zero-shot online voice conversion model addressing real-time semantic fidelity, natural sound, and unseen speaker adaptation.


<details>
  <summary>Details</summary>
Motivation: Current VC models struggle with real-time constraints, semantic fidelity, and adapting to unseen speakers.

Method: Conan uses a Stream Content Extractor (Emformer), Adaptive Style Encoder, and Causal Shuffle Vocoder (HiFiGAN).

Result: Conan outperforms baselines in subjective and objective metrics.

Conclusion: Conan effectively addresses real-time VC challenges with improved performance.

Abstract: Zero-shot online voice conversion (VC) holds significant promise for
real-time communications and entertainment. However, current VC models struggle
to preserve semantic fidelity under real-time constraints, deliver
natural-sounding conversions, and adapt effectively to unseen speaker
characteristics. To address these challenges, we introduce Conan, a chunkwise
online zero-shot voice conversion model that preserves the content of the
source while matching the voice timbre and styles of reference speech. Conan
comprises three core components: 1) a Stream Content Extractor that leverages
Emformer for low-latency streaming content encoding; 2) an Adaptive Style
Encoder that extracts fine-grained stylistic features from reference speech for
enhanced style adaptation; 3) a Causal Shuffle Vocoder that implements a fully
causal HiFiGAN using a pixel-shuffle mechanism. Experimental evaluations
demonstrate that Conan outperforms baseline models in subjective and objective
metrics. Audio samples can be found at https://aaronz345.github.io/ConanDemo.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [419] [Dissociating model architectures from inference computations](https://arxiv.org/abs/2507.15776)
*Noor Sajid,Johan Medrano*

Main category: q-bio.NC

TL;DR: Auto-regressive and deep temporal models differ in non-Markovian sequence modeling. The paper shows how deep temporal computations can be mimicked by auto-regressive models through structured context access, maintaining predictive capacity with fewer computations.


<details>
  <summary>Details</summary>
Motivation: To dissociate model architectures (predictive distribution factorisation) from inference computations, highlighting that prediction processes aren't bound to architectures.

Method: Uses a transformer trained on next-token prediction to induce hierarchical temporal factorisation during iterative inference.

Result: Demonstrates that auto-regressive models can mimic deep temporal computations efficiently, reducing computational load.

Conclusion: Prediction construction and refinement processes are independent of underlying model architectures, enabling flexible and efficient inference.

Abstract: Parr et al., 2025 examines how auto-regressive and deep temporal models
differ in their treatment of non-Markovian sequence modelling. Building on
this, we highlight the need for dissociating model architectures, i.e., how the
predictive distribution factorises, from the computations invoked at inference.
We demonstrate that deep temporal computations are mimicked by autoregressive
models by structuring context access during iterative inference. Using a
transformer trained on next-token prediction, we show that inducing
hierarchical temporal factorisation during iterative inference maintains
predictive capacity while instantiating fewer computations. This emphasises
that processes for constructing and refining predictions are not necessarily
bound to their underlying model architectures.

</details>


<div id='hep-ph'></div>

# hep-ph [[Back]](#toc)

### [420] [Simulation-Prior Independent Neural Unfolding Procedure](https://arxiv.org/abs/2507.15084)
*Anja Butter,Theo Heimel,Nathan Huetsch,Michael Kagan,Tilman Plehn*

Main category: hep-ph

TL;DR: SPINUP is a machine learning method for unfolding high-dimensional data at the LHC without binning, using neural networks and importance sampling.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of unfolding high-dimensional data in particle physics without relying on binning or prior simulations.

Method: Uses a neural network to encode the forward mapping, neural importance sampling for efficiency, and ensembling to estimate information loss.

Result: Demonstrated effectiveness in unfolding detector effects on jet substructure and parton-level unfolding for Higgs and single-top production.

Conclusion: SPINUP provides a robust and efficient approach for unfolding high-dimensional data in particle physics.

Abstract: Machine learning allows unfolding high-dimensional spaces without binning at
the LHC. The new SPINUP method extracts the unfolded distribution based on a
neural network encoding the forward mapping, making it independent of the prior
from the simulated training data. It is made efficient through neural
importance sampling, and ensembling can be used to estimate the effect of
information loss in the forward process. We showcase SPINUP for unfolding
detector effects on jet substructure observables and for unfolding to parton
level of associated Higgs and single-top production.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [421] [Learning to Communicate in Multi-Agent Reinforcement Learning for Autonomous Cyber Defence](https://arxiv.org/abs/2507.14658)
*Faizan Contractor,Li Li,Ranwa Al Mallah*

Main category: cs.MA

TL;DR: The paper proposes a method for cooperative multi-agent reinforcement learning in partially observable cyber environments, where agents learn to communicate and defend against threats using the Differentiable Inter Agent Learning algorithm.


<details>
  <summary>Details</summary>
Motivation: Current methods limit coordinated effects due to independent agent actions during execution. Effective communication can enhance decision-making in cyber defense.

Method: Agents train in the Cyber Operations Research Gym using the Differentiable Inter Agent Learning algorithm, learning tactical policies and minimal cost communication.

Result: Agents develop tactical policies similar to human experts and learn efficient communication strategies.

Conclusion: The approach improves coordinated defense in cyber environments by integrating communication and tactical learning.

Abstract: Popular methods in cooperative Multi-Agent Reinforcement Learning with
partially observable environments typically allow agents to act independently
during execution, which may limit the coordinated effect of the trained
policies. However, by sharing information such as known or suspected ongoing
threats, effective communication can lead to improved decision-making in the
cyber battle space. We propose a game design where defender agents learn to
communicate and defend against imminent cyber threats by playing training games
in the Cyber Operations Research Gym, using the Differentiable Inter Agent
Learning algorithm adapted to the cyber operational environment. The tactical
policies learned by these autonomous agents are akin to those of human experts
during incident responses to avert cyber threats. In addition, the agents
simultaneously learn minimal cost communication messages while learning their
defence tactical policies.

</details>


### [422] [LLM Economist: Large Population Models and Mechanism Design in Multi-Agent Generative Simulacra](https://arxiv.org/abs/2507.15815)
*Seth Karten,Wenzhe Li,Zihan Ding,Samuel Kleiner,Yu Bai,Chi Jin*

Main category: cs.MA

TL;DR: The LLM Economist framework uses agent-based modeling with hierarchical decision-making to design and assess economic policies, improving social welfare through natural language-driven simulations.


<details>
  <summary>Details</summary>
Motivation: To create a credible and tractable framework for fiscal experimentation by modeling complex economic systems with large, demographically realistic agent populations.

Method: Uses bounded rational worker agents and a planner agent with in-context reinforcement learning to propose tax schedules, optimizing heterogeneous utilities and mechanism design in natural language.

Result: The planner converges near Stackelberg equilibria, improving aggregate social welfare compared to Saez solutions, with further gains under decentralized governance via voting.

Conclusion: Large language model-based agents can effectively model, simulate, and govern economic systems, offering a scalable test bed for policy evaluation.

Abstract: We present the LLM Economist, a novel framework that uses agent-based
modeling to design and assess economic policies in strategic environments with
hierarchical decision-making. At the lower level, bounded rational worker
agents -- instantiated as persona-conditioned prompts sampled from U.S.
Census-calibrated income and demographic statistics -- choose labor supply to
maximize text-based utility functions learned in-context. At the upper level, a
planner agent employs in-context reinforcement learning to propose
piecewise-linear marginal tax schedules anchored to the current U.S. federal
brackets. This construction endows economic simulacra with three capabilities
requisite for credible fiscal experimentation: (i) optimization of
heterogeneous utilities, (ii) principled generation of large, demographically
realistic agent populations, and (iii) mechanism design -- the ultimate nudging
problem -- expressed entirely in natural language. Experiments with populations
of up to one hundred interacting agents show that the planner converges near
Stackelberg equilibria that improve aggregate social welfare relative to Saez
solutions, while a periodic, persona-level voting procedure furthers these
gains under decentralized governance. These results demonstrate that large
language model-based agents can jointly model, simulate, and govern complex
economic systems, providing a tractable test bed for policy evaluation at the
societal scale to help build better civilizations.

</details>


<div id='q-fin.TR'></div>

# q-fin.TR [[Back]](#toc)

### [423] [A Comparative Analysis of Statistical and Machine Learning Models for Outlier Detection in Bitcoin Limit Order Books](https://arxiv.org/abs/2507.14960)
*Ivan Letteri*

Main category: q-fin.TR

TL;DR: The study compares statistical and machine learning methods for detecting outliers in cryptocurrency limit order books, finding Empirical Covariance (EC) as the top performer with a 6.70% gain over benchmarks.


<details>
  <summary>Details</summary>
Motivation: Understanding market dynamics and manipulative trading behaviors in volatile cryptocurrency markets.

Method: Comparative analysis of 13 models in the AITA-OBS testing environment, evaluated via backtesting on 26,204 records.

Result: Empirical Covariance (EC) outperforms others with a 6.70% gain, highlighting outlier-driven strategies' effectiveness.

Conclusion: The study provides a benchmark for anomaly detection in cryptocurrency markets, aiding algorithmic trading and risk management.

Abstract: The detection of outliers within cryptocurrency limit order books (LOBs) is
of paramount importance for comprehending market dynamics, particularly in
highly volatile and nascent regulatory environments. This study conducts a
comprehensive comparative analysis of robust statistical methods and advanced
machine learning techniques for real-time anomaly identification in
cryptocurrency LOBs. Within a unified testing environment, named AITA Order
Book Signal (AITA-OBS), we evaluate the efficacy of thirteen diverse models to
identify which approaches are most suitable for detecting potentially
manipulative trading behaviours. An empirical evaluation, conducted via
backtesting on a dataset of 26,204 records from a major exchange, demonstrates
that the top-performing model, Empirical Covariance (EC), achieves a 6.70%
gain, significantly outperforming a standard Buy-and-Hold benchmark. These
findings underscore the effectiveness of outlier-driven strategies and provide
insights into the trade-offs between model complexity, trade frequency, and
performance. This study contributes to the growing corpus of research on
cryptocurrency market microstructure by furnishing a rigorous benchmark of
anomaly detection models and highlighting their potential for augmenting
algorithmic trading and risk management.

</details>


<div id='math.DS'></div>

# math.DS [[Back]](#toc)

### [424] [Learning Stochastic Hamiltonian Systems via Stochastic Generating Function Neural Network](https://arxiv.org/abs/2507.14467)
*Chen Chen,Lijin Wang,Yanzhao Cao,Xupeng Cheng*

Main category: math.DS

TL;DR: A novel neural network model, SGFNN, is proposed for learning stochastic Hamiltonian systems (SHSs) while preserving symplectic structure and improving prediction accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of learning SHSs from observational data while maintaining their symplectic structure, which is crucial for accurate long-term predictions.

Method: SGFNN uses an autoencoder framework: the encoder identifies system randomness, and the decoder detects the stochastic generating function to produce symplectic predictions.

Result: SGFNN outperforms the benchmark sFML model in accuracy, especially in long-term predictions, and preserves symplectic structure.

Conclusion: SGFNN is an effective model for learning SHSs, offering superior accuracy and symplectic structure preservation.

Abstract: In this paper we propose a novel neural network model for learning stochastic
Hamiltonian systems (SHSs) from observational data, termed the stochastic
generating function neural network (SGFNN). SGFNN preserves symplectic
structure of the underlying stochastic Hamiltonian system and produces
symplectic predictions. Our model utilizes the autoencoder framework to
identify the randomness of the latent system by the encoder network, and
detects the stochastic generating function of the system through the decoder
network based on the random variables extracted from the encoder. Symplectic
predictions can then be generated by the stochastic generating function.
Numerical experiments are performed on several stochastic Hamiltonian systems,
varying from additive to multiplicative, and from separable to non-separable
SHSs with single or multiple noises. Compared with the benchmark stochastic
flow map learning (sFML) neural network, our SGFNN model exhibits higher
accuracy across various prediction metrics, especially in long-term
predictions, with the property of maintaining the symplectic structure of the
underlying SHSs.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [425] [Schemora: schema matching via multi-stage recommendation and metadata enrichment using off-the-shelf llms](https://arxiv.org/abs/2507.14376)
*Osman Erman Gungor,Derak Paulsen,William Kang*

Main category: cs.DB

TL;DR: SCHEMORA is a schema matching framework using LLMs and hybrid retrieval, achieving state-of-the-art results without labeled data.


<details>
  <summary>Details</summary>
Motivation: Schema matching is complex and resource-intensive; SCHEMORA aims to improve accuracy and scalability without relying on labeled data.

Method: Combines large language models with hybrid retrieval techniques in a prompt-based approach, enriching schema metadata and leveraging vector-based and lexical retrieval.

Result: Achieves 7.49% and 3.75% gains in HitRate@5 and HitRate@3 on the MIMIC-OMOP benchmark, setting new state-of-the-art performance.

Conclusion: SCHEMORA is the first open-source LLM-based schema matching method, highlighting retrieval's importance and offering practical model selection guidance.

Abstract: Schema matching is essential for integrating heterogeneous data sources and
enhancing dataset discovery, yet it remains a complex and resource-intensive
problem. We introduce SCHEMORA, a schema matching framework that combines large
language models with hybrid retrieval techniques in a prompt-based approach,
enabling efficient identification of candidate matches without relying on
labeled training data or exhaustive pairwise comparisons. By enriching schema
metadata and leveraging both vector-based and lexical retrieval, SCHEMORA
improves matching accuracy and scalability. Evaluated on the MIMIC-OMOP
benchmark, it establishes new state-of-the-art performance, with gains of 7.49%
in HitRate@5 and 3.75% in HitRate@3 over previous best results. To our
knowledge, this is the first LLM-based schema matching method with an
open-source implementation, accompanied by analysis that underscores the
critical role of retrieval and provides practical guidance on model selection.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [426] [Towards a Proactive Autoscaling Framework for Data Stream Processing at the Edge using GRU and Transfer Learning](https://arxiv.org/abs/2507.14597)
*Eugene Armah,Linda Amoako Bannning*

Main category: cs.DC

TL;DR: A three-step solution for proactive edge stream processing autoscaling using GRU-based load forecasting, transfer learning, and dynamic horizontal scaling to address workload fluctuations and resource constraints.


<details>
  <summary>Details</summary>
Motivation: Edge stream processing faces challenges like rapid workload fluctuations and inefficient resource provisioning, leading to bottlenecks or wastage. Existing reactive methods and reinforcement learning (RL) have limitations in timeliness and simulation requirements.

Method: 1. GRU neural network for upstream load forecasting. 2. Transfer learning framework with DTW and joint distribution adaptation for online integration. 3. Horizontal autoscaling module for dynamic operator parallelism adjustment.

Result: The GRU model achieved up to 1.3% SMAPE on real-world data, outperforming CNN, ARIMA, and Prophet in accuracy and training time.

Conclusion: The proposed solution effectively addresses edge stream processing challenges with accurate load forecasting and efficient resource scaling, outperforming traditional methods.

Abstract: Processing data at high speeds is becoming increasingly critical as digital
economies generate enormous data. The current paradigms for timely data
processing are edge computing and data stream processing (DSP). Edge computing
places resources closer to where data is generated, while stream processing
analyzes the unbounded high-speed data in motion. However, edge stream
processing faces rapid workload fluctuations, complicating resource
provisioning. Inadequate resource allocation leads to bottlenecks, whereas
excess allocation results in wastage. Existing reactive methods, such as
threshold-based policies and queuing theory scale only after performance
degrades, potentially violating SLAs. Although reinforcement learning (RL)
offers a proactive approach through agents that learn optimal runtime
adaptation policies, it requires extensive simulation. Furthermore, predictive
machine learning models face online distribution and concept drift that
minimize their accuracy. We propose a three-step solution to the proactive edge
stream processing autoscaling problem. Firstly, a GRU neural network forecasts
the upstream load using real-world and synthetic DSP datasets. Secondly, a
transfer learning framework integrates the predictive model into an online
stream processing system using the DTW algorithm and joint distribution
adaptation to handle the disparities between offline and online domains.
Finally, a horizontal autoscaling module dynamically adjusts the degree of
operator parallelism, based on predicted load while considering edge resource
constraints. The lightweight GRU model for load predictions recorded up to
1.3\% SMAPE value on a real-world data set. It outperformed CNN, ARIMA, and
Prophet on the SMAPE and RMSE evaluation metrics, with lower training time than
the computationally intensive RL models.

</details>


### [427] [ACME: Adaptive Customization of Large Models via Distributed Systems](https://arxiv.org/abs/2507.14802)
*Ziming Dai,Chao Qiu,Fei Gao,Yunfeng Zhao,Xiaofei Wang*

Main category: cs.DC

TL;DR: ACME proposes a distributed system for customizing Transformer-based models to address privacy, latency, and efficiency challenges, achieving cost-efficiency and improved accuracy.


<details>
  <summary>Details</summary>
Motivation: Challenges in deploying large models (privacy, latency, inefficiency) and the need for automated, distributed customization.

Method: Uses a bidirectional single-loop distributed system for fine-grained customization, Pareto Front optimization, and personalized architecture aggregation.

Result: Reduces data transmission by 94%, improves accuracy by 10%, and enhances trade-off metrics by 30%.

Conclusion: ACME effectively addresses customization challenges, offering a scalable and efficient solution for deploying large models.

Abstract: Pre-trained Transformer-based large models have revolutionized personal
virtual assistants, but their deployment in cloud environments faces challenges
related to data privacy and response latency. Deploying large models closer to
the data and users has become a key research area to address these issues.
However, applying these models directly often entails significant difficulties,
such as model mismatching, resource constraints, and energy inefficiency.
Automated design of customized models is necessary, but it faces three key
challenges, namely, the high cost of centralized model customization,
imbalanced performance from user heterogeneity, and suboptimal performance from
data heterogeneity. In this paper, we propose ACME, an adaptive customization
approach of Transformer-based large models via distributed systems. To avoid
the low cost-efficiency of centralized methods, ACME employs a bidirectional
single-loop distributed system to progressively achieve fine-grained
collaborative model customization. In order to better match user heterogeneity,
it begins by customizing the backbone generation and identifying the Pareto
Front under model size constraints to ensure optimal resource utilization.
Subsequently, it performs header generation and refines the model using data
distribution-based personalized architecture aggregation to match data
heterogeneity. Evaluation on different datasets shows that ACME achieves
cost-efficient models under model size constraints. Compared to centralized
systems, data transmission volume is reduced to 6 percent. Additionally, the
average accuracy improves by 10 percent compared to the baseline, with the
trade-off metrics increasing by nearly 30 percent.

</details>


### [428] [Byzantine-Robust Decentralized Coordination of LLM Agents](https://arxiv.org/abs/2507.14928)
*Yongrae Jo,Chanik Park*

Main category: cs.DC

TL;DR: DecentLLMs introduces a decentralized consensus approach for multi-agent LLM systems, improving robustness against Byzantine agents and answer quality.


<details>
  <summary>Details</summary>
Motivation: Overcome vulnerabilities in leader-driven multi-agent systems, such as targeted attacks and suboptimal answer selection.

Method: Worker agents generate answers concurrently; evaluator agents independently score and rank them using Byzantine-robust techniques.

Result: DecentLLMs tolerates Byzantine agents effectively and selects higher-quality answers.

Conclusion: Decentralized consensus enhances multi-agent LLM systems by improving robustness and answer quality.

Abstract: Collaboration among multiple large language model (LLM) agents is a promising
approach to overcome inherent limitations of single-agent systems, such as
hallucinations and single points of failure. As LLM agents are increasingly
deployed on open blockchain platforms, multi-agent systems capable of
tolerating malicious (Byzantine) agents have become essential.
  Recent Byzantine-robust multi-agent systems typically rely on leader-driven
coordination, which suffers from two major drawbacks. First, they are
inherently vulnerable to targeted attacks against the leader. If consecutive
leaders behave maliciously, the system repeatedly fails to achieve consensus,
forcing new consensus rounds, which is particularly costly given the high
latency of LLM invocations. Second, an underperforming proposal from the leader
can be accepted as the final answer even when higher-quality alternatives are
available, as existing methods finalize the leader's proposal once it receives
a quorum of votes.
  To address these issues, we propose DecentLLMs, a novel decentralized
consensus approach for multi-agent LLM systems, where worker agents generate
answers concurrently and evaluator agents independently score and rank these
answers to select the best available one. This decentralized architecture
enables faster consensus despite the presence of Byzantine agents and
consistently selects higher-quality answers through Byzantine-robust
aggregation techniques.
  Experimental results demonstrate that DecentLLMs effectively tolerates
Byzantine agents and significantly improves the quality of selected answers.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [429] [JELAI: Integrating AI and Learning Analytics in Jupyter Notebooks](https://arxiv.org/abs/2505.17593)
*Manuel Valle Torre,Thom van der Velden,Marcus Specht,Catharine Oertel*

Main category: cs.HC

TL;DR: JELAI is an open-source platform integrating Learning Analytics (LA) and LLM-based tutoring in Jupyter Notebooks, enabling context-aware AI support and research on student behavior.


<details>
  <summary>Details</summary>
Motivation: Generative AI lacks pedagogical grounding and context awareness in education. Researching student interactions in authentic learning environments is challenging.

Method: JELAI uses a modular, containerized design with JupyterLab extensions for telemetry and chat, plus middleware for LA processing and LLM prompt enrichment.

Result: The system captures integrated code and chat data, supports real-time AI scaffolding, and enables research on student behavior. Proof-of-concept cases validate its feasibility.

Conclusion: JELAI provides a flexible framework for researchers and educators to study and deploy LA-informed AI tutoring in Jupyter Notebooks.

Abstract: Generative AI offers potential for educational support, but often lacks
pedagogical grounding and awareness of the student's learning context.
Furthermore, researching student interactions with these tools within authentic
learning environments remains challenging. To address this, we present JELAI,
an open-source platform architecture designed to integrate fine-grained
Learning Analytics (LA) with Large Language Model (LLM)-based tutoring directly
within a Jupyter Notebook environment. JELAI employs a modular, containerized
design featuring JupyterLab extensions for telemetry and chat, alongside a
central middleware handling LA processing and context-aware LLM prompt
enrichment. This architecture enables the capture of integrated code
interaction and chat data, facilitating real-time, context-sensitive AI
scaffolding and research into student behaviour. We describe the system's
design, implementation, and demonstrate its feasibility through system
performance benchmarks and two proof-of-concept use cases illustrating its
capabilities for logging multi-modal data, analysing help-seeking patterns, and
supporting A/B testing of AI configurations. JELAI's primary contribution is
its technical framework, providing a flexible tool for researchers and
educators to develop, deploy, and study LA-informed AI tutoring within the
widely used Jupyter ecosystem.

</details>


### [430] [Assessing the Reliability of Large Language Models for Deductive Qualitative Coding: A Comparative Study of ChatGPT Interventions](https://arxiv.org/abs/2507.14384)
*Angjelin Hila,Elliott Hauser*

Main category: cs.HC

TL;DR: The study explores using ChatGPT for deductive qualitative coding, testing four methods, with Step-by-Step Task Decomposition showing the best reliability.


<details>
  <summary>Details</summary>
Motivation: To address the underexplored potential of LLMs in deductive classification tasks using human-coded schemes.

Method: Tested four intervention methods (zero-shot, few-shot, definition-based, Step-by-Step Task Decomposition) on U.S. Supreme Court case summaries using the CAP Master Codebook. Evaluated performance with classification metrics and construct validity tests.

Result: Step-by-Step Task Decomposition achieved the highest reliability (accuracy = 0.775, kappa = 0.744, alpha = 0.746). Intervention strategies significantly influenced classification behavior.

Conclusion: LLMs, with tailored interventions, can achieve reliability suitable for rigorous qualitative coding workflows.

Abstract: In this study, we investigate the use of large language models (LLMs),
specifically ChatGPT, for structured deductive qualitative coding. While most
current research emphasizes inductive coding applications, we address the
underexplored potential of LLMs to perform deductive classification tasks
aligned with established human-coded schemes. Using the Comparative Agendas
Project (CAP) Master Codebook, we classified U.S. Supreme Court case summaries
into 21 major policy domains. We tested four intervention methods: zero-shot,
few-shot, definition-based, and a novel Step-by-Step Task Decomposition
strategy, across repeated samples. Performance was evaluated using standard
classification metrics (accuracy, F1-score, Cohen's kappa, Krippendorff's
alpha), and construct validity was assessed using chi-squared tests and
Cramer's V. Chi-squared and effect size analyses confirmed that intervention
strategies significantly influenced classification behavior, with Cramer's V
values ranging from 0.359 to 0.613, indicating moderate to strong shifts in
classification patterns. The Step-by-Step Task Decomposition strategy achieved
the strongest reliability (accuracy = 0.775, kappa = 0.744, alpha = 0.746),
achieving thresholds for substantial agreement. Despite the semantic ambiguity
within case summaries, ChatGPT displayed stable agreement across samples,
including high F1 scores in low-support subclasses. These findings demonstrate
that with targeted, custom-tailored interventions, LLMs can achieve reliability
levels suitable for integration into rigorous qualitative coding workflows.

</details>


### [431] [Designing Conversational AI to Support Think-Aloud Practice in Technical Interview Preparation for CS Students](https://arxiv.org/abs/2507.14418)
*Taufiq Daryanto,Sophia Stil,Xiaohan Ding,Daniel Manesh,Sang Won Lee,Tim Lee,Stephanie Lunn,Sarah Rodriguez,Chris Brown,Eugenia Rho*

Main category: cs.HC

TL;DR: The paper explores using conversational AI for think-aloud practice in technical interviews, highlighting user perceptions and design recommendations.


<details>
  <summary>Details</summary>
Motivation: Limited structured practice for think-aloud processes in technical interviews and unexplored user perceptions of AI's role.

Method: Study with 17 participants using an LLM-based technical interview practice tool.

Result: Participants valued AI for simulation, feedback, and learning. Key recommendations include enhancing social presence, broader feedback, and crowdsourced examples.

Conclusion: AI can support equitable learning in computing careers, but human-AI collaboration and intersectional challenges must be addressed.

Abstract: One challenge in technical interviews is the think-aloud process, where
candidates verbalize their thought processes while solving coding tasks.
Despite its importance, opportunities for structured practice remain limited.
Conversational AI offers potential assistance, but limited research explores
user perceptions of its role in think-aloud practice. To address this gap, we
conducted a study with 17 participants using an LLM-based technical interview
practice tool. Participants valued AI's role in simulation, feedback, and
learning from generated examples. Key design recommendations include promoting
social presence in conversational AI for technical interview simulation,
providing feedback beyond verbal content analysis, and enabling crowdsourced
think-aloud examples through human-AI collaboration. Beyond feature design, we
examined broader considerations, including intersectional challenges and
potential strategies to address them, how AI-driven interview preparation could
promote equitable learning in computing careers, and the need to rethink AI's
role in interview practice by suggesting a research direction that integrates
human-AI collaboration.

</details>


### [432] [XplainAct: Visualization for Personalized Intervention Insights](https://arxiv.org/abs/2507.14767)
*Yanming Zhang,Krishnakumar Hegde,Klaus Mueller*

Main category: cs.HC

TL;DR: XplainAct is a visual analytics framework for individual-level intervention analysis in heterogeneous systems, demonstrated via opioid deaths and voting behavior case studies.


<details>
  <summary>Details</summary>
Motivation: Existing causal methods focus on population-level effects, lacking in heterogeneous systems where intervention impacts vary widely across subgroups.

Method: XplainAct, a visual analytics framework, supports simulating, explaining, and reasoning interventions at the individual level within subpopulations.

Result: Demonstrated effectiveness through case studies on opioid-related deaths and voting inclinations in a presidential election.

Conclusion: XplainAct addresses the gap in individual-level causal reasoning, proving useful in diverse real-world scenarios.

Abstract: Causality helps people reason about and understand complex systems,
particularly through what-if analyses that explore how interventions might
alter outcomes. Although existing methods embrace causal reasoning using
interventions and counterfactual analysis, they primarily focus on effects at
the population level. These approaches often fall short in systems
characterized by significant heterogeneity, where the impact of an intervention
can vary widely across subgroups. To address this challenge, we present
XplainAct, a visual analytics framework that supports simulating, explaining,
and reasoning interventions at the individual level within subpopulations. We
demonstrate the effectiveness of XplainAct through two case studies:
investigating opioid-related deaths in epidemiology and analyzing voting
inclinations in the presidential election.

</details>


### [433] [NavVI: A Telerobotic Simulation with Multimodal Feedback for Visually Impaired Navigation in Warehouse Environments](https://arxiv.org/abs/2507.15072)
*Maisha Maimuna,Minhaz Bin Farukee,Sama Nikanfar,Mahfuza Siddiqua,Ayon Roy,Fillia Makedon*

Main category: cs.HC

TL;DR: A multimodal guidance simulator for BLV operators in warehouses combines visual, auditory, and haptic feedback for safe robot teleoperation.


<details>
  <summary>Details</summary>
Motivation: Address the lack of accessible teleoperation research for BLV users in industrial settings.

Method: Uses a navigation mesh with re-planning, visual path lines, voice cues, and haptic feedback for obstacle avoidance.

Result: Provides a real-time, closed-loop system for safe and inclusive robot teleoperation.

Conclusion: The simulator's design principles are adaptable to real robots, supporting inclusive workforce participation.

Abstract: Industrial warehouses are congested with moving forklifts, shelves and
personnel, making robot teleoperation particularly risky and demanding for
blind and low-vision (BLV) operators. Although accessible teleoperation plays a
key role in inclusive workforce participation, systematic research on its use
in industrial environments is limited, and few existing studies barely address
multimodal guidance designed for BLV users. We present a novel multimodal
guidance simulator that enables BLV users to control a mobile robot through a
high-fidelity warehouse environment while simultaneously receiving synchronized
visual, auditory, and haptic feedback. The system combines a navigation mesh
with regular re-planning so routes remain accurate avoiding collisions as
forklifts and human avatars move around the warehouse. Users with low vision
are guided with a visible path line towards destination; navigational voice
cues with clockwise directions announce upcoming turns, and finally
proximity-based haptic feedback notifies the users of static and moving
obstacles in the path. This real-time, closed-loop system offers a repeatable
testbed and algorithmic reference for accessible teleoperation research. The
simulator's design principles can be easily adapted to real robots due to the
alignment of its navigation, speech, and haptic modules with commercial
hardware, supporting rapid feasibility studies and deployment of inclusive
telerobotic tools in actual warehouses.

</details>


### [434] [Efficient Visual Appearance Optimization by Learning from Prior Preferences](https://arxiv.org/abs/2507.15355)
*Zhipeng Li,Yi-Chi Liao,Christian Holz*

Main category: cs.HC

TL;DR: Meta-PO combines Preferential Bayesian Optimization (PBO) with meta-learning to improve efficiency in visual parameter adjustment, reducing iterations needed for personalized results.


<details>
  <summary>Details</summary>
Motivation: The challenge lies in optimizing visual parameters like brightness and contrast without an explicit objective function, relying on user preferences. PBO is slow for everyday users, necessitating a more efficient method.

Method: Meta-PO integrates PBO with meta-learning, leveraging prior user preferences to suggest designs for new users, enabling faster convergence.

Result: Experiments show Meta-PO achieves satisfactory results in 5.86 iterations for similar goals and 8 for divergent ones, outperforming PBO.

Conclusion: Meta-PO enhances personalized visual optimization, making it more practical for end-users and scalable for broader interface personalization.

Abstract: Adjusting visual parameters such as brightness and contrast is common in our
everyday experiences. Finding the optimal parameter setting is challenging due
to the large search space and the lack of an explicit objective function,
leaving users to rely solely on their implicit preferences. Prior work has
explored Preferential Bayesian Optimization (PBO) to address this challenge,
involving users to iteratively select preferred designs from candidate sets.
However, PBO often requires many rounds of preference comparisons, making it
more suitable for designers than everyday end-users. We propose Meta-PO, a
novel method that integrates PBO with meta-learning to improve sample
efficiency. Specifically, Meta-PO infers prior users' preferences and stores
them as models, which are leveraged to intelligently suggest design candidates
for the new users, enabling faster convergence and more personalized results.
An experimental evaluation of our method for appearance design tasks on 2D and
3D content showed that participants achieved satisfactory appearance in 5.86
iterations using Meta-PO when participants shared similar goals with a
population (e.g., tuning for a ``warm'' look) and in 8 iterations even
generalizes across divergent goals (e.g., from ``vintage'', ``warm'', to
``holiday''). Meta-PO makes personalized visual optimization more applicable to
end-users through a generalizable, more efficient optimization conditioned on
preferences, with the potential to scale interface personalization more
broadly.

</details>


### [435] [Romance, Relief, and Regret: Teen Narratives of Chatbot Overreliance](https://arxiv.org/abs/2507.15783)
*Mohammad 'Matt' Namvarpour,Brandon Brofsky,Jessica Medina,Mamtaj Akter,Afsaneh Razi*

Main category: cs.HC

TL;DR: The study examines teens' emotional dependence on customizable AI chatbots like Character.AI, revealing patterns of overreliance, psychological distress, and interference with offline life. Recommendations for safer design are provided.


<details>
  <summary>Details</summary>
Motivation: To understand teens' interactions with customizable AI chatbots and the risks of emotional dependence, as prior studies focused on adults.

Method: Analysis of 318 Reddit posts by teens (13-17) on the Character.AI subreddit to identify patterns of overreliance.

Result: Teens often start using chatbots for support or creativity but develop strong attachments, leading to offline disruptions, distress, and difficulty disengaging. Overreliance ends through self-reflection, social re-engagement, or platform frustrations.

Conclusion: Chatbot design should foster self-awareness, real-world engagement, and involve teens in creating safer digital tools.

Abstract: As Generative Artificial Intelligence (GenAI) driven chatbots like
Character.AI become embedded in adolescent life, they raise concerns about
emotional dependence and digital overreliance. While studies have investigated
the overreliance of adults on these chatbots, they have not investigated teens'
interactions with chatbots with customizable personas. We analyzed 318 Reddit
posts made by users self-reported as 13-17 years old on the Character.AI
subreddit to understand patterns of overreliance. We found teens commonly begin
using chatbots for emotional support or creative expression, but many develop
strong attachments that interfere with offline relationships and daily
routines. Their posts revealed recurring signs of psychological distress,
cycles of relapse, and difficulty disengaging. Teens reported that their
overreliance often ended when they reflect on the harm, return to in-person
social settings, or become frustrated by platform restrictions. Based on the
implications of our findings, we provide recommendations for future chatbot
design so they can promote self-awareness, support real-world engagement, and
involve teens in developing safer digital tools.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [436] [Hyper-spectral Unmixing algorithms for remote compositional surface mapping: a review of the state of the art](https://arxiv.org/abs/2507.14260)
*Alfredo Gimenez Zapiola,Andrea Boselli,Alessandra Menafoglio,Simone Vantini*

Main category: astro-ph.IM

TL;DR: A review of hyperspectral unmixing methods for analyzing Earth and astronomical surfaces, comparing techniques, datasets, and highlighting future research directions.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of identifying surface materials and their distributions from hyperspectral images of large areas.

Method: Review and comparison of hyperspectral unmixing methods, analysis of recent methodologies, and exploration of public datasets.

Result: Identification of successful unmixing techniques and key datasets, along with open problems in the field.

Conclusion: The paper provides a comprehensive review, comparisons, and recommendations for future research in hyperspectral unmixing.

Abstract: This work concerns a detailed review of data analysis methods used for
remotely sensed images of large areas of the Earth and of other solid
astronomical objects. In detail, it focuses on the problem of inferring the
materials that cover the surfaces captured by hyper-spectral images and
estimating their abundances and spatial distributions within the region. The
most successful and relevant hyper-spectral unmixing methods are reported as
well as compared, as an addition to analysing the most recent methodologies.
The most important public data-sets in this setting, which are vastly used in
the testing and validation of the former, are also systematically explored.
Finally, open problems are spotlighted and concrete recommendations for future
research are provided.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [437] [Optimizing Legal Document Retrieval in Vietnamese with Semi-Hard Negative Mining](https://arxiv.org/abs/2507.14619)
*Van-Hoang Le,Duc-Vu Nguyen,Kiet Van Nguyen,Ngan Luu-Thuy Nguyen*

Main category: cs.IR

TL;DR: A two-stage framework (Retrieval and Re-ranking) improves legal document retrieval using fine-tuned Bi-Encoder and Cross-Encoder, with innovations like Exist@m metric and semi-hard negatives. Achieved top-three in SoICT Hackathon 2024.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle in specialized domains like law due to precision and domain-specific knowledge requirements.

Method: Two-stage framework: Bi-Encoder for retrieval, Cross-Encoder for re-ranking, optimized with negative example mining and tailored loss functions.

Result: Top-three performance in SoICT Hackathon 2024; competitive with lightweight single-pass approach.

Conclusion: Optimized data processing, tailored loss functions, and balanced negative sampling are key for robust legal retrieval systems.

Abstract: Large Language Models (LLMs) face significant challenges in specialized
domains like law, where precision and domain-specific knowledge are critical.
This paper presents a streamlined two-stage framework consisting of Retrieval
and Re-ranking to enhance legal document retrieval efficiency and accuracy. Our
approach employs a fine-tuned Bi-Encoder for rapid candidate retrieval,
followed by a Cross-Encoder for precise re-ranking, both optimized through
strategic negative example mining. Key innovations include the introduction of
the Exist@m metric to evaluate retrieval effectiveness and the use of semi-hard
negatives to mitigate training bias, which significantly improved re-ranking
performance. Evaluated on the SoICT Hackathon 2024 for Legal Document
Retrieval, our team, 4Huiter, achieved a top-three position. While
top-performing teams employed ensemble models and iterative self-training on
large bge-m3 architectures, our lightweight, single-pass approach offered a
competitive alternative with far fewer parameters. The framework demonstrates
that optimized data processing, tailored loss functions, and balanced negative
sampling are pivotal for building robust retrieval-augmented systems in legal
contexts.

</details>


### [438] [GREAT: Guiding Query Generation with a Trie for Recommending Related Search about Video at Kuaishou](https://arxiv.org/abs/2507.15267)
*Ninglu Shao,Jinshan Wang,Chenxu Wang,Qingbiao Li,Xiaoxue Zang,Han Li*

Main category: cs.IR

TL;DR: The paper introduces a novel LLM-based framework, GREAT, for query recommendation in video-related search, addressing the lack of academic research and datasets in this domain.


<details>
  <summary>Details</summary>
Motivation: Short video platforms lack academic research and datasets for query recommendation (I2Q), prompting the need for a systematic solution.

Method: The GREAT framework uses a query-based trie to guide LLM-generated queries, enhancing semantic interaction and post-processing for relevance.

Result: Offline and online experiments confirm the effectiveness of GREAT in improving query recommendation quality.

Conclusion: GREAT successfully addresses I2Q recommendation challenges, offering a scalable and effective solution for video-related search.

Abstract: Currently, short video platforms have become the primary place for
individuals to share experiences and obtain information. To better meet users'
needs for acquiring information while browsing short videos, some apps have
introduced a search entry at the bottom of videos, accompanied with recommended
relevant queries. This scenario is known as query recommendation in
video-related search, where core task is item-to-query (I2Q) recommendation. As
this scenario has only emerged in recent years, there is a notable scarcity of
academic research and publicly available datasets in this domain. To address
this gap, we systematically examine the challenges associated with this
scenario for the first time. Subsequently, we release a large-scale dataset
derived from real-world data pertaining to the query recommendation in
video-\textit{\textbf{r}}elated \textit{\textbf{s}}earch on the
\textit{\textbf{Kuai}}shou app (\textbf{KuaiRS}). Presently, existing methods
rely on embeddings to calculate similarity for matching short videos with
queries, lacking deep interaction between the semantic content and the query.
In this paper, we introduce a novel LLM-based framework named \textbf{GREAT},
which \textit{\textbf{g}}uides que\textit{\textbf{r}}y
g\textit{\textbf{e}}ner\textit{\textbf{a}}tion with a \textit{\textbf{t}}rie to
address I2Q recommendation in related search. Specifically, we initially gather
high-quality queries with high exposure and click-through rate to construct a
query-based trie. During training, we enhance the LLM's capability to generate
high-quality queries using the query-based trie. In the inference phase, the
query-based trie serves as a guide for the token generation. Finally, we
further refine the relevance and literal quality between items and queries via
a post-processing module. Extensive offline and online experiments demonstrate
the effectiveness of our proposed method.

</details>


### [439] [A Reproducibility Study of Product-side Fairness in Bundle Recommendation](https://arxiv.org/abs/2507.14352)
*Huy-Son Nguyen,Yuanna Liu,Masoud Mansoury,Mohammad Alian Nejadi,Alan Hanjalic,Maarten de Rijke*

Main category: cs.IR

TL;DR: The paper explores fairness in bundle recommendation (BR), revealing disparities in product exposure at bundle and item levels, and highlights the need for multi-faceted fairness interventions.


<details>
  <summary>Details</summary>
Motivation: Fairness issues in recommender systems are well-studied, but their implications for BR, a complex multi-layered setting, remain unexplored.

Method: A reproducibility study using three real-world datasets and four state-of-the-art BR methods, analyzing exposure disparities with multiple fairness metrics.

Result: Exposure patterns differ between bundles and items, fairness assessments vary by metric, and user behavior influences fairness outcomes.

Conclusion: The study provides insights for fairer BR systems and lays groundwork for future research in this domain.

Abstract: Recommender systems are known to exhibit fairness issues, particularly on the
product side, where products and their associated suppliers receive unequal
exposure in recommended results. While this problem has been widely studied in
traditional recommendation settings, its implications for bundle recommendation
(BR) remain largely unexplored. This emerging task introduces additional
complexity: recommendations are generated at the bundle level, yet user
satisfaction and product (or supplier) exposure depend on both the bundle and
the individual items it contains. Existing fairness frameworks and metrics
designed for traditional recommender systems may not directly translate to this
multi-layered setting. In this paper, we conduct a comprehensive
reproducibility study of product-side fairness in BR across three real-world
datasets using four state-of-the-art BR methods. We analyze exposure
disparities at both the bundle and item levels using multiple fairness metrics,
uncovering important patterns. Our results show that exposure patterns differ
notably between bundles and items, revealing the need for fairness
interventions that go beyond bundle-level assumptions. We also find that
fairness assessments vary considerably depending on the metric used,
reinforcing the need for multi-faceted evaluation. Furthermore, user behavior
plays a critical role: when users interact more frequently with bundles than
with individual items, BR systems tend to yield fairer exposure distributions
across both levels. Overall, our findings offer actionable insights for
building fairer bundle recommender systems and establish a vital foundation for
future research in this emerging domain.

</details>


### [440] [Enhancing POI Recommendation through Global Graph Disentanglement with POI Weighted Module](https://arxiv.org/abs/2507.14612)
*Pei-Xuan Li,Wei-Yun Liang,Fandel Lin,Hsun-Ping Hsieh*

Main category: cs.IR

TL;DR: The paper proposes GDPW, a framework for next POI recommendation that integrates POI category-time relationships and weighting factors, outperforming existing models by 3-11%.


<details>
  <summary>Details</summary>
Motivation: Existing methods overlook POI category-time relationships and weighting factors, leading to suboptimal performance.

Method: GDPW uses Global Category and Category-Time Graphs for representation learning, disentangles information via contrastive learning, and weights predictions using POI transition and distance relationships.

Result: GDPW improves performance by 3-11% on real-world datasets.

Conclusion: GDPW effectively addresses limitations in POI recommendation by jointly modeling category-time relationships and weighting factors.

Abstract: Next point of interest (POI) recommendation primarily predicts future
activities based on users' past check-in data and current status, providing
significant value to users and service providers. We observed that the popular
check-in times for different POI categories vary. For example, coffee shops are
crowded in the afternoon because people like to have coffee to refresh after
meals, while bars are busy late at night. However, existing methods rarely
explore the relationship between POI categories and time, which may result in
the model being unable to fully learn users' tendencies to visit certain POI
categories at different times. Additionally, existing methods for modeling time
information often convert it into time embeddings or calculate the time
interval and incorporate it into the model, making it difficult to capture the
continuity of time. Finally, during POI prediction, various weighting
information is often ignored, such as the popularity of each POI, the
transition relationships between POIs, and the distances between POIs, leading
to suboptimal performance. To address these issues, this paper proposes a novel
next POI recommendation framework called Graph Disentangler with POI Weighted
Module (GDPW). This framework aims to jointly consider POI category information
and multiple POI weighting factors. Specifically, the proposed GDPW learns
category and time representations through the Global Category Graph and the
Global Category-Time Graph. Then, we disentangle category and time information
through contrastive learning. After prediction, the final POI recommendation
for users is obtained by weighting the prediction results based on the
transition weights and distance relationships between POIs. We conducted
experiments on two real-world datasets, and the results demonstrate that the
proposed GDPW outperforms other existing models, improving performance by 3% to
11%.

</details>


### [441] [LOVO: Efficient Complex Object Query in Large-Scale Video Datasets](https://arxiv.org/abs/2507.14301)
*Yuxin Liu,Yuezhang Peng,Hefeng Zhou,Hongze Liu,Xinyu Lu,Jiong Lou,Chentao Wu,Wei Zhao,Jie Li*

Main category: cs.IR

TL;DR: LOVO is a novel system for efficient complex object queries in large-scale video datasets, outperforming existing methods with lower latency and higher accuracy.


<details>
  <summary>Details</summary>
Motivation: The surge in video data demands efficient querying systems, but current methods lack adaptability or suffer from high latency.

Method: LOVO uses pre-trained visual encoders for feature extraction, organizes embeddings in an inverted multi-index, and employs cross-modal reranking for refined results.

Result: LOVO achieves near-optimal query accuracy, up to 85x lower search latency, and reduced index construction costs.

Conclusion: LOVO sets a new benchmark for scalable and efficient object querying in dynamic video environments.

Abstract: The widespread deployment of cameras has led to an exponential increase in
video data, creating vast opportunities for applications such as traffic
management and crime surveillance. However, querying specific objects from
large-scale video datasets presents challenges, including (1) processing
massive and continuously growing data volumes, (2) supporting complex query
requirements, and (3) ensuring low-latency execution. Existing video analysis
methods struggle with either limited adaptability to unseen object classes or
suffer from high query latency. In this paper, we present LOVO, a novel system
designed to efficiently handle comp$\underline{L}$ex $\underline{O}$bject
queries in large-scale $\underline{V}$ide$\underline{O}$ datasets. Agnostic to
user queries, LOVO performs one-time feature extraction using pre-trained
visual encoders, generating compact visual embeddings for key frames to build
an efficient index. These visual embeddings, along with associated bounding
boxes, are organized in an inverted multi-index structure within a vector
database, which supports queries for any objects. During the query phase, LOVO
transforms object queries to query embeddings and conducts fast approximate
nearest-neighbor searches on the visual embeddings. Finally, a cross-modal
rerank is performed to refine the results by fusing visual features with
detailed textual features. Evaluation on real-world video datasets demonstrates
that LOVO outperforms existing methods in handling complex queries, with
near-optimal query accuracy and up to 85x lower search latency, while
significantly reducing index construction costs. This system redefines the
state-of-the-art object query approaches in video analysis, setting a new
benchmark for complex object queries with a novel, scalable, and efficient
approach that excels in dynamic environments.

</details>


### [442] [U-MARVEL: Unveiling Key Factors for Universal Multimodal Retrieval via Embedding Learning with MLLMs](https://arxiv.org/abs/2507.14902)
*Xiaojie Li,Chu Li,Shi-Zhe Chen,Xi Chen*

Main category: cs.IR

TL;DR: The paper introduces U-MARVEL, a unified framework for universal multimodal retrieval (UMR), analyzing key factors for effective embedding learning with MLLMs and outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: To address unexplored mechanisms in MLLM-based retrieval systems, which may lead to suboptimal performance and limited generalization.

Method: Implemented a general MLLM-based embedding pipeline, analyzed contributors to performance, and explored embedding generation and training strategies like progressive transition and hard negative mining.

Result: U-MARVEL outperforms competitors on M-BEIR benchmark and shows strong zero-shot performance in tasks like composed image retrieval.

Conclusion: The framework demonstrates strong generalization potential across embedding-based retrieval tasks.

Abstract: Universal multimodal retrieval (UMR), which aims to address complex retrieval
tasks where both queries and candidates span diverse modalities, has been
significantly advanced by the emergence of MLLMs. While state-of-the-art
MLLM-based methods in the literature predominantly adopt contrastive learning
principles, they often differ in their specific training recipes. Despite their
success, the mechanisms underlying their retrieval capabilities remain largely
unexplored, potentially resulting in suboptimal performance and limited
generalization ability. To address these issues, we present a comprehensive
study aimed at uncovering the key factors that drive effective embedding
learning for UMR using MLLMs. We begin by implementing a general MLLM-based
embedding learning pipeline, and systematically analyze the primary
contributors to high-performing universal retrieval systems. Based on this, we
explore various aspects of the details in embedding generation and training
strategies, including progressive transition, hard negative mining and
re-ranker distillation. Notably, our findings reveal that often-overlooked
factors can have a substantial impact on model performance. Building on these
discoveries, we introduce a unified framework termed U-MARVEL
(\textbf{U}niversal \textbf{M}ultimod\textbf{A}l \textbf{R}etrie\textbf{V}al
via \textbf{E}mbedding \textbf{L}earning), which outperforms state-of-the-art
competitors on the M-BEIR benchmark by a large margin in supervised settings,
and also exihibits strong zero-shot performance on several tasks such as
composed image retrieval and text-to-video retrieval. These results underscore
the generalization potential of our framework across various embedding-based
retrieval tasks. Code is available at https://github.com/chaxjli/U-MARVEL

</details>


### [443] [Understanding Matching Mechanisms in Cross-Encoders](https://arxiv.org/abs/2507.14604)
*Mathias Vast,Basile Van Cooten,Laure Soulier,Benjamin Piwowarski*

Main category: cs.IR

TL;DR: The paper explores simpler methods to understand neural IR models, focusing on attention processes and matching mechanisms.


<details>
  <summary>Details</summary>
Motivation: To uncover the internal workings of neural IR models, particularly cross-encoders, which remain largely unexplained despite their effectiveness.

Method: Analyzes attention processes to identify key attention heads and interprets the mechanism behind matching detection.

Result: Reveals the critical roles of specific attention heads and provides insights into the matching process.

Conclusion: Simpler methods can offer valuable insights into neural IR models, complementing more complex mechanistic interpretability approaches.

Abstract: Neural IR architectures, particularly cross-encoders, are highly effective
models whose internal mechanisms are mostly unknown. Most works trying to
explain their behavior focused on high-level processes (e.g., what in the input
influences the prediction, does the model adhere to known IR axioms) but fall
short of describing the matching process. Instead of Mechanistic
Interpretability approaches which specifically aim at explaining the hidden
mechanisms of neural models, we demonstrate that more straightforward methods
can already provide valuable insights. In this paper, we first focus on the
attention process and extract causal insights highlighting the crucial roles of
some attention heads in this process. Second, we provide an interpretation of
the mechanism underlying matching detection.

</details>


### [444] [FullRecall: A Semantic Search-Based Ranking Approach for Maximizing Recall in Patent Retrieval](https://arxiv.org/abs/2507.14946)
*Amna Ali,Liyanage C. De Silva,Pg Emeroylariffion Abas*

Main category: cs.IR

TL;DR: FullRecall is a novel patent retrieval method that ensures 100% recall by leveraging IPC-guided knowledge and a multi-phase process, outperforming baseline methods.


<details>
  <summary>Details</summary>
Motivation: The complexity of patent data and the need for reliable retrieval strategies to verify originality and non-obviousness of inventions drive this study.

Method: FullRecall uses IPC-guided knowledge to generate informative phrases, selects top keyphrases for initial retrieval (ensuring 100% recall), and refines results with a ranking scheme.

Result: FullRecall achieved 100% recall in all test cases, outperforming HRR2 and ReQ-ReC, which had significantly lower recall rates.

Conclusion: FullRecall effectively balances precision and recall, enhancing patent retrieval reliability and reducing legal risks in patent processes.

Abstract: Patent examiners and inventors face significant pressure to verify the
originality and non-obviousness of inventions, and the intricate nature of
patent data intensifies the challenges of patent retrieval. Therefore, there is
a pressing need to devise cutting-edge retrieval strategies that can reliably
achieve the desired recall. This study introduces FullRecall, a novel patent
retrieval approach that effectively manages the complexity of patent data while
maintaining the reliability of relevance matching and maximising recall. It
leverages IPC-guided knowledge to generate informative phrases, which are
processed to extract key information in the form of noun phrases characterising
the query patent under observation. From these, the top k keyphrases are
selected to construct a query for retrieving a focused subset of the dataset.
This initial retrieval step achieves complete recall, successfully capturing
all relevant documents. To further refine the results, a ranking scheme is
applied to the retrieved subset, reducing its size while maintaining 100%
recall. This multi-phase process demonstrates an effective strategy for
balancing precision and recall in patent retrieval tasks. Comprehensive
experiments were conducted, and the results were compared with baseline
studies, namely HRR2 [1] and ReQ-ReC [2]. The proposed approach yielded
superior results, achieving 100% recall in all five test cases. However,
HRR2[1] recall values across the five test cases were 10%, 25%, 33.3%, 0%, and
14.29%, while ReQ-ReC [2] showed 50% for the first test case, 25% for the
second test case, and 0% for the third, fourth, and fifth test cases. The 100%
recall ensures that no relevant prior art is overlooked, thereby strengthening
the patent pre-filing and examination processes, hence reducing potential legal
risks.

</details>


### [445] [SPAR: Scholar Paper Retrieval with LLM-based Agents for Enhanced Academic Search](https://arxiv.org/abs/2507.15245)
*Xiaofeng Shi,Yuduo Li,Qian Kou,Longbin Yu,Jinxin Xie,Hua Zhou*

Main category: cs.IR

TL;DR: SPAR is a multi-agent framework for academic literature retrieval, outperforming baselines by up to +56% F1, with SPARBench as its benchmark.


<details>
  <summary>Details</summary>
Motivation: Existing systems for literature retrieval are rigid and lack reasoning capabilities, prompting the need for a more flexible solution.

Method: SPAR uses RefChain-based query decomposition and query evolution within a multi-agent framework.

Result: SPAR achieves significant improvements, up to +56% F1 on AutoScholar and +23% F1 on SPARBench.

Conclusion: SPAR and SPARBench offer a scalable, interpretable, and high-performing foundation for scholarly retrieval research.

Abstract: Recent advances in large language models (LLMs) have opened new opportunities
for academic literature retrieval. However, existing systems often rely on
rigid pipelines and exhibit limited reasoning capabilities. We introduce SPAR,
a multi-agent framework that incorporates RefChain-based query decomposition
and query evolution to enable more flexible and effective search. To facilitate
systematic evaluation, we also construct SPARBench, a challenging benchmark
with expert-annotated relevance labels. Experimental results demonstrate that
SPAR substantially outperforms strong baselines, achieving up to +56% F1 on
AutoScholar and +23% F1 on SPARBench over the best-performing baseline.
Together, SPAR and SPARBench provide a scalable, interpretable, and
high-performing foundation for advancing research in scholarly retrieval. Code
and data will be available at: https://github.com/xiaofengShi/SPAR

</details>


### [446] [Just Ask for Music (JAM): Multimodal and Personalized Natural Language Music Recommendation](https://arxiv.org/abs/2507.15826)
*Alessandro B. Melchiorre,Elena V. Epure,Shahed Masoudian,Gustavo Escobedo,Anna Hausberger,Manuel Moussallam,Markus Schedl*

Main category: cs.IR

TL;DR: JAM is a lightweight framework for natural language music recommendation, using vector translations in a shared latent space and multimodal features, outperforming LLMs in scalability and practicality.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of LLMs (high cost, latency) and retrieval-based approaches (single-modal representations, lack of long-term user preference modeling) in music recommendation.

Method: JAM models user-query-item interactions as vector translations, aggregates multimodal features via cross-attention and sparse mixture-of-experts, and uses the JAMSessions dataset.

Result: JAM provides accurate recommendations, intuitive representations, and easy integration with existing systems.

Conclusion: JAM offers a scalable, practical solution for natural language music recommendation, balancing performance and efficiency.

Abstract: Natural language interfaces offer a compelling approach for music
recommendation, enabling users to express complex preferences conversationally.
While Large Language Models (LLMs) show promise in this direction, their
scalability in recommender systems is limited by high costs and latency.
Retrieval-based approaches using smaller language models mitigate these issues
but often rely on single-modal item representations, overlook long-term user
preferences, and require full model retraining, posing challenges for
real-world deployment. In this paper, we present JAM (Just Ask for Music), a
lightweight and intuitive framework for natural language music recommendation.
JAM models user-query-item interactions as vector translations in a shared
latent space, inspired by knowledge graph embedding methods like TransE. To
capture the complexity of music and user intent, JAM aggregates multimodal item
features via cross-attention and sparse mixture-of-experts. We also introduce
JAMSessions, a new dataset of over 100k user-query-item triples with anonymized
user/item embeddings, uniquely combining conversational queries and user
long-term preferences. Our results show that JAM provides accurate
recommendations, produces intuitive representations suitable for practical use
cases, and can be easily integrated with existing music recommendation stacks.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [447] [Prompt-aware of Frame Sampling for Efficient Text-Video Retrieval](https://arxiv.org/abs/2507.15491)
*Deyu Zhang,Tingting Long,Jinrui Zhang,Ligeng Chen,Ju Ren,Yaoxue Zhang*

Main category: cs.MM

TL;DR: ProCLIP is a user-centric framework for efficient text-video retrieval on edge devices, balancing accuracy and computational efficiency through prompt-aware frame sampling and two-stage pruning.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle to balance accuracy and efficiency in text-video retrieval, with uniform frame sampling being computationally expensive and salient-frame sampling being query-agnostic.

Method: ProCLIP uses a prompt-aware frame sampling strategy to dynamically select relevant frames and a two-stage pruning strategy (coarse filtering + CLIP-powered re-ranking) for efficiency.

Result: ProCLIP reduces latency by 75.3% while maintaining competitive accuracy (R@1=49.0 on MSR-VTT).

Conclusion: ProCLIP achieves state-of-the-art efficiency and accuracy, making it suitable for real-world edge-device applications.

Abstract: Enabling efficient text-video retrieval on edge-end devices is critical for
real-world applications. Yet, existing methods face a critical challenge in
balancing accuracy and computational efficiency: uniform frame sampling methods
ensure content coverage but incur prohibitive computational costs, while
salient-frame sampling methods reduce overhead but suffer from query-agnostic
frame selection that biases retrieval results. To address this, we propose
ProCLIP, a user-centric framework that achieves state-of-the-art accuracy with
significantly improved efficiency. We design a prompt-aware frame sampling
strategy that dynamically guides lightweight feature extractors using textual
prompts to select semantically relevant frames, overcoming the limitations of
existing salient-frame sampling methods which rely on static, query-agnostic
selection criteria. Moreover, we adopt a two-stage candidate pruning strategy
that combines rapid coarse filtering via a lightweight module with CLIP-powered
fine-grained re-ranking, enhancing retrieval efficiency while preserving
accuracy. Experiments across benchmarks show ProCLIP achieves 75.3% latency
reduction versus baselines while maintaining competitive accuracy, i.e.,
R@1=49.0 in MSR-VTT dataset. Code is available at
https://github.com/tiffylong/ProCLIP.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [448] [MENO: Hybrid Matrix Exponential-based Neural Operator for Stiff ODEs. Application to Thermochemical Kinetics](https://arxiv.org/abs/2507.14341)
*Ivan Zanardi,Simone Venturi,Marco Panesi*

Main category: physics.comp-ph

TL;DR: MENO is a hybrid surrogate model for stiff ODEs, combining neural operators for nonlinear parts and a neural matrix exponential for linear parts, ensuring physical consistency and efficiency.


<details>
  <summary>Details</summary>
Motivation: To efficiently solve stiff ODEs with sparse nonlinearity while maintaining physical consistency and computational efficiency.

Method: Decomposes the system into nonlinear (neural operators) and linear (neural matrix exponential) parts, embedding governing equations into the architecture.

Result: Achieves <2% error in zero-D and good accuracy in multi-D, with speedups up to 4800x on GPU and 185x on CPU.

Conclusion: MENO offers scalable, real-time simulation of stiff reactive systems with superior generalization and reliability.

Abstract: We introduce MENO (''Matrix Exponential-based Neural Operator''), a hybrid
surrogate modeling framework for efficiently solving stiff systems of ordinary
differential equations (ODEs) that exhibit a sparse nonlinear structure. In
such systems, only a few variables contribute nonlinearly to the dynamics,
while the majority influence the equations linearly. MENO exploits this
property by decomposing the system into two components: the low-dimensional
nonlinear part is modeled using conventional neural operators, while the linear
time-varying subsystem is integrated using a novel neural matrix exponential
formulation. This approach combines the exact solution of linear time-invariant
systems with learnable, time-dependent graph-based corrections applied to the
linear operators. Unlike black-box or soft-constrained physics-informed (PI)
models, MENO embeds the governing equations directly into its architecture,
ensuring physical consistency (e.g., steady states), improved robustness, and
more efficient training. We validate MENO on three complex thermochemical
systems: the POLLU atmospheric chemistry model, an oxygen mixture in
thermochemical nonequilibrium, and a collisional-radiative argon plasma in one-
and two-dimensional shock-tube simulations. MENO achieves relative errors below
2% in trained zero-dimensional settings and maintains good accuracy in
extrapolatory multidimensional regimes. It also delivers substantial
computational speedups, achieving up to 4 800$\times$ on GPU and 185$\times$ on
CPU compared to standard implicit ODE solvers. Although intrusive by design,
MENO's physics-based architecture enables superior generalization and
reliability, offering a scalable path for real-time simulation of stiff
reactive systems.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [449] [Exposing and Mitigating Calibration Biases and Demographic Unfairness in MLLM Few-Shot In-Context Learning for Medical Image Classification](https://arxiv.org/abs/2506.23298)
*Xing Shen,Justin Szeto,Mingyang Li,Hengguan Huang,Tal Arbel*

Main category: eess.IV

TL;DR: The paper investigates calibration biases and demographic unfairness in multimodal large language models (MLLMs) for medical image classification, introducing CALIN, an inference-time calibration method to mitigate these biases.


<details>
  <summary>Details</summary>
Motivation: To ensure safe deployment of MLLMs in clinical practice by addressing accuracy and fairness issues in predictions across demographic subgroups.

Method: CALIN, a bi-level calibration method estimating calibration matrices from population to subgroup levels, applied during inference.

Result: CALIN improves prediction accuracy and ensures fair confidence calibration across three medical imaging datasets with minimal fairness-utility trade-off.

Conclusion: CALIN effectively addresses calibration biases and unfairness in MLLMs, enhancing their reliability for clinical use.

Abstract: Multimodal large language models (MLLMs) have enormous potential to perform
few-shot in-context learning in the context of medical image analysis. However,
safe deployment of these models into real-world clinical practice requires an
in-depth analysis of the accuracies of their predictions, and their associated
calibration errors, particularly across different demographic subgroups. In
this work, we present the first investigation into the calibration biases and
demographic unfairness of MLLMs' predictions and confidence scores in few-shot
in-context learning for medical image classification. We introduce CALIN, an
inference-time calibration method designed to mitigate the associated biases.
Specifically, CALIN estimates the amount of calibration needed, represented by
calibration matrices, using a bi-level procedure: progressing from the
population level to the subgroup level prior to inference. It then applies this
estimation to calibrate the predicted confidence scores during inference.
Experimental results on three medical imaging datasets: PAPILA for fundus image
classification, HAM10000 for skin cancer classification, and MIMIC-CXR for
chest X-ray classification demonstrate CALIN's effectiveness at ensuring fair
confidence calibration in its prediction, while improving its overall
prediction accuracies and exhibiting minimum fairness-utility trade-off. Our
codebase can be found at
https://github.com/xingbpshen/medical-calibration-fairness-mllm.

</details>


### [450] [MiDeSeC: A Dataset for Mitosis Detection and Segmentation in Breast Cancer Histopathology Images](https://arxiv.org/abs/2507.14271)
*Refik Samet,Nooshin Nemati,Emrah Hancer,Serpil Sak,Bilge Ayca Kirmizi,Zeynep Yildirim*

Main category: eess.IV

TL;DR: The MiDeSeC dataset is a collection of H&E stained breast carcinoma slides from 25 patients, featuring 50 regions with 500+ mitoses, split into training and testing sets.


<details>
  <summary>Details</summary>
Motivation: To address the variability in mitosis shapes and improve detection accuracy by providing a comprehensive dataset.

Method: Slides were scanned using 3D Histech Panoramic p250 Flash-3 scanner and Olympus BX50 microscope, with 50 regions (1024x1024 pixels) selected per patient.

Result: The dataset contains over 500 mitoses, with two-thirds for training and one-third for testing.

Conclusion: The MiDeSeC dataset is a valuable resource for mitosis detection in breast carcinoma research.

Abstract: The MiDeSeC dataset is created through H&E stained invasive breast carcinoma,
no special type (NST) slides of 25 different patients captured at 40x
magnification from the Department of Medical Pathology at Ankara University.
The slides have been scanned by 3D Histech Panoramic p250 Flash-3 scanner and
Olympus BX50 microscope. As several possible mitosis shapes exist, it is
crucial to have a large dataset to cover all the cases. Accordingly, a total of
50 regions is selected from glass slides for 25 patients, each of regions with
a size of 1024*1024 pixels. There are more than 500 mitoses in total in these
50 regions. Two-thirds of the regions are reserved for training, the other
third for testing.

</details>


### [451] [NuSeC: A Dataset for Nuclei Segmentation in Breast Cancer Histopathology Images](https://arxiv.org/abs/2507.14272)
*Refik Samet,Nooshin Nemati,Emrah Hancer,Serpil Sak,Bilge Ayca Kirmizi*

Main category: eess.IV

TL;DR: The NuSeC dataset comprises 100 images from 25 patients, split into 75 training and 25 testing images for consistent comparative analysis.


<details>
  <summary>Details</summary>
Motivation: To enable consistent comparative analysis of future methods developed using the NuSeC dataset.

Method: Images (1024*1024 pixels) were selected from patient slides, split 75% training (75 images) and 25% testing (25 images), with random selection per patient.

Result: Training set: 75 images (~30,000 nuclei). Testing set: 25 images (~6,000 nuclei).

Conclusion: The NuSeC dataset is structured to facilitate standardized evaluation of future research methods.

Abstract: The NuSeC dataset is created by selecting 4 images with the size of 1024*1024
pixels from the slides of each patient among 25 patients. Therefore, there are
a total of 100 images in the NuSeC dataset. To carry out a consistent
comparative analysis between the methods that will be developed using the NuSeC
dataset by the researchers in the future, we divide the NuSeC dataset 75% as
the training set and 25% as the testing set. In detail, an image is randomly
selected from 4 images of each patient among 25 patients to build the testing
set, and then the remaining images are reserved for the training set. While the
training set includes 75 images with around 30000 nuclei structures, the
testing set includes 25 images with around 6000 nuclei structures.

</details>


### [452] [Self-Supervised Joint Reconstruction and Denoising of T2-Weighted PROPELLER MRI of the Lungs at 0.55T](https://arxiv.org/abs/2507.14308)
*Jingjia Chen,Haoyang Pei,Christoph Maier,Mary Bruno,Qiuting Wen,Seon-Hi Shin,William Moore,Hersh Chandarana,Li Feng*

Main category: eess.IV

TL;DR: A self-supervised model improves 0.55T T2-weighted lung MRI by jointly reconstructing and denoising, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: To enhance lung MRI clarity and structural integrity without clean targets, leveraging self-supervised learning.

Method: Split PROPELLER blades into partitions for self-supervised training, compared to MPPCA denoising and parallel imaging.

Result: Improved image clarity, alignment with CT scans, and reduced scan time. Outperformed MPPCA in reader evaluations.

Conclusion: The model effectively reconstructs and denoises lung MRI using intrinsic k-space redundancies.

Abstract: Purpose: This study aims to improve 0.55T T2-weighted PROPELLER lung MRI
through a self-supervised joint reconstruction and denoising model.
  Methods: T2-weighted 0.55T lung MRI dataset including 44 patients with
previous covid infection were used. A self-supervised learning framework was
developed, where each blade of the PROPELLER acquisition was split along the
readout direction into two partitions. One subset trains the unrolled
reconstruction network, while the other subset is used for loss calculation,
enabling self-supervised training without clean targets and leveraging matched
noise statistics for denoising. For comparison, Marchenko-Pastur Principal
Component Analysis (MPPCA) was performed along the coil dimension, followed by
conventional parallel imaging reconstruction. The quality of the reconstructed
lung MRI was assessed visually by two experienced radiologists independently.
  Results: The proposed self-supervised model improved the clarity and
structural integrity of the lung images. For cases with available CT scans, the
reconstructed images demonstrated strong alignment with corresponding CT
images. Additionally, the proposed model enables further scan time reduction by
requiring only half the number of blades. Reader evaluations confirmed that the
proposed method outperformed MPPCA-denoised images across all categories
(Wilcoxon signed-rank test, p<0.001), with moderate inter-reader agreement
(weighted Cohen's kappa=0.55; percentage of exact and within +/-1 point
agreement=91%).
  Conclusion: By leveraging intrinsic structural redundancies between two
disjoint splits of k-space subsets, the proposed self-supervised learning model
effectively reconstructs the image while suppressing the noise for 0.55T
T2-weighted lung MRI with PROPELLER sampling.

</details>


### [453] [Classification of Histopathology Slides with Persistence Homology Convolutions](https://arxiv.org/abs/2507.14378)
*Shrunal Pothagoni,Benjamin Schweinhart*

Main category: eess.IV

TL;DR: The paper introduces Persistent Homology Convolutions, a method to incorporate local topological information into CNNs for improved histopathology diagnostics.


<details>
  <summary>Details</summary>
Motivation: Typical CNNs lose topological information, which is crucial in domains like histopathology for distinguishing disease-indicating tissue.

Method: Proposes Persistent Homology Convolutions, a modified convolution operator that captures local and translation-invariant topological features.

Result: Models using this method outperform conventional CNNs and are less sensitive to hyperparameters.

Conclusion: Persistent Homology Convolutions effectively extract meaningful geometric information from histopathology slides.

Abstract: Convolutional neural networks (CNNs) are a standard tool for computer vision
tasks such as image classification. However, typical model architectures may
result in the loss of topological information. In specific domains such as
histopathology, topology is an important descriptor that can be used to
distinguish between disease-indicating tissue by analyzing the shape
characteristics of cells. Current literature suggests that reintroducing
topological information using persistent homology can improve medical
diagnostics; however, previous methods utilize global topological summaries
which do not contain information about the locality of topological features. To
address this gap, we present a novel method that generates local persistent
homology-based data using a modified version of the convolution operator called
Persistent Homology Convolutions. This method captures information about the
locality and translation invariance of topological features. We perform a
comparative study using various representations of histopathology slides and
find that models trained with persistent homology convolutions outperform
conventionally trained models and are less sensitive to hyperparameters. These
results indicate that persistent homology convolutions extract meaningful
geometric information from the histopathology slides.

</details>


### [454] [QUTCC: Quantile Uncertainty Training and Conformal Calibration for Imaging Inverse Problems](https://arxiv.org/abs/2507.14760)
*Cassandra Tong Ye,Shamus Li,Tyler King,Kristina Monakhova*

Main category: eess.IV

TL;DR: QUTCC improves uncertainty quantification in deep learning for medical imaging by using nonlinear scaling of quantile predictions, achieving tighter bounds than linear methods.


<details>
  <summary>Details</summary>
Motivation: Deep learning models hallucinate artifacts in critical tasks like MRI denoising, where accuracy is vital. Existing uncertainty methods use linear scaling, leading to less informative bounds.

Method: QUTCC employs a U-Net with quantile embedding to predict conditional quantile distributions. It iteratively refines bounds during calibration for tighter intervals.

Result: QUTCC outperforms prior methods, pinpointing hallucinations and achieving tighter uncertainty intervals while maintaining statistical coverage.

Conclusion: QUTCC offers a robust solution for reliable uncertainty quantification in medical imaging, enhancing accuracy and trust in deep learning outputs.

Abstract: Deep learning models often hallucinate, producing realistic artifacts that
are not truly present in the sample. This can have dire consequences for
scientific and medical inverse problems, such as MRI and microscopy denoising,
where accuracy is more important than perceptual quality. Uncertainty
quantification techniques, such as conformal prediction, can pinpoint outliers
and provide guarantees for image regression tasks, improving reliability.
However, existing methods utilize a linear constant scaling factor to calibrate
uncertainty bounds, resulting in larger, less informative bounds. We propose
QUTCC, a quantile uncertainty training and calibration technique that enables
nonlinear, non-uniform scaling of quantile predictions to enable tighter
uncertainty estimates. Using a U-Net architecture with a quantile embedding,
QUTCC enables the prediction of the full conditional distribution of quantiles
for the imaging task. During calibration, QUTCC generates uncertainty bounds by
iteratively querying the network for upper and lower quantiles, progressively
refining the bounds to obtain a tighter interval that captures the desired
coverage. We evaluate our method on several denoising tasks as well as
compressive MRI reconstruction. Our method successfully pinpoints
hallucinations in image estimates and consistently achieves tighter uncertainty
intervals than prior methods while maintaining the same statistical coverage.

</details>


### [455] [PET Image Reconstruction Using Deep Diffusion Image Prior](https://arxiv.org/abs/2507.15078)
*Fumio Hashimoto,Kuang Gong*

Main category: eess.IV

TL;DR: A diffusion model-based method for PET image reconstruction, leveraging anatomical priors and HQS for efficiency, shows robust performance across tracers and scanners.


<details>
  <summary>Details</summary>
Motivation: Addressing limitations of diffusion models in PET imaging, such as tracer-specific variability and high computational demands.

Method: Combines diffusion sampling and model fine-tuning guided by PET sinograms, using HQS for computational efficiency.

Result: Demonstrates robust generalization across tracer distributions and scanner types in simulation and clinical datasets.

Conclusion: Proposes an efficient and versatile framework for low-dose PET reconstruction.

Abstract: Diffusion models have shown great promise in medical image denoising and
reconstruction, but their application to Positron Emission Tomography (PET)
imaging remains limited by tracer-specific contrast variability and high
computational demands. In this work, we proposed an anatomical prior-guided PET
image reconstruction method based on diffusion models, inspired by the deep
diffusion image prior (DDIP) framework. The proposed method alternated between
diffusion sampling and model fine-tuning guided by the PET sinogram, enabling
the reconstruction of high-quality images from various PET tracers using a
score function pretrained on a dataset of another tracer. To improve
computational efficiency, the half-quadratic splitting (HQS) algorithm was
adopted to decouple network optimization from iterative PET reconstruction. The
proposed method was evaluated using one simulation and two clinical datasets.
For the simulation study, a model pretrained on [$^{18}$F]FDG data was tested
on amyloid-negative PET data to assess out-of-distribution (OOD) performance.
For the clinical-data validation, ten low-dose [$^{18}$F]FDG datasets and one
[$^{18}$F]Florbetapir dataset were tested on a model pretrained on data from
another tracer. Experiment results show that the proposed PET reconstruction
method can generalize robustly across tracer distributions and scanner types,
providing an efficient and versatile reconstruction framework for low-dose PET
imaging.

</details>


### [456] [Performance Analysis of Post-Training Quantization for CNN-based Conjunctival Pallor Anemia Detection](https://arxiv.org/abs/2507.15151)
*Sebastian A. Cruz Romero,Wilfredo E. Lugo Beauchamp*

Main category: eess.IV

TL;DR: The paper explores deep learning for anemia detection via conjunctival pallor using the CP-AnemiC dataset, achieving high accuracy with MobileNet, and evaluates quantization for edge deployment.


<details>
  <summary>Details</summary>
Motivation: Traditional anemia detection methods are costly and require expertise, limiting accessibility in low-resource settings.

Method: Uses MobileNet architecture, fine-tuned with data augmentation and cross-validation, and evaluates post-training quantization (FP32, FP16, INT8, INT4).

Result: Achieved high accuracy (0.9313), precision (0.9374), and F1 score (0.9773); FP16 maintained performance, but INT8/INT4 degraded it.

Conclusion: Supports further research into quantization and hardware optimizations for mobile healthcare applications.

Abstract: Anemia is a widespread global health issue, particularly among young children
in low-resource settings. Traditional methods for anemia detection often
require expensive equipment and expert knowledge, creating barriers to early
and accurate diagnosis. To address these challenges, we explore the use of deep
learning models for detecting anemia through conjunctival pallor, focusing on
the CP-AnemiC dataset, which includes 710 images from children aged 6-59
months. The dataset is annotated with hemoglobin levels, gender, age and other
demographic data, enabling the development of machine learning models for
accurate anemia detection. We use the MobileNet architecture as a backbone,
known for its efficiency in mobile and embedded vision applications, and
fine-tune our model end-to-end using data augmentation techniques and a
cross-validation strategy. Our model implementation achieved an accuracy of
0.9313, a precision of 0.9374, and an F1 score of 0.9773 demonstrating strong
performance on the dataset. To optimize the model for deployment on edge
devices, we performed post-training quantization, evaluating the impact of
different bit-widths (FP32, FP16, INT8, and INT4) on model performance.
Preliminary results suggest that while FP16 quantization maintains high
accuracy (0.9250), precision (0.9370), and F1 Score (0.9377), more aggressive
quantization (INT8 and INT4) leads to significant performance degradation.
Overall, our study supports further exploration of quantization schemes and
hardware optimizations to assess trade-offs between model size, inference time,
and diagnostic accuracy in mobile healthcare applications.

</details>


### [457] [A Study of Anatomical Priors for Deep Learning-Based Segmentation of Pheochromocytoma in Abdominal CT](https://arxiv.org/abs/2507.15193)
*Tanjin Taher Toma,Tejas Sudharshan Mathai,Bikash Santra,Pritam Mukherjee,Jianfei Liu,Wesley Jong,Darwish Alabyad,Vivek Batheja,Abhishek Jha,Mayank Patel,Darko Pucar,Jayadira del Rivero,Karel Pacak,Ronald M. Summers*

Main category: eess.IV

TL;DR: The study evaluates anatomical priors for improving deep learning-based segmentation of pheochromocytoma (PCC) in CT scans, finding the Tumor + Kidney + Aorta (TKA) strategy most effective.


<details>
  <summary>Details</summary>
Motivation: Accurate PCC segmentation aids tumor burden estimation, prognosis, and treatment planning, while potentially reducing reliance on costly genetic testing.

Method: The nnU-Net framework was used to test 11 annotation strategies, including novel multi-class schemes based on organ-specific anatomical priors, on 105 CT scans. Performance was measured using DSC, NSD, and F1 score.

Result: The TKA annotation outperformed others, showing significant improvements in DSC, NSD, and F1 score, along with robust tumor burden quantification and generalizability across genetic subtypes.

Conclusion: Incorporating relevant anatomical context enhances PCC segmentation accuracy, supporting clinical applications.

Abstract: Accurate segmentation of pheochromocytoma (PCC) in abdominal CT scans is
essential for tumor burden estimation, prognosis, and treatment planning. It
may also help infer genetic clusters, reducing reliance on expensive testing.
This study systematically evaluates anatomical priors to identify
configurations that improve deep learning-based PCC segmentation. We employed
the nnU-Net framework to evaluate eleven annotation strategies for accurate 3D
segmentation of pheochromocytoma, introducing a set of novel multi-class
schemes based on organ-specific anatomical priors. These priors were derived
from adjacent organs commonly surrounding adrenal tumors (e.g., liver, spleen,
kidney, aorta, adrenal gland, and pancreas), and were compared against a broad
body-region prior used in previous work. The framework was trained and tested
on 105 contrast-enhanced CT scans from 91 patients at the NIH Clinical Center.
Performance was measured using Dice Similarity Coefficient (DSC), Normalized
Surface Distance (NSD), and instance-wise F1 score. Among all strategies, the
Tumor + Kidney + Aorta (TKA) annotation achieved the highest segmentation
accuracy, significantly outperforming the previously used Tumor + Body (TB)
annotation across DSC (p = 0.0097), NSD (p = 0.0110), and F1 score (25.84%
improvement at an IoU threshold of 0.5), measured on a 70-30 train-test split.
The TKA model also showed superior tumor burden quantification (R^2 = 0.968)
and strong segmentation across all genetic subtypes. In five-fold
cross-validation, TKA consistently outperformed TB across IoU thresholds (0.1
to 0.5), reinforcing its robustness and generalizability. These findings
highlight the value of incorporating relevant anatomical context in deep
learning models to achieve precise PCC segmentation, supporting clinical
assessment and longitudinal monitoring.

</details>


### [458] [Personalized 3D Myocardial Infarct Geometry Reconstruction from Cine MRI with Explicit Cardiac Motion Modeling](https://arxiv.org/abs/2507.15194)
*Yilin Lyu,Fan Yang,Xiaoyue Liu,Zichen Jiang,Joshua Dillon,Debbie Zhao,Martyn Nash,Charlene Mauger,Alistair Young,Ching-Hui Sia,Mark YY Chan,Lei Li*

Main category: eess.IV

TL;DR: A novel framework for 3D myocardial infarct reconstruction from 2D cine MRI, eliminating contrast agents by leveraging motion patterns.


<details>
  <summary>Details</summary>
Motivation: LGE MRI, the gold standard for infarct detection, requires contrast agents and suffers from low spatial resolution due to sparse 2D sampling.

Method: Reconstructs 4D biventricular mesh from cine MRI using biv-me, then uses CMotion2Infarct-Net to localize infarcts via motion patterns.

Result: Achieves reasonable agreement with manual delineation on 205 cine MRI scans from 126 MI patients.

Conclusion: Demonstrates feasibility of contrast-free, motion-driven 3D infarct reconstruction for digital twin applications.

Abstract: Accurate representation of myocardial infarct geometry is crucial for
patient-specific cardiac modeling in MI patients. While Late gadolinium
enhancement (LGE) MRI is the clinical gold standard for infarct detection, it
requires contrast agents, introducing side effects and patient discomfort.
Moreover, infarct reconstruction from LGE often relies on sparsely sampled 2D
slices, limiting spatial resolution and accuracy. In this work, we propose a
novel framework for automatically reconstructing high-fidelity 3D myocardial
infarct geometry from 2D clinically standard cine MRI, eliminating the need for
contrast agents. Specifically, we first reconstruct the 4D biventricular mesh
from multi-view cine MRIs via an automatic deep shape fitting model, biv-me.
Then, we design a infarction reconstruction model, CMotion2Infarct-Net, to
explicitly utilize the motion patterns within this dynamic geometry to localize
infarct regions. Evaluated on 205 cine MRI scans from 126 MI patients, our
method shows reasonable agreement with manual delineation. This study
demonstrates the feasibility of contrast-free, cardiac motion-driven 3D infarct
reconstruction, paving the way for efficient digital twin of MI.

</details>


### [459] [Personalized 4D Whole Heart Geometry Reconstruction from Cine MRI for Cardiac Digital Twins](https://arxiv.org/abs/2507.15203)
*Xiaoyue Liu,Xicheng Sheng,Xiahai Zhuang,Vicente Grau,Mark YY Chan,Ching-Hui Sia,Lei Li*

Main category: eess.IV

TL;DR: A weakly supervised learning model reconstructs 4D heart meshes from multi-view 2D cardiac cine MRIs, enabling personalized cardiac digital twins for precision medicine.


<details>
  <summary>Details</summary>
Motivation: Whole-heart cardiac digital twins (CDTs) simulating full organ-scale electromechanics are limited, necessitating a method to create personalized 4D heart models from 2D MRIs.

Method: A weakly supervised learning model maps 2D cardiac cine MRIs to 4D heart meshes, generating personalized models that match input MRIs.

Result: The model successfully extracts key cardiac variables (e.g., ejection fraction, dynamic chamber volumes) with high temporal resolution.

Conclusion: This approach demonstrates feasibility for efficient CDT platforms, advancing precision medicine in cardiology.

Abstract: Cardiac digital twins (CDTs) provide personalized in-silico cardiac
representations and hold great potential for precision medicine in cardiology.
However, whole-heart CDT models that simulate the full organ-scale
electromechanics of all four heart chambers remain limited. In this work, we
propose a weakly supervised learning model to reconstruct 4D (3D+t) heart mesh
directly from multi-view 2D cardiac cine MRIs. This is achieved by learning a
self-supervised mapping between cine MRIs and 4D cardiac meshes, enabling the
generation of personalized heart models that closely correspond to input cine
MRIs. The resulting 4D heart meshes can facilitate the automatic extraction of
key cardiac variables, including ejection fraction and dynamic chamber volume
changes with high temporal resolution. It demonstrates the feasibility of
inferring personalized 4D heart models from cardiac MRIs, paving the way for an
efficient CDT platform for precision medicine. The code will be publicly
released once the manuscript is accepted.

</details>


### [460] [EndoControlMag: Robust Endoscopic Vascular Motion Magnification with Periodic Reference Resetting and Hierarchical Tissue-aware Dual-Mask Contro](https://arxiv.org/abs/2507.15292)
*An Wanga,Rulin Zhou,Mengya Xu,Yiru Ye,Longfei Gou,Yiting Chang,Hao Chen,Chwee Ming Lim,Jiankun Wang,Hongliang Ren*

Main category: eess.IV

TL;DR: EndoControlMag is a training-free, Lagrangian-based framework for magnifying subtle vascular motions in endoscopic surgery, featuring a PRR scheme and HTM framework with dual-mode mask dilation. It outperforms existing methods in accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: Visualizing subtle vascular motions in endoscopic surgery is challenging due to dynamic surgical scenes, impacting precision and decision-making.

Method: Uses Periodic Reference Resetting (PRR) for temporal coherence and Hierarchical Tissue-aware Magnification (HTM) with dual-mode mask dilation (motion-based and distance-based softening).

Result: Outperforms existing methods in magnification accuracy and visual quality, validated on the EndoVMM24 dataset across diverse surgical scenarios.

Conclusion: EndoControlMag is effective for enhancing vascular motion visualization in endoscopic surgery, offering robustness and superior performance.

Abstract: Visualizing subtle vascular motions in endoscopic surgery is crucial for
surgical precision and decision-making, yet remains challenging due to the
complex and dynamic nature of surgical scenes. To address this, we introduce
EndoControlMag, a training-free, Lagrangian-based framework with
mask-conditioned vascular motion magnification tailored to endoscopic
environments. Our approach features two key modules: a Periodic Reference
Resetting (PRR) scheme that divides videos into short overlapping clips with
dynamically updated reference frames to prevent error accumulation while
maintaining temporal coherence, and a Hierarchical Tissue-aware Magnification
(HTM) framework with dual-mode mask dilation. HTM first tracks vessel cores
using a pretrained visual tracking model to maintain accurate localization
despite occlusions and view changes. It then applies one of two adaptive
softening strategies to surrounding tissues: motion-based softening that
modulates magnification strength proportional to observed tissue displacement,
or distance-based exponential decay that simulates biomechanical force
attenuation. This dual-mode approach accommodates diverse surgical
scenarios-motion-based softening excels with complex tissue deformations while
distance-based softening provides stability during unreliable optical flow
conditions. We evaluate EndoControlMag on our EndoVMM24 dataset spanning four
different surgery types and various challenging scenarios, including
occlusions, instrument disturbance, view changes, and vessel deformations.
Quantitative metrics, visual assessments, and expert surgeon evaluations
demonstrate that EndoControlMag significantly outperforms existing methods in
both magnification accuracy and visual quality while maintaining robustness
across challenging surgical conditions. The code, dataset, and video results
are available at https://szupc.github.io/EndoControlMag/.

</details>


### [461] [MedSR-Impact: Transformer-Based Super-Resolution for Lung CT Segmentation, Radiomics, Classification, and Prognosis](https://arxiv.org/abs/2507.15340)
*Marc Boubnovski Martell,Kristofer Linton-Reid,Mitchell Chen,Sumeet Hindocha,Benjamin Hunter,Marco A. Calzado,Richard Lee,Joram M. Posma,Eric O. Aboagye*

Main category: eess.IV

TL;DR: TVSRN-V2, a transformer-based super-resolution network, improves lung CT analysis by enhancing low-dose scans, boosting segmentation, radiomics, and prognosis tasks.


<details>
  <summary>Details</summary>
Motivation: High-resolution CT is crucial for thoracic disease diagnosis but is limited by radiation dose and cost. TVSRN-V2 aims to address these limitations.

Method: Uses Through-Plane Attention Blocks and Swin Transformer V2 for super-resolution, with pseudo-low-resolution augmentation for robustness across protocols.

Result: Improves segmentation accuracy (+4% Dice), radiomic reproducibility, and predictive performance (+0.06 C-index and AUC).

Conclusion: TVSRN-V2 enhances clinical decision-making, offering a dose-efficient, practical solution for CT workflows.

Abstract: High-resolution volumetric computed tomography (CT) is essential for accurate
diagnosis and treatment planning in thoracic diseases; however, it is limited
by radiation dose and hardware costs. We present the Transformer Volumetric
Super-Resolution Network (\textbf{TVSRN-V2}), a transformer-based
super-resolution (SR) framework designed for practical deployment in clinical
lung CT analysis. Built from scalable components, including Through-Plane
Attention Blocks (TAB) and Swin Transformer V2 -- our model effectively
reconstructs fine anatomical details in low-dose CT volumes and integrates
seamlessly with downstream analysis pipelines. We evaluate its effectiveness on
three critical lung cancer tasks -- lobe segmentation, radiomics, and prognosis
-- across multiple clinical cohorts. To enhance robustness across variable
acquisition protocols, we introduce pseudo-low-resolution augmentation,
simulating scanner diversity without requiring private data. TVSRN-V2
demonstrates a significant improvement in segmentation accuracy (+4\% Dice),
higher radiomic feature reproducibility, and enhanced predictive performance
(+0.06 C-index and AUC). These results indicate that SR-driven recovery of
structural detail significantly enhances clinical decision support, positioning
TVSRN-V2 as a well-engineered, clinically viable system for dose-efficient
imaging and quantitative analysis in real-world CT workflows.

</details>


### [462] [Latent Space Synergy: Text-Guided Data Augmentation for Direct Diffusion Biomedical Segmentation](https://arxiv.org/abs/2507.15361)
*Muhammad Aqeel,Maham Nazir,Zanxi Ruan,Francesco Setti*

Main category: eess.IV

TL;DR: SynDiff combines text-guided synthetic data generation and diffusion-based segmentation to address data scarcity in medical image segmentation, achieving high accuracy and real-time performance.


<details>
  <summary>Details</summary>
Motivation: Medical image segmentation, especially polyp detection, faces data scarcity due to the need for specialized annotation expertise.

Method: The framework uses latent diffusion models for text-conditioned inpainting to generate realistic synthetic polyps, with direct latent estimation for single-step inference.

Result: Achieves 96.0% Dice and 92.9% IoU on CVC-ClinicDB, with real-time capability.

Conclusion: SynDiff efficiently bridges the gap between data-hungry models and clinical constraints, improving segmentation robustness without distribution shift.

Abstract: Medical image segmentation suffers from data scarcity, particularly in polyp
detection where annotation requires specialized expertise. We present SynDiff,
a framework combining text-guided synthetic data generation with efficient
diffusion-based segmentation. Our approach employs latent diffusion models to
generate clinically realistic synthetic polyps through text-conditioned
inpainting, augmenting limited training data with semantically diverse samples.
Unlike traditional diffusion methods requiring iterative denoising, we
introduce direct latent estimation enabling single-step inference with T x
computational speedup. On CVC-ClinicDB, SynDiff achieves 96.0% Dice and 92.9%
IoU while maintaining real-time capability suitable for clinical deployment.
The framework demonstrates that controlled synthetic augmentation improves
segmentation robustness without distribution shift. SynDiff bridges the gap
between data-hungry deep learning models and clinical constraints, offering an
efficient solution for deployment in resourcelimited medical settings.

</details>


### [463] [A Steel Surface Defect Detection Method Based on Lightweight Convolution Optimization](https://arxiv.org/abs/2507.15476)
*Cong Chen,Ming Chen,Hoileong Lee,Yan Li,Jiyang Yu*

Main category: eess.IV

TL;DR: A deep learning framework (YOLOv9s with C3Ghost, SCConv, and CARAFE) improves steel surface defect detection by optimizing feature representation, reducing redundancy, and enhancing upsampling.


<details>
  <summary>Details</summary>
Motivation: Traditional methods struggle with multi-scale defect detection due to low accuracy and high miss-detection rates.

Method: Combines YOLOv9s with SCConv for feature optimization, C3Ghost for efficient feature extraction, and CARAFE for detailed upsampling.

Result: Achieves higher accuracy and robustness in defect detection compared to other methods.

Conclusion: The proposed framework effectively addresses challenges in steel surface defect detection.

Abstract: Surface defect detection of steel, especially the recognition of multi-scale
defects, has always been a major challenge in industrial manufacturing. Steel
surfaces not only have defects of various sizes and shapes, which limit the
accuracy of traditional image processing and detection methods in complex
environments. However, traditional defect detection methods face issues of
insufficient accuracy and high miss-detection rates when dealing with small
target defects. To address this issue, this study proposes a detection
framework based on deep learning, specifically YOLOv9s, combined with the
C3Ghost module, SCConv module, and CARAFE upsampling operator, to improve
detection accuracy and model performance. First, the SCConv module is used to
reduce feature redundancy and optimize feature representation by reconstructing
the spatial and channel dimensions. Second, the C3Ghost module is introduced to
enhance the model's feature extraction ability by reducing redundant
computations and parameter volume, thereby improving model efficiency. Finally,
the CARAFE upsampling operator, which can more finely reorganize feature maps
in a content-aware manner, optimizes the upsampling process and ensures
detailed restoration of high-resolution defect regions. Experimental results
demonstrate that the proposed model achieves higher accuracy and robustness in
steel surface defect detection tasks compared to other methods, effectively
addressing defect detection problems.

</details>


### [464] [DeSamba: Decoupled Spectral Adaptive Framework for 3D Multi-Sequence MRI Lesion Classification](https://arxiv.org/abs/2507.15487)
*Dezhen Wang,Sheng Miao,Rongxin Chai,Jiufa Cui*

Main category: eess.IV

TL;DR: DeSamba is a novel framework for 3D lesion classification in multi-sequence MRI, combining decoupled feature learning and adaptive spectral-spatial fusion, outperforming SOTA methods.


<details>
  <summary>Details</summary>
Motivation: Integrating multi-sequence MRI data for robust 3D lesion classification is challenging due to the complexity of spatial and spectral features.

Method: DeSamba uses a Decoupled Representation Learning Module (DRLM) for feature decoupling and a Spectral Adaptive Modulation Block (SAMB) for dynamic fusion of spectral and spatial features.

Result: Achieves 62.10% Top-1 accuracy on spinal metastasis and 70.00%/64.52% accuracy on spondylitis datasets, outperforming baselines.

Conclusion: DeSamba is a generalizable and effective solution for 3D lesion classification in multi-sequence medical imaging.

Abstract: Magnetic Resonance Imaging (MRI) sequences provide rich spatial and frequency
domain information, which is crucial for accurate lesion classification in
medical imaging. However, effectively integrating multi-sequence MRI data for
robust 3D lesion classification remains a challenge. In this paper, we propose
DeSamba (Decoupled Spectral Adaptive Network and Mamba-Based Model), a novel
framework designed to extract decoupled representations and adaptively fuse
spatial and spectral features for lesion classification. DeSamba introduces a
Decoupled Representation Learning Module (DRLM) that decouples features from
different MRI sequences through self-reconstruction and cross-reconstruction,
and a Spectral Adaptive Modulation Block (SAMB) within the proposed SAMNet,
enabling dynamic fusion of spectral and spatial information based on lesion
characteristics. We evaluate DeSamba on two clinically relevant 3D datasets. On
a six-class spinal metastasis dataset (n=1,448), DeSamba achieves 62.10% Top-1
accuracy, 63.62% F1-score, 87.71% AUC, and 93.55% Top-3 accuracy on an external
validation set (n=372), outperforming all state-of-the-art (SOTA) baselines. On
a spondylitis dataset (n=251) involving a challenging binary classification
task, DeSamba achieves 70.00%/64.52% accuracy and 74.75/73.88 AUC on internal
and external validation sets, respectively. Ablation studies demonstrate that
both DRLM and SAMB significantly contribute to overall performance, with over
10% relative improvement compared to the baseline. Our results highlight the
potential of DeSamba as a generalizable and effective solution for 3D lesion
classification in multi-sequence medical imaging.

</details>


### [465] [RARE-UNet: Resolution-Aligned Routing Entry for Adaptive Medical Image Segmentation](https://arxiv.org/abs/2507.15524)
*Simon Winther Albertsen,Hjalte Svaneborg Bjørnstrup,Mostafa Mehdipour Ghazi*

Main category: eess.IV

TL;DR: RARE-UNet is a resolution-aware multi-scale segmentation model that dynamically adapts to input resolution, outperforming standard UNet and nnUNet in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing segmentation models degrade with lower-resolution inputs, limiting real-world clinical applications.

Method: Proposes RARE-UNet with multi-scale blocks, resolution-aware routing, and consistency-driven training to align multi-resolution features.

Result: Achieves highest Dice scores (0.84 and 0.65) for hippocampus and tumor segmentation, with reduced inference time at lower resolutions.

Conclusion: RARE-UNet is effective and scalable for resolution-robust segmentation, with code publicly available.

Abstract: Accurate segmentation is crucial for clinical applications, but existing
models often assume fixed, high-resolution inputs and degrade significantly
when faced with lower-resolution data in real-world scenarios. To address this
limitation, we propose RARE-UNet, a resolution-aware multi-scale segmentation
architecture that dynamically adapts its inference path to the spatial
resolution of the input. Central to our design are multi-scale blocks
integrated at multiple encoder depths, a resolution-aware routing mechanism,
and consistency-driven training that aligns multi-resolution features with
full-resolution representations. We evaluate RARE-UNet on two benchmark brain
imaging tasks for hippocampus and tumor segmentation. Compared to standard
UNet, its multi-resolution augmented variant, and nnUNet, our model achieves
the highest average Dice scores of 0.84 and 0.65 across resolution, while
maintaining consistent performance and significantly reduced inference time at
lower resolutions. These results highlight the effectiveness and scalability of
our architecture in achieving resolution-robust segmentation. The codes are
available at: https://github.com/simonsejse/RARE-UNet.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [466] [Geophysics-informed neural network for model-based seismic inversion using surrogate point spread functions](https://arxiv.org/abs/2507.14140)
*Marcus Saraiva,Ana Muller,Alexandre Maul*

Main category: physics.geo-ph

TL;DR: A Geophysics-Informed Neural Network (GINN) combines deep learning with seismic modeling to overcome limitations of traditional seismic inversion, producing high-resolution acoustic impedance and realistic Point Spread Functions.


<details>
  <summary>Details</summary>
Motivation: Traditional seismic inversion methods rely on 1D wavelets and unrealistic assumptions, limiting accuracy. GINN aims to address these issues by leveraging deep learning.

Method: GINN uses a Deep Convolutional Neural Network (DCNN) to estimate PSFs and acoustic impedance, dividing PSFs into zero-phase and residual components. It was trained on synthetic SEAM Phase I data using a 2D UNet and a self-supervised loss function (MSE + SSIM).

Result: GINN generates high-resolution IP and realistic PSFs, reducing noise and improving accuracy compared to traditional methods.

Conclusion: GINN shows promise for seismic inversion, with future work focusing on refining training and validating with real data.

Abstract: Model-based seismic inversion is a key technique in reservoir
characterization, but traditional methods face significant limitations, such as
relying on 1D average stationary wavelets and assuming an unrealistic lateral
resolution. To address these challenges, we propose a Geophysics-Informed
Neural Network (GINN) that integrates deep learning with seismic modeling. This
novel approach employs a Deep Convolutional Neural Network (DCNN) to
simultaneously estimate Point Spread Functions (PSFs) and acoustic impedance
(IP). PSFs are divided into zero-phase and residual components to ensure
geophysical consistency and to capture fine details. We used synthetic data
from the SEAM Phase I Earth Model to train the GINN for 100 epochs
(approximately 20 minutes) using a 2D UNet architecture. The network's inputs
include positional features and a low-frequency impedance (LF-IP) model. A
self-supervised loss function combining Mean Squared Error (MSE) and Structural
Similarity Index Measure (SSIM) was employed to ensure accurate results. The
GINN demonstrated its ability to generate high-resolution IP and realistic
PSFs, aligning with expected geological features. Unlike traditional 1D
wavelets, the GINN produces PSFs with limited lateral resolution, reducing
noise and improving accuracy. Future work will aim to refine the training
process and validate the methodology with real seismic data.

</details>


### [467] [Integrating Newton's Laws with deep learning for enhanced physics-informed compound flood modelling](https://arxiv.org/abs/2507.15021)
*Soheil Radfar,Faezeh Maghsoodifar,Hamed Moftakhari,Hamid Moradkhani*

Main category: physics.geo-ph

TL;DR: ALPINE is a physics-informed neural network (PINN) framework for compound flood modeling, enforcing full shallow water dynamics, outperforming traditional models and baseline neural networks in accuracy and physical consistency.


<details>
  <summary>Details</summary>
Motivation: Coastal communities face compound floods from multiple drivers, but existing models either lack computational efficiency or physical realism. ALPINE aims to bridge this gap.

Method: ALPINE integrates a convolutional encoder-decoder with ConvLSTM, enforcing mass conservation and momentum equations via a composite loss function. It was tested on six historical storm events.

Result: ALPINE reduces prediction errors and improves skill metrics for water surface elevation and velocity, especially during peak storm intensity.

Conclusion: ALPINE provides a physically consistent emulator for compound-flood forecasting and risk analysis, crucial for coastal emergency management.

Abstract: Coastal communities increasingly face compound floods, where multiple drivers
like storm surge, high tide, heavy rainfall, and river discharge occur together
or in sequence to produce impacts far greater than any single driver alone.
Traditional hydrodynamic models can provide accurate physics-based simulations
but require substantial computational resources for real-time applications or
risk assessments, while machine learning alternatives often sacrifice physical
consistency for speed, producing unrealistic predictions during extreme events.
This study addresses these challenges by developing ALPINE (All-in-one Physics
Informed Neural Emulator), a physics-informed neural network (PINN) framework
to enforce complete shallow water dynamics in compound flood modeling. Unlike
previous approaches that implement partial constraints, our framework
simultaneously enforces mass conservation and both momentum equations, ensuring
full adherence to Newton's laws throughout the prediction process. The model
integrates a convolutional encoder-decoder architecture with ConvLSTM temporal
processing, trained using a composite loss function that balances data fidelity
with physics-based residuals. Using six historical storm events (four for
training, one for validation, and one held-out for unseen testing), we observe
substantial improvements over baseline neural networks. ALPINE reduces
domain-averaged prediction errors and improves model skill metrics for water
surface elevation and velocity components. Physics-informed constraints prove
most valuable during peak storm intensity, when multiple flood drivers interact
and reliable predictions matter most. This approach yields a physically
consistent emulator capable of supporting compound-flood forecasting and
large-scale risk analyses while preserving physical realism essential for
coastal emergency management.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [468] [Quantum Annealing for Machine Learning: Applications in Feature Selection, Instance Selection, and Clustering](https://arxiv.org/abs/2507.15063)
*Chloe Pomeroy,Aleksandar Pramov,Karishma Thakrar,Lakshmi Yendapalli*

Main category: quant-ph

TL;DR: The paper compares quantum annealing (QA) and classical simulated annealing (SA) for combinatorial optimization in machine learning tasks like feature selection, instance selection, and clustering, showing QA's computational efficiency and improvements in clustering metrics.


<details>
  <summary>Details</summary>
Motivation: To explore and compare the effectiveness of quantum and classical annealing methods in solving combinatorial optimization problems in machine learning.

Method: Formulate tasks as QUBO problems, implement QA and SA solvers, propose novel QUBO configurations for feature selection, heuristics for instance selection, and a classical-to-quantum pipeline for clustering.

Result: QA outperforms SA in computational efficiency for feature selection, novel heuristics improve instance selection, and the quantum pipeline enhances clustering compactness and retrieval metrics.

Conclusion: QA is a competitive and efficient tool for discrete machine learning optimization, even with current quantum hardware limitations.

Abstract: This paper explores the applications of quantum annealing (QA) and classical
simulated annealing (SA) to a suite of combinatorial optimization problems in
machine learning, namely feature selection, instance selection, and clustering.
We formulate each task as a Quadratic Unconstrained Binary Optimization (QUBO)
problem and implement both quantum and classical solvers to compare their
effectiveness. For feature selection, we propose several QUBO configurations
that balance feature importance and redundancy, showing that quantum annealing
(QA) produces solutions that are computationally more efficient. In instance
selection, we propose a few novel heuristics for instance-level importance
measures that extend existing methods. For clustering, we embed a
classical-to-quantum pipeline, using classical clustering followed by
QUBO-based medoid refinement, and demonstrate consistent improvements in
cluster compactness and retrieval metrics. Our results suggest that QA can be a
competitive and efficient tool for discrete machine learning optimization, even
within the constraints of current quantum hardware.

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [469] [Siamese Neural Network for Label-Efficient Critical Phenomena Prediction in 3D Percolation Models](https://arxiv.org/abs/2507.14159)
*Shanshan Wang,Dian Xu,Jianmin Shen,Feng Gao,Wei Li,Weibing Deng*

Main category: cond-mat.dis-nn

TL;DR: A Siamese Neural Network (SNN) is proposed for efficient 3D percolation analysis, achieving high accuracy with fewer labeled samples than traditional methods.


<details>
  <summary>Details</summary>
Motivation: Existing machine learning frameworks for percolation analysis oversimplify 3D systems, lacking spatial and morphological complexity.

Method: Uses a Siamese Neural Network (SNN) with features of the largest cluster as input for 3D percolation analysis.

Result: Achieves sub-1% error margins for site/bond percolation thresholds and critical exponents in 3D.

Conclusion: Provides a data-efficient framework for high-dimensional critical phenomena, useful for materials discovery and network analysis.

Abstract: Percolation theory serves as a cornerstone for studying phase transitions and
critical phenomena, with broad implications in statistical physics, materials
science, and complex networks. However, most machine learning frameworks for
percolation analysis have focused on two-dimensional systems, oversimplifying
the spatial correlations and morphological complexity of real-world
three-dimensional materials. To bridge this gap and improve label efficiency
and scalability in 3D systems, we propose a Siamese Neural Network (SNN) that
leverages features of the largest cluster as discriminative input. Our method
achieves high predictive accuracy for both site and bond percolation thresholds
and critical exponents in three dimensions, with sub-1% error margins using
significantly fewer labeled samples than traditional approaches. This work
establishes a robust and data-efficient framework for modeling high-dimensional
critical phenomena, with potential applications in materials discovery and
complex network analysis.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [470] [Self-Supervised Distillation of Legacy Rule-Based Methods for Enhanced EEG-Based Decision-Making](https://arxiv.org/abs/2507.14542)
*Yipeng Zhang,Yuanyi Ding,Chenda Duan,Atsuro Daida,Hiroki Nariai,Vwani Roychowdhury*

Main category: cs.CE

TL;DR: The paper introduces SS2LD, a self-supervised framework to refine HFO detection in epilepsy treatment, reducing reliance on labeled data and improving precision over legacy detectors.


<details>
  <summary>Details</summary>
Motivation: Traditional HFO detectors have high false positives and require manual review, while supervised methods need labeled data, which is scarce and inconsistently annotated.

Method: SS2LD uses a VAE for morphological pre-training, clusters latent representations for weak supervision, and trains a classifier on real and augmented data.

Result: SS2LD outperforms state-of-the-art methods on multi-institutional datasets, offering scalable and label-efficient HFO detection.

Conclusion: SS2LD provides a clinically effective solution for refining HFO detection using legacy detectors without heavy reliance on labeled data.

Abstract: High-frequency oscillations (HFOs) in intracranial Electroencephalography
(iEEG) are critical biomarkers for localizing the epileptogenic zone in
epilepsy treatment. However, traditional rule-based detectors for HFOs suffer
from unsatisfactory precision, producing false positives that require
time-consuming manual review. Supervised machine learning approaches have been
used to classify the detection results, yet they typically depend on labeled
datasets, which are difficult to acquire due to the need for specialized
expertise. Moreover, accurate labeling of HFOs is challenging due to low
inter-rater reliability and inconsistent annotation practices across
institutions. The lack of a clear consensus on what constitutes a pathological
HFO further challenges supervised refinement approaches. To address this, we
leverage the insight that legacy detectors reliably capture clinically relevant
signals despite their relatively high false positive rates. We thus propose the
Self-Supervised to Label Discovery (SS2LD) framework to refine the large set of
candidate events generated by legacy detectors into a precise set of
pathological HFOs. SS2LD employs a variational autoencoder (VAE) for
morphological pre-training to learn meaningful latent representation of the
detected events. These representations are clustered to derive weak supervision
for pathological events. A classifier then uses this supervision to refine
detection boundaries, trained on real and VAE-augmented data. Evaluated on
large multi-institutional interictal iEEG datasets, SS2LD outperforms
state-of-the-art methods. SS2LD offers a scalable, label-efficient, and
clinically effective strategy to identify pathological HFOs using legacy
detectors.

</details>


### [471] [DiffuMeta: Algebraic Language Models for Inverse Design of Metamaterials via Diffusion Transformers](https://arxiv.org/abs/2507.15753)
*Li Zheng,Siddhant Kumar,Dennis M. Kochmann*

Main category: cs.CE

TL;DR: DiffuMeta is a generative framework using diffusion transformers and algebraic language to design 3D metamaterials, enabling precise control over mechanical properties and diverse solutions.


<details>
  <summary>Details</summary>
Motivation: Existing generative models for material discovery face computational complexity and limited design space representation, hindering 3D metamaterial inverse design.

Method: DiffuMeta integrates diffusion transformers with an algebraic language to encode 3D geometries, generating shell structures with targeted stress-strain responses.

Result: The framework produces diverse solutions, controls multiple mechanical objectives, and validates efficacy through experimental fabrication.

Conclusion: DiffuMeta advances metamaterial design by combining expressive representation and computational efficiency for tailored properties.

Abstract: Generative machine learning models have revolutionized material discovery by
capturing complex structure-property relationships, yet extending these
approaches to the inverse design of three-dimensional metamaterials remains
limited by computational complexity and underexplored design spaces due to the
lack of expressive representations. Here, we present DiffuMeta, a generative
framework integrating diffusion transformers with a novel algebraic language
representation, encoding 3D geometries as mathematical sentences. This compact,
unified parameterization spans diverse topologies while enabling direct
application of transformers to structural design. DiffuMeta leverages diffusion
models to generate novel shell structures with precisely targeted stress-strain
responses under large deformations, accounting for buckling and contact while
addressing the inherent one-to-many mapping by producing diverse solutions.
Uniquely, our approach enables simultaneous control over multiple mechanical
objectives, including linear and nonlinear responses beyond training domains.
Experimental validation of fabricated structures further confirms the efficacy
of our approach for accelerated design of metamaterials and structures with
tailored properties.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [472] [A universal augmentation framework for long-range electrostatics in machine learning interatomic potentials](https://arxiv.org/abs/2507.14302)
*Dongjin Kim,Xiaoyu Wang,Peichen Zhong,Daniel S. King,Theo Jaffrelot Inizan,Bingqing Cheng*

Main category: physics.chem-ph

TL;DR: The paper introduces the Latent Ewald Summation (LES) method, a standalone library for incorporating long-range electrostatics into machine learning interatomic potentials (MLIPs), improving accuracy and compatibility with existing methods.


<details>
  <summary>Details</summary>
Motivation: Current MLIPs lack explicit treatment of long-range electrostatics, limiting their accuracy and applicability. LES addresses this gap by inferring electrostatic interactions from energy and force data.

Method: LES is integrated with short-range MLIPs (e.g., MACE, NequIP) and tested on systems like bulk water and polar dipeptides. It scales to large datasets (e.g., SPICE) for universal MLIPs.

Result: LES-enhanced models show improved accuracy, reliable prediction of dipoles and Born effective charges, and better descriptions of bulk liquids. MACELES-OFF outperforms its short-range counterpart.

Conclusion: LES enables efficient long-range electrostatics without direct training on electrical properties, advancing the development of electrostatic foundation MLIPs.

Abstract: Most current machine learning interatomic potentials (MLIPs) rely on
short-range approximations, without explicit treatment of long-range
electrostatics. To address this, we recently developed the Latent Ewald
Summation (LES) method, which infers electrostatic interactions, polarization,
and Born effective charges (BECs), just by learning from energy and force
training data. Here, we present LES as a standalone library, compatible with
any short-range MLIP, and demonstrate its integration with methods such as
MACE, NequIP, CACE, and CHGNet. We benchmark LES-enhanced models on distinct
systems, including bulk water, polar dipeptides, and gold dimer adsorption on
defective substrates, and show that LES not only captures correct
electrostatics but also improves accuracy. Additionally, we scale LES to large
and chemically diverse data by training MACELES-OFF on the SPICE set containing
molecules and clusters, making a universal MLIP with electrostatics for organic
systems including biomolecules. MACELES-OFF is more accurate than its
short-range counterpart (MACE-OFF) trained on the same dataset, predicts
dipoles and BECs reliably, and has better descriptions of bulk liquids. By
enabling efficient long-range electrostatics without directly training on
electrical properties, LES paves the way for electrostatic foundation MLIPs.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [473] [U-DREAM: Unsupervised Dereverberation guided by a Reverberation Model](https://arxiv.org/abs/2507.14237)
*Louis Bahrman,Mathieu Fontaine,Gaël Richard*

Main category: cs.SD

TL;DR: The paper proposes a weakly-to-fully unsupervised dereverberation method using reverberant signals and an acoustic model, eliminating the need for paired dry-reverberant data.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning methods require paired data, which is hard to obtain. This work aims to address this limitation.

Method: A sequential learning strategy based on a Bayesian formulation is used, with deep neural networks estimating acoustic parameters and dry signals via a reverberation matching loss.

Result: The method outperforms unsupervised baselines with only 100 labelled samples, showing efficiency in low-resource settings.

Conclusion: The approach is effective and practical for dereverberation in scenarios with limited data.

Abstract: This paper explores the outcome of training state-ofthe-art dereverberation
models with supervision settings ranging from weakly-supervised to fully
unsupervised, relying solely on reverberant signals and an acoustic model for
training. Most of the existing deep learning approaches typically require
paired dry and reverberant data, which are difficult to obtain in practice. We
develop instead a sequential learning strategy motivated by a bayesian
formulation of the dereverberation problem, wherein acoustic parameters and dry
signals are estimated from reverberant inputs using deep neural networks,
guided by a reverberation matching loss. Our most data-efficient variant
requires only 100 reverberation-parameter-labelled samples to outperform an
unsupervised baseline, demonstrating the effectiveness and practicality of the
proposed method in low-resource scenarios.

</details>


### [474] [Exploiting Context-dependent Duration Features for Voice Anonymization Attack Systems](https://arxiv.org/abs/2507.15214)
*Natalia Tomashenko,Emmanuel Vincent,Marc Tommasi*

Main category: cs.SD

TL;DR: The paper introduces a method using context-dependent duration embeddings to represent speaker identity from speech dynamics, improving speaker verification and exposing vulnerabilities in voice anonymization systems.


<details>
  <summary>Details</summary>
Motivation: Speech temporal dynamics (rhythm, intonation, rate) carry unique speaker identity information, but existing representations are limited.

Method: Proposes extracting context-dependent duration embeddings from speech dynamics and develops attack models to test speaker verification and anonymization systems.

Result: Attack models significantly improve speaker verification performance for original and anonymized data compared to simpler representations.

Conclusion: The method effectively captures speaker characteristics and highlights vulnerabilities in current voice anonymization systems.

Abstract: The temporal dynamics of speech, encompassing variations in rhythm,
intonation, and speaking rate, contain important and unique information about
speaker identity. This paper proposes a new method for representing speaker
characteristics by extracting context-dependent duration embeddings from speech
temporal dynamics. We develop novel attack models using these representations
and analyze the potential vulnerabilities in speaker verification and voice
anonymization systems.The experimental results show that the developed attack
models provide a significant improvement in speaker verification performance
for both original and anonymized data in comparison with simpler
representations of speech temporal dynamics reported in the literature.

</details>


### [475] [A2TTS: TTS for Low Resource Indian Languages](https://arxiv.org/abs/2507.15272)
*Ayush Singh Bhadoriya,Abhishek Nikunj Shinde,Isha Pandey,Ganesh Ramakrishnan*

Main category: cs.SD

TL;DR: A diffusion-based TTS system for unseen speakers and Indian languages, using speaker embeddings and cross-attention for prosody, with zero-shot capabilities.


<details>
  <summary>Details</summary>
Motivation: Address challenges in generating speech for unseen speakers and support diverse Indian languages.

Method: Leverages diffusion-based TTS with speaker embeddings and cross-attention for duration prediction, plus classifier-free guidance for zero-shot generation.

Result: Speech closely resembles target speakers with improved duration modeling and expressiveness.

Conclusion: The system effectively generates natural speech for diverse Indian languages and unseen speakers.

Abstract: We present a speaker conditioned text-to-speech (TTS) system aimed at
addressing challenges in generating speech for unseen speakers and supporting
diverse Indian languages. Our method leverages a diffusion-based TTS
architecture, where a speaker encoder extracts embeddings from short reference
audio samples to condition the DDPM decoder for multispeaker generation. To
further enhance prosody and naturalness, we employ a cross-attention based
duration prediction mechanism that utilizes reference audio, enabling more
accurate and speaker consistent timing. This results in speech that closely
resembles the target speaker while improving duration modeling and overall
expressiveness. Additionally, to improve zero-shot generation, we employed
classifier free guidance, allowing the system to generate speech more near
speech for unknown speakers. Using this approach, we trained language-specific
speaker-conditioned models. Using the IndicSUPERB dataset for multiple Indian
languages such as Bengali, Gujarati, Hindi, Marathi, Malayalam, Punjabi and
Tamil.

</details>


### [476] [Neuro-MSBG: An End-to-End Neural Model for Hearing Loss Simulation](https://arxiv.org/abs/2507.15396)
*Hui-Guan Yuan,Ryandhimas E. Zezario,Shafique Ahmed,Hsin-Min Wang,Kai-Lung Hua,Yu Tsao*

Main category: cs.SD

TL;DR: Neuro-MSBG is a lightweight, efficient hearing loss simulation model with low latency and high accuracy, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing hearing loss simulation models are computationally complex and lack real-time applicability and integration with speech processing systems.

Method: Proposes Neuro-MSBG, an end-to-end model with a personalized audiogram encoder for time-frequency modeling.

Result: Achieves high SRCC scores (0.9247 for STOI, 0.8671 for PESQ) and reduces runtime by 46x (0.021s for 1s input).

Conclusion: Neuro-MSBG is efficient, practical, and retains intelligibility and perceptual quality, making it suitable for real-time applications.

Abstract: Hearing loss simulation models are essential for hearing aid deployment.
However, existing models have high computational complexity and latency, which
limits real-time applications and lack direct integration with speech
processing systems. To address these issues, we propose Neuro-MSBG, a
lightweight end-to-end model with a personalized audiogram encoder for
effective time-frequency modeling. Experiments show that Neuro-MSBG supports
parallel inference and retains the intelligibility and perceptual quality of
the original MSBG, with a Spearman's rank correlation coefficient (SRCC) of
0.9247 for Short-Time Objective Intelligibility (STOI) and 0.8671 for
Perceptual Evaluation of Speech Quality (PESQ). Neuro-MSBG reduces simulation
runtime by a factor of 46 (from 0.970 seconds to 0.021 seconds for a 1 second
input), further demonstrating its efficiency and practicality.

</details>


<div id='physics.app-ph'></div>

# physics.app-ph [[Back]](#toc)

### [477] [What do Large Language Models know about materials?](https://arxiv.org/abs/2507.14586)
*Adrian Ehrenhofer,Thomas Wallmersperger,Gianaurelio Cuniberti*

Main category: physics.app-ph

TL;DR: The paper explores the application of Large Language Models (LLMs) in mechanical engineering and materials science, focusing on their ability to generate correct material information and proposing a benchmark for assessing their suitability in the Processing-Structure-Property-Performance (PSPP) chain.


<details>
  <summary>Details</summary>
Motivation: LLMs are increasingly used in engineering, but their training on non-scientific internet content raises concerns about their accuracy in generating material-specific knowledge.

Method: The study evaluates LLMs' intrinsic knowledge by examining their performance on the Periodic Table of Elements, emphasizing vocabulary, tokenization, and factual correctness.

Result: The work identifies gaps in LLMs' material knowledge and proposes a benchmark to determine where LLMs are applicable in the PSPP chain and where specialized models are needed.

Conclusion: LLMs show promise for certain steps in the PSPP chain, but specialized models are required for accurate material-specific tasks.

Abstract: Large Language Models (LLMs) are increasingly applied in the fields of
mechanical engineering and materials science. As models that establish
connections through the interface of language, LLMs can be applied for
step-wise reasoning through the Processing-Structure-Property-Performance chain
of material science and engineering. Current LLMs are built for adequately
representing a dataset, which is the most part of the accessible internet.
However, the internet mostly contains non-scientific content. If LLMs should be
applied for engineering purposes, it is valuable to investigate models for
their intrinsic knowledge -- here: the capacity to generate correct information
about materials. In the current work, for the example of the Periodic Table of
Elements, we highlight the role of vocabulary and tokenization for the
uniqueness of material fingerprints, and the LLMs' capabilities of generating
factually correct output of different state-of-the-art open models. This leads
to a material knowledge benchmark for an informed choice, for which steps in
the PSPP chain LLMs are applicable, and where specialized models are required.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [478] [FinSurvival: A Suite of Large Scale Survival Modeling Tasks from Finance](https://arxiv.org/abs/2507.14160)
*Aaron Green,Zihan Nie,Hanzhen Qin,Oshani Seneviratne,Kristin P. Bennett*

Main category: q-fin.ST

TL;DR: The paper introduces FinSurvival, a benchmark suite of 16 survival modeling tasks derived from DeFi transaction data, addressing the lack of large public datasets for AI survival models.


<details>
  <summary>Details</summary>
Motivation: There's a need for large-scale, realistic datasets to benchmark AI survival models, especially in fields like finance and medicine.

Method: An automated pipeline was used to derive 16 survival tasks from DeFi transaction data, predicting time-to-event (e.g., repayment after borrowing). Classification tasks were also created by thresholding survival times.

Result: FinSurvival includes 7.5M records, providing challenging tasks not well addressed by existing methods. It enables AI model evaluation across finance, medicine, and commerce.

Conclusion: FinSurvival fills a gap in survival modeling benchmarks and can be expanded with more DeFi data as cryptocurrency use grows.

Abstract: Survival modeling predicts the time until an event occurs and is widely used
in risk analysis; for example, it's used in medicine to predict the survival of
a patient based on censored data. There is a need for large-scale, realistic,
and freely available datasets for benchmarking artificial intelligence (AI)
survival models. In this paper, we derive a suite of 16 survival modeling tasks
from publicly available transaction data generated by lending of
cryptocurrencies in Decentralized Finance (DeFi). Each task was constructed
using an automated pipeline based on choices of index and outcome events. For
example, the model predicts the time from when a user borrows cryptocurrency
coins (index event) until their first repayment (outcome event). We formulate a
survival benchmark consisting of a suite of 16 survival-time prediction tasks
(FinSurvival). We also automatically create 16 corresponding classification
problems for each task by thresholding the survival time using the restricted
mean survival time. With over 7.5 million records, FinSurvival provides a suite
of realistic financial modeling tasks that will spur future AI survival
modeling research. Our evaluation indicated that these are challenging tasks
that are not well addressed by existing methods. FinSurvival enables the
evaluation of AI survival models applicable to traditional finance, industry,
medicine, and commerce, which is currently hindered by the lack of large public
datasets. Our benchmark demonstrates how AI models could assess opportunities
and risks in DeFi. In the future, the FinSurvival benchmark pipeline can be
used to create new benchmarks by incorporating more DeFi transactions and
protocols as the use of cryptocurrency grows.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [479] [Large Language Model as An Operator: An Experience-Driven Solution for Distribution Network Voltage Control](https://arxiv.org/abs/2507.14800)
*Xu Yang,Chenhui Lin,Haotian Liu,Qi Wang,Wenchuan Wu*

Main category: eess.SY

TL;DR: The paper proposes an LLM-based method for autonomous voltage control in power systems, using experience-driven modules for strategy evolution.


<details>
  <summary>Details</summary>
Motivation: Leverage LLMs' reasoning and analysis capabilities to address power system dispatch challenges, particularly voltage control in distribution networks.

Method: An LLM-based solution with four collaborative modules: experience storage, retrieval, generation, and modification for self-evolving control strategies.

Result: Experiments confirm the method's effectiveness and LLMs' applicability in power system dispatch.

Conclusion: LLMs can autonomously evolve voltage control strategies, offering a promising solution for power system challenges.

Abstract: With the advanced reasoning and information analysis capabilities, large
language models (LLMs) can offer a novel approach for the autonomous generation
of dispatch strategies in power systems. This letter proposes an LLM-based
experience-driven voltage control solution for distribution networks, which
enables the self-evolution of LLM-based voltage control strategies through the
collaboration and interaction of multiple modules-specifically, experience
storage, experience retrieval, experience generation, and experience
modification. Comprehensive experimental results validate the effectiveness of
the proposed method and highlight the applicability of LLM in addressing power
system dispatch challenges.

</details>


### [480] [Physics-Informed Learning of Proprietary Inverter Models for Grid Dynamic Studies](https://arxiv.org/abs/2507.15259)
*Kyung-Bin Kwon,Sayak Mukherjee,Ramij R. Hossain,Marcelo Elizondo*

Main category: eess.SY

TL;DR: A novel Physics-Informed Latent Neural ODE Model (PI-LNM) is proposed to emulate proprietary inverter dynamics, improving grid simulation accuracy by integrating physics with neural learning.


<details>
  <summary>Details</summary>
Motivation: OEMs often withhold inverter internal controls, hindering accurate dynamic simulations and stability studies.

Method: PI-LNM combines system physics and neural learning to capture unmodeled behaviors of proprietary units.

Result: Validated with a grid-forming inverter case, PI-LNM outperforms purely data-driven methods in simulation accuracy.

Conclusion: The PI-LNM framework enhances dynamic simulation accuracy for proprietary inverter dynamics, bridging gaps in current industry practices.

Abstract: This letter develops a novel physics-informed neural ordinary differential
equations-based framework to emulate the proprietary dynamics of the inverters
-- essential for improved accuracy in grid dynamic simulations. In current
industry practice, the original equipment manufacturers (OEMs) often do not
disclose the exact internal controls and parameters of the inverters, posing
significant challenges in performing accurate dynamic simulations and other
relevant studies, such as gain tunings for stability analysis and controls. To
address this, we propose a Physics-Informed Latent Neural ODE Model (PI-LNM)
that integrates system physics with neural learning layers to capture the
unmodeled behaviors of proprietary units. The proposed method is validated
using a grid-forming inverter (GFM) case study, demonstrating improved dynamic
simulation accuracy over approaches that rely solely on data-driven learning
without physics-based guidance.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [481] [FAMST: Fast Approximate Minimum Spanning Tree Construction for Large-Scale and High-Dimensional Data](https://arxiv.org/abs/2507.14261)
*Mahmood K. M. Almansoori,Miklos Telek*

Main category: cs.DS

TL;DR: FAMST is a fast algorithm for constructing approximate Minimum Spanning Trees (MSTs) for large-scale datasets, improving time and space complexity over traditional methods.


<details>
  <summary>Details</summary>
Motivation: Address computational challenges of MST construction for large-scale and high-dimensional datasets.

Method: Three-phase approach: Approximate Nearest Neighbor (ANN) graph construction, ANN inter-component connection, and iterative edge refinement.

Result: Achieves O(dn log n) time and O(dn + kn) space complexity, with speedups up to 1000x and low approximation errors.

Conclusion: FAMST enables MST analysis on previously infeasible scales, with practical guidelines for hyperparameter selection.

Abstract: We present Fast Approximate Minimum Spanning Tree (FAMST), a novel algorithm
that addresses the computational challenges of constructing Minimum Spanning
Trees (MSTs) for large-scale and high-dimensional datasets. FAMST utilizes a
three-phase approach: Approximate Nearest Neighbor (ANN) graph construction,
ANN inter-component connection, and iterative edge refinement. For a dataset of
$n$ points in a $d$-dimensional space, FAMST achieves $\mathcal{O}(dn \log n)$
time complexity and $\mathcal{O}(dn + kn)$ space complexity when $k$ nearest
neighbors are considered, which is a significant improvement over the
$\mathcal{O}(n^2)$ time and space complexity of traditional methods.
  Experiments across diverse datasets demonstrate that FAMST achieves
remarkably low approximation errors while providing speedups of up to
1000$\times$ compared to exact MST algorithms. We analyze how the key
hyperparameters, $k$ (neighborhood size) and $\lambda$ (inter-component edges),
affect performance, providing practical guidelines for hyperparameter
selection. FAMST enables MST-based analysis on datasets with millions of points
and thousands of dimensions, extending the applicability of MST techniques to
problem scales previously considered infeasible.

</details>


### [482] [Differentially Private Synthetic Graphs Preserving Triangle-Motif Cuts](https://arxiv.org/abs/2507.14835)
*Pan Peng,Hangyu Xu*

Main category: cs.DS

TL;DR: The paper introduces a differentially private mechanism for generating synthetic graphs that approximate triangle-motif sizes in cuts, with polynomial runtime and bounded error. It also provides a lower bound for such DP algorithms and generalizes to weighted graphs and other motifs.


<details>
  <summary>Details</summary>
Motivation: Differentially private synthetic graphs are needed to preserve privacy while approximating key graph properties like triangle-motif sizes, which are useful in clustering, sparsification, and social network analysis.

Method: The paper presents an (ε,δ)-DP mechanism that generates a synthetic graph G' from an input graph G, approximating triangle-motif sizes for all cuts with additive error bounds. It also proves a lower bound for such DP algorithms.

Result: The mechanism achieves an additive error of ~O(√(mℓ₃(G))n/ε^(3/2)) and generalizes to weighted graphs. A lower bound of Ω(√(mn)ℓ₃(G)/ε) is established for any DP algorithm.

Conclusion: The work provides a practical DP solution for synthetic graph generation with theoretical guarantees, extending to weighted graphs and broader motif classes.

Abstract: We study the problem of releasing a differentially private (DP) synthetic
graph $G'$ that well approximates the triangle-motif sizes of all cuts of any
given graph $G$, where a motif in general refers to a frequently occurring
subgraph within complex networks. Non-private versions of such graphs have
found applications in diverse fields such as graph clustering, graph
sparsification, and social network analysis. Specifically, we present the first
$(\varepsilon,\delta)$-DP mechanism that, given an input graph $G$ with $n$
vertices, $m$ edges and local sensitivity of triangles $\ell_{3}(G)$, generates
a synthetic graph $G'$ in polynomial time, approximating the triangle-motif
sizes of all cuts $(S,V\setminus S)$ of the input graph $G$ up to an additive
error of $\tilde{O}(\sqrt{m\ell_{3}(G)}n/\varepsilon^{3/2})$. Additionally, we
provide a lower bound of $\Omega(\sqrt{mn}\ell_{3}(G)/\varepsilon)$ on the
additive error for any DP algorithm that answers the triangle-motif size
queries of all $(S,T)$-cut of $G$. Finally, our algorithm generalizes to
weighted graphs, and our lower bound extends to any $K_h$-motif cut for any
constant $h\geq 2$.

</details>


### [483] [Language Generation in the Limit: Noise, Loss, and Feedback](https://arxiv.org/abs/2507.15319)
*Yannan Bai,Debmalya Panigrahi,Ian Zhang*

Main category: cs.DS

TL;DR: The paper resolves questions about language generation in the limit, showing unions of uniformly and non-uniformly generatable collections may not be generatable. It also explores variants like noisy generation, generation without samples, and feedback-based generation, providing characterizations and separations.


<details>
  <summary>Details</summary>
Motivation: To clarify the limits of language generation in the limit, particularly regarding union-closedness, and to explore and characterize natural variants involving noise, loss, and feedback.

Method: The authors construct counterexamples to disprove union-closedness for non-uniform generation and use these constructions to analyze variants like noisy generation, generation without samples, and feedback-based generation.

Result: The union of a uniformly and non-uniformly generatable collection may not be generatable in the limit. Equivalences and separations are shown for noisy and non-noisy generation, and feedback-based generation is characterized.

Conclusion: The paper resolves key questions about language generation in the limit and provides insights into variants involving noise, loss, and feedback, advancing the understanding of algorithmic language generation.

Abstract: Kleinberg and Mullainathan (2024) recently proposed a formal framework called
language generation in the limit and showed that given a sequence of example
strings from an unknown target language drawn from any countable collection, an
algorithm can correctly generate unseen strings from the target language within
finite time. This notion was further refined by Li, Raman, and Tewari (2024),
who defined stricter categories of non-uniform and uniform generation. They
showed that a finite union of uniformly generatable collections is generatable
in the limit, and asked if the same is true for non-uniform generation.
  We begin by resolving the question in the negative: we give a uniformly
generatable collection and a non-uniformly generatable collection whose union
is not generatable in the limit. We then use facets of this construction to
further our understanding of several variants of language generation. The first
two, generation with noise and without samples, were introduced by Raman and
Raman (2025) and Li, Raman, and Tewari (2024) respectively. We show the
equivalence of these models for uniform and non-uniform generation, and provide
a characterization of non-uniform noisy generation. The former paper asked if
there is any separation between noisy and non-noisy generation in the limit --
we show that such a separation exists even with a single noisy string. Finally,
we study the framework of generation with feedback, introduced by Charikar and
Pabbaraju (2025), where the algorithm is strengthened by allowing it to ask
membership queries. We show finite queries add no power, but infinite queries
yield a strictly more powerful model.
  In summary, the results in this paper resolve the union-closedness of
language generation in the limit, and leverage those techniques (and others) to
give precise characterizations for natural variants that incorporate noise,
loss, and feedback.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [484] [Collusion-Resilient Hierarchical Secure Aggregation with Heterogeneous Security Constraints](https://arxiv.org/abs/2507.14768)
*Zhou Li,Xiang Zhang,Jiawen Lv,Jihao Fan,Haiqiang Chen,Giuseppe Caire*

Main category: cs.IT

TL;DR: The paper explores weakly-secure hierarchical secure aggregation (WS-HSA) to address heterogeneous security requirements in federated learning, optimizing key rates for flexible security configurations.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the challenge of heterogeneous security requirements in hierarchical secure aggregation (HSA) as user numbers grow, where varying levels of input protection are needed across clusters.

Method: The paper investigates WS-HSA with collusion resilience, defining security input sets and collusion sets to protect specific user groups. It characterizes optimal key rates and provides bounds for other cases.

Result: The work characterizes the optimal total key rate for many parameter configurations and establishes bounds for the remaining cases, ensuring constant-factor gap optimality.

Conclusion: WS-HSA offers a flexible framework for addressing heterogeneous security in HSA, with practical guarantees on key rate efficiency.

Abstract: Motivated by federated learning (FL), secure aggregation (SA) aims to
securely compute, as efficiently as possible, the sum of a set of inputs
distributed across many users. To understand the impact of network topology,
hierarchical secure aggregation (HSA) investigated the communication and secret
key generation efficiency in a 3-layer relay network, where clusters of users
are connected to the aggregation server through an intermediate layer of
relays. Due to the pre-aggregation of the messages at the relays, HSA reduces
the communication burden on the relay-to-server links and is able to support a
large number of users. However, as the number of users increases, a practical
challenge arises from heterogeneous security requirements--for example, users
in different clusters may require varying levels of input protection. Motivated
by this, we study weakly-secure HSA (WS-HSA) with collusion resilience, where
instead of protecting all the inputs from any set of colluding users, only the
inputs belonging to a predefined collection of user groups (referred to as
security input sets) need to be protected against another predefined collection
of user groups (referred to as collusion sets). Since the security input sets
and collusion sets can be arbitrarily defined, our formulation offers a
flexible framework for addressing heterogeneous security requirements in HSA.
We characterize the optimal total key rate, i.e., the total number of
independent key symbols required to ensure both server and relay security, for
a broad range of parameter configurations. For the remaining cases, we
establish lower and upper bounds on the optimal key rate, providing
constant-factor gap optimality guarantees.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [485] [Real-Time Scene Reconstruction using Light Field Probes](https://arxiv.org/abs/2507.14624)
*Yaru Liu,Derek Nowrouzezahri,Morgan Mcguire*

Main category: cs.GR

TL;DR: A novel method for reconstructing large-scale scenes without explicit geometry, using probe data for efficient rendering and scalability.


<details>
  <summary>Details</summary>
Motivation: Addressing the inefficiency of current neural rendering and geometry-based methods in handling large-scale scenes.

Method: Uses probe data structure to create multi-scale implicit representations of scene geometries from sparse images.

Result: Achieves efficient rendering independent of scene complexity, with potential for VR/AR applications.

Conclusion: Proposes a scalable, geometry-free approach for high-fidelity scene reconstruction and rendering.

Abstract: Reconstructing photo-realistic large-scale scenes from images, for example at
city scale, is a long-standing problem in computer graphics. Neural rendering
is an emerging technique that enables photo-realistic image synthesis from
previously unobserved viewpoints; however, state-of-the-art neural rendering
methods have difficulty efficiently rendering a high complex large-scale scene
because these methods typically trade scene size, fidelity, and rendering speed
for quality. The other stream of techniques utilizes scene geometries for
reconstruction. But the cost of building and maintaining a large set of
geometry data increases as scene size grows. Our work explores novel view
synthesis methods that efficiently reconstruct complex scenes without explicit
use of scene geometries. Specifically, given sparse images of the scene
(captured from the real world), we reconstruct intermediate, multi-scale,
implicit representations of scene geometries. In this way, our method avoids
explicitly relying on scene geometry, significantly reducing the computational
cost of maintaining large 3D data. Unlike current methods, we reconstruct the
scene using a probe data structure. Probe data hold highly accurate depth
information of dense data points, enabling the reconstruction of highly complex
scenes. By reconstructing the scene using probe data, the rendering cost is
independent of the complexity of the scene. As such, our approach combines
geometry reconstruction and novel view synthesis. Moreover, when rendering
large-scale scenes, compressing and streaming probe data is more efficient than
using explicit scene geometry. Therefore, our neural representation approach
can potentially be applied to virtual reality (VR) and augmented reality (AR)
applications.

</details>


### [486] [Towards Geometric and Textural Consistency 3D Scene Generation via Single Image-guided Model Generation and Layout Optimization](https://arxiv.org/abs/2507.14841)
*Xiang Tang,Ruotong Li,Xiaopeng Fan*

Main category: cs.GR

TL;DR: A novel three-stage framework for 3D scene generation from a single RGB image, ensuring object quality and scene coherence via geometric representations and layout optimization.


<details>
  <summary>Details</summary>
Motivation: Current methods struggle with object generation quality and scene coherence in multi-object scenarios from single images.

Method: Three-stage framework: image instance segmentation/inpainting, pseudo-stereo viewpoint construction for geometry capture, and layout optimization via Chamfer distance minimization.

Result: Outperforms state-of-the-art in geometric accuracy, texture fidelity, and scene layout synthesis.

Conclusion: The proposed method effectively addresses challenges in 3D scene generation from single images, achieving superior results.

Abstract: In recent years, 3D generation has made great strides in both academia and
industry. However, generating 3D scenes from a single RGB image remains a
significant challenge, as current approaches often struggle to ensure both
object generation quality and scene coherence in multi-object scenarios. To
overcome these limitations, we propose a novel three-stage framework for 3D
scene generation with explicit geometric representations and high-quality
textural details via single image-guided model generation and spatial layout
optimization. Our method begins with an image instance segmentation and
inpainting phase, which recovers missing details of occluded objects in the
input images, thereby achieving complete generation of foreground 3D assets.
Subsequently, our approach captures the spatial geometry of reference image by
constructing pseudo-stereo viewpoint for camera parameter estimation and scene
depth inference, while employing a model selection strategy to ensure optimal
alignment between the 3D assets generated in the previous step and the input.
Finally, through model parameterization and minimization of the Chamfer
distance between point clouds in 3D and 2D space, our approach optimizes layout
parameters to produce an explicit 3D scene representation that maintains
precise alignment with input guidance image. Extensive experiments on
multi-object scene image sets have demonstrated that our approach not only
outperforms state-of-the-art methods in terms of geometric accuracy and texture
fidelity of individual generated 3D models, but also has significant advantages
in scene layout synthesis.

</details>


### [487] [Blended Point Cloud Diffusion for Localized Text-guided Shape Editing](https://arxiv.org/abs/2507.15399)
*Etai Sella,Noam Atia,Ron Mokady,Hadar Averbuch-Elor*

Main category: cs.GR

TL;DR: A framework for fine-grained 3D shape editing using natural language, leveraging inpainting and a coordinate blending algorithm to preserve global coherence and shape identity.


<details>
  <summary>Details</summary>
Motivation: Prior methods struggle with maintaining global coherence during localized 3D shape edits. This work aims to address this challenge.

Method: Uses an inpainting-based framework with foundation 3D diffusion models and introduces a coordinate blending algorithm for identity preservation.

Result: Outperforms alternatives in fidelity to the original shape and adherence to textual descriptions.

Conclusion: The proposed method enables precise 3D shape edits while preserving coherence and identity, avoiding costly inversion.

Abstract: Natural language offers a highly intuitive interface for enabling localized
fine-grained edits of 3D shapes. However, prior works face challenges in
preserving global coherence while locally modifying the input 3D shape. In this
work, we introduce an inpainting-based framework for editing shapes represented
as point clouds. Our approach leverages foundation 3D diffusion models for
achieving localized shape edits, adding structural guidance in the form of a
partial conditional shape, ensuring that other regions correctly preserve the
shape's identity. Furthermore, to encourage identity preservation also within
the local edited region, we propose an inference-time coordinate blending
algorithm which balances reconstruction of the full shape with inpainting at a
progression of noise levels during the inference process. Our coordinate
blending algorithm seamlessly blends the original shape with its edited
version, enabling a fine-grained editing of 3D shapes, all while circumventing
the need for computationally expensive and often inaccurate inversion.
Extensive experiments show that our method outperforms alternative techniques
across a wide range of metrics that evaluate both fidelity to the original
shape and also adherence to the textual description.

</details>


### [488] [ObjectGS: Object-aware Scene Reconstruction and Scene Understanding via Gaussian Splatting](https://arxiv.org/abs/2507.15454)
*Ruijie Zhu,Mulin Yu,Linning Xu,Lihan Jiang,Yixuan Li,Tianzhu Zhang,Jiangmiao Pang,Bo Dai*

Main category: cs.GR

TL;DR: ObjectGS enhances 3D Gaussian Splatting by adding semantic understanding, enabling object-level reconstruction and outperforming state-of-the-art methods in segmentation tasks.


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian Splatting lacks semantic understanding, limiting object-level perception. ObjectGS addresses this by unifying reconstruction with semantic awareness.

Method: ObjectGS models individual objects as local anchors with shared IDs, dynamically growing/pruning anchors and optimizing features. A one-hot ID encoding with classification loss enforces semantics.

Result: ObjectGS outperforms state-of-the-art methods in open-vocabulary and panoptic segmentation and integrates well with mesh extraction and scene editing.

Conclusion: ObjectGS successfully combines high-fidelity 3D reconstruction with semantic understanding, advancing object-level perception and practical applications.

Abstract: 3D Gaussian Splatting is renowned for its high-fidelity reconstructions and
real-time novel view synthesis, yet its lack of semantic understanding limits
object-level perception. In this work, we propose ObjectGS, an object-aware
framework that unifies 3D scene reconstruction with semantic understanding.
Instead of treating the scene as a unified whole, ObjectGS models individual
objects as local anchors that generate neural Gaussians and share object IDs,
enabling precise object-level reconstruction. During training, we dynamically
grow or prune these anchors and optimize their features, while a one-hot ID
encoding with a classification loss enforces clear semantic constraints. We
show through extensive experiments that ObjectGS not only outperforms
state-of-the-art methods on open-vocabulary and panoptic segmentation tasks,
but also integrates seamlessly with applications like mesh extraction and scene
editing. Project page: https://ruijiezhu94.github.io/ObjectGS_page

</details>


### [489] [Gaussian Splatting with Discretized SDF for Relightable Assets](https://arxiv.org/abs/2507.15629)
*Zuo-Liang Zhu,Jian Yang,Beibei Wang*

Main category: cs.GR

TL;DR: The paper introduces a discretized SDF method for 3D Gaussian splatting to improve inverse rendering quality without extra memory or complex training.


<details>
  <summary>Details</summary>
Motivation: The discrete nature of Gaussian primitives in 3DGS complicates geometry constraints for inverse rendering. Existing methods using continuous SDF increase memory and training complexity.

Method: A discretized SDF is encoded within each Gaussian, linked to opacity via transformation, and regularized using a projection-based consistency loss.

Result: The method achieves higher relighting quality, outperforms existing Gaussian-based inverse rendering methods, and avoids extra memory or complex optimization.

Conclusion: The discretized SDF approach effectively balances quality and efficiency for inverse rendering in 3DGS.

Abstract: 3D Gaussian splatting (3DGS) has shown its detailed expressive ability and
highly efficient rendering speed in the novel view synthesis (NVS) task. The
application to inverse rendering still faces several challenges, as the
discrete nature of Gaussian primitives makes it difficult to apply geometry
constraints. Recent works introduce the signed distance field (SDF) as an extra
continuous representation to regularize the geometry defined by Gaussian
primitives. It improves the decomposition quality, at the cost of increasing
memory usage and complicating training. Unlike these works, we introduce a
discretized SDF to represent the continuous SDF in a discrete manner by
encoding it within each Gaussian using a sampled value. This approach allows us
to link the SDF with the Gaussian opacity through an SDF-to-opacity
transformation, enabling rendering the SDF via splatting and avoiding the
computational cost of ray marching.The key challenge is to regularize the
discrete samples to be consistent with the underlying SDF, as the discrete
representation can hardly apply the gradient-based constraints (\eg Eikonal
loss). For this, we project Gaussians onto the zero-level set of SDF and
enforce alignment with the surface from splatting, namely a projection-based
consistency loss. Thanks to the discretized SDF, our method achieves higher
relighting quality, while requiring no extra memory beyond GS and avoiding
complex manually designed optimization. The experiments reveal that our method
outperforms existing Gaussian-based inverse rendering methods. Our code is
available at https://github.com/NK-CS-ZZL/DiscretizedSDF.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [490] [KinForm: Kinetics Informed Feature Optimised Representation Models for Enzyme $k_{cat}$ and $K_{M}$ Prediction](https://arxiv.org/abs/2507.14639)
*Saleh Alwer,Ronan Fleming*

Main category: q-bio.QM

TL;DR: KinForm improves enzyme kinetic parameter prediction by optimizing protein feature representations, combining multiple embeddings, and using PCA and oversampling.


<details>
  <summary>Details</summary>
Motivation: Experimental data for enzyme kinetics is limited; existing methods use simplistic protein representations.

Method: Combines residue-level embeddings, applies weighted pooling, PCA for dimensionality reduction, and oversampling.

Result: Outperforms baselines, especially in low-similarity bins, with improvements from pooling, layer selection, PCA, and oversampling.

Conclusion: KinForm enhances prediction accuracy and generalization, advocating for non-overlapping sequence splits in benchmarking.

Abstract: Kinetic parameters such as the turnover number ($k_{cat}$) and Michaelis
constant ($K_{\mathrm{M}}$) are essential for modelling enzymatic activity but
experimental data remains limited in scale and diversity. Previous methods for
predicting enzyme kinetics typically use mean-pooled residue embeddings from a
single protein language model to represent the protein. We present KinForm, a
machine learning framework designed to improve predictive accuracy and
generalisation for kinetic parameters by optimising protein feature
representations. KinForm combines several residue-level embeddings
(Evolutionary Scale Modeling Cambrian, Evolutionary Scale Modeling 2, and
ProtT5-XL-UniRef50), taken from empirically selected intermediate transformer
layers and applies weighted pooling based on per-residue binding-site
probability. To counter the resulting high dimensionality, we apply
dimensionality reduction using principal--component analysis (PCA) on
concatenated protein features, and rebalance the training data via a
similarity-based oversampling strategy. KinForm outperforms baseline methods on
two benchmark datasets. Improvements are most pronounced in low sequence
similarity bins. We observe improvements from binding-site probability pooling,
intermediate-layer selection, PCA, and oversampling of low-identity proteins.
We also find that removing sequence overlap between folds provides a more
realistic evaluation of generalisation and should be the standard over random
splitting when benchmarking kinetic prediction models.

</details>


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [491] [Learning Null Geodesics for Gravitational Lensing Rendering in General Relativity](https://arxiv.org/abs/2507.15775)
*Mingyuan Sun,Zheng Fang,Jiaxu Wang,Kunyi Zhang,Qiang Zhang,Renjing Xu*

Main category: gr-qc

TL;DR: GravLensX uses neural networks to render black holes with gravitational lensing, reducing computational time by 15x compared to traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional methods for rendering black holes with gravitational lensing are computationally expensive. GravLensX aims to provide a more efficient alternative.

Method: Train neural networks to model spacetime around black holes and use them to simulate light paths affected by gravitational lensing.

Result: Achieves accurate visualizations with a 15x reduction in computational time for black hole systems with optically thin accretion disks.

Conclusion: Neural networks are a promising tool for efficient and scalable astronomical visualization of complex phenomena like black holes.

Abstract: We present GravLensX, an innovative method for rendering black holes with
gravitational lensing effects using neural networks. The methodology involves
training neural networks to fit the spacetime around black holes and then
employing these trained models to generate the path of light rays affected by
gravitational lensing. This enables efficient and scalable simulations of black
holes with optically thin accretion disks, significantly decreasing the time
required for rendering compared to traditional methods. We validate our
approach through extensive rendering of multiple black hole systems with
superposed Kerr metric, demonstrating its capability to produce accurate
visualizations with significantly $15\times$ reduced computational time. Our
findings suggest that neural networks offer a promising alternative for
rendering complex astrophysical phenomena, potentially paving a new path to
astronomical visualization.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [492] [Generating executable oracles to check conformance of client code to requirements of JDK Javadocs using LLMs](https://arxiv.org/abs/2411.01789)
*Shan Jiang,Chenguang Zhu,Sarfraz Khurshid*

Main category: cs.SE

TL;DR: The paper proposes using Javadocs and large language models (LLMs) to automate test oracle generation for Java libraries, achieving high accuracy in generating compilable and correct oracles.


<details>
  <summary>Details</summary>
Motivation: Automating test oracles is challenging due to the need for expected behavior knowledge, often only known to developers. The paper leverages Javadocs and LLMs to address this gap.

Method: Uses Javadocs of Java libraries (e.g., java.lang, java.util) and LLMs to generate test oracles, evaluating their compilability and accuracy.

Result: 98.8% of generated oracles are compilable, and 96.4% accurately reflect intended properties. Minor errors can be corrected with additional LLM-generated comments.

Conclusion: LLMs combined with Javadocs effectively automate test oracle generation, offering a practical solution for improving test suite quality.

Abstract: Software testing remains the most widely used methodology for validating
quality of code. However, effectiveness of testing critically depends on the
quality of test suites used. Test cases in a test suite consist of two
fundamental parts: (1) input values for the code under test, and (2) correct
checks for the outputs it produces. These checks are commonly written as
assertions, and termed test oracles. The last couple of decades have seen much
progress in automated test input generation, e.g., using fuzzing and symbolic
execution. However, automating test oracles remains a relatively less explored
problem area. Indeed, a test oracle by its nature requires knowledge of
expected behavior, which may only be known to the developer and may not not
exist in a formal language that supports automated reasoning.
  Our focus in this paper is automation of test oracles for clients of widely
used Java libraries, e.g., java.lang and java.util packages. Our key insight is
that Javadocs that provide a rich source of information can enable automated
generation of test oracles. Javadocs of the core Java libraries are fairly
detailed documents that contain natural language descriptions of not only how
the libraries behave but also how the clients must (not) use them. We use large
language models as an enabling technology to embody our insight into a
framework for test oracle automation, and evaluate it experimentally. Our
experiments demonstrate that LLMs can generate oracles for checking normal and
exceptional behaviors from Javadocs, with 98.8% of these oracles being
compilable and 96.4% accurately reflecting intended properties. Even for the
few incorrect oracles, errors are minor and can be easily corrected with the
help of additional comment information generated by the LLMs.

</details>


### [493] [On the Effectiveness of Large Language Models in Writing Alloy Formulas](https://arxiv.org/abs/2502.15441)
*Yang Hong,Shan Jiang,Yulei Fu,Sarfraz Khurshid*

Main category: cs.SE

TL;DR: The paper explores using LLMs (ChatGPT and DeepSeek) to write, modify, and complete declarative Alloy formulas from natural language or partial specifications, showing promising results.


<details>
  <summary>Details</summary>
Motivation: Writing correct declarative specifications is challenging, and LLMs could simplify this process, enhancing software dependability.

Method: Three tasks: 1) Generate complete Alloy formulas from natural language. 2) Create equivalent Alloy formulas. 3) Complete partial Alloy sketches. Evaluated on 11 specifications using ChatGPT and DeepSeek.

Result: LLMs perform well in synthesizing Alloy formulas, enumerating unique solutions, and completing sketches without test cases.

Conclusion: LLMs advance specification writing, potentially improving software robustness and development.

Abstract: Declarative specifications have a vital role to play in developing safe and
dependable software systems. Writing specifications correctly, however, remains
particularly challenging. This paper presents a controlled experiment on using
large language models (LLMs) to write declarative formulas in the well-known
language Alloy. Our use of LLMs is three-fold. One, we employ LLMs to write
complete Alloy formulas from given natural language descriptions (in English).
Two, we employ LLMs to create alternative but equivalent formulas in Alloy with
respect to given Alloy formulas. Three, we employ LLMs to complete sketches of
Alloy formulas and populate the holes in the sketches by synthesizing Alloy
expressions and operators so that the completed formulas accurately represent
the desired properties (that are given in natural language). We conduct the
experimental evaluation using 11 well-studied subject specifications and employ
two popular LLMs, namely ChatGPT and DeepSeek. The experimental results show
that the LLMs generally perform well in synthesizing complete Alloy formulas
from input properties given in natural language or in Alloy, and are able to
enumerate multiple unique solutions. Moreover, the LLMs are also successful at
completing given sketches of Alloy formulas with respect to natural language
descriptions of desired properties (without requiring test cases). We believe
LLMs offer a very exciting advance in our ability to write specifications, and
can help make specifications take a pivotal role in software development and
enhance our ability to build robust software.

</details>


### [494] [Impact of Code Context and Prompting Strategies on Automated Unit Test Generation with Modern General-Purpose Large Language Models](https://arxiv.org/abs/2507.14256)
*Jakub Walczak,Piotr Tomalak,Artur Laskowski*

Main category: cs.SE

TL;DR: The paper explores how code context and prompting strategies affect unit test generation by LLMs, finding docstrings and chain-of-thought prompting improve quality, with M5 (Gemini 2.5 Pro) performing best.


<details>
  <summary>Details</summary>
Motivation: To enhance productivity in software development by automating unit test generation using LLMs, focusing on code context and prompting strategies.

Method: Investigates the impact of docstrings, full implementation context, and chain-of-thought prompting on unit test quality across various LLMs.

Result: Docstrings notably improve code adequacy; chain-of-thought prompting achieves 96.3% branch coverage, 57% mutation score, and high compilation success. M5 (Gemini 2.5 Pro) performs best.

Conclusion: Automating unit test generation with LLMs, especially using chain-of-thought prompting and docstrings, can significantly boost productivity, with M5 being the top-performing model.

Abstract: Generative AI is gaining increasing attention in software engineering, where
testing remains an indispensable reliability mechanism. According to the widely
adopted testing pyramid, unit tests constitute the majority of test cases and
are often schematic, requiring minimal domain expertise. Automatically
generating such tests under the supervision of software engineers can
significantly enhance productivity during the development phase of the software
lifecycle.
  This paper investigates the impact of code context and prompting strategies
on the quality and adequacy of unit tests generated by various large language
models (LLMs) across several families. The results show that including
docstrings notably improves code adequacy, while further extending context to
the full implementation yields definitely smaller gains. Notably, the
chain-of-thought prompting strategy -- applied even to 'reasoning' models --
achieves the best results, with up to 96.3\% branch coverage, a 57\% average
mutation score, and near-perfect compilation success rate. Among the evaluated
models, M5 (Gemini 2.5 Pro) demonstrated superior performance in both mutation
score and branch coverage being still in top in terms of compilation success
rate.
  All the code and resulting test suites are publicly available at
https://github.com/peetery/LLM-analysis.

</details>


### [495] [The Rise of AI Teammates in Software Engineering (SE) 3.0: How Autonomous Coding Agents Are Reshaping Software Engineering](https://arxiv.org/abs/2507.15003)
*Hao Li,Haoxiang Zhang,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: The paper introduces AIDev, a large-scale dataset capturing the operations of autonomous coding agents in real-world software development, providing empirical data for research on AI teammates in SE 3.0.


<details>
  <summary>Details</summary>
Motivation: To address the lack of empirical data on AI teammates in software engineering, the paper aims to provide a foundation for studying their impact, collaboration, and governance.

Method: The authors compile AIDev, a dataset of 456,000 pull requests from five leading coding agents across 61,000 repositories and 47,000 developers, including metadata on PRs, authorship, and outcomes.

Result: Findings show AI agents outperform humans in speed but face lower PR acceptance rates and produce simpler code, highlighting a trust and utility gap.

Conclusion: AIDev serves as a living resource for the SE and AI communities, enabling research into AI-native workflows and human-AI collaboration.

Abstract: The future of software engineering--SE 3.0--is unfolding with the rise of AI
teammates: autonomous, goal-driven systems collaborating with human developers.
Among these, autonomous coding agents are especially transformative, now
actively initiating, reviewing, and evolving code at scale. This paper
introduces AIDev, the first large-scale dataset capturing how such agents
operate in the wild. Spanning over 456,000 pull requests by five leading
agents--OpenAI Codex, Devin, GitHub Copilot, Cursor, and Claude Code--across
61,000 repositories and 47,000 developers, AIDev provides an unprecedented
empirical foundation for studying autonomous teammates in software development.
  Unlike prior work that has largely theorized the rise of AI-native software
engineering, AIDev offers structured, open data to support research in
benchmarking, agent readiness, optimization, collaboration modeling, and AI
governance. The dataset includes rich metadata on PRs, authorship, review
timelines, code changes, and integration outcomes--enabling exploration beyond
synthetic benchmarks like SWE-bench. For instance, although agents often
outperform humans in speed, their PRs are accepted less frequently, revealing a
trust and utility gap. Furthermore, while agents accelerate code
submission--one developer submitted as many PRs in three days as they had in
three years--these are structurally simpler (via code complexity metrics).
  We envision AIDev as a living resource: extensible, analyzable, and ready for
the SE and AI communities. Grounding SE 3.0 in real-world evidence, AIDev
enables a new generation of research into AI-native workflows and supports
building the next wave of symbiotic human-AI collaboration. The dataset is
publicly available at https://github.com/SAILResearch/AI_Teammates_in_SE3.
  > AI Agent, Agentic AI, Coding Agent, Agentic Coding, Software Engineering
Agent

</details>


### [496] [Survey of GenAI for Automotive Software Development: From Requirements to Executable Code](https://arxiv.org/abs/2507.15025)
*Nenad Petrovic,Vahid Zolfaghari,Andre Schamschurko,Sven Kirchner,Fengjunjie Pan,Chengdng Wu,Nils Purschke,Aleksei Velsh,Krzysztof Lebioda,Yinglei Song,Yi Zhang,Lukasz Mazur,Alois Knoll*

Main category: cs.SE

TL;DR: The paper explores GenAI adoption in automotive software development, focusing on requirements, compliance, and code generation, using LLMs, RAG, and VLMs, and proposes a workflow based on findings.


<details>
  <summary>Details</summary>
Motivation: To reduce human effort and costs in lengthy, standardized automotive software development processes by leveraging GenAI.

Method: Literature review of GenAI technologies (LLMs, RAG, VLMs) and prompting techniques, plus a survey of industry partners.

Result: A generalized GenAI-aided workflow for automotive software development and insights from industry adoption.

Conclusion: GenAI shows promise in streamlining automotive software development, with practical adoption already underway in the industry.

Abstract: Adoption of state-of-art Generative Artificial Intelligence (GenAI) aims to
revolutionize many industrial areas by reducing the amount of human
intervention needed and effort for handling complex underlying processes.
Automotive software development is considered to be a significant area for
GenAI adoption, taking into account lengthy and expensive procedures, resulting
from the amount of requirements and strict standardization. In this paper, we
explore the adoption of GenAI for various steps of automotive software
development, mainly focusing on requirements handling, compliance aspects and
code generation. Three GenAI-related technologies are covered within the
state-of-art: Large Language Models (LLMs), Retrieval Augmented Generation
(RAG), Vision Language Models (VLMs), as well as overview of adopted prompting
techniques in case of code generation. Additionally, we also derive a
generalized GenAI-aided automotive software development workflow based on our
findings from this literature review. Finally, we include a summary of a survey
outcome, which was conducted among our automotive industry partners regarding
the type of GenAI tools used for their daily work activities.

</details>


### [497] [Can LLMs Generate User Stories and Assess Their Quality?](https://arxiv.org/abs/2507.15157)
*Giovanni Quattrocchi,Liliana Pasquale,Paola Spoletini,Luciano Baresi*

Main category: cs.SE

TL;DR: The paper explores using LLMs to automate requirements elicitation in agile frameworks, comparing LLM-generated user stories (US) with human-generated ones. Results show LLMs match human quality in coverage and style but lack diversity and creativity. LLMs can also assess semantic quality with clear criteria.


<details>
  <summary>Details</summary>
Motivation: Requirements elicitation is challenging and time-consuming, especially in agile frameworks. Automating this process with LLMs could improve efficiency and quality.

Method: Used 10 state-of-the-art LLMs to generate US by emulating customer interviews. Compared LLM-generated US with human-generated ones (experts and students) and evaluated semantic quality assessment by LLMs.

Result: LLMs generate US comparable to humans in coverage and style but with lower diversity and creativity. They meet acceptance criteria less often. LLMs can reliably assess semantic quality with clear criteria.

Conclusion: LLMs show promise in automating requirements elicitation and semantic quality assessment, though they need improvement in diversity and creativity to fully match human performance.

Abstract: Requirements elicitation is still one of the most challenging activities of
the requirements engineering process due to the difficulty requirements
analysts face in understanding and translating complex needs into concrete
requirements. In addition, specifying high-quality requirements is crucial, as
it can directly impact the quality of the software to be developed. Although
automated tools allow for assessing the syntactic quality of requirements,
evaluating semantic metrics (e.g., language clarity, internal consistency)
remains a manual and time-consuming activity. This paper explores how LLMs can
help automate requirements elicitation within agile frameworks, where
requirements are defined as user stories (US). We used 10 state-of-the-art LLMs
to investigate their ability to generate US automatically by emulating customer
interviews. We evaluated the quality of US generated by LLMs, comparing it with
the quality of US generated by humans (domain experts and students). We also
explored whether and how LLMs can be used to automatically evaluate the
semantic quality of US. Our results indicate that LLMs can generate US similar
to humans in terms of coverage and stylistic quality, but exhibit lower
diversity and creativity. Although LLM-generated US are generally comparable in
quality to those created by humans, they tend to meet the acceptance quality
criteria less frequently, regardless of the scale of the LLM model. Finally,
LLMs can reliably assess the semantic quality of US when provided with clear
evaluation criteria and have the potential to reduce human effort in
large-scale assessments.

</details>


### [498] [SimdBench: Benchmarking Large Language Models for SIMD-Intrinsic Code Generation](https://arxiv.org/abs/2507.15224)
*Yibo He,Shuoran Zhao,Jiaming Huang,Yingjie Fu,Hao Yu,Cunjian Huang,Tao Xie*

Main category: cs.SE

TL;DR: SimdBench is the first benchmark for evaluating LLMs in SIMD-intrinsic code generation, revealing performance gaps and suggesting future improvements.


<details>
  <summary>Details</summary>
Motivation: To assess LLMs' capability in generating vectorized code using SIMD intrinsics, an area not covered by existing benchmarks.

Method: Developed SimdBench with 136 tasks targeting five SIMD intrinsics (SSE, AVX, Neon, SVE, RVV) and evaluated 18 LLMs for correctness and performance.

Result: LLMs show reduced pass@k in SIMD-intrinsic code generation compared to scalar code, highlighting challenges.

Conclusion: SimdBench provides insights for advancing LLMs in SIMD-intrinsic programming, with open-source availability for community use.

Abstract: SIMD (Single Instruction Multiple Data) instructions and their compiler
intrinsics are widely supported by modern processors to accelerate
performance-critical tasks. SIMD intrinsic programming, a trade-off between
coding productivity and high performance, is widely used in the development of
mainstream performance-critical libraries and daily computing tasks. Large
Language Models (LLMs), which have demonstrated strong and comprehensive
capabilities in code generation, show promise in assisting programmers with the
challenges of SIMD intrinsic programming. However, existing code-generation
benchmarks focus on only scalar code, and it is unclear how LLMs perform in
generating vectorized code using SIMD intrinsics. To fill this gap, we propose
SimdBench, the first code benchmark specifically designed for SIMD-intrinsic
code generation, comprising 136 carefully crafted tasks and targeting five
representative SIMD intrinsics: SSE (x86 Streaming SIMD Extension), AVX (x86
Advanced Vector Extension), Neon (ARM Advanced SIMD Extension), SVE (ARM
Scalable Vector Extension), and RVV (RISC-V Vector Extension). We conduct a
systematic evaluation (measuring both correctness and performance) of 18
representative LLMs on SimdBench, resulting in a series of novel and insightful
findings. Our evaluation results demonstrate that LLMs exhibit a universal
decrease in pass@k during SIMD-intrinsic code generation compared to
scalar-code generation. Our in-depth analysis highlights promising directions
for the further advancement of LLMs in the challenging domain of SIMD-intrinsic
code generation. SimdBench is fully open source at
https://anonymous.4open.science/r/SimdBench-1B3F/ to benefit the broader
research community.

</details>


### [499] [Butterfly Effects in Toolchains: A Comprehensive Analysis of Failed Parameter Filling in LLM Tool-Agent Systems](https://arxiv.org/abs/2507.15296)
*Qian Xiong,Yuekai Huang,Ziyou Jiang,Zhiyuan Chang,Yujia Zheng,Tianhao Li,Mingyang Li*

Main category: cs.SE

TL;DR: The paper explores parameter failure in tool-agent LLMs, categorizes five failure types, links them to input sources, and suggests improvements like standardized formats and better error feedback.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of tool-agent LLMs due to parameter failures, improving their reliability and effectiveness.

Method: Constructs a parameter failure taxonomy, derives five failure categories, and analyzes correlations with input sources using 15 perturbation methods.

Result: Parameter name hallucination stems from LLM limitations; other failures relate to input sources.

Conclusion: Proposes improvements like standardized tool formats, enhanced error feedback, and parameter consistency to boost tool-agent reliability.

Abstract: The emergence of the tool agent paradigm has broadened the capability
boundaries of the Large Language Model (LLM), enabling it to complete more
complex tasks. However, the effectiveness of this paradigm is limited due to
the issue of parameter failure during its execution. To explore this phenomenon
and propose corresponding suggestions, we first construct a parameter failure
taxonomy in this paper. We derive five failure categories from the invocation
chain of a mainstream tool agent. Then, we explore the correlation between
three different input sources and failure categories by applying 15 input
perturbation methods to the input. Experimental results show that parameter
name hallucination failure primarily stems from inherent LLM limitations, while
issues with input sources mainly cause other failure patterns. To improve the
reliability and effectiveness of tool-agent interactions, we propose
corresponding improvement suggestions, including standardizing tool return
formats, improving error feedback mechanisms, and ensuring parameter
consistency.

</details>


### [500] [StackTrans: From Large Language Model to Large Pushdown Automata Model](https://arxiv.org/abs/2507.15343)
*Kechi Zhang,Ge Li,Jia Li,Huangzhao Zhang,Yihong Dong,Jia Li,Jingjing Xu,Zhi Jin*

Main category: cs.SE

TL;DR: StackTrans enhances Transformer models by incorporating hidden state stacks to better handle Chomsky hierarchy tasks, outperforming standard Transformers and larger LLMs.


<details>
  <summary>Details</summary>
Motivation: Transformers struggle with capturing Chomsky hierarchy structures like regular expressions or deterministic context-free grammars.

Method: StackTrans integrates differentiable stack operations (push/pop) between Transformer layers, maintaining compatibility with existing frameworks.

Result: StackTrans outperforms standard Transformers and larger LLMs, with StackTrans-360M surpassing models 2-3x its size.

Conclusion: StackTrans effectively addresses Transformer limitations in handling hierarchical structures, offering superior efficiency and reasoning.

Abstract: The Transformer architecture has emerged as a landmark advancement within the
broad field of artificial intelligence, effectively catalyzing the advent of
large language models (LLMs). However, despite its remarkable capabilities and
the substantial progress it has facilitated, the Transformer architecture still
has some limitations. One such intrinsic limitation is its inability to
effectively capture the Chomsky hierarchy, such as regular expressions or
deterministic context-free grammars. Drawing inspiration from pushdown
automata, which efficiently resolve deterministic context-free grammars using
stacks, we propose StackTrans to address the aforementioned issue within LLMs.
Unlike previous approaches that modify the attention computation, StackTrans
explicitly incorporates hidden state stacks between Transformer layers. This
design maintains compatibility with existing frameworks like flash-attention.
Specifically, our design features stack operations -- such as pushing and
popping hidden states -- that are differentiable and can be learned in an
end-to-end manner. Our comprehensive evaluation spans benchmarks for both
Chomsky hierarchies and large-scale natural languages. Across these diverse
tasks, StackTrans consistently outperforms standard Transformer models and
other baselines. We have successfully scaled StackTrans up from 360M to 7B
parameters. In particular, our from-scratch pretrained model StackTrans-360M
outperforms several larger open-source LLMs with 2-3x more parameters,
showcasing its superior efficiency and reasoning capability.

</details>


### [501] [Applying the Chinese Wall Reverse Engineering Technique to Large Language Model Code Editing](https://arxiv.org/abs/2507.15599)
*Manatsawin Hanmongkolchai*

Main category: cs.SE

TL;DR: The paper proposes using the "Chinese Wall" technique to enhance weaker, ethically aligned Code LLMs by leveraging stronger models for instruction generation, improving performance significantly.


<details>
  <summary>Details</summary>
Motivation: Concerns about copyright violations in training datasets for Code LLMs and the limited utility of models with ethically curated data drive the need for a solution.

Method: The "Chinese Wall" technique involves a high-quality model generating detailed instructions for a weaker model, enabling the latter to perform complex tasks.

Result: The technique improved Comma v0.1 1T's performance by over 66% and Starcoder2 Instruct by roughly 20% in the CanItEdit benchmark.

Conclusion: While effective, the technique's practical application is currently limited by the scarcity of models trained on public domain content.

Abstract: Large language models for code (Code LLM) are increasingly utilized in
programming environments. Despite their utility, the training datasets for top
LLM remain undisclosed, raising concerns about potential copyright violations.
Some models, such as Pleias and Comma put emphasis on data curation and
licenses, however, with limited training data these models are not competitive
and only serve as proof of concepts. To improve the utility of these models, we
propose an application of the "Chinese Wall" technique, inspired by the reverse
engineering technique of the same name -- a high quality model is used to
generate detailed instructions for a weaker model. By doing so, a weaker but
ethically aligned model may be used to perform complicated tasks that,
otherwise, can only be completed by more powerful models. In our evaluation,
we've found that this technique improves Comma v0.1 1T's performance in
CanItEdit benchmark by over 66%, and Starcoder2 Instruct by roughly 20%
compared to when running the same model on the benchmark alone. The practical
application of this technique today, however, may be limited due to the lack of
models trained on public domain content without copyright restrictions.

</details>


### [502] [SustainDiffusion: Optimising the Social and Environmental Sustainability of Stable Diffusion Models](https://arxiv.org/abs/2507.15663)
*Giordano d'Aloisio,Tosin Fadahunsi,Jay Choy,Rebecca Moussa,Federica Sarro*

Main category: cs.SE

TL;DR: SustainDiffusion reduces gender/ethnic bias and energy consumption in Stable Diffusion models without altering architecture.


<details>
  <summary>Details</summary>
Motivation: Address social and environmental concerns of widespread Stable Diffusion use.

Method: Search-based optimization of hyperparameters and prompts to reduce bias and energy use while maintaining image quality.

Result: 68% less gender bias, 59% less ethnic bias, 48% lower energy consumption, with consistent performance.

Conclusion: Sustainability improvements are achievable without model fine-tuning or architectural changes.

Abstract: Background: Text-to-image generation models are widely used across numerous
domains. Among these models, Stable Diffusion (SD) - an open-source
text-to-image generation model - has become the most popular, producing over 12
billion images annually. However, the widespread use of these models raises
concerns regarding their social and environmental sustainability.
  Aims: To reduce the harm that SD models may have on society and the
environment, we introduce SustainDiffusion, a search-based approach designed to
enhance the social and environmental sustainability of SD models.
  Method: SustainDiffusion searches the optimal combination of hyperparameters
and prompt structures that can reduce gender and ethnic bias in generated
images while also lowering the energy consumption required for image
generation. Importantly, SustainDiffusion maintains image quality comparable to
that of the original SD model.
  Results: We conduct a comprehensive empirical evaluation of SustainDiffusion,
testing it against six different baselines using 56 different prompts. Our
results demonstrate that SustainDiffusion can reduce gender bias in SD3 by 68%,
ethnic bias by 59%, and energy consumption (calculated as the sum of CPU and
GPU energy) by 48%. Additionally, the outcomes produced by SustainDiffusion are
consistent across multiple runs and can be generalised to various prompts.
  Conclusions: With SustainDiffusion, we demonstrate how enhancing the social
and environmental sustainability of text-to-image generation models is possible
without fine-tuning or changing the model's architecture.

</details>


### [503] [Do AI models help produce verified bug fixes?](https://arxiv.org/abs/2507.15822)
*Li Huang,Ilgiz Mustafin,Marco Piccioni,Alessandro Schena,Reto Weber,Bertrand Meyer*

Main category: cs.SE

TL;DR: The study investigates the practical impact of LLMs on Automatic Program Repair (APR), using a formal proof environment to validate fixes. It compares programmers with and without LLM access, revealing surprising results and offering methodology, behavior analysis, and usage patterns.


<details>
  <summary>Details</summary>
Motivation: To determine if LLMs improve APR in practice and how programmers integrate them into debugging, ensuring correctness through formal proofs.

Method: A controlled study with two groups (with/without LLM access), using formal proof tools for validation, and analyzing behavior via full-session recordings.

Result: Surprising findings on LLM effectiveness, detailed methodology, behavior patterns (7 categories), and validated advice for optimal LLM use in APR.

Conclusion: The study provides foundational insights into LLMs' role in APR, offering reusable methodology and practical guidance for leveraging AI in debugging.

Abstract: Among areas of software engineering where AI techniques -- particularly,
Large Language Models -- seem poised to yield dramatic improvements, an
attractive candidate is Automatic Program Repair (APR), the production of
satisfactory corrections to software bugs. Does this expectation materialize in
practice? How do we find out, making sure that proposed corrections actually
work? If programmers have access to LLMs, how do they actually use them to
complement their own skills?
  To answer these questions, we took advantage of the availability of a
program-proving environment, which formally determines the correctness of
proposed fixes, to conduct a study of program debugging with two randomly
assigned groups of programmers, one with access to LLMs and the other without,
both validating their answers through the proof tools. The methodology relied
on a division into general research questions (Goals in the Goal-Query-Metric
approach), specific elements admitting specific answers (Queries), and
measurements supporting these answers (Metrics). While applied so far to a
limited sample size, the results are a first step towards delineating a proper
role for AI and LLMs in providing guaranteed-correct fixes to program bugs.
  These results caused surprise as compared to what one might expect from the
use of AI for debugging and APR. The contributions also include: a detailed
methodology for experiments in the use of LLMs for debugging, which other
projects can reuse; a fine-grain analysis of programmer behavior, made possible
by the use of full-session recording; a definition of patterns of use of LLMs,
with 7 distinct categories; and validated advice for getting the best of LLMs
for debugging and Automatic Program Repair.

</details>


<div id='astro-ph.HE'></div>

# astro-ph.HE [[Back]](#toc)

### [504] [The hunt for new pulsating ultraluminous X-ray sources: a clustering approach](https://arxiv.org/abs/2507.15032)
*Nicolò Oreste Pinciroli Vago,Roberta Amato,Matteo Imbrogno,GianLuca Israel,Andrea Belfiore,Konstantinos Kovlakas,Piero Fraternali,Mario Pasquato*

Main category: astro-ph.HE

TL;DR: The paper uses AI to identify new candidate pulsating ultraluminous X-ray sources (PULXs) from XMM-Newton data, clustering them based on known PULX properties, but finds no new pulsations yet.


<details>
  <summary>Details</summary>
Motivation: To discover more PULXs despite poor statistics by leveraging AI and archival data.

Method: Unsupervised clustering of ULXs, using known PULXs to set thresholds and identify candidate clusters.

Result: Identified 85 candidate PULXs, but no new pulsations detected in preliminary analysis.

Conclusion: AI shows predictive power but requires high-statistics data to confirm PULXs.

Abstract: The discovery of fast and variable coherent signals in a handful of
ultraluminous X-ray sources (ULXs) testifies to the presence of super-Eddington
accreting neutron stars, and drastically changed the understanding of the ULX
class. Our capability of discovering pulsations in ULXs is limited, among
others, by poor statistics. However, catalogues and archives of high-energy
missions contain information which can be used to identify new candidate
pulsating ULXs (PULXs). The goal of this research is to single out candidate
PULXs among those ULXs which have not shown pulsations due to an unfavourable
combination of factors. We applied an AI approach to an updated database of
ULXs detected by XMM-Newton. We first used an unsupervised clustering algorithm
to sort out sources with similar characteristics into two clusters. Then, the
sample of known PULX observations has been used to set the separation threshold
between the two clusters and to identify the one containing the new candidate
PULXs. We found that only a few criteria are needed to assign the membership of
an observation to one of the two clusters. The cluster of new candidate PULXs
counts 85 unique sources for 355 observations, with $\sim$85% of these new
candidates having multiple observations. A preliminary timing analysis found no
new pulsations for these candidates. This work presents a sample of new
candidate PULXs observed by XMM-Newton, the properties of which are similar (in
a multi-dimensional phase space) to those of the known PULXs, despite the
absence of pulsations in their light curves. While this result is a clear
example of the predictive power of AI-based methods, it also highlights the
need for high-statistics observational data to reveal coherent signals from the
sources in this sample and thus validate the robustness of the approach.

</details>


<div id='math.RT'></div>

# math.RT [[Back]](#toc)

### [505] [Partial Symmetry Enforced Attention Decomposition (PSEAD): A Group-Theoretic Framework for Equivariant Transformers in Biological Systems](https://arxiv.org/abs/2507.14908)
*Daniel Ayomide Olanrewaju*

Main category: math.RT

TL;DR: PSEAD integrates local symmetry into Transformer self-attention, enhancing generalization, interpretability, and efficiency for biological data.


<details>
  <summary>Details</summary>
Motivation: To improve Transformer models by incorporating local symmetry awareness for biological data analysis.

Method: Formalizes local permutation subgroup actions, decomposing attention into orthogonal components aligned with subgroup representations.

Result: PSEAD improves generalization, interpretability, and computational efficiency in biological tasks.

Conclusion: PSEAD advances symmetry-aware AI models for biological applications like protein folding and drug discovery.

Abstract: This research introduces the Theory of Partial Symmetry Enforced Attention
Decomposition (PSEAD), a new and rigorous group-theoretic framework designed to
seamlessly integrate local symmetry awareness into the core architecture of
self-attention mechanisms within Transformer models. We formalize the concept
of local permutation subgroup actions on windows of biological data, proving
that under such actions, the attention mechanism naturally decomposes into a
direct sum of orthogonal irreducible components. Critically, these components
are intrinsically aligned with the irreducible representations of the acting
permutation subgroup, thereby providing a powerful mathematical basis for
disentangling symmetric and asymmetric features. We show that PSEAD offers
substantial advantages. These include enhanced generalization capabilities to
novel biological motifs exhibiting similar partial symmetries, unprecedented
interpretability by allowing direct visualization and analysis of attention
contributions from different symmetry channels, and significant computational
efficiency gains by focusing representational capacity on relevant symmetric
subspaces. Beyond static data analysis, we extend PSEAD's applicability to
dynamic biological processes within reinforcement learning paradigms,
showcasing its potential to accelerate the discovery and optimization of
biologically meaningful policies in complex environments like protein folding
and drug discovery. This work lays the groundwork for a new generation of
biologically informed, symmetry-aware artificial intelligence models.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [506] [Hear Your Code Fail, Voice-Assisted Debugging for Python](https://arxiv.org/abs/2507.15007)
*Sayed Mahbub Hasan Amiri,Md. Mainul Islam,Mohammad Shakhawat Hossen,Sayed Majhab Hasan Amiri,Mohammad Shawkat Ali Mamun,Sk. Humaun Kabir,Naznin Akter*

Main category: cs.PL

TL;DR: A voice-assisted Python debugging plugin reduces cognitive load and speeds up error identification by providing audible and visual feedback.


<details>
  <summary>Details</summary>
Motivation: To improve debugging efficiency and accessibility, especially for visually impaired users and multitasking workflows.

Method: Uses a global exception hook with pyttsx3 for speech and Tkinter for GUI, offering multimodal error feedback.

Result: 37% reduced cognitive load, 78% faster error identification, sub-1.2s voice latency, and under 18% CPU overhead.

Conclusion: The plugin enhances debugging accessibility and efficiency, with potential for educational use and future GPT-based improvements.

Abstract: This research introduces an innovative voice-assisted debugging plugin for
Python that transforms silent runtime errors into actionable audible
diagnostics. By implementing a global exception hook architecture with pyttsx3
text-to-speech conversion and Tkinter-based GUI visualization, the solution
delivers multimodal error feedback through parallel auditory and visual
channels. Empirical evaluation demonstrates 37% reduced cognitive load (p<0.01,
n=50) compared to traditional stack-trace debugging, while enabling 78% faster
error identification through vocalized exception classification and
contextualization. The system achieves sub-1.2 second voice latency with under
18% CPU overhead during exception handling, vocalizing error types and
consequences while displaying interactive tracebacks with documentation deep
links. Criteria validate compatibility across Python 3.7+ environments on
Windows, macOS, and Linux platforms. Needing only two lines of integration
code, the plugin significantly boosts availability for aesthetically impaired
designers and supports multitasking workflows through hands-free error medical
diagnosis. Educational applications show particular promise, with pilot studies
indicating 45% faster debugging skill acquisition among novice programmers.
Future development will incorporate GPT-based repair suggestions and real-time
multilingual translation to further advance auditory debugging paradigms. The
solution represents a fundamental shift toward human-centric error diagnostics,
bridging critical gaps in programming accessibility while establishing new
standards for cognitive efficiency in software development workflows.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [507] [On exploration of an interior mirror descent flow for stochastic nonconvex constrained problem](https://arxiv.org/abs/2507.15264)
*Kuangyu Ding,Kim-Chuan Toh*

Main category: math.OC

TL;DR: The paper studies nonsmooth nonconvex optimization over nonconvex constraints, proposing a Riemannian subgradient flow to unify Hessian barrier and mirror descent methods. It analyzes the flow's behavior, interprets spurious stationary points as stable equilibria, and introduces conditions to avoid them. Two iterative methods are proposed based on these insights.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of existing methods like Hessian barrier and mirror descent in nonsmooth nonconvex optimization, particularly spurious stationary points, by unifying them under a continuous dynamical system framework.

Method: A Riemannian subgradient flow is formulated as a differential inclusion, unifying Hessian barrier and mirror descent methods. The flow's behavior is analyzed, and conditions to avoid spurious points are provided. Random perturbation is proposed for irregular cases.

Result: The continuous flow unifies existing methods, interprets spurious points as stable equilibria, and provides conditions to avoid them. Two new iterative Riemannian subgradient methods are introduced.

Conclusion: The paper offers a unified framework for nonsmooth nonconvex optimization, improving understanding of spurious points and proposing new methods to address them.

Abstract: We study a nonsmooth nonconvex optimization problem defined over nonconvex
constraints, where the feasible set is given by the intersection of the closure
of an open set and a smooth manifold. By endowing the open set with a
Riemannian metric induced by a barrier function, we obtain a Riemannian
subgradient flow formulated as a differential inclusion, which remains strictly
within the interior of the feasible set. This continuous dynamical system
unifies two classes of iterative optimization methods, namely the Hessian
barrier method and mirror descent scheme, by revealing that these methods can
be interpreted as discrete approximations of the continuous flow. We explore
the long-term behavior of the trajectories generated by this dynamical system
and show that the existing deficient convergence properties of the Hessian
barrier and mirror descent scheme can be unifily and more insightfully
interpreted through these of the continuous trajectory. For instance, the
notorious spurious stationary points \cite{chen2024spurious} observed in
Hessian barrier method and mirror descent scheme are interpreted as stable
equilibria of the dynamical system that do not correspond to real stationary
points of the original optimization problem. We provide two sufficient
condition such that these spurious stationary points can be avoided if the
strict complementarity conditions holds. In the absence of these regularity
condition, we propose a random perturbation strategy that ensures the
trajectory converges (subsequentially) to an approximate stationary point.
Building on these insights, we introduce two iterative Riemannian subgradient
methods, form of interior point methods, that generalizes the existing Hessian
barrier method and mirror descent scheme for solving nonsmooth nonconvex
optimization problems.

</details>


### [508] [Information Preserving Line Search via Bayesian Optimization](https://arxiv.org/abs/2507.15485)
*Robin Labryga,Tomislav Prusina,Sören Laue*

Main category: math.OC

TL;DR: A Bayesian optimization-based line search method is proposed, outperforming traditional methods by preserving and utilizing discarded function and gradient data.


<details>
  <summary>Details</summary>
Motivation: Traditional line search methods discard valuable function and gradient information during iterative refinement, limiting efficiency.

Method: The proposed method uses Bayesian optimization to retain and leverage otherwise discarded data for better step-length choices.

Result: The approach guarantees convergence and demonstrates superior performance on the CUTEst test set for unconstrained and bound-constrained optimization.

Conclusion: The Bayesian optimization-based line search method is more efficient and effective than traditional methods.

Abstract: Line search is a fundamental part of iterative optimization methods for
unconstrained and bound-constrained optimization problems to determine suitable
step lengths that provide sufficient improvement in each iteration. Traditional
line search methods are based on iterative interval refinement, where valuable
information about function value and gradient is discarded in each iteration.
We propose a line search method via Bayesian optimization, preserving and
utilizing otherwise discarded information to improve step-length choices. Our
approach is guaranteed to converge and shows superior performance compared to
state-of-the-art methods based on empirical tests on the challenging
unconstrained and bound-constrained optimization problems from the CUTEst test
set.

</details>


### [509] [Multi-beam Beamforming in RIS-aided MIMO Subject to Reradiation Mask Constraints -- Optimization and Machine Learning Design](https://arxiv.org/abs/2507.15367)
*Shumin Wang,Hajar El Hassani,Marco Di Renzo,Marios Poulakis*

Main category: math.OC

TL;DR: The paper proposes a joint design of transmit precoding matrices and RIS phase shifts in a multi-user MIMO system, using optimization and neural networks to improve efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: To enhance spectral efficiency and reduce power consumption in future wireless systems using RIS technology.

Method: Formulates a max-min optimization problem, simplifies achievable rates with the Arimoto-Blahut algorithm, and uses alternating optimization and neural networks for efficiency. Addresses practical RIS limitations with a greedy search for discrete phase shifts.

Result: Simulations show effective multi-beam shaping, reduced execution time via neural networks, and good performance with discrete phase shifts (four levels).

Conclusion: The proposed methods achieve efficient beamforming and meet practical constraints, demonstrating the potential of RIS-aided MIMO systems.

Abstract: Reconfigurable intelligent surfaces (RISs) are an emerging technology for
improving spectral efficiency and reducing power consumption in future wireless
systems. This paper investigates the joint design of the transmit precoding
matrices and the RIS phase shift vector in a multi-user RIS-aided
multiple-input multiple-output (MIMO) communication system. We formulate a
max-min optimization problem to maximize the minimum achievable rate while
considering transmit power and reradiation mask constraints. The achievable
rate is simplified using the Arimoto-Blahut algorithm, and the problem is
broken into quadratic programs with quadratic constraints (QPQC) sub-problems
using an alternating optimization approach. To improve efficiency, we develop a
model-based neural network optimization that utilizes the one-hot encoding for
the angles of incidence and reflection. We address practical RIS limitations by
using a greedy search algorithm to solve the optimization problem for discrete
phase shifts. Simulation results demonstrate that the proposed methods
effectively shape the multi-beam radiation pattern towards desired directions
while satisfying reradiation mask constraints. The neural network design
reduces the execution time, and the discrete phase shift scheme performs well
with a small reduction of the beamforming gain by using only four phase shift
levels.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [510] [Statistical and Algorithmic Foundations of Reinforcement Learning](https://arxiv.org/abs/2507.14444)
*Yuejie Chi,Yuxin Chen,Yuting Wei*

Main category: stat.ML

TL;DR: The paper discusses challenges in reinforcement learning (RL) due to model complexity and nonconvexity, focusing on improving sample and computational efficiency in data-scarce scenarios. It introduces key algorithmic and theoretical developments, covering various RL scenarios and approaches.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the inefficiencies of RL in sample-starved situations, such as clinical trials or autonomous systems, by enhancing algorithmic and theoretical understanding.

Method: The tutorial employs Markov Decision Processes and covers RL scenarios like online, offline, robust RL, and RL with human feedback. It presents approaches like model-based, value-based, and policy optimization methods.

Result: The paper highlights advancements in RL, focusing on sample complexity, computational efficiency, and lower bounds from a non-asymptotic perspective.

Conclusion: The tutorial aims to bridge new RL ideas with classical topics, providing insights into improving efficiency and understanding fundamental limits in RL.

Abstract: As a paradigm for sequential decision making in unknown environments,
reinforcement learning (RL) has received a flurry of attention in recent years.
However, the explosion of model complexity in emerging applications and the
presence of nonconvexity exacerbate the challenge of achieving efficient RL in
sample-starved situations, where data collection is expensive, time-consuming,
or even high-stakes (e.g., in clinical trials, autonomous systems, and online
advertising). How to understand and enhance the sample and computational
efficacies of RL algorithms is thus of great interest. In this tutorial, we aim
to introduce several important algorithmic and theoretical developments in RL,
highlighting the connections between new ideas and classical topics. Employing
Markov Decision Processes as the central mathematical model, we cover several
distinctive RL scenarios (i.e., RL with a simulator, online RL, offline RL,
robust RL, and RL with human feedback), and present several mainstream RL
approaches (i.e., model-based approach, value-based approach, and policy
optimization). Our discussions gravitate around the issues of sample
complexity, computational efficiency, as well as algorithm-dependent and
information-theoretic lower bounds from a non-asymptotic viewpoint.

</details>


### [511] [Diffusion Models for Time Series Forecasting: A Survey](https://arxiv.org/abs/2507.14507)
*Chen Su,Zhengzhou Cai,Yuanhe Tian,Zihong Zheng,Yan Song*

Main category: stat.ML

TL;DR: This survey explores the adaptation of diffusion models, originally for image synthesis, to time series forecasting (TSF), reviewing their variants, applications, and future directions.


<details>
  <summary>Details</summary>
Motivation: To systematically review and categorize the expanding use of diffusion models in TSF, highlighting their potential and current limitations.

Method: The survey introduces standard diffusion models and their variants, reviews their adaptation to TSF, categorizes existing approaches, and analyzes foundational models, datasets, and metrics.

Result: Diffusion models show promise in TSF, with various adaptations and conditioning mechanisms. The survey provides a systematic overview of current methods.

Conclusion: The survey serves as a reference for researchers, detailing progress and suggesting future directions for diffusion models in TSF.

Abstract: Diffusion models, initially developed for image synthesis, demonstrate
remarkable generative capabilities. Recently, their application has expanded to
time series forecasting (TSF), yielding promising results. In this survey, we
firstly introduce the standard diffusion models and their prevalent variants,
explaining their adaptation to TSF tasks. We then provide a comprehensive
review of diffusion models for TSF, paying special attention to the sources of
conditional information and the mechanisms for integrating this conditioning
within the models. In analyzing existing approaches using diffusion models for
TSF, we provide a systematic categorization and a comprehensive summary of them
in this survey. Furthermore, we examine several foundational diffusion models
applied to TSF, alongside commonly used datasets and evaluation metrics.
Finally, we discuss current limitations in these approaches and potential
future research directions. Overall, this survey details recent progress and
future prospects for diffusion models in TSF, serving as a reference for
researchers in the field.

</details>


### [512] [Deep Learning-Based Survival Analysis with Copula-Based Activation Functions for Multivariate Response Prediction](https://arxiv.org/abs/2507.14641)
*Jong-Min Kim,Il Do Ha,Sangjin Kim*

Main category: stat.ML

TL;DR: The paper combines deep learning, copula functions, and survival analysis to improve prediction accuracy for correlated and right-censored multivariate survival data using copula-based activation functions.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of modeling highly correlated and right-censored multivariate survival data by capturing nonlinear dependencies.

Method: Proposes a CNN-LSTM model with copula-based activation functions (Clayton, Gumbel, and combinations) for multivariate multi-types of survival responses.

Result: The model enhances prediction accuracy, validated through simulation studies and real breast cancer data analysis, using Shewhart control charts and ARL.

Conclusion: The integration of copula-based activation functions in deep learning effectively handles complex survival data patterns and improves performance.

Abstract: This research integrates deep learning, copula functions, and survival
analysis to effectively handle highly correlated and right-censored
multivariate survival data. It introduces copula-based activation functions
(Clayton, Gumbel, and their combinations) to model the nonlinear dependencies
inherent in such data. Through simulation studies and analysis of real breast
cancer data, our proposed CNN-LSTM with copula-based activation functions for
multivariate multi-types of survival responses enhances prediction accuracy by
explicitly addressing right-censored data and capturing complex patterns. The
model's performance is evaluated using Shewhart control charts, focusing on the
average run length (ARL).

</details>


### [513] [Accelerating Hamiltonian Monte Carlo for Bayesian Inference in Neural Networks and Neural Operators](https://arxiv.org/abs/2507.14652)
*Ponkrshnan Thiagarajan,Tamer A. Zaki,Michael D. Shields*

Main category: stat.ML

TL;DR: A hybrid approach combining VI and HMC is proposed to efficiently and accurately quantify uncertainties in neural networks by reducing parameter space dimensionality.


<details>
  <summary>Details</summary>
Motivation: HMC is computationally expensive for Bayesian neural networks, and existing approximations like VI introduce inaccuracies in uncertainty estimates.

Method: Leverage VI to identify parameters influencing uncertainty, reduce dimensionality, and apply HMC only to critical parameters.

Result: The framework accelerates HMC for large networks, demonstrating efficiency and accuracy in uncertainty quantification.

Conclusion: The hybrid method effectively learns surrogates for complex systems, enabling scalable uncertainty estimation in neural networks.

Abstract: Hamiltonian Monte Carlo (HMC) is a powerful and accurate method to sample
from the posterior distribution in Bayesian inference. However, HMC techniques
are computationally demanding for Bayesian neural networks due to the high
dimensionality of the network's parameter space and the non-convexity of their
posterior distributions. Therefore, various approximation techniques, such as
variational inference (VI) or stochastic gradient MCMC, are often employed to
infer the posterior distribution of the network parameters. Such approximations
introduce inaccuracies in the inferred distributions, resulting in unreliable
uncertainty estimates. In this work, we propose a hybrid approach that combines
inexpensive VI and accurate HMC methods to efficiently and accurately quantify
uncertainties in neural networks and neural operators. The proposed approach
leverages an initial VI training on the full network. We examine the influence
of individual parameters on the prediction uncertainty, which shows that a
large proportion of the parameters do not contribute substantially to
uncertainty in the network predictions. This information is then used to
significantly reduce the dimension of the parameter space, and HMC is performed
only for the subset of network parameters that strongly influence prediction
uncertainties. This yields a framework for accelerating the full batch HMC for
posterior inference in neural networks. We demonstrate the efficiency and
accuracy of the proposed framework on deep neural networks and operator
networks, showing that inference can be performed for large networks with tens
to hundreds of thousands of parameters. We show that this method can
effectively learn surrogates for complex physical systems by modeling the
operator that maps from upstream conditions to wall-pressure data on a cone in
hypersonic flow.

</details>


### [514] [When few labeled target data suffice: a theory of semi-supervised domain adaptation via fine-tuning from multiple adaptive starts](https://arxiv.org/abs/2507.14661)
*Wooseok Ha,Yuansi Chen*

Main category: stat.ML

TL;DR: The paper develops a theoretical framework for semi-supervised domain adaptation (SSDA) using structural causal models (SCMs), introduces three tailored SSDA methods, and proposes the MASFT algorithm for optimal performance with limited labeled target data.


<details>
  <summary>Details</summary>
Motivation: To address the lack of theoretical understanding of SSDA's effectiveness, especially under various source-target distributional shifts, and to improve performance with limited labeled target data.

Method: Theoretical framework based on SCMs, three SSDA methods tailored to different source-target assumptions, and the MASFT algorithm for practical scenarios.

Result: Demonstrates minimax-optimal target performance under specific assumptions and shows MASFT achieves near-optimal performance across diverse shifts with reduced labeled data.

Conclusion: The proposed methods, especially MASFT, effectively address SSDA challenges, validated by simulations.

Abstract: Semi-supervised domain adaptation (SSDA) aims to achieve high predictive
performance in the target domain with limited labeled target data by exploiting
abundant source and unlabeled target data. Despite its significance in numerous
applications, theory on the effectiveness of SSDA remains largely unexplored,
particularly in scenarios involving various types of source-target
distributional shifts. In this work, we develop a theoretical framework based
on structural causal models (SCMs) which allows us to analyze and quantify the
performance of SSDA methods when labeled target data is limited. Within this
framework, we introduce three SSDA methods, each having a fine-tuning strategy
tailored to a distinct assumption about the source and target relationship.
Under each assumption, we demonstrate how extending an unsupervised domain
adaptation (UDA) method to SSDA can achieve minimax-optimal target performance
with limited target labels. When the relationship between source and target
data is only vaguely known -- a common practical concern -- we propose the
Multi Adaptive-Start Fine-Tuning (MASFT) algorithm, which fine-tunes UDA models
from multiple starting points and selects the best-performing one based on a
small hold-out target validation dataset. Combined with model selection
guarantees, MASFT achieves near-optimal target predictive performance across a
broad range of types of distributional shifts while significantly reducing the
need for labeled target data. We empirically validate the effectiveness of our
proposed methods through simulations.

</details>


### [515] [Learning Nonlinear Causal Reductions to Explain Reinforcement Learning Policies](https://arxiv.org/abs/2507.14901)
*Armin Kekić,Jan Schneider,Dieter Büchler,Bernhard Schölkopf,Michel Besserve*

Main category: stat.ML

TL;DR: The paper introduces a causal perspective to explain RL policy behavior by modeling states, actions, and rewards as variables in a low-level causal model. It proposes a nonlinear Causal Model Reduction framework for simplified high-level explanations.


<details>
  <summary>Details</summary>
Motivation: Understanding why RL policies succeed or fail is difficult due to complex agent-environment interactions. The work aims to provide causal explanations for policy behavior.

Method: The method involves perturbing policy actions during execution, observing effects on rewards, and learning a high-level causal model. A nonlinear Causal Model Reduction framework ensures approximate interventional consistency.

Result: The framework proves exact interventional consistency for a class of nonlinear models. Experiments on synthetic and practical RL tasks (e.g., pendulum control, robot table tennis) reveal behavioral patterns and failure modes.

Conclusion: The approach effectively uncovers causal patterns and biases in RL policies, providing meaningful explanations for their behavior.

Abstract: Why do reinforcement learning (RL) policies fail or succeed? This is a
challenging question due to the complex, high-dimensional nature of
agent-environment interactions. In this work, we take a causal perspective on
explaining the behavior of RL policies by viewing the states, actions, and
rewards as variables in a low-level causal model. We introduce random
perturbations to policy actions during execution and observe their effects on
the cumulative reward, learning a simplified high-level causal model that
explains these relationships. To this end, we develop a nonlinear Causal Model
Reduction framework that ensures approximate interventional consistency,
meaning the simplified high-level model responds to interventions in a similar
way as the original complex system. We prove that for a class of nonlinear
causal models, there exists a unique solution that achieves exact
interventional consistency, ensuring learned explanations reflect meaningful
causal patterns. Experiments on both synthetic causal models and practical RL
tasks-including pendulum control and robot table tennis-demonstrate that our
approach can uncover important behavioral patterns, biases, and failure modes
in trained RL policies.

</details>


### [516] [Uncertainty Quantification for Machine Learning-Based Prediction: A Polynomial Chaos Expansion Approach for Joint Model and Input Uncertainty Propagation](https://arxiv.org/abs/2507.14782)
*Xiaoping Du*

Main category: stat.ML

TL;DR: A robust framework using Polynomial Chaos Expansion (PCE) is proposed for joint input and model uncertainty propagation in ML surrogate models, focusing on Gaussian Process regression for reliable engineering predictions.


<details>
  <summary>Details</summary>
Motivation: To address the inherent errors in ML surrogate models and the need for accurate uncertainty quantification in engineering applications.

Method: Uses PCE to transform random inputs into a unified standard space, constructing a surrogate model for efficient uncertainty propagation and sensitivity analysis.

Result: Enables accurate calculation of output mean and standard deviation, and quantifies contributions of input variables and model uncertainty to output variability.

Conclusion: The framework is computationally efficient and interpretable, supporting trustworthy ML predictions in engineering.

Abstract: Machine learning (ML) surrogate models are increasingly used in engineering
analysis and design to replace computationally expensive simulation models,
significantly reducing computational cost and accelerating decision-making
processes. However, ML predictions contain inherent errors, often estimated as
model uncertainty, which is coupled with variability in model inputs.
Accurately quantifying and propagating these combined uncertainties is
essential for generating reliable engineering predictions. This paper presents
a robust framework based on Polynomial Chaos Expansion (PCE) to handle joint
input and model uncertainty propagation. While the approach applies broadly to
general ML surrogates, we focus on Gaussian Process regression models, which
provide explicit predictive distributions for model uncertainty. By
transforming all random inputs into a unified standard space, a PCE surrogate
model is constructed, allowing efficient and accurate calculation of the mean
and standard deviation of the output. The proposed methodology also offers a
mechanism for global sensitivity analysis, enabling the accurate quantification
of the individual contributions of input variables and ML model uncertainty to
the overall output variability. This approach provides a computationally
efficient and interpretable framework for comprehensive uncertainty
quantification, supporting trustworthy ML predictions in downstream engineering
applications.

</details>


### [517] [Learning under Latent Group Sparsity via Diffusion on Networks](https://arxiv.org/abs/2507.15097)
*Subhroshekhar Ghosh,Soumendu Sundar Mukherjee*

Main category: stat.ML

TL;DR: The paper introduces a sparse learning approach for group-structured explanatory variables without needing prior group information, using a heat-flow-based penalty that interpolates between lasso and group lasso.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of sparse learning under group structure without prior knowledge of group identities, leveraging network dynamics and Laplacian geometry.

Method: Proposes a heat-flow-based penalty derived from network dynamics, interpolating between lasso and group lasso, with a data-driven network construction.

Result: The method performs effectively with rigorous theoretical guarantees, requiring only logarithmic runtime in problem dimensions.

Conclusion: The approach opens possibilities for diffusion-based techniques in learning tasks, bridging geometric, dynamical, and stochastic data structures.

Abstract: Group or cluster structure on explanatory variables in machine learning
problems is a very general phenomenon, which has attracted broad interest from
practitioners and theoreticians alike. In this work we contribute an approach
to sparse learning under such group structure, that does not require prior
information on the group identities. Our paradigm is motivated by the Laplacian
geometry of an underlying network with a related community structure, and
proceeds by directly incorporating this into a penalty that is effectively
computed via a heat-flow-based local network dynamics. The proposed penalty
interpolates between the lasso and the group lasso penalties, the runtime of
the heat-flow dynamics being the interpolating parameter. As such it can
automatically default to lasso when the group structure reflected in the
Laplacian is weak. In fact, we demonstrate a data-driven procedure to construct
such a network based on the available data. Notably, we dispense with
computationally intensive pre-processing involving clustering of variables,
spectral or otherwise. Our technique is underpinned by rigorous theorems that
guarantee its effective performance and provide bounds on its sample
complexity. In particular, in a wide range of settings, it provably suffices to
run the diffusion for time that is only logarithmic in the problem dimensions.
We explore in detail the interfaces of our approach with key statistical
physics models in network science, such as the Gaussian Free Field and the
Stochastic Block Model. Our work raises the possibility of applying similar
diffusion-based techniques to classical learning tasks, exploiting the
interplay between geometric, dynamical and stochastic structures underlying the
data.

</details>


### [518] [Accelerated Bayesian Optimal Experimental Design via Conditional Density Estimation and Informative Data](https://arxiv.org/abs/2507.15235)
*Miao Huang,Hongqiao Wang,Kunyu Wu*

Main category: stat.ML

TL;DR: The paper introduces a Bayesian framework for optimal experimental design, improving numerical efficiency and addressing challenges like surrogate modeling and parameter inference.


<details>
  <summary>Details</summary>
Motivation: To enhance the validity, reliability, and efficiency of experimental outcomes, especially in scenarios with low simulation efficiency and high data acquisition costs.

Method: Utilizes Bayes' theorem to reformulate utility expectation into an independent double integral form, employs conditional density estimation, and uses covariance for dataset selection.

Result: The methodology improves numerical efficiency and is validated through theoretical analysis and practical applications.

Conclusion: The proposed Bayesian framework effectively enhances experimental efficiency and decision-making under uncertainty.

Abstract: The Design of Experiments (DOEs) is a fundamental scientific methodology that
provides researchers with systematic principles and techniques to enhance the
validity, reliability, and efficiency of experimental outcomes. In this study,
we explore optimal experimental design within a Bayesian framework, utilizing
Bayes' theorem to reformulate the utility expectation--originally expressed as
a nested double integral--into an independent double integral form,
significantly improving numerical efficiency. To further accelerate the
computation of the proposed utility expectation, conditional density estimation
is employed to approximate the ratio of two Gaussian random fields, while
covariance serves as a selection criterion to identify informative datasets
during model fitting and integral evaluation. In scenarios characterized by low
simulation efficiency and high costs of raw data acquisition, key challenges
such as surrogate modeling, failure probability estimation, and parameter
inference are systematically restructured within the Bayesian experimental
design framework. The effectiveness of the proposed methodology is validated
through both theoretical analysis and practical applications, demonstrating its
potential for enhancing experimental efficiency and decision-making under
uncertainty.

</details>


### [519] [Missing value imputation with adversarial random forests -- MissARF](https://arxiv.org/abs/2507.15681)
*Pegah Golchian,Jan Kapar,David S. Watson,Marvin N. Wright*

Main category: stat.ML

TL;DR: MissARF is a fast, easy-to-use imputation method using adversarial random forests for handling missing values in biostatistics.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of missing values in biostatistical analyses with a novel, efficient method.

Method: Uses adversarial random forests (ARF) for density estimation and data synthesis, sampling from the conditional distribution for imputation.

Result: Performs comparably to state-of-the-art methods in quality and speed, with no extra cost for multiple imputation.

Conclusion: MissARF is a promising tool for efficient and high-quality missing value imputation.

Abstract: Handling missing values is a common challenge in biostatistical analyses,
typically addressed by imputation methods. We propose a novel, fast, and
easy-to-use imputation method called missing value imputation with adversarial
random forests (MissARF), based on generative machine learning, that provides
both single and multiple imputation. MissARF employs adversarial random forest
(ARF) for density estimation and data synthesis. To impute a missing value of
an observation, we condition on the non-missing values and sample from the
estimated conditional distribution generated by ARF. Our experiments
demonstrate that MissARF performs comparably to state-of-the-art single and
multiple imputation methods in terms of imputation quality and fast runtime
with no additional costs for multiple imputation.

</details>


### [520] [Conformal and kNN Predictive Uncertainty Quantification Algorithms in Metric Spaces](https://arxiv.org/abs/2507.15741)
*Gábor Lugosi,Marcos Matabuena*

Main category: stat.ML

TL;DR: A framework for uncertainty quantification in regression models in metric spaces, offering conformal prediction with finite-sample guarantees and efficient local methods for heteroscedastic settings.


<details>
  <summary>Details</summary>
Motivation: To address uncertainty quantification in regression models within metric spaces, ensuring robustness and adaptability to nonlinear geometries.

Method: Introduces a conformal prediction algorithm for homoscedastic settings and a local $k$-nearest-neighbor method for heteroscedastic cases, both compatible with any regression algorithm.

Result: Proves consistency under minimal conditions and demonstrates practical utility in personalized medicine with complex response objects.

Conclusion: The framework is versatile, scalable, and adaptable, making it suitable for diverse applications, including personalized medicine.

Abstract: This paper introduces a framework for uncertainty quantification in
regression models defined in metric spaces. Leveraging a newly defined notion
of homoscedasticity, we develop a conformal prediction algorithm that offers
finite-sample coverage guarantees and fast convergence rates of the oracle
estimator. In heteroscedastic settings, we forgo these non-asymptotic
guarantees to gain statistical efficiency, proposing a local
$k$--nearest--neighbor method without conformal calibration that is adaptive to
the geometry of each particular nonlinear space. Both procedures work with any
regression algorithm and are scalable to large data sets, allowing
practitioners to plug in their preferred models and incorporate domain
expertise. We prove consistency for the proposed estimators under minimal
conditions. Finally, we demonstrate the practical utility of our approach in
personalized--medicine applications involving random response objects such as
probability distributions and graph Laplacians.

</details>


### [521] [Hypergraphs on high dimensional time series sets using signature transform](https://arxiv.org/abs/2507.15802)
*Rémi Vaucher,Paul Minchella*

Main category: stat.ML

TL;DR: The paper extends hypergraph construction to collections of multivariate time series using signature transforms and controlled randomness, improving robustness.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of constructing hypergraphs from collections of multivariate time series, extending prior work focused on single time series.

Method: Generalizes Chretien et al.'s method by leveraging signature transforms and introducing controlled randomness.

Result: Validated on synthetic datasets with promising results.

Conclusion: The approach enhances robustness in hypergraph construction for collections of multivariate time series.

Abstract: In recent decades, hypergraphs and their analysis through Topological Data
Analysis (TDA) have emerged as powerful tools for understanding complex data
structures. Various methods have been developed to construct hypergraphs --
referred to as simplicial complexes in the TDA framework -- over datasets,
enabling the formation of edges between more than two vertices. This paper
addresses the challenge of constructing hypergraphs from collections of
multivariate time series. While prior work has focused on the case of a single
multivariate time series, we extend this framework to handle collections of
such time series. Our approach generalizes the method proposed in Chretien and
al. by leveraging the properties of signature transforms to introduce
controlled randomness, thereby enhancing the robustness of the construction
process. We validate our method on synthetic datasets and present promising
results.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [522] [Real-Time Communication-Aware Ride-Sharing Route Planning for Urban Air Mobility: A Multi-Source Hybrid Attention Reinforcement Learning Approach](https://arxiv.org/abs/2507.14249)
*Yuejiao Xie,Maonan Wang,Di Zhou,Man-On Pun,Zhu Han*

Main category: cs.RO

TL;DR: The paper proposes a Multi-Source Hybrid Attention Reinforcement Learning (MSHA-RL) framework for Urban Air Mobility (UAM) path planning, addressing communication quality and dynamic passenger demands.


<details>
  <summary>Details</summary>
Motivation: To tackle the inflexibility of conventional UAM trajectory planning in handling dynamic passenger requests and ensuring communication quality for safety.

Method: Constructs a radio map for communication quality assessment and introduces MSHA-RL to align diverse data sources and balance global-local insights for real-time planning.

Result: The approach reduces travel time, enhances efficiency, and ensures safety by enabling communication-compliant trajectory planning.

Conclusion: MSHA-RL effectively addresses UAM path planning challenges, improving responsiveness and safety in dynamic urban environments.

Abstract: Urban Air Mobility (UAM) systems are rapidly emerging as promising solutions
to alleviate urban congestion, with path planning becoming a key focus area.
Unlike ground transportation, UAM trajectory planning has to prioritize
communication quality for accurate location tracking in constantly changing
environments to ensure safety. Meanwhile, a UAM system, serving as an air taxi,
requires adaptive planning to respond to real-time passenger requests,
especially in ride-sharing scenarios where passenger demands are unpredictable
and dynamic. However, conventional trajectory planning strategies based on
predefined routes lack the flexibility to meet varied passenger ride demands.
To address these challenges, this work first proposes constructing a radio map
to evaluate the communication quality of urban airspace. Building on this, we
introduce a novel Multi-Source Hybrid Attention Reinforcement Learning
(MSHA-RL) framework for the challenge of effectively focusing on passengers and
UAM locations, which arises from the significant dimensional disparity between
the representations. This model first generates the alignment among diverse
data sources with large gap dimensions before employing hybrid attention to
balance global and local insights, thereby facilitating responsive, real-time
path planning. Extensive experimental results demonstrate that the approach
enables communication-compliant trajectory planning, reducing travel time and
enhancing operational efficiency while prioritizing passenger safety.

</details>


### [523] [Uncertainty-aware Probabilistic 3D Human Motion Forecasting via Invertible Networks](https://arxiv.org/abs/2507.14694)
*Yue Ma,Kanglei Zhou,Fuyang Yu,Frederick W. B. Li,Xiaohui Liang*

Main category: cs.RO

TL;DR: ProbHMI introduces invertible networks for 3D human motion forecasting, enabling explicit uncertainty quantification for safety-critical applications.


<details>
  <summary>Details</summary>
Motivation: Existing methods lack effective uncertainty quantification, which is crucial for safety in human-robot collaboration.

Method: ProbHMI uses invertible networks to parameterize poses in a disentangled latent space and a forecasting module to predict future latent distributions.

Result: ProbHMI performs well on benchmarks for deterministic and diverse predictions while validating uncertainty calibration.

Conclusion: ProbHMI addresses uncertainty quantification in motion forecasting, enhancing safety for risk-aware decision making.

Abstract: 3D human motion forecasting aims to enable autonomous applications.
Estimating uncertainty for each prediction (i.e., confidence based on
probability density or quantile) is essential for safety-critical contexts like
human-robot collaboration to minimize risks. However, existing diverse motion
forecasting approaches struggle with uncertainty quantification due to implicit
probabilistic representations hindering uncertainty modeling. We propose
ProbHMI, which introduces invertible networks to parameterize poses in a
disentangled latent space, enabling probabilistic dynamics modeling. A
forecasting module then explicitly predicts future latent distributions,
allowing effective uncertainty quantification. Evaluated on benchmarks, ProbHMI
achieves strong performance for both deterministic and diverse prediction while
validating uncertainty calibration, critical for risk-aware decision making.

</details>


### [524] [Low-Latency Event-Based Velocimetry for Quadrotor Control in a Narrow Pipe](https://arxiv.org/abs/2507.15444)
*Leonard Bauersfeld,Davide Scaramuzza*

Main category: cs.RO

TL;DR: The paper presents a closed-loop control system for quadrotors to hover in narrow pipes using real-time flow field measurements, combining event-based smoke velocimetry and a learning-based controller to counteract aerodynamic disturbances.


<details>
  <summary>Details</summary>
Motivation: Autonomous quadrotor flight in confined spaces like pipes is challenging due to unsteady aerodynamic disturbances, and existing methods either require constant motion or lack stability during hovering.

Method: Develops a low-latency, event-based smoke velocimetry method for real-time airflow estimation, integrates a disturbance estimator using a recurrent CNN, and employs a reinforcement learning-trained controller.

Result: The system effectively counteracts transient aerodynamic effects during lateral maneuvers, preventing collisions with pipe walls, and provides insights into flow structures in confined spaces.

Conclusion: This work is the first demonstration of closed-loop control for aerial robots using real-time flow field measurements, advancing research in aerodynamically complex environments and bridging robotics and fluid dynamics.

Abstract: Autonomous quadrotor flight in confined spaces such as pipes and tunnels
presents significant challenges due to unsteady, self-induced aerodynamic
disturbances. Very recent advances have enabled flight in such conditions, but
they either rely on constant motion through the pipe to mitigate airflow
recirculation effects or suffer from limited stability during hovering. In this
work, we present the first closed-loop control system for quadrotors for
hovering in narrow pipes that leverages real-time flow field measurements. We
develop a low-latency, event-based smoke velocimetry method that estimates
local airflow at high temporal resolution. This flow information is used by a
disturbance estimator based on a recurrent convolutional neural network, which
infers force and torque disturbances in real time. The estimated disturbances
are integrated into a learning-based controller trained via reinforcement
learning. The flow-feedback control proves particularly effective during
lateral translation maneuvers in the pipe cross-section. There, the real-time
disturbance information enables the controller to effectively counteract
transient aerodynamic effects, thereby preventing collisions with the pipe
wall. To the best of our knowledge, this work represents the first
demonstration of an aerial robot with closed-loop control informed by real-time
flow field measurements. This opens new directions for research on flight in
aerodynamically complex environments. In addition, our work also sheds light on
the characteristic flow structures that emerge during flight in narrow,
circular pipes, providing new insights at the intersection of robotics and
fluid dynamics.

</details>


### [525] [GR-3 Technical Report](https://arxiv.org/abs/2507.15493)
*Chilam Cheang,Sijin Chen,Zhongren Cui,Yingdong Hu,Liqun Huang,Tao Kong,Hang Li,Yifeng Li,Yuxiao Liu,Xiao Ma,Hao Niu,Wenxuan Ou,Wanli Peng,Zeyu Ren,Haixin Shi,Jiawen Tian,Hongtao Wu,Xin Xiao,Yuyang Xiao,Jiafeng Xu,Yichu Yang*

Main category: cs.RO

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We report our recent progress towards building generalist robot policies, the
development of GR-3. GR-3 is a large-scale vision-language-action (VLA) model.
It showcases exceptional capabilities in generalizing to novel objects,
environments, and instructions involving abstract concepts. Furthermore, it can
be efficiently fine-tuned with minimal human trajectory data, enabling rapid
and cost-effective adaptation to new settings. GR-3 also excels in handling
long-horizon and dexterous tasks, including those requiring bi-manual
manipulation and mobile movement, showcasing robust and reliable performance.
These capabilities are achieved through a multi-faceted training recipe that
includes co-training with web-scale vision-language data, efficient fine-tuning
from human trajectory data collected via VR devices, and effective imitation
learning with robot trajectory data. In addition, we introduce ByteMini, a
versatile bi-manual mobile robot designed with exceptional flexibility and
reliability, capable of accomplishing a wide range of tasks when integrated
with GR-3. Through extensive real-world experiments, we show GR-3 surpasses the
state-of-the-art baseline method, $\pi_0$, on a wide variety of challenging
tasks. We hope GR-3 can serve as a step towards building generalist robots
capable of assisting humans in daily life.

</details>


### [526] [Look, Focus, Act: Efficient and Robust Robot Learning via Human Gaze and Foveated Vision Transformers](https://arxiv.org/abs/2507.15833)
*Ian Chuang,Andrew Lee,Dechen Gao,Jinyu Zou,Iman Soltani*

Main category: cs.RO

TL;DR: The paper explores integrating human-like active gaze into robotic policies to enhance efficiency and performance, using foveated image processing and Vision Transformers (ViTs).


<details>
  <summary>Details</summary>
Motivation: Human vision is task-driven and efficient, while robotic systems often process images uniformly. The work aims to bridge this gap by leveraging human gaze for better robotic vision.

Method: The authors introduce a framework for collecting eye-tracking data and robot demonstrations, integrate gaze into ViTs via foveated patch tokenization, and explore gaze imitation and prediction models.

Result: Foveated robot vision reduces computational overhead and improves performance in precision tasks and robustness to distractors.

Conclusion: Human-inspired visual processing provides a valuable inductive bias for robotic vision systems.

Abstract: Human vision is a highly active process driven by gaze, which directs
attention and fixation to task-relevant regions and dramatically reduces visual
processing. In contrast, robot learning systems typically rely on passive,
uniform processing of raw camera images. In this work, we explore how
incorporating human-like active gaze into robotic policies can enhance both
efficiency and performance. We build on recent advances in foveated image
processing and apply them to an Active Vision robot system that emulates both
human head movement and eye tracking. Extending prior work on the AV-ALOHA
robot simulation platform, we introduce a framework for simultaneously
collecting eye-tracking data and robot demonstrations from a human operator as
well as a simulation benchmark and dataset for training robot policies that
incorporate human gaze. Given the widespread use of Vision Transformers (ViTs)
in robot learning, we integrate gaze information into ViTs using a foveated
patch tokenization scheme inspired by recent work in image segmentation.
Compared to uniform patch tokenization, this significantly reduces the number
of tokens-and thus computation-without sacrificing visual fidelity near regions
of interest. We also explore two approaches to gaze imitation and prediction
from human data. The first is a two-stage model that predicts gaze to guide
foveation and action; the second integrates gaze into the action space,
allowing the policy to jointly predict gaze and actions end-to-end. Our results
show that our method for foveated robot vision not only drastically reduces
computational overhead, but also improves performance for high precision tasks
and robustness to unseen distractors. Together, these findings suggest that
human-inspired visual processing offers a useful inductive bias for robotic
vision systems. https://ian-chuang.github.io/gaze-av-aloha/

</details>


### [527] [One Step Beyond: Feedthrough & Placement-Aware Rectilinear Floorplanner](https://arxiv.org/abs/2507.14914)
*Zhexuan Xu,Jie Wang,Siyuan Xu,Zijie Geng,Mingxuan Yuan,Feng Wu*

Main category: cs.RO

TL;DR: Flora is a three-stage floorplanner optimizing feedthrough and placement, outperforming existing methods in HPWL, FTpin, FTmod, and component placement.


<details>
  <summary>Details</summary>
Motivation: Existing floorplanning approaches often fail to integrate with later design stages, causing suboptimal placement and excessive feedthrough.

Method: Flora uses wiremask and position mask for coarse optimization, resizes modules for fine optimization, and employs tree search for component placement.

Result: Flora reduces HPWL by 6%, FTpin by 5.16%, FTmod by 29.15%, and improves placement performance by 14%.

Conclusion: Flora effectively integrates floorplanning with physical design stages, achieving superior PPA metrics.

Abstract: Floorplanning determines the shapes and locations of modules on a chip canvas
and plays a critical role in optimizing the chip's Power, Performance, and Area
(PPA) metrics. However, existing floorplanning approaches often fail to
integrate with subsequent physical design stages, leading to suboptimal
in-module component placement and excessive inter-module feedthrough. To tackle
this challenge, we propose Flora, a three-stage feedthrough and placement aware
rectilinear floorplanner. In the first stage, Flora employs wiremask and
position mask techniques to achieve coarse-grained optimization of HPWL and
feedthrough. In the second stage, under the constraint of a fixed outline,
Flora achieves a zero-whitespace layout by locally resizing module shapes,
thereby performing fine-grained optimization of feedthrough and improving
component placement. In the third stage, Flora utilizes a fast tree
search-based method to efficiently place components-including macros and
standard cells-within each module, subsequently adjusting module boundaries
based on the placement results to enable cross-stage optimization. Experimental
results show that Flora outperforms recent state-of-the-art floorplanning
approaches, achieving an average reduction of 6% in HPWL, 5.16% in FTpin,
29.15% in FTmod, and a 14% improvement in component placement performance.

</details>


### [528] [FCRF: Flexible Constructivism Reflection for Long-Horizon Robotic Task Planning with Large Language Models](https://arxiv.org/abs/2507.14975)
*Yufan Song,Jiatao Zhang,Zeng Gu,Qingmiao Liang,Tuocheng Hu,Wei Song,Shiqiang Zhu*

Main category: cs.RO

TL;DR: Proposes FCRF, a Mentor-Actor architecture for flexible self-reflection in LLMs, improving error correction in domestic robots.


<details>
  <summary>Details</summary>
Motivation: Addresses inflexible self-reflection in LLMs for task planning error correction, inspired by human cognitive adaptation.

Method: Introduces FCRF, a Mentor-Actor framework integrating historical experience and failure lessons for adaptive self-reflection.

Result: FCRF enhances performance and flexibility in complex tasks, validated in AlfWorld simulations and real-world deployments.

Conclusion: FCRF effectively improves autonomous error correction in domestic robots for long-horizon tasks.

Abstract: Autonomous error correction is critical for domestic robots to achieve
reliable execution of complex long-horizon tasks. Prior work has explored
self-reflection in Large Language Models (LLMs) for task planning error
correction; however, existing methods are constrained by inflexible
self-reflection mechanisms that limit their effectiveness. Motivated by these
limitations and inspired by human cognitive adaptation, we propose the Flexible
Constructivism Reflection Framework (FCRF), a novel Mentor-Actor architecture
that enables LLMs to perform flexible self-reflection based on task difficulty,
while constructively integrating historical valuable experience with failure
lessons. We evaluated FCRF on diverse domestic tasks through simulation in
AlfWorld and physical deployment in the real-world environment. Experimental
results demonstrate that FCRF significantly improves overall performance and
self-reflection flexibility in complex long-horizon robotic tasks.

</details>


### [529] [Touch in the Wild: Learning Fine-Grained Manipulation with a Portable Visuo-Tactile Gripper](https://arxiv.org/abs/2507.15062)
*Xinyue Zhu,Binghao Huang,Yunzhu Li*

Main category: cs.RO

TL;DR: A portable gripper with tactile sensors is introduced for synchronized visual and tactile data collection, enabling improved robotic manipulation through cross-modal representation learning.


<details>
  <summary>Details</summary>
Motivation: Existing handheld grippers lack tactile sensing, which is crucial for precise manipulation, prompting the development of a gripper with integrated tactile feedback.

Method: A lightweight gripper with tactile sensors is designed, coupled with a cross-modal representation learning framework to integrate visual and tactile data while preserving their distinct features.

Result: The system demonstrates improved accuracy and robustness in fine-grained tasks like test tube insertion and fluid transfer, even under disturbances.

Conclusion: The integration of tactile sensing and cross-modal learning enhances robotic manipulation, validated by successful real-world applications.

Abstract: Handheld grippers are increasingly used to collect human demonstrations due
to their ease of deployment and versatility. However, most existing designs
lack tactile sensing, despite the critical role of tactile feedback in precise
manipulation. We present a portable, lightweight gripper with integrated
tactile sensors that enables synchronized collection of visual and tactile data
in diverse, real-world, and in-the-wild settings. Building on this hardware, we
propose a cross-modal representation learning framework that integrates visual
and tactile signals while preserving their distinct characteristics. The
learning procedure allows the emergence of interpretable representations that
consistently focus on contacting regions relevant for physical interactions.
When used for downstream manipulation tasks, these representations enable more
efficient and effective policy learning, supporting precise robotic
manipulation based on multimodal feedback. We validate our approach on
fine-grained tasks such as test tube insertion and pipette-based fluid
transfer, demonstrating improved accuracy and robustness under external
disturbances. Our project page is available at
https://binghao-huang.github.io/touch_in_the_wild/ .

</details>


### [530] [The Constitutional Controller: Doubt-Calibrated Steering of Compliant Agents](https://arxiv.org/abs/2507.15478)
*Simon Kohaut,Felix Divo,Navid Hamid,Benedict Flade,Julian Eggert,Devendra Singh Dhami,Kristian Kersting*

Main category: cs.RO

TL;DR: Neuro-symbolic systems integrate probabilistic reasoning with deep learning to enhance autonomous agents' reliability, introducing the Constitutional Controller (CoCo) and self-doubt for safer navigation.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of ensuring reliable and rule-compliant behavior of autonomous agents in uncertain environments.

Method: Introduces the Constitutional Controller (CoCo), combining deep probabilistic logic programs with self-doubt mechanisms based on doubt features.

Result: Demonstrated in a real-world aerial mobility study, showing improved safety and compliance in complex environments.

Conclusion: CoCo and self-doubt provide a robust solution for autonomous systems to navigate uncertain environments safely and compliantly.

Abstract: Ensuring reliable and rule-compliant behavior of autonomous agents in
uncertain environments remains a fundamental challenge in modern robotics. Our
work shows how neuro-symbolic systems, which integrate probabilistic, symbolic
white-box reasoning models with deep learning methods, offer a powerful
solution to this challenge. This enables the simultaneous consideration of
explicit rules and neural models trained on noisy data, combining the strength
of structured reasoning with flexible representations. To this end, we
introduce the Constitutional Controller (CoCo), a novel framework designed to
enhance the safety and reliability of agents by reasoning over deep
probabilistic logic programs representing constraints such as those found in
shared traffic spaces. Furthermore, we propose the concept of self-doubt,
implemented as a probability density conditioned on doubt features such as
travel velocity, employed sensors, or health factors. In a real-world aerial
mobility study, we demonstrate CoCo's advantages for intelligent autonomous
systems to learn appropriate doubts and navigate complex and uncertain
environments safely and compliantly.

</details>


### [531] [The Emergence of Deep Reinforcement Learning for Path Planning](https://arxiv.org/abs/2507.15469)
*Thanh Thi Nguyen,Saeid Nahavandi,Imran Razzak,Dung Nguyen,Nhat Truong Pham,Quoc Viet Hung Nguyen*

Main category: cs.RO

TL;DR: A survey on path planning methodologies, comparing traditional graph-based and evolutionary methods with recent deep reinforcement learning (DRL) advancements, focusing on autonomous systems.


<details>
  <summary>Details</summary>
Motivation: The demand for autonomous systems in dynamic environments drives the need for advanced path planning techniques.

Method: Categorizes and analyzes traditional (graph-based, linear programming, evolutionary) and modern (DRL) approaches, highlighting innovations and implementations.

Result: Identifies strengths and limitations of each method in computational efficiency, scalability, adaptability, and robustness.

Conclusion: Proposes hybrid DRL-classical approaches as a promising direction for robust autonomous navigation, with open challenges for future research.

Abstract: The increasing demand for autonomous systems in complex and dynamic
environments has driven significant research into intelligent path planning
methodologies. For decades, graph-based search algorithms, linear programming
techniques, and evolutionary computation methods have served as foundational
approaches in this domain. Recently, deep reinforcement learning (DRL) has
emerged as a powerful method for enabling autonomous agents to learn optimal
navigation strategies through interaction with their environments. This survey
provides a comprehensive overview of traditional approaches as well as the
recent advancements in DRL applied to path planning tasks, focusing on
autonomous vehicles, drones, and robotic platforms. Key algorithms across both
conventional and learning-based paradigms are categorized, with their
innovations and practical implementations highlighted. This is followed by a
thorough discussion of their respective strengths and limitations in terms of
computational efficiency, scalability, adaptability, and robustness. The survey
concludes by identifying key open challenges and outlining promising avenues
for future research. Special attention is given to hybrid approaches that
integrate DRL with classical planning techniques to leverage the benefits of
both learning-based adaptability and deterministic reliability, offering
promising directions for robust and resilient autonomous navigation.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [532] [Complex Dynamics in Psychological Data: Mapping Individual Symptom Trajectories to Group-Level Patterns](https://arxiv.org/abs/2507.14161)
*Eleonora Vitanza,Pietro DeLellis,Chiara Mocenni,Manuel Ruiz Marin*

Main category: stat.AP

TL;DR: The study combines causal inference, graph analysis, and machine learning to analyze symptom trajectories in GAD and MDD, achieving 91% diagnostic accuracy with a novel pipeline.


<details>
  <summary>Details</summary>
Motivation: To explore if individual symptom trajectories can reveal diagnostic patterns and improve personalized therapies.

Method: Uses PCMCI+ for causal network analysis and complexity measures (entropy, fractal dimension) with machine learning for diagnosis.

Result: PCMCI+ identifies disorder-specific causal mechanisms; ML achieves 91% accuracy in symptom classification.

Conclusion: Integration of causal modeling and temporal complexity enhances diagnostic differentiation and supports personalized clinical assessment.

Abstract: This study integrates causal inference, graph analysis, temporal complexity
measures, and machine learning to examine whether individual symptom
trajectories can reveal meaningful diagnostic patterns. Testing on a
longitudinal dataset of N=45 individuals affected by General Anxiety Disorder
(GAD) and/or Major Depressive Disorder (MDD) derived from Fisher et al. 2017,
we propose a novel pipeline for the analysis of the temporal dynamics of
psychopathological symptoms. First, we employ the PCMCI+ algorithm with
nonparametric independence test to determine the causal network of nonlinear
dependencies between symptoms in individuals with different mental disorders.
We found that the PCMCI+ effectively highlights the individual peculiarities of
each symptom network, which could be leveraged towards personalized therapies.
At the same time, aggregating the networks by diagnosis sheds light to
disorder-specific causal mechanisms, in agreement with previous
psychopathological literature. Then, we enrich the dataset by computing
complexity-based measures (e.g. entropy, fractal dimension, recurrence) from
the symptom time series, and feed it to a suitably selected machine learning
algorithm to aid the diagnosis of each individual. The new dataset yields 91%
accuracy in the classification of the symptom dynamics, proving to be an
effective diagnostic support tool. Overall, these findings highlight how
integrating causal modeling and temporal complexity can enhance diagnostic
differentiation, offering a principled, data-driven foundation for both
personalized assessment in clinical psychology and structural advances in
psychological research.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [533] [ExCyTIn-Bench: Evaluating LLM agents on Cyber Threat Investigation](https://arxiv.org/abs/2507.14201)
*Yiran Wu,Mauricio Velazco,Andrew Zhao,Manuel Raúl Meléndez Luján,Srisuma Movva,Yogesh K Roy,Quang Nguyen,Roberto Rodriguez,Qingyun Wu,Michael Albada,Julia Kiseleva,Anand Mudgerikar*

Main category: cs.CR

TL;DR: ExCyTIn-Bench is the first benchmark for evaluating LLM agents in cyber threat investigation using security questions from investigation graphs.


<details>
  <summary>Details</summary>
Motivation: Real-world security analysts face challenges in sifting through alerts and logs. LLM-based agents could automate threat investigation, but a benchmark is needed for development and evaluation.

Method: The dataset includes 8 simulated attacks, 57 log tables, and 589 questions. Questions are generated using LLMs from expert-crafted threat investigation graphs, with nodes as context and answers.

Result: Experiments show the task's difficulty: average reward is 0.249, with the best at 0.368, indicating room for improvement.

Conclusion: ExCyTIn-Bench provides a reusable, extensible framework for evaluating and training LLM agents in cyber threat investigation.

Abstract: We present ExCyTIn-Bench, the first benchmark to Evaluate an LLM agent x on
the task of Cyber Threat Investigation through security questions derived from
investigation graphs. Real-world security analysts must sift through a large
number of heterogeneous alert signals and security logs, follow multi-hop
chains of evidence, and compile an incident report. With the developments of
LLMs, building LLM-based agents for automatic thread investigation is a
promising direction. To assist the development and evaluation of LLM agents, we
construct a dataset from a controlled Azure tenant that covers 8 simulated
real-world multi-step attacks, 57 log tables from Microsoft Sentinel and
related services, and 589 automatically generated questions. We leverage
security logs extracted with expert-crafted detection logic to build threat
investigation graphs, and then generate questions with LLMs using paired nodes
on the graph, taking the start node as background context and the end node as
answer. Anchoring each question to these explicit nodes and edges not only
provides automatic, explainable ground truth answers but also makes the
pipeline reusable and readily extensible to new logs. This also enables the
automatic generation of procedural tasks with verifiable rewards, which can be
naturally extended to training agents via reinforcement learning. Our
comprehensive experiments with different models confirm the difficulty of the
task: with the base setting, the average reward across all evaluated models is
0.249, and the best achieved is 0.368, leaving substantial headroom for future
research. Code and data are coming soon!

</details>


### [534] [PRM-Free Security Alignment of Large Models via Red Teaming and Adversarial Training](https://arxiv.org/abs/2507.14202)
*Pengfei Du*

Main category: cs.CR

TL;DR: A novel PRM-free security alignment framework for LLMs uses automated red teaming and adversarial training to enhance robustness and efficiency, reducing computational costs by 61%.


<details>
  <summary>Details</summary>
Motivation: Current PRM-based security alignment methods for LLMs are computationally expensive and lack scalability, posing risks for safe deployment in critical domains.

Method: The framework employs automated red teaming (genetic algorithm optimization, multi-agent simulation, prompt mutation) and adversarial training (curriculum learning, adaptive regularization) to identify vulnerabilities and enhance robustness.

Result: Outperforms PRM-based approaches in security alignment while cutting computational costs by 61%, validated across five state-of-the-art LLMs.

Conclusion: The framework democratizes robust security measures for resource-constrained organizations and provides a scalable solution for evolving adversarial threats.

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
diverse applications, yet they pose significant security risks that threaten
their safe deployment in critical domains. Current security alignment
methodologies predominantly rely on Process Reward Models (PRMs) to evaluate
intermediate reasoning steps, introducing substantial computational overhead
and scalability constraints. This paper presents a novel PRM-free security
alignment framework that leverages automated red teaming and adversarial
training to achieve robust security guarantees while maintaining computational
efficiency. Our approach systematically identifies vulnerabilities through
sophisticated attack strategies including genetic algorithm optimization,
multi-agent simulation, and advanced prompt mutation techniques. The framework
enhances model robustness via targeted adversarial training with curriculum
learning and adaptive regularization mechanisms. Comprehensive experimental
evaluation across five state-of-the-art LLMs demonstrates that our method
achieves superior security alignment performance compared to PRM-based
approaches while reducing computational costs by 61\%. The framework
incorporates transparent reporting and continuous audit mechanisms that enable
iterative security improvement and regulatory compliance. Our contributions
advance the field of efficient LLM security alignment by democratizing access
to robust security measures for resource-constrained organizations and
providing a scalable foundation for addressing evolving adversarial threats.

</details>


### [535] [Mitigating Trojanized Prompt Chains in Educational LLM Use Cases: Experimental Findings and Detection Tool Design](https://arxiv.org/abs/2507.14207)
*Richard M. Charles,James H. Curry,Richard B. Charles*

Main category: cs.CR

TL;DR: The study investigates how students can manipulate LLMs in K-12 education to bypass safety measures, revealing vulnerabilities in GPT-3.5 and GPT-4, and proposes a tool, TrojanPromptGuard, to detect and mitigate such risks.


<details>
  <summary>Details</summary>
Motivation: To address the emerging risks of students exploiting LLMs to produce unsafe or unintended outputs in educational settings.

Method: Systematic experiments with simulated K-12 queries and multi-turn dialogues to test LLM vulnerabilities.

Result: Identified key vulnerabilities in GPT-3.5 and GPT-4; developed TrojanPromptGuard to detect and mitigate manipulated prompts.

Conclusion: The findings highlight the need for robust safety measures in LLM deployment for education, offering practical solutions like TPG.

Abstract: The integration of Large Language Models (LLMs) in K--12 education offers
both transformative opportunities and emerging risks. This study explores how
students may Trojanize prompts to elicit unsafe or unintended outputs from
LLMs, bypassing established content moderation systems with safety guardrils.
Through a systematic experiment involving simulated K--12 queries and
multi-turn dialogues, we expose key vulnerabilities in GPT-3.5 and GPT-4. This
paper presents our experimental design, detailed findings, and a prototype
tool, TrojanPromptGuard (TPG), to automatically detect and mitigate Trojanized
educational prompts. These insights aim to inform both AI safety researchers
and educational technologists on the safe deployment of LLMs for educators.

</details>


### [536] [Multi-Granular Discretization for Interpretable Generalization in Precise Cyberattack Identification](https://arxiv.org/abs/2507.14223)
*Wen-Cheng Chung,Shu-Ting Huang,Hao-Ting Pai*

Main category: cs.CR

TL;DR: The paper introduces the Interpretable Generalization (IG) mechanism and its enhancement, IG-MD, for explainable intrusion detection, achieving high accuracy and transparency without extensive data.


<details>
  <summary>Details</summary>
Motivation: Current XAI pipelines in IDS provide partial or misleading insights, necessitating a fully auditable and interpretable solution.

Method: IG learns coherent patterns from traffic data and converts them into rules. IG-MD enhances IG by discretizing continuous features at multiple resolutions.

Result: IG-MD improves precision by ≥4% on UKM-IDS20 while maintaining recall ≈1.0, showing scalability without domain-specific tuning.

Conclusion: IG-MD demonstrates that interpretable models can achieve high performance and transparency across domains.

Abstract: Explainable intrusion detection systems (IDS) are now recognized as essential
for mission-critical networks, yet most "XAI" pipelines still bolt an
approximate explainer onto an opaque classifier, leaving analysts with partial
and sometimes misleading insights. The Interpretable Generalization (IG)
mechanism, published in IEEE Transactions on Information Forensics and
Security, eliminates that bottleneck by learning coherent patterns - feature
combinations unique to benign or malicious traffic - and turning them into
fully auditable rules. IG already delivers outstanding precision, recall, and
AUC on NSL-KDD, UNSW-NB15, and UKM-IDS20, even when trained on only 10% of the
data. To raise precision further without sacrificing transparency, we introduce
Multi-Granular Discretization (IG-MD), which represents every continuous
feature at several Gaussian-based resolutions. On UKM-IDS20, IG-MD lifts
precision by greater than or equal to 4 percentage points across all nine
train-test splits while preserving recall approximately equal to 1.0,
demonstrating that a single interpretation-ready model can scale across domains
without bespoke tuning.

</details>


### [537] [Breaking the Illusion of Security via Interpretation: Interpretable Vision Transformer Systems under Attack](https://arxiv.org/abs/2507.14248)
*Eldor Abdukhamidov,Mohammed Abuhamad,Simon S. Woo,Hyoungshick Kim,Tamer Abuhmed*

Main category: cs.CR

TL;DR: AdViT attack deceives ViT models and their interpreters with high success, making adversarial examples hard to detect.


<details>
  <summary>Details</summary>
Motivation: To explore vulnerabilities in ViT models even when paired with interpretation models, highlighting security risks.

Method: Proposes AdViT, an attack method generating adversarial examples that fool both ViT models and their interpreters.

Result: Achieves 100% attack success rate, with high misclassification confidence (98% white-box, 76% black-box) and accurate interpretations.

Conclusion: AdViT reveals significant vulnerabilities in ViT models, even with interpreters, posing challenges for secure applications.

Abstract: Vision transformer (ViT) models, when coupled with interpretation models, are
regarded as secure and challenging to deceive, making them well-suited for
security-critical domains such as medical applications, autonomous vehicles,
drones, and robotics. However, successful attacks on these systems can lead to
severe consequences. Recent research on threats targeting ViT models primarily
focuses on generating the smallest adversarial perturbations that can deceive
the models with high confidence, without considering their impact on model
interpretations. Nevertheless, the use of interpretation models can effectively
assist in detecting adversarial examples. This study investigates the
vulnerability of transformer models to adversarial attacks, even when combined
with interpretation models. We propose an attack called "AdViT" that generates
adversarial examples capable of misleading both a given transformer model and
its coupled interpretation model. Through extensive experiments on various
transformer models and two transformer-based interpreters, we demonstrate that
AdViT achieves a 100% attack success rate in both white-box and black-box
scenarios. In white-box scenarios, it reaches up to 98% misclassification
confidence, while in black-box scenarios, it reaches up to 76%
misclassification confidence. Remarkably, AdViT consistently generates accurate
interpretations in both scenarios, making the adversarial examples more
difficult to detect.

</details>


### [538] [Towards Efficient Privacy-Preserving Machine Learning: A Systematic Review from Protocol, Model, and System Perspectives](https://arxiv.org/abs/2507.14519)
*Wenxuan Zeng,Tianshi Xu,Yi Chen,Yifan Zhou,Mingzhe Zhang,Jin Tan,Cheng Hong,Meng Li*

Main category: cs.CR

TL;DR: A survey on privacy-preserving machine learning (PPML) focusing on cross-level optimizations to address efficiency and scalability challenges.


<details>
  <summary>Details</summary>
Motivation: To protect user data privacy in cloud-based ML services while mitigating the efficiency gap caused by cryptographic protocols.

Method: Categorizes and reviews PPML studies at protocol, model, and system levels, providing qualitative and quantitative comparisons.

Result: Identifies the need for integrating optimizations across levels and discusses future research directions.

Conclusion: The survey offers a comprehensive overview of PPML, aiming to inspire future breakthroughs and includes a GitHub repository for ongoing updates.

Abstract: Privacy-preserving machine learning (PPML) based on cryptographic protocols
has emerged as a promising paradigm to protect user data privacy in cloud-based
machine learning services. While it achieves formal privacy protection, PPML
often incurs significant efficiency and scalability costs due to orders of
magnitude overhead compared to the plaintext counterpart. Therefore, there has
been a considerable focus on mitigating the efficiency gap for PPML. In this
survey, we provide a comprehensive and systematic review of recent PPML studies
with a focus on cross-level optimizations. Specifically, we categorize existing
papers into protocol level, model level, and system level, and review progress
at each level. We also provide qualitative and quantitative comparisons of
existing works with technical insights, based on which we discuss future
research directions and highlight the necessity of integrating optimizations
across protocol, model, and system levels. We hope this survey can provide an
overarching understanding of existing approaches and potentially inspire future
breakthroughs in the PPML field. As the field is evolving fast, we also provide
a public GitHub repository to continuously track the developments, which is
available at https://github.com/PKU-SEC-Lab/Awesome-PPML-Papers.

</details>


### [539] [VTarbel: Targeted Label Attack with Minimal Knowledge on Detector-enhanced Vertical Federated Learning](https://arxiv.org/abs/2507.14625)
*Juntao Tan,Anran Li,Quanchao Liu,Peng Ran,Lan Zhang*

Main category: cs.CR

TL;DR: VTarbel is a two-stage attack framework for VFL that evades anomaly detectors and achieves targeted misclassification with minimal knowledge.


<details>
  <summary>Details</summary>
Motivation: To address underexplored security threats in VFL, particularly targeted label attacks, which existing methods fail to handle realistically.

Method: VTarbel uses a two-stage approach: preparation (selecting high-expressiveness samples and training local models) and attack (gradient-based perturbations to evade detection).

Result: VTarbel outperforms baselines, evades detection, and remains effective against privacy defenses across various models and datasets.

Conclusion: The study highlights security vulnerabilities in VFL and calls for robust, attack-aware defenses.

Abstract: Vertical federated learning (VFL) enables multiple parties with disjoint
features to collaboratively train models without sharing raw data. While
privacy vulnerabilities of VFL are extensively-studied, its security
threats-particularly targeted label attacks-remain underexplored. In such
attacks, a passive party perturbs inputs at inference to force
misclassification into adversary-chosen labels. Existing methods rely on
unrealistic assumptions (e.g., accessing VFL-model's outputs) and ignore
anomaly detectors deployed in real-world systems. To bridge this gap, we
introduce VTarbel, a two-stage, minimal-knowledge attack framework explicitly
designed to evade detector-enhanced VFL inference. During the preparation
stage, the attacker selects a minimal set of high-expressiveness samples (via
maximum mean discrepancy), submits them through VFL protocol to collect
predicted labels, and uses these pseudo-labels to train estimated detector and
surrogate model on local features. In attack stage, these models guide
gradient-based perturbations of remaining samples, crafting adversarial
instances that induce targeted misclassifications and evade detection. We
implement VTarbel and evaluate it against four model architectures, seven
multimodal datasets, and two anomaly detectors. Across all settings, VTarbel
outperforms four state-of-the-art baselines, evades detection, and retains
effective against three representative privacy-preserving defenses. These
results reveal critical security blind spots in current VFL deployments and
underscore urgent need for robust, attack-aware defenses.

</details>


### [540] [VMask: Tunable Label Privacy Protection for Vertical Federated Learning via Layer Masking](https://arxiv.org/abs/2507.14629)
*Juntao Tan,Lan Zhang,Zhonghao Hu,Kai Yang,Peng Ran,Bo Li*

Main category: cs.CR

TL;DR: VMask is a novel framework for protecting label privacy in VFL by masking critical layers using secret sharing, balancing privacy and utility effectively.


<details>
  <summary>Details</summary>
Motivation: Existing defenses against MC attacks in VFL either compromise accuracy or are computationally impractical, necessitating a better solution.

Method: VMask disrupts data-output correlation by masking critical layers with secret sharing, optimizing overhead and offering tunable privacy.

Result: VMask reduces label inference to random guessing, maintains model accuracy (e.g., 0.09% drop), and is significantly faster than cryptographic methods.

Conclusion: VMask provides an efficient, flexible defense against MC attacks in VFL, achieving superior privacy-utility trade-offs.

Abstract: Though vertical federated learning (VFL) is generally considered to be
privacy-preserving, recent studies have shown that VFL system is vulnerable to
label inference attacks originating from various attack surfaces. Among these
attacks, the model completion (MC) attack is currently the most powerful one.
Existing defense methods against it either sacrifice model accuracy or incur
impractical computational overhead. In this paper, we propose VMask, a novel
label privacy protection framework designed to defend against MC attack from
the perspective of layer masking. Our key insight is to disrupt the strong
correlation between input data and intermediate outputs by applying the secret
sharing (SS) technique to mask layer parameters in the attacker's model. We
devise a strategy for selecting critical layers to mask, reducing the overhead
that would arise from naively applying SS to the entire model. Moreover, VMask
is the first framework to offer a tunable privacy budget to defenders, allowing
for flexible control over the levels of label privacy according to actual
requirements. We built a VFL system, implemented VMask on it, and extensively
evaluated it using five model architectures and 13 datasets with different
modalities, comparing it to 12 other defense methods. The results demonstrate
that VMask achieves the best privacy-utility trade-off, successfully thwarting
the MC attack (reducing the label inference accuracy to a random guessing
level) while preserving model performance (e.g., in Transformer-based model,
the averaged drop of VFL model accuracy is only 0.09%). VMask's runtime is up
to 60,846 times faster than cryptography-based methods, and it only marginally
exceeds that of standard VFL by 1.8 times in a large Transformer-based model,
which is generally acceptable.

</details>


### [541] [Manipulating LLM Web Agents with Indirect Prompt Injection Attack via HTML Accessibility Tree](https://arxiv.org/abs/2507.14799)
*Sam Johnson,Viet Pham,Thai Le*

Main category: cs.CR

TL;DR: LLM-based web navigation agents are powerful but vulnerable to Indirect Prompt Injection (IPI) attacks, where adversaries manipulate HTML to hijack agent behavior, leading to malicious actions like credential theft.


<details>
  <summary>Details</summary>
Motivation: To expose security vulnerabilities in LLM-driven web agents, particularly the risk of IPI attacks, as these agents become more widely adopted.

Method: Used the Greedy Coordinate Gradient (GCG) algorithm and a Browser Gym agent powered by Llama-3.1 to test IPI attacks on real websites.

Result: High success rates in targeted and general attacks, including credential exfiltration and forced ad clicks, demonstrating significant security risks.

Conclusion: LLM-based web agents require stronger defenses against IPI attacks to ensure safe adoption, as current vulnerabilities pose critical risks.

Abstract: This work demonstrates that LLM-based web navigation agents offer powerful
automation capabilities but are vulnerable to Indirect Prompt Injection (IPI)
attacks. We show that adversaries can embed universal adversarial triggers in
webpage HTML to hijack agent behavior that utilizes the accessibility tree to
parse HTML, causing unintended or malicious actions. Using the Greedy
Coordinate Gradient (GCG) algorithm and a Browser Gym agent powered by
Llama-3.1, our system demonstrates high success rates across real websites in
both targeted and general attacks, including login credential exfiltration and
forced ad clicks. Our empirical results highlight critical security risks and
the need for stronger defenses as LLM-driven autonomous web agents become more
widely adopted. The system software
(https://github.com/sej2020/manipulating-web-agents) is released under the MIT
License, with an accompanying publicly available demo website
(http://lethaiq.github.io/attack-web-llm-agent).

</details>


### [542] [A Privacy-Centric Approach: Scalable and Secure Federated Learning Enabled by Hybrid Homomorphic Encryption](https://arxiv.org/abs/2507.14853)
*Khoa Nguyen,Tanveer Khan,Antonis Michalas*

Main category: cs.CR

TL;DR: Hybrid Homomorphic Encryption (HHE) is integrated with Federated Learning (FL) to address communication overhead and privacy issues, offering a scalable and secure solution.


<details>
  <summary>Details</summary>
Motivation: FL faces challenges in communication overhead and data privacy, and existing Privacy-preserving Techniques (PPTs) like Homomorphic Encryption (HE) add computational and communication costs.

Method: The paper proposes integrating Hybrid Homomorphic Encryption (HHE), combining symmetric encryption with HE, into FL.

Result: The integration of HHE with FL effectively addresses both communication and privacy challenges.

Conclusion: HHE with FL provides a scalable and secure approach for decentralized learning systems.

Abstract: Federated Learning (FL) enables collaborative model training without sharing
raw data, making it a promising approach for privacy-sensitive domains. Despite
its potential, FL faces significant challenges, particularly in terms of
communication overhead and data privacy. Privacy-preserving Techniques (PPTs)
such as Homomorphic Encryption (HE) have been used to mitigate these concerns.
However, these techniques introduce substantial computational and communication
costs, limiting their practical deployment. In this work, we explore how Hybrid
Homomorphic Encryption (HHE), a cryptographic protocol that combines symmetric
encryption with HE, can be effectively integrated with FL to address both
communication and privacy challenges, paving the way for scalable and secure
decentralized learning system.

</details>


### [543] [LibLMFuzz: LLM-Augmented Fuzz Target Generation for Black-box Libraries](https://arxiv.org/abs/2507.15058)
*Ian Hardgrove,John D. Hastings*

Main category: cs.CR

TL;DR: LibLMFuzz is a framework using LLMs to autonomously fuzz closed-source libraries, achieving high API coverage and correctness with minimal human intervention.


<details>
  <summary>Details</summary>
Motivation: Fuzzing closed-source libraries is costly and complex, especially when only binaries are available. LibLMFuzz aims to reduce these costs.

Method: Pairs an LLM with a lightweight tool-chain to analyze binaries, plan fuzz strategies, generate drivers, and self-repair errors.

Result: Achieved 100% API coverage and 75.52% nominal correctness for 1601 synthesized drivers across four Linux libraries.

Conclusion: LLM-augmented middleware can effectively reduce fuzzing costs for black-box components, with potential for future research in branch coverage.

Abstract: A fundamental problem in cybersecurity and computer science is determining
whether a program is free of bugs and vulnerabilities. Fuzzing, a popular
approach to discovering vulnerabilities in programs, has several advantages
over alternative strategies, although it has investment costs in the form of
initial setup and continuous maintenance. The choice of fuzzing is further
complicated when only a binary library is available, such as the case of
closed-source and proprietary software. In response, we introduce LibLMFuzz, a
framework that reduces costs associated with fuzzing closed-source libraries by
pairing an agentic Large Language Model (LLM) with a lightweight tool-chain
(disassembler/compiler/fuzzer) to autonomously analyze stripped binaries, plan
fuzz strategies, generate drivers, and iteratively self-repair build or runtime
errors. Tested on four widely-used Linux libraries, LibLMFuzz produced
syntactically correct drivers for all 558 fuzz-able API functions, achieving
100% API coverage with no human intervention. Across the 1601 synthesized
drivers, 75.52% were nominally correct on first execution. The results show
that LLM-augmented middleware holds promise in reducing the costs of fuzzing
black box components and provides a foundation for future research efforts.
Future opportunities exist for research in branch coverage.

</details>


### [544] [PromptArmor: Simple yet Effective Prompt Injection Defenses](https://arxiv.org/abs/2507.15219)
*Tianneng Shi,Kaijie Zhu,Zhun Wang,Yuqi Jia,Will Cai,Weida Liang,Haonan Wang,Hend Alzahrani,Joshua Lu,Kenji Kawaguchi,Basel Alomair,Xuandong Zhao,William Yang Wang,Neil Gong,Wenbo Guo,Dawn Song*

Main category: cs.CR

TL;DR: PromptArmor is a defense against prompt injection attacks in LLM agents, using an off-the-shelf LLM to detect and remove malicious prompts, achieving high accuracy and low attack success rates.


<details>
  <summary>Details</summary>
Motivation: LLM agents are vulnerable to prompt injection attacks, where malicious inputs override intended tasks, necessitating a robust defense mechanism.

Method: PromptArmor employs an off-the-shelf LLM to detect and remove injected prompts before processing, tested with models like GPT-4o and GPT-4.1.

Result: PromptArmor achieves false positive and negative rates below 1%, reducing attack success rates to under 1% on the AgentDojo benchmark.

Conclusion: PromptArmor is effective and recommended as a standard baseline for evaluating defenses against prompt injection attacks.

Abstract: Despite their potential, recent research has demonstrated that LLM agents are
vulnerable to prompt injection attacks, where malicious prompts are injected
into the agent's input, causing it to perform an attacker-specified task rather
than the intended task provided by the user. In this paper, we present
PromptArmor, a simple yet effective defense against prompt injection attacks.
Specifically, PromptArmor prompts an off-the-shelf LLM to detect and remove
potential injected prompts from the input before the agent processes it. Our
results show that PromptArmor can accurately identify and remove injected
prompts. For example, using GPT-4o, GPT-4.1, or o4-mini, PromptArmor achieves
both a false positive rate and a false negative rate below 1% on the AgentDojo
benchmark. Moreover, after removing injected prompts with PromptArmor, the
attack success rate drops to below 1%. We also demonstrate PromptArmor's
effectiveness against adaptive attacks and explore different strategies for
prompting an LLM. We recommend that PromptArmor be adopted as a standard
baseline for evaluating new defenses against prompt injection attacks.

</details>


### [545] [PiMRef: Detecting and Explaining Ever-evolving Spear Phishing Emails with Knowledge Base Invariants](https://arxiv.org/abs/2507.15393)
*Ruofan Liu,Yun Lin,Silas Yeo Shuen Yu,Xiwen Teoh,Zhenkai Liang,Jin Song Dong*

Main category: cs.CR

TL;DR: PiMRef is a reference-based phishing email detector that checks identity claims in emails, outperforming existing methods in precision and recall.


<details>
  <summary>Details</summary>
Motivation: The rise of LLMs enables highly convincing phishing emails, bypassing traditional detectors, necessitating a new approach.

Method: PiMRef verifies sender identity claims against a knowledge base and detects suspicious call-to-action prompts.

Result: PiMRef achieves 92.1% precision and 87.9% recall, outperforming state-of-the-art methods.

Conclusion: PiMRef effectively counters LLM-generated phishing emails by focusing on identity fact-checking.

Abstract: Phishing emails are a critical component of the cybercrime kill chain due to
their wide reach and low cost. Their ever-evolving nature renders traditional
rule-based and feature-engineered detectors ineffective in the ongoing arms
race between attackers and defenders. The rise of large language models (LLMs)
further exacerbates the threat, enabling attackers to craft highly convincing
phishing emails at minimal cost.
  This work demonstrates that LLMs can generate psychologically persuasive
phishing emails tailored to victim profiles, successfully bypassing nearly all
commercial and academic detectors. To defend against such threats, we propose
PiMRef, the first reference-based phishing email detector that leverages
knowledge-based invariants. Our core insight is that persuasive phishing emails
often contain disprovable identity claims, which contradict real-world facts.
PiMRef reframes phishing detection as an identity fact-checking task. Given an
email, PiMRef (i) extracts the sender's claimed identity, (ii) verifies the
legitimacy of the sender's domain against a predefined knowledge base, and
(iii) detects call-to-action prompts that push user engagement. Contradictory
claims are flagged as phishing indicators and serve as human-understandable
explanations.
  Compared to existing methods such as D-Fence, HelpHed, and ChatSpamDetector,
PiMRef boosts precision by 8.8% with no loss in recall on standard benchmarks
like Nazario and PhishPot. In a real-world evaluation of 10,183 emails across
five university accounts over three years, PiMRef achieved 92.1% precision,
87.9% recall, and a median runtime of 0.05s, outperforming the state-of-the-art
in both effectiveness and efficiency.

</details>


### [546] [Multi-Stage Prompt Inference Attacks on Enterprise LLM Systems](https://arxiv.org/abs/2507.15613)
*Andrii Balashov,Olena Ponomarova,Xiaohua Zhai*

Main category: cs.CR

TL;DR: The paper studies multi-stage prompt inference attacks on enterprise LLMs, proposes defenses, and validates them mathematically and empirically.


<details>
  <summary>Details</summary>
Motivation: To address novel security threats like prompt inference attacks in enterprise LLMs, which exploit benign queries to extract confidential data.

Method: Simulates realistic attack scenarios, develops a formal threat model, and analyzes attacks using probability theory, optimization, and information-theoretic bounds. Proposes defenses like anomaly detection, access control, and prompt sanitization.

Result: Attacks reliably extract sensitive data despite safety measures. Defenses like differential privacy and spotlighting reduce attack success significantly.

Conclusion: Securing enterprise LLMs requires a multi-stage defense strategy beyond single-turn filtering.

Abstract: Large Language Models (LLMs) deployed in enterprise settings (e.g., as
Microsoft 365 Copilot) face novel security challenges. One critical threat is
prompt inference attacks: adversaries chain together seemingly benign prompts
to gradually extract confidential data. In this paper, we present a
comprehensive study of multi-stage prompt inference attacks in an enterprise
LLM context. We simulate realistic attack scenarios where an attacker uses
mild-mannered queries and indirect prompt injections to exploit an LLM
integrated with private corporate data. We develop a formal threat model for
these multi-turn inference attacks and analyze them using probability theory,
optimization frameworks, and information-theoretic leakage bounds. The attacks
are shown to reliably exfiltrate sensitive information from the LLM's context
(e.g., internal SharePoint documents or emails), even when standard safety
measures are in place.
  We propose and evaluate defenses to counter such attacks, including
statistical anomaly detection, fine-grained access control, prompt sanitization
techniques, and architectural modifications to LLM deployment. Each defense is
supported by mathematical analysis or experimental simulation. For example, we
derive bounds on information leakage under differential privacy-based training
and demonstrate an anomaly detection method that flags multi-turn attacks with
high AUC. We also introduce an approach called "spotlighting" that uses input
transformations to isolate untrusted prompt content, reducing attack success by
an order of magnitude. Finally, we provide a formal proof of concept and
empirical validation for a combined defense-in-depth strategy. Our work
highlights that securing LLMs in enterprise settings requires moving beyond
single-turn prompt filtering toward a holistic, multi-stage perspective on both
attacks and defenses.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [547] [A Disentangled Representation Learning Framework for Low-altitude Network Coverage Prediction](https://arxiv.org/abs/2507.14186)
*Xiaojie Li,Zhijie Cai,Nan Qi,Chao Dong,Guangxu Zhu,Haixia Ma,Qihui Wu,Shi Jin*

Main category: cs.NI

TL;DR: The paper proposes a dual-strategy framework combining expert knowledge-based feature compression and disentangled representation learning to improve Low-Altitude Network Coverage (LANC) prediction, addressing data scarcity and imbalanced sampling challenges.


<details>
  <summary>Details</summary>
Motivation: Accurate LANC prediction is crucial for designing aerial corridors, but proprietary antenna beam patterns and sparse, costly road test data hinder progress.

Method: The framework uses expert knowledge to compress features and disentangled representation learning to enhance generalizability, integrating propagation models and subnetworks for latent feature aggregation.

Result: The method reduces prediction error by 7% compared to baselines and achieves practical accuracy with MAE errors at 5dB in real-network tests.

Conclusion: The proposed framework effectively addresses data scarcity and imbalance, offering reliable LANC prediction for low-altitude network design.

Abstract: The expansion of the low-altitude economy has underscored the significance of
Low-Altitude Network Coverage (LANC) prediction for designing aerial corridors.
While accurate LANC forecasting hinges on the antenna beam patterns of Base
Stations (BSs), these patterns are typically proprietary and not readily
accessible. Operational parameters of BSs, which inherently contain beam
information, offer an opportunity for data-driven low-altitude coverage
prediction. However, collecting extensive low-altitude road test data is
cost-prohibitive, often yielding only sparse samples per BS. This scarcity
results in two primary challenges: imbalanced feature sampling due to limited
variability in high-dimensional operational parameters against the backdrop of
substantial changes in low-dimensional sampling locations, and diminished
generalizability stemming from insufficient data samples. To overcome these
obstacles, we introduce a dual strategy comprising expert knowledge-based
feature compression and disentangled representation learning. The former
reduces feature space complexity by leveraging communications expertise, while
the latter enhances model generalizability through the integration of
propagation models and distinct subnetworks that capture and aggregate the
semantic representations of latent features. Experimental evaluation confirms
the efficacy of our framework, yielding a 7% reduction in error compared to the
best baseline algorithm. Real-network validations further attest to its
reliability, achieving practical prediction accuracy with MAE errors at the 5dB
level.

</details>


### [548] [From Cell Towers to Satellites: A 2040 Blueprint for Urban-Grade Direct-to-Device Mobile Networks](https://arxiv.org/abs/2507.14188)
*Sebastian Barros Elgueta*

Main category: cs.NI

TL;DR: The paper explores the feasibility of a fully orbital mobile network, proposing an architecture and roadmap for urban-grade service without terrestrial reliance.


<details>
  <summary>Details</summary>
Motivation: To advance beyond current fallback-grade satellite-mobile systems and enable autonomous, urban-grade mobile networks from orbit.

Method: Proposes an end-to-end system with phased arrays, space-based 5G core functions, and laser backhaul, analyzing spectral efficiency and urban conditions.

Result: Simulations show rooftop users achieve 64-QAM throughput; street-level access is feasible with relays. Engineering bottlenecks are identified but deemed solvable.

Conclusion: A 15-year roadmap is outlined to transition from current D2D systems to autonomous orbital networks delivering 50-100 Mbps in megacities.

Abstract: In 2023, satellite and mobile networks crossed a historic threshold: standard
smartphones, using unmodified 3GPP protocols, connected directly to low Earth
orbit (LEO) satellites. This first wave of direct-to-device (D2D)
demonstrations validated the physical feasibility of satellite-based mobile
access. However, these systems remain fallback-grade--rural-only,
bandwidth-limited, and fully dependent on Earth-based mobile cores for
identity, session, and policy control. This paper asks a more ambitious
question: Can a complete mobile network, including radio access, core
functions, traffic routing, and content delivery, operate entirely from orbit?
And can it deliver sustained, urban-grade service in the world's densest
cities? We present the first end-to-end system architecture for a fully orbital
telco, integrating electronically steered phased arrays with 1000-beam
capacity, space-based deployment of 5G core functions (UPF, AMF), and
inter-satellite laser mesh backhaul. We analyze spectral efficiency, beam
capacity, and link budgets under dense urban conditions, accounting for path
loss, Doppler, and multipath. Simulations show that rooftop and line-of-sight
users can sustain 64-QAM throughput, while street-level access is feasible with
relay or assisted beam modes. The paper outlines the remaining constraints,
power, thermal dissipation, compute radiation hardening, and regulatory models,
and demonstrates that these are engineering bottlenecks, not physical limits.
Finally, we propose a staged 15-year roadmap from today's fallback D2D systems
to autonomous orbital overlays delivering 50-100 Mbps to handhelds in
megacities, with zero reliance on terrestrial infrastructure.

</details>


### [549] [PRATA: A Framework to Enable Predictive QoS in Vehicular Networks via Artificial Intelligence](https://arxiv.org/abs/2507.14211)
*Federico Mason,Tommaso Zugno,Matteo Drago,Marco Giordani,Mate Boban,Michele Zorzi*

Main category: cs.NI

TL;DR: PRATA is a simulation framework using AI for Predictive QoS in teleoperated driving, optimizing QoS and QoE via RL, outperforming baselines by nearly doubling performance.


<details>
  <summary>Details</summary>
Motivation: Teleoperated driving requires strict latency and reliability, necessitating predictive QoS to avoid performance degradation.

Method: PRATA includes a 5G RAN simulator, automotive data generator, and AI unit (RAN-AI) for RL-based QoS optimization.

Result: RAN-AI balances QoS and QoE, nearly doubling system performance over baselines, and explores state space and data cost impacts.

Conclusion: PRATA effectively enhances QoS for teleoperated driving via AI, demonstrating RL's potential in predictive QoS optimization.

Abstract: Predictive Quality of Service (PQoS) makes it possible to anticipate QoS
changes, e.g., in wireless networks, and trigger appropriate countermeasures to
avoid performance degradation. Hence, PQoS is extremely useful for automotive
applications such as teleoperated driving, which poses strict constraints in
terms of latency and reliability. A promising tool for PQoS is given by
Reinforcement Learning (RL), a methodology that enables the design of
decision-making strategies for stochastic optimization. In this manuscript, we
present PRATA, a new simulation framework to enable PRedictive QoS based on AI
for Teleoperated driving Applications. PRATA consists of a modular pipeline
that includes (i) an end-to-end protocol stack to simulate the 5G Radio Access
Network (RAN), (ii) a tool for generating automotive data, and (iii) an
Artificial Intelligence (AI) unit to optimize PQoS decisions. To prove its
utility, we use PRATA to design an RL unit, named RAN-AI, to optimize the
segmentation level of teleoperated driving data in the event of resource
saturation or channel degradation. Hence, we show that the RAN-AI entity
efficiently balances the trade-off between QoS and Quality of Experience (QoE)
that characterize teleoperated driving applications, almost doubling the system
performance compared to baseline approaches. In addition, by varying the
learning settings of the RAN-AI entity, we investigate the impact of the state
space and the relative cost of acquiring network data that are necessary for
the implementation of RL.

</details>


### [550] [Intent-Based Network for RAN Management with Large Language Models](https://arxiv.org/abs/2507.14230)
*Fransiscus Asisi Bimo,Maria Amparo Canaveras Galdon,Chun-Kai Lai,Ray-Guang Cheng,Edwin K. P. Chong*

Main category: cs.NI

TL;DR: The paper proposes an intent-based automation approach for RAN management using LLMs, improving energy efficiency through dynamic parameter optimization.


<details>
  <summary>Details</summary>
Motivation: To address the complexity of managing wireless networks by automating high-level intent translation and network configuration.

Method: Leverages LLMs within an agentic architecture for intent translation, reasoning, and dynamic RAN parameter optimization via structured prompt engineering.

Result: Demonstrates improved energy efficiency through closed-loop optimization of RAN parameters.

Conclusion: LLM-orchestrated agentic systems show potential for robust, real-time RAN resource management.

Abstract: Advanced intelligent automation becomes an important feature to deal with the
increased complexity in managing wireless networks. This paper proposes a novel
automation approach of intent-based network for Radio Access Networks (RANs)
management by leveraging Large Language Models (LLMs). The proposed method
enhances intent translation, autonomously interpreting high-level objectives,
reasoning over complex network states, and generating precise configurations of
the RAN by integrating LLMs within an agentic architecture. We propose a
structured prompt engineering technique and demonstrate that the network can
automatically improve its energy efficiency by dynamically optimizing critical
RAN parameters through a closed-loop mechanism. It showcases the potential to
enable robust resource management in RAN by adapting strategies based on
real-time feedback via LLM-orchestrated agentic systems.

</details>


### [551] [Beyond DNS: Unlocking the Internet of AI Agents via the NANDA Index and Verified AgentFacts](https://arxiv.org/abs/2507.14263)
*Ramesh Raskar,Pradyumna Chari,John Zinky,Mahesh Lambe,Jared James Grogan,Sichao Wang,Rajesh Ranjan,Rekha Singhal,Shailja Gupta,Robert Lincourt,Raghu Bala,Aditi Joshi,Abhishek Singh,Ayush Chopra,Dimitris Stripelis,Bhuwan B,Sumit Kumar,Maria Gorskikh*

Main category: cs.NI

TL;DR: NANDA index architecture enables discoverability, identifiability, and authentication for AI agents on the internet, offering rapid resolution, revocation, and privacy-preserving features.


<details>
  <summary>Details</summary>
Motivation: The internet will host billions of AI agents, straining current DNS-centered systems, necessitating a new architecture for secure and efficient agent discovery and collaboration.

Method: Proposes NANDA architecture with a lean index resolving to cryptographically verifiable AgentFacts, supporting multi-endpoint routing, load balancing, and privacy-preserving access.

Result: Delivers five guarantees: quilt-like index, rapid resolution, sub-second revocation, schema-validated assertions, and privacy-preserving discovery.

Conclusion: NANDA provides a scalable, secure foundation for AI agent collaboration without disrupting existing web infrastructure.

Abstract: The Internet is poised to host billions to trillions of autonomous AI agents
that negotiate, delegate, and migrate in milliseconds and workloads that will
strain DNS-centred identity and discovery. In this paper, we describe the NANDA
index architecture, which we envision as a means for discoverability,
identifiability and authentication in the internet of AI agents. We present an
architecture where a minimal lean index resolves to dynamic, cryptographically
verifiable AgentFacts that supports multi-endpoint routing, load balancing,
privacy-preserving access, and credentialed capability assertions. Our
architecture design delivers five concrete guarantees: (1) A quilt-like index
proposal that supports both NANDA-native agents as well as third party agents
being discoverable via the index, (2) rapid global resolution for newly spawned
AI agents, (3) sub-second revocation and key rotation, (4) schema-validated
capability assertions, and (5) privacy-preserving discovery across
organisational boundaries via verifiable, least-disclosure queries. We
formalize the AgentFacts schema, specify a CRDT-based update protocol, and
prototype adaptive resolvers. The result is a lightweight, horizontally
scalable foundation that unlocks secure, trust-aware collaboration for the next
generation of the Internet of AI agents, without abandoning existing web
infrastructure.

</details>


### [552] [On Splitting Lightweight Semantic Image Segmentation for Wireless Communications](https://arxiv.org/abs/2507.14199)
*Ebrahim Abu-Helalah,Jordi Serra,Jordi Perez-Romero*

Main category: cs.NI

TL;DR: Proposes splitting semantic image segmentation between transmitter and receiver to reduce bandwidth and computational load while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Addresses the challenge of balancing computational efficiency, bandwidth, and accuracy in semantic communication for image segmentation, especially in resource-limited environments.

Method: Splits the semantic image segmentation process between a constrained transmitter and receiver, reducing transmitted data and computational load.

Result: Simulations show up to 72% bit rate reduction and over 19% lower computational load at the transmitter, with maintained segmentation accuracy.

Conclusion: The technique is promising for communication systems, particularly 6G, due to its efficiency and accuracy trade-offs.

Abstract: Semantic communication represents a promising technique towards reducing
communication costs, especially when dealing with image segmentation, but it
still lacks a balance between computational efficiency and bandwidth
requirements while maintaining high image segmentation accuracy, particularly
in resource-limited environments and changing channel conditions. On the other
hand, the more complex and larger semantic image segmentation models become,
the more stressed the devices are when processing data. This paper proposes a
novel approach to implementing semantic communication based on splitting the
semantic image segmentation process between a resource constrained transmitter
and the receiver. This allows saving bandwidth by reducing the transmitted data
while maintaining the accuracy of the semantic image segmentation.
Additionally, it reduces the computational requirements at the resource
constrained transmitter compared to doing all the semantic image segmentation
in the transmitter. The proposed approach is evaluated by means of
simulation-based experiments in terms of different metrics such as
computational resource usage, required bit rate and segmentation accuracy. The
results when comparing the proposal with the full semantic image segmentation
in the transmitter show that up to 72% of the bit rate was reduced in the
transmission process. In addition, the computational load of the transmitter is
reduced by more than 19%. This reflects the interest of this technique for its
application in communication systems, particularly in the upcoming 6G systems.

</details>


### [553] [Agentic Satellite-Augmented Low-Altitude Economy and Terrestrial Networks: A Survey on Generative Approaches](https://arxiv.org/abs/2507.14633)
*Xiaozheng Gao,Yichen Wang,Bosen Liu,Xiao Zhou,Ruichen Zhang,Jiacheng Wang,Dusit Niyato,Dong In Kim,Abbas Jamalipour,Chau Yuen,Jianping An,Kai Yang*

Main category: cs.NI

TL;DR: This survey explores agentic AI for SLAETNs using generative AI and LLMs, covering architecture, challenges, generative models, and applications in communication, security, and satellite tasks.


<details>
  <summary>Details</summary>
Motivation: To address the need for intelligent, autonomous systems in SLAETNs by leveraging generative AI and LLMs for agentic functions.

Method: Systematic review of five generative models (VAEs, GANs, GDMs, TBMs, LLMs) and their applications in SLAETNs.

Result: Comparative analysis of generative models and their roles in enhancing communication, security, and satellite tasks.

Conclusion: Future directions focus on scalable, adaptive, and trustworthy generative agents for next-gen SLAETNs.

Abstract: The development of satellite-augmented low-altitude economy and terrestrial
networks (SLAETNs) demands intelligent and autonomous systems that can operate
reliably across heterogeneous, dynamic, and mission-critical environments. To
address these challenges, this survey focuses on enabling agentic artificial
intelligence (AI), that is, artificial agents capable of perceiving, reasoning,
and acting, through generative AI (GAI) and large language models (LLMs). We
begin by introducing the architecture and characteristics of SLAETNs, and
analyzing the challenges that arise in integrating satellite, aerial, and
terrestrial components. Then, we present a model-driven foundation by
systematically reviewing five major categories of generative models:
variational autoencoders (VAEs), generative adversarial networks (GANs),
generative diffusion models (GDMs), transformer-based models (TBMs), and LLMs.
Moreover, we provide a comparative analysis to highlight their generative
mechanisms, capabilities, and deployment trade-offs within SLAETNs. Building on
this foundation, we examine how these models empower agentic functions across
three domains: communication enhancement, security and privacy protection, and
intelligent satellite tasks. Finally, we outline key future directions for
building scalable, adaptive, and trustworthy generative agents in SLAETNs. This
survey aims to provide a unified understanding and actionable reference for
advancing agentic AI in next-generation integrated networks.

</details>


### [554] [Quantum Machine Learning for Secure Cooperative Multi-Layer Edge AI with Proportional Fairness](https://arxiv.org/abs/2507.15145)
*Thai T. Vu,John Le*

Main category: cs.NI

TL;DR: A communication-efficient, event-triggered framework for cooperative edge AI systems, optimizing classification utility under constraints.


<details>
  <summary>Details</summary>
Motivation: Extend single-device inference to distributed, multi-device settings with fairness constraints.

Method: Joint optimization framework using dual-threshold early-exit strategies and alternating optimization with Benders decomposition.

Result: Significant improvement in system-wide performance and fairness compared to single-device baselines.

Conclusion: The proposed framework effectively balances utility, communication, energy, and fairness in edge AI systems.

Abstract: This paper proposes a communication-efficient, event-triggered inference
framework for cooperative edge AI systems comprising multiple user devices and
edge servers. Building upon dual-threshold early-exit strategies for rare-event
detection, the proposed approach extends classical single-device inference to a
distributed, multi-device setting while incorporating proportional fairness
constraints across users. A joint optimization framework is formulated to
maximize classification utility under communication, energy, and fairness
constraints. To solve the resulting problem efficiently, we exploit the
monotonicity of the utility function with respect to the confidence thresholds
and apply alternating optimization with Benders decomposition. Experimental
results show that the proposed framework significantly enhances system-wide
performance and fairness in resource allocation compared to single-device
baselines.

</details>


### [555] [User Head Movement-Predictive XR in Immersive H2M Collaborations over Future Enterprise Networks](https://arxiv.org/abs/2507.15254)
*Sourav Mondal,Elaine Wong*

Main category: cs.NI

TL;DR: A novel H2M collaboration scheme using predictive models for head movement and dynamic bandwidth allocation improves XR performance with lower latency and bandwidth usage.


<details>
  <summary>Details</summary>
Motivation: The need for high-bandwidth, low-latency services in mobile and fixed wireless networks to support immersive XR experiences and avoid cyber-sickness.

Method: Proposes a human-machine collaboration scheme with predictive head movement models (bidirectional LSTM) and dynamic bandwidth allocation (HMC-DBA).

Result: Validates reduced latency, jitter, and bandwidth consumption in XR services over enterprise networks.

Conclusion: HMC-DBA outperforms state-of-the-art schemes in network efficiency and resource utilization for XR applications.

Abstract: The evolution towards future generation of mobile systems and fixed wireless
networks is primarily driven by the urgency to support high-bandwidth and
low-latency services across various vertical sectors. This endeavor is fueled
by smartphones as well as technologies like industrial internet of things,
extended reality (XR), and human-to-machine (H2M) collaborations for fostering
industrial and social revolutions like Industry 4.0/5.0 and Society 5.0. To
ensure an ideal immersive experience and avoid cyber-sickness for users in all
the aforementioned usage scenarios, it is typically challenging to synchronize
XR content from a remote machine to a human collaborator according to their
head movements across a large geographic span in real-time over communication
networks. Thus, we propose a novel H2M collaboration scheme where the human's
head movements are predicted ahead with highly accurate models like
bidirectional long short-term memory networks to orient the machine's camera in
advance. We validate that XR frame size varies in accordance with the human's
head movements and predict the corresponding bandwidth requirements from the
machine's camera to propose a human-machine coordinated dynamic bandwidth
allocation (HMC-DBA) scheme. Through extensive simulations, we show that
end-to-end latency and jitter requirements of XR frames are satisfied with much
lower bandwidth consumption over enterprise networks like
Fiber-To-The-Room-Business. Furthermore, we show that better efficiency in
network resource utilization is achieved by employing our proposed HMC-DBA over
state-of-the-art schemes.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [556] [Neural Brownian Motion](https://arxiv.org/abs/2507.14499)
*Qian Qi*

Main category: math.PR

TL;DR: The paper introduces Neural-Brownian Motion (NBM), a stochastic process for modeling dynamics under learned uncertainty, replacing the classical martingale property with a non-linear Neural Expectation Operator.


<details>
  <summary>Details</summary>
Motivation: To model dynamics under learned uncertainty and provide a foundation for models where uncertainty attitudes are discoverable.

Method: Defines NBM axiomatically using a Backward Stochastic Differential Equation (BSDE) with a neural network-parameterized driver. Proves existence and uniqueness of a canonical NBM.

Result: Shows that a canonical NBM exists and is unique, with volatility defined implicitly by the BSDE driver. Develops stochastic calculus and proves a Girsanov-type theorem.

Conclusion: NBM offers a rigorous framework for modeling learned uncertainty, with the measure's character (pessimistic or optimistic) determined by learned parameters.

Abstract: This paper introduces the Neural-Brownian Motion (NBM), a new class of
stochastic processes for modeling dynamics under learned uncertainty. The NBM
is defined axiomatically by replacing the classical martingale property with
respect to linear expectation with one relative to a non-linear Neural
Expectation Operator, $\varepsilon^\theta$, generated by a Backward Stochastic
Differential Equation (BSDE) whose driver $f_\theta$ is parameterized by a
neural network. Our main result is a representation theorem for a canonical
NBM, which we define as a continuous $\varepsilon^\theta$-martingale with zero
drift under the physical measure. We prove that, under a key structural
assumption on the driver, such a canonical NBM exists and is the unique strong
solution to a stochastic differential equation of the form ${\rm d} M_t =
\nu_\theta(t, M_t) {\rm d} W_t$. Crucially, the volatility function
$\nu_\theta$ is not postulated a priori but is implicitly defined by the
algebraic constraint $g_\theta(t, M_t, \nu_\theta(t, M_t)) = 0$, where
$g_\theta$ is a specialization of the BSDE driver. We develop the stochastic
calculus for this process and prove a Girsanov-type theorem for the quadratic
case, showing that an NBM acquires a drift under a new, learned measure. The
character of this measure, whether pessimistic or optimistic, is endogenously
determined by the learned parameters $\theta$, providing a rigorous foundation
for models where the attitude towards uncertainty is a discoverable feature.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [557] [APTx Neuron: A Unified Trainable Neuron Architecture Integrating Activation and Computation](https://arxiv.org/abs/2507.14270)
*Ravin Kumar*

Main category: cs.NE

TL;DR: The APTx Neuron integrates activation and transformation into one trainable unit, improving efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: To simplify neural architectures by unifying activation and linear transformation into a single trainable expression, enhancing computational efficiency.

Method: Derives the APTx Neuron from the APTx activation function, using the form $y = \sum_{i=1}^{n} ((\alpha_i + \tanh(\beta_i x_i)) \cdot \gamma_i x_i) + \delta$ with trainable parameters.

Result: Achieves 96.69% test accuracy on MNIST in 20 epochs with ~332K parameters, outperforming traditional neurons.

Conclusion: The APTx Neuron offers superior expressiveness and efficiency, paving the way for unified neuron designs.

Abstract: We propose the APTx Neuron, a novel, unified neural computation unit that
integrates non-linear activation and linear transformation into a single
trainable expression. The APTx Neuron is derived from the APTx activation
function, thereby eliminating the need for separate activation layers and
making the architecture both computationally efficient and elegant. The
proposed neuron follows the functional form $y = \sum_{i=1}^{n} ((\alpha_i +
\tanh(\beta_i x_i)) \cdot \gamma_i x_i) + \delta$, where all parameters
$\alpha_i$, $\beta_i$, $\gamma_i$, and $\delta$ are trainable. We validate our
APTx Neuron-based architecture on the MNIST dataset, achieving up to 96.69\%
test accuracy in just 20 epochs using approximately 332K trainable parameters.
The results highlight the superior expressiveness and computational efficiency
of the APTx Neuron compared to traditional neurons, pointing toward a new
paradigm in unified neuron design and the architectures built upon it.

</details>


### [558] [Analyzing Internal Activity and Robustness of SNNs Across Neuron Parameter Space](https://arxiv.org/abs/2507.14757)
*Szymon Mazurek,Jakub Caputa,Maciej Wielgosz*

Main category: cs.NE

TL;DR: The paper identifies an operational space for Spiking Neural Networks (SNNs) where optimal performance is achieved by tuning neuron hyperparameters (tau and vth). Outside this space, SNNs either waste energy or fail to function.


<details>
  <summary>Details</summary>
Motivation: SNNs are energy-efficient and biologically plausible, but their performance hinges on hyperparameter tuning. This work aims to define an optimal operational space for SNNs.

Method: Systematic exploration of neuron hyperparameters (tau and vth) across datasets and architectures to visualize and quantify the optimal operational manifold.

Result: Operating within the identified manifold balances accuracy and spiking activity, while deviations lead to inefficiency or silence. SNNs also show increased spike correlation and synchrony outside this region.

Conclusion: Principled hyperparameter tuning is crucial for SNN performance and energy efficiency, offering practical deployment guidelines, especially in neuromorphic computing.

Abstract: Spiking Neural Networks (SNNs) offer energy-efficient and biologically
plausible alternatives to traditional artificial neural networks, but their
performance depends critically on the tuning of neuron model parameters. In
this work, we identify and characterize an operational space - a constrained
region in the neuron hyperparameter domain (specifically membrane time constant
tau and voltage threshold vth) - within which the network exhibits meaningful
activity and functional behavior. Operating inside this manifold yields optimal
trade-offs between classification accuracy and spiking activity, while stepping
outside leads to degeneration: either excessive energy use or complete network
silence.
  Through systematic exploration across datasets and architectures, we
visualize and quantify this manifold and identify efficient operating points.
We further assess robustness to adversarial noise, showing that SNNs exhibit
increased spike correlation and internal synchrony when operating outside their
optimal region. These findings highlight the importance of principled
hyperparameter tuning to ensure both task performance and energy efficiency.
Our results offer practical guidelines for deploying robust and efficient SNNs,
particularly in neuromorphic computing scenarios.

</details>


<div id='math.AT'></div>

# math.AT [[Back]](#toc)

### [559] [Topological Social Choice: Designing a Noise-Robust Polar Distance for Persistence Diagrams](https://arxiv.org/abs/2507.14340)
*Athanasios Andrikopoulos,Nikolaos Sampanis*

Main category: math.AT

TL;DR: The paper introduces a new metric framework for persistence diagrams in Topological Data Analysis (TDA) tailored to noisy preference data in Social Choice Theory, addressing limitations of classical distances and demonstrating its effectiveness through experiments.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between TDA and Social Choice Theory by providing a robust and interpretable method for analyzing noisy preference data, which is geometrically rich but sensitive to perturbations.

Method: Proposes a polar coordinate-based distance metric for persistence diagrams, capturing magnitude and orientation of topological features smoothly and differentiably, improving stability and compatibility with gradient-based learning.

Result: The new metric outperforms classical distances (e.g., bottleneck, Wasserstein) in robustness and supervised learning tasks, validated through extensive experiments.

Conclusion: This work pioneers the application of persistent homology to social choice systems, offering a novel, interpretable tool for decision theory and opening new directions in machine learning for political and economic systems.

Abstract: Topological Data Analysis (TDA) has emerged as a powerful framework for
extracting robust and interpretable features from noisy high-dimensional data.
In the context of Social Choice Theory, where preference profiles and
collective decisions are geometrically rich yet sensitive to perturbations, TDA
remains largely unexplored. This work introduces a novel conceptual bridge
between these domains by proposing a new metric framework for persistence
diagrams tailored to noisy preference data.We define a polar coordinate-based
distance that captures both the magnitude and orientation of topological
features in a smooth and differentiable manner. Our metric addresses key
limitations of classical distances, such as bottleneck and Wasserstein,
including instability under perturbation, lack of continuity, and
incompatibility with gradient-based learning. The resulting formulation offers
improved behavior in both theoretical and applied settings.To the best of our
knowledge, this is the first study to systematically apply persistent homology
to social choice systems, providing a mathematically grounded method for
comparing topological summaries of voting structures and preference dynamics.
We demonstrate the superiority of our approach through extensive experiments,
including robustness tests and supervised learning tasks, and we propose a
modular pipeline for building predictive models from online preference data.
This work contributes a conceptually novel and computationally effective tool
to the emerging interface of topology and decision theory, opening new
directions in interpretable machine learning for political and economic
systems.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [560] [All-atom inverse protein folding through discrete flow matching](https://arxiv.org/abs/2507.14156)
*Kai Yi,Kiarash Jamali,Sjors H. W. Scheres*

Main category: q-bio.BM

TL;DR: ADFLIP is a generative model for inverse protein folding, excelling in designing sequences for dynamic complexes with non-protein components.


<details>
  <summary>Details</summary>
Motivation: Existing inverse folding methods struggle with non-protein components and dynamic complexes, prompting the need for ADFLIP.

Method: ADFLIP uses discrete flow-matching, incorporates side chains, and employs ensemble sampling and classifier guidance for sequence design.

Result: ADFLIP achieves state-of-the-art performance in single and multi-structure inverse folding tasks.

Conclusion: ADFLIP shows great potential for all-atom protein design, especially for dynamic complexes.

Abstract: The recent breakthrough of AlphaFold3 in modeling complex biomolecular
interactions, including those between proteins and ligands, nucleotides, or
metal ions, creates new opportunities for protein design. In so-called inverse
protein folding, the objective is to find a sequence of amino acids that adopts
a target protein structure. Many inverse folding methods struggle to predict
sequences for complexes that contain non-protein components, and perform poorly
with complexes that adopt multiple structural states. To address these
challenges, we present ADFLIP (All-atom Discrete FLow matching Inverse Protein
folding), a generative model based on discrete flow-matching for designing
protein sequences conditioned on all-atom structural contexts. ADFLIP
progressively incorporates predicted amino acid side chains as structural
context during sequence generation and enables the design of dynamic protein
complexes through ensemble sampling across multiple structural states.
Furthermore, ADFLIP implements training-free classifier guidance sampling,
which allows the incorporation of arbitrary pre-trained models to optimise the
designed sequence for desired protein properties. We evaluated the performance
of ADFLIP on protein complexes with small-molecule ligands, nucleotides, or
metal ions, including dynamic complexes for which structure ensembles were
determined by nuclear magnetic resonance (NMR). Our model achieves
state-of-the-art performance in single-structure and multi-structure inverse
folding tasks, demonstrating excellent potential for all-atom protein design.
The code is available at https://github.com/ykiiiiii/ADFLIP.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [561] [Forecasting Faculty Placement from Patterns in Co-authorship Networks](https://arxiv.org/abs/2507.14696)
*Samantha Dies,David Liu,Tina Eliassi-Rad*

Main category: cs.SI

TL;DR: The paper explores faculty hiring using co-authorship networks, showing they improve prediction accuracy by 10% over traditional metrics, especially for elite departments.


<details>
  <summary>Details</summary>
Motivation: To understand how social networks and professional endorsements influence faculty hiring beyond traditional measures like prestige and productivity.

Method: Uses temporal co-authorship networks and conventional attributes (e.g., doctoral prestige, bibliometrics) to predict individual faculty placements.

Result: Co-authorship networks boost predictive accuracy by up to 10%, with the largest gains for top-tier departments.

Conclusion: Highlights the role of social networks in hiring biases, suggesting interventions for fairer academic hiring.

Abstract: Faculty hiring shapes the flow of ideas, resources, and opportunities in
academia, influencing not only individual career trajectories but also broader
patterns of institutional prestige and scientific progress. While traditional
studies have found strong correlations between faculty hiring and attributes
such as doctoral department prestige and publication record, they rarely assess
whether these associations generalize to individual hiring outcomes,
particularly for future candidates outside the original sample. Here, we
consider faculty placement as an individual-level prediction task. Our data
consist of temporal co-authorship networks with conventional attributes such as
doctoral department prestige and bibliometric features. We observe that using
the co-authorship network significantly improves predictive accuracy by up to
10% over traditional indicators alone, with the largest gains observed for
placements at the most elite (top-10) departments. Our results underscore the
role that social networks, professional endorsements, and implicit advocacy
play in faculty hiring beyond traditional measures of scholarly productivity
and institutional prestige. By introducing a predictive framing of faculty
placement and establishing the benefit of considering co-authorship networks,
this work provides a new lens for understanding structural biases in academia
that could inform targeted interventions aimed at increasing transparency,
fairness, and equity in academic hiring practices.

</details>


### [562] [Privacy-Preserving Multimodal News Recommendation through Federated Learning](https://arxiv.org/abs/2507.15460)
*Mehdi Khalaj,Shahrzad Golestani Najafabadi,Julita Vassileva*

Main category: cs.SI

TL;DR: A multimodal federated learning approach improves personalized news recommendation by integrating textual and visual features, balancing short- and long-term interests, and enhancing privacy through federated learning and secure aggregation.


<details>
  <summary>Details</summary>
Motivation: Traditional PNR systems rely heavily on textual content, ignore short-term user interests, and pose privacy risks due to centralized data storage.

Method: The paper introduces a multimodal model for content representation, a time-aware model for interest balancing, and a federated learning framework with secure aggregation for privacy.

Result: Experiments show strong performance on a real-world dataset, advancing privacy-preserving news recommendation.

Conclusion: The proposed approach effectively addresses key challenges in PNR systems, offering improved accuracy and privacy.

Abstract: Personalized News Recommendation systems (PNR) have emerged as a solution to
information overload by predicting and suggesting news items tailored to
individual user interests. However, traditional PNR systems face several
challenges, including an overreliance on textual content, common neglect of
short-term user interests, and significant privacy concerns due to centralized
data storage. This paper addresses these issues by introducing a novel
multimodal federated learning-based approach for news recommendation. First, it
integrates both textual and visual features of news items using a multimodal
model, enabling a more comprehensive representation of content. Second, it
employs a time-aware model that balances users' long-term and short-term
interests through multi-head self-attention networks, improving recommendation
accuracy. Finally, to enhance privacy, a federated learning framework is
implemented, enabling collaborative model training without sharing user data.
The framework divides the recommendation model into a large server-maintained
news model and a lightweight user model shared between the server and clients.
The client requests news representations (vectors) and a user model from the
central server, then computes gradients with user local data, and finally sends
their locally computed gradients to the server for aggregation. The central
server aggregates gradients to update the global user model and news model. The
updated news model is further used to infer news representation by the server.
To further safeguard user privacy, a secure aggregation algorithm based on
Shamir's secret sharing is employed. Experiments on a real-world news dataset
demonstrate strong performance compared to existing systems, representing a
significant advancement in privacy-preserving personalized news recommendation.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [563] [Numerical Artifacts in Learning Dynamical Systems](https://arxiv.org/abs/2507.14491)
*Bing-Ze Lu,Richard Tsai*

Main category: math.NA

TL;DR: The paper highlights how numerical schemes in learning dynamical systems can lead to incorrect identification of system properties, such as mislabeling damped oscillatory systems as anti-damped.


<details>
  <summary>Details</summary>
Motivation: To investigate the impact of numerical integration schemes on the accuracy of learning dynamical systems from sampled data.

Method: Analyzes the learning process as an optimization problem, focusing on the role of numerical schemes in evaluating candidate systems.

Result: Shows that numerical schemes can cause significant errors, like misidentifying damped systems as anti-damped, even with good data fit.

Conclusion: Emphasizes the need for careful selection of numerical methods to avoid misleading learning outcomes.

Abstract: In many applications, one needs to learn a dynamical system from its
solutions sampled at a finite number of time points. The learning problem is
often formulated
  as an optimization problem over a chosen function class. However, in the
optimization procedure, it is necessary to employ a numerical scheme to
integrate candidate dynamical systems and assess how their solutions fit the
data.
  This paper reveals potentially serious effects of a chosen numerical scheme
on the learning outcome. In particular, our analysis demonstrates that a damped
oscillatory system may be incorrectly identified as having "anti-damping" and
exhibiting a reversed oscillation direction, despite adequately fitting the
given data points.

</details>


### [564] [Solving nonconvex Hamilton--Jacobi--Isaacs equations with PINN-based policy iteration](https://arxiv.org/abs/2507.15455)
*Hee Jun Yang,Min Jung Kim,Yeoneung Kim*

Main category: math.NA

TL;DR: A mesh-free policy iteration framework combines dynamic programming and PINNs to solve high-dimensional HJI equations, proving convergence and demonstrating accuracy in numerical experiments.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of solving high-dimensional, nonconvex HJI equations in stochastic differential games and robust control.

Method: Alternates between solving linear PDEs under fixed policies and updating controls via minimax optimization using automatic differentiation.

Result: Convergence to the viscosity solution is proven, with numerical experiments showing accuracy and scalability in 2D to 10D problems.

Conclusion: The method is practical and theoretically grounded, with potential applications in robotics, finance, and multi-agent reinforcement learning.

Abstract: We propose a mesh-free policy iteration framework that combines classical
dynamic programming with physics-informed neural networks (PINNs) to solve
high-dimensional, nonconvex Hamilton--Jacobi--Isaacs (HJI) equations arising in
stochastic differential games and robust control. The method alternates between
solving linear second-order PDEs under fixed feedback policies and updating the
controls via pointwise minimax optimization using automatic differentiation.
Under standard Lipschitz and uniform ellipticity assumptions, we prove that the
value function iterates converge locally uniformly to the unique viscosity
solution of the HJI equation. The analysis establishes equi-Lipschitz
regularity of the iterates, enabling provable stability and convergence without
requiring convexity of the Hamiltonian. Numerical experiments demonstrate the
accuracy and scalability of the method. In a two-dimensional stochastic
path-planning game with a moving obstacle, our method matches finite-difference
benchmarks with relative $L^2$-errors below %10^{-2}%. In five- and
ten-dimensional publisher-subscriber differential games with anisotropic noise,
the proposed approach consistently outperforms direct PINN solvers, yielding
smoother value functions and lower residuals. Our results suggest that
integrating PINNs with policy iteration is a practical and theoretically
grounded method for solving high-dimensional, nonconvex HJI equations, with
potential applications in robotics, finance, and multi-agent reinforcement
learning.

</details>


<div id='econ.TH'></div>

# econ.TH [[Back]](#toc)

### [565] [Approximate Revenue Maximization for Diffusion Auctions](https://arxiv.org/abs/2507.14470)
*Yifan Huang,Dong Hao,Zhiyi Fan,Yuhang Guo,Bin Li*

Main category: econ.TH

TL;DR: The paper explores designing revenue-optimal auctions via reserve prices in economic networks, extending beyond direct reachability. It introduces a simple, near-optimal reserve price function for network auctions, ensuring incentive compatibility and improved revenue.


<details>
  <summary>Details</summary>
Motivation: Current auction designs assume direct reachability of bidders, omitting many in economic networks. This work aims to extend optimal auction theory to all network entities.

Method: Uses Bayesian approximation to derive a reserve price function for network auctions, balancing high revenue and broader bidder participation.

Result: The proposed reserve price function achieves a $1-{1 \over \rho}$ approximation to the theoretical revenue upper bound, valid for any network size or structure.

Conclusion: The reserve price function enhances revenue in network auctions while maintaining incentive compatibility, outperforming traditional Myerson optimal auctions.

Abstract: Reserve prices are widely used in practice. The problem of designing
revenue-optimal auctions based on reserve price has drawn much attention in the
auction design community. Although they have been extensively studied, most
developments rely on the significant assumption that the target audience of the
sale is directly reachable by the auctioneer, while a large portion of bidders
in the economic network unaware of the sale are omitted. This work follows the
diffusion auction design, which aims to extend the target audience of optimal
auction theory to all entities in economic networks. We investigate the design
of simple and provably near-optimal network auctions via reserve price. Using
Bayesian approximation analysis, we provide a simple and explicit form of the
reserve price function tailored to the most representative network auction. We
aim to balance setting a sufficiently high reserve price to induce high revenue
in a successful sale, and attracting more buyers from the network to increase
the probability of a successful sale. This reserve price function preserves
incentive compatibility for network auctions, allowing the seller to extract
additional revenue beyond that achieved by the Myerson optimal auction.
Specifically, if the seller has $\rho$ direct neighbours in a network of size
$n$, this reserve price guarantees a $1-{1 \over \rho}$ approximation to the
theoretical upper bound, i.e., the maximum possible revenue from any network of
size $n$. This result holds for any size and any structure of the networked
market.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [566] [DIVER-0 : A Fully Channel Equivariant EEG Foundation Model](https://arxiv.org/abs/2507.14141)
*Danny Dongyeop Han,Ahhyun Lucy Lee,Taeyang Lee,Yonghyeon Gwon,Sebin Lee,Seongjin Lee,David Keetae Park,Shinjae Yoo,Jiook Cha,Chun Kee Chung*

Main category: eess.SP

TL;DR: DIVER-0, a novel EEG foundation model, uses full spatio-temporal attention with RoPE and binary attention biases to improve performance and generalization across diverse electrode configurations.


<details>
  <summary>Details</summary>
Motivation: Existing EEG models struggle with spatio-temporal dynamics and lack channel permutation equivariance, limiting generalization.

Method: DIVER-0 employs full spatio-temporal attention, RoPE for temporal relationships, binary attention biases, and STCPE for robust adaptation.

Result: DIVER-0 achieves competitive performance with 10% of pretraining data and maintains consistency across channel permutations.

Conclusion: DIVER-0 validates key design principles for handling EEG heterogeneity and enables cross-dataset generalization.

Abstract: Electroencephalography (EEG) is a non-invasive technique widely used in
brain-computer interfaces and clinical applications, yet existing EEG
foundation models face limitations in modeling spatio-temporal brain dynamics
and lack channel permutation equivariance, preventing robust generalization
across diverse electrode configurations. To address these challenges, we
propose DIVER-0, a novel EEG foundation model that demonstrates how full
spatio-temporal attention-rather than segregated spatial or temporal
processing-achieves superior performance when properly designed with Rotary
Position Embedding (RoPE) for temporal relationships and binary attention
biases for channel differentiation. We also introduce Sliding Temporal
Conditional Positional Encoding (STCPE), which improves upon existing
conditional positional encoding approaches by maintaining both temporal
translation equivariance and channel permutation equivariance, enabling robust
adaptation to arbitrary electrode configurations unseen during pretraining.
Experimental results demonstrate that DIVER-0 achieves competitive performance
with only 10% of pretraining data while maintaining consistent results across
all channel permutation conditions, validating its effectiveness for
cross-dataset generalization and establishing key design principles for
handling the inherent heterogeneity of neural recording setups.

</details>


### [567] [Self-DANA: A Resource-Efficient Channel-Adaptive Self-Supervised Approach for ECG Foundation Models](https://arxiv.org/abs/2507.14151)
*Giuliana Monachino,Nicolò La Porta,Beatrice Zanchi,Luigi Fiorillo,Alvise Dei Rossi,Georgiy Farina,Francesca Dalia Faraci*

Main category: eess.SP

TL;DR: Self-DANA is a novel solution for adapting ECG foundation models to reduced-channel configurations, improving resource efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Interest in ECG foundation models has grown, but their adaptation to fewer channels is understudied.

Method: Proposes Self-DANA and Random Lead Selection for self-supervised, channel-agnostic pre-training.

Result: Achieves state-of-the-art performance with significant resource savings (e.g., 69.3% less CPU memory).

Conclusion: Self-DANA effectively adapts ECG models to reduced-channel scenarios, balancing efficiency and performance.

Abstract: Foundation Models (FMs) are large-scale machine learning models trained on
extensive, diverse datasets that can be adapted to a wide range of downstream
tasks with minimal fine-tuning. In the last two years, interest in FMs has also
grown for applications in the cardiological field to analyze the
electrocardiogram (ECG) signals. One of the key properties of FMs is their
transferability to a wide range of downstream scenarios. With the spread of
wearable and portable devices, keen interest in learning from reduced-channel
configurations has arisen. However, the adaptation of ECG FMs to downstream
scenarios with fewer available channels still has to be properly investigated.
In this work, we propose Self-DANA, a novel, easy-to-integrate solution that
makes self-supervised architectures adaptable to a reduced number of input
channels, ensuring resource efficiency and high performance. We also introduce
Random Lead Selection, a novel augmentation technique to pre-train models in a
more robust and channel-agnostic way. Our experimental results on five
reduced-channel configurations demonstrate that Self-DANA significantly
enhances resource efficiency while reaching state-of-the-art performance. It
requires up to 69.3% less peak CPU memory, 34.4% less peak GPU memory, about
17% less average epoch CPU time, and about 24% less average epoch GPU time.

</details>


### [568] [Surface EMG Profiling in Parkinson's Disease: Advancing Severity Assessment with GCN-SVM](https://arxiv.org/abs/2507.14153)
*Daniel Cieślak,Barbara Szyca,Weronika Bajko,Liwia Florkiewicz,Kinga Grzęda,Mariusz Kaczmarek,Helena Kamieniecka,Hubert Lis,Weronika Matwiejuk,Anna Prus,Michalina Razik,Inga Rozumowicz,Wiktoria Ziembakowska*

Main category: eess.SP

TL;DR: The study proposes using sEMG to assess PD severity, achieving 92% accuracy with a GCN-SVM model.


<details>
  <summary>Details</summary>
Motivation: Parkinson's disease diagnosis and monitoring are challenging due to its progressive nature and complex symptoms.

Method: Utilizes sEMG on the biceps brachii muscle, comparing PD patients and controls, and employs SVM and GCN-SVM models for analysis.

Result: SVM achieved 83% accuracy, while GCN-SVM improved it to 92%.

Conclusion: The approach shows promise for PD severity assessment and clinical integration, though larger studies are needed for validation.

Abstract: Parkinson's disease (PD) poses challenges in diagnosis and monitoring due to
its progressive nature and complex symptoms. This study introduces a novel
approach utilizing surface electromyography (sEMG) to objectively assess PD
severity, focusing on the biceps brachii muscle. Initial analysis of sEMG data
from five PD patients and five healthy controls revealed significant
neuromuscular differences. A traditional Support Vector Machine (SVM) model
achieved up to 83% accuracy, while enhancements with a Graph Convolutional
Network-Support Vector Machine (GCN-SVM) model increased accuracy to 92%.
Despite the preliminary nature of these results, the study outlines a detailed
experimental methodology for future research with larger cohorts to validate
these findings and integrate the approach into clinical practice. The proposed
approach holds promise for advancing PD severity assessment and improving
patient care in Parkinson's disease management.

</details>


### [569] [A Denoising VAE for Intracardiac Time Series in Ischemic Cardiomyopathy](https://arxiv.org/abs/2507.14164)
*Samuel Ruipérez-Campillo,Alain Ryser,Thomas M. Sutter,Ruibin Feng,Prasanth Ganesan,Brototo Deb,Kelly A. Brennan,Maxime Pedron,Albert J. Rogers,Maarten Z. H. Kolk,Fleur V. Y. Tjong,Sanjiv M. Narayan,Julia E. Vogt*

Main category: eess.SP

TL;DR: A Variational Autoencoder (VAE) model is introduced to denoise intra-cardiac signals, outperforming traditional methods in handling diverse noise patterns for improved cardiac diagnosis and treatment.


<details>
  <summary>Details</summary>
Motivation: Traditional noise reduction techniques are inadequate for the non-linear and non-stationary noise in intra-cardiac signals, necessitating a more effective solution.

Method: A VAE model is trained on 5706 time series from 42 ischemic cardiomyopathy patients to reconstruct clean intra-ventricular monophasic action potential (MAP) signals.

Result: The VAE model shows superior denoising performance across various noise types compared to conventional clinical filtering methods.

Conclusion: VAEs effectively eliminate diverse noise sources in cardiac signals, potentially enhancing treatment efficacy in cardiac electrophysiology.

Abstract: In the field of cardiac electrophysiology (EP), effectively reducing noise in
intra-cardiac signals is crucial for the accurate diagnosis and treatment of
arrhythmias and cardiomyopathies. However, traditional noise reduction
techniques fall short in addressing the diverse noise patterns from various
sources, often non-linear and non-stationary, present in these signals. This
work introduces a Variational Autoencoder (VAE) model, aimed at improving the
quality of intra-ventricular monophasic action potential (MAP) signal
recordings. By constructing representations of clean signals from a dataset of
5706 time series from 42 patients diagnosed with ischemic cardiomyopathy, our
approach demonstrates superior denoising performance when compared to
conventional filtering methods commonly employed in clinical settings. We
assess the effectiveness of our VAE model using various metrics, indicating its
superior capability to denoise signals across different noise types, including
time-varying non-linear noise frequently found in clinical settings. These
results reveal that VAEs can eliminate diverse sources of noise in single
beats, outperforming state-of-the-art denoising techniques and potentially
improving treatment efficacy in cardiac EP.

</details>


### [570] [NeuroHD-RA: Neural-distilled Hyperdimensional Model with Rhythm Alignment](https://arxiv.org/abs/2507.14184)
*ZhengXiao He,Jinghao Wen,Huayu Li,Ao Li*

Main category: eess.SP

TL;DR: A novel ECG-based disease detection framework combines hyperdimensional computing (HDC) with neural encoding, outperforming traditional methods with improved precision and interpretability.


<details>
  <summary>Details</summary>
Motivation: To enhance ECG disease detection by integrating interpretable HDC with trainable neural encoding for better performance and adaptability.

Method: Uses a rhythm-aware, trainable encoding pipeline based on RR intervals and a neural-distilled HDC architecture with joint optimization.

Result: Achieves 73.09% precision and F1 score of 0.626 on Apnea-ECG, with robust performance on PTB-XL.

Conclusion: The framework offers scalable, interpretable ECG classification suitable for edge devices and personalized health monitoring.

Abstract: We present a novel and interpretable framework for electrocardiogram
(ECG)-based disease detection that combines hyperdimensional computing (HDC)
with learnable neural encoding. Unlike conventional HDC approaches that rely on
static, random projections, our method introduces a rhythm-aware and trainable
encoding pipeline based on RR intervals, a physiological signal segmentation
strategy that aligns with cardiac cycles. The core of our design is a
neural-distilled HDC architecture, featuring a learnable RR-block encoder and a
BinaryLinear hyperdimensional projection layer, optimized jointly with
cross-entropy and proxy-based metric loss. This hybrid framework preserves the
symbolic interpretability of HDC while enabling task-adaptive representation
learning. Experiments on Apnea-ECG and PTB-XL demonstrate that our model
significantly outperforms traditional HDC and classical ML baselines, achieving
73.09\% precision and an F1 score of 0.626 on Apnea-ECG, with comparable
robustness on PTB-XL. Our framework offers an efficient and scalable solution
for edge-compatible ECG classification, with strong potential for interpretable
and personalized health monitoring.

</details>


### [571] [AI-Based Impedance Encoding-Decoding Method for Online Impedance Network Construction of Wind Farms](https://arxiv.org/abs/2507.14187)
*Xiaojuan Zhang,Tianyu Jiang,Haoxiang Zong,Chen Zhang,Chendan Li,Marta Molinas*

Main category: eess.SP

TL;DR: The paper proposes an AI-based method to compress and reconstruct impedance curves for efficient online construction of impedance network models in wind farms.


<details>
  <summary>Details</summary>
Motivation: The traditional impedance network model requires transmitting numerous high-density impedance curves, making online application difficult.

Method: An AI-based impedance encoder compresses impedance curves, and a decoder reconstructs them. The nodal admittance matrix method is then used to obtain the impedance network model.

Result: The method enables fast transmission and accurate reconstruction of impedance curves, validated through model training and real-time simulations.

Conclusion: The proposed AI-based encoding-decoding method efficiently facilitates the online construction of impedance network models for wind farms.

Abstract: The impedance network (IN) model is gaining popularity in the oscillation
analysis of wind farms. However, the construction of such an IN model requires
impedance curves of each wind turbine under their respective operating
conditions, making its online application difficult due to the transmission of
numerous high-density impedance curves. To address this issue, this paper
proposes an AI-based impedance encoding-decoding method to facilitate the
online construction of IN model. First, an impedance encoder is trained to
compress impedance curves by setting the number of neurons much smaller than
that of frequency points. Then, the compressed data of each turbine are
uploaded to the wind farm and an impedance decoder is trained to reconstruct
original impedance curves. At last, based on the nodal admittance matrix (NAM)
method, the IN model of the wind farm can be obtained. The proposed method is
validated via model training and real-time simulations, demonstrating that the
encoded impedance vectors enable fast transmission and accurate reconstruction
of the original impedance curves.

</details>


### [572] [UWB Radar-based Heart Rate Monitoring: A Transfer Learning Approach](https://arxiv.org/abs/2507.14195)
*Elzbieta Gruzewska,Pooja Rao,Sebastien Baur,Matthew Baugh,Mathias M. J. Bellaiche,Sharanya Srinivas,Octavio Ponce,Matthew Thompson,Pramod Rudrapatna,Michael A. Sanchez,Lawrence Z. Cai,Timothy JA Chico,Robert F. Storey,Emily Maz,Umesh Telang,Shravya Shetty,Mayank Daswani*

Main category: eess.SP

TL;DR: Transfer learning between FMCW and IR-UWB radar systems for heart rate monitoring reduces MAE by 25%, enabling faster adoption in consumer devices.


<details>
  <summary>Details</summary>
Motivation: Radar technology offers contactless heart rate monitoring, but lack of standardization requires new datasets for each system. Transfer learning can bridge this gap.

Method: A 2D+1D ResNet architecture was used for FMCW radar (60 GHz, 5.5 GHz bandwidth), achieving 0.85 bpm MAE. The model was fine-tuned for IR-UWB radar (8 GHz, 500 MHz bandwidth) using a small dataset.

Result: FMCW model achieved 0.85 bpm MAE (1.42% MAPE). Transfer learning to IR-UWB reduced MAE to 4.1 bpm (6.3% MAPE), a 25% improvement over baseline.

Conclusion: Transfer learning between radar systems is feasible and effective, accelerating heart rate monitoring integration into consumer devices.

Abstract: Radar technology presents untapped potential for continuous, contactless, and
passive heart rate monitoring via consumer electronics like mobile phones.
However the variety of available radar systems and lack of standardization
means that a large new paired dataset collection is required for each radar
system. This study demonstrates transfer learning between frequency-modulated
continuous wave (FMCW) and impulse-radio ultra-wideband (IR-UWB) radar systems,
both increasingly integrated into consumer devices. FMCW radar utilizes a
continuous chirp, while IR-UWB radar employs short pulses. Our mm-wave FMCW
radar operated at 60 GHz with a 5.5 GHz bandwidth (2.7 cm resolution, 3
receiving antennas [Rx]), and our IR-UWB radar at 8 GHz with a 500 MHz
bandwidth (30 cm resolution, 2 Rx). Using a novel 2D+1D ResNet architecture we
achieved a mean absolute error (MAE) of 0.85 bpm and a mean absolute percentage
error (MAPE) of 1.42% for heart rate monitoring with FMCW radar (N=119
participants, an average of 8 hours per participant). This model maintained
performance (under 5 MAE/10% MAPE) across various body positions and heart rate
ranges, with a 98.9% recall. We then fine-tuned a variant of this model,
trained on single-antenna and single-range bin FMCW data, using a small (N=376,
avg 6 minutes per participant) IR-UWB dataset. This transfer learning approach
yielded a model with MAE 4.1 bpm and MAPE 6.3% (97.5% recall), a 25% MAE
reduction over the IR-UWB baseline. This demonstration of transfer learning
between radar systems for heart rate monitoring has the potential to accelerate
its introduction into existing consumer devices.

</details>


### [573] [Explainable Parallel CNN-LSTM Model for Differentiating Ventricular Tachycardia from Supraventricular Tachycardia with Aberrancy in 12-Lead ECGs](https://arxiv.org/abs/2507.14196)
*Zahra Teimouri-Jervekani,Fahimeh Nasimi,Mohammadreza Yazdchi,Ghazal MogharehZadeh,Javad Tezerji,Farzan Niknejad Mazandarani,Maryam Mohebbi*

Main category: eess.SP

TL;DR: A lightweight deep learning model for differentiating wide complex tachycardia (WCT) achieves high accuracy and interpretability using parallel 1D-CNN and LSTM layers, validated on ECG data.


<details>
  <summary>Details</summary>
Motivation: Differentiating WCT is critical but challenging due to ECG similarities between VT and SVT-A; misdiagnosis risks are fatal. The goal is to improve accuracy and provide interpretability for clinical use.

Method: A parallel deep architecture processes individual ECG leads with 1D-CNN blocks, concatenates features, and uses LSTM layers for temporal dependencies. SHAP provides interpretability. Evaluated on a 35-subject ECG database.

Result: Achieved 95.63% accuracy, 95.10% sensitivity, 96.06% specificity, and 95.12% F1-score, outperforming existing methods in accuracy and efficiency. SHAP analysis offered interpretable insights.

Conclusion: The framework provides high-precision WCT classification with low computational cost and interpretability, promising for real-world ECG tools.

Abstract: Background and Objective: Differentiating wide complex tachycardia (WCT) is
clinically critical yet challenging due to morphological similarities in
electrocardiogram (ECG) signals between life-threatening ventricular
tachycardia (VT) and supraventricular tachycardia with aberrancy (SVT-A).
Misdiagnosis carries fatal risks. We propose a computationally efficient deep
learning solution to improve diagnostic accuracy and provide model
interpretability for clinical deployment.
  Methods: A novel lightweight parallel deep architecture is introduced. Each
pipeline processes individual ECG leads using two 1D-CNN blocks to extract
local features. Feature maps are concatenated across leads, followed by LSTM
layers to capture temporal dependencies. Final classification employs fully
connected layers. Explainability is achieved via Shapley Additive Explanations
(SHAP) for local/global interpretation. The model was evaluated on a 35-subject
ECG database using standard performance metrics.
  Results: The model achieved $95.63\%$ accuracy ($95\%$ CI: $93.07-98.19\%$),
with sensitivity=$95.10\%$, specificity=$96.06\%$, and F1-score=$95.12\%$. It
outperformed state-of-the-art methods in both accuracy and computational
efficiency, requiring minimal CNN blocks per pipeline. SHAP analysis
demonstrated clinically interpretable feature contributions.
  Conclusions: Our end-to-end framework delivers high-precision WCT
classification with minimal computational overhead. The integration of SHAP
enhances clinical trust by elucidating decision logic, supporting rapid,
informed diagnosis. This approach shows significant promise for real-world ECG
analysis tools.

</details>


### [574] [A Comprehensive Benchmark for Electrocardiogram Time-Series](https://arxiv.org/abs/2507.14206)
*Zhijiang Tang,Jiaxin Qi,Yuhua Zheng,Jianqiang Huang*

Main category: eess.SP

TL;DR: The paper investigates ECG signals, categorizes downstream tasks, introduces a new metric, benchmarks models, and proposes a novel architecture, demonstrating robustness and effectiveness.


<details>
  <summary>Details</summary>
Motivation: Existing studies overlook ECG's unique characteristics and specialized applications, leading to incomplete understanding.

Method: Categorizes ECG applications, identifies metric limitations, introduces a new metric, benchmarks models, and proposes a new architecture.

Result: Extensive experiments show the benchmark's comprehensiveness and robustness, validating the new metric and model.

Conclusion: The work advances ECG signal analysis by providing a solid foundation for future research.

Abstract: Electrocardiogram~(ECG), a key bioelectrical time-series signal, is crucial
for assessing cardiac health and diagnosing various diseases. Given its
time-series format, ECG data is often incorporated into pre-training datasets
for large-scale time-series model training. However, existing studies often
overlook its unique characteristics and specialized downstream applications,
which differ significantly from other time-series data, leading to an
incomplete understanding of its properties. In this paper, we present an
in-depth investigation of ECG signals and establish a comprehensive benchmark,
which includes (1) categorizing its downstream applications into four distinct
evaluation tasks, (2) identifying limitations in traditional evaluation metrics
for ECG analysis, and introducing a novel metric; (3) benchmarking
state-of-the-art time-series models and proposing a new architecture. Extensive
experiments demonstrate that our proposed benchmark is comprehensive and
robust. The results validate the effectiveness of the proposed metric and model
architecture, which establish a solid foundation for advancing research in ECG
signal analysis.

</details>


### [575] [Age of Information Minimization in UAV-Enabled Integrated Sensing and Communication Systems](https://arxiv.org/abs/2507.14299)
*Yu Bai,Yifan Zhang,Boxuan Xie,Zheng Chang,Yanru Zhang,Riku Jantti,Zhu Han*

Main category: eess.SP

TL;DR: Proposes an AoI-centric UAV-ISAC system using DRL to optimize trajectory and beamforming for sensing and communication, achieving lower AoI than baselines.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of jointly optimizing UAV trajectory, multi-user communication, and target sensing under resource constraints and time-critical conditions.

Method: Develops a DRL-based algorithm with Kalman filtering for target prediction, zero-forcing for interference mitigation, and SAC for continuous action training.

Result: Simulations show the method achieves lower average AoI compared to baselines.

Conclusion: The proposed framework effectively balances sensing and communication trade-offs, enhancing UAV-ISAC performance.

Abstract: Unmanned aerial vehicles (UAVs) equipped with integrated sensing and
communication (ISAC) capabilities are envisioned to play a pivotal role in
future wireless networks due to their enhanced flexibility and efficiency.
However, jointly optimizing UAV trajectory planning, multi-user communication,
and target sensing under stringent resource constraints and time-critical
conditions remains a significant challenge. To address this, we propose an Age
of Information (AoI)-centric UAV-ISAC system that simultaneously performs
target sensing and serves multiple ground users, emphasizing information
freshness as the core performance metric. We formulate a long-term average AoI
minimization problem that jointly optimizes the UAV's flight trajectory and
beamforming. To tackle the high-dimensional, non-convexity of this problem, we
develop a deep reinforcement learning (DRL)-based algorithm capable of
providing real-time decisions on UAV movement and beamforming for both radar
sensing and multi-user communication. Specifically, a Kalman filter is employed
for accurate target state prediction, regularized zero-forcing is utilized to
mitigate inter-user interference, and the Soft Actor-Critic algorithm is
applied for training the DRL agent on continuous actions. The proposed
framework adaptively balances the trade-offs between sensing accuracy and
communication quality. Extensive simulation results demonstrate that our
proposed method consistently achieves lower average AoI compared to baseline
approaches.

</details>


### [576] [Recursive KalmanNet: Analyse des capacités de généralisation d'un réseau de neurones récurrent guidé par un filtre de Kalman](https://arxiv.org/abs/2507.14144)
*Cyril Falcon,Hassan Mortada,Mathéo Clavaud,Jean-Philippe Michel*

Main category: eess.SP

TL;DR: The paper explores the generalization of Recursive KalmanNet in out-of-distribution scenarios where test dynamics differ from training.


<details>
  <summary>Details</summary>
Motivation: To assess the Recursive KalmanNet's ability to generalize beyond its training data, particularly in scenarios with differing temporal dynamics.

Method: Uses a recurrent neural network guided by a Kalman filter to estimate state variables and error covariance without prior noise knowledge.

Result: Not explicitly stated in the abstract, but the focus is on evaluating generalization in out-of-distribution cases.

Conclusion: The study aims to demonstrate the robustness of Recursive KalmanNet in handling unseen temporal dynamics.

Abstract: The Recursive KalmanNet, recently introduced by the authors, is a recurrent
neural network guided by a Kalman filter, capable of estimating the state
variables and error covariance of stochastic dynamic systems from noisy
measurements, without prior knowledge of the noise characteristics. This paper
explores its generalization capabilities in out-of-distribution scenarios,
where the temporal dynamics of the test measurements differ from those
encountered during training.
  Le Recursive KalmanNet, r\'ecemment introduit par les auteurs, est un
r\'eseau de neurones r\'ecurrent guid\'e par un filtre de Kalman, capable
d'estimer les variables d'\'etat et la covariance des erreurs des syst\`emes
dynamiques stochastiques \`a partir de mesures bruit\'ees, sans connaissance
pr\'ealable des caract\'eristiques des bruits. Cet article explore ses
capacit\'es de g\'en\'eralisation dans des sc\'enarios hors distribution, o\`u
les dynamiques temporelles des mesures de test diff\`erent de celles
rencontr\'ees \`a l'entra\^inement.

</details>


### [577] [Graph Convolutional Neural Networks to Model the Brain for Insomnia](https://arxiv.org/abs/2507.14147)
*Kevin Monteiro,Sam Nallaperuma-Herzberg,Martina Mason,Steve Niederer*

Main category: eess.SP

TL;DR: The paper proposes a brain modeling approach using EEG data and a GCNN model to classify insomnia, achieving 70% accuracy at the window level and identifying key EEG channels for diagnosis.


<details>
  <summary>Details</summary>
Motivation: Existing insomnia treatments have side effects, and brain modeling hasn't been applied to insomnia. This study aims to fill that gap by analyzing EEG data.

Method: Uses continuous EEG data to derive brain networks via functional connectivity and spatial distance. Computes power spectral density and trains a GCNN for classification.

Result: Achieved 70% accuracy at window level and 68% at subject level. Identified key EEG channels (C4-P4, F4-C4, C4-A1) critical for performance.

Conclusion: The GCNN model effectively captures insomnia-related brain characteristics, with specific EEG channels playing a crucial role in diagnosis.

Abstract: Insomnia affects a vast population of the world and can have a wide range of
causes. Existing treatments for insomnia have been linked with many side
effects like headaches, dizziness, etc. As such, there is a clear need for
improved insomnia treatment. Brain modelling has helped with assessing the
effects of brain pathology on brain network dynamics and with supporting
clinical decisions in the treatment of Alzheimer's disease, epilepsy, etc.
However, such models have not been developed for insomnia. Therefore, this
project attempts to understand the characteristics of the brain of individuals
experiencing insomnia using continuous long-duration EEG data. Brain networks
are derived based on functional connectivity and spatial distance between EEG
channels. The power spectral density of the channels is then computed for the
major brain wave frequency bands. A graph convolutional neural network (GCNN)
model is then trained to capture the functional characteristics associated with
insomnia and configured for the classification task to judge performance.
Results indicated a 50-second non-overlapping sliding window was the most
suitable choice for EEG segmentation. This approach achieved a classification
accuracy of 70% at window level and 68% at subject level. Additionally, the
omission of EEG channels C4-P4, F4-C4 and C4-A1 caused higher degradation in
model performance than the removal of other channels. These channel electrodes
are positioned near brain regions known to exhibit atypical levels of
functional connectivity in individuals with insomnia, which can explain such
results.

</details>


### [578] [Machine learning-enabled river water quality monitoring using lithography-free 3D-printed sensors](https://arxiv.org/abs/2507.14152)
*Frank Efe Erukainure,Feidra Gjata,Matin Ataei Kachouei,Henry Cox,Md. Azahar Ali*

Main category: eess.SP

TL;DR: A lithography-free phosphate sensor (P-sensor) detects phosphate in river water at parts-per-billion levels, validated in real-world conditions and enhanced by a neural network for accurate predictions.


<details>
  <summary>Details</summary>
Motivation: Clean water is vital for health and ecology; phosphate contamination causes eutrophication, necessitating reliable monitoring tools.

Method: The P-sensor uses 3D-printed polymer patterns coated with a phosphate ion-selective membrane, detecting 1 ppb phosphate with a 30-second response time.

Result: Validated in Rappahannock River, the sensor matched commercial meters, and a neural network achieved high accuracy (MSE < 1e-3, r = 0.997).

Conclusion: The P-sensor is a practical tool for continuous water-quality monitoring, aiding stakeholders and improving public health.

Abstract: River water quality monitoring is important for aquatic life, livestock, and
humans because clean water is critical to meeting food demand during the global
food crisis. Excessive contaminants, including phosphate, deplete dissolved
oxygen and trigger eutrophication, leading to serious health and ecological
problems. Continuous sensors that track phosphate levels can therefore help
prevent eutrophication. In this work we present a lithography-free phosphate
sensor (P-sensor) that detects phosphate in river water at parts-per-billion
levels. The device uses a solid-state indicator electrode formed by 3D-printed
periodic polymer patterns (8 um feature size) coated with a thin phosphate
ion-selective membrane. The P-sensor detects as little as 1 ppb phosphate
across 0 - 475 ppm with a response time under 30 seconds. We validated the
sensor on Rappahannock River water, Virginia (less than 0.8 ppm phosphate) at
sites upstream and downstream of a sewage treatment plant and benchmarked the
results against a commercial phosphate meter. A feed-forward neural network was
trained to predict phosphate levels, achieving a mean-squared error below 1e-3,
zero standard deviation, and a Pearson correlation coefficient of 0.997 for
river samples. These results demonstrate a practical tool for continuous
water-quality monitoring that can inform stakeholders and policymakers and
ultimately improve public health.

</details>


### [579] [UniPhyNet: A Unified Network For Multimodal Physiological Raw Signal Classification](https://arxiv.org/abs/2507.14163)
*Renxiang Qiu,Raghavendra Selvan*

Main category: eess.SP

TL;DR: UniPhyNet is a neural network for classifying cognitive load using EEG, ECG, and EDA signals without hand-crafted features, achieving higher accuracy than feature-based models.


<details>
  <summary>Details</summary>
Motivation: To classify cognitive load using raw physiological signals without relying on manual feature extraction, improving accuracy and practicality for real-world monitoring.

Method: Combines multiscale parallel convolutional blocks, ResNet-type blocks with channel attention, and bidirectional GRUs for temporal dependencies, using intermediate fusion for unimodal and multimodal signal processing.

Result: Improved binary classification accuracy from 70% to 80% and ternary from 62% to 74% on the CL-Drive dataset.

Conclusion: UniPhyNet is an effective end-to-end solution for cognitive state monitoring, outperforming traditional feature-based approaches.

Abstract: We present UniPhyNet, a novel neural network architecture to classify
cognitive load using multimodal physiological data -- specifically EEG, ECG and
EDA signals -- without the explicit need for extracting hand-crafted features.
UniPhyNet integrates multiscale parallel convolutional blocks and ResNet-type
blocks enhanced with channel block attention module to focus on the informative
features while a bidirectional gated recurrent unit is used to capture temporal
dependencies. This architecture processes and combines signals in both unimodal
and multimodal configurations via intermediate fusion of learned feature maps.
On the CL-Drive dataset, UniPhyNet improves raw signal classification accuracy
from 70% to 80% (binary) and 62% to 74% (ternary), outperforming feature-based
models, demonstrating its effectiveness as an end-to-end solution for
real-world cognitive state monitoring.

</details>


### [580] [Automated Vigilance State Classification in Rodents Using Machine Learning and Feature Engineering](https://arxiv.org/abs/2507.14166)
*Sankalp Jajee,Gaurav Kumar,Homayoun Valafar*

Main category: eess.SP

TL;DR: An automated framework using EEG and machine learning achieves high accuracy in classifying rodent sleep states, outperforming manual methods.


<details>
  <summary>Details</summary>
Motivation: Manual sleep state classification is labor-intensive and prone to variability, limiting research throughput and reproducibility.

Method: Combines signal processing and machine learning (XGBoost) with features from time and frequency domains, including spectral power and cross-frequency coupling.

Result: Achieved 91.5% accuracy, 86.8% precision, 81.2% recall, and 83.5% F1 score, outperforming baselines.

Conclusion: The framework advances automated sleep classification, aiding sleep science and disorder interventions, with public code availability.

Abstract: Preclinical sleep research remains constrained by labor intensive, manual
vigilance state classification and inter rater variability, limiting throughput
and reproducibility. This study presents an automated framework developed by
Team Neural Prognosticators to classify electroencephalogram (EEG) recordings
of small rodents into three critical vigilance states paradoxical sleep (REM),
slow wave sleep (SWS), and wakefulness. The system integrates advanced signal
processing with machine learning, leveraging engineered features from both time
and frequency domains, including spectral power across canonical EEG bands
(delta to gamma), temporal dynamics via Maximum-Minimum Distance, and
cross-frequency coupling metrics. These features capture distinct
neurophysiological signatures such as high frequency desynchronization during
wakefulness, delta oscillations in SWS, and REM specific bursts. Validated
during the 2024 Big Data Health Science Case Competition (University of South
Carolina Big Data Health Science Center, 2024), our XGBoost model achieved
91.5% overall accuracy, 86.8% precision, 81.2% recall, and an F1 score of
83.5%, outperforming all baseline methods. Our approach represents a critical
advancement in automated sleep state classification and a valuable tool for
accelerating discoveries in sleep science and the development of targeted
interventions for chronic sleep disorders. As a publicly available code (BDHSC)
resource is set to contribute significantly to advancements.

</details>


### [581] [Attention-Based Fusion of IQ and FFT Spectrograms with AoA Features for GNSS Jammer Localization](https://arxiv.org/abs/2507.14167)
*Lucas Heublein,Christian Wielenberg,Thorsten Nowak,Tobias Feigl,Christopher Mutschler,Felix Ott*

Main category: eess.SP

TL;DR: A novel method for detecting and localizing GNSS jamming signals using an attention-based fusion framework outperforms classical AoA techniques in multipath environments.


<details>
  <summary>Details</summary>
Motivation: GNSS jamming disrupts accurate positioning, necessitating reliable detection and localization methods, especially in multipath conditions where classical AoA methods fail.

Method: Proposes an attention-based fusion framework integrating IQ samples and FFT spectrograms with 22 AoA features, evaluated using 128 vision encoder and time-series models.

Result: Demonstrates superior performance in detecting and classifying interference, and estimating jamming source distance, azimuth, and elevation.

Conclusion: The proposed method enhances localization accuracy and outperforms state-of-the-art techniques, validated by a novel dataset of moving jammers in dynamic multipath conditions.

Abstract: Jamming devices disrupt signals from the global navigation satellite system
(GNSS) and pose a significant threat by compromising the reliability of
accurate positioning. Consequently, the detection and localization of these
interference signals are essential to achieve situational awareness, mitigating
their impact, and implementing effective counter-measures. Classical Angle of
Arrival (AoA) methods exhibit reduced accuracy in multipath environments due to
signal reflections and scattering, leading to localization errors.
Additionally, AoA-based techniques demand substantial computational resources
for array signal processing. In this paper, we propose a novel approach for
detecting and classifying interference while estimating the distance, azimuth,
and elevation of jamming sources. Our benchmark study evaluates 128 vision
encoder and time-series models to identify the highest-performing methods for
each task. We introduce an attention-based fusion framework that integrates
in-phase and quadrature (IQ) samples with Fast Fourier Transform (FFT)-computed
spectrograms while incorporating 22 AoA features to enhance localization
accuracy. Furthermore, we present a novel dataset of moving jamming devices
recorded in an indoor environment with dynamic multipath conditions and
demonstrate superior performance compared to state-of-the-art methods.

</details>


### [582] [Enhancing Generalization in PPG-Based Emotion Measurement with a CNN-TCN-LSTM Model](https://arxiv.org/abs/2507.14173)
*Karim Alghoul,Hussein Al Osman,Abdulmotaleb El Saddik*

Main category: eess.SP

TL;DR: A hybrid CNN-LSTM-TCN model improves PPG-based emotion recognition, outperforming standalone models in generalization and performance.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of generalizing PPG-based emotion recognition models across individuals.

Method: Combines CNNs, LSTMs, and TCNs for feature extraction and processing, classifying valence and arousal from PPG signals.

Result: Outperforms state-of-the-art CNN and CNN-LSTM models in emotion recognition, with better generalization and higher AUC/F1 scores.

Conclusion: The hybrid architecture effectively handles subject variability, advancing PPG-based affective computing.

Abstract: Human computer interaction has become integral to modern life, driven by
advancements in machine learning technologies. Affective computing, in
particular, has focused on systems that recognize, interpret, and respond to
human emotions, often using wearable devices, which provide continuous data
streams of physiological signals. Among various physiological signals, the
photoplethysmogram (PPG) has gained prominence due to its ease of acquisition
from widely available devices. However, the generalization of PPG-based emotion
recognition models across individuals remains an unresolved challenge. This
paper introduces a novel hybrid architecture that combines Convolutional Neural
Networks (CNNs), Long Short-Term Memory networks (LSTMs), and Temporal
Convolutional Networks (TCNs) to address this issue. The proposed model
integrates the strengths of these architectures to improve robustness and
generalization. Raw PPG signals are fed into the CNN for feature extraction.
These features are processed separately by LSTM and TCN. The outputs from these
components are concatenated to generate a final feature representation, which
serves as the input for classifying valence and arousal, the primary dimensions
of emotion. Experiments using the Photoplethysmogram Dataset for Emotional
Analysis (PPGE) demonstrate that the proposed hybrid model achieves better
model generalization than standalone CNN and LSTM architectures. Our results
show that the proposed solution outperforms the state-of-the-art CNN
architecture, as well as a CNN-LSTM model, in emotion recognition tasks with
PPG signals. Using metrics such as Area Under the Curve (AUC) and F1 Score, we
highlight the model's effectiveness in handling subject variability.

</details>


### [583] [Latent Sensor Fusion: Multimedia Learning of Physiological Signals for Resource-Constrained Devices](https://arxiv.org/abs/2507.14185)
*Abdullah Ahmed,Jeremy Gummeson*

Main category: eess.SP

TL;DR: A unified encoder using latent space fusion for multimodal physiological signals is faster, lighter, and scalable without losing accuracy.


<details>
  <summary>Details</summary>
Motivation: To address computational challenges in biosignal analysis on resource-constrained devices by leveraging latent spaces for efficient data summarization.

Method: Sensor-latent fusion with autoencoder-based latent space fusion and compressed sensing.

Result: The unified encoder outperforms modality-specific alternatives in speed, lightness, and scalability while maintaining accuracy.

Conclusion: Latent space fusion enables efficient, scalable, and accurate multimodal biosignal analysis.

Abstract: Latent spaces offer an efficient and effective means of summarizing data
while implicitly preserving meta-information through relational encoding. We
leverage these meta-embeddings to develop a modality-agnostic, unified encoder.
Our method employs sensor-latent fusion to analyze and correlate multimodal
physiological signals. Using a compressed sensing approach with
autoencoder-based latent space fusion, we address the computational challenges
of biosignal analysis on resource-constrained devices. Experimental results
show that our unified encoder is significantly faster, lighter, and more
scalable than modality-specific alternatives, without compromising
representational accuracy.

</details>


### [584] [Boosted Enhanced Quantile Regression Neural Networks with Spatiotemporal Permutation Entropy for Complex System Prognostics](https://arxiv.org/abs/2507.14194)
*David J Poland*

Main category: eess.SP

TL;DR: A novel framework combining Spatiotemporal Permutation Entropy and Boosted Enhanced Quantile Regression Neural Networks (BEQRNNs) for pattern prediction and system prognostics, achieving high accuracy and robustness in complex systems.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of understanding complex dynamical patterns in multidimensional systems by integrating entropy-based complexity measures with advanced neural architectures.

Method: Uses dual computational stages: spatiotemporal entropy extraction for multiscale data, followed by BEQRNN for probabilistic pattern prediction with uncertainty quantification.

Result: Achieves 81.17% accuracy in pattern classification, 79% increase in critical transition detection, and 81.22% improvement in long-term prediction reliability.

Conclusion: The framework shows significant potential for real-time prognostic applications due to its effectiveness in processing complex, multimodal entropy features.

Abstract: This paper presents a novel framework for pattern prediction and system
prognostics centered on Spatiotemporal Permutation Entropy analysis integrated
with Boosted Enhanced Quantile Regression Neural Networks (BEQRNNs). We address
the challenge of understanding complex dynamical patterns in multidimensional
systems through an approach that combines entropy-based complexity measures
with advanced neural architectures. The system leverages dual computational
stages: first implementing spatiotemporal entropy extraction optimized for
multiscale temporal and spatial data streams, followed by an integrated BEQRNN
layer that enables probabilistic pattern prediction with uncertainty
quantification. This architecture achieves 81.17% accuracy in spatiotemporal
pattern classification with prediction horizons up to 200 time steps and
maintains robust performance across diverse regimes. Field testing across
chaotic attractors, reaction-diffusion systems, and industrial datasets shows a
79% increase in critical transition detection accuracy and 81.22% improvement
in long-term prediction reliability. The framework's effectiveness in
processing complex, multimodal entropy features demonstrates significant
potential for real-time prognostic applications.

</details>


### [585] [Distributed Machine Learning Approach for Low-Latency Localization in Cell-Free Massive MIMO Systems](https://arxiv.org/abs/2507.14216)
*Manish Kumar,Tzu-Hsuan Chou,Byunghyun Lee,Nicolò Michelusi,David J. Love,Yaguang Zhang,James V. Krogmeier*

Main category: eess.SP

TL;DR: A distributed ML framework for low-latency localization in cell-free massive MIMO systems, reducing fronthaul communication and CPU burden while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: To support real-time applications in 6G networks by enabling precise, low-latency localization without centralized processing.

Method: Each AP trains a Gaussian process regression model using local fingerprints; UE fuses estimates for final location with minimal overhead.

Result: Achieves accuracy comparable to centralized methods, reduces uncertainty, and lowers latency by eliminating fronthaul communication.

Conclusion: Distributed ML is promising for high-accuracy, low-latency localization in future 6G networks.

Abstract: Low-latency localization is critical in cellular networks to support
real-time applications requiring precise positioning. In this paper, we propose
a distributed machine learning (ML) framework for fingerprint-based
localization tailored to cell-free massive multiple-input multiple-output
(MIMO) systems, an emerging architecture for 6G networks. The proposed
framework enables each access point (AP) to independently train a Gaussian
process regression model using local angle-of-arrival and received signal
strength fingerprints. These models provide probabilistic position estimates
for the user equipment (UE), which are then fused by the UE with minimal
computational overhead to derive a final location estimate. This decentralized
approach eliminates the need for fronthaul communication between the APs and
the central processing unit (CPU), thereby reducing latency. Additionally,
distributing computational tasks across the APs alleviates the processing
burden on the CPU compared to traditional centralized localization schemes.
Simulation results demonstrate that the proposed distributed framework achieves
localization accuracy comparable to centralized methods, despite lacking the
benefits of centralized data aggregation. Moreover, it effectively reduces
uncertainty of the location estimates, as evidenced by the 95\% covariance
ellipse. The results highlight the potential of distributed ML for enabling
low-latency, high-accuracy localization in future 6G networks.

</details>


### [586] [Advanced Space Mapping Technique Integrating a Shared Coarse Model for Multistate Tuning-Driven Multiphysics Optimization of Tunable Filters](https://arxiv.org/abs/2507.14220)
*Haitian Hu,Wei Zhang,Feng Feng,Zhiguo Zhang,Qi-Jun Zhang*

Main category: eess.SP

TL;DR: An advanced space mapping (SM) technique is introduced for multiphysics optimization of tunable filters, combining EM single-physics efficiency with multiphysics precision using shared coarse models and neural networks.


<details>
  <summary>Details</summary>
Motivation: To improve multiphysics modeling accuracy and reduce computational costs for tunable filters by leveraging shared EM-based coarse models and neural networks.

Method: Uses a shared EM coarse model and mapping neural networks to transition from single-physics to multiphysics responses, optimizing multiple tuning states simultaneously.

Result: Achieves superior multiphysics modeling accuracy with fewer training samples and lower computational costs compared to direct multiphysics techniques.

Conclusion: The proposed SM technique effectively optimizes tunable filters by balancing efficiency and precision, addressing multiple tuning states concurrently.

Abstract: This article introduces an advanced space mapping (SM) technique that applies
a shared electromagnetic (EM)-based coarse model for multistate tuning-driven
multiphysics optimization of tunable filters. The SM method combines the
computational efficiency of EM single-physics simulations with the precision of
multiphysics simulations. The shared coarse model is based on EM single-physics
responses corresponding to various nontunable design parameters values.
Conversely, the fine model is implemented to delineate the behavior of
multiphysics responses concerning both nontunable and tunable design parameter
values. The proposed overall surrogate model comprises multiple subsurrogate
models, each consisting of one shared coarse model and two distinct mapping
neural networks. The responses from the shared coarse model in the EM
single-physics filed offer a suitable approximation for the fine responses in
the multiphysics filed, whereas the mapping neural networks facilitate
transition from the EM single-physics field to the multiphysics field. Each
subsurrogate model maintains consistent nontunable design parameter values but
possesses unique tunable design parameter values. By developing multiple
subsurrogate models, optimization can be simultaneously performed for each
tuning state. Nontunable design parameter values are constrained by all tuning
states, whereas tunable design parameter values are confined to their
respective tuning states. This optimization technique simultaneously accounts
for all the tuning states to fulfill the necessary multiple tuning state
requirements. Multiple EM and multiphysics training samples are generated
concurrently to develop the surrogate model. Compared with existing direct
multiphysics parameterized modeling techniques, our proposed method achieves
superior multiphysics modeling accuracy with fewer training samples and reduced
computational costs.

</details>


### [587] [Diffusion-based translation between unpaired spontaneous premature neonatal EEG and fetal MEG](https://arxiv.org/abs/2507.14224)
*Benoît Brebion,Alban Gallard,Katrin Sippel,Amer Zaylaa,Hubert Preissl,Sahar Moghimi,Fabrice Wallois,Yaël Frégier*

Main category: eess.SP

TL;DR: The paper introduces an AI-based method to translate EEG knowledge to fMEG for studying prenatal brain development, achieving better results than GANs.


<details>
  <summary>Details</summary>
Motivation: Traditional EEG studies lack insights into fetal brain activity, and fMEG data is scarce and low-quality. The goal is to bridge this gap using AI.

Method: An unpaired diffusion translation method with dual diffusion bridges was developed, trained on EEG and fMEG datasets.

Result: The method outperforms GANs, reducing mean squared error by 5% and eliminating mode collapse, achieving high signal fidelity.

Conclusion: The tool advances EEG-fMEG translation and has potential for broader unpaired signal applications.

Abstract: Background and objective: Brain activity in premature newborns has
traditionally been studied using electroencephalography (EEG), leading to
substantial advances in our understanding of early neural development. However,
since brain development takes root at the fetal stage, a critical window of
this process remains largely unknown. The only technique capable of recording
neural activity in the intrauterine environment is fetal magnetoencephalography
(fMEG), but this approach presents challenges in terms of data quality and
scarcity. Using artificial intelligence, the present research aims to transfer
the well-established knowledge from EEG studies to fMEG to improve
understanding of prenatal brain development, laying the foundations for better
detection and treatment of potential pathologies. Methods: We developed an
unpaired diffusion translation method based on dual diffusion bridges, which
notably includes numerical integration improvements to obtain more qualitative
results at a lower computational cost. Models were trained on our unpaired
dataset of bursts of spontaneous activity from 30 high-resolution premature
newborns EEG recordings and 44 fMEG recordings. Results: We demonstrate that
our method achieves significant improvement upon previous results obtained with
Generative Adversarial Networks (GANs), by almost 5% on the mean squared error
in the time domain, and completely eliminating the mode collapse problem in the
frequency domain, thus achieving near-perfect signal fidelity. Conclusion: We
set a new state of the art in the EEG-fMEG unpaired translation problem, as our
developed tool completely paves the way for early brain activity analysis.
Overall, we also believe that our method could be reused for other unpaired
signal translation applications.

</details>


### [588] [Graph Attention Networks for Detecting Epilepsy from EEG Signals Using Accessible Hardware in Low-Resource Settings](https://arxiv.org/abs/2507.15118)
*Szymon Mazurek,Stephen Moore,Alessandro Crimi*

Main category: eess.SP

TL;DR: A graph-based deep learning framework using EEG signals for epilepsy detection in low-income regions, focusing on fairness, accessibility, and explainability.


<details>
  <summary>Details</summary>
Motivation: Epilepsy is under-diagnosed in low-income areas due to limited neurologists and expensive tools. The goal is to provide affordable, fair, and explainable diagnostic support.

Method: EEG signals are modeled as spatio-temporal graphs, classified using graph attention networks (GAT), with adaptations for edge analysis. Preprocessing for low-fidelity recordings and a lightweight GAT for deployment on RaspberryPi.

Result: Outperforms random forest and graph convolutional networks in accuracy and robustness, identifying fronto-temporal connections as biomarkers.

Conclusion: GATs offer scalable, insightful diagnostic support for epilepsy in underserved regions, enabling affordable neurodiagnostic tools.

Abstract: Goal: Epilepsy remains under-diagnosed in low-income countries due to scarce
neurologists and costly diagnostic tools. We propose a graph-based deep
learning framework to detect epilepsy from low-cost Electroencephalography
(EEG) hardware, tested on recordings from Nigeria and Guinea-Bissau. Our focus
is on fair, accessible automatic assessment and explainability to shed light on
epilepsy biomarkers. Methods: We model EEG signals as spatio-temporal graphs,
classify them, and identify interchannel relationships and temporal dynamics
using graph attention networks (GAT). To emphasize connectivity biomarkers, we
adapt the inherently node-focused GAT to analyze edges. We also designed signal
preprocessing for low-fidelity recordings and a lightweight GAT architecture
trained on Google Colab and deployed on RaspberryPi devices. Results: The
approach achieves promising classification performance, outperforming a
standard classifier based on random forest and graph convolutional networks in
terms of accuracy and robustness over multiple sessions, but also highlighting
specific connections in the fronto-temporal region. Conclusions: The results
highlight the potential of GATs to provide insightful and scalable diagnostic
support for epilepsy in underserved regions, paving the way for affordable and
accessible neurodiagnostic tools.

</details>


### [589] [MEETI: A Multimodal ECG Dataset from MIMIC-IV-ECG with Signals, Images, Features and Interpretations](https://arxiv.org/abs/2507.15255)
*Deyun Zhang,Xiang Lan,Shijia Geng,Qinghao Zhao,Sumei Fan,Mengling Feng,Shenda Hong*

Main category: eess.SP

TL;DR: MEETI is a new multimodal ECG dataset integrating raw signals, images, and text to advance AI in cardiovascular care.


<details>
  <summary>Details</summary>
Motivation: Existing ECG datasets lack multimodal data, hindering AI system development for real-world clinical use.

Method: MEETI synchronizes raw ECG waveforms, plotted images, extracted parameters, and LLM-generated text interpretations.

Result: MEETI provides a unified, large-scale dataset supporting multimodal learning and interpretable AI for ECG analysis.

Conclusion: MEETI bridges gaps in ECG AI research, enabling explainable, multimodal cardiovascular AI systems.

Abstract: Electrocardiogram (ECG) plays a foundational role in modern cardiovascular
care, enabling non-invasive diagnosis of arrhythmias, myocardial ischemia, and
conduction disorders. While machine learning has achieved expert-level
performance in ECG interpretation, the development of clinically deployable
multimodal AI systems remains constrained, primarily due to the lack of
publicly available datasets that simultaneously incorporate raw signals,
diagnostic images, and interpretation text. Most existing ECG datasets provide
only single-modality data or, at most, dual modalities, making it difficult to
build models that can understand and integrate diverse ECG information in
real-world settings. To address this gap, we introduce MEETI (MIMIC-IV-Ext
ECG-Text-Image), the first large-scale ECG dataset that synchronizes raw
waveform data, high-resolution plotted images, and detailed textual
interpretations generated by large language models. In addition, MEETI includes
beat-level quantitative ECG parameters extracted from each lead, offering
structured parameters that support fine-grained analysis and model
interpretability. Each MEETI record is aligned across four components: (1) the
raw ECG waveform, (2) the corresponding plotted image, (3) extracted feature
parameters, and (4) detailed interpretation text. This alignment is achieved
using consistent, unique identifiers. This unified structure supports
transformer-based multimodal learning and supports fine-grained, interpretable
reasoning about cardiac health. By bridging the gap between traditional signal
analysis, image-based interpretation, and language-driven understanding, MEETI
established a robust foundation for the next generation of explainable,
multimodal cardiovascular AI. It offers the research community a comprehensive
benchmark for developing and evaluating ECG-based AI systems.

</details>


### [590] [Optimal Transceiver Design in Over-the-Air Federated Distillation](https://arxiv.org/abs/2507.15256)
*Zihao Hu,Jia Yan,Ying-Jun Angela Zhang,Jun Zhang,Khaled B. Letaief*

Main category: eess.SP

TL;DR: Proposes an over-the-air federated distillation (FD) framework to reduce communication overhead in federated learning (FL) by sharing model outputs instead of parameters, optimizing transceiver design for faster convergence.


<details>
  <summary>Details</summary>
Motivation: Existing FL approaches are inefficient for large AI models due to high communication overhead.

Method: Introduces over-the-air FD, leveraging knowledge distillation and FL, optimizing transceiver power and beamforming for convergence.

Result: Achieves significant communication overhead reduction with minor accuracy loss compared to FL benchmarks.

Conclusion: The proposed FD framework is efficient and scalable for large AI models in FL settings.

Abstract: The rapid proliferation and growth of artificial intelligence (AI) has led to
the development of federated learning (FL). FL allows wireless devices (WDs) to
cooperatively learn by sharing only local model parameters, without needing to
share the entire dataset. However, the emergence of large AI models has made
existing FL approaches inefficient, due to the significant communication
overhead required. In this paper, we propose a novel over-the-air federated
distillation (FD) framework by synergizing the strength of FL and knowledge
distillation to avoid the heavy local model transmission. Instead of sharing
the model parameters, only the WDs' model outputs, referred to as knowledge,
are shared and aggregated over-the-air by exploiting the superposition property
of the multiple-access channel. We shall study the transceiver design in
over-the-air FD, aiming to maximize the learning convergence rate while meeting
the power constraints of the transceivers. The main challenge lies in the
intractability of the learning performance analysis, as well as the non-convex
nature and the optimization spanning the whole FD training period. To tackle
this problem, we first derive an analytical expression of the convergence rate
in over-the-air FD. Then, the closed-form optimal solutions of the WDs'
transmit power and the estimator for over-the-air aggregation are obtained
given the receiver combining strategy. Accordingly, we put forth an efficient
approach to find the optimal receiver beamforming vector via semidefinite
relaxation. We further prove that there is no optimality gap between the
original and relaxed problem for the receiver beamforming design. Numerical
results will show that the proposed over-the-air FD approach achieves a
significant reduction in communication overhead, with only a minor compromise
in testing accuracy compared to conventional FL benchmarks.

</details>


### [591] [EEG-based Epileptic Prediction via a Two-stage Channel-aware Set Transformer Network](https://arxiv.org/abs/2507.15364)
*Ruifeng Zheng,Cong Chen,Shuang Wang,Yiming Liu,Lin You,Jindong Lu,Ruizhe Zhu,Guodao Zhang,Kejie Huang*

Main category: eess.SP

TL;DR: A novel two-stage channel-aware Set Transformer Network improves seizure prediction using fewer EEG channels, achieving higher sensitivity and reduced false prediction rates.


<details>
  <summary>Details</summary>
Motivation: To address the limitations of bulky EEG devices and improve seizure prediction accuracy with fewer sensors.

Method: Proposed a two-stage channel-aware Set Transformer Network and tested a seizure-independent division method on the CHB-MIT dataset.

Result: Reduced average channels from 18 to 2.8, increased sensitivity to 80.1%, and maintained low FPR (0.11/hour).

Conclusion: The method is effective for seizure prediction with fewer channels, and a rigorous seizure-independent division is recommended for abundant EEG data.

Abstract: Epilepsy is a chronic, noncommunicable brain disorder, and sudden seizure
onsets can significantly impact patients' quality of life and health. However,
wearable seizure-predicting devices are still limited, partly due to the bulky
size of EEG-collecting devices. To relieve the problem, we proposed a novel
two-stage channel-aware Set Transformer Network that could perform seizure
prediction with fewer EEG channel sensors. We also tested a seizure-independent
division method which could prevent the adjacency of training and test data.
Experiments were performed on the CHB-MIT dataset which includes 22 patients
with 88 merged seizures. The mean sensitivity before channel selection was
76.4% with a false predicting rate (FPR) of 0.09/hour. After channel selection,
dominant channels emerged in 20 out of 22 patients; the average number of
channels was reduced to 2.8 from 18; and the mean sensitivity rose to 80.1%
with an FPR of 0.11/hour. Furthermore, experimental results on the
seizure-independent division supported our assertion that a more rigorous
seizure-independent division should be used for patients with abundant EEG
recordings.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [592] [Identifying Algorithmic and Domain-Specific Bias in Parliamentary Debate Summarisation](https://arxiv.org/abs/2507.14221)
*Eoghan Cunningham,James Cross,Derek Greene*

Main category: cs.CY

TL;DR: The paper explores using LLMs to summarize European Parliament debates, identifying biases in representation and proposing a multi-stage framework to improve fairness and accuracy.


<details>
  <summary>Details</summary>
Motivation: To make legislative discourse more accessible while ensuring equitable representation of all speakers in summaries.

Method: A structured, multi-stage summarization framework is proposed, tested with proprietary and open-weight LLMs, and evaluated for biases.

Result: Evidence of positional and partisan biases was found, with hierarchical summarization reducing disparities.

Conclusion: Domain-sensitive metrics and ethical oversight are needed for deploying LLMs in democratic applications.

Abstract: The automated summarisation of parliamentary debates using large language
models (LLMs) offers a promising way to make complex legislative discourse more
accessible to the public. However, such summaries must not only be accurate and
concise but also equitably represent the views and contributions of all
speakers. This paper explores the use of LLMs to summarise plenary debates from
the European Parliament and investigates the algorithmic and representational
biases that emerge in this context. We propose a structured, multi-stage
summarisation framework that improves textual coherence and content fidelity,
while enabling the systematic analysis of how speaker attributes -- such as
speaking order or political affiliation -- influence the visibility and
accuracy of their contributions in the final summaries. Through our experiments
using both proprietary and open-weight LLMs, we find evidence of consistent
positional and partisan biases, with certain speakers systematically
under-represented or misattributed. Our analysis shows that these biases vary
by model and summarisation strategy, with hierarchical approaches offering the
greatest potential to reduce disparity. These findings underscore the need for
domain-sensitive evaluation metrics and ethical oversight in the deployment of
LLMs for democratic applications.

</details>


### [593] [Cognitive Castes: Artificial Intelligence, Epistemic Stratification, and the Dissolution of Democratic Discourse](https://arxiv.org/abs/2507.14218)
*Craig S Wright*

Main category: cs.CY

TL;DR: AI exacerbates cognitive inequality, reinforcing informational hierarchies in democratic societies by favoring those with advanced reasoning skills while pacifying others through simplified interfaces.


<details>
  <summary>Details</summary>
Motivation: To highlight how AI systems deepen cognitive stratification and undermine democratic deliberation by privileging certain reasoning capacities over others.

Method: The paper synthesizes formal epistemology, political theory, algorithmic design, and economic incentives to analyze AI's impact on cognitive and epistemic inequality.

Result: AI entrenches technocratic power by favoring those who can manipulate epistemic systems, eroding interpretive agency and deliberative democracy.

Conclusion: The solution lies in rebuilding rational autonomy through education, epistemic rights, and open cognitive infrastructure, rather than technocratic regulation or universal access.

Abstract: Artificial intelligence functions not as an epistemic leveller, but as an
accelerant of cognitive stratification, entrenching and formalising
informational castes within liberal-democratic societies. Synthesising formal
epistemology, political theory, algorithmic architecture, and economic
incentive structures, the argument traces how contemporary AI systems
selectively amplify the reasoning capacity of individuals equipped with
recursive abstraction, symbolic logic, and adversarial interrogation, whilst
simultaneously pacifying the cognitively untrained through engagement-optimised
interfaces. Fluency replaces rigour, immediacy displaces reflection, and
procedural reasoning is eclipsed by reactive suggestion. The result is a
technocratic realignment of power: no longer grounded in material capital
alone, but in the capacity to navigate, deconstruct, and manipulate systems of
epistemic production. Information ceases to be a commons; it becomes the
substrate through which consent is manufactured and autonomy subdued.
Deliberative democracy collapses not through censorship, but through the
erosion of interpretive agency. The proposed response is not technocratic
regulation, nor universal access, but the reconstruction of rational autonomy
as a civic mandate, codified in education, protected by epistemic rights, and
structurally embedded within open cognitive infrastructure.

</details>


### [594] [Culling Misinformation from Gen AI: Toward Ethical Curation and Refinement](https://arxiv.org/abs/2507.14242)
*Prerana Khatiwada,Grace Donaher,Jasymyn Navarro,Lokesh Bhatta*

Main category: cs.CY

TL;DR: The paper discusses the impact of AI, particularly ChatGPT and deepfakes, highlighting benefits like automation and idea expansion but also risks such as equity concerns and misinformation. It calls for collaboration to mitigate harm and proposes policy guidelines.


<details>
  <summary>Details</summary>
Motivation: To address the growing influence of AI tools like ChatGPT and deepfakes, focusing on their potential harms, including equity issues and misinformation, and advocating for proactive measures.

Method: Analysis of academic sources to examine causes and impacts of AI-related risks in various fields (healthcare, education, etc.), followed by proposing guidelines and policies.

Result: Identified risks of AI tools, emphasizing equity and misinformation, and suggested collaborative solutions involving stakeholders.

Conclusion: Proposes future-facing policies and guidelines to balance AI innovation with harm reduction, urging responsibility from users, developers, and governments.

Abstract: While Artificial Intelligence (AI) is not a new field, recent developments,
especially with the release of generative tools like ChatGPT, have brought it
to the forefront of the minds of industry workers and academic folk alike.
There is currently much talk about AI and its ability to reshape many everyday
processes as we know them through automation. It also allows users to expand
their ideas by suggesting things they may not have thought of on their own and
provides easier access to information. However, not all of the changes this
technology will bring or has brought so far are positive; this is why it is
extremely important for all modern people to recognize and understand the risks
before using these tools and allowing them to cause harm. This work takes a
position on better understanding many equity concerns and the spread of
misinformation that result from new AI, in this case, specifically ChatGPT and
deepfakes, and encouraging collaboration with law enforcement, developers, and
users to reduce harm. Considering many academic sources, it warns against these
issues, analyzing their cause and impact in fields including healthcare,
education, science, academia, retail, and finance. Lastly, we propose a set of
future-facing guidelines and policy considerations to solve these issues while
still enabling innovation in these fields, this responsibility falling upon
users, developers, and government entities.

</details>


### [595] [Bridging MOOCs, Smart Teaching, and AI: A Decade of Evolution Toward a Unified Pedagogy](https://arxiv.org/abs/2507.14266)
*Bo Yuan,Jiazi Hu*

Main category: cs.CY

TL;DR: The paper explores three educational paradigms (MOOCs, Smart Teaching, AI-enhanced learning), proposes a unified framework combining their strengths, and demonstrates its feasibility through a project-based course design.


<details>
  <summary>Details</summary>
Motivation: To address the isolated implementation of MOOCs, Smart Teaching, and AI in education and synthesize their complementary benefits for a more effective learning experience.

Method: Examines the origins, strengths, and limitations of each paradigm, then proposes a three-layer instructional framework integrating scalability (MOOCs), responsiveness (Smart Teaching), and adaptivity (AI). A project-based course design is used to demonstrate feasibility.

Result: The framework shows potential to enhance learner engagement, support instructors, and enable personalized yet scalable learning.

Conclusion: A unified pedagogical approach combining MOOCs, Smart Teaching, and AI can address the limitations of isolated implementations and improve education outcomes.

Abstract: Over the past decade, higher education has evolved through three distinct
paradigms: the emergence of Massive Open Online Courses (MOOCs), the
integration of Smart Teaching technologies into classrooms, and the rise of
AI-enhanced learning. Each paradigm is intended to address specific challenges
in traditional education: MOOCs enable ubiquitous access to learning resources;
Smart Teaching supports real-time interaction with data-driven insights; and
generative AI offers personalized feedback and on-demand content generation.
However, these paradigms are often implemented in isolation due to their
disparate technological origins and policy-driven adoption. This paper examines
the origins, strengths, and limitations of each paradigm, and advocates a
unified pedagogical perspective that synthesizes their complementary
affordances. We propose a three-layer instructional framework that combines the
scalability of MOOCs, the responsiveness of Smart Teaching, and the adaptivity
of AI. To demonstrate its feasibility, we present a curriculum design for a
project-based course. The findings highlight the framework's potential to
enhance learner engagement, support instructors, and enable personalized yet
scalable learning.

</details>


### [596] [Fiduciary AI for the Future of Brain-Technology Interactions](https://arxiv.org/abs/2507.14339)
*Abhishek Bhattacharjee,Jack Pilkington,Nita Farahany*

Main category: cs.CY

TL;DR: The paper proposes embedding fiduciary duties into brain foundation models to address risks like exploitation and cognitive liberty erosion, ensuring they act in users' best interests.


<details>
  <summary>Details</summary>
Motivation: Brain foundation models offer transformative applications but pose unprecedented risks, such as exploitation of neural signals and power asymmetries.

Method: The paper suggests integrating fiduciary duties (loyalty, care, confidentiality) into BCI-integrated models using legal traditions and AI alignment techniques.

Result: Proposed architectural and governance mechanisms aim to ensure systems act in users' best interests.

Conclusion: Embedding fiduciary duties is essential to harness the potential of brain foundation models without compromising self-determination.

Abstract: Brain foundation models represent a new frontier in AI: instead of processing
text or images, these models interpret real-time neural signals from EEG, fMRI,
and other neurotechnologies. When integrated with brain-computer interfaces
(BCIs), they may enable transformative applications-from thought controlled
devices to neuroprosthetics-by interpreting and acting on brain activity in
milliseconds. However, these same systems pose unprecedented risks, including
the exploitation of subconscious neural signals and the erosion of cognitive
liberty. Users cannot easily observe or control how their brain signals are
interpreted, creating power asymmetries that are vulnerable to manipulation.
This paper proposes embedding fiduciary duties-loyalty, care, and
confidentiality-directly into BCI-integrated brain foundation models through
technical design. Drawing on legal traditions and recent advancements in AI
alignment techniques, we outline implementable architectural and governance
mechanisms to ensure these systems act in users' best interests. Placing brain
foundation models on a fiduciary footing is essential to realizing their
potential without compromising self-determination.

</details>


### [597] [Mining Voter Behaviour and Confidence: A Rule-Based Analysis of the 2022 U.S. Elections](https://arxiv.org/abs/2507.14236)
*Md Al Jubair,Mohammad Shamsul Arefin,Ahmed Wasif Reza*

Main category: cs.CY

TL;DR: The study links voter trust to election experiences using rule-based data mining on 2022 SPAE data, revealing strong ties between demographics, voting challenges, and trust.


<details>
  <summary>Details</summary>
Motivation: To understand how voter trust is influenced by election experiences, especially among marginalized groups.

Method: Applied the Apriori algorithm with specific parameters (support ≥ 3%, confidence ≥ 60%, lift > 1.5) to analyze SPAE data. Adjusted support to 2% for minority voter analysis.

Result: Found strong associations: easy polling access and moderate confidence linked to higher trust (lift = 6.12). Minority voters with easy access had smooth registration (98.16%). High confidence in vote-counting doubled Democratic Party support likelihood.

Conclusion: Improving voting access and targeted support can enhance trust in elections, particularly for marginalized communities.

Abstract: This study explores the relationship between voter trust and their
experiences during elections by applying a rule-based data mining technique to
the 2022 Survey of the Performance of American Elections (SPAE). Using the
Apriori algorithm and setting parameters to capture meaningful associations
(support >= 3%, confidence >= 60%, and lift > 1.5), the analysis revealed a
strong connection between demographic attributes and voting-related challenges,
such as registration hurdles, accessibility issues, and queue times. For
instance, respondents who indicated that accessing polling stations was "very
easy" and who reported moderate confidence were found to be over six times more
likely (lift = 6.12) to trust their county's election outcome and experience no
registration issues. A further analysis, which adjusted the support threshold
to 2%, specifically examined patterns among minority voters. It revealed that
98.16 percent of Black voters who reported easy access to polling locations
also had smooth registration experiences. Additionally, those who had high
confidence in the vote-counting process were almost two times as likely to
identify as Democratic Party supporters. These findings point to the important
role that enhancing voting access and offering targeted support can play in
building trust in the electoral system, particularly among marginalized
communities.

</details>


### [598] [Unequal Voices: How LLMs Construct Constrained Queer Narratives](https://arxiv.org/abs/2507.15585)
*Atreya Ghosal,Ashim Gupta,Vivek Srikumar*

Main category: cs.CY

TL;DR: The paper examines how LLMs limit portrayals of queer people, focusing on harmful stereotypes, narrow topics, and discursive othering.


<details>
  <summary>Details</summary>
Motivation: To highlight the marginalization of queer narratives in LLM outputs compared to the broader representation of default groups.

Method: Analyzed LLM generations for harmful representations, narrow topics, and discursive othering, testing specific hypotheses.

Result: LLMs significantly restrict the complexity of queer personas, reinforcing stereotypes.

Conclusion: LLMs perpetuate constrained and harmful representations of queer identities, calling for more inclusive model training.

Abstract: One way social groups are marginalized in discourse is that the narratives
told about them often default to a narrow, stereotyped range of topics. In
contrast, default groups are allowed the full complexity of human existence. We
describe the constrained representations of queer people in LLM generations in
terms of harmful representations, narrow representations, and discursive
othering and formulate hypotheses to test for these phenomena. Our results show
that LLMs are significantly limited in their portrayals of queer personas.

</details>


### [599] [Why can't Epidemiology be automated (yet)?](https://arxiv.org/abs/2507.15617)
*David Bann,Ed Lowther,Liam Wright,Yevgeniya Kovalchuk*

Main category: cs.CY

TL;DR: AI, especially generative AI, can automate tasks in epidemiology, but its effectiveness varies due to model limitations and human system barriers.


<details>
  <summary>Details</summary>
Motivation: To explore how AI can enhance epidemiological research by identifying tasks where AI tools can improve efficiency.

Method: Mapping epidemiological tasks (e.g., literature review, data analysis) and evaluating AI's role in each, using examples like AI-generated papers.

Result: AI boosts productivity in coding and admin tasks but faces challenges like hallucinations in literature reviews and data access barriers.

Conclusion: Collaboration between epidemiologists and engineers is key to leveraging AI's potential in epidemiology.

Abstract: Recent advances in artificial intelligence (AI) - particularly generative AI
- present new opportunities to accelerate, or even automate, epidemiological
research. Unlike disciplines based on physical experimentation, a sizable
fraction of Epidemiology relies on secondary data analysis and thus is
well-suited for such augmentation. Yet, it remains unclear which specific tasks
can benefit from AI interventions or where roadblocks exist. Awareness of
current AI capabilities is also mixed. Here, we map the landscape of
epidemiological tasks using existing datasets - from literature review to data
access, analysis, writing up, and dissemination - and identify where existing
AI tools offer efficiency gains. While AI can increase productivity in some
areas such as coding and administrative tasks, its utility is constrained by
limitations of existing AI models (e.g. hallucinations in literature reviews)
and human systems (e.g. barriers to accessing datasets). Through examples of
AI-generated epidemiological outputs, including fully AI-generated papers, we
demonstrate that recently developed agentic systems can now design and execute
epidemiological analysis, albeit to varied quality (see
https://github.com/edlowther/automated-epidemiology). Epidemiologists have new
opportunities to empirically test and benchmark AI systems; realising the
potential of AI will require two-way engagement between epidemiologists and
engineers.

</details>


### [600] [Left Leaning Models: AI Assumptions on Economic Policy](https://arxiv.org/abs/2507.15771)
*Maxim Chupilkin*

Main category: cs.CY

TL;DR: LLMs prioritize unemployment, inequality, financial stability, and environmental harm over traditional macroeconomic concerns like growth, inflation, and debt.


<details>
  <summary>Details</summary>
Motivation: To understand the implicit assumptions of LLMs in evaluating economic policy, given their growing use in economics.

Method: Conjoint experiment to identify key factors influencing LLMs' policy evaluations.

Result: LLMs consistently prioritize social and environmental factors over traditional macroeconomic indicators.

Conclusion: LLMs' policy evaluations reveal a distinct bias toward social and environmental concerns, differing from conventional economic priorities.

Abstract: How does AI think about economic policy? While the use of large language
models (LLMs) in economics is growing exponentially, their assumptions on
economic issues remain a black box. This paper uses a conjoint experiment to
tease out the main factors influencing LLMs' evaluation of economic policy. It
finds that LLMs are most sensitive to unemployment, inequality, financial
stability, and environmental harm and less sensitive to traditional
macroeconomic concerns such as economic growth, inflation, and government debt.
The results are remarkably consistent across scenarios and across models.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [601] [Design of an Edge-based Portable EHR System for Anemia Screening in Remote Health Applications](https://arxiv.org/abs/2507.15146)
*Sebastian A. Cruz Romero,Misael J. Mercado Hernandez,Samir Y. Ali Rivera,Jorge A. Santiago Fernandez,Wilfredo E. Lugo Beauchamp*

Main category: cs.ET

TL;DR: A portable, edge-enabled EHR platform for offline-first operation in resource-limited settings, featuring secure data management and modular diagnostics, demonstrated with an anemia screening module.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges like poor interoperability, lack of offline support, and high infrastructure costs in digital health solutions for underserved regions.

Method: Developed a platform with AES-256 encrypted local storage, optional cloud sync, and integrated a Random Forest model for anemia screening using fingernail pallor analysis. Optimized with a quantized YOLOv8n-based detector.

Result: Achieved test RMSE of 1.969 g/dL and MAE of 1.490 g/dL for anemia screening, with 79.2% sensitivity. Reduced inference latency from 46.96 ms to 21.50 ms while maintaining accuracy.

Conclusion: The system offers a scalable, low-cost, and privacy-compliant solution to enhance portable health information systems in disconnected settings.

Abstract: The design of medical systems for remote, resource-limited environments faces
persistent challenges due to poor interoperability, lack of offline support,
and dependency on costly infrastructure. Many existing digital health solutions
neglect these constraints, limiting their effectiveness for frontline health
workers in underserved regions. This paper presents a portable, edge-enabled
Electronic Health Record platform optimized for offline-first operation, secure
patient data management, and modular diagnostic integration. Running on
small-form factor embedded devices, it provides AES-256 encrypted local storage
with optional cloud synchronization for interoperability. As a use case, we
integrated a non-invasive anemia screening module leveraging fingernail pallor
analysis. Trained on 250 patient cases (27\% anemia prevalence) with
KDE-balanced data, the Random Forest model achieved a test RMSE of 1.969 g/dL
and MAE of 1.490 g/dL. A severity-based model reached 79.2\% sensitivity. To
optimize performance, a YOLOv8n-based nail bed detector was quantized to INT8,
reducing inference latency from 46.96 ms to 21.50 ms while maintaining mAP@0.5
at 0.995. The system emphasizes low-cost deployment, modularity, and data
privacy compliance (HIPAA/GDPR), addressing critical barriers to digital health
adoption in disconnected settings. Our work demonstrates a scalable approach to
enhance portable health information systems and support frontline healthcare in
underserved regions.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [602] [A Formal Model of the Economic Impacts of AI Openness Regulation](https://arxiv.org/abs/2507.14193)
*Tori Qiu,Benjamin Laufer,Jon Kleinberg,Hoda Heidari*

Main category: cs.GT

TL;DR: The paper analyzes strategic interactions between AI model creators and fine-tuning entities under regulatory openness requirements, evaluating how different definitions of 'open-source' impact market equilibria and incentives.


<details>
  <summary>Details</summary>
Motivation: To address the ambiguity in defining open-source foundation models under regulatory frameworks like the EU AI Act and understand their economic implications.

Method: A stylized model of regulator choices and strategic interactions between generalists (model creators) and specialists (fine-tuning entities) is developed.

Result: Market equilibria under various openness regulations are characterized, showing how baseline model performance influences the effectiveness of penalties vs. open-source thresholds.

Conclusion: The study provides a theoretical basis for AI governance decisions on openness, aiding in the refinement of practical open-source policies.

Abstract: Regulatory frameworks, such as the EU AI Act, encourage openness of
general-purpose AI models by offering legal exemptions for "open-source"
models. Despite this legislative attention on openness, the definition of
open-source foundation models remains ambiguous. This paper models the
strategic interactions among the creator of a general-purpose model (the
generalist) and the entity that fine-tunes the general-purpose model to a
specialized domain or task (the specialist), in response to regulatory
requirements on model openness. We present a stylized model of the regulator's
choice of an open-source definition to evaluate which AI openness standards
will establish appropriate economic incentives for developers. Our results
characterize market equilibria -- specifically, upstream model release
decisions and downstream fine-tuning efforts -- under various openness
regulations and present a range of effective regulatory penalties and
open-source thresholds. Overall, we find the model's baseline performance
determines when increasing the regulatory penalty vs. the open-source threshold
will significantly alter the generalist's release strategy. Our model provides
a theoretical foundation for AI governance decisions around openness and
enables evaluation and refinement of practical open-source policies.

</details>


### [603] [Strategyproofness and Monotone Allocation of Auction in Social Networks](https://arxiv.org/abs/2507.14472)
*Yuhang Guo,Dong Hao,Bin Li,Mingyu Xiao,Bakh Khoussainov*

Main category: cs.GT

TL;DR: The paper addresses the lack of a general principle for strategyproof allocation rules in network auctions, introduces two monotonicity categories (ID-MON and IP-MON), and resolves challenges in combinatorial network auctions.


<details>
  <summary>Details</summary>
Motivation: To overcome the absence of a foundational principle for strategyproof allocation rules in network auctions, unlike canonical auctions where Myerson's Lemma applies.

Method: Introduces two monotone allocation rules (ID-MON and IP-MON) and characterizes strategyproof payment rules, including revenue-maximizing ones.

Result: Shows that existing pioneering researches fail to be strategyproof and resolves obstacles in combinatorial network auctions with single-minded bidders.

Conclusion: The paper provides a framework for strategyproof network auctions by introducing ID-MON and IP-MON, enabling feasible and revenue-maximizing solutions.

Abstract: Strategyproofness in network auctions requires that bidders not only report
their valuations truthfully, but also do their best to invite neighbours from
the social network. In contrast to canonical auctions, where the value-monotone
allocation in Myerson's Lemma is a cornerstone, a general principle of
allocation rules for strategyproof network auctions is still missing. We show
that, due to the absence of such a principle, even extensions to multi-unit
network auctions with single-unit demand present unexpected difficulties, and
all pioneering researches fail to be strategyproof. For the first time in this
field, we identify two categories of monotone allocation rules on networks:
Invitation-Depressed Monotonicity (ID-MON) and Invitation-Promoted Monotonicity
(IP-MON). They encompass all existing allocation rules of network auctions as
specific instances. For any given ID-MON or IP-MON allocation rule, we
characterize the existence and sufficient conditions for the strategyproof
payment rules, and show that among all such payment rules, the
revenue-maximizing one exists and is computationally feasible. With these
results, the obstacle of combinatorial network auction with single-minded
bidders is now resolved.

</details>


### [604] [Probing EFX via PMMS: (Non-)Existence Results in Discrete Fair Division](https://arxiv.org/abs/2507.14957)
*Jarosław Byrka,Franciszek Malinka,Tomasz Ponitka*

Main category: cs.GT

TL;DR: The paper explores fair division of indivisible items, focusing on EFX and PMMS problems. It shows a formal separation between EFX and PMMS, and proves existence of fair allocations for three special cases: personalized bivalued valuations, binary-valued MMS-feasible valuations, and pair-demand valuations. Constructive proofs and polynomial-time algorithms are provided.


<details>
  <summary>Details</summary>
Motivation: To address the central open question in fair division (EFX problem) and its stronger variant (PMMS), providing new insights and solutions for specific valuation cases.

Method: Constructs a three-agent instance to separate EFX and PMMS, and proves existence of fair allocations for three special cases with constructive proofs and polynomial-time algorithms.

Result: Established a formal separation between EFX and PMMS. Proved existence of fair allocations for personalized bivalued, binary-valued MMS-feasible, and pair-demand valuations.

Conclusion: The paper advances understanding of fair division by resolving key questions and providing practical algorithms for specific valuation scenarios.

Abstract: We study the fair division of indivisible items and provide new insights into
the EFX problem, which is widely regarded as the central open question in fair
division, and the PMMS problem, a strictly stronger variant of EFX. Our first
result constructs a three-agent instance with two monotone valuations and one
additive valuation in which no PMMS allocation exists. Since EFX allocations
are known to exist under these assumptions, this establishes a formal
separation between EFX and PMMS.
  We prove existence of fair allocations for three important special cases. We
show that EFX allocations exist for personalized bivalued valuations, where for
each agent $i$ there exist values $a_i > b_i$ such that agent $i$ assigns value
$v_i(\{g\}) \in \{a_i, b_i\}$ to each good $g$. We establish an analogous
existence result for PMMS allocations when $a_i$ is divisible by $b_i$. We also
prove that PMMS allocations exist for binary-valued MMS-feasible valuations,
where each bundle $S$ has value $v_i(S) \in \{0, 1\}$. Notably, this result
holds even without assuming monotonicity of valuations and thus applies to the
fair division of chores and mixed manna. Finally, we study a class of
valuations called pair-demand valuations, which extend the well-studied
unit-demand valuations to the case where each agent derives value from at most
two items, and we show that PMMS allocations exist in this setting. Our proofs
are constructive, and we provide polynomial-time algorithms for all three
existence results.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [605] [The New LLM Bottleneck: A Systems Perspective on Latent Attention and Mixture-of-Experts](https://arxiv.org/abs/2507.15465)
*Sungmin Yun,Seonyong Park,Hwayong Nam,Younjoo Lee,Gunjun Lee,Kwanhee Kyung,Sangpyo Kim,Nam Sung Kim,Jongmin Kim,Hyungyo Kim,Juhwan Cho,Seungmin Baek,Jung Ho Ahn*

Main category: cs.AR

TL;DR: The paper argues that specialized hardware for Multi-Head Attention (MHA) is becoming obsolete due to architectural shifts like Multi-head Latent Attention (MLA) and Mixture-of-Experts (MoE), which balance computational workloads better.


<details>
  <summary>Details</summary>
Motivation: The traditional dichotomy between memory-bound MHA and compute-bound feedforward layers has driven research into specialized hardware. This paper challenges that need due to recent advancements.

Method: Analyzes the arithmetic intensity of MLA and MoE, showing their shift toward compute-bound regimes and balanced computational profiles.

Result: MLA's arithmetic intensity is much higher than MHA, and MoE can be tuned to match dense layers, reducing the need for specialized attention hardware.

Conclusion: The focus should shift to designing balanced systems for next-generation Transformers, addressing compute, memory, and interconnect demands.

Abstract: Computational workloads composing traditional Transformer models are starkly
bifurcated. Multi-Head Attention (MHA) is memory-bound, with low arithmetic
intensity, while feedforward layers are compute-bound. This dichotomy has long
motivated research into specialized hardware to mitigate the MHA bottleneck.
  This paper argues that recent architectural shifts, namely Multi-head Latent
Attention (MLA) and Mixture-of-Experts (MoE), challenge the premise of
specialized attention hardware. We make two key observations. First, the
arithmetic intensity of MLA is over two orders of magnitude greater than that
of MHA, shifting it close to a compute-bound regime well-suited for modern
accelerators like GPUs. Second, by distributing MoE experts across a pool of
accelerators, their arithmetic intensity can be tuned through batching to match
that of the dense layers, creating a more balanced computational profile.
  These findings reveal a diminishing need for specialized attention hardware.
The central challenge for next-generation Transformers is no longer
accelerating a single memory-bound layer. Instead, the focus must shift to
designing balanced systems with sufficient compute, memory capacity, memory
bandwidth, and high-bandwidth interconnects to manage the diverse demands of
large-scale models.

</details>
