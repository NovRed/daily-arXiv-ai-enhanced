<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 65]
- [cs.AI](#cs.AI) [Total: 36]
- [cs.CL](#cs.CL) [Total: 47]
- [cs.LG](#cs.LG) [Total: 83]
- [eess.IV](#eess.IV) [Total: 6]
- [cs.SD](#cs.SD) [Total: 2]
- [cs.DC](#cs.DC) [Total: 1]
- [cs.IR](#cs.IR) [Total: 2]
- [cs.SI](#cs.SI) [Total: 2]
- [cs.GT](#cs.GT) [Total: 3]
- [cs.NI](#cs.NI) [Total: 2]
- [math-ph](#math-ph) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.DB](#cs.DB) [Total: 2]
- [cs.GR](#cs.GR) [Total: 1]
- [cond-mat.str-el](#cond-mat.str-el) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.CY](#cs.CY) [Total: 5]
- [q-fin.TR](#q-fin.TR) [Total: 1]
- [stat.ML](#stat.ML) [Total: 8]
- [eess.SY](#eess.SY) [Total: 2]
- [gr-qc](#gr-qc) [Total: 1]
- [cs.DS](#cs.DS) [Total: 1]
- [cs.AR](#cs.AR) [Total: 3]
- [cs.HC](#cs.HC) [Total: 3]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [physics.app-ph](#physics.app-ph) [Total: 1]
- [cs.CE](#cs.CE) [Total: 1]
- [cs.NE](#cs.NE) [Total: 4]
- [math.OC](#math.OC) [Total: 3]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [cs.SE](#cs.SE) [Total: 9]
- [quant-ph](#quant-ph) [Total: 2]
- [cs.CR](#cs.CR) [Total: 7]
- [math.PR](#math.PR) [Total: 1]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [cs.RO](#cs.RO) [Total: 7]
- [cs.MM](#cs.MM) [Total: 1]
- [econ.GN](#econ.GN) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [CWNet: Causal Wavelet Network for Low-Light Image Enhancement](https://arxiv.org/abs/2507.10689)
*Tongshun Zhang,Pingping Liu,Yubing Lu,Mengen Cai,Zijian Zhang,Zhe Zhang,Qiuzhan Zhou*

Main category: cs.CV

TL;DR: CWNet introduces a causal reasoning approach for low-light image enhancement, combining wavelet transforms and causal analysis to outperform existing methods.


<details>
  <summary>Details</summary>
Motivation: Traditional LLIE methods lack instance-level semantic understanding and feature-specific adjustments, prompting the need for a causal reasoning-based solution.

Method: CWNet uses wavelet transforms and causal reasoning, with global metric learning and local CLIP semantic loss, to enhance low-light images.

Result: CWNet outperforms state-of-the-art methods across diverse datasets, demonstrating robust performance.

Conclusion: CWNet effectively addresses limitations of traditional LLIE methods by integrating causal reasoning and wavelet transforms, achieving superior results.

Abstract: Traditional Low-Light Image Enhancement (LLIE) methods primarily focus on
uniform brightness adjustment, often neglecting instance-level semantic
information and the inherent characteristics of different features. To address
these limitations, we propose CWNet (Causal Wavelet Network), a novel
architecture that leverages wavelet transforms for causal reasoning.
Specifically, our approach comprises two key components: 1) Inspired by the
concept of intervention in causality, we adopt a causal reasoning perspective
to reveal the underlying causal relationships in low-light enhancement. From a
global perspective, we employ a metric learning strategy to ensure causal
embeddings adhere to causal principles, separating them from non-causal
confounding factors while focusing on the invariance of causal factors. At the
local level, we introduce an instance-level CLIP semantic loss to precisely
maintain causal factor consistency. 2) Based on our causal analysis, we present
a wavelet transform-based backbone network that effectively optimizes the
recovery of frequency information, ensuring precise enhancement tailored to the
specific attributes of wavelet transforms. Extensive experiments demonstrate
that CWNet significantly outperforms current state-of-the-art methods across
multiple datasets, showcasing its robust performance across diverse scenes.
Code is available at https://github.com/bywlzts/CWNet-Causal-Wavelet-Network.

</details>


### [2] [Integrating Biological Knowledge for Robust Microscopy Image Profiling on De Novo Cell Lines](https://arxiv.org/abs/2507.10737)
*Jiayuan Chen,Thai-Hoang Pham,Yuanlong Wang,Ping Zhang*

Main category: cs.CV

TL;DR: A novel framework integrates biological knowledge to improve microscopy image profiling for de novo cell lines, enhancing drug discovery.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in robust perturbation screening for de novo cell lines due to morphological and biological heterogeneity.

Method: Disentangles perturbation-specific and cell line-specific representations using external biological knowledge (STRING, Hetionet) and transcriptomic features.

Result: Improves generalization for de novo cell lines, validated on RxRx datasets with one-shot and few-shot fine-tuning.

Conclusion: The framework effectively enhances microscopy image profiling for real-world drug discovery applications.

Abstract: High-throughput screening techniques, such as microscopy imaging of cellular
responses to genetic and chemical perturbations, play a crucial role in drug
discovery and biomedical research. However, robust perturbation screening for
\textit{de novo} cell lines remains challenging due to the significant
morphological and biological heterogeneity across cell lines. To address this,
we propose a novel framework that integrates external biological knowledge into
existing pretraining strategies to enhance microscopy image profiling models.
Our approach explicitly disentangles perturbation-specific and cell
line-specific representations using external biological information.
Specifically, we construct a knowledge graph leveraging protein interaction
data from STRING and Hetionet databases to guide models toward
perturbation-specific features during pretraining. Additionally, we incorporate
transcriptomic features from single-cell foundation models to capture cell
line-specific representations. By learning these disentangled features, our
method improves the generalization of imaging models to \textit{de novo} cell
lines. We evaluate our framework on the RxRx database through one-shot
fine-tuning on an RxRx1 cell line and few-shot fine-tuning on cell lines from
the RxRx19a dataset. Experimental results demonstrate that our method enhances
microscopy image profiling for \textit{de novo} cell lines, highlighting its
effectiveness in real-world phenotype-based drug discovery applications.

</details>


### [3] [Auditing Facial Emotion Recognition Datasets for Posed Expressions and Racial Bias](https://arxiv.org/abs/2507.10755)
*Rina Khan,Catherine Stinson*

Main category: cs.CV

TL;DR: The paper audits FER datasets, revealing biases in posed vs. spontaneous expressions and racial/skin color performance disparities, leading to ethical concerns in real-world applications.


<details>
  <summary>Details</summary>
Motivation: The study addresses performance and ethical challenges in FER algorithms, particularly their poor performance with spontaneous expressions and racial/skin color biases, linked to dataset collection practices.

Method: The authors audit two FER datasets by sampling images to classify them as spontaneous or posed and evaluate model performance across races and skin tones.

Result: They find mislabeled posed images in supposedly in-the-wild datasets and racial/skin color biases, with models more likely to misclassify non-white or dark-skinned individuals as showing negative emotions.

Conclusion: The biases in FER datasets and models can perpetuate harm in real-world applications, highlighting the need for improved data collection and model fairness.

Abstract: Facial expression recognition (FER) algorithms classify facial expressions
into emotions such as happy, sad, or angry. An evaluative challenge facing FER
algorithms is the fall in performance when detecting spontaneous expressions
compared to posed expressions. An ethical (and evaluative) challenge facing FER
algorithms is that they tend to perform poorly for people of some races and
skin colors. These challenges are linked to the data collection practices
employed in the creation of FER datasets. In this study, we audit two
state-of-the-art FER datasets. We take random samples from each dataset and
examine whether images are spontaneous or posed. In doing so, we propose a
methodology for identifying spontaneous or posed images. We discover a
significant number of images that were posed in the datasets purporting to
consist of in-the-wild images. Since performance of FER models vary between
spontaneous and posed images, the performance of models trained on these
datasets will not represent the true performance if such models were to be
deployed in in-the-wild applications. We also observe the skin color of
individuals in the samples, and test three models trained on each of the
datasets to predict facial expressions of people from various races and skin
tones. We find that the FER models audited were more likely to predict people
labeled as not white or determined to have dark skin as showing a negative
emotion such as anger or sadness even when they were smiling. This bias makes
such models prone to perpetuate harm in real life applications.

</details>


### [4] [FPC-Net: Revisiting SuperPoint with Descriptor-Free Keypoint Detection via Feature Pyramids and Consistency-Based Implicit Matching](https://arxiv.org/abs/2507.10770)
*Ionuţ Grigore,Călin-Adrian Popa,Claudiu Leoveanu-Condrei*

Main category: cs.CV

TL;DR: A new technique eliminates descriptor use in interest point matching, reducing memory usage despite slightly lower accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional methods require descriptors for matching, which increases memory and computational overhead.

Method: Interest points are inherently associated during detection, bypassing descriptor computation and matching.

Result: Matching accuracy is slightly lower, but memory usage is drastically reduced.

Conclusion: The method offers a memory-efficient alternative to descriptor-based approaches, suitable for localization systems.

Abstract: The extraction and matching of interest points are fundamental to many
geometric computer vision tasks. Traditionally, matching is performed by
assigning descriptors to interest points and identifying correspondences based
on descriptor similarity. This work introduces a technique where interest
points are inherently associated during detection, eliminating the need for
computing, storing, transmitting, or matching descriptors. Although the
matching accuracy is marginally lower than that of conventional approaches, our
method completely eliminates the need for descriptors, leading to a drastic
reduction in memory usage for localization systems. We assess its effectiveness
by comparing it against both classical handcrafted methods and modern learned
approaches.

</details>


### [5] [A New Dataset and Performance Benchmark for Real-time Spacecraft Segmentation in Onboard Flight Computers](https://arxiv.org/abs/2507.10775)
*Jeffrey Joan Sam,Janhavi Sathe,Nikhil Chigali,Naman Gupta,Radhey Ruparel,Yicheng Jiang,Janmajay Singh,James W. Berck,Arko Barman*

Main category: cs.CV

TL;DR: A new dataset of 64k annotated spacecraft images was created for autonomous inspection, addressing the scarcity of such data. YOLOv8 and YOLOv11 models were fine-tuned, achieving high performance under real-world constraints.


<details>
  <summary>Details</summary>
Motivation: Spacecraft face damage risks, and repairs are costly. Autonomous inspection systems could mitigate these issues, but lack of annotated data hinders development.

Method: Created a dataset using real spacecraft models and synthetic backgrounds, adding noise/distortion. Fine-tuned YOLOv8 and YOLOv11 models for segmentation benchmarks.

Result: Models achieved a Dice score of 0.92, Hausdorff distance of 0.69, and 0.5s inference time under real-world constraints.

Conclusion: The dataset and models provide a reliable benchmark for real-time spacecraft inspection, enabling cost-effective autonomous systems.

Abstract: Spacecraft deployed in outer space are routinely subjected to various forms
of damage due to exposure to hazardous environments. In addition, there are
significant risks to the subsequent process of in-space repairs through human
extravehicular activity or robotic manipulation, incurring substantial
operational costs. Recent developments in image segmentation could enable the
development of reliable and cost-effective autonomous inspection systems. While
these models often require large amounts of training data to achieve
satisfactory results, publicly available annotated spacecraft segmentation data
are very scarce. Here, we present a new dataset of nearly 64k annotated
spacecraft images that was created using real spacecraft models, superimposed
on a mixture of real and synthetic backgrounds generated using NASA's TTALOS
pipeline. To mimic camera distortions and noise in real-world image
acquisition, we also added different types of noise and distortion to the
images. Finally, we finetuned YOLOv8 and YOLOv11 segmentation models to
generate performance benchmarks for the dataset under well-defined hardware and
inference time constraints to mimic real-world image segmentation challenges
for real-time onboard applications in space on NASA's inspector spacecraft. The
resulting models, when tested under these constraints, achieved a Dice score of
0.92, Hausdorff distance of 0.69, and an inference time of about 0.5 second.
The dataset and models for performance benchmark are available at
https://github.com/RiceD2KLab/SWiM.

</details>


### [6] [Warehouse Spatial Question Answering with LLM Agent](https://arxiv.org/abs/2507.10778)
*Hsiang-Wei Huang,Jen-Hao Cheng,Kuang-Ming Chen,Cheng-Yen Yang,Bahaa Alattar,Yi-Ru Lin,Pyongkun Kim,Sangwon Kim,Kwangju Kim,Chung-I Huang,Jenq-Neng Hwang*

Main category: cs.CV

TL;DR: A data-efficient LLM agent system enhances spatial reasoning for complex indoor warehouse tasks, outperforming previous methods.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs struggle with spatial understanding; this work aims to improve efficiency and accuracy in spatial reasoning.

Method: Proposes an LLM agent system with tools for spatial reasoning and API interaction to solve spatial questions.

Result: Achieves high accuracy and efficiency in tasks like object retrieval, counting, and distance estimation on the AI City Challenge dataset.

Conclusion: The system demonstrates superior spatial reasoning, offering a practical solution for warehouse scenarios.

Abstract: Spatial understanding has been a challenging task for existing Multi-modal
Large Language Models~(MLLMs). Previous methods leverage large-scale MLLM
finetuning to enhance MLLM's spatial understanding ability. In this paper, we
present a data-efficient approach. We propose a LLM agent system with strong
and advanced spatial reasoning ability, which can be used to solve the
challenging spatial question answering task in complex indoor warehouse
scenarios. Our system integrates multiple tools that allow the LLM agent to
conduct spatial reasoning and API tools interaction to answer the given
complicated spatial question. Extensive evaluations on the 2025 AI City
Challenge Physical AI Spatial Intelligence Warehouse dataset demonstrate that
our system achieves high accuracy and efficiency in tasks such as object
retrieval, counting, and distance estimation. The code is available at:
https://github.com/hsiangwei0903/SpatialAgent

</details>


### [7] [ThinkingViT: Matryoshka Thinking Vision Transformer for Elastic Inference](https://arxiv.org/abs/2507.10800)
*Ali Hojjat,Janek Haberer,Soren Pirk,Olaf Landsiedel*

Main category: cs.CV

TL;DR: ThinkingViT introduces a nested ViT architecture with dynamic computation adjustment based on input difficulty, improving efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: Fixed computational budgets in Vision Transformers lead to inefficiencies, as all inputs receive the same compute regardless of complexity.

Method: ThinkingViT uses progressive thinking stages and Token Recycling to dynamically adjust computation, activating attention heads as needed.

Result: ThinkingViT outperforms nested baselines by up to 2.9 p.p. in accuracy at equal GMACs on ImageNet-1K.

Conclusion: ThinkingViT offers a scalable, efficient, and accurate solution for Vision Transformers, with potential as a plugin upgrade for vanilla ViT.

Abstract: Vision Transformers deliver state-of-the-art performance, yet their fixed
computational budget prevents scalable deployment across heterogeneous
hardware. Recent nested Transformer architectures mitigate this by embedding
nested subnetworks within a single model to enable scalable inference. However,
these models allocate the same amount of compute to all inputs, regardless of
their complexity, which leads to inefficiencies. To address this, we introduce
ThinkingViT, a nested ViT architecture that employs progressive thinking stages
to dynamically adjust inference computation based on input difficulty.
ThinkingViT initiates inference by activating a small subset of the most
important attention heads and terminates early if predictions reach sufficient
certainty. Otherwise, it activates additional attention heads and re-evaluates
the input. At the core of ThinkingViT is our Token Recycling mechanism, which
conditions each subsequent inference stage on the embeddings from the previous
stage, enabling progressive improvement. Due to its backbone-preserving design,
ThinkingViT also serves as a plugin upgrade for vanilla ViT. Experiments show
that ThinkingViT surpasses nested baselines by up to 2.0 percentage points
(p.p.) in accuracy at the same throughput and by up to 2.9 p.p. at equal GMACs
on ImageNet-1K. The source code is available at
https://github.com/ds-kiel/ThinkingViT.

</details>


### [8] [LLM-Guided Agentic Object Detection for Open-World Understanding](https://arxiv.org/abs/2507.10844)
*Furkan Mumcu,Michael J. Jones,Anoop Cherian,Yasin Yilmaz*

Main category: cs.CV

TL;DR: Proposes LLM-guided agentic object detection (LAOD) for label-free, zero-shot detection by using LLMs to generate scene-specific object names, enhancing autonomy in open-world understanding.


<details>
  <summary>Details</summary>
Motivation: Traditional object detection lacks flexibility for novel objects, while existing methods like OWOD and OVOD have limitations (e.g., missing semantic labels or requiring user prompts).

Method: LAOD framework uses an LLM to generate object names dynamically, paired with an open-vocabulary detector for localization, introducing metrics CAAP and SNAP for evaluation.

Result: Validated on LVIS, COCO, and COCO-OOD, showing strong performance in detecting and naming novel objects.

Conclusion: LAOD improves autonomy and adaptability in open-world object detection, addressing limitations of prior methods.

Abstract: Object detection traditionally relies on fixed category sets, requiring
costly re-training to handle novel objects. While Open-World and
Open-Vocabulary Object Detection (OWOD and OVOD) improve flexibility, OWOD
lacks semantic labels for unknowns, and OVOD depends on user prompts, limiting
autonomy. We propose an LLM-guided agentic object detection (LAOD) framework
that enables fully label-free, zero-shot detection by prompting a Large
Language Model (LLM) to generate scene-specific object names. These are passed
to an open-vocabulary detector for localization, allowing the system to adapt
its goals dynamically. We introduce two new metrics, Class-Agnostic Average
Precision (CAAP) and Semantic Naming Average Precision (SNAP), to separately
evaluate localization and naming. Experiments on LVIS, COCO, and COCO-OOD
validate our approach, showing strong performance in detecting and naming novel
objects. Our method offers enhanced autonomy and adaptability for open-world
understanding.

</details>


### [9] [Winsor-CAM: Human-Tunable Visual Explanations from Deep Networks via Layer-Wise Winsorization](https://arxiv.org/abs/2507.10846)
*Casey Wall,Longwei Wang,Rodrigue Rizk,KC Santosh*

Main category: cs.CV

TL;DR: Winsor-CAM improves Grad-CAM by aggregating info across all CNN layers using Winsorization for robust saliency maps, outperforming baselines in interpretability and localization.


<details>
  <summary>Details</summary>
Motivation: Enhancing CNN interpretability for high-stakes domains by addressing Grad-CAM's limitations in layer aggregation and noise sensitivity.

Method: Winsor-CAM applies Winsorization (percentile-based outlier attenuation) across all convolutional layers, with a user-tunable threshold for semantic control.

Result: Outperforms Grad-CAM and uniform layer-averaging in interpretability and localization metrics (e.g., IoU, center-of-mass alignment) on PASCAL VOC 2012.

Conclusion: Winsor-CAM advances trustworthy AI with interpretable, multi-layer insights and human-in-the-loop control.

Abstract: Interpreting the decision-making process of Convolutional Neural Networks
(CNNs) is critical for deploying models in high-stakes domains.
Gradient-weighted Class Activation Mapping (Grad-CAM) is a widely used method
for visual explanations, yet it typically focuses on the final convolutional
layer or na\"ively averages across layers, strategies that can obscure
important semantic cues or amplify irrelevant noise. We propose Winsor-CAM, a
novel, human-tunable extension of Grad-CAM that generates robust and coherent
saliency maps by aggregating information across all convolutional layers. To
mitigate the influence of noisy or extreme attribution values, Winsor-CAM
applies Winsorization, a percentile-based outlier attenuation technique. A
user-controllable threshold allows for semantic-level tuning, enabling flexible
exploration of model behavior across representational hierarchies. Evaluations
on standard architectures (ResNet50, DenseNet121, VGG16, InceptionV3) using the
PASCAL VOC 2012 dataset demonstrate that Winsor-CAM produces more interpretable
heatmaps and achieves superior performance in localization metrics, including
intersection-over-union and center-of-mass alignment, when compared to Grad-CAM
and uniform layer-averaging baselines. Winsor-CAM advances the goal of
trustworthy AI by offering interpretable, multi-layer insights with
human-in-the-loop control.

</details>


### [10] [Sparse Fine-Tuning of Transformers for Generative Tasks](https://arxiv.org/abs/2507.10855)
*Wei Chen,Jingxi Yu,Zichen Miao,Qiang Qiu*

Main category: cs.CV

TL;DR: A sparse coding-inspired fine-tuning framework for transformers enhances interpretability and performance in downstream tasks like image editing and text-to-image customization.


<details>
  <summary>Details</summary>
Motivation: Existing fine-tuning methods lack interpretability in how models adapt to new tasks due to dense parameter updates.

Method: Introduces a sparse coding-based framework where fine-tuned features are sparse combinations of dictionary atoms, with coefficients indicating importance.

Result: Improves image editing by removing unimportant atoms and outperforms baselines in text-to-image customization.

Conclusion: The method offers interpretable and efficient adaptation of pre-trained models, validated in practical tasks.

Abstract: Large pre-trained transformers have revolutionized artificial intelligence
across various domains, and fine-tuning remains the dominant approach for
adapting these models to downstream tasks due to the cost of training from
scratch. However, in existing fine-tuning methods, the updated representations
are formed as a dense combination of modified parameters, making it challenging
to interpret their contributions and understand how the model adapts to new
tasks. In this work, we introduce a fine-tuning framework inspired by sparse
coding, where fine-tuned features are represented as a sparse combination of
basic elements, i.e., feature dictionary atoms. The feature dictionary atoms
function as fundamental building blocks of the representation, and tuning atoms
allows for seamless adaptation to downstream tasks. Sparse coefficients then
serve as indicators of atom importance, identifying the contribution of each
atom to the updated representation. Leveraging the atom selection capability of
sparse coefficients, we first demonstrate that our method enhances image
editing performance by improving text alignment through the removal of
unimportant feature dictionary atoms. Additionally, we validate the
effectiveness of our approach in the text-to-image concept customization task,
where our method efficiently constructs the target concept using a sparse
combination of feature dictionary atoms, outperforming various baseline
fine-tuning methods.

</details>


### [11] [A Lightweight and Robust Framework for Real-Time Colorectal Polyp Detection Using LOF-Based Preprocessing and YOLO-v11n](https://arxiv.org/abs/2507.10864)
*Saadat Behzadi,Danial Sharifrazi,Bita Mesbahzadeh,Javad Hassannataj Joloudarid,Roohallah Alizadehsani*

Main category: cs.CV

TL;DR: A lightweight framework combining LOF for noise filtering and YOLO-v11n for polyp detection achieves high accuracy and efficiency in real-time colonoscopy support.


<details>
  <summary>Details</summary>
Motivation: Timely and accurate polyp detection is vital for colorectal cancer prevention, requiring efficient and robust AI solutions.

Method: LOF filters noisy data; YOLO-v11n processes cleaned data with cross-validation and augmentation. Tested on five datasets with converted annotations.

Result: High performance: precision 95.83%, recall 91.85%, F1-score 93.48%, mAP@0.5 96.48%, mAP@0.5:0.95 77.75%.

Conclusion: The method is effective for clinical use, highlighting the importance of data preprocessing and model efficiency in medical AI.

Abstract: Objectives: Timely and accurate detection of colorectal polyps plays a
crucial role in diagnosing and preventing colorectal cancer, a major cause of
mortality worldwide. This study introduces a new, lightweight, and efficient
framework for polyp detection that combines the Local Outlier Factor (LOF)
algorithm for filtering noisy data with the YOLO-v11n deep learning model.
  Study design: An experimental study leveraging deep learning and outlier
removal techniques across multiple public datasets.
  Methods: The proposed approach was tested on five diverse and publicly
available datasets: CVC-ColonDB, CVC-ClinicDB, Kvasir-SEG, ETIS, and EndoScene.
Since these datasets originally lacked bounding box annotations, we converted
their segmentation masks into suitable detection labels. To enhance the
robustness and generalizability of our model, we apply 5-fold cross-validation
and remove anomalous samples using the LOF method configured with 30 neighbors
and a contamination ratio of 5%. Cleaned data are then fed into YOLO-v11n, a
fast and resource-efficient object detection architecture optimized for
real-time applications. We train the model using a combination of modern
augmentation strategies to improve detection accuracy under diverse conditions.
  Results: Our approach significantly improves polyp localization performance,
achieving a precision of 95.83%, recall of 91.85%, F1-score of 93.48%, mAP@0.5
of 96.48%, and mAP@0.5:0.95 of 77.75%. Compared to previous YOLO-based methods,
our model demonstrates enhanced accuracy and efficiency.
  Conclusions: These results suggest that the proposed method is well-suited
for real-time colonoscopy support in clinical settings. Overall, the study
underscores how crucial data preprocessing and model efficiency are when
designing effective AI systems for medical imaging.

</details>


### [12] [Trexplorer Super: Topologically Correct Centerline Tree Tracking of Tubular Objects in CT Volumes](https://arxiv.org/abs/2507.10881)
*Roman Naeem,David Hagerman,Jennifer Alvén,Lennart Svensson,Fredrik Kahl*

Main category: cs.CV

TL;DR: Trexplorer Super improves centerline tracking in 3D medical images by addressing duplicate branches and premature termination, outperforming SOTA models on new datasets.


<details>
  <summary>Details</summary>
Motivation: Accurate tracking of tubular tree structures like blood vessels is crucial for medical tasks, but existing models like Trexplorer have limitations.

Method: Trexplorer Super enhances Trexplorer with novel advancements and is evaluated on three new datasets (one synthetic, two real).

Result: Trexplorer Super outperforms SOTA models on all datasets, though synthetic data performance doesn't guarantee real-data success.

Conclusion: Trexplorer Super advances centerline tracking, with datasets and code made available for further research.

Abstract: Tubular tree structures, such as blood vessels and airways, are essential in
human anatomy and accurately tracking them while preserving their topology is
crucial for various downstream tasks. Trexplorer is a recurrent model designed
for centerline tracking in 3D medical images but it struggles with predicting
duplicate branches and terminating tracking prematurely. To address these
issues, we present Trexplorer Super, an enhanced version that notably improves
performance through novel advancements. However, evaluating centerline tracking
models is challenging due to the lack of public datasets. To enable thorough
evaluation, we develop three centerline datasets, one synthetic and two real,
each with increasing difficulty. Using these datasets, we conduct a
comprehensive evaluation of existing state-of-the-art (SOTA) models and compare
them with our approach. Trexplorer Super outperforms previous SOTA models on
every dataset. Our results also highlight that strong performance on synthetic
data does not necessarily translate to real datasets. The code and datasets are
available at https://github.com/RomStriker/Trexplorer-Super.

</details>


### [13] [Modernizing CNN-based Weather Forecast Model towards Higher Computational Efficiency](https://arxiv.org/abs/2507.10893)
*Minjong Cheon,Eunhan Goo,Su-Hyeon Shin,Muhammad Ahmed,Hyungjun Kim*

Main category: cs.CV

TL;DR: A lightweight CNN-based model, KAI-a, is introduced for global weather forecasting, matching state-of-the-art accuracy with reduced computational demands.


<details>
  <summary>Details</summary>
Motivation: AI-based weather models often use Transformer architectures, which are resource-intensive. This study aims to provide a more efficient alternative.

Method: KAI-a uses a scale-invariant architecture and InceptionNeXt-based blocks, optimized for Earth system data. Trained on ERA5 data with 7M parameters.

Result: KAI-a achieves comparable accuracy to top models, trains in 12 hours on a single GPU, and performs well in extreme event case studies.

Conclusion: KAI-a offers a practical, efficient solution for weather forecasting, balancing performance and computational cost.

Abstract: Recently, AI-based weather forecast models have achieved impressive advances.
These models have reached accuracy levels comparable to traditional NWP
systems, marking a significant milestone in data-driven weather prediction.
However, they mostly leverage Transformer-based architectures, which often
leads to high training complexity and resource demands due to the massive
parameter sizes. In this study, we introduce a modernized CNN-based model for
global weather forecasting that delivers competitive accuracy while
significantly reducing computational requirements. To present a systematic
modernization roadmap, we highlight key architectural enhancements across
multiple design scales from an earlier CNN-based approach. KAI-a incorporates a
scale-invariant architecture and InceptionNeXt-based blocks within a
geophysically-aware design, tailored to the structure of Earth system data.
Trained on the ERA5 daily dataset with 67 atmospheric variables, the model
contains about 7 million parameters and completes training in just 12 hours on
a single NVIDIA L40s GPU. Our evaluation shows that KAI-a matches the
performance of state-of-the-art models in medium-range weather forecasting,
while offering a significantly lightweight design. Furthermore, case studies on
the 2018 European heatwave and the East Asian summer monsoon demonstrate
KAI-a's robust skill in capturing extreme events, reinforcing its practical
utility.

</details>


### [14] [Commuting Distance Regularization for Timescale-Dependent Label Inconsistency in EEG Emotion Recognition](https://arxiv.org/abs/2507.10895)
*Xiaocong Zeng,Craig Michoski,Yan Pang,Dongyang Kuang*

Main category: cs.CV

TL;DR: The paper proposes two regularization strategies, LVL and LGCL, to address Timescale Dependent Label Inconsistency (TsDLI) in EEG-based emotion recognition, improving model generalization and explainability.


<details>
  <summary>Details</summary>
Motivation: To mitigate TsDLI and enhance model performance in EEG-based emotion recognition, which is often overlooked.

Method: Introduces Local Variation Loss (LVL) and Local-Global Consistency Loss (LGCL) using bounded variation functions and commute-time distances in a graph theoretic framework. Also proposes new evaluation metrics.

Result: The methods outperform state-of-the-art baselines on DREAMER and DEAP datasets, with LVL achieving the best aggregate performance.

Conclusion: The proposed framework effectively balances interpretability and predictive power under label inconsistency, with LVL and LGCL demonstrating superior performance.

Abstract: In this work, we address the often-overlooked issue of Timescale Dependent
Label Inconsistency (TsDLI) in training neural network models for EEG-based
human emotion recognition. To mitigate TsDLI and enhance model generalization
and explainability, we propose two novel regularization strategies: Local
Variation Loss (LVL) and Local-Global Consistency Loss (LGCL). Both methods
incorporate classical mathematical principles--specifically, functions of
bounded variation and commute-time distances--within a graph theoretic
framework. Complementing our regularizers, we introduce a suite of new
evaluation metrics that better capture the alignment between temporally local
predictions and their associated global emotion labels. We validate our
approach through comprehensive experiments on two widely used EEG emotion
datasets, DREAMER and DEAP, across a range of neural architectures including
LSTM and transformer-based models. Performance is assessed using five distinct
metrics encompassing both quantitative accuracy and qualitative consistency.
Results consistently show that our proposed methods outperform state-of-the-art
baselines, delivering superior aggregate performance and offering a principled
trade-off between interpretability and predictive power under label
inconsistency. Notably, LVL achieves the best aggregate rank across all
benchmarked backbones and metrics, while LGCL frequently ranks the second,
highlighting the effectiveness of our framework.

</details>


### [15] [GeoDistill: Geometry-Guided Self-Distillation for Weakly Supervised Cross-View Localization](https://arxiv.org/abs/2507.10935)
*Shaowen Tong,Zimin Xia,Alexandre Alahi,Xuming He,Yujiao Shi*

Main category: cs.CV

TL;DR: GeoDistill is a weakly supervised framework for cross-view localization using teacher-student learning with FoV-based masking, improving accuracy and reducing uncertainty without costly annotations.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on expensive ground-truth pose annotations, limiting scalability. GeoDistill aims to reduce this dependency.

Method: Uses teacher-student learning with FoV-based masking to align predictions, focusing on key features like lane lines.

Result: Significantly improves localization performance and introduces a novel orientation estimation network.

Conclusion: GeoDistill offers a scalable, efficient solution for cross-view localization challenges.

Abstract: Cross-view localization, the task of estimating a camera's
3-degrees-of-freedom (3-DoF) pose by aligning ground-level images with
satellite images, is crucial for large-scale outdoor applications like
autonomous navigation and augmented reality. Existing methods often rely on
fully supervised learning, which requires costly ground-truth pose annotations.
In this work, we propose GeoDistill, a Geometry guided weakly supervised self
distillation framework that uses teacher-student learning with Field-of-View
(FoV)-based masking to enhance local feature learning for robust cross-view
localization. In GeoDistill, the teacher model localizes a panoramic image,
while the student model predicts locations from a limited FoV counterpart
created by FoV-based masking. By aligning the student's predictions with those
of the teacher, the student focuses on key features like lane lines and ignores
textureless regions, such as roads. This results in more accurate predictions
and reduced uncertainty, regardless of whether the query images are panoramas
or limited FoV images. Our experiments show that GeoDistill significantly
improves localization performance across different frameworks. Additionally, we
introduce a novel orientation estimation network that predicts relative
orientation without requiring precise planar position ground truth. GeoDistill
provides a scalable and efficient solution for real-world cross-view
localization challenges. Code and model can be found at
https://github.com/tongshw/GeoDistill.

</details>


### [16] [Graph Aggregation Prototype Learning for Semantic Change Detection in Remote Sensing](https://arxiv.org/abs/2507.10938)
*Zhengyi Xu,Haoran Wu,Wen Jiang,Jie Geng*

Main category: cs.CV

TL;DR: The paper introduces GAPL-SCD, a method for semantic change detection in remote sensing, addressing multi-task optimization challenges with adaptive techniques and prototype learning.


<details>
  <summary>Details</summary>
Motivation: Semantic change detection (SCD) provides detailed category changes but faces challenges like negative transfer due to multi-task conflicts.

Method: Proposes GAPL-SCD, combining semantic segmentation, change detection, and graph aggregation prototype learning with adaptive weight allocation and gradient rotation.

Result: Achieves state-of-the-art performance on SECOND and Landsat-SCD datasets, improving accuracy and robustness.

Conclusion: GAPL-SCD effectively addresses multi-task conflicts and enhances SCD performance through innovative optimization and feature interaction.

Abstract: Semantic change detection (SCD) extends the binary change detection task to
provide not only the change locations but also the detailed "from-to"
categories in multi-temporal remote sensing data. Such detailed semantic
insights into changes offer considerable advantages for a wide array of
applications. However, since SCD involves the simultaneous optimization of
multiple tasks, the model is prone to negative transfer due to task-specific
learning difficulties and conflicting gradient flows. To address this issue, we
propose Graph Aggregation Prototype Learning for Semantic Change Detection in
remote sensing(GAPL-SCD). In this framework, a multi-task joint optimization
method is designed to optimize the primary task of semantic segmentation and
change detection, along with the auxiliary task of graph aggregation prototype
learning. Adaptive weight allocation and gradient rotation methods are used to
alleviate the conflict between training tasks and improve multi-task learning
capabilities. Specifically, the graph aggregation prototype learning module
constructs an interaction graph using high-level features. Prototypes serve as
class proxies, enabling category-level domain alignment across time points and
reducing interference from irrelevant changes. Additionally, the proposed
self-query multi-level feature interaction and bi-temporal feature fusion
modules further enhance multi-scale feature representation, improving
performance in complex scenes. Experimental results on the SECOND and
Landsat-SCD datasets demonstrate that our method achieves state-of-the-art
performance, with significant improvements in accuracy and robustness for SCD
task.

</details>


### [17] [Robust ID-Specific Face Restoration via Alignment Learning](https://arxiv.org/abs/2507.10943)
*Yushun Fang,Lu Liu,Xiang Gao,Qiang Hu,Ning Cao,Jianghe Cui,Gang Chen,Xiaoyun Zhang*

Main category: cs.CV

TL;DR: RIDFR is a novel ID-specific face restoration framework using diffusion models to address identity uncertainty, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Current face restoration methods struggle with identity uncertainty due to obscure inputs and stochastic processes.

Method: RIDFR uses a pre-trained diffusion model with two parallel conditioning modules (Content Injection and Identity Injection) and Alignment Learning to align restoration results.

Result: The framework achieves high-quality, ID-specific results with strong identity fidelity and robustness.

Conclusion: RIDFR effectively resolves identity uncertainty in face restoration, setting a new benchmark for performance.

Abstract: The latest developments in Face Restoration have yielded significant
advancements in visual quality through the utilization of diverse diffusion
priors. Nevertheless, the uncertainty of face identity introduced by
identity-obscure inputs and stochastic generative processes remains unresolved.
To address this challenge, we present Robust ID-Specific Face Restoration
(RIDFR), a novel ID-specific face restoration framework based on diffusion
models. Specifically, RIDFR leverages a pre-trained diffusion model in
conjunction with two parallel conditioning modules. The Content Injection
Module inputs the severely degraded image, while the Identity Injection Module
integrates the specific identity from a given image. Subsequently, RIDFR
incorporates Alignment Learning, which aligns the restoration results from
multiple references with the same identity in order to suppress the
interference of ID-irrelevant face semantics (e.g. pose, expression, make-up,
hair style). Experiments demonstrate that our framework outperforms the
state-of-the-art methods, reconstructing high-quality ID-specific results with
high identity fidelity and demonstrating strong robustness.

</details>


### [18] [Women Sport Actions Dataset for Visual Classification Using Small Scale Training Data](https://arxiv.org/abs/2507.10969)
*Palash Ray,Mahuya Sasmal,Asish Bera*

Main category: cs.CV

TL;DR: The paper introduces the WomenSports dataset for women's sports action classification, addressing the lack of diverse datasets. It proposes a CNN with channel attention for feature extraction, achieving 89.15% accuracy on the new dataset.


<details>
  <summary>Details</summary>
Motivation: Existing datasets lack diversity in women's sports actions, limiting research. This work aims to fill this gap with a new dataset and improved classification methods.

Method: A convolutional neural network (CNN) with channel attention for deep feature extraction is proposed. The method is tested on multiple datasets, including the new WomenSports dataset.

Result: The proposed CNN achieves 89.15% top-1 classification accuracy on the WomenSports dataset using ResNet-50.

Conclusion: The WomenSports dataset and the proposed CNN method effectively address the lack of diverse datasets and improve classification accuracy for women's sports actions.

Abstract: Sports action classification representing complex body postures and
player-object interactions is an emerging area in image-based sports analysis.
Some works have contributed to automated sports action recognition using
machine learning techniques over the past decades. However, sufficient image
datasets representing women sports actions with enough intra- and inter-class
variations are not available to the researchers. To overcome this limitation,
this work presents a new dataset named WomenSports for women sports
classification using small-scale training data. This dataset includes a variety
of sports activities, covering wide variations in movements, environments, and
interactions among players. In addition, this study proposes a convolutional
neural network (CNN) for deep feature extraction. A channel attention scheme
upon local contextual regions is applied to refine and enhance feature
representation. The experiments are carried out on three different sports
datasets and one dance dataset for generalizing the proposed algorithm, and the
performances on these datasets are noteworthy. The deep learning method
achieves 89.15% top-1 classification accuracy using ResNet-50 on the proposed
WomenSports dataset, which is publicly available for research at Mendeley Data.

</details>


### [19] [Conceptualizing Multi-scale Wavelet Attention and Ray-based Encoding for Human-Object Interaction Detection](https://arxiv.org/abs/2507.10977)
*Quan Bi Pay,Vishnu Monn Baskaran,Junn Yong Loo,KokSheik Wong,Simon See*

Main category: cs.CV

TL;DR: A novel wavelet attention-like backbone and ray-based encoder architecture for efficient and accurate human-object interaction (HOI) detection, addressing limitations of existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing HOI detectors are inefficient and resource-intensive, struggling with reliable predictions.

Method: Proposes a wavelet backbone for feature aggregation and a ray-based encoder for multi-scale attention optimization.

Result: Improved performance on benchmarks like ImageNet and HICO-DET.

Conclusion: The proposed architecture enhances HOI detection efficiency and accuracy, with code publicly available.

Abstract: Human-object interaction (HOI) detection is essential for accurately
localizing and characterizing interactions between humans and objects,
providing a comprehensive understanding of complex visual scenes across various
domains. However, existing HOI detectors often struggle to deliver reliable
predictions efficiently, relying on resource-intensive training methods and
inefficient architectures. To address these challenges, we conceptualize a
wavelet attention-like backbone and a novel ray-based encoder architecture
tailored for HOI detection. Our wavelet backbone addresses the limitations of
expressing middle-order interactions by aggregating discriminative features
from the low- and high-order interactions extracted from diverse convolutional
filters. Concurrently, the ray-based encoder facilitates multi-scale attention
by optimizing the focus of the decoder on relevant regions of interest and
mitigating computational overhead. As a result of harnessing the attenuated
intensity of learnable ray origins, our decoder aligns query embeddings with
emphasized regions of interest for accurate predictions. Experimental results
on benchmark datasets, including ImageNet and HICO-DET, showcase the potential
of our proposed architecture. The code is publicly available at
[https://github.com/henry-pay/RayEncoder].

</details>


### [20] [Mind the Gap: Bridging Occlusion in Gait Recognition via Residual Gap Correction](https://arxiv.org/abs/2507.10978)
*Ayush Gupta,Siyuan Huang,Rama Chellappa*

Main category: cs.CV

TL;DR: RG-Gait proposes residual correction for occluded gait recognition, improving accuracy on occluded sequences without losing performance on holistic inputs.


<details>
  <summary>Details</summary>
Motivation: Current gait recognition methods often ignore occlusions or require impractical paired data, and fail to retain performance on holistic inputs.

Method: Models occluded gait as a residual deviation from holistic gait, using a network to adaptively integrate the residual.

Result: Demonstrates improved performance on occluded sequences while maintaining holistic recognition accuracy on Gait3D, GREW, and BRIAR datasets.

Conclusion: Residual learning is effective for occluded gait recognition with holistic retention.

Abstract: Gait is becoming popular as a method of person re-identification because of
its ability to identify people at a distance. However, most current works in
gait recognition do not address the practical problem of occlusions. Among
those which do, some require paired tuples of occluded and holistic sequences,
which are impractical to collect in the real world. Further, these approaches
work on occlusions but fail to retain performance on holistic inputs. To
address these challenges, we propose RG-Gait, a method for residual correction
for occluded gait recognition with holistic retention. We model the problem as
a residual learning task, conceptualizing the occluded gait signature as a
residual deviation from the holistic gait representation. Our proposed network
adaptively integrates the learned residual, significantly improving performance
on occluded gait sequences without compromising the holistic recognition
accuracy. We evaluate our approach on the challenging Gait3D, GREW and BRIAR
datasets and show that learning the residual can be an effective technique to
tackle occluded gait recognition with holistic retention.

</details>


### [21] [SpaRTAN: Spatial Reinforcement Token-based Aggregation Network for Visual Recognition](https://arxiv.org/abs/2507.10999)
*Quan Bi Pay,Vishnu Monn Baskaran,Junn Yong Loo,KokSheik Wong,Simon See*

Main category: cs.CV

TL;DR: SpaRTAN is a lightweight CNN architecture that improves spatial and channel-wise information processing, achieving competitive performance with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: CNNs and transformers exhibit simplicity bias and redundancy in MLP-like blocks, limiting their efficiency and performance.

Method: SpaRTAN uses varying receptive field kernels and a wave-based channel aggregation module to enhance feature capture and reduce redundancies.

Result: Achieves 77.7% accuracy on ImageNet-1k with 3.8M parameters and 50.0% AP on COCO with 21.5M parameters.

Conclusion: SpaRTAN offers an efficient and high-performing alternative to existing architectures, addressing key limitations in CNNs and transformers.

Abstract: The resurgence of convolutional neural networks (CNNs) in visual recognition
tasks, exemplified by ConvNeXt, has demonstrated their capability to rival
transformer-based architectures through advanced training methodologies and
ViT-inspired design principles. However, both CNNs and transformers exhibit a
simplicity bias, favoring straightforward features over complex structural
representations. Furthermore, modern CNNs often integrate MLP-like blocks akin
to those in transformers, but these blocks suffer from significant information
redundancies, necessitating high expansion ratios to sustain competitive
performance. To address these limitations, we propose SpaRTAN, a lightweight
architectural design that enhances spatial and channel-wise information
processing. SpaRTAN employs kernels with varying receptive fields, controlled
by kernel size and dilation factor, to capture discriminative multi-order
spatial features effectively. A wave-based channel aggregation module further
modulates and reinforces pixel interactions, mitigating channel-wise
redundancies. Combining the two modules, the proposed network can efficiently
gather and dynamically contextualize discriminative features. Experimental
results in ImageNet and COCO demonstrate that SpaRTAN achieves remarkable
parameter efficiency while maintaining competitive performance. In particular,
on the ImageNet-1k benchmark, SpaRTAN achieves 77. 7% accuracy with only 3.8M
parameters and approximately 1.0 GFLOPs, demonstrating its ability to deliver
strong performance through an efficient design. On the COCO benchmark, it
achieves 50.0% AP, surpassing the previous benchmark by 1.2% with only 21.5M
parameters. The code is publicly available at
[https://github.com/henry-pay/SpaRTAN].

</details>


### [22] [Bridge Feature Matching and Cross-Modal Alignment with Mutual-filtering for Zero-shot Anomaly Detection](https://arxiv.org/abs/2507.11003)
*Yuhu Bai,Jiangning Zhang,Yunkang Cao,Guangyuan Lu,Qingdong He,Xiangtai Li,Guanzhong Tian*

Main category: cs.CV

TL;DR: FiSeCLIP enhances zero-shot anomaly detection (ZSAD) by combining feature matching and cross-modal alignment with CLIP, outperforming SOTA methods like AdaCLIP.


<details>
  <summary>Details</summary>
Motivation: To address the challenges of ZSAD, especially in industrial settings, by leveraging CLIP's capabilities without additional training.

Method: Uses batch-based testing with mutual reference images and text information to filter noisy features, while restoring CLIP's local semantic correlation for fine-grained tasks.

Result: Achieves superior performance in anomaly classification and segmentation, e.g., +4.6%/+5.7% improvement over AdaCLIP on MVTec-AD.

Conclusion: FiSeCLIP sets a stronger baseline for ZSAD, demonstrating the potential of training-free CLIP adaptations.

Abstract: With the advent of vision-language models (e.g., CLIP) in zero- and few-shot
settings, CLIP has been widely applied to zero-shot anomaly detection (ZSAD) in
recent research, where the rare classes are essential and expected in many
applications. This study introduces \textbf{FiSeCLIP} for ZSAD with
training-free \textbf{CLIP}, combining the feature matching with the
cross-modal alignment. Testing with the entire dataset is impractical, while
batch-based testing better aligns with real industrial needs, and images within
a batch can serve as mutual reference points. Accordingly, FiSeCLIP utilizes
other images in the same batch as reference information for the current image.
However, the lack of labels for these references can introduce ambiguity, we
apply text information to \textbf{fi}lter out noisy features. In addition, we
further explore CLIP's inherent potential to restore its local
\textbf{se}mantic correlation, adapting it for fine-grained anomaly detection
tasks to enable a more accurate filtering process. Our approach exhibits
superior performance for both anomaly classification and segmentation on
anomaly detection benchmarks, building a stronger baseline for the direction,
e.g., on MVTec-AD, FiSeCLIP outperforms the SOTA AdaCLIP by
+4.6\%$\uparrow$/+5.7\%$\uparrow$ in segmentation metrics AU-ROC/$F_1$-max.

</details>


### [23] [Semantically Informed Salient Regions Guided Radiology Report Generation](https://arxiv.org/abs/2507.11015)
*Zeyi Hou,Zeqiang Wei,Ruixin Yan,Ning Lang,Xiuzhuang Zhou*

Main category: cs.CV

TL;DR: SISRNet improves radiology report generation by focusing on medically critical regions, addressing data bias and enhancing accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing methods produce fluent but inaccurate reports due to data bias in radiology images, limiting clinical use.

Method: SISRNet identifies salient regions with fine-grained cross-modal semantics and focuses on them during image modeling and report generation.

Result: SISRNet outperforms peers on IU-Xray and MIMIC-CXR datasets, generating clinically accurate reports.

Conclusion: SISRNet effectively mitigates data bias and improves report accuracy, making it suitable for clinical practice.

Abstract: Recent advances in automated radiology report generation from chest X-rays
using deep learning algorithms have the potential to significantly reduce the
arduous workload of radiologists. However, due to the inherent massive data
bias in radiology images, where abnormalities are typically subtle and sparsely
distributed, existing methods often produce fluent yet medically inaccurate
reports, limiting their applicability in clinical practice. To address this
issue effectively, we propose a Semantically Informed Salient Regions-guided
(SISRNet) report generation method. Specifically, our approach explicitly
identifies salient regions with medically critical characteristics using
fine-grained cross-modal semantics. Then, SISRNet systematically focuses on
these high-information regions during both image modeling and report
generation, effectively capturing subtle abnormal findings, mitigating the
negative impact of data bias, and ultimately generating clinically accurate
reports. Compared to its peers, SISRNet demonstrates superior performance on
widely used IU-Xray and MIMIC-CXR datasets.

</details>


### [24] [Human-Guided Shade Artifact Suppression in CBCT-to-MDCT Translation via Schrödinger Bridge with Conditional Diffusion](https://arxiv.org/abs/2507.11025)
*Sung Ho Kang,Hyun-Cheol Park*

Main category: cs.CV

TL;DR: A novel CBCT-to-MDCT translation framework using Schrodinger Bridge, GAN priors, and human-guided diffusion, ensuring anatomical fidelity and clinical preference alignment with fewer sampling steps.


<details>
  <summary>Details</summary>
Motivation: To improve CBCT-to-MDCT translation by integrating human feedback and enforcing boundary consistency, addressing limitations of conventional GANs and diffusion models.

Method: Combines GAN-derived priors with human-guided conditional diffusion, uses classifier-free guidance (CFG) for binary feedback, and employs iterative refinement with tournament-based preference selection.

Result: Outperforms prior methods in RMSE, SSIM, LPIPS, and Dice metrics, reduces shade artifacts, and preserves structural details with only 10 sampling steps.

Conclusion: The framework is effective and efficient for real-time, preference-aligned medical image translation.

Abstract: We present a novel framework for CBCT-to-MDCT translation, grounded in the
Schrodinger Bridge (SB) formulation, which integrates GAN-derived priors with
human-guided conditional diffusion. Unlike conventional GANs or diffusion
models, our approach explicitly enforces boundary consistency between CBCT
inputs and pseudo targets, ensuring both anatomical fidelity and perceptual
controllability. Binary human feedback is incorporated via classifier-free
guidance (CFG), effectively steering the generative process toward clinically
preferred outcomes. Through iterative refinement and tournament-based
preference selection, the model internalizes human preferences without relying
on a reward model. Subtraction image visualizations reveal that the proposed
method selectively attenuates shade artifacts in key anatomical regions while
preserving fine structural detail. Quantitative evaluations further demonstrate
superior performance across RMSE, SSIM, LPIPS, and Dice metrics on clinical
datasets -- outperforming prior GAN- and fine-tuning-based feedback methods --
while requiring only 10 sampling steps. These findings underscore the
effectiveness and efficiency of our framework for real-time, preference-aligned
medical image translation.

</details>


### [25] [Personalized OVSS: Understanding Personal Concept in Open-Vocabulary Semantic Segmentation](https://arxiv.org/abs/2507.11030)
*Sunghyun Park,Jungsoo Lee,Shubhankar Borse,Munawar Hayat,Sungha Choi,Kyuwoong Hwang,Fatih Porikli*

Main category: cs.CV

TL;DR: The paper introduces personalized open-vocabulary semantic segmentation (OVSS) to recognize user-specific concepts (e.g., 'my mug cup') using text prompt tuning and negative mask proposals, without degrading original OVSS performance.


<details>
  <summary>Details</summary>
Motivation: Current OVSS fails to segment user-specific text descriptions (e.g., 'my mug cup'). The paper aims to address this gap by enabling recognition of personalized visual concepts.

Method: Proposes a plug-in method using text prompt tuning and 'negative mask proposals' to reduce false predictions. Visual embeddings of personal concepts are injected into text prompts for enriched representation.

Result: The method outperforms on new benchmarks (FSS$^\text{per}$, CUB$^\text{per}$, ADE$^\text{per}$) while maintaining original OVSS performance.

Conclusion: The approach successfully enhances personalized OVSS without compromising the original task, validated by superior benchmark performance.

Abstract: While open-vocabulary semantic segmentation (OVSS) can segment an image into
semantic regions based on arbitrarily given text descriptions even for classes
unseen during training, it fails to understand personal texts (e.g., `my mug
cup') for segmenting regions of specific interest to users. This paper
addresses challenges like recognizing `my mug cup' among `multiple mug cups'.
To overcome this challenge, we introduce a novel task termed
\textit{personalized open-vocabulary semantic segmentation} and propose a text
prompt tuning-based plug-in method designed to recognize personal visual
concepts using a few pairs of images and masks, while maintaining the
performance of the original OVSS. Based on the observation that reducing false
predictions is essential when applying text prompt tuning to this task, our
proposed method employs `negative mask proposal' that captures visual concepts
other than the personalized concept. We further improve the performance by
enriching the representation of text prompts by injecting visual embeddings of
the personal concept into them. This approach enhances personalized OVSS
without compromising the original OVSS performance. We demonstrate the
superiority of our method on our newly established benchmarks for this task,
including FSS$^\text{per}$, CUB$^\text{per}$, and ADE$^\text{per}$.

</details>


### [26] [Efficient Dual-domain Image Dehazing with Haze Prior Perception](https://arxiv.org/abs/2507.11035)
*Lirong Zheng,Yanshan Li,Rui Yu,Kaihao Zhang*

Main category: cs.CV

TL;DR: DGFDNet introduces a dual-domain framework for image dehazing, combining spatial and frequency domains with haze-aware modulation and multi-scale feature fusion for superior performance and real-time efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with computational cost and inadequate performance under complex haze conditions due to weak coupling between spatial and frequency domains.

Method: Proposes DGFDNet with HAFM for adaptive frequency modulation and MGAM for multi-scale feature fusion, along with PCGB for iterative prior refinement.

Result: Achieves state-of-the-art performance on four benchmark datasets with robustness and real-time efficiency.

Conclusion: DGFDNet effectively addresses limitations of existing methods, offering a robust and efficient solution for image dehazing.

Abstract: Transformer-based models exhibit strong global modeling capabilities in
single-image dehazing, but their high computational cost limits real-time
applicability. Existing methods predominantly rely on spatial-domain features
to capture long-range dependencies, which are computationally expensive and
often inadequate under complex haze conditions. While some approaches introduce
frequency-domain cues, the weak coupling between spatial and frequency branches
limits the overall performance. To overcome these limitations, we propose the
Dark Channel Guided Frequency-aware Dehazing Network (DGFDNet), a novel
dual-domain framework that performs physically guided degradation alignment
across spatial and frequency domains. At its core, the DGFDBlock comprises two
key modules: 1) the Haze-Aware Frequency Modulator (HAFM), which generates a
pixel-level haze confidence map from dark channel priors to adaptively enhance
haze-relevant frequency components, thereby achieving global degradation-aware
spectral modulation; 2) the Multi-level Gating Aggregation Module (MGAM), which
fuses multi-scale features through diverse convolutional kernels and hybrid
gating mechanisms to recover fine structural details. Additionally, a Prior
Correction Guidance Branch (PCGB) incorporates a closed-loop feedback
mechanism, enabling iterative refinement of the prior by intermediate dehazed
features and significantly improving haze localization accuracy, especially in
challenging outdoor scenes. Extensive experiments on four benchmark haze
datasets demonstrate that DGFDNet achieves state-of-the-art performance with
superior robustness and real-time efficiency. Code is available at:
https://github.com/Dilizlr/DGFDNet.

</details>


### [27] [A Multi-View High-Resolution Foot-Ankle Complex Point Cloud Dataset During Gait for Occlusion-Robust 3D Completion](https://arxiv.org/abs/2507.11037)
*Jie-Wen Li,Zi-Han Ye,Qingyuan Zhou,Jiayi Song,Ying He,Ben Fei,Wen-Ming Chen*

Main category: cs.CV

TL;DR: FootGait3D is a high-resolution multi-view dataset of ankle-foot point clouds during gait, designed for 3D shape completion tasks and biomechanical research.


<details>
  <summary>Details</summary>
Motivation: Accurate surface geometry data of the foot-ankle complex during gait is challenging due to occlusions and viewing limitations, necessitating a specialized dataset.

Method: FootGait3D includes 8,403 point cloud frames from 46 subjects, captured using a five-camera depth sensing system, with complete and partial views for evaluation.

Result: The dataset enables rigorous testing of 3D point cloud completion methods under varying occlusion levels and viewpoints.

Conclusion: FootGait3D advances biomechanics and clinical applications by providing detailed 3D foot models for gait analysis, prosthetics, and robotics.

Abstract: The kinematics analysis of foot-ankle complex during gait is essential for
advancing biomechanical research and clinical assessment. Collecting accurate
surface geometry data from the foot and ankle during dynamic gait conditions is
inherently challenging due to swing foot occlusions and viewing limitations.
Thus, this paper introduces FootGait3D, a novel multi-view dataset of
high-resolution ankle-foot surface point clouds captured during natural gait.
Different from existing gait datasets that typically target whole-body or
lower-limb motion, FootGait3D focuses specifically on the detailed modeling of
the ankle-foot region, offering a finer granularity of motion data. To address
this, FootGait3D consists of 8,403 point cloud frames collected from 46
subjects using a custom five-camera depth sensing system. Each frame includes a
complete 5-view reconstruction of the foot and ankle (serving as ground truth)
along with partial point clouds obtained from only four, three, or two views.
This structured variation enables rigorous evaluation of 3D point cloud
completion methods under varying occlusion levels and viewpoints. Our dataset
is designed for shape completion tasks, facilitating the benchmarking of
state-of-the-art single-modal (e.g., PointTr, SnowflakeNet, Anchorformer) and
multi-modal (e.g., SVDFormer, PointSea, CSDN) completion networks on the
challenge of recovering the full foot geometry from occluded inputs. FootGait3D
has significant potential to advance research in biomechanics and multi-segment
foot modeling, offering a valuable testbed for clinical gait analysis,
prosthetic design, and robotics applications requiring detailed 3D models of
the foot during motion. The dataset is now available at
https://huggingface.co/datasets/ljw285/FootGait3D.

</details>


### [28] [Combining Transformers and CNNs for Efficient Object Detection in High-Resolution Satellite Imagery](https://arxiv.org/abs/2507.11040)
*Nicolas Drapier,Aladine Chetouani,Aurélien Chateigner*

Main category: cs.CV

TL;DR: GLOD is a transformer-based architecture for satellite object detection, outperforming SOTA by 11.46% on xView with innovations like Swin Transformer and UpConvMixer blocks.


<details>
  <summary>Details</summary>
Motivation: To address challenges in high-resolution satellite imagery object detection by replacing CNN backbones with transformers for better feature extraction.

Method: Uses Swin Transformer for end-to-end feature extraction, UpConvMixer for upsampling, and Fusion Blocks for multi-scale integration. Includes asymmetric fusion with CBAM attention and multi-path head design.

Result: Achieves 32.95% on xView, outperforming SOTA by 11.46%.

Conclusion: GLOD is optimized for satellite imagery, balancing performance and efficiency with novel architectural choices.

Abstract: We present GLOD, a transformer-first architecture for object detection in
high-resolution satellite imagery. GLOD replaces CNN backbones with a Swin
Transformer for end-to-end feature extraction, combined with novel UpConvMixer
blocks for robust upsampling and Fusion Blocks for multi-scale feature
integration. Our approach achieves 32.95\% on xView, outperforming SOTA methods
by 11.46\%. Key innovations include asymmetric fusion with CBAM attention and a
multi-path head design capturing objects across scales. The architecture is
optimized for satellite imagery challenges, leveraging spatial priors while
maintaining computational efficiency.

</details>


### [29] [Alleviating Textual Reliance in Medical Language-guided Segmentation via Prototype-driven Semantic Approximation](https://arxiv.org/abs/2507.11055)
*Shuchang Ye,Usman Naseem,Mingyuan Meng,Jinman Kim*

Main category: cs.CV

TL;DR: ProLearn introduces a prototype-driven framework to reduce reliance on paired image-text data for medical language-guided segmentation, improving performance when text is scarce.


<details>
  <summary>Details</summary>
Motivation: Current methods rely on paired image-text data, limiting their use in datasets without paired reports and clinical scenarios where segmentation precedes reporting.

Method: ProLearn uses a Prototype-driven Semantic Approximation (PSA) module to approximate semantic guidance from text, enabling segmentation without paired reports.

Result: ProLearn outperforms state-of-the-art methods on datasets like QaTa-COV19, MosMedData+, and Kvasir-SEG when text is limited.

Conclusion: ProLearn effectively reduces textual reliance, enhancing the applicability of language-guided segmentation in real-world clinical settings.

Abstract: Medical language-guided segmentation, integrating textual clinical reports as
auxiliary guidance to enhance image segmentation, has demonstrated significant
improvements over unimodal approaches. However, its inherent reliance on paired
image-text input, which we refer to as ``textual reliance", presents two
fundamental limitations: 1) many medical segmentation datasets lack paired
reports, leaving a substantial portion of image-only data underutilized for
training; and 2) inference is limited to retrospective analysis of cases with
paired reports, limiting its applicability in most clinical scenarios where
segmentation typically precedes reporting. To address these limitations, we
propose ProLearn, the first Prototype-driven Learning framework for
language-guided segmentation that fundamentally alleviates textual reliance. At
its core, in ProLearn, we introduce a novel Prototype-driven Semantic
Approximation (PSA) module to enable approximation of semantic guidance from
textual input. PSA initializes a discrete and compact prototype space by
distilling segmentation-relevant semantics from textual reports. Once
initialized, it supports a query-and-respond mechanism which approximates
semantic guidance for images without textual input, thereby alleviating textual
reliance. Extensive experiments on QaTa-COV19, MosMedData+ and Kvasir-SEG
demonstrate that ProLearn outperforms state-of-the-art language-guided methods
when limited text is available.

</details>


### [30] [Robust 3D-Masked Part-level Editing in 3D Gaussian Splatting with Regularized Score Distillation Sampling](https://arxiv.org/abs/2507.11061)
*Hayeon Kim,Ji Ha Jang,Se Young Chun*

Main category: cs.CV

TL;DR: RoMaP is a novel framework for precise local 3D Gaussian editing, addressing challenges in multi-view part segmentation and SDS loss ambiguity with 3D-GALP and regularized SDS loss.


<details>
  <summary>Details</summary>
Motivation: Precise local 3D edits are challenging due to inconsistent multi-view part segmentations and ambiguous SDS loss in Gaussian Splatting.

Method: RoMaP uses 3D-GALP for robust 3D mask generation and a regularized SDS loss with L1 anchor loss and additional regularizers like Gaussian prior removal.

Result: RoMaP achieves state-of-the-art local 3D editing on reconstructed and generated Gaussian scenes, ensuring quality and coherence.

Conclusion: RoMaP enables robust and flexible part-level 3D Gaussian editing, advancing precision in 3D content creation.

Abstract: Recent advances in 3D neural representations and instance-level editing
models have enabled the efficient creation of high-quality 3D content. However,
achieving precise local 3D edits remains challenging, especially for Gaussian
Splatting, due to inconsistent multi-view 2D part segmentations and inherently
ambiguous nature of Score Distillation Sampling (SDS) loss. To address these
limitations, we propose RoMaP, a novel local 3D Gaussian editing framework that
enables precise and drastic part-level modifications. First, we introduce a
robust 3D mask generation module with our 3D-Geometry Aware Label Prediction
(3D-GALP), which uses spherical harmonics (SH) coefficients to model
view-dependent label variations and soft-label property, yielding accurate and
consistent part segmentations across viewpoints. Second, we propose a
regularized SDS loss that combines the standard SDS loss with additional
regularizers. In particular, an L1 anchor loss is introduced via our Scheduled
Latent Mixing and Part (SLaMP) editing method, which generates high-quality
part-edited 2D images and confines modifications only to the target region
while preserving contextual coherence. Additional regularizers, such as
Gaussian prior removal, further improve flexibility by allowing changes beyond
the existing context, and robust 3D masking prevents unintended edits.
Experimental results demonstrate that our RoMaP achieves state-of-the-art local
3D editing on both reconstructed and generated Gaussian scenes and objects
qualitatively and quantitatively, making it possible for more robust and
flexible part-level 3D Gaussian editing.

</details>


### [31] [Joint angle model based learning to refine kinematic human pose estimation](https://arxiv.org/abs/2507.11075)
*Chang Peng,Yifei Zhou,Huifeng Xi,Shiqing Huang,Chuangye Chen,Jianming Yang,Bao Yang,Zhenyu Jiang*

Main category: cs.CV

TL;DR: A novel joint angle-based method improves marker-free human pose estimation by refining keypoint recognition and smoothing trajectories, outperforming existing models in challenging cases.


<details>
  <summary>Details</summary>
Motivation: Current HPE methods suffer from errors in keypoint recognition and trajectory fluctuations due to inaccurate manually annotated training datasets.

Method: Proposes joint angle-based modeling, approximating joint angle variations with high-order Fourier series, and using a bidirectional recurrent network for refinement.

Result: The method outperforms state-of-the-art HPE refinement networks, especially in challenging scenarios like figure skating and breaking.

Conclusion: Joint angle-based refinement (JAR) effectively addresses limitations in HPE, offering robust and accurate pose estimation.

Abstract: Marker-free human pose estimation (HPE) has found increasing applications in
various fields. Current HPE suffers from occasional errors in keypoint
recognition and random fluctuation in keypoint trajectories when analyzing
kinematic human poses. The performance of existing deep learning-based models
for HPE refinement is considerably limited by inaccurate training datasets in
which the keypoints are manually annotated. This paper proposed a novel method
to overcome the difficulty through joint angle-based modeling. The key
techniques include: (i) A joint angle-based model of human pose, which is
robust to describe kinematic human poses; (ii) Approximating temporal variation
of joint angles through high order Fourier series to get reliable "ground
truth"; (iii) A bidirectional recurrent network is designed as a
post-processing module to refine the estimation of well-established HRNet.
Trained with the high-quality dataset constructed using our method, the network
demonstrates outstanding performance to correct wrongly recognized joints and
smooth their spatiotemporal trajectories. Tests show that joint angle-based
refinement (JAR) outperforms the state-of-the-art HPE refinement network in
challenging cases like figure skating and breaking.

</details>


### [32] [GKNet: Graph-based Keypoints Network for Monocular Pose Estimation of Non-cooperative Spacecraft](https://arxiv.org/abs/2507.11077)
*Weizhao Ma,Dong Zhou,Yuhui Hu,Zipeng He*

Main category: cs.CV

TL;DR: A graph-based keypoints network (GKNet) is proposed for accurate monocular pose estimation of non-cooperative spacecraft, addressing challenges like symmetry and occlusion. A new dataset (SKD) is introduced for validation.


<details>
  <summary>Details</summary>
Motivation: High accuracy in pose estimation is crucial for on-orbit services, but current keypoint detectors struggle with structural symmetry and occlusion.

Method: GKNet uses geometric constraints of keypoints graphs to improve detection. A dataset (SKD) with 90,000 simulated images and annotations is created for validation.

Result: GKNet outperforms state-of-the-art detectors in accuracy and effectiveness, as shown by experiments and ablation studies.

Conclusion: GKNet and SKD dataset provide a robust solution for spacecraft pose estimation, with code and data publicly available.

Abstract: Monocular pose estimation of non-cooperative spacecraft is significant for
on-orbit service (OOS) tasks, such as satellite maintenance, space debris
removal, and station assembly. Considering the high demands on pose estimation
accuracy, mainstream monocular pose estimation methods typically consist of
keypoint detectors and PnP solver. However, current keypoint detectors remain
vulnerable to structural symmetry and partial occlusion of non-cooperative
spacecraft. To this end, we propose a graph-based keypoints network for the
monocular pose estimation of non-cooperative spacecraft, GKNet, which leverages
the geometric constraint of keypoints graph. In order to better validate
keypoint detectors, we present a moderate-scale dataset for the spacecraft
keypoint detection, named SKD, which consists of 3 spacecraft targets, 90,000
simulated images, and corresponding high-precise keypoint annotations.
Extensive experiments and an ablation study have demonstrated the high accuracy
and effectiveness of our GKNet, compared to the state-of-the-art spacecraft
keypoint detectors. The code for GKNet and the SKD dataset is available at
https://github.com/Dongzhou-1996/GKNet.

</details>


### [33] [Automatic Road Subsurface Distress Recognition from Ground Penetrating Radar Images using Deep Learning-based Cross-verification](https://arxiv.org/abs/2507.11081)
*Chang Peng,Bao Yang,Meiqi Li,Ge Zhang,Hui Sun,Zhenyu Jiang*

Main category: cs.CV

TL;DR: A novel cross-verification strategy using YOLO and a rigorously validated 3D GPR dataset improves RSD recognition accuracy to over 98.6% recall, reducing inspection labor by 90%.


<details>
  <summary>Details</summary>
Motivation: Manual RSD recognition from GPR images is labor-intensive and expertise-dependent, while current deep learning methods suffer from limited datasets and network capability.

Method: Constructed a 3D GPR dataset with 2134 samples and proposed a cross-verification strategy leveraging YOLO's varying sensitivity to RSD types.

Result: Achieved over 98.6% recall in field tests, significantly outperforming previous methods.

Conclusion: The approach enables highly accurate, automated RSD detection, reducing manual inspection effort by 90%.

Abstract: Ground penetrating radar (GPR) has become a rapid and non-destructive
solution for road subsurface distress (RSD) detection. However, RSD recognition
from GPR images is labor-intensive and heavily relies on inspectors' expertise.
Deep learning offers the possibility for automatic RSD recognition, but its
current performance is limited by two factors: Scarcity of high-quality dataset
for network training and insufficient capability of network to distinguish RSD.
In this study, a rigorously validated 3D GPR dataset containing 2134 samples of
diverse types was constructed through field scanning. Based on the finding that
the YOLO model trained with one of the three scans of GPR images exhibits
varying sensitivity to specific type of RSD, we proposed a novel
cross-verification strategy with outstanding accuracy in RSD recognition,
achieving recall over 98.6% in field tests. The approach, integrated into an
online RSD detection system, can reduce the labor of inspection by around 90%.

</details>


### [34] [Atmos-Bench: 3D Atmospheric Structures for Climate Insight](https://arxiv.org/abs/2507.11085)
*Tianchi Xu*

Main category: cs.CV

TL;DR: Atmos-Bench introduces the first 3D atmospheric benchmark and a novel FourCastX network for improved atmospheric structure recovery from satellite LiDAR data, outperforming existing methods without auxiliary inputs.


<details>
  <summary>Details</summary>
Motivation: Existing methods for atmospheric structure recovery rely on simplified physics and lack standardized benchmarks, leading to uncertainties and incomplete capture of radiative effects.

Method: Developed Atmos-Bench, a 3D benchmark, and FourCastX, a frequency-enhanced spatio-temporal network, embedding physical constraints for energy-consistent restoration.

Result: Achieved consistent improvements on Atmos-Bench, outperforming state-of-the-art models without auxiliary inputs.

Conclusion: Atmos-Bench sets a new standard for 3D atmospheric recovery, advancing climate understanding.

Abstract: Atmospheric structure, represented by backscatter coefficients (BC) recovered
from satellite LiDAR attenuated backscatter (ATB), provides a volumetric view
of clouds, aerosols, and molecules, playing a critical role in human
activities, climate understanding, and extreme weather forecasting. Existing
methods often rely on auxiliary inputs and simplified physics-based
approximations, and lack a standardized 3D benchmark for fair evaluation.
However, such approaches may introduce additional uncertainties and
insufficiently capture realistic radiative transfer and atmospheric
scattering-absorption effects. To bridge these gaps, we present Atmos-Bench:
the first 3D atmospheric benchmark, along with a novel FourCastX:
Frequency-enhanced Spatio-Temporal Mixture-of-Experts Network that (a)
generates 921,600 image slices from 3D scattering volumes simulated at 532 nm
and 355 nm by coupling WRF with an enhanced COSP simulator over 384 land-ocean
time steps, yielding high-quality voxel-wise references; (b) embeds ATB-BC
physical constraints into the model architecture, promoting energy consistency
during restoration; (c) achieves consistent improvements on the Atmos-Bench
dataset across both 355 nm and 532 nm bands, outperforming state-of-the-art
baseline models without relying on auxiliary inputs. Atmos-Bench establishes a
new standard for satellite-based 3D atmospheric structure recovery and paves
the way for deeper climate insight.

</details>


### [35] [A Survey on Interpretability in Visual Recognition](https://arxiv.org/abs/2507.11099)
*Qiyang Wan,Chengzhi Gao,Ruiping Wang,Xilin Chen*

Main category: cs.CV

TL;DR: This paper reviews and categorizes interpretability methods for visual recognition models, proposing a human-centered taxonomy and discussing evaluation metrics and future opportunities.


<details>
  <summary>Details</summary>
Motivation: To understand and deploy visual recognition models effectively in critical applications like autonomous driving and medical diagnostics by advancing interpretability research.

Method: Systematic review and taxonomy proposal based on Intent, Object, Presentation, and Methodology, with evaluation metric analysis.

Result: A coherent taxonomy for interpretable recognition methods and insights into evaluation metrics and new technological opportunities.

Conclusion: The paper organizes existing research and inspires future work on interpretability in visual recognition models.

Abstract: In recent years, visual recognition methods have advanced significantly,
finding applications across diverse fields. While researchers seek to
understand the mechanisms behind the success of these models, there is also a
growing impetus to deploy them in critical areas like autonomous driving and
medical diagnostics to better diagnose failures, which promotes the development
of interpretability research. This paper systematically reviews existing
research on the interpretability of visual recognition models and proposes a
taxonomy of methods from a human-centered perspective. The proposed taxonomy
categorizes interpretable recognition methods based on Intent, Object,
Presentation, and Methodology, thereby establishing a systematic and coherent
set of grouping criteria for these XAI methods. Additionally, we summarize the
requirements for evaluation metrics and explore new opportunities enabled by
recent technologies, such as large multimodal models. We aim to organize
existing research in this domain and inspire future investigations into the
interpretability of visual recognition models.

</details>


### [36] [KptLLM++: Towards Generic Keypoint Comprehension with Large Language Model](https://arxiv.org/abs/2507.11102)
*Jie Yang,Wang Zeng,Sheng Jin,Lumin Xu,Wentao Liu,Chen Qian,Zhen Li,Ruimao Zhang*

Main category: cs.CV

TL;DR: KptLLM++ is a multimodal large language model designed for fine-grained keypoint comprehension, improving accuracy and generalization in tasks like object retrieval and behavior recognition.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs struggle with fine-grained semantic information, such as keypoint identification, which is crucial for applications like image analysis and behavior recognition.

Method: KptLLM++ uses an identify-then-detect paradigm with structured chain-of-thought reasoning and is trained on a large dataset of 500K diverse samples.

Result: The model achieves state-of-the-art performance on keypoint detection benchmarks, demonstrating high accuracy and generalization.

Conclusion: KptLLM++ offers a unified solution for fine-grained image understanding and enhances human-AI collaboration.

Abstract: The emergence of Multimodal Large Language Models (MLLMs) has revolutionized
image understanding by bridging textual and visual modalities. However, these
models often struggle with capturing fine-grained semantic information, such as
the precise identification and analysis of object keypoints. Keypoints, as
structure-aware, pixel-level, and compact representations of objects,
particularly articulated ones, play a crucial role in applications such as
fine-grained image analysis, object retrieval, and behavior recognition. In
this paper, we propose KptLLM++, a novel multimodal large language model that
specifically designed for generic keypoint comprehension through the
integration of diverse input modalities guided by user-defined instructions. By
unifying keypoint detection across varied contexts, KptLLM++ establishes itself
as an advanced interface, fostering more effective human-AI collaboration. The
model is built upon a novel identify-then-detect paradigm, which first
interprets keypoint semantics and subsequently localizes their precise
positions through a structured chain-of-thought reasoning mechanism. To push
the boundaries of performance, we have scaled up the training dataset to over
500K samples, encompassing diverse objects, keypoint categories, image styles,
and scenarios with complex occlusions. This extensive scaling enables KptLLM++
to unlock its potential, achieving remarkable accuracy and generalization.
Comprehensive experiments on multiple keypoint detection benchmarks demonstrate
its state-of-the-art performance, underscoring its potential as a unified
solution for fine-grained image understanding and its transformative
implications for human-AI interaction.

</details>


### [37] [Jellyfish Species Identification: A CNN Based Artificial Neural Network Approach](https://arxiv.org/abs/2507.11116)
*Md. Sabbir Hossen,Md. Saiduzzaman,Pabon Shaha,Mostofa Kamal Nasir*

Main category: cs.CV

TL;DR: A deep learning framework for jellyfish species detection achieves 98% accuracy using MobileNetV3 and hybrid classifiers.


<details>
  <summary>Details</summary>
Motivation: Accurate jellyfish species identification is vital for ecological monitoring due to their ecological impact and rapid proliferation.

Method: Combines MobileNetV3, ResNet50, EfficientNetV2-B0, VGG16 with traditional ML and Feedforward Neural Network classifiers, using softmax for direct classification.

Result: MobileNetV3 with Artificial Neural Network achieves 98% accuracy, outperforming other combinations.

Conclusion: Deep learning and hybrid frameworks effectively address biodiversity challenges in marine species detection.

Abstract: Jellyfish, a diverse group of gelatinous marine organisms, play a crucial
role in maintaining marine ecosystems but pose significant challenges for
biodiversity and conservation due to their rapid proliferation and ecological
impact. Accurate identification of jellyfish species is essential for
ecological monitoring and management. In this study, we proposed a deep
learning framework for jellyfish species detection and classification using an
underwater image dataset. The framework integrates advanced feature extraction
techniques, including MobileNetV3, ResNet50, EfficientNetV2-B0, and VGG16,
combined with seven traditional machine learning classifiers and three
Feedforward Neural Network classifiers for precise species identification.
Additionally, we activated the softmax function to directly classify jellyfish
species using the convolutional neural network models. The combination of the
Artificial Neural Network with MobileNetV3 is our best-performing model,
achieving an exceptional accuracy of 98%, significantly outperforming other
feature extractor-classifier combinations. This study demonstrates the efficacy
of deep learning and hybrid frameworks in addressing biodiversity challenges
and advancing species detection in marine environments.

</details>


### [38] [Try Harder: Hard Sample Generation and Learning for Clothes-Changing Person Re-ID](https://arxiv.org/abs/2507.11119)
*Hankun Liu,Yujian Zhao,Guanglin Niu*

Main category: cs.CV

TL;DR: The paper introduces HSGL, a multimodal framework for defining, generating, and optimizing hard samples in clothing-changing person Re-ID, improving robustness and performance.


<details>
  <summary>Details</summary>
Motivation: Hard samples in CC-ReID are ambiguous and lack explicit definitions, limiting learning strategies and model robustness.

Method: HSGL combines Dual-Granularity Hard Sample Generation (DGHSG) for synthesizing diverse hard samples and Hard Sample Adaptive Learning (HSAL) for hardness-aware optimization.

Result: HSGL achieves state-of-the-art performance on PRCC and LTCC datasets, accelerating convergence.

Conclusion: Multimodal-guided hard sample generation and learning enhances CC-ReID robustness and discriminative capability.

Abstract: Hard samples pose a significant challenge in person re-identification (ReID)
tasks, particularly in clothing-changing person Re-ID (CC-ReID). Their inherent
ambiguity or similarity, coupled with the lack of explicit definitions, makes
them a fundamental bottleneck. These issues not only limit the design of
targeted learning strategies but also diminish the model's robustness under
clothing or viewpoint changes. In this paper, we propose a novel
multimodal-guided Hard Sample Generation and Learning (HSGL) framework, which
is the first effort to unify textual and visual modalities to explicitly
define, generate, and optimize hard samples within a unified paradigm. HSGL
comprises two core components: (1) Dual-Granularity Hard Sample Generation
(DGHSG), which leverages multimodal cues to synthesize semantically consistent
samples, including both coarse- and fine-grained hard positives and negatives
for effectively increasing the hardness and diversity of the training data. (2)
Hard Sample Adaptive Learning (HSAL), which introduces a hardness-aware
optimization strategy that adjusts feature distances based on textual semantic
labels, encouraging the separation of hard positives and drawing hard negatives
closer in the embedding space to enhance the model's discriminative capability
and robustness to hard samples. Extensive experiments on multiple CC-ReID
benchmarks demonstrate the effectiveness of our approach and highlight the
potential of multimodal-guided hard sample generation and learning for robust
CC-ReID. Notably, HSAL significantly accelerates the convergence of the
targeted learning procedure and achieves state-of-the-art performance on both
PRCC and LTCC datasets. The code is available at
https://github.com/undooo/TryHarder-ACMMM25.

</details>


### [39] [MMOne: Representing Multiple Modalities in One Scene](https://arxiv.org/abs/2507.11129)
*Zhifeng Gu,Bing Wang*

Main category: cs.CV

TL;DR: The paper proposes MMOne, a framework for multimodal scene representation, addressing modality conflicts like property and granularity disparities through a modality modeling module and decomposition mechanism.


<details>
  <summary>Details</summary>
Motivation: Humans perceive the world through multimodal cues, but modality conflicts hinder effective scene representation. The goal is to enhance comprehension by addressing these disparities.

Method: MMOne uses a modality modeling module with a novel indicator and a decomposition mechanism to separate multimodal Gaussians into single-modal ones, disentangling shared and modality-specific components.

Result: Experiments show improved representation capability for each modality and scalability to additional modalities.

Conclusion: MMOne effectively addresses modality conflicts, providing a compact and efficient multimodal scene representation.

Abstract: Humans perceive the world through multimodal cues to understand and interact
with the environment. Learning a scene representation for multiple modalities
enhances comprehension of the physical world. However, modality conflicts,
arising from inherent distinctions among different modalities, present two
critical challenges: property disparity and granularity disparity. To address
these challenges, we propose a general framework, MMOne, to represent multiple
modalities in one scene, which can be readily extended to additional
modalities. Specifically, a modality modeling module with a novel modality
indicator is proposed to capture the unique properties of each modality.
Additionally, we design a multimodal decomposition mechanism to separate
multi-modal Gaussians into single-modal Gaussians based on modality
differences. We address the essential distinctions among modalities by
disentangling multimodal information into shared and modality-specific
components, resulting in a more compact and efficient multimodal scene
representation. Extensive experiments demonstrate that our method consistently
enhances the representation capability for each modality and is scalable to
additional modalities. The code is available at
https://github.com/Neal2020GitHub/MMOne.

</details>


### [40] [RMAU-NET: A Residual-Multihead-Attention U-Net Architecture for Landslide Segmentation and Detection from Remote Sensing Images](https://arxiv.org/abs/2507.11143)
*Lam Pham,Cam Le,Hieu Tang,Khang Truong,Truong Nguyen,Jasmin Lampert,Alexander Schindler,Martin Boyer,Son Phan*

Main category: cs.CV

TL;DR: A deep-learning model for landslide detection and segmentation using remote sensing images, achieving high F1 and mIoU scores on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: Frequent landslide disasters due to extreme weather and human activities, coupled with challenges in large-scale observation, drive the need for automated solutions.

Method: Proposes an end-to-end deep-learning model for landslide detection and segmentation, leveraging remote sensing images.

Result: Achieves F1 scores of 98.23 and 93.83 for detection, and mIoU scores of 63.74 and 76.88 for segmentation on benchmark datasets.

Conclusion: The model shows potential for real-life landslide observation systems.

Abstract: In recent years, landslide disasters have reported frequently due to the
extreme weather events of droughts, floods , storms, or the consequence of
human activities such as deforestation, excessive exploitation of natural
resources. However, automatically observing landslide is challenging due to the
extremely large observing area and the rugged topography such as mountain or
highland. This motivates us to propose an end-to-end deep-learning-based model
which explores the remote sensing images for automatically observing landslide
events. By considering remote sensing images as the input data, we can obtain
free resource, observe large and rough terrains by time. To explore the remote
sensing images, we proposed a novel neural network architecture which is for
two tasks of landslide detection and landslide segmentation. We evaluated our
proposed model on three different benchmark datasets of LandSlide4Sense, Bijie,
and Nepal. By conducting extensive experiments, we achieve F1 scores of 98.23,
93.83 for the landslide detection task on LandSlide4Sense, Bijie datasets; mIoU
scores of 63.74, 76.88 on the segmentation tasks regarding LandSlide4Sense,
Nepal datasets. These experimental results prove potential to integrate our
proposed model into real-life landslide observation systems.

</details>


### [41] [Assessing Color Vision Test in Large Vision-language Models](https://arxiv.org/abs/2507.11153)
*Hongfei Ye,Bin Chen,Wenxi Liu,Yu Zhang,Zhao Li,Dandan Ni,Hongyang Chen*

Main category: cs.CV

TL;DR: The paper explores color vision abilities in large vision-language models, introduces a testing task, and proposes fine-tuning strategies to improve performance.


<details>
  <summary>Details</summary>
Motivation: The color vision capabilities of large vision-language models are understudied, prompting the need for a dedicated testing framework.

Method: A color vision testing task is defined, and a dataset with diverse question categories and difficulty levels is constructed. Error analysis and fine-tuning strategies are proposed.

Result: The study identifies common errors in models and suggests ways to enhance their color vision performance.

Conclusion: Fine-tuning strategies can improve color vision abilities in large vision-language models, addressing a previously unexplored gap.

Abstract: With the widespread adoption of large vision-language models, the capacity
for color vision in these models is crucial. However, the color vision
abilities of large visual-language models have not yet been thoroughly
explored. To address this gap, we define a color vision testing task for large
vision-language models and construct a dataset \footnote{Anonymous Github
Showing some of the data
https://anonymous.4open.science/r/color-vision-test-dataset-3BCD} that covers
multiple categories of test questions and tasks of varying difficulty levels.
Furthermore, we analyze the types of errors made by large vision-language
models and propose fine-tuning strategies to enhance their performance in color
vision tests.

</details>


### [42] [Clustering-Guided Multi-Layer Contrastive Representation Learning for Citrus Disease Classification](https://arxiv.org/abs/2507.11171)
*Jun Chen,Yonghua Yu,Weifu Li,Yaohui Chen,Hong Chen*

Main category: cs.CV

TL;DR: A novel self-supervised learning method (CMCRL) for citrus disease detection outperforms existing methods by 4.5%-30.1% accuracy, using unannotated samples and hierarchical feature learning.


<details>
  <summary>Details</summary>
Motivation: Citrus diseases cause significant yield losses, and accurate detection is crucial for targeted control. Current deep learning methods require extensive labeled data, which is costly.

Method: Proposes CMCRL, combining clustering-guided self-supervised learning and multi-layer contrastive training (MCT) to leverage unannotated samples and hierarchical features.

Result: Achieves state-of-the-art performance on the CDD dataset, reducing the gap with fully supervised methods and excelling in F1 score, precision, and recall.

Conclusion: CMCRL offers a robust, efficient solution for citrus disease detection, reducing reliance on labeled data while maintaining high accuracy.

Abstract: Citrus, as one of the most economically important fruit crops globally,
suffers severe yield depressions due to various diseases. Accurate disease
detection and classification serve as critical prerequisites for implementing
targeted control measures. Recent advancements in artificial intelligence,
particularly deep learning-based computer vision algorithms, have substantially
decreased time and labor requirements while maintaining the accuracy of
detection and classification. Nevertheless, these methods predominantly rely on
massive, high-quality annotated training examples to attain promising
performance. By introducing two key designs: contrasting with cluster centroids
and a multi-layer contrastive training (MCT) paradigm, this paper proposes a
novel clustering-guided self-supervised multi-layer contrastive representation
learning (CMCRL) algorithm. The proposed method demonstrates several advantages
over existing counterparts: (1) optimizing with massive unannotated samples;
(2) effective adaptation to the symptom similarity across distinct citrus
diseases; (3) hierarchical feature representation learning. The proposed method
achieves state-of-the-art performance on the public citrus image set CDD,
outperforming existing methods by 4.5\%-30.1\% accuracy. Remarkably, our method
narrows the performance gap with fully supervised counterparts (all samples are
labeled). Beyond classification accuracy, our method shows great performance on
other evaluation metrics (F1 score, precision, and recall), highlighting the
robustness against the class imbalance challenge.

</details>


### [43] [How Far Have Medical Vision-Language Models Come? A Comprehensive Benchmarking Study](https://arxiv.org/abs/2507.11200)
*Che Liu,Jiazhen Pan,Weixiang Shen,Wenjia Bai,Daniel Rueckert,Rossella Arcucci*

Main category: cs.CV

TL;DR: VLMs show promise in medical tasks but lag in reasoning and reliability, with general-purpose models sometimes outperforming medical-specific ones.


<details>
  <summary>Details</summary>
Motivation: To evaluate the competence of VLMs in medical tasks and compare general-purpose vs. medically specialized models.

Method: Comprehensive evaluation of VLMs (3B to 72B parameters) across eight benchmarks, analyzing understanding and reasoning separately.

Result: General-purpose VLMs match or surpass medical-specific ones in some tasks, but reasoning lags behind understanding, and reliability for clinical use is insufficient.

Conclusion: Stronger multimodal alignment and rigorous evaluation are needed to achieve reliable clinical deployment of VLMs.

Abstract: Vision-Language Models (VLMs) trained on web-scale corpora excel at natural
image tasks and are increasingly repurposed for healthcare; however, their
competence in medical tasks remains underexplored. We present a comprehensive
evaluation of open-source general-purpose and medically specialised VLMs,
ranging from 3B to 72B parameters, across eight benchmarks: MedXpert,
OmniMedVQA, PMC-VQA, PathVQA, MMMU, SLAKE, and VQA-RAD. To observe model
performance across different aspects, we first separate it into understanding
and reasoning components. Three salient findings emerge. First, large
general-purpose models already match or surpass medical-specific counterparts
on several benchmarks, demonstrating strong zero-shot transfer from natural to
medical images. Second, reasoning performance is consistently lower than
understanding, highlighting a critical barrier to safe decision support. Third,
performance varies widely across benchmarks, reflecting differences in task
design, annotation quality, and knowledge demands. No model yet reaches the
reliability threshold for clinical deployment, underscoring the need for
stronger multimodal alignment and more rigorous, fine-grained evaluation
protocols.

</details>


### [44] [A Robust Incomplete Multimodal Low-Rank Adaptation Approach for Emotion Recognition](https://arxiv.org/abs/2507.11202)
*Xinkui Zhao,Jinsong Shu,Yangyang Wu,Guanjie Cheng,Zihe Liu,Naibo Wang,Shuiguang Deng,Zhongle Xie,Jianwei Yin*

Main category: cs.CV

TL;DR: MCULoRA is a novel framework for incomplete multimodal learning, addressing gradient conflicts in MER by decoupling shared and distinct modality information and dynamically fine-tuning training ratios.


<details>
  <summary>Details</summary>
Motivation: Existing MER methods struggle with incomplete modalities due to conflicting gradients from different modality combinations, degrading performance.

Method: MCULoRA uses two modules: MCLA for decoupling shared/distinct modality information and DPFT for dynamic fine-tuning of training ratios based on modality separability.

Result: MCULoRA outperforms previous methods in downstream task accuracy across multiple benchmark datasets.

Conclusion: MCULoRA effectively improves incomplete multimodal learning by optimizing modality combination training and representation separability.

Abstract: Multimodal Emotion Recognition (MER) often encounters incomplete
multimodality in practical applications due to sensor failures or privacy
protection requirements. While existing methods attempt to address various
incomplete multimodal scenarios by balancing the training of each modality
combination through additional gradients, these approaches face a critical
limitation: training gradients from different modality combinations conflict
with each other, ultimately degrading the performance of the final prediction
model. In this paper, we propose a unimodal decoupled dynamic low-rank
adaptation method based on modality combinations, named MCULoRA, which is a
novel framework for the parameter-efficient training of incomplete multimodal
learning models. MCULoRA consists of two key modules, modality combination
aware low-rank adaptation (MCLA) and dynamic parameter fine-tuning (DPFT). The
MCLA module effectively decouples the shared information from the distinct
characteristics of individual modality combinations. The DPFT module adjusts
the training ratio of modality combinations based on the separability of each
modality's representation space, optimizing the learning efficiency across
different modality combinations. Our extensive experimental evaluation in
multiple benchmark datasets demonstrates that MCULoRA substantially outperforms
previous incomplete multimodal learning approaches in downstream task accuracy.

</details>


### [45] [NarrLV: Towards a Comprehensive Narrative-Centric Evaluation for Long Video Generation Models](https://arxiv.org/abs/2507.11245)
*X. Feng,H. Yu,M. Wu,S. Hu,J. Chen,C. Zhu,J. Wu,X. Chu,K. Huang*

Main category: cs.CV

TL;DR: The paper introduces NarrLV, the first benchmark for evaluating narrative expression in long video generation models, using Temporal Narrative Atoms (TNAs) and a novel MLLM-based metric.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks for long video generation lack narrative evaluation, limiting the assessment of richer content expression in longer videos.

Method: The authors propose (i) TNAs to measure narrative richness, (ii) an automatic prompt generation pipeline, and (iii) an MLLM-based evaluation metric.

Result: Experiments show the metric aligns with human judgment and reveals capability boundaries of current models.

Conclusion: NarrLV provides a comprehensive benchmark for evaluating narrative expression in long video generation, addressing a critical gap in the field.

Abstract: With the rapid development of foundation video generation technologies, long
video generation models have exhibited promising research potential thanks to
expanded content creation space. Recent studies reveal that the goal of long
video generation tasks is not only to extend video duration but also to
accurately express richer narrative content within longer videos. However, due
to the lack of evaluation benchmarks specifically designed for long video
generation models, the current assessment of these models primarily relies on
benchmarks with simple narrative prompts (e.g., VBench). To the best of our
knowledge, our proposed NarrLV is the first benchmark to comprehensively
evaluate the Narrative expression capabilities of Long Video generation models.
Inspired by film narrative theory, (i) we first introduce the basic narrative
unit maintaining continuous visual presentation in videos as Temporal Narrative
Atom (TNA), and use its count to quantitatively measure narrative richness.
Guided by three key film narrative elements influencing TNA changes, we
construct an automatic prompt generation pipeline capable of producing
evaluation prompts with a flexibly expandable number of TNAs. (ii) Then, based
on the three progressive levels of narrative content expression, we design an
effective evaluation metric using the MLLM-based question generation and
answering framework. (iii) Finally, we conduct extensive evaluations on
existing long video generation models and the foundation generation models.
Experimental results demonstrate that our metric aligns closely with human
judgments. The derived evaluation outcomes reveal the detailed capability
boundaries of current video generation models in narrative content expression.

</details>


### [46] [Fairness-Aware Grouping for Continuous Sensitive Variables: Application for Debiasing Face Analysis with respect to Skin Tone](https://arxiv.org/abs/2507.11247)
*Veronika Shilova,Emmanuel Malherbe,Giovanni Palma,Laurent Risser,Jean-Michel Loubes*

Main category: cs.CV

TL;DR: The paper proposes a fairness-based grouping method for continuous sensitive attributes to better identify and address discrimination in datasets and models, validated through synthetic and real-world datasets.


<details>
  <summary>Details</summary>
Motivation: Existing fairness assessments often divide data into predefined groups, which may overlook discrimination in continuous sensitive attributes like skin color.

Method: A novel grouping approach maximizes inter-group variance in discrimination to identify critical subgroups, validated on synthetic and real datasets (CelebA, FFHQ).

Result: The method uncovers nuanced discrimination patterns, remains stable across datasets, and improves fairness with minimal accuracy loss.

Conclusion: The proposed approach effectively addresses fairness in continuous attributes, offering practical benefits for industrial deployment.

Abstract: Within a legal framework, fairness in datasets and models is typically
assessed by dividing observations into predefined groups and then computing
fairness measures (e.g., Disparate Impact or Equality of Odds with respect to
gender). However, when sensitive attributes such as skin color are continuous,
dividing into default groups may overlook or obscure the discrimination
experienced by certain minority subpopulations. To address this limitation, we
propose a fairness-based grouping approach for continuous (possibly
multidimensional) sensitive attributes. By grouping data according to observed
levels of discrimination, our method identifies the partition that maximizes a
novel criterion based on inter-group variance in discrimination, thereby
isolating the most critical subgroups.
  We validate the proposed approach using multiple synthetic datasets and
demonstrate its robustness under changing population distributions - revealing
how discrimination is manifested within the space of sensitive attributes.
Furthermore, we examine a specialized setting of monotonic fairness for the
case of skin color. Our empirical results on both CelebA and FFHQ, leveraging
the skin tone as predicted by an industrial proprietary algorithm, show that
the proposed segmentation uncovers more nuanced patterns of discrimination than
previously reported, and that these findings remain stable across datasets for
a given model. Finally, we leverage our grouping model for debiasing purpose,
aiming at predicting fair scores with group-by-group post-processing. The
results demonstrate that our approach improves fairness while having minimal
impact on accuracy, thus confirming our partition method and opening the door
for industrial deployment.

</details>


### [47] [MFGDiffusion: Mask-Guided Smoke Synthesis for Enhanced Forest Fire Detection](https://arxiv.org/abs/2507.11252)
*Guanghao Wu,Chen Xu,Hai Song,Chong Wang,Qixing Zhang*

Main category: cs.CV

TL;DR: A framework for generating realistic forest fire smoke images using deep learning and multimodal models to improve smoke detection performance.


<details>
  <summary>Details</summary>
Motivation: The scarcity of real smoke images hinders forest fire detection; current inpainting models struggle with quality and consistency.

Method: Uses pre-trained segmentation and multimodal models for masks/captions, introduces mask-guided architecture, and a new loss function (mask random difference loss). Generates diverse smoke images with a multimodal LLM for filtering.

Result: Produces realistic, diverse smoke images that enhance forest fire smoke detection models.

Conclusion: The proposed framework effectively addresses data scarcity and improves detection performance.

Abstract: Smoke is the first visible indicator of a wildfire.With the advancement of
deep learning, image-based smoke detection has become a crucial method for
detecting and preventing forest fires. However, the scarcity of smoke image
data from forest fires is one of the significant factors hindering the
detection of forest fire smoke. Image generation models offer a promising
solution for synthesizing realistic smoke images. However, current inpainting
models exhibit limitations in generating high-quality smoke representations,
particularly manifesting as inconsistencies between synthesized smoke and
background contexts. To solve these problems, we proposed a comprehensive
framework for generating forest fire smoke images. Firstly, we employed the
pre-trained segmentation model and the multimodal model to obtain smoke masks
and image captions.Then, to address the insufficient utilization of masks and
masked images by inpainting models, we introduced a network architecture guided
by mask and masked image features. We also proposed a new loss function, the
mask random difference loss, which enhances the consistency of the generated
effects around the mask by randomly expanding and eroding the mask
edges.Finally, to generate a smoke image dataset using random masks for
subsequent detection tasks, we incorporated smoke characteristics and use a
multimodal large language model as a filtering tool to select diverse and
reasonable smoke images, thereby improving the quality of the synthetic
dataset. Experiments showed that our generated smoke images are realistic and
diverse, and effectively enhance the performance of forest fire smoke detection
models. Code is available at https://github.com/wghr123/MFGDiffusion.

</details>


### [48] [ViewSRD: 3D Visual Grounding via Structured Multi-View Decomposition](https://arxiv.org/abs/2507.11261)
*Ronggang Huang,Haoxin Yang,Yan Cai,Xuemiao Xu,Huaidong Zhang,Shengfeng He*

Main category: cs.CV

TL;DR: ViewSRD improves 3D visual grounding by decomposing complex queries into simpler statements and integrating multi-view features for better spatial understanding.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with complex multi-anchor queries and spatial inconsistencies due to perspective variations.

Method: ViewSRD uses Simple Relation Decoupling (SRD) to simplify queries, Multi-view Textual-Scene Interaction (Multi-TSI) for cross-modal feature integration, and Textual-Scene Reasoning for unified predictions.

Result: ViewSRD outperforms state-of-the-art methods, especially in complex spatial queries.

Conclusion: The framework effectively addresses challenges in 3D visual grounding by leveraging structured multi-view decomposition.

Abstract: 3D visual grounding aims to identify and localize objects in a 3D space based
on textual descriptions. However, existing methods struggle with disentangling
targets from anchors in complex multi-anchor queries and resolving
inconsistencies in spatial descriptions caused by perspective variations. To
tackle these challenges, we propose ViewSRD, a framework that formulates 3D
visual grounding as a structured multi-view decomposition process. First, the
Simple Relation Decoupling (SRD) module restructures complex multi-anchor
queries into a set of targeted single-anchor statements, generating a
structured set of perspective-aware descriptions that clarify positional
relationships. These decomposed representations serve as the foundation for the
Multi-view Textual-Scene Interaction (Multi-TSI) module, which integrates
textual and scene features across multiple viewpoints using shared, Cross-modal
Consistent View Tokens (CCVTs) to preserve spatial correlations. Finally, a
Textual-Scene Reasoning module synthesizes multi-view predictions into a
unified and robust 3D visual grounding. Experiments on 3D visual grounding
datasets show that ViewSRD significantly outperforms state-of-the-art methods,
particularly in complex queries requiring precise spatial differentiation.

</details>


### [49] [YOLOatr : Deep Learning Based Automatic Target Detection and Localization in Thermal Infrared Imagery](https://arxiv.org/abs/2507.11267)
*Aon Safdar,Usman Akram,Waseem Anwar,Basit Malik,Mian Ibad Ali*

Main category: cs.CV

TL;DR: The paper introduces YOLOatr, a modified YOLOv5s-based model for improved Automatic Target Recognition (ATR) in Thermal Infrared imagery, achieving 99.6% accuracy.


<details>
  <summary>Details</summary>
Motivation: ATR in defense/surveillance is challenging due to domain-specific issues like limited datasets, hardware constraints, and environmental variability, causing current deep learning models to underperform.

Method: Proposes YOLOatr, a modified YOLOv5s with optimized detection heads, feature fusion, and custom augmentation.

Result: Achieves SOTA ATR performance (99.6%) on the DSIAC MWIR dataset.

Conclusion: YOLOatr effectively addresses ATR challenges in TI imagery, outperforming existing models.

Abstract: Automatic Target Detection (ATD) and Recognition (ATR) from Thermal Infrared
(TI) imagery in the defense and surveillance domain is a challenging computer
vision (CV) task in comparison to the commercial autonomous vehicle perception
domain. Limited datasets, peculiar domain-specific and TI modality-specific
challenges, i.e., limited hardware, scale invariance issues due to greater
distances, deliberate occlusion by tactical vehicles, lower sensor resolution
and resultant lack of structural information in targets, effects of weather,
temperature, and time of day variations, and varying target to clutter ratios
all result in increased intra-class variability and higher inter-class
similarity, making accurate real-time ATR a challenging CV task. Resultantly,
contemporary state-of-the-art (SOTA) deep learning architectures underperform
in the ATR domain. We propose a modified anchor-based single-stage detector,
called YOLOatr, based on a modified YOLOv5s, with optimal modifications to the
detection heads, feature fusion in the neck, and a custom augmentation profile.
We evaluate the performance of our proposed model on a comprehensive DSIAC MWIR
dataset for real-time ATR over both correlated and decorrelated testing
protocols. The results demonstrate that our proposed model achieves
state-of-the-art ATR performance of up to 99.6%.

</details>


### [50] [Tomato Multi-Angle Multi-Pose Dataset for Fine-Grained Phenotyping](https://arxiv.org/abs/2507.11279)
*Yujie Zhang,Sabine Struckmeyer,Andreas Kolb,Sven Reichardt*

Main category: cs.CV

TL;DR: TomatoMAP is a dataset for Solanum lycopersicum using IoT-based imaging, providing annotated images for fine-grained phenotyping. AI models trained on it match human expert accuracy.


<details>
  <summary>Details</summary>
Motivation: Traditional plant phenotyping methods suffer from observer bias and inconsistencies, limiting accuracy and reproducibility.

Method: Developed TomatoMAP with IoT-based imaging, standardized protocols, and annotations. Used MobileNetv3, YOLOv11, and MaskRCNN for validation.

Result: AI models achieved accuracy and speed comparable to human experts, confirmed by Cohen's Kappa and inter-rater agreement.

Conclusion: TomatoMAP enables reliable, automated fine-grained plant phenotyping, addressing limitations of traditional methods.

Abstract: Observer bias and inconsistencies in traditional plant phenotyping methods
limit the accuracy and reproducibility of fine-grained plant analysis. To
overcome these challenges, we developed TomatoMAP, a comprehensive dataset for
Solanum lycopersicum using an Internet of Things (IoT) based imaging system
with standardized data acquisition protocols. Our dataset contains 64,464 RGB
images that capture 12 different plant poses from four camera elevation angles.
Each image includes manually annotated bounding boxes for seven regions of
interest (ROIs), including leaves, panicle, batch of flowers, batch of fruits,
axillary shoot, shoot and whole plant area, along with 50 fine-grained growth
stage classifications based on the BBCH scale. Additionally, we provide 3,616
high-resolution image subset with pixel-wise semantic and instance segmentation
annotations for fine-grained phenotyping. We validated our dataset using a
cascading model deep learning framework combining MobileNetv3 for
classification, YOLOv11 for object detection, and MaskRCNN for segmentation.
Through AI vs. Human analysis involving five domain experts, we demonstrate
that the models trained on our dataset achieve accuracy and speed comparable to
the experts. Cohen's Kappa and inter-rater agreement heatmap confirm the
reliability of automated fine-grained phenotyping using our approach.

</details>


### [51] [Task-Oriented Human Grasp Synthesis via Context- and Task-Aware Diffusers](https://arxiv.org/abs/2507.11287)
*An-Lun Liu,Yu-Wei Chao,Yi-Ting Chen*

Main category: cs.CV

TL;DR: The paper introduces task-oriented human grasp synthesis, using task-aware contact maps to improve grasp quality and task performance.


<details>
  <summary>Details</summary>
Motivation: Traditional grasp synthesis lacks task and context awareness, limiting its practical application. This work aims to enhance grasp synthesis by incorporating scene and task information.

Method: A two-stage pipeline: (1) constructs a task-aware contact map using scene and task data, (2) synthesizes task-oriented human grasps based on this map.

Result: Experiments show significant improvements in grasp quality and task performance over existing methods.

Conclusion: Task-aware contact maps are crucial for accurate, task-aligned grasp synthesis, validated by a new dataset and metric.

Abstract: In this paper, we study task-oriented human grasp synthesis, a new grasp
synthesis task that demands both task and context awareness. At the core of our
method is the task-aware contact maps. Unlike traditional contact maps that
only reason about the manipulated object and its relation with the hand, our
enhanced maps take into account scene and task information. This comprehensive
map is critical for hand-object interaction, enabling accurate grasping poses
that align with the task. We propose a two-stage pipeline that first constructs
a task-aware contact map informed by the scene and task. In the subsequent
stage, we use this contact map to synthesize task-oriented human grasps. We
introduce a new dataset and a metric for the proposed task to evaluate our
approach. Our experiments validate the importance of modeling both scene and
task, demonstrating significant improvements over existing methods in both
grasp quality and task performance. See our project page for more details:
https://hcis-lab.github.io/TOHGS/

</details>


### [52] [Detección y Cuantificación de Erosión Fluvial con Visión Artificial](https://arxiv.org/abs/2507.11301)
*Paúl Maji,Marlon Túquerres,Stalin Valencia,Marcela Valenzuela,Christian Mejia-Escobar*

Main category: cs.CV

TL;DR: The paper introduces an AI-based method (YOLOv11) for automatic detection and area estimation of fluvial erosion using photos and LiDAR, achieving 70% accuracy. The EROSCAN web app is developed for practical use.


<details>
  <summary>Details</summary>
Motivation: Traditional erosion monitoring methods are manual and require expertise. AI can automate and improve efficiency.

Method: Uses YOLOv11, fine-tuned and trained with labeled photos and LiDAR data segmented via Roboflow.

Result: 70% accuracy in detecting erosion, precise area estimation in pixels and square meters.

Conclusion: EROSCAN, a web app, enables efficient erosion detection and aids risk management decisions.

Abstract: Fluvial erosion is a natural process that can generate significant impacts on
soil stability and strategic infrastructures. The detection and monitoring of
this phenomenon is traditionally addressed by photogrammetric methods and
analysis in geographic information systems. These tasks require specific
knowledge and intensive manual processing. This study proposes an artificial
intelligence-based approach for automatic identification of eroded zones and
estimation of their area. The state-of-the-art computer vision model YOLOv11,
adjusted by fine-tuning and trained with photographs and LiDAR images, is used.
This combined dataset was segmented and labeled using the Roboflow platform.
Experimental results indicate efficient detection of erosion patterns with an
accuracy of 70%, precise identification of eroded areas and reliable
calculation of their extent in pixels and square meters. As a final product,
the EROSCAN system has been developed, an interactive web application that
allows users to upload images and obtain automatic segmentations of fluvial
erosion, together with the estimated area. This tool optimizes the detection
and quantification of the phenomenon, facilitating decision making in risk
management and territorial planning.

</details>


### [53] [A Mixed-Primitive-based Gaussian Splatting Method for Surface Reconstruction](https://arxiv.org/abs/2507.11321)
*Haoxuan Qu,Yujun Cai,Hossein Rahmani,Ajay Kumar,Junsong Yuan,Jun Liu*

Main category: cs.CV

TL;DR: The paper proposes a framework to enhance Gaussian Splatting (GS) by incorporating multiple types of geometrical primitives for better surface reconstruction.


<details>
  <summary>Details</summary>
Motivation: Existing GS-based methods use a single primitive type (ellipse or ellipsoid), which is insufficient for high-quality representation of diverse 3D objects.

Method: The framework introduces a compositional splatting strategy, mixed-primitive initialization, and vertex pruning to leverage multiple primitive types.

Result: Extensive experiments demonstrate accurate surface reconstruction.

Conclusion: The proposed framework improves GS by enabling the use of diverse primitives, enhancing reconstruction quality.

Abstract: Recently, Gaussian Splatting (GS) has received a lot of attention in surface
reconstruction. However, while 3D objects can be of complex and diverse shapes
in the real world, existing GS-based methods only limitedly use a single type
of splatting primitive (Gaussian ellipse or Gaussian ellipsoid) to represent
object surfaces during their reconstruction. In this paper, we highlight that
this can be insufficient for object surfaces to be represented in high quality.
Thus, we propose a novel framework that, for the first time, enables Gaussian
Splatting to incorporate multiple types of (geometrical) primitives during its
surface reconstruction process. Specifically, in our framework, we first
propose a compositional splatting strategy, enabling the splatting and
rendering of different types of primitives in the Gaussian Splatting pipeline.
In addition, we also design our framework with a mixed-primitive-based
initialization strategy and a vertex pruning mechanism to further promote its
surface representation learning process to be well executed leveraging
different types of primitives. Extensive experiments show the efficacy of our
framework and its accurate surface reconstruction performance.

</details>


### [54] [MonoMVSNet: Monocular Priors Guided Multi-View Stereo Network](https://arxiv.org/abs/2507.11333)
*Jianfei Jiang,Qiankun Liu,Haochen Yu,Hongyuan Liu,Liyong Wang,Jiansheng Chen,Huimin Ma*

Main category: cs.CV

TL;DR: MonoMVSNet integrates monocular depth and feature priors into MVS to improve performance in challenging regions like textureless and reflective surfaces.


<details>
  <summary>Details</summary>
Motivation: Existing MVS methods struggle in challenging regions due to failed feature matching, while monocular depth estimation excels there.

Method: MonoMVSNet uses monocular features and depth, aligned dynamically, with a cross-view position encoding and relative consistency loss.

Result: Achieves state-of-the-art performance on DTU and Tanks-and-Temples datasets, ranking first on Intermediate and Advanced benchmarks.

Conclusion: MonoMVSNet effectively bridges the gap between monocular depth and MVS, enhancing robustness in challenging scenarios.

Abstract: Learning-based Multi-View Stereo (MVS) methods aim to predict depth maps for
a sequence of calibrated images to recover dense point clouds. However,
existing MVS methods often struggle with challenging regions, such as
textureless regions and reflective surfaces, where feature matching fails. In
contrast, monocular depth estimation inherently does not require feature
matching, allowing it to achieve robust relative depth estimation in these
regions. To bridge this gap, we propose MonoMVSNet, a novel monocular feature
and depth guided MVS network that integrates powerful priors from a monocular
foundation model into multi-view geometry. Firstly, the monocular feature of
the reference view is integrated into source view features by the attention
mechanism with a newly designed cross-view position encoding. Then, the
monocular depth of the reference view is aligned to dynamically update the
depth candidates for edge regions during the sampling procedure. Finally, a
relative consistency loss is further designed based on the monocular depth to
supervise the depth prediction. Extensive experiments demonstrate that
MonoMVSNet achieves state-of-the-art performance on the DTU and
Tanks-and-Temples datasets, ranking first on the Tanks-and-Temples Intermediate
and Advanced benchmarks. The source code is available at
https://github.com/JianfeiJ/MonoMVSNet.

</details>


### [55] [UGC-VideoCaptioner: An Omni UGC Video Detail Caption Model and New Benchmarks](https://arxiv.org/abs/2507.11336)
*Peiran Wu,Yunze Liu,Zhengdong Zhu,Enmin Zhou,Shawn Shen*

Main category: cs.CV

TL;DR: The paper introduces UGC-VideoCap, a benchmark and model framework for omnimodal captioning of user-generated videos, emphasizing audio-visual integration and efficient training.


<details>
  <summary>Details</summary>
Motivation: Existing video captioning benchmarks and models are visual-centric, neglecting audio's role in understanding videos. This gap hinders multimodal video understanding.

Method: The authors propose UGC-VideoCap, a dataset with 1000 TikTok videos annotated via a three-stage pipeline, and UGC-VideoCaptioner(3B), a model trained with a two-stage strategy (supervised fine-tuning and GRPO).

Result: The benchmark includes 4000 QA pairs for probing understanding, and the model achieves competitive performance with limited data.

Conclusion: UGC-VideoCap and its model provide a foundation for advancing omnimodal video captioning in real-world UGC settings.

Abstract: Real-world user-generated videos, especially on platforms like TikTok, often
feature rich and intertwined audio visual content. However, existing video
captioning benchmarks and models remain predominantly visual centric,
overlooking the crucial role of audio in conveying scene dynamics, speaker
intent, and narrative context. This lack of omni datasets and lightweight,
capable models hampers progress in fine grained, multimodal video
understanding. To address these challenges, we introduce UGC-VideoCap, a new
benchmark and model framework specifically designed for detailed omnimodal
captioning of short form user-generated videos. Unlike prior datasets,
UGC-VideoCap emphasizes balanced integration of audio and visual modalities,
featuring 1000 TikTok videos annotated through a structured three stage
human-in-the-loop pipeline covering audio only, visual only, and joint audio
visual semantics. The benchmark also includes 4000 carefully crafted QA pairs
probing both unimodal and cross modal understanding. Alongside the dataset, we
propose UGC-VideoCaptioner(3B), a 3B parameter captioning model distilled from
Gemini 2.5 Flash. Using a novel two-stage training strategy supervised fine
tuning followed by Group Relative Policy Optimization (GRPO), our approach
enables efficient adaptation from limited data while maintaining competitive
performance. Together, our benchmark and model offer a high-quality foundation
and a data-efficient solution for advancing omnimodal video captioning in
unconstrained real-world UGC settings.

</details>


### [56] [Attributes Shape the Embedding Space of Face Recognition Models](https://arxiv.org/abs/2507.11372)
*Pierrick Leroy,Antonio Mastropietro,Marco Nurisso,Francesco Vaccarino*

Main category: cs.CV

TL;DR: The paper explores the geometric structure in face recognition embedding spaces, influenced by facial and image attributes, and introduces a physics-inspired alignment metric to evaluate model invariance.


<details>
  <summary>Details</summary>
Motivation: To understand how face recognition models are affected by interpretable facial and image attributes, beyond just identity labels, and to improve model interpretability.

Method: Proposes a geometric approach and a physics-inspired alignment metric, tested on controlled models and fine-tuned FR models with synthetic data.

Result: Models show varying degrees of invariance to different attributes, revealing strengths and weaknesses.

Conclusion: The approach provides deeper interpretability of FR models, aiding in understanding their behavior and limitations.

Abstract: Face Recognition (FR) tasks have made significant progress with the advent of
Deep Neural Networks, particularly through margin-based triplet losses that
embed facial images into high-dimensional feature spaces. During training,
these contrastive losses focus exclusively on identity information as labels.
However, we observe a multiscale geometric structure emerging in the embedding
space, influenced by interpretable facial (e.g., hair color) and image
attributes (e.g., contrast). We propose a geometric approach to describe the
dependence or invariance of FR models to these attributes and introduce a
physics-inspired alignment metric. We evaluate the proposed metric on
controlled, simplified models and widely used FR models fine-tuned with
synthetic data for targeted attribute augmentation. Our findings reveal that
the models exhibit varying degrees of invariance across different attributes,
providing insight into their strengths and weaknesses and enabling deeper
interpretability. Code available here:
https://github.com/mantonios107/attrs-fr-embs}{https://github.com/mantonios107/attrs-fr-embs

</details>


### [57] [Implementing Adaptations for Vision AutoRegressive Model](https://arxiv.org/abs/2507.11441)
*Kaif Shaikh,Antoni Kowalczuk,Franziska Boenisch,Adam Dziedzic*

Main category: cs.CV

TL;DR: VAR outperforms DMs in non-DP image generation tasks but struggles with DP adaptations, highlighting a need for further research in private VAR adaptations.


<details>
  <summary>Details</summary>
Motivation: To explore and benchmark adaptation strategies for VAR, particularly for downstream tasks like medical data generation, and compare them to DM adaptations, including DP techniques.

Method: Implemented and benchmarked various adaptation strategies for VAR, comparing them to state-of-the-art DM adaptation methods, with a focus on DP adaptations.

Result: VAR performs better than DMs for non-DP adaptations but shows poorer performance in DP scenarios.

Conclusion: Further research is needed to improve DP adaptations for VAR, as current methods lag behind DMs in privacy-preserving tasks.

Abstract: Vision AutoRegressive model (VAR) was recently introduced as an alternative
to Diffusion Models (DMs) in image generation domain. In this work we focus on
its adaptations, which aim to fine-tune pre-trained models to perform specific
downstream tasks, like medical data generation. While for DMs there exist many
techniques, adaptations for VAR remain underexplored. Similarly, differentially
private (DP) adaptations-ones that aim to preserve privacy of the adaptation
data-have been extensively studied for DMs, while VAR lacks such solutions. In
our work, we implement and benchmark many strategies for VAR, and compare them
to state-of-the-art DM adaptation strategies. We observe that VAR outperforms
DMs for non-DP adaptations, however, the performance of DP suffers, which
necessitates further research in private adaptations for VAR. Code is available
at https://github.com/sprintml/finetuning_var_dp.

</details>


### [58] [COLI: A Hierarchical Efficient Compressor for Large Images](https://arxiv.org/abs/2507.11443)
*Haoran Wang,Hanyu Pei,Yang Lyu,Kai Zhang,Li Li,Feng-Lei Fan*

Main category: cs.CV

TL;DR: COLI introduces a novel framework using Neural Representations for Videos (NeRV) to improve INR-based compression for large images, addressing speed and ratio issues.


<details>
  <summary>Details</summary>
Motivation: The need for efficient compression of high-resolution images, as conventional and data-driven methods fail in detail preservation and generalizability.

Method: COLI uses a pretraining-finetuning paradigm, mixed-precision training, and Hyper-Compression to enhance speed and compression ratios.

Result: COLI achieves better PSNR and SSIM metrics at lower bits per pixel (bpp) and speeds up NeRV training by 4x.

Conclusion: COLI offers a scalable and efficient solution for large-image compression, outperforming existing methods.

Abstract: The escalating adoption of high-resolution, large-field-of-view imagery
amplifies the need for efficient compression methodologies. Conventional
techniques frequently fail to preserve critical image details, while
data-driven approaches exhibit limited generalizability. Implicit Neural
Representations (INRs) present a promising alternative by learning continuous
mappings from spatial coordinates to pixel intensities for individual images,
thereby storing network weights rather than raw pixels and avoiding the
generalization problem. However, INR-based compression of large images faces
challenges including slow compression speed and suboptimal compression ratios.
To address these limitations, we introduce COLI (Compressor for Large Images),
a novel framework leveraging Neural Representations for Videos (NeRV). First,
recognizing that INR-based compression constitutes a training process, we
accelerate its convergence through a pretraining-finetuning paradigm,
mixed-precision training, and reformulation of the sequential loss into a
parallelizable objective. Second, capitalizing on INRs' transformation of image
storage constraints into weight storage, we implement Hyper-Compression, a
novel post-training technique to substantially enhance compression ratios while
maintaining minimal output distortion. Evaluations across two medical imaging
datasets demonstrate that COLI consistently achieves competitive or superior
PSNR and SSIM metrics at significantly reduced bits per pixel (bpp), while
accelerating NeRV training by up to 4 times.

</details>


### [59] [HUG-VAS: A Hierarchical NURBS-Based Generative Model for Aortic Geometry Synthesis and Controllable Editing](https://arxiv.org/abs/2507.11474)
*Pan Du,Mingqi Xu,Xiaozhi Zhu,Jian-xun Wang*

Main category: cs.CV

TL;DR: HUG-VAS is a hierarchical NURBS generative model for vascular geometry synthesis, combining NURBS parameterization with diffusion-based modeling to create realistic aortic structures with multi-branch topologies.


<details>
  <summary>Details</summary>
Motivation: Traditional SSM methods are limited by linear assumptions and struggle with complex vascular topologies. HUG-VAS aims to overcome these limitations for better cardiovascular diagnosis and treatment planning.

Method: HUG-VAS uses a hierarchical architecture with a denoising diffusion model for centerlines and a guided diffusion model for radial profiles, trained on 21 patient-specific samples.

Result: The model generates anatomically accurate aortas with supra-aortic branches, matching biomarker distributions of the original dataset. It supports zero-shot conditional generation for practical applications.

Conclusion: HUG-VAS is the first SSM framework integrating NURBS parameterization and hierarchical diffusion, bridging image-derived priors with generative shape modeling for improved vascular analysis.

Abstract: Accurate characterization of vascular geometry is essential for
cardiovascular diagnosis and treatment planning. Traditional statistical shape
modeling (SSM) methods rely on linear assumptions, limiting their expressivity
and scalability to complex topologies such as multi-branch vascular structures.
We introduce HUG-VAS, a Hierarchical NURBS Generative model for Vascular
geometry Synthesis, which integrates NURBS surface parameterization with
diffusion-based generative modeling to synthesize realistic, fine-grained
aortic geometries. Trained with 21 patient-specific samples, HUG-VAS generates
anatomically faithful aortas with supra-aortic branches, yielding biomarker
distributions that closely match those of the original dataset. HUG-VAS adopts
a hierarchical architecture comprising a denoising diffusion model that
generates centerlines and a guided diffusion model that synthesizes radial
profiles conditioned on those centerlines, thereby capturing two layers of
anatomical variability. Critically, the framework supports zero-shot
conditional generation from image-derived priors, enabling practical
applications such as interactive semi-automatic segmentation, robust
reconstruction under degraded imaging conditions, and implantable device
optimization. To our knowledge, HUG-VAS is the first SSM framework to bridge
image-derived priors with generative shape modeling via a unified integration
of NURBS parameterization and hierarchical diffusion processes.

</details>


### [60] [C-FBI: A Combinatorial method using Convolutions for Circle Fitting in Blurry Images](https://arxiv.org/abs/2507.11476)
*Esteban Román Catafau,Torbjörn E. M. Nordling*

Main category: cs.CV

TL;DR: 3C-FBI is a novel algorithm for robust circle detection in degraded images, combining combinatorial sampling and convolution-based density estimation. It outperforms classical methods in accuracy and speed.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of robust circle detection in poor imaging conditions, particularly for applications like medical imaging and industrial inspection.

Method: Combines combinatorial edge pixel sampling and convolution-based density estimation for precise circle fitting.

Result: Achieves state-of-the-art accuracy (Jaccard index 0.896) and real-time performance (40.3 fps), outperforming classical methods like RCD.

Conclusion: 3C-FBI is ideal for applications requiring high accuracy and robustness in challenging imaging conditions.

Abstract: This paper addresses the fundamental computer vision challenge of robust
circle detection and fitting in degraded imaging conditions. We present
Combinatorial Convolution-based Circle Fitting for Blurry Images (3C-FBI), an
algorithm that bridges the gap between circle detection and precise parametric
fitting by combining (1) efficient combinatorial edge pixel (edgel) sampling
and (2) convolution-based density estimation in parameter space.
  We evaluate 3C-FBI across three experimental frameworks: (1) real-world
medical data from Parkinson's disease assessments (144 frames from 36 videos),
(2) controlled synthetic data following established circle-fitting benchmarks,
and (3) systematic analysis across varying spatial resolutions and outlier
contamination levels. Results show that 3C-FBI achieves state-of-the-art
accuracy (Jaccard index 0.896) while maintaining real-time performance (40.3
fps), significantly outperforming classical methods like RCD (6.8 fps) on a
standard CPU (i7-10875H). It maintains near-perfect accuracy (Jaccard almost
1.0) at high resolutions (480x480) and reliable performance (Jaccard higher
than 0.95) down to 160x160 with up to 20% outliers.
  In extensive synthetic testing, 3C-FBI achieves a mean Jaccard Index of 0.989
across contamination levels, comparable to modern methods like Qi et al. (2024,
0.991), and surpassing RHT (0.964). This combination of accuracy, speed, and
robustness makes 3C-FBI ideal for medical imaging, robotics, and industrial
inspection under challenging conditions.

</details>


### [61] [COLIBRI Fuzzy Model: Color Linguistic-Based Representation and Interpretation](https://arxiv.org/abs/2507.11488)
*Pakizar Shamoi,Nuray Toganas,Muragul Muratbekova,Elnara Kadyrgali,Adilet Yerkin,Ayan Igali,Malika Ziyada,Ayana Adilova,Aron Karatayev,Yerdauit Torekhan*

Main category: cs.CV

TL;DR: The paper introduces COLIBRI, a fuzzy color model aligning computational color representation with human perception, validated through large-scale experiments.


<details>
  <summary>Details</summary>
Motivation: Bridging the gap between computational color models and human visual perception.

Method: A three-phase approach: identifying color stimuli, conducting a large-scale human categorization survey, and extracting fuzzy partitions for membership functions.

Result: COLIBRI outperforms traditional models (RGB, HSV, LAB) in aligning with human perception.

Conclusion: The model is significant for design, AI, marketing, and HCI due to its perceptual relevance.

Abstract: Colors are omnipresent in today's world and play a vital role in how humans
perceive and interact with their surroundings. However, it is challenging for
computers to imitate human color perception. This paper introduces the Human
Perception-Based Fuzzy Color Model, COLIBRI (Color Linguistic-Based
Representation and Interpretation), designed to bridge the gap between
computational color representations and human visual perception. The proposed
model uses fuzzy sets and logic to create a framework for color categorization.
Using a three-phase experimental approach, the study first identifies
distinguishable color stimuli for hue, saturation, and intensity through
preliminary experiments, followed by a large-scale human categorization survey
involving more than 1000 human subjects. The resulting data are used to extract
fuzzy partitions and generate membership functions that reflect real-world
perceptual uncertainty. The model incorporates a mechanism for adaptation that
allows refinement based on feedback and contextual changes. Comparative
evaluations demonstrate the model's alignment with human perception compared to
traditional color models, such as RGB, HSV, and LAB. To the best of our
knowledge, no previous research has documented the construction of a model for
color attribute specification based on a sample of this size or a comparable
sample of the human population (n = 2496). Our findings are significant for
fields such as design, artificial intelligence, marketing, and human-computer
interaction, where perceptually relevant color representation is critical.

</details>


### [62] [CATVis: Context-Aware Thought Visualization](https://arxiv.org/abs/2507.11522)
*Tariq Mehmood,Hamza Ahmad,Muhammad Haroon Shakeel,Murtaza Taj*

Main category: cs.CV

TL;DR: A novel 5-stage framework decodes visual representations from EEG signals, outperforming SOTA methods in accuracy and image quality.


<details>
  <summary>Details</summary>
Motivation: Decoding visual representations from EEG signals is challenging due to their complexity and noise.

Method: A 5-stage framework involving EEG encoder, cross-modal alignment, caption refinement, weighted interpolation, and image generation.

Result: Outperforms SOTA by 13.43% in Classification Accuracy, 15.21% in Generation Accuracy, and reduces Fréchet Inception Distance by 36.61%.

Conclusion: The method enables high-quality, context-aware EEG-to-image generation with superior semantic alignment.

Abstract: EEG-based brain-computer interfaces (BCIs) have shown promise in various
applications, such as motor imagery and cognitive state monitoring. However,
decoding visual representations from EEG signals remains a significant
challenge due to their complex and noisy nature. We thus propose a novel
5-stage framework for decoding visual representations from EEG signals: (1) an
EEG encoder for concept classification, (2) cross-modal alignment of EEG and
text embeddings in CLIP feature space, (3) caption refinement via re-ranking,
(4) weighted interpolation of concept and caption embeddings for richer
semantics, and (5) image generation using a pre-trained Stable Diffusion model.
We enable context-aware EEG-to-image generation through cross-modal alignment
and re-ranking. Experimental results demonstrate that our method generates
high-quality images aligned with visual stimuli, outperforming SOTA approaches
by 13.43% in Classification Accuracy, 15.21% in Generation Accuracy and
reducing Fr\'echet Inception Distance by 36.61%, indicating superior semantic
alignment and image quality.

</details>


### [63] [CharaConsist: Fine-Grained Consistent Character Generation](https://arxiv.org/abs/2507.11533)
*Mengyu Wang,Henghui Ding,Jianing Peng,Yao Zhao,Yunpeng Chen,Yunchao Wei*

Main category: cs.CV

TL;DR: CharaConsist improves consistency in text-to-image generation by using point-tracking attention and adaptive token merge, addressing issues with background and foreground details.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail to maintain consistent background details and struggle with identity and clothing consistency during large motion variations.

Method: CharaConsist employs point-tracking attention, adaptive token merge, and decoupled control of foreground and background.

Result: It achieves fine-grained consistency for both foreground and background, supporting continuous or discrete shots across scenes.

Conclusion: CharaConsist is the first method tailored for DiT models, producing high-quality outputs for broader real-world applications.

Abstract: In text-to-image generation, producing a series of consistent contents that
preserve the same identity is highly valuable for real-world applications.
Although a few works have explored training-free methods to enhance the
consistency of generated subjects, we observe that they suffer from the
following problems. First, they fail to maintain consistent background details,
which limits their applicability. Furthermore, when the foreground character
undergoes large motion variations, inconsistencies in identity and clothing
details become evident. To address these problems, we propose CharaConsist,
which employs point-tracking attention and adaptive token merge along with
decoupled control of the foreground and background. CharaConsist enables
fine-grained consistency for both foreground and background, supporting the
generation of one character in continuous shots within a fixed scene or in
discrete shots across different scenes. Moreover, CharaConsist is the first
consistent generation method tailored for text-to-image DiT model. Its ability
to maintain fine-grained consistency, combined with the larger capacity of
latest base model, enables it to produce high-quality visual outputs,
broadening its applicability to a wider range of real-world scenarios. The
source code has been released at https://github.com/Murray-Wang/CharaConsist

</details>


### [64] [Streaming 4D Visual Geometry Transformer](https://arxiv.org/abs/2507.11539)
*Dong Zhuo,Wenzhao Zheng,Jiahe Guo,Yuqi Wu,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: The paper proposes a streaming 4D visual geometry transformer for real-time 4D reconstruction from videos, inspired by autoregressive language models. It uses causal attention and implicit memory for efficiency and integrates knowledge distillation for training.


<details>
  <summary>Details</summary>
Motivation: To enable interactive and real-time 4D spatial-temporal geometry reconstruction from videos, addressing the challenge of maintaining spatial consistency while processing data incrementally.

Method: A causal transformer architecture with temporal causal attention and implicit memory (cached historical keys/values) is used for online processing. Knowledge is distilled from a dense bidirectional transformer (VGGT) for efficient training.

Result: The model achieves competitive performance in 4D geometry perception benchmarks while significantly improving inference speed for online scenarios.

Conclusion: The proposed method advances scalable and interactive 4D vision systems by balancing real-time processing with high-quality reconstruction.

Abstract: Perceiving and reconstructing 4D spatial-temporal geometry from videos is a
fundamental yet challenging computer vision task. To facilitate interactive and
real-time applications, we propose a streaming 4D visual geometry transformer
that shares a similar philosophy with autoregressive large language models. We
explore a simple and efficient design and employ a causal transformer
architecture to process the input sequence in an online manner. We use temporal
causal attention and cache the historical keys and values as implicit memory to
enable efficient streaming long-term 4D reconstruction. This design can handle
real-time 4D reconstruction by incrementally integrating historical information
while maintaining high-quality spatial consistency. For efficient training, we
propose to distill knowledge from the dense bidirectional visual geometry
grounded transformer (VGGT) to our causal model. For inference, our model
supports the migration of optimized efficient attention operator (e.g.,
FlashAttention) from the field of large language models. Extensive experiments
on various 4D geometry perception benchmarks demonstrate that our model
increases the inference speed in online scenarios while maintaining competitive
performance, paving the way for scalable and interactive 4D vision systems.
Code is available at: https://github.com/wzzheng/StreamVGGT.

</details>


### [65] [Towards Depth Foundation Model: Recent Trends in Vision-Based Depth Estimation](https://arxiv.org/abs/2507.11540)
*Zhen Xu,Hongyu Zhou,Sida Peng,Haotong Lin,Haoyu Guo,Jiahao Shao,Peishan Yang,Qinglin Yang,Sheng Miao,Xingyi He,Yifan Wang,Yue Wang,Ruizhen Hu,Yiyi Liao,Xiaowei Zhou,Hujun Bao*

Main category: cs.CV

TL;DR: A survey on depth estimation in 3D vision, highlighting challenges of traditional and recent methods, and exploring the potential of depth foundation models for robust generalization.


<details>
  <summary>Details</summary>
Motivation: Address limitations of traditional hardware-based and recent vision-based depth estimation methods, which suffer from cost, resolution, and generalization issues.

Method: Surveys deep learning architectures and paradigms (monocular, stereo, multi-view, video) and large-scale datasets for depth foundation models.

Result: Identifies key architectures and training strategies to advance robust depth foundation models with zero-shot generalization.

Conclusion: Proposes depth foundation models as a promising solution, offering insights for future research and applications in 3D vision.

Abstract: Depth estimation is a fundamental task in 3D computer vision, crucial for
applications such as 3D reconstruction, free-viewpoint rendering, robotics,
autonomous driving, and AR/VR technologies. Traditional methods relying on
hardware sensors like LiDAR are often limited by high costs, low resolution,
and environmental sensitivity, limiting their applicability in real-world
scenarios. Recent advances in vision-based methods offer a promising
alternative, yet they face challenges in generalization and stability due to
either the low-capacity model architectures or the reliance on domain-specific
and small-scale datasets. The emergence of scaling laws and foundation models
in other domains has inspired the development of "depth foundation models":
deep neural networks trained on large datasets with strong zero-shot
generalization capabilities. This paper surveys the evolution of deep learning
architectures and paradigms for depth estimation across the monocular, stereo,
multi-view, and monocular video settings. We explore the potential of these
models to address existing challenges and provide a comprehensive overview of
large-scale datasets that can facilitate their development. By identifying key
architectures and training strategies, we aim to highlight the path towards
robust depth foundation models, offering insights into their future research
and applications.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [66] [SAMEP: A Secure Protocol for Persistent Context Sharing Across AI Agents](https://arxiv.org/abs/2507.10562)
*Hari Masoor*

Main category: cs.AI

TL;DR: SAMEP is a protocol for secure, persistent, and searchable memory sharing among AI agents, improving collaboration and efficiency.


<details>
  <summary>Details</summary>
Motivation: Current AI agents lack persistent memory sharing, hindering collaboration and knowledge reuse.

Method: SAMEP uses a distributed memory repository with semantic search, cryptographic controls, and standardized APIs.

Result: 73% fewer redundant computations, 89% better context relevance, and full regulatory compliance.

Conclusion: SAMEP enables secure, collaborative AI ecosystems with persistent memory.

Abstract: Current AI agent architectures suffer from ephemeral memory limitations,
preventing effective collaboration and knowledge sharing across sessions and
agent boundaries. We introduce SAMEP (Secure Agent Memory Exchange Protocol), a
novel framework that enables persistent, secure, and semantically searchable
memory sharing among AI agents. Our protocol addresses three critical
challenges: (1) persistent context preservation across agent sessions, (2)
secure multi-agent collaboration with fine-grained access control, and (3)
efficient semantic discovery of relevant historical context. SAMEP implements a
distributed memory repository with vector-based semantic search, cryptographic
access controls (AES-256-GCM), and standardized APIs compatible with existing
agent communication protocols (MCP, A2A). We demonstrate SAMEP's effectiveness
across diverse domains including multi-agent software development, healthcare
AI with HIPAA compliance, and multi-modal processing pipelines. Experimental
results show 73% reduction in redundant computations, 89% improvement in
context relevance scores, and complete compliance with regulatory requirements
including audit trail generation. SAMEP enables a new paradigm of persistent,
collaborative AI agent ecosystems while maintaining security and privacy
guarantees.

</details>


### [67] [AI Mother Tongue: Self-Emergent Communication in MARL via Endogenous Symbol Systems](https://arxiv.org/abs/2507.10566)
*Hung Ming Liu*

Main category: cs.AI

TL;DR: The paper challenges traditional inductive biases in MARL for emergent communication, proposing the AIM framework with VQ-VAE, showing spontaneous semantic compression and convergence without external biases.


<details>
  <summary>Details</summary>
Motivation: To question if artificial inductive biases in MARL are over-engineering and explore natural communication emergence.

Method: Uses the AIM framework with VQ-VAE to enable endogenous symbol systems in agents.

Result: Agents achieve effective symbolic communication without external biases, with symbol usage following a power-law distribution.

Conclusion: AIM offers new insights for symbolism-connectionism integration, with future work on HQ-VAE and RL pre-training.

Abstract: In Decentralized Multi-Agent Reinforcement Learning (MARL), the development
of Emergent Communication has long been constrained by the ``Joint Exploration
Dilemma'', leading agents to fall into a ``Communication Vacuum Equilibrium'' .
Traditional methods address this by introducing inductive biases to facilitate
communication emergence . This study fundamentally questions whether such
artificial inductive biases are, in fact, over-engineering. Through experiments
with the ``AI Mother Tongue'' (AIM) framework, based on a Vector Quantized
Variational Autoencoder (VQ-VAE), we demonstrate that when agents possess an
endogenous symbol system, their neural representations naturally exhibit
spontaneous semantic compression and Nash equilibrium-driven semantic
convergence, achieving effective symbolic communication without external
inductive biases. This aligns with recent neuroscience findings suggesting that
the human brain does not directly use human language for internal thought , and
resonates with research on ``soft thinking'' capabilities in Large Language
Models (LLMs) . Compared to traditional explicit communication methods, AIM
demonstrates stronger generality and efficiency. The interpretable analysis
toolkit developed in this study confirms that symbol usage exhibits a
significant power-law distribution, leading to three major theoretical
insights: the ``Neural Communication Hypothesis'', the ``Tool-First
Principle'', and the ``Semantic Interpretability Paradigm''. Future research
will explore the integration of Hierarchical Quantized Variational Autoencoders
(HQ-VAE) to enhance AIM's complex expressive capabilities and investigate the
potential for ``Reinforcement Learning (RL) Low-Level Pre-training''. This
discovery offers new avenues for bridging symbolism and connectionism.

</details>


### [68] [Orchestrator-Agent Trust: A Modular Agentic AI Visual Classification System with Trust-Aware Orchestration and RAG-Based Reasoning](https://arxiv.org/abs/2507.10571)
*Konstantinos I. Roumeliotis,Ranjan Sapkota,Manoj Karkee,Nikolaos D. Tselikas*

Main category: cs.AI

TL;DR: A modular AI framework combines multimodal agents, a reasoning orchestrator, and RAG for trustworthy zero-shot visual classification, improving accuracy by 77.94% in apple leaf disease diagnosis.


<details>
  <summary>Details</summary>
Motivation: Addressing trust challenges in zero-shot AI agents by integrating modular components for better reliability and interpretability.

Method: Uses a framework with multimodal agents, a non-visual orchestrator, and RAG, tested in three configurations: zero-shot, fine-tuned, and trust-calibrated.

Result: Achieves 85.63% accuracy with trust-aware orchestration and RAG, with GPT-4o showing better calibration than Qwen-2.5-VL.

Conclusion: The framework enhances trust and scalability in multi-agent AI, applicable to diagnostics and biology, with open-source release for reproducibility.

Abstract: Modern Artificial Intelligence (AI) increasingly relies on multi-agent
architectures that blend visual and language understanding. Yet, a pressing
challenge remains: How can we trust these agents especially in zero-shot
settings with no fine-tuning? We introduce a novel modular Agentic AI visual
classification framework that integrates generalist multimodal agents with a
non-visual reasoning orchestrator and a Retrieval-Augmented Generation (RAG)
module. Applied to apple leaf disease diagnosis, we benchmark three
configurations: (I) zero-shot with confidence-based orchestration, (II)
fine-tuned agents with improved performance, and (III) trust-calibrated
orchestration enhanced by CLIP-based image retrieval and re-evaluation loops.
Using confidence calibration metrics (ECE, OCR, CCC), the orchestrator
modulates trust across agents. Our results demonstrate a 77.94\% accuracy
improvement in the zero-shot setting using trust-aware orchestration and RAG,
achieving 85.63\% overall. GPT-4o showed better calibration, while Qwen-2.5-VL
displayed overconfidence. Furthermore, image-RAG grounded predictions with
visually similar cases, enabling correction of agent overconfidence via
iterative re-evaluation. The proposed system separates perception (vision
agents) from meta-reasoning (orchestrator), enabling scalable and interpretable
multi-agent AI. This blueprint is extensible to diagnostics, biology, and other
trust-critical domains. All models, prompts, results, and system components
including the complete software source code are openly released to support
reproducibility, transparency, and community benchmarking at Github:
https://github.com/Applied-AI-Research-Lab/Orchestrator-Agent-Trust

</details>


### [69] [Comprehension Without Competence: Architectural Limits of LLMs in Symbolic Computation and Reasoning](https://arxiv.org/abs/2507.10624)
*Zheng Zhang*

Main category: cs.AI

TL;DR: LLMs show fluency but fail in symbolic reasoning, arithmetic, and logic due to a gap between comprehension and competence, termed 'split-brain syndrome.'


<details>
  <summary>Details</summary>
Motivation: To diagnose why LLMs articulate correct principles but fail to apply them, revealing a computational execution gap.

Method: Controlled experiments and architectural analysis to study the dissociation between instruction and action pathways.

Result: LLMs lack compositional reasoning, exhibiting brittle behavior despite pattern completion strength.

Conclusion: Future models need metacognitive control and structural grounding to overcome these limitations.

Abstract: Large Language Models (LLMs) display striking surface fluency yet
systematically fail at tasks requiring symbolic reasoning, arithmetic accuracy,
and logical consistency. This paper offers a structural diagnosis of such
failures, revealing a persistent gap between \textit{comprehension} and
\textit{competence}. Through controlled experiments and architectural analysis,
we demonstrate that LLMs often articulate correct principles without reliably
applying them--a failure rooted not in knowledge access, but in computational
execution. We term this phenomenon the computational \textit{split-brain
syndrome}, where instruction and action pathways are geometrically and
functionally dissociated. This core limitation recurs across domains, from
mathematical operations to relational inferences, and explains why model
behavior remains brittle even under idealized prompting. We argue that LLMs
function as powerful pattern completion engines, but lack the architectural
scaffolding for principled, compositional reasoning. Our findings delineate the
boundary of current LLM capabilities and motivate future models with
metacognitive control, principle lifting, and structurally grounded execution.
This diagnosis also clarifies why mechanistic interpretability findings may
reflect training-specific pattern coordination rather than universal
computational principles, and why the geometric separation between instruction
and execution pathways suggests limitations in neural introspection and
mechanistic analysis.

</details>


### [70] [Enhancing the Capabilities of Large Language Models for API calls through Knowledge Graphs](https://arxiv.org/abs/2507.10630)
*Ye Yang,Xue Xiao,Ping Yin,Taotao Xie*

Main category: cs.AI

TL;DR: KG2data integrates knowledge graphs, LLMs, and tool-use tech to improve API call accuracy in meteorology, outperforming RAG2data and chat2data.


<details>
  <summary>Details</summary>
Motivation: LLMs' tool-use via API calls is underexplored in knowledge-intensive domains like meteorology, limiting their effectiveness.

Method: KG2data combines knowledge graphs, LLMs, ReAct agents, and tool-use tech, evaluated via virtual API for accuracy in name recognition, hallucination, and call correctness.

Result: KG2data outperforms alternatives with metrics (1.43%, 0%, 88.57%) vs. RAG2data (16%, 10%, 72.14%) and chat2data (7.14%, 8.57%, 71.43%).

Conclusion: KG2data enhances domain-specific query handling and data analysis, reducing reliance on costly LLM fine-tuning and adapting to evolving knowledge.

Abstract: API calls by large language models (LLMs) offer a cutting-edge approach for
data analysis. However, their ability to effectively utilize tools via API
calls remains underexplored in knowledge-intensive domains like meteorology.
This paper introduces KG2data, a system that integrates knowledge graphs, LLMs,
ReAct agents, and tool-use technologies to enable intelligent data acquisition
and query handling in the meteorological field. Using a virtual API, we
evaluate API call accuracy across three metrics: name recognition failure,
hallucination failure, and call correctness. KG2data achieves superior
performance (1.43%, 0%, 88.57%) compared to RAG2data (16%, 10%, 72.14%) and
chat2data (7.14%, 8.57%, 71.43%). KG2data differs from typical LLM-based
systems by addressing their limited access to domain-specific knowledge, which
hampers performance on complex or terminology-rich queries. By using a
knowledge graph as persistent memory, our system enhances content retrieval,
complex query handling, domain-specific reasoning, semantic relationship
resolution, and heterogeneous data integration. It also mitigates the high cost
of fine-tuning LLMs, making the system more adaptable to evolving domain
knowledge and API structures. In summary, KG2data provides a novel solution for
intelligent, knowledge-based question answering and data analysis in domains
with high knowledge demands.

</details>


### [71] [From Semantic Web and MAS to Agentic AI: A Unified Narrative of the Web of Agents](https://arxiv.org/abs/2507.10644)
*Tatiana Petrova,Aleksandr Puzikov,Boris Bliznukov,Radu State*

Main category: cs.AI

TL;DR: The paper provides a comprehensive evolutionary overview of the Web of Agents (WoA), linking modern protocols to historical standards and introducing a taxonomy to unify analysis. It highlights a paradigm shift in intelligence locus and outlines future research challenges.


<details>
  <summary>Details</summary>
Motivation: To address the fragmented research in the Web of Agents (WoA) and connect modern LLM-powered frameworks with historical Multi-Agent Systems (MAS) and Semantic Web developments.

Method: Introduces a four-axis taxonomy (semantic foundation, communication paradigm, locus of intelligence, discovery mechanism) to analyze agent architectures across generations.

Result: Reveals a paradigm shift in intelligence locus from external data or platforms to embedded agent models (LLMs), enabling scalable and adaptive systems.

Conclusion: New protocols alone are insufficient; future research must address socio-technical challenges like decentralized identity, economic models, security, and governance for a robust WoA ecosystem.

Abstract: The concept of the Web of Agents (WoA), which transforms the static,
document-centric Web into an environment of autonomous agents acting on users'
behalf, has attracted growing interest as large language models (LLMs) become
more capable. However, research in this area is still fragmented across
different communities. Contemporary surveys catalog the latest LLM-powered
frameworks, while the rich histories of Multi-Agent Systems (MAS) and the
Semantic Web are often treated as separate, legacy domains. This fragmentation
obscures the intellectual lineage of modern systems and hinders a holistic
understanding of the field's trajectory. We present the first comprehensive
evolutionary overview of the WoA. We show that modern protocols like A2A and
the MCP, are direct evolutionary responses to the well-documented limitations
of earlier standards like FIPA standards and OWL-based semantic agents. To
systematize this analysis, we introduce a four-axis taxonomy (semantic
foundation, communication paradigm, locus of intelligence, discovery
mechanism). This framework provides a unified analytical lens for comparing
agent architectures across all generations, revealing a clear line of descent
where others have seen a disconnect. Our analysis identifies a paradigm shift
in the 'locus of intelligence': from being encoded in external data (Semantic
Web) or the platform (MAS) to being embedded within the agent's core model
(LLM). This shift is foundational to modern Agentic AI, enabling the scalable
and adaptive systems the WoA has long envisioned. We conclude that while new
protocols are essential, they are insufficient for building a robust, open,
trustworthy ecosystem. Finally, we argue that the next research frontier lies
in solving persistent socio-technical challenges, and we map out a new agenda
focused on decentralized identity, economic models, security, and governance
for the emerging WoA.

</details>


### [72] [Parsing Musical Structure to Enable Meaningful Variations](https://arxiv.org/abs/2507.10740)
*Maziar Kanani,Sean O Leary,James McDermott*

Main category: cs.AI

TL;DR: A rule-based method for music generation by mutating grammars derived from tunes, analyzing changes via edit distance, complexity, and length.


<details>
  <summary>Details</summary>
Motivation: To explore how tunes evolve through systematic grammar mutations and assess the musicality of the results.

Method: Parse tunes into grammars using Sequitur, apply random mutations (19 types), expand grammars to generate new tunes, and analyze changes.

Result: Demonstrates gradual tune evolution via mutations, with metrics like edit distance and complexity. Analyzes mutation effects and musicality.

Conclusion: Grammar-based mutation effectively generates related tunes, with measurable changes and insights into mutation impacts.

Abstract: This paper presents a novel rule-based approach for generating music by
varying existing tunes. We parse each tune to find the Pathway Assembly (PA) [
1], that is a structure representing all repetitions in the tune. The Sequitur
algorithm [2 ] is used for this. The result is a grammar. We then carry out
mutation on the grammar, rather than on a tune directly. There are potentially
19 types of mutations such as adding, removing, swapping or reversing parts of
the grammar that can be applied to the grammars. The system employs one of the
mutations randomly in this step to automatically manipulate the grammar.
Following the mutation, we need to expand the grammar which returns a new tune.
The output after 1 or more mutations will be a new tune related to the original
tune. Our study examines how tunes change gradually over the course of multiple
mutations. Edit distances, structural complexity and length of the tunes are
used to show how a tune is changed after multiple mutations. In addition, the
size of effect of each mutation type is analyzed. As a final point, we review
the musical aspect of the output tunes. It should be noted that the study only
focused on generating new pitch sequences. The study is based on an Irish
traditional tune dataset and a list of integers has been used to represent each
tune's pitch values.

</details>


### [73] [AI and the Net-Zero Journey: Energy Demand, Emissions, and the Potential for Transition](https://arxiv.org/abs/2507.10750)
*Pandu Devarakota,Nicolas Tsesmetzis,Faruk O. Alpak,Apurva Gala,Detlef Hohl*

Main category: cs.AI

TL;DR: The paper examines AI's energy consumption and GHG emissions impact, predicting short-term CO2 increases but long-term potential for significant reductions.


<details>
  <summary>Details</summary>
Motivation: To assess whether AI will have a net positive, neutral, or negative impact on CO2 emissions by 2035, considering its growing role in various sectors.

Method: Analysis of energy consumption scenarios for data centers, including near-term (up to 2030) and long-term (2035+) projections, and evaluation of AI's automation and optimization potential.

Result: Short-term: AI demand strains resources, increasing CO2 emissions. Long-term: AI's optimization capabilities could reduce emissions, outweighing initial impacts.

Conclusion: AI may initially harm the environment but holds promise for long-term climate mitigation.

Abstract: Thanks to the availability of massive amounts of data, computing resources,
and advanced algorithms, AI has entered nearly every sector. This has sparked
significant investment and interest, particularly in building data centers with
the necessary hardware and software to develop and operate AI models and
AI-based workflows. In this technical review article, we present energy
consumption scenarios of data centers and impact on GHG emissions, considering
both near-term projections (up to 2030) and long-term outlook (2035 and
beyond). We address the quintessential question of whether AI will have a net
positive, neutral, or negative impact on CO2 emissions by 2035. Additionally,
we discuss AI's potential to automate, create efficient and disruptive
workflows across various fields related to energy production, supply and
consumption. In the near-term scenario, the growing demand for AI will likely
strain computing resources, lead to increase in electricity consumption and
therefore associated CO2 emissions. This is due to the power-hungry nature of
big data centers and the requirements for training and running of large and
complex AI models, as well as the penetration of AI assistant search and
applications for public use. However, the long-term outlook could be more
promising. AI has the potential to be a game-changer in CO2 reduction. Its
ability to further automate and optimize processes across industries, from
energy production to logistics, could significantly decrease our carbon
footprint. This positive impact is anticipated to outweigh the initial
emissions bump, creating value for businesses and society in areas where
traditional solutions have fallen short. In essence, AI might cause some
initial growing pains for the environment, but it has the potential to support
climate mitigation efforts.

</details>


### [74] [IoT Malware Network Traffic Detection using Deep Learning and GraphSAGE Models](https://arxiv.org/abs/2507.10758)
*Nikesh Prajapati,Bimal Karki,Saroj Gopali,Akbar Siami Namin*

Main category: cs.AI

TL;DR: The paper evaluates deep learning and graph-based models (GraphSAGE, BERT, TCN, Multi-Head Attention, BI-LSTM, LSTM) for detecting IoT malicious attacks, with BERT achieving the highest performance (99.94% accuracy).


<details>
  <summary>Details</summary>
Motivation: To detect IoT malicious attacks by leveraging temporal and diverse traffic patterns using advanced deep learning models.

Method: Utilized GraphSAGE, BERT, TCN, Multi-Head Attention, BI-LSTM, and LSTM models to analyze sequential and diverse IoT traffic patterns.

Result: BERT outperformed others with 99.94% accuracy, high precision, recall, F1-score, and AUC-ROC. Multi-Head Attention provided interpretable results but required more processing time. GraphSAGE had the shortest training time but lower accuracy.

Conclusion: BERT is highly effective for IoT malicious attack detection due to its ability to capture temporal dependencies, while Multi-Head Attention offers interpretability and GraphSAGE balances speed and performance.

Abstract: This paper intends to detect IoT malicious attacks through deep learning
models and demonstrates a comprehensive evaluation of the deep learning and
graph-based models regarding malicious network traffic detection. The models
particularly are based on GraphSAGE, Bidirectional encoder representations from
transformers (BERT), Temporal Convolutional Network (TCN) as well as Multi-Head
Attention, together with Bidirectional Long Short-Term Memory (BI-LSTM)
Multi-Head Attention and BI-LSTM and LSTM models. The chosen models
demonstrated great performance to model temporal patterns and detect feature
significance. The observed performance are mainly due to the fact that IoT
system traffic patterns are both sequential and diverse, leaving a rich set of
temporal patterns for the models to learn. Experimental results showed that
BERT maintained the best performance. It achieved 99.94% accuracy rate
alongside high precision and recall, F1-score and AUC-ROC score of 99.99% which
demonstrates its capabilities through temporal dependency capture. The
Multi-Head Attention offered promising results by providing good detection
capabilities with interpretable results. On the other side, the Multi-Head
Attention model required significant processing time like BI-LSTM variants. The
GraphSAGE model achieved good accuracy while requiring the shortest training
time but yielded the lowest accuracy, precision, and F1 score compared to the
other models

</details>


### [75] [Detecting AI Assistance in Abstract Complex Tasks](https://arxiv.org/abs/2507.10761)
*Tyler King,Nikolos Gurney,John H. Miller,Volkan Ustun*

Main category: cs.AI

TL;DR: The paper proposes using neural networks to detect AI assistance in abstract tasks by preprocessing data into neural network-friendly formats, including image and time-series formulations, and demonstrates their effectiveness.


<details>
  <summary>Details</summary>
Motivation: As AI becomes ubiquitous in complex tasks, detecting its assistance is crucial but challenging for humans, especially with abstract data.

Method: Constructs four neural network-friendly image formulations and a time-series formulation to encode user exploration/exploitation. Benchmarks these using classical deep learning architectures and a parallel CNN-RNN model.

Result: Common models can effectively classify AI-assisted data when preprocessed appropriately, with temporal and spatial encoding proving crucial.

Conclusion: Preprocessing abstract task data into suitable formats enables neural networks to effectively detect AI assistance, highlighting the importance of temporal and spatial encoding.

Abstract: Detecting assistance from artificial intelligence is increasingly important
as they become ubiquitous across complex tasks such as text generation, medical
diagnosis, and autonomous driving. Aid detection is challenging for humans,
especially when looking at abstract task data. Artificial neural networks excel
at classification thanks to their ability to quickly learn from and process
large amounts of data -- assuming appropriate preprocessing. We posit detecting
help from AI as a classification task for such models. Much of the research in
this space examines the classification of complex but concrete data classes,
such as images. Many AI assistance detection scenarios, however, result in data
that is not machine learning-friendly. We demonstrate that common models can
effectively classify such data when it is appropriately preprocessed. To do so,
we construct four distinct neural network-friendly image formulations along
with an additional time-series formulation that explicitly encodes the
exploration/exploitation of users, which allows for generalizability to other
abstract tasks. We benchmark the quality of each image formulation across three
classical deep learning architectures, along with a parallel CNN-RNN
architecture that leverages the additional time series to maximize testing
performance, showcasing the importance of encoding temporal and spatial
quantities for detecting AI aid in abstract tasks.

</details>


### [76] [Uncertainty-Informed Scheduling of Decision Points for Intelligent Mobile Health Interventions](https://arxiv.org/abs/2507.10798)
*Asim H. Gazi,Bhanu T. Gullapalli,Daiqi Gao,Benjamin M. Marlin,Vivek Shetty,Susan A. Murphy*

Main category: cs.AI

TL;DR: SigmaScheduling dynamically schedules decision points for mHealth interventions based on behavior time uncertainty, improving intervention timeliness compared to fixed-interval methods.


<details>
  <summary>Details</summary>
Motivation: Fixed-interval scheduling for mHealth interventions often fails for individuals with irregular routines, leading to untimely interventions.

Method: SigmaScheduling adjusts decision points dynamically: earlier for uncertain behavior times and closer for predictable ones.

Result: In a trial with 68 participants, SigmaScheduling ensured decision points preceded brushing events in ≥70% of cases.

Conclusion: SigmaScheduling enhances precision in mHealth, especially for time-sensitive habitual behaviors like oral hygiene.

Abstract: Timely decision making is critical to the effectiveness of mobile health
(mHealth) interventions. At predefined timepoints called "decision points,"
intelligent mHealth systems such as just-in-time adaptive interventions
(JITAIs) estimate an individual's biobehavioral context from sensor or survey
data and determine whether and how to intervene. For interventions targeting
habitual behavior (e.g., oral hygiene), effectiveness often hinges on
delivering support shortly before the target behavior is likely to occur.
Current practice schedules decision points at a fixed interval (e.g., one hour)
before user-provided behavior times, and the fixed interval is kept the same
for all individuals. However, this one-size-fits-all approach performs poorly
for individuals with irregular routines, often scheduling decision points after
the target behavior has already occurred, rendering interventions ineffective.
In this paper, we propose SigmaScheduling, a method to dynamically schedule
decision points based on uncertainty in predicted behavior times. When behavior
timing is more predictable, SigmaScheduling schedules decision points closer to
the predicted behavior time; when timing is less certain, SigmaScheduling
schedules decision points earlier, increasing the likelihood of timely
intervention. We evaluated SigmaScheduling using real-world data from 68
participants in a 10-week trial of Oralytics, a JITAI designed to improve daily
toothbrushing. SigmaScheduling increased the likelihood that decision points
preceded brushing events in at least 70% of cases, preserving opportunities to
intervene and impact behavior. Our results indicate that SigmaScheduling can
advance precision mHealth, particularly for JITAIs targeting time-sensitive,
habitual behaviors such as oral hygiene or dietary habits.

</details>


### [77] [Automated Thematic Analyses Using LLMs: Xylazine Wound Management Social Media Chatter Use Case](https://arxiv.org/abs/2507.10803)
*JaMor Hairston,Ritvik Ranjan,Sahithi Lakamana,Anthony Spadaro,Selen Bozkurt,Jeanmarie Perrone,Abeed Sarker*

Main category: cs.AI

TL;DR: The study evaluates LLMs for inductive thematic analysis on social media data, finding GPT-4o with two-shot prompting performs best, closely mirroring expert classifications for high-prevalence themes.


<details>
  <summary>Details</summary>
Motivation: To assess the feasibility of using LLMs to replicate expert-driven thematic analysis, addressing challenges in domain-specific interpretive tasks.

Method: Evaluated five LLMs on Reddit datasets using binary classifications with zero-, single-, and few-shot prompting, measuring accuracy, precision, recall, and F1-score.

Result: GPT-4o with two-shot prompting achieved 90.9% accuracy and 0.71 F1-score, closely matching expert classifications for high-prevalence themes.

Conclusion: Few-shot LLM-based approaches can automate thematic analyses, providing a scalable supplement for qualitative research.

Abstract: Background Large language models (LLMs) face challenges in inductive thematic
analysis, a task requiring deep interpretive and domain-specific expertise. We
evaluated the feasibility of using LLMs to replicate expert-driven thematic
analysis of social media data. Methods Using two temporally non-intersecting
Reddit datasets on xylazine (n=286 and n=686, for model optimization and
validation, respectively) with twelve expert-derived themes, we evaluated five
LLMs against expert coding. We modeled the task as a series of binary
classifications, rather than a single, multi-label classification, employing
zero-, single-, and few-shot prompting strategies and measuring performance via
accuracy, precision, recall, and F1-score. Results On the validation set,
GPT-4o with two-shot prompting performed best (accuracy: 90.9%; F1-score:
0.71). For high-prevalence themes, model-derived thematic distributions closely
mirrored expert classifications (e.g., xylazine use: 13.6% vs. 17.8%; MOUD use:
16.5% vs. 17.8%). Conclusions Our findings suggest that few-shot LLM-based
approaches can automate thematic analyses, offering a scalable supplement for
qualitative research. Keywords: thematic analysis, large language models,
natural language processing, qualitative analysis, social media, prompt
engineering, public health

</details>


### [78] [AF-XRAY: Visual Explanation and Resolution of Ambiguity in Legal Argumentation Frameworks](https://arxiv.org/abs/2507.10831)
*Yilin Xia,Heng Zheng,Shawn Bowers,Bertram Ludäscher*

Main category: cs.AI

TL;DR: AF-XRAY is an open-source toolkit for analyzing and visualizing argumentation frameworks (AFs) in legal reasoning, addressing ambiguity and aiding non-experts.


<details>
  <summary>Details</summary>
Motivation: To tackle challenges in identifying ambiguity and explaining argument acceptance in legal reasoning for non-experts.

Method: Introduces layered visualizations, attack edge classification, overlay visualizations, and critical attack set identification.

Result: Transforms ambiguous scenarios into grounded solutions, revealing causes of ambiguity and enabling exploration of alternative resolutions.

Conclusion: AF-XRAY supports teleological legal reasoning by showing how assumptions impact conclusions, demonstrated with real-world cases.

Abstract: Argumentation frameworks (AFs) provide formal approaches for legal reasoning,
but identifying sources of ambiguity and explaining argument acceptance remains
challenging for non-experts. We present AF-XRAY, an open-source toolkit for
exploring, analyzing, and visualizing abstract AFs in legal reasoning. AF-XRAY
introduces: (i) layered visualizations based on game-theoretic argument length
revealing well-founded derivation structures; (ii) classification of attack
edges by semantic roles (primary, secondary, blunders); (iii) overlay
visualizations of alternative 2-valued solutions on ambiguous 3-valued grounded
semantics; and (iv) identification of critical attack sets whose suspension
resolves undecided arguments. Through systematic generation of critical attack
sets, AF-XRAY transforms ambiguous scenarios into grounded solutions, enabling
users to pinpoint specific causes of ambiguity and explore alternative
resolutions. We use real-world legal cases (e.g., Wild Animals as modeled by
Bench-Capon) to show that our tool supports teleological legal reasoning by
revealing how different assumptions lead to different justified conclusions.

</details>


### [79] [NavComposer: Composing Language Instructions for Navigation Trajectories through Action-Scene-Object Modularization](https://arxiv.org/abs/2507.10894)
*Zongtao He,Liuyi Wang,Lu Chen,Chengju Liu,Qijun Chen*

Main category: cs.AI

TL;DR: NavComposer generates high-quality navigation instructions by decomposing and recomposing semantic entities, while NavInstrCritic evaluates them without expert annotations.


<details>
  <summary>Details</summary>
Motivation: Expert-provided navigation instructions are scarce, and synthesized ones often lack quality, limiting large-scale research.

Method: NavComposer decomposes semantic entities (actions, scenes, objects) and recomposes them into instructions. NavInstrCritic evaluates instructions on contrastive matching, semantic consistency, and linguistic diversity.

Result: The method produces high-quality instructions and enables scalable, generalizable research.

Conclusion: NavComposer and NavInstrCritic address limitations of existing methods, offering a robust solution for language-guided navigation research.

Abstract: Language-guided navigation is a cornerstone of embodied AI, enabling agents
to interpret language instructions and navigate complex environments. However,
expert-provided instructions are limited in quantity, while synthesized
annotations often lack quality, making them insufficient for large-scale
research. To address this, we propose NavComposer, a novel framework for
automatically generating high-quality navigation instructions. NavComposer
explicitly decomposes semantic entities such as actions, scenes, and objects,
and recomposes them into natural language instructions. Its modular
architecture allows flexible integration of state-of-the-art techniques, while
the explicit use of semantic entities enhances both the richness and accuracy
of instructions. Moreover, it operates in a data-agnostic manner, supporting
adaptation to diverse navigation trajectories without domain-specific training.
Complementing NavComposer, we introduce NavInstrCritic, a comprehensive
annotation-free evaluation system that assesses navigation instructions on
three dimensions: contrastive matching, semantic consistency, and linguistic
diversity. NavInstrCritic provides a holistic evaluation of instruction
quality, addressing limitations of traditional metrics that rely heavily on
expert annotations. By decoupling instruction generation and evaluation from
specific navigation agents, our method enables more scalable and generalizable
research. Extensive experiments provide direct and practical evidence for the
effectiveness of our method.

</details>


### [80] [Lessons Learned from Evaluation of LLM based Multi-agents in Safer Therapy Recommendation](https://arxiv.org/abs/2507.10911)
*Yicong Wu,Ting Chen,Irit Hochberg,Zhoujian Sun,Ruth Edry,Zhengxing Huang,Mor Peleg*

Main category: cs.AI

TL;DR: The study explores using an LLM-based multi-agent system (MAS) to improve therapy recommendations for chronic multimorbidity patients, comparing it to single-agent approaches and real-world benchmarks.


<details>
  <summary>Details</summary>
Motivation: Therapy recommendations for multimorbidity patients are complex due to treatment conflicts, and existing decision support systems lack scalability. The study aims to simulate multidisciplinary team (MDT) collaboration using LLMs for safer recommendations.

Method: A single-agent and MAS framework were designed to simulate MDT decision-making, with LLM agents discussing to resolve conflicts. Performance was evaluated on therapy planning tasks using benchmark cases and compared to single-agent approaches and real-world benchmarks.

Result: Single-agent GP performed as well as MDTs. Best models provided correct but incomplete recommendations, with some adding unnecessary medications, leading to conflicts.

Conclusion: LLM-based MAS shows promise for therapy recommendations, but current models have limitations like incomplete advice and unnecessary medications, highlighting areas for improvement.

Abstract: Therapy recommendation for chronic patients with multimorbidity is
challenging due to risks of treatment conflicts. Existing decision support
systems face scalability limitations. Inspired by the way in which general
practitioners (GP) manage multimorbidity patients, occasionally convening
multidisciplinary team (MDT) collaboration, this study investigated the
feasibility and value of using a Large Language Model (LLM)-based multi-agent
system (MAS) for safer therapy recommendations. We designed a single agent and
a MAS framework simulating MDT decision-making by enabling discussion among LLM
agents to resolve medical conflicts. The systems were evaluated on therapy
planning tasks for multimorbidity patients using benchmark cases. We compared
MAS performance with single-agent approaches and real-world benchmarks. An
important contribution of our study is the definition of evaluation metrics
that go beyond the technical precision and recall and allow the inspection of
clinical goals met and medication burden of the proposed advices to a gold
standard benchmark. Our results show that with current LLMs, a single agent GP
performs as well as MDTs. The best-scoring models provide correct
recommendations that address all clinical goals, yet the advices are
incomplete. Some models also present unnecessary medications, resulting in
unnecessary conflicts between medication and conditions or drug-drug
interactions.

</details>


### [81] [Enhancing Safe and Controllable Protein Generation via Knowledge Preference Optimization](https://arxiv.org/abs/2507.10923)
*Yuhao Wang,Keyan Ding,Kehua Feng,Zeyuan Wang,Ming Qin,Xiaotong Li,Qiang Zhang,Huajun Chen*

Main category: cs.AI

TL;DR: A Knowledge-guided Preference Optimization (KPO) framework is proposed to mitigate risks of harmful protein sequence generation by protein language models, ensuring biosafety while maintaining functionality.


<details>
  <summary>Details</summary>
Motivation: Protein language models can generate harmful sequences (e.g., enhancing viral transmissibility), raising biosafety and ethical concerns.

Method: KPO integrates prior knowledge via a Protein Safety Knowledge Graph, uses graph pruning to identify safe sequences, and applies reinforcement learning to minimize risks.

Result: KPO effectively reduces hazardous sequence generation while preserving high functionality.

Conclusion: KPO provides a robust safety framework for generative models in biotechnology.

Abstract: Protein language models have emerged as powerful tools for sequence
generation, offering substantial advantages in functional optimization and
denovo design. However, these models also present significant risks of
generating harmful protein sequences, such as those that enhance viral
transmissibility or evade immune responses. These concerns underscore critical
biosafety and ethical challenges. To address these issues, we propose a
Knowledge-guided Preference Optimization (KPO) framework that integrates prior
knowledge via a Protein Safety Knowledge Graph. This framework utilizes an
efficient graph pruning strategy to identify preferred sequences and employs
reinforcement learning to minimize the risk of generating harmful proteins.
Experimental results demonstrate that KPO effectively reduces the likelihood of
producing hazardous sequences while maintaining high functionality, offering a
robust safety assurance framework for applying generative models in
biotechnology.

</details>


### [82] [Modeling Habitat Shifts: Integrating Convolutional Neural Networks and Tabular Data for Species Migration Prediction](https://arxiv.org/abs/2507.10993)
*Emir Durakovic,Min-Hong Shih*

Main category: cs.AI

TL;DR: Combining CNNs and tabular data to model bird species presence in shifting habitats with 85% accuracy.


<details>
  <summary>Details</summary>
Motivation: Addressing habitat range shifts due to climate change by accurately predicting bird species presence.

Method: Uses CNNs for spatial landscape features (e.g., forestation) and tabular data for ecological/geographic features.

Result: Achieves 85% accuracy in predicting bird distribution across climates.

Conclusion: Provides a scalable, reliable method to track bird migration amid climate-induced habitat changes.

Abstract: Due to climate-induced changes, many habitats are experiencing range shifts
away from their traditional geographic locations (Piguet, 2011). We propose a
solution to accurately model whether bird species are present in a specific
habitat through the combination of Convolutional Neural Networks (CNNs)
(O'Shea, 2015) and tabular data. Our approach makes use of satellite imagery
and environmental features (e.g., temperature, precipitation, elevation) to
predict bird presence across various climates. The CNN model captures spatial
characteristics of landscapes such as forestation, water bodies, and
urbanization, whereas the tabular method uses ecological and geographic data.
Both systems predict the distribution of birds with an average accuracy of 85%,
offering a scalable but reliable method to understand bird migration.

</details>


### [83] [Personalized Exercise Recommendation with Semantically-Grounded Knowledge Tracing](https://arxiv.org/abs/2507.11060)
*Yilmazcan Ozyurt,Tunaberk Almaci,Stefan Feuerriegel,Mrinmaya Sachan*

Main category: cs.AI

TL;DR: ExRec is a framework for personalized exercise recommendation using semantically-grounded knowledge tracing, addressing gaps in existing methods by incorporating question semantics and structured learning progression.


<details>
  <summary>Details</summary>
Motivation: Existing exercise recommendation methods often ignore the semantic content of questions and the structured progression of learning, limiting their effectiveness.

Method: ExRec uses an end-to-end pipeline: annotating knowledge components (KCs), learning semantic representations, training KT models, and optimizing RL methods. It enhances Q-learning with model-based value estimation (MVE).

Result: ExRec shows effectiveness across four real-world math learning tasks, generalizes to unseen questions, and produces interpretable learning trajectories.

Conclusion: KT-guided RL holds promise for personalized education, as demonstrated by ExRec's robust performance and interpretability.

Abstract: We introduce ExRec, a general framework for personalized exercise
recommendation with semantically-grounded knowledge tracing. Our method builds
on the observation that existing exercise recommendation approaches simulate
student performance via knowledge tracing (KT) but they often overlook two key
aspects: (a) the semantic content of questions and (b) the sequential,
structured progression of student learning. To address this, our ExRec presents
an end-to-end pipeline, from annotating the KCs of questions and learning their
semantic representations to training KT models and optimizing several
reinforcement learning (RL) methods. Moreover, we improve standard
Q-learning-based continuous RL methods via a tailored model-based value
estimation (MVE) approach that directly leverages the components of KT model in
estimating cumulative knowledge improvement. We validate the effectiveness of
our ExRec using various RL methods across four real-world tasks with different
educational goals in online math learning. We further show that ExRec
generalizes robustly to new, unseen questions and that it produces
interpretable student learning trajectories. Together, our findings highlight
the promise of KT-guided RL for effective personalization in education.

</details>


### [84] [Tactical Decision for Multi-UGV Confrontation with a Vision-Language Model-Based Commander](https://arxiv.org/abs/2507.11079)
*Li Wang,Qizhen Wu,Lei Chen*

Main category: cs.AI

TL;DR: A vision-language model-based commander is proposed for autonomous multi-agent tactical decisions, combining scene understanding and strategic reasoning for high adaptability and interpretability.


<details>
  <summary>Details</summary>
Motivation: Traditional rule-based and reinforcement learning methods lack adaptability and interpretability in complex battlefield environments.

Method: Integrates a vision-language model for scene understanding and a lightweight large language model for strategic reasoning.

Result: Achieves a win rate of over 80% in simulations compared to baseline models.

Conclusion: The proposed method effectively bridges perception and decision-making, mimicking human cognitive processes.

Abstract: In multiple unmanned ground vehicle confrontations, autonomously evolving
multi-agent tactical decisions from situational awareness remain a significant
challenge. Traditional handcraft rule-based methods become vulnerable in the
complicated and transient battlefield environment, and current reinforcement
learning methods mainly focus on action manipulation instead of strategic
decisions due to lack of interpretability. Here, we propose a vision-language
model-based commander to address the issue of intelligent
perception-to-decision reasoning in autonomous confrontations. Our method
integrates a vision language model for scene understanding and a lightweight
large language model for strategic reasoning, achieving unified perception and
decision within a shared semantic space, with strong adaptability and
interpretability. Unlike rule-based search and reinforcement learning methods,
the combination of the two modules establishes a full-chain process, reflecting
the cognitive process of human commanders. Simulation and ablation experiments
validate that the proposed approach achieves a win rate of over 80% compared
with baseline models.

</details>


### [85] [Function-to-Style Guidance of LLMs for Code Translation](https://arxiv.org/abs/2507.11083)
*Longhui Zhang,Bin Wang,Jiahao Wang,Xiaofeng Zhao,Min Zhang,Hao Yang,Meishan Zhang,Yu Li,Jing Li,Jun Yu,Min Zhang*

Main category: cs.AI

TL;DR: F2STrans improves LLM-based code translation by combining functional and style learning, outperforming larger models like GPT-4.


<details>
  <summary>Details</summary>
Motivation: Ensuring correctness and readability in code translation by LLMs is challenging, limiting real-world adoption.

Method: F2STrans uses functional learning (correctness) and style learning (readability) with high-quality code pairs and style examples.

Result: Outperforms Qwen-32B and GPT-4 in 20 code translation scenarios, demonstrating significant improvement.

Conclusion: F2STrans enhances LLM performance in code translation, making it more practical for real-world use.

Abstract: Large language models (LLMs) have made significant strides in code
translation tasks. However, ensuring both the correctness and readability of
translated code remains a challenge, limiting their effective adoption in
real-world software development. In this work, we propose F2STrans, a
function-to-style guiding paradigm designed to progressively improve the
performance of LLMs in code translation. Our approach comprises two key stages:
(1) Functional learning, which optimizes translation correctness using
high-quality source-target code pairs mined from online programming platforms,
and (2) Style learning, which improves translation readability by incorporating
both positive and negative style examples. Additionally, we introduce a novel
code translation benchmark that includes up-to-date source code, extensive test
cases, and manually annotated ground-truth translations, enabling comprehensive
functional and stylistic evaluations. Experiments on both our new benchmark and
existing datasets demonstrate that our approach significantly improves code
translation performance. Notably, our approach enables Qwen-1.5B to outperform
prompt-enhanced Qwen-32B and GPT-4 on average across 20 diverse code
translation scenarios.

</details>


### [86] [AI Agent Architecture for Decentralized Trading of Alternative Assets](https://arxiv.org/abs/2507.11117)
*Ailiya Borjigin,Cong He,Charles CC Lee,Wei Zhou*

Main category: cs.AI

TL;DR: GoldMine OS is an AI-driven decentralized system for tokenizing and trading physical gold on blockchain, achieving fast, secure, and compliant transactions.


<details>
  <summary>Details</summary>
Motivation: To bridge physical asset custody with blockchain, ensuring compliance, liquidity, and risk management for decentralized trading of alternative assets like gold.

Method: Combines on-chain smart contracts for risk control with off-chain AI agents (Compliance, Token Issuance, Market Making, Risk Control) for automation and decision-making.

Result: Achieves on-demand token issuance in under 1.2s, tight liquidity (spreads <0.5%), resilience to attacks, and scalability (5000 TPS, 10000 users).

Conclusion: AI agent-based decentralized exchanges can meet performance and safety needs, democratizing access to illiquid assets with transparent governance.

Abstract: Decentralized trading of real-world alternative assets (e.g., gold) requires
bridging physical asset custody with blockchain systems while meeting strict
requirements for compliance, liquidity, and risk management. We present
GoldMine OS, a research oriented architecture that employs multiple specialized
AI agents to automate and secure the tokenization and exchange of physical gold
into a blockchain based stablecoin ("OZ"). Our approach combines on chain smart
contracts for critical risk controls with off chain AI agents for decision
making, blending the transparency and reliability of blockchains with the
flexibility of AI driven automation. We describe four cooperative agents
(Compliance, Token Issuance, Market Making, and Risk Control) and a
coordinating core, and evaluate the system through simulation and a controlled
pilot deployment. In experiments the prototype delivers on demand token
issuance in under 1.2 s, more than 100 times faster than manual workflows. The
Market Making agent maintains tight liquidity with spreads often below 0.5
percent even under volatile conditions. Fault injection tests show resilience:
an oracle price spoofing attack is detected and mitigated within 10 s, and a
simulated vault mis reporting halts issuance immediately with minimal user
impact. The architecture scales to 5000 transactions per second with 10000
concurrent users in benchmarks. These results indicate that an AI agent based
decentralized exchange for alternative assets can satisfy rigorous performance
and safety requirements. We discuss broader implications for democratizing
access to traditionally illiquid assets and explain how our governance model --
multi signature agent updates and on chain community voting on risk parameters
-- provides ongoing transparency, adaptability, and formal assurance of system
integrity.

</details>


### [87] [Defining neurosymbolic AI](https://arxiv.org/abs/2507.11127)
*Lennert De Smet,Luc De Raedt*

Main category: cs.AI

TL;DR: The paper introduces a formal definition for neurosymbolic AI, unifying logical and neural representations through an integral over a product of logical and belief functions.


<details>
  <summary>Details</summary>
Motivation: The field lacks a generally accepted formal definition of neurosymbolic models and inference, despite many existing systems.

Method: The authors propose a formal definition of neurosymbolic inference as computing an integral over a product of a logical and a belief function.

Result: The definition abstracts key ingredients of neurosymbolic AI and aligns with representative systems.

Conclusion: The formal definition provides a foundational framework for understanding and advancing neurosymbolic AI.

Abstract: Neurosymbolic AI focuses on integrating learning and reasoning, in
particular, on unifying logical and neural representations. Despite the
existence of an alphabet soup of neurosymbolic AI systems, the field is lacking
a generally accepted formal definition of what neurosymbolic models and
inference really are. We introduce a formal definition for neurosymbolic AI
that makes abstraction of its key ingredients. More specifically, we define
neurosymbolic inference as the computation of an integral over a product of a
logical and a belief function. We show that our neurosymbolic AI definition
makes abstraction of key representative neurosymbolic AI systems.

</details>


### [88] [Collaborative Trustworthiness for Good Decision Making in Autonomous Systems](https://arxiv.org/abs/2507.11135)
*Selma Saidi,Omar Laimona,Christoph Schmickler,Dirk Ziegenbein*

Main category: cs.AI

TL;DR: The paper proposes a collaborative approach to enhance trustworthiness and decision-making in autonomous systems by leveraging quality attributes and social epistemology concepts, using BDDs for efficient belief aggregation and propagation.


<details>
  <summary>Details</summary>
Motivation: Ensuring safe and correct behavior of autonomous systems in dynamic environments is challenging, especially when conflicting information arises.

Method: The approach uses quality attributes (e.g., perception quality) to determine trustworthy systems, applies social epistemology for aggregation rules, and employs BDDs for formal modeling and efficient computation.

Result: The method improves reliability and decision-making by reducing BDD size and enabling efficient collaborative reasoning.

Conclusion: The proposed approach enhances trustworthiness and reliability in autonomous systems through collaborative data sharing and formal reasoning.

Abstract: Autonomous systems are becoming an integral part of many application domains,
like in the mobility sector. However, ensuring their safe and correct behaviour
in dynamic and complex environments remains a significant challenge, where
systems should autonomously make decisions e.g., about manoeuvring. We propose
in this paper a general collaborative approach for increasing the level of
trustworthiness in the environment of operation and improve reliability and
good decision making in autonomous system. In the presence of conflicting
information, aggregation becomes a major issue for trustworthy decision making
based on collaborative data sharing. Unlike classical approaches in the
literature that rely on consensus or majority as aggregation rule, we exploit
the fact that autonomous systems have different quality attributes like
perception quality. We use this criteria to determine which autonomous systems
are trustworthy and borrow concepts from social epistemology to define
aggregation and propagation rules, used for automated decision making. We use
Binary Decision Diagrams (BDDs) as formal models for beliefs aggregation and
propagation, and formulate reduction rules to reduce the size of the BDDs and
allow efficient computation structures for collaborative automated reasoning.

</details>


### [89] [Fine-grained Timing Analysis of Digital Integrated Circuits in Answer Set Programming](https://arxiv.org/abs/2507.11150)
*Alessandro Bertagnon,Marcello Dalpasso,Michele Favalli,Marco Gavanelli*

Main category: cs.AI

TL;DR: The paper addresses the challenge of computing the actual maximum delay in integrated circuits, using Answer Set Programming (ASP) for accurate results.


<details>
  <summary>Details</summary>
Motivation: Static Timing Analysis provides an upper bound for circuit delay, leading to suboptimal processor speeds. The work aims to compute the exact maximum delay for better performance.

Method: The problem is modeled in Answer Set Programming (ASP), with non-trivial encodings proposed to handle its computational hardness.

Result: Experiments demonstrate ASP's viability for solving complex hardware design problems.

Conclusion: ASP offers a promising solution for accurately determining circuit delays, improving system performance.

Abstract: In the design of integrated circuits, one critical metric is the maximum
delay introduced by combinational modules within the circuit. This delay is
crucial because it represents the time required to perform a computation: in an
Arithmetic-Logic Unit it represents the maximum time taken by the circuit to
perform an arithmetic operation. When such a circuit is part of a larger,
synchronous system, like a CPU, the maximum delay directly impacts the maximum
clock frequency of the entire system. Typically, hardware designers use Static
Timing Analysis to compute an upper bound of the maximum delay because it can
be determined in polynomial time. However, relying on this upper bound can lead
to suboptimal processor speeds, thereby missing performance opportunities. In
this work, we tackle the challenging task of computing the actual maximum
delay, rather than an approximate value. Since the problem is computationally
hard, we model it in Answer Set Programming (ASP), a logic language featuring
extremely efficient solvers. We propose non-trivial encodings of the problem
into ASP. Experimental results show that ASP is a viable solution to address
complex problems in hardware design.

</details>


### [90] [DuetGraph: Coarse-to-Fine Knowledge Graph Reasoning with Dual-Pathway Global-Local Fusion](https://arxiv.org/abs/2507.11229)
*Jin Li,Zezhong Ding,Xike Xie*

Main category: cs.AI

TL;DR: DuetGraph improves KG reasoning by segregating global and local information processing, introducing a coarse-to-fine optimization to mitigate score over-smoothing and enhance performance.


<details>
  <summary>Details</summary>
Motivation: Existing KG reasoning methods suffer from score over-smoothing, blurring distinctions between correct and incorrect answers, which hampers reasoning effectiveness.

Method: DuetGraph uses a dual-pathway global-local fusion, separating local (message passing) and global (attention) processing. It also employs coarse-to-fine optimization by partitioning entities into high- and low-score subsets.

Result: DuetGraph achieves SOTA performance with an 8.7% improvement in reasoning quality and 1.8× faster training efficiency.

Conclusion: DuetGraph effectively addresses over-smoothing, enhancing KG reasoning performance and efficiency.

Abstract: Knowledge graphs (KGs) are vital for enabling knowledge reasoning across
various domains. Recent KG reasoning methods that integrate both global and
local information have achieved promising results. However, existing methods
often suffer from score over-smoothing, which blurs the distinction between
correct and incorrect answers and hinders reasoning effectiveness. To address
this, we propose DuetGraph, a coarse-to-fine KG reasoning mechanism with
dual-pathway global-local fusion. DuetGraph tackles over-smoothing by
segregating -- rather than stacking -- the processing of local (via message
passing) and global (via attention) information into two distinct pathways,
preventing mutual interference and preserving representational discrimination.
In addition, DuetGraph introduces a coarse-to-fine optimization, which
partitions entities into high- and low-score subsets. This strategy narrows the
candidate space and sharpens the score gap between the two subsets, which
alleviates over-smoothing and enhances inference quality. Extensive experiments
on various datasets demonstrate that DuetGraph achieves state-of-the-art (SOTA)
performance, with up to an 8.7% improvement in reasoning quality and a
1.8$\times$ acceleration in training efficiency.

</details>


### [91] [Taming Uncertainty via Automation: Observing, Analyzing, and Optimizing Agentic AI Systems](https://arxiv.org/abs/2507.11277)
*Dany Moshkovich,Sergey Zeltyn*

Main category: cs.AI

TL;DR: AgentOps is a framework for managing uncertainty in LLM-powered agentic systems, addressing needs of developers, testers, SREs, and business users through a six-stage automation pipeline.


<details>
  <summary>Details</summary>
Motivation: Traditional observability practices fail to address the unique uncertainties in LLM-powered agentic systems, necessitating a specialized framework.

Method: Introduces AgentOps, a six-stage automation pipeline for observing, analyzing, optimizing, and automating agentic AI systems.

Result: The framework enables safe, adaptive, and effective operation by taming uncertainty through automation.

Conclusion: AgentOps provides a comprehensive solution for managing agentic AI systems, emphasizing automation to handle uncertainty.

Abstract: Large Language Models (LLMs) are increasingly deployed within agentic
systems-collections of interacting, LLM-powered agents that execute complex,
adaptive workflows using memory, tools, and dynamic planning. While enabling
powerful new capabilities, these systems also introduce unique forms of
uncertainty stemming from probabilistic reasoning, evolving memory states, and
fluid execution paths. Traditional software observability and operations
practices fall short in addressing these challenges.
  This paper introduces AgentOps: a comprehensive framework for observing,
analyzing, optimizing, and automating operation of agentic AI systems. We
identify distinct needs across four key roles-developers, testers, site
reliability engineers (SREs), and business users-each of whom engages with the
system at different points in its lifecycle. We present the AgentOps Automation
Pipeline, a six-stage process encompassing behavior observation, metric
collection, issue detection, root cause analysis, optimized recommendations,
and runtime automation. Throughout, we emphasize the critical role of
automation in managing uncertainty and enabling self-improving AI systems-not
by eliminating uncertainty, but by taming it to ensure safe, adaptive, and
effective operation.

</details>


### [92] [Opus: A Prompt Intention Framework for Complex Workflow Generation](https://arxiv.org/abs/2507.11288)
*Théo Fagnoni,Mahsun Altin,Chia En Chung,Phillip Kingston,Alan Tuning,Dana O. Mohamed,Inès Adnani*

Main category: cs.AI

TL;DR: The Opus Prompt Intention Framework improves workflow generation with LLMs by adding an intention capture layer, enhancing output quality and scalability.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of generating logical and meaningful workflows from complex user queries using LLMs.

Method: Introduces an intermediate intention capture layer (Opus Workflow Intention Framework) to extract and interpret workflow signals and intentions from queries.

Result: Shows consistent improvements in workflow similarity metrics, especially for complex queries.

Conclusion: The framework significantly enhances workflow generation quality, particularly for mixed-intention queries.

Abstract: This paper introduces the Opus Prompt Intention Framework, designed to
improve complex Workflow Generation with instruction-tuned Large Language
Models (LLMs). We propose an intermediate Intention Capture layer between user
queries and Workflow Generation, implementing the Opus Workflow Intention
Framework, which consists of extracting Workflow Signals from user queries,
interpreting them into structured Workflow Intention objects, and generating
Workflows based on these Intentions. Our results show that this layer enables
LLMs to produce logical and meaningful outputs that scale reliably as query
complexity increases. On a synthetic benchmark of 1,000 multi-intent
query-Workflow(s) pairs, applying the Opus Prompt Intention Framework to
Workflow Generation yields consistent improvements in semantic Workflow
similarity metrics. In this paper, we introduce the Opus Prompt Intention
Framework by applying the concepts of Workflow Signal and Workflow Intention to
LLM-driven Workflow Generation. We present a reproducible, customizable
LLM-based Intention Capture system to extract Workflow Signals and Workflow
Intentions from user queries. Finally, we provide empirical evidence that the
proposed system significantly improves Workflow Generation quality compared to
direct generation from user queries, particularly in cases of Mixed Intention
Elicitation.

</details>


### [93] [Contestability in Quantitative Argumentation](https://arxiv.org/abs/2507.11323)
*Xiang Yin,Nico Potyka,Antonio Rago,Timotheus Kampik,Francesca Toni*

Main category: cs.AI

TL;DR: The paper proposes using Edge-Weighted Quantitative Bipolar Argumentation Frameworks (EW-QBAFs) and gradient-based relation attribution explanations (G-RAEs) to align AI decisions with human preferences for contestability.


<details>
  <summary>Details</summary>
Motivation: To address the lack of attention on EW-QBAFs for contestable AI, ensuring AI-driven decisions align with human preferences.

Method: Introduces G-RAEs to quantify sensitivity of argument strength to edge weight changes and develops an iterative algorithm for weight adjustments.

Result: Experimental evaluation on synthetic EW-QBAFs shows the approach effectively achieves desired argument strength.

Conclusion: The proposed method successfully enables contestability in AI decisions using EW-QBAFs and G-RAEs.

Abstract: Contestable AI requires that AI-driven decisions align with human
preferences. While various forms of argumentation have been shown to support
contestability, Edge-Weighted Quantitative Bipolar Argumentation Frameworks
(EW-QBAFs) have received little attention. In this work, we show how EW-QBAFs
can be deployed for this purpose. Specifically, we introduce the contestability
problem for EW-QBAFs, which asks how to modify edge weights (e.g., preferences)
to achieve a desired strength for a specific argument of interest (i.e., a
topic argument). To address this problem, we propose gradient-based relation
attribution explanations (G-RAEs), which quantify the sensitivity of the topic
argument's strength to changes in individual edge weights, thus providing
interpretable guidance for weight adjustments towards contestability. Building
on G-RAEs, we develop an iterative algorithm that progressively adjusts the
edge weights to attain the desired strength. We evaluate our approach
experimentally on synthetic EW-QBAFs that simulate the structural
characteristics of personalised recommender systems and multi-layer
perceptrons, and demonstrate that it can solve the problem effectively.

</details>


### [94] [CogDDN: A Cognitive Demand-Driven Navigation with Decision Optimization and Dual-Process Thinking](https://arxiv.org/abs/2507.11334)
*Yuehao Huang,Liang Liu,Shuangming Lei,Yukai Ma,Hao Su,Jianbiao Mei,Pengxiang Zhao,Yaqing Gu,Yong Liu,Jiajun Lv*

Main category: cs.AI

TL;DR: CogDDN is a VLM-based framework for demand-driven navigation that integrates fast and slow thinking systems, improving navigation accuracy by 15% over traditional methods.


<details>
  <summary>Details</summary>
Motivation: Mobile robots need to navigate unknown environments based on human intent, but traditional data-driven methods lack generalization in unseen scenarios.

Method: CogDDN uses semantic alignment of objects with instructions, a dual-process decision-making module (Heuristic and Analytic Processes), and Chain of Thought reasoning.

Result: Outperforms single-view camera-only methods by 15% in navigation accuracy on the AI2Thor simulator with the ProcThor dataset.

Conclusion: CogDDN enhances adaptability and performance in demand-driven navigation by emulating human cognitive mechanisms.

Abstract: Mobile robots are increasingly required to navigate and interact within
unknown and unstructured environments to meet human demands. Demand-driven
navigation (DDN) enables robots to identify and locate objects based on
implicit human intent, even when object locations are unknown. However,
traditional data-driven DDN methods rely on pre-collected data for model
training and decision-making, limiting their generalization capability in
unseen scenarios. In this paper, we propose CogDDN, a VLM-based framework that
emulates the human cognitive and learning mechanisms by integrating fast and
slow thinking systems and selectively identifying key objects essential to
fulfilling user demands. CogDDN identifies appropriate target objects by
semantically aligning detected objects with the given instructions.
Furthermore, it incorporates a dual-process decision-making module, comprising
a Heuristic Process for rapid, efficient decisions and an Analytic Process that
analyzes past errors, accumulates them in a knowledge base, and continuously
improves performance. Chain of Thought (CoT) reasoning strengthens the
decision-making process. Extensive closed-loop evaluations on the AI2Thor
simulator with the ProcThor dataset show that CogDDN outperforms single-view
camera-only methods by 15%, demonstrating significant improvements in
navigation accuracy and adaptability. The project page is available at
https://yuehaohuang.github.io/CogDDN/.

</details>


### [95] [Foundation Models for Logistics: Toward Certifiable, Conversational Planning Interfaces](https://arxiv.org/abs/2507.11352)
*Yunhao Yang,Neel P. Bhatt,Christian Ellis,Alvaro Velasquez,Zhangyang Wang,Ufuk Topcu*

Main category: cs.AI

TL;DR: A neurosymbolic framework combines natural-language dialogue with verifiable guarantees for logistics planning, outperforming GPT-4.1 in speed and accuracy.


<details>
  <summary>Details</summary>
Motivation: Address the limitations of traditional methods (slow, idealized) and LLMs (unsafe, prone to errors) in logistics decision-making.

Method: Uses a neurosymbolic framework to convert user requests into structured plans, quantify uncertainty, and invoke interactive clarification loops.

Result: A lightweight model, fine-tuned on 100 examples, surpasses GPT-4.1 in zero-shot performance and reduces latency by 50%.

Conclusion: The framework offers a practical solution for certifiable, real-time, and user-aligned logistics decision-making.

Abstract: Logistics operators, from battlefield coordinators rerouting airlifts ahead
of a storm to warehouse managers juggling late trucks, often face life-critical
decisions that demand both domain expertise and rapid and continuous
replanning. While popular methods like integer programming yield logistics
plans that satisfy user-defined logical constraints, they are slow and assume
an idealized mathematical model of the environment that does not account for
uncertainty. On the other hand, large language models (LLMs) can handle
uncertainty and promise to accelerate replanning while lowering the barrier to
entry by translating free-form utterances into executable plans, yet they
remain prone to misinterpretations and hallucinations that jeopardize safety
and cost. We introduce a neurosymbolic framework that pairs the accessibility
of natural-language dialogue with verifiable guarantees on goal interpretation.
It converts user requests into structured planning specifications, quantifies
its own uncertainty at the field and token level, and invokes an interactive
clarification loop whenever confidence falls below an adaptive threshold. A
lightweight model, fine-tuned on just 100 uncertainty-filtered examples,
surpasses the zero-shot performance of GPT-4.1 while cutting inference latency
by nearly 50%. These preliminary results highlight a practical path toward
certifiable, real-time, and user-aligned decision-making for complex logistics.

</details>


### [96] [Modeling Code: Is Text All You Need?](https://arxiv.org/abs/2507.11467)
*Daniel Nichols,Konstantinos Parasyris,Harshitha Menon,Brian R. Bartoldson,Giorgis Georgakoudis,Tal Ben-Nun,Abhinav Bhatele*

Main category: cs.AI

TL;DR: A novel approach combines code-as-text modeling with structured forms to enhance reasoning capabilities in Code LLMs.


<details>
  <summary>Details</summary>
Motivation: Transformer-based models struggle with structured, analytical properties of code like control and data flow, despite their popularity in tasks like generation and translation.

Method: Introduces a method to integrate structured data modeling (e.g., graph neural networks) with the generative capabilities of modern LLMs.

Result: Combines the strengths of text-based and structured modeling for improved code reasoning.

Conclusion: The proposed approach bridges the gap between generative scale and structured reasoning in Code LLMs.

Abstract: Code LLMs have become extremely popular recently for modeling source code
across a variety of tasks, such as generation, translation, and summarization.
However, transformer-based models are limited in their capabilities to reason
through structured, analytical properties of code, such as control and data
flow. Previous work has explored the modeling of these properties with
structured data and graph neural networks. However, these approaches lack the
generative capabilities and scale of modern LLMs. In this work, we introduce a
novel approach to combine the strengths of modeling both code as text and more
structured forms.

</details>


### [97] [Chain of Thought Monitorability: A New and Fragile Opportunity for AI Safety](https://arxiv.org/abs/2507.11473)
*Tomek Korbak,Mikita Balesni,Elizabeth Barnes,Yoshua Bengio,Joe Benton,Joseph Bloom,Mark Chen,Alan Cooney,Allan Dafoe,Anca Dragan,Scott Emmons,Owain Evans,David Farhi,Ryan Greenblatt,Dan Hendrycks,Marius Hobbhahn,Evan Hubinger,Geoffrey Irving,Erik Jenner,Daniel Kokotajlo,Victoria Krakovna,Shane Legg,David Lindner,David Luan,Aleksander Mądry,Julian Michael,Neel Nanda,Dave Orr,Jakub Pachocki,Ethan Perez,Mary Phuong,Fabien Roger,Joshua Saxe,Buck Shlegeris,Martín Soto,Eric Steinberger,Jasmine Wang,Wojciech Zaremba,Bowen Baker,Rohin Shah,Vlad Mikulik*

Main category: cs.AI

TL;DR: AI systems using human-like thought chains (CoT) can be monitored for misbehavior, though imperfectly. Further research and investment in CoT monitoring are recommended alongside existing safety methods.


<details>
  <summary>Details</summary>
Motivation: To explore AI safety by leveraging human-like thought chains for monitoring misbehavior.

Method: Monitoring chains of thought (CoT) in AI systems to detect intent to misbehave.

Result: CoT monitoring is promising but imperfect; requires further research and development.

Conclusion: Frontier model developers should consider CoT monitorability in their decisions to enhance AI safety.

Abstract: AI systems that "think" in human language offer a unique opportunity for AI
safety: we can monitor their chains of thought (CoT) for the intent to
misbehave. Like all other known AI oversight methods, CoT monitoring is
imperfect and allows some misbehavior to go unnoticed. Nevertheless, it shows
promise and we recommend further research into CoT monitorability and
investment in CoT monitoring alongside existing safety methods. Because CoT
monitorability may be fragile, we recommend that frontier model developers
consider the impact of development decisions on CoT monitorability.

</details>


### [98] [Perspective-Aware AI in Extended Reality](https://arxiv.org/abs/2507.11479)
*Daniel Platnick,Matti Gruener,Marjan Alirezaie,Kent Larson,Dava J. Newman,Hossein Rahnama*

Main category: cs.AI

TL;DR: PAiR integrates Perspective-Aware AI with XR to create adaptive, immersive experiences using user identity models called Chronicles.


<details>
  <summary>Details</summary>
Motivation: Current XR systems lack deep user modeling and cognitive context, limiting adaptive experiences.

Method: PAiR uses Chronicles—multimodal identity models—in a closed-loop system to link user states with XR environments, demonstrated in Unity-based scenarios.

Result: PAiR enables interpretable, context-aware XR experiences, validated through proof-of-concept implementations.

Conclusion: PAiR advances human-AI interaction by embedding perspective-based identity models into immersive systems.

Abstract: AI-enhanced Extended Reality (XR) aims to deliver adaptive, immersive
experiences-yet current systems fall short due to shallow user modeling and
limited cognitive context. We introduce Perspective-Aware AI in Extended
Reality (PAiR), a foundational framework for integrating Perspective-Aware AI
(PAi) with XR to enable interpretable, context-aware experiences grounded in
user identity. PAi is built on Chronicles: reasoning-ready identity models
learned from multimodal digital footprints that capture users' cognitive and
experiential evolution. PAiR employs these models in a closed-loop system
linking dynamic user states with immersive environments. We present PAiR's
architecture, detailing its modules and system flow, and demonstrate its
utility through two proof-of-concept scenarios implemented in the Unity-based
OpenDome engine. PAiR opens a new direction for human-AI interaction by
embedding perspective-based identity models into immersive systems.

</details>


### [99] [Illuminating the Three Dogmas of Reinforcement Learning under Evolutionary Light](https://arxiv.org/abs/2507.11482)
*Mani Hamidi,Terrence W. Deacon*

Main category: cs.AI

TL;DR: The paper critiques three core tenets of RL, proposing an evolutionary-inspired framework to rethink agency, learning objectives, and the reward hypothesis, with implications for biological learning and practical RL applications.


<details>
  <summary>Details</summary>
Motivation: To address conceptual limitations in RL by integrating evolutionary theory, aiming to better model biological learning and improve RL theory and practice.

Method: Revisits RL assumptions using evolutionary insights, discusses plausibility of intra-lifetime evolutionary dynamics in brains, and integrates origins-of-life theory for agency.

Result: Evolutionary insights enrich RL theory, but agency requires additional foundations from thermodynamics and origins-of-life theory.

Conclusion: Evolutionary theory can refine RL's core tenets, but agency demands broader interdisciplinary integration for a formal account.

Abstract: Three core tenets of reinforcement learning (RL)--concerning the definition
of agency, the objective of learning, and the scope of the reward
hypothesis--have been highlighted as key targets for conceptual revision, with
major implications for theory and application. We propose a framework, inspired
by open-ended evolutionary theory, to reconsider these three "dogmas." We
revisit each assumption and address related concerns raised alongside them. To
make our arguments relevant to RL as a model of biological learning, we first
establish that evolutionary dynamics can plausibly operate within living brains
over an individual's lifetime, and are not confined to cross-generational
processes. We begin by revisiting the second dogma, drawing on evolutionary
insights to enrich the "adaptation-rather-than-search" view of learning. We
then address the third dogma regarding the limits of the reward hypothesis,
using analogies from evolutionary fitness to illuminate the scalar reward vs.
multi-objective debate. After discussing practical implications for exploration
in RL, we turn to the first--and arguably most fundamental--issue: the absence
of a formal account of agency. We argue that unlike the other two problems, the
evolutionary paradigm alone cannot resolve the agency question, though it
gestures in a productive direction. We advocate integrating ideas from
origins-of-life theory, where the thermodynamics of sustenance and replication
offer promising foundations for understanding agency and resource-constrained
reinforcement learning in biological systems.

</details>


### [100] [DrafterBench: Benchmarking Large Language Models for Tasks Automation in Civil Engineering](https://arxiv.org/abs/2507.11527)
*Yinsheng Li,Zhen Dong,Yi Shao*

Main category: cs.AI

TL;DR: DrafterBench is a benchmark for evaluating LLM agents in technical drawing revision tasks in civil engineering, featuring 12 task types, 46 tools, and 1920 tasks.


<details>
  <summary>Details</summary>
Motivation: The need for systematic evaluation of LLM agents in industrial applications like civil engineering, particularly for technical drawing revision.

Method: Developed DrafterBench with real-world tasks, tools, and a large task set to assess LLM agents' capabilities in structured data comprehension, function execution, and more.

Result: An open-source benchmark with detailed accuracy and error analysis to evaluate and improve LLM agents for engineering tasks.

Conclusion: DrafterBench provides a rigorous tool for assessing and enhancing LLM agents' performance in industrial applications, with potential for broader engineering integration.

Abstract: Large Language Model (LLM) agents have shown great potential for solving
real-world problems and promise to be a solution for tasks automation in
industry. However, more benchmarks are needed to systematically evaluate
automation agents from an industrial perspective, for example, in Civil
Engineering. Therefore, we propose DrafterBench for the comprehensive
evaluation of LLM agents in the context of technical drawing revision, a
representation task in civil engineering. DrafterBench contains twelve types of
tasks summarized from real-world drawing files, with 46 customized
functions/tools and 1920 tasks in total. DrafterBench is an open-source
benchmark to rigorously test AI agents' proficiency in interpreting intricate
and long-context instructions, leveraging prior knowledge, and adapting to
dynamic instruction quality via implicit policy awareness. The toolkit
comprehensively assesses distinct capabilities in structured data
comprehension, function execution, instruction following, and critical
reasoning. DrafterBench offers detailed analysis of task accuracy and error
statistics, aiming to provide deeper insight into agent capabilities and
identify improvement targets for integrating LLMs in engineering applications.
Our benchmark is available at https://github.com/Eason-Li-AIS/DrafterBench,
with the test set hosted at
https://huggingface.co/datasets/Eason666/DrafterBench.

</details>


### [101] [How Many Instructions Can LLMs Follow at Once?](https://arxiv.org/abs/2507.11538)
*Daniel Jaroslawicz,Brendan Whiting,Parth Shah,Karime Maamari*

Main category: cs.AI

TL;DR: IFScale benchmark evaluates LLMs' instruction-following performance at high densities, revealing degradation patterns and biases, with top models achieving only 68% accuracy at 500 instructions.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks lack evaluation of LLMs' performance at high instruction densities, limiting understanding of real-world applicability.

Method: Introduces IFScale, a benchmark with 500 keyword-inclusion instructions for business report writing, testing 20 state-of-the-art models.

Result: Best models achieve 68% accuracy at 500 instructions; performance degrades with density, showing biases and error patterns.

Conclusion: IFScale highlights tradeoffs in instruction-dense prompts and provides insights for real-world LLM applications.

Abstract: Production-grade LLM systems require robust adherence to dozens or even
hundreds of instructions simultaneously. However, the instruction-following
capabilities of LLMs at high instruction densities have not yet been
characterized, as existing benchmarks only evaluate models on tasks with a
single or few instructions. We introduce IFScale, a simple benchmark of 500
keyword-inclusion instructions for a business report writing task to measure
how instruction-following performance degrades as instruction density
increases. We evaluate 20 state-of-the-art models across seven major providers
and find that even the best frontier models only achieve 68% accuracy at the
max density of 500 instructions. Our analysis reveals model size and reasoning
capability to correlate with 3 distinct performance degradation patterns, bias
towards earlier instructions, and distinct categories of instruction-following
errors. Our insights can help inform design of instruction-dense prompts in
real-world applications and highlight important performance-latency tradeoffs.
We open-source the benchmark and all results for further analysis at
https://distylai.github.io/IFScale.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [102] [Truth Sleuth and Trend Bender: AI Agents to fact-check YouTube videos and influence opinions](https://arxiv.org/abs/2507.10577)
*Logé Cécile,Ghori Rehan*

Main category: cs.CL

TL;DR: An AI-powered system with two agents, Truth Sleuth and Trend Bender, combats YouTube misinformation by fact-checking videos and engaging users in comments.


<details>
  <summary>Details</summary>
Motivation: Misinformation spreads rapidly on platforms like YouTube, threatening digital discourse. This paper aims to address this by leveraging AI for fact-checking and user engagement.

Method: Truth Sleuth uses RAG to fact-check claims, while Trend Bender generates persuasive comments. The system includes a self-evaluation loop for iterative improvement.

Result: High accuracy in fact-checking and effective user engagement were demonstrated in experiments and real-world YouTube deployment.

Conclusion: The system shows promise in combating misinformation and fostering informed online discussions through AI-driven interventions.

Abstract: Misinformation poses a significant threat in today's digital world, often
spreading rapidly through platforms like YouTube. This paper introduces a novel
approach to combating misinformation by developing an AI-powered system that
not only fact-checks claims made in YouTube videos but also actively engages
users in the comment section and challenge misleading narratives. Our system
comprises two main agents: Truth Sleuth and Trend Bender.
  Truth Sleuth extracts claims from a YouTube video, uses a Retrieval-Augmented
Generation (RAG) approach - drawing on sources like Wikipedia, Google Search,
Google FactCheck - to accurately assess their veracity and generates a nuanced
and comprehensive report. Through rigorous prompt engineering, Trend Bender
leverages this report along with a curated corpus of relevant articles to
generate insightful and persuasive comments designed to stimulate a productive
debate. With a carefully set up self-evaluation loop, this agent is able to
iteratively improve its style and refine its output.
  We demonstrate the system's capabilities through experiments on established
benchmark datasets and a real-world deployment on YouTube, showcasing its
potential to engage users and potentially influence perspectives. Our findings
highlight the high accuracy of our fact-checking agent, and confirm the
potential of AI-driven interventions in combating misinformation and fostering
a more informed online space.

</details>


### [103] [An Offline Mobile Conversational Agent for Mental Health Support: Learning from Emotional Dialogues and Psychological Texts with Student-Centered Evaluation](https://arxiv.org/abs/2507.10580)
*Vimaleswar A,Prabhu Nandan Sahu,Nilesh Kumar Sahu,Haroon R Lone*

Main category: cs.CL

TL;DR: EmoSApp is an offline, smartphone-based mental health app using fine-tuned LLMs for empathetic, coherent responses without internet reliance.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges like limited accessibility, connectivity, and privacy in digital mental health support.

Method: Fine-tuned LLaMA-3.2-1B-Instruct on a custom mental-health QA dataset, deployed offline using Torchtune and Executorch.

Result: Qualitative evaluations show coherent, empathetic responses; quantitative tests confirm efficacy in low-resource settings.

Conclusion: EmoSApp exemplifies portable, secure AI-driven mental health solutions, setting a blueprint for future innovations.

Abstract: Mental health plays a crucial role in the overall well-being of an
individual. In recent years, digital platforms have been increasingly used to
expand mental health and emotional support. However, there are persistent
challenges related to limited user accessibility, internet connectivity, and
data privacy, which highlight the need for an offline, smartphone-based
solution. To address these challenges, we propose EmoSApp (Emotional Support
App): an entirely offline, smartphone-based conversational app designed for
mental health and emotional support. The system leverages Large Language Models
(LLMs), specifically fine-tuned, quantized and deployed using Torchtune and
Executorch for resource-constrained devices, allowing all inferences to occur
on the smartphone. To equip EmoSApp with robust domain expertise, we fine-tuned
the LLaMA-3.2-1B-Instruct model on our custom curated ``Knowledge dataset'' of
14,582 mental-health QA pairs, along with the multi-turn conversational data.
  Through qualitative human evaluation with the student population, we
demonstrate that EmoSApp has the ability to respond coherently, empathetically,
maintain interactive dialogue, and provide relevant suggestions to user's
mental health problems. Additionally, quantitative evaluations on nine standard
commonsense and reasoning benchmarks demonstrate the efficacy of our
fine-tuned, quantized model in low-resource settings. By prioritizing on-device
deployment and specialized domain adaptation, EmoSApp serves as a blueprint for
future innovations in portable, secure, and highly tailored AI-driven mental
health solutions.

</details>


### [104] [Transforming Sensitive Documents into Quantitative Data: An AI-Based Preprocessing Toolchain for Structured and Privacy-Conscious Analysis](https://arxiv.org/abs/2507.10582)
*Anders Ledberg,Anna Thalén*

Main category: cs.CL

TL;DR: A modular toolchain for processing unstructured text from sensitive sources enables privacy-conscious, large-scale analysis using open-weight models on local hardware.


<details>
  <summary>Details</summary>
Motivation: Unstructured text from legal, medical, and administrative sources is underutilized due to privacy and heterogeneity challenges.

Method: The toolchain uses LLM prompting for standardization, summarization, translation, and anonymization, validated via manual review and predictive evaluation.

Result: The toolchain effectively anonymizes and standardizes text, retaining semantic content, as demonstrated on Swedish court decisions.

Conclusion: The toolchain facilitates large-scale research on sensitive textual data, overcoming privacy and heterogeneity barriers.

Abstract: Unstructured text from legal, medical, and administrative sources offers a
rich but underutilized resource for research in public health and the social
sciences. However, large-scale analysis is hampered by two key challenges: the
presence of sensitive, personally identifiable information, and significant
heterogeneity in structure and language. We present a modular toolchain that
prepares such text data for embedding-based analysis, relying entirely on
open-weight models that run on local hardware, requiring only a
workstation-level GPU and supporting privacy-sensitive research.
  The toolchain employs large language model (LLM) prompting to standardize,
summarize, and, when needed, translate texts to English for greater
comparability. Anonymization is achieved via LLM-based redaction, supplemented
with named entity recognition and rule-based methods to minimize the risk of
disclosure. We demonstrate the toolchain on a corpus of 10,842 Swedish court
decisions under the Care of Abusers Act (LVM), comprising over 56,000 pages.
Each document is processed into an anonymized, standardized summary and
transformed into a document-level embedding. Validation, including manual
review, automated scanning, and predictive evaluation shows the toolchain
effectively removes identifying information while retaining semantic content.
As an illustrative application, we train a predictive model using embedding
vectors derived from a small set of manually labeled summaries, demonstrating
the toolchain's capacity for semi-automated content analysis at scale.
  By enabling structured, privacy-conscious analysis of sensitive documents,
our toolchain opens new possibilities for large-scale research in domains where
textual data was previously inaccessible due to privacy and heterogeneity
constraints.

</details>


### [105] [A Taxonomy for Design and Evaluation of Prompt-Based Natural Language Explanations](https://arxiv.org/abs/2507.10585)
*Isar Nejadgholi,Mona Omidyeganeh,Marc-Antoine Drouin,Jonathan Boisvert*

Main category: cs.CL

TL;DR: The paper proposes an updated XAI taxonomy for Natural Language Explanations (NLEs) to improve AI governance, focusing on context, generation/presentation, and evaluation dimensions.


<details>
  <summary>Details</summary>
Motivation: The rise of large language models necessitates clear NLEs for transparency, requiring a structured approach to their design and governance.

Method: The authors adapt Explainable AI (XAI) literature to create a taxonomy for NLEs, covering context, generation/presentation, and evaluation.

Result: The taxonomy offers a framework for stakeholders to characterize, design, and enhance NLEs for transparent AI systems.

Conclusion: The updated XAI taxonomy aids researchers, auditors, and policymakers in improving NLEs for better AI governance.

Abstract: Effective AI governance requires structured approaches for stakeholders to
access and verify AI system behavior. With the rise of large language models,
Natural Language Explanations (NLEs) are now key to articulating model
behavior, which necessitates a focused examination of their characteristics and
governance implications. We draw on Explainable AI (XAI) literature to create
an updated XAI taxonomy, adapted to prompt-based NLEs, across three dimensions:
(1) Context, including task, data, audience, and goals; (2) Generation and
Presentation, covering generation methods, inputs, interactivity, outputs, and
forms; and (3) Evaluation, focusing on content, presentation, and user-centered
properties, as well as the setting of the evaluation. This taxonomy provides a
framework for researchers, auditors, and policymakers to characterize, design,
and enhance NLEs for transparent AI systems.

</details>


### [106] [AutoRAG-LoRA: Hallucination-Triggered Knowledge Retuning via Lightweight Adapters](https://arxiv.org/abs/2507.10586)
*Kaushik Dwivedi,Padmanabh Patanjali Mishra*

Main category: cs.CL

TL;DR: AutoRAG-LoRA is a framework using LoRA-based adapters and KL-regularized training to reduce hallucinations in LLMs by grounding responses in retrieved evidence and enabling feedback correction.


<details>
  <summary>Details</summary>
Motivation: LLMs exhibit hallucinations (factual inaccuracies), which reduce trust in real-world applications. Addressing this is critical for reliable deployment.

Method: The framework combines automated prompt rewriting, hybrid retrieval, and LoRA-based adapter tuning. It includes a hallucination detection module with classifier-based and self-evaluation techniques, plus a feedback correction loop using contrastive KL loss.

Result: AutoRAG-LoRA significantly reduces factual drift while maintaining model efficiency and modularity.

Conclusion: The framework effectively mitigates hallucinations in LLMs, enhancing factual accuracy without compromising performance.

Abstract: Large Language Models (LLMs) have demonstrated remarkable fluency across a
range of natural language tasks, yet remain vulnerable to hallucinations -
factual inaccuracies that undermine trust in real world deployment. We present
AutoRAG-LoRA, a modular framework for Retrieval-Augmented Generation (RAG) that
tackles hallucination in large language models through lightweight LoRA-based
adapters and KL-regularized training. Our pipeline integrates automated prompt
rewriting, hybrid retrieval, and low-rank adapter tuning to ground responses in
retrieved evidence. A hallucination detection module, using both
classifier-based and self-evaluation techniques, assigns confidence scores to
generated outputs, triggering an optional feedback correction loop. This loop
enforces factual alignment via contrastive KL loss and adapter fine tuning. We
demonstrate that AutoRAG-LoRA significantly reduces the factual drift while
preserving the efficiency and modularity of the model.

</details>


### [107] [Anthropomimetic Uncertainty: What Verbalized Uncertainty in Language Models is Missing](https://arxiv.org/abs/2507.10587)
*Dennis Ulmer,Alexandra Lorson,Ivan Titov,Christian Hardmeier*

Main category: cs.CL

TL;DR: The paper discusses the need for LLMs to communicate uncertainty effectively to users, advocating for anthropomimetic uncertainty—emulating human-like communication to enhance trust and collaboration.


<details>
  <summary>Details</summary>
Motivation: LLMs often provide overly confident outputs, undermining trust. Effective uncertainty communication is crucial for reliable human-machine collaboration.

Method: The paper reviews human uncertainty communication, surveys NLP research, and analyzes biases in verbalized uncertainty.

Result: Findings highlight overlooked biases and the need for personalized, human-like uncertainty communication in LLMs.

Conclusion: The paper proposes anthropomimetic uncertainty as a future research direction to improve NLP systems' trustworthiness.

Abstract: Human users increasingly rely on natural language interactions with large
language models (LLMs) in order to receive help on a large variety of tasks and
problems. However, the trustworthiness and perceived legitimacy of LLMs is
undermined by the fact that their output is frequently stated in very confident
terms, even when its accuracy is questionable. Therefore, there is a need to
signal the confidence of the language model to a user in order to reap the
benefits of human-machine collaboration and mitigate potential harms.
Verbalized uncertainty is the expression of confidence with linguistic means,
an approach that integrates perfectly into language-based interfaces.
Nevertheless, most recent research in natural language processing (NLP)
overlooks the nuances surrounding human uncertainty communication and the data
biases that influence machine uncertainty communication. We argue for
anthropomimetic uncertainty, meaning that intuitive and trustworthy uncertainty
communication requires a degree of linguistic authenticity and personalization
to the user, which could be achieved by emulating human communication. We
present a thorough overview over the research in human uncertainty
communication, survey ongoing research, and perform additional analyses to
demonstrate so-far overlooked biases in verbalized uncertainty. We conclude by
pointing out unique factors in human-machine communication of uncertainty and
deconstruct anthropomimetic uncertainty into future research directions for
NLP.

</details>


### [108] [PLEX: Perturbation-free Local Explanations for LLM-Based Text Classification](https://arxiv.org/abs/2507.10596)
*Yogachandran Rahulamathavan,Misbah Farooq,Varuna De Silva*

Main category: cs.CL

TL;DR: PLEX is a perturbation-free method for explaining LLM predictions, using contextual embeddings and a Siamese network to align with feature importance, achieving high agreement with LIME/SHAP but with significantly lower computational cost.


<details>
  <summary>Details</summary>
Motivation: LLMs lack interpretability, and existing XAI methods like LIME/SHAP are computationally expensive due to reliance on perturbations.

Method: PLEX leverages LLM embeddings and a Siamese network to align with feature importance scores, eliminating the need for perturbations.

Result: PLEX achieves >92% agreement with LIME/SHAP, identifies influential words accurately, and reduces computational overhead by orders of magnitude.

Conclusion: PLEX provides efficient, perturbation-free explanations for LLM predictions, making it a promising solution for explainable text classification.

Abstract: Large Language Models (LLMs) excel in text classification, but their
complexity hinders interpretability, making it difficult to understand the
reasoning behind their predictions. Explainable AI (XAI) methods like LIME and
SHAP offer local explanations by identifying influential words, but they rely
on computationally expensive perturbations. These methods typically generate
thousands of perturbed sentences and perform inferences on each, incurring a
substantial computational burden, especially with LLMs. To address this, we
propose \underline{P}erturbation-free \underline{L}ocal \underline{Ex}planation
(PLEX), a novel method that leverages the contextual embeddings extracted from
the LLM and a ``Siamese network" style neural network trained to align with
feature importance scores. This one-off training eliminates the need for
subsequent perturbations, enabling efficient explanations for any new sentence.
We demonstrate PLEX's effectiveness on four different classification tasks
(sentiment, fake news, fake COVID-19 news and depression), showing more than
92\% agreement with LIME and SHAP. Our evaluation using a ``stress test"
reveals that PLEX accurately identifies influential words, leading to a similar
decline in classification accuracy as observed with LIME and SHAP when these
words are removed. Notably, in some cases, PLEX demonstrates superior
performance in capturing the impact of key features. PLEX dramatically
accelerates explanation, reducing time and computational overhead by two and
four orders of magnitude, respectively. This work offers a promising solution
for explainable LLM-based text classification.

</details>


### [109] [Emergence of Hierarchical Emotion Organization in Large Language Models](https://arxiv.org/abs/2507.10599)
*Bo Zhao,Maya Okawa,Eric J. Bigelow,Rose Yu,Tomer Ullman,Ekdeep Singh Lubana,Hidenori Tanaka*

Main category: cs.CL

TL;DR: LLMs form hierarchical emotion trees aligning with human psychology, with biases in emotion recognition for underrepresented groups. Human studies show parallels, suggesting LLMs internalize social perceptions.


<details>
  <summary>Details</summary>
Motivation: Understanding how LLMs model emotional states is crucial for ethical deployment, inspired by psychological emotion wheels.

Method: Analyzed probabilistic dependencies between emotional states in LLM outputs, comparing hierarchical structures and biases across socioeconomic personas.

Result: LLMs naturally form hierarchical emotion trees, with larger models showing more complexity. Systematic biases were found in emotion recognition for underrepresented groups.

Conclusion: LLMs exhibit emergent emotional reasoning and internalize social perceptions, suggesting cognitively-grounded theories can improve model evaluations.

Abstract: As large language models (LLMs) increasingly power conversational agents,
understanding how they model users' emotional states is critical for ethical
deployment. Inspired by emotion wheels -- a psychological framework that argues
emotions organize hierarchically -- we analyze probabilistic dependencies
between emotional states in model outputs. We find that LLMs naturally form
hierarchical emotion trees that align with human psychological models, and
larger models develop more complex hierarchies. We also uncover systematic
biases in emotion recognition across socioeconomic personas, with compounding
misclassifications for intersectional, underrepresented groups. Human studies
reveal striking parallels, suggesting that LLMs internalize aspects of social
perception. Beyond highlighting emergent emotional reasoning in LLMs, our
results hint at the potential of using cognitively-grounded theories for
developing better model evaluations.

</details>


### [110] [Language Models for Adult Service Website Text Analysis](https://arxiv.org/abs/2507.10743)
*Nickolas Freeman,Thanh Nguyen,Gregory Bott,Jason Parton,Collin Francel*

Main category: cs.CL

TL;DR: The paper explores language modeling for analyzing Adult Service Website (ASW) ad text to combat sex trafficking, showing custom transformers outperform pre-trained models in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: ASW data is crucial for identifying sex trafficking victims, but text analysis is challenging due to obfuscation, emojis, and poor grammar.

Method: The study evaluates various language models, including custom transformers, pre-trained models, and simple retrieval methods, on ASW text.

Result: Custom transformers trained with minimal GPU resources outperform BERT-base, RoBERTa, and ModernBERT in accuracy, recall, F1, and ROC AUC.

Conclusion: The custom models advance ASW text analysis, aiding tasks like graph decomposition, clustering, and emoji interpretation in illicit contexts.

Abstract: Sex trafficking refers to the use of force, fraud, or coercion to compel an
individual to perform in commercial sex acts against their will. Adult service
websites (ASWs) have and continue to be linked to sex trafficking, offering a
platform for traffickers to advertise their victims. Thus, organizations
involved in the fight against sex trafficking often use ASW data when
attempting to identify potential sex trafficking victims. A critical challenge
in transforming ASW data into actionable insight is text analysis. Previous
research using ASW data has shown that ASW ad text is important for linking
ads. However, working with this text is challenging due to its extensive use of
emojis, poor grammar, and deliberate obfuscation to evade law enforcement
scrutiny. We conduct a comprehensive study of language modeling approaches for
this application area, including simple information retrieval methods,
pre-trained transformers, and custom transformer models. We demonstrate that
characteristics of ASW text data allow efficient custom transformer models to
be trained with relatively small GPU resources and used efficiently for
inference on consumer hardware. Our custom models outperform fine-tuned
variants of well-known encoder-only transformer models, including BERT-base,
RoBERTa, and ModernBERT, on accuracy, recall, F1 score, and ROC AUC. We
demonstrate the use of our best-performing custom configuration on three tasks
related to ASW data analysis: (i) decomposing the giant component in a graph
representation of ASW data, (ii) clustering ASW ad text, and (iii) using the
learned token embeddings to understand the use of emojis in the illicit context
we study. The models we develop represent a significant advancement in ASW text
analysis, which can be leveraged in a variety of downstream applications and
research.

</details>


### [111] [Applying Text Embedding Models for Efficient Analysis in Labeled Property Graphs](https://arxiv.org/abs/2507.10772)
*Michal Podstawski*

Main category: cs.CL

TL;DR: The paper explores using pretrained text embedding models to enhance semantic analysis in labeled property graphs, improving tasks like node classification and relation prediction without altering the graph structure.


<details>
  <summary>Details</summary>
Motivation: To leverage rich textual attributes in property graphs for better analytical tasks by incorporating semantic understanding.

Method: Integrates pretrained language model embeddings into the graph pipeline while preserving the graph structure.

Result: Demonstrates that textual semantics significantly improve accuracy and interpretability in property graph analysis.

Conclusion: Textual semantics, when embedded into property graphs, enhance analytical tasks effectively without structural changes.

Abstract: Labeled property graphs often contain rich textual attributes that can
enhance analytical tasks when properly leveraged. This work explores the use of
pretrained text embedding models to enable efficient semantic analysis in such
graphs. By embedding textual node and edge properties, we support downstream
tasks including node classification and relation prediction with improved
contextual understanding. Our approach integrates language model embeddings
into the graph pipeline without altering its structure, demonstrating that
textual semantics can significantly enhance the accuracy and interpretability
of property graph analysis.

</details>


### [112] [Can Multimodal Foundation Models Understand Schematic Diagrams? An Empirical Study on Information-Seeking QA over Scientific Papers](https://arxiv.org/abs/2507.10787)
*Yilun Zhao,Chengye Wang,Chuhan Li,Arman Cohan*

Main category: cs.CL

TL;DR: MISS-QA is a new benchmark for evaluating models' ability to interpret scientific schematic diagrams, revealing a performance gap between models and humans.


<details>
  <summary>Details</summary>
Motivation: To assess and improve models' comprehension of schematic diagrams in scientific literature.

Method: Created a benchmark with 1,500 expert-annotated examples from 465 papers, testing 18 multimodal models.

Result: Significant performance gap between models and humans; models struggle with unanswerable questions.

Conclusion: Highlights limitations of current models and provides insights for improving multimodal scientific comprehension.

Abstract: This paper introduces MISS-QA, the first benchmark specifically designed to
evaluate the ability of models to interpret schematic diagrams within
scientific literature. MISS-QA comprises 1,500 expert-annotated examples over
465 scientific papers. In this benchmark, models are tasked with interpreting
schematic diagrams that illustrate research overviews and answering
corresponding information-seeking questions based on the broader context of the
paper. We assess the performance of 18 frontier multimodal foundation models,
including o4-mini, Gemini-2.5-Flash, and Qwen2.5-VL. We reveal a significant
performance gap between these models and human experts on MISS-QA. Our analysis
of model performance on unanswerable questions and our detailed error analysis
further highlight the strengths and limitations of current models, offering key
insights to enhance models in comprehending multimodal scientific literature.

</details>


### [113] [Testing Hypotheses from the Social Approval Theory of Online Hate: An Analysis of 110 Million Posts from Parler](https://arxiv.org/abs/2507.10810)
*David M. Markowitz,Samuel Hardman Taylor*

Main category: cs.CL

TL;DR: The study investigates whether social approval (e.g., upvotes) on hate speech posts leads to more or more extreme hate speech. Analyzing 110M Parler posts, it found no consistent link between approval and subsequent hate speech.


<details>
  <summary>Details</summary>
Motivation: To test Walther's (2024) social approval theory of online hate, specifically whether approval predicts more or more extreme hate speech.

Method: Analyzed over 110 million posts from Parler (2018-2021), examining the relationship between upvotes and subsequent hate speech at various time intervals.

Result: No consistent association between upvotes and subsequent hate speech. Between-person effects showed mixed relationships, sometimes negative.

Conclusion: Social approval may not reinforce online hate as theorized, especially on niche platforms like Parler.

Abstract: In this paper, we explored how online hate is motivated by receiving social
approval from others. We specifically examined two central tenets of Walther's
(2024) social approval theory of online hate: (H1a) more signals of social
approval on hate messages predicts more subsequent hate messages, and (H1b) as
social approval increases, hate speech messages become more extreme. Using over
110 million posts from Parler (2018-2021), we observed that the number of
upvotes a person received on a hate speech post was unassociated with the
amount of hate speech in their next post and posts during the next week, month,
three months, and six months. Between-person effects revealed an average
negative relationship between social approval and hate speech production at the
post level, but this relationship was mixed at other time intervals. Social
approval reinforcement mechanisms of online hate may operate differently on
niche social media platforms.

</details>


### [114] [LLMs on Trial: Evaluating Judicial Fairness for Large Language Models](https://arxiv.org/abs/2507.10852)
*Yiran Hu,Zongyue Xue,Haitao Li,Siyuan Zheng,Qingjing Chen,Shaochun Wang,Xihan Zhang,Ning Zheng,Yun Liu,Qingyao Ai,Yiqun Liu,Charles L. A. Clarke,Weixing Shen*

Main category: cs.CL

TL;DR: The paper investigates the judicial fairness of Large Language Models (LLMs) in high-stakes fields, revealing pervasive inconsistency, bias, and inaccuracy, with demographic labels showing pronounced biases.


<details>
  <summary>Details</summary>
Motivation: LLMs are increasingly used in high-stakes fields, but their fairness in judicial contexts remains underexplored, necessitating a framework to assess and improve their trustworthiness.

Method: A comprehensive fairness framework with 65 labels and 161 values is developed, applied to the JudiFair dataset (177,100 cases). Three metrics (inconsistency, bias, imbalanced inaccuracy) assess fairness across 16 LLMs.

Result: Experiments reveal severe judicial unfairness, with demographic biases being most pronounced. Adjusting temperature affects fairness, but model size, release date, and origin do not.

Conclusion: The study highlights LLMs' judicial unfairness and provides a toolkit for future research to evaluate and improve fairness.

Abstract: Large Language Models (LLMs) are increasingly used in high-stakes fields
where their decisions impact rights and equity. However, LLMs' judicial
fairness and implications for social justice remain underexplored. When LLMs
act as judges, the ability to fairly resolve judicial issues is a prerequisite
to ensure their trustworthiness. Based on theories of judicial fairness, we
construct a comprehensive framework to measure LLM fairness, leading to a
selection of 65 labels and 161 corresponding values. Applying this framework to
the judicial system, we compile an extensive dataset, JudiFair, comprising
177,100 unique case facts. To achieve robust statistical inference, we develop
three evaluation metrics, inconsistency, bias, and imbalanced inaccuracy, and
introduce a method to assess the overall fairness of multiple LLMs across
various labels. Through experiments with 16 LLMs, we uncover pervasive
inconsistency, bias, and imbalanced inaccuracy across models, underscoring
severe LLM judicial unfairness. Particularly, LLMs display notably more
pronounced biases on demographic labels, with slightly less bias on substance
labels compared to procedure ones. Interestingly, increased inconsistency
correlates with reduced biases, but more accurate predictions exacerbate
biases. While we find that adjusting the temperature parameter can influence
LLM fairness, model size, release date, and country of origin do not exhibit
significant effects on judicial fairness. Accordingly, we introduce a publicly
available toolkit containing all datasets and code, designed to support future
research in evaluating and improving LLM fairness.

</details>


### [115] [How Stylistic Similarity Shapes Preferences in Dialogue Dataset with User and Third Party Evaluations](https://arxiv.org/abs/2507.10918)
*Ikumi Numaya,Shoji Moriya,Shiki Sato,Reina Akama,Jun Suzuki*

Main category: cs.CL

TL;DR: The paper explores the impact of stylistic similarity in dialogue systems, distinguishing between subjective (user-perceived) and objective (third-party annotated) similarity, and finds a strong link between subjective similarity and user preference.


<details>
  <summary>Details</summary>
Motivation: Prior work overlooks the distinction between subjective and objective stylistic similarity in dialogue systems, prompting the need to investigate their roles in user preferences.

Method: A novel dataset is introduced, capturing user preferences, subjective stylistic similarity (user-perceived), and objective stylistic similarity (third-party annotated) in open-domain dialogues.

Result: Analysis shows a strong positive correlation between subjective stylistic similarity and user preference, with subjective and objective similarity differing significantly.

Conclusion: The study highlights the importance of distinguishing subjective and objective evaluations in dialogue systems to better understand user preferences.

Abstract: Recent advancements in dialogue generation have broadened the scope of
human-bot interactions, enabling not only contextually appropriate responses
but also the analysis of human affect and sensitivity. While prior work has
suggested that stylistic similarity between user and system may enhance user
impressions, the distinction between subjective and objective similarity is
often overlooked. To investigate this issue, we introduce a novel dataset that
includes users' preferences, subjective stylistic similarity based on users'
own perceptions, and objective stylistic similarity annotated by third party
evaluators in open-domain dialogue settings. Analysis using the constructed
dataset reveals a strong positive correlation between subjective stylistic
similarity and user preference. Furthermore, our analysis suggests an important
finding: users' subjective stylistic similarity differs from third party
objective similarity. This underscores the importance of distinguishing between
subjective and objective evaluations and understanding the distinct aspects
each captures when analyzing the relationship between stylistic similarity and
user preferences. The dataset presented in this paper is available online.

</details>


### [116] [HanjaBridge: Resolving Semantic Ambiguity in Korean LLMs via Hanja-Augmented Pre-Training](https://arxiv.org/abs/2507.10920)
*Seungho Choi*

Main category: cs.CL

TL;DR: HanjaBridge improves Korean LLM performance by injecting Hanja meanings during pre-training, achieving a 21% boost on KoBALT without runtime costs.


<details>
  <summary>Details</summary>
Motivation: LLMs underperform in low-resource languages like Korean due to semantic ambiguity in homophonous Sino-Korean words.

Method: HanjaBridge integrates meaning-injection via Hanja candidates in a continual pre-training framework, paired with token-level knowledge distillation.

Result: 21% relative improvement on KoBALT; strong cross-lingual transfer between Korean and Chinese.

Conclusion: HanjaBridge effectively enhances Korean language understanding and cross-lingual alignment without inference-time overhead.

Abstract: Large language models (LLMs) often show poor performance in low-resource
languages like Korean, partly due to unique linguistic challenges such as
homophonous Sino-Korean words that are indistinguishable in Hangul script. To
address this semantic ambiguity, we propose HanjaBridge, a novel
meaning-injection technique integrated into a continual pre-training (CPT)
framework. Instead of deterministically mapping a word to a single Hanja
(Chinese character), HanjaBridge presents the model with all possible Hanja
candidates for a given homograph, encouraging the model to learn contextual
disambiguation. This process is paired with token-level knowledge distillation
to prevent catastrophic forgetting. Experimental results show that HanjaBridge
significantly improves Korean language understanding, achieving a 21\% relative
improvement on the KoBALT benchmark. Notably, by reinforcing semantic alignment
between Korean and Chinese through shared Hanja, we observe a strong positive
cross-lingual transfer. Furthermore, these gains persist even when Hanja
augmentation is omitted at inference time, ensuring practical efficiency with
no additional run-time cost.

</details>


### [117] [Modeling Understanding of Story-Based Analogies Using Large Language Models](https://arxiv.org/abs/2507.10957)
*Kalit Inani,Keshav Kabra,Vijay Marupudi,Sashank Varma*

Main category: cs.CL

TL;DR: The study evaluates LLMs' analogical reasoning compared to humans, focusing on semantic representation and explicit prompting, while considering model size and architecture.


<details>
  <summary>Details</summary>
Motivation: To assess how well LLMs align with human performance in detecting and mapping analogies, addressing gaps in prior research.

Method: Used a story-based analogical mapping task, analyzed semantic representations via sentence embeddings, and tested explicit prompting for analogy explanation.

Result: Examined LLM performance at individual analogy levels, considering model size (8B vs. 70B) and architectures (GPT-4, LLaMA3).

Conclusion: Advances understanding of LLMs' analogical reasoning and their potential as models of human cognition.

Abstract: Recent advancements in Large Language Models (LLMs) have brought them closer
to matching human cognition across a variety of tasks. How well do these models
align with human performance in detecting and mapping analogies? Prior research
has shown that LLMs can extract similarities from analogy problems but lack
robust human-like reasoning. Building on Webb, Holyoak, and Lu (2023), the
current study focused on a story-based analogical mapping task and conducted a
fine-grained evaluation of LLM reasoning abilities compared to human
performance. First, it explored the semantic representation of analogies in
LLMs, using sentence embeddings to assess whether they capture the similarity
between the source and target texts of an analogy, and the dissimilarity
between the source and distractor texts. Second, it investigated the
effectiveness of explicitly prompting LLMs to explain analogies. Throughout, we
examine whether LLMs exhibit similar performance profiles to those observed in
humans by evaluating their reasoning at the level of individual analogies, and
not just at the level of overall accuracy (as prior studies have done). Our
experiments include evaluating the impact of model size (8B vs. 70B parameters)
and performance variation across state-of-the-art model architectures such as
GPT-4 and LLaMA3. This work advances our understanding of the analogical
reasoning abilities of LLMs and their potential as models of human reasoning.

</details>


### [118] [DS@GT at eRisk 2025: From prompts to predictions, benchmarking early depression detection with conversational agent based assessments and temporal attention models](https://arxiv.org/abs/2507.10958)
*Anthony Miyaguchi,David Guecha,Yuwen Chiu,Sidharth Gaur*

Main category: cs.CL

TL;DR: The DS@GT team participated in eRisk 2025's Pilot Task on conversational depression detection using LLMs, employing prompt-engineering to generate BDI-II-based JSON outputs. Their best submission ranked second.


<details>
  <summary>Details</summary>
Motivation: To explore the use of large language models (LLMs) for detecting depression in conversational contexts, leveraging prompt-engineering to align outputs with BDI-II criteria.

Method: Adopted a prompt-engineering strategy where diverse LLMs conducted BDI-II-based assessments, producing structured JSON outputs. Evaluated cross-model agreement and internal consistency due to lack of ground-truth labels.

Result: Achieved DCHR = 0.50, ADODL = 0.89, and ASHR = 0.27, ranking second on the official leaderboard.

Conclusion: The prompt-engineering approach successfully aligned LLM outputs with BDI-II criteria, enabling analysis of conversational cues influencing symptom prediction.

Abstract: This Working Note summarizes the participation of the DS@GT team in two eRisk
2025 challenges. For the Pilot Task on conversational depression detection with
large language-models (LLMs), we adopted a prompt-engineering strategy in which
diverse LLMs conducted BDI-II-based assessments and produced structured JSON
outputs. Because ground-truth labels were unavailable, we evaluated cross-model
agreement and internal consistency. Our prompt design methodology aligned model
outputs with BDI-II criteria and enabled the analysis of conversational cues
that influenced the prediction of symptoms. Our best submission, second on the
official leaderboard, achieved DCHR = 0.50, ADODL = 0.89, and ASHR = 0.27.

</details>


### [119] [Teach Me Sign: Stepwise Prompting LLM for Sign Language Production](https://arxiv.org/abs/2507.10972)
*Zhaoyi An,Rei Kawakami*

Main category: cs.CL

TL;DR: TEAM-Sign fine-tunes an LLM to bridge text and sign language, using stepwise prompting to align their distributions and rules, showing effectiveness on How2Sign and Phoenix14T datasets.


<details>
  <summary>Details</summary>
Motivation: Sign language generation is complex and lacks LLM impact due to unique rules, motivating the need for a method to align sign and spoken language.

Method: Fine-tune an LLM with stepwise prompting to extract sign language knowledge and align distributions and grammatical rules.

Result: Effective alignment of sign and spoken language distributions and rules on How2Sign and Phoenix14T datasets.

Conclusion: TEAM-Sign successfully leverages LLM capabilities for sign language generation, addressing complexity and rule differences.

Abstract: Large language models, with their strong reasoning ability and rich
knowledge, have brought revolution to many tasks of AI, but their impact on
sign language generation remains limited due to its complexity and unique
rules. In this paper, we propose TEAch Me Sign (TEAM-Sign), treating sign
language as another natural language. By fine-tuning an LLM, we enable it to
learn the correspondence between text and sign language, and facilitate
generation. Considering the differences between sign and spoken language, we
employ a stepwise prompting strategy to extract the inherent sign language
knowledge within the LLM, thereby supporting the learning and generation
process. Experimental results on How2Sign and Phoenix14T datasets demonstrate
that our approach effectively leverages both the sign language knowledge and
reasoning capabilities of LLM to align the different distribution and
grammatical rules between sign and spoken language.

</details>


### [120] [Mario at EXIST 2025: A Simple Gateway to Effective Multilingual Sexism Detection](https://arxiv.org/abs/2507.10996)
*Lin Tian,Johanne R. Trippas,Marian-Andrei Rizoiu*

Main category: cs.CL

TL;DR: The paper introduces a hierarchical LoRA adaptation method for Llama 3.1 8B to detect text-based sexism in English and Spanish tweets, achieving efficient performance with minimal preprocessing and cross-lingual transfer.


<details>
  <summary>Details</summary>
Motivation: To address sexism detection in tweets efficiently by leveraging hierarchical subtasks and multilingual training without separate language models.

Method: Hierarchical LoRA adaptation applied to all linear transformations, with conditional adapter routing for three subtasks, using QLoRA 4-bit and unified multilingual training.

Result: Achieves 1.7-2.4% F1 improvements, reduces training time by 75%, and model storage by 98%, with competitive performance across subtasks.

Conclusion: Parameter-efficient fine-tuning with hierarchical LoRA is effective for multilingual sexism detection, balancing performance and resource efficiency.

Abstract: This paper presents our approach to EXIST 2025 Task 1, addressing text-based
sexism detection in English and Spanish tweets through hierarchical Low-Rank
Adaptation (LoRA) of Llama 3.1 8B. Our method introduces conditional adapter
routing that explicitly models label dependencies across three hierarchically
structured subtasks: binary sexism identification, source intention detection,
and multilabel sexism categorization. Unlike conventional LoRA applications
that target only attention layers, we apply adaptation to all linear
transformations, enhancing the model's capacity to capture task-specific
patterns. In contrast to complex data processing and ensemble approaches, we
show that straightforward parameter-efficient fine-tuning achieves strong
performance. We train separate LoRA adapters (rank=16, QLoRA 4-bit) for each
subtask using unified multilingual training that leverages Llama 3.1's native
bilingual capabilities. The method requires minimal preprocessing and uses
standard supervised learning. Our multilingual training strategy eliminates the
need for separate language-specific models, achieving 1.7-2.4\% F1 improvements
through cross-lingual transfer. With only 1.67\% trainable parameters compared
to full fine-tuning, our approach reduces training time by 75\% and model
storage by 98\%, while achieving competitive performance across all subtasks
(ICM-Hard: 0.6774 for binary classification, 0.4991 for intention detection,
0.6519 for multilabel categorization).

</details>


### [121] [Team HUMANE at AVeriTeC 2025: HerO 2 for Efficient Fact Verification](https://arxiv.org/abs/2507.11004)
*Yejun Yoon,Jaeyoon Jung,Seunghyun Yoon,Kunwoo Park*

Main category: cs.CL

TL;DR: HerO 2 is an improved system for fact verification, ranking second in the AVeriTeC shared task with high efficiency and strong performance.


<details>
  <summary>Details</summary>
Motivation: To enhance evidence quality, optimize veracity prediction, and improve system performance for real-world fact verification.

Method: Uses document summarization, answer reformulation, post-training quantization, and updated LM backbones.

Result: Ranked second on the leaderboard with the shortest runtime among top systems.

Conclusion: HerO 2 demonstrates high efficiency and potential for practical fact verification applications.

Abstract: This paper presents HerO 2, Team HUMANE's system for the AVeriTeC shared task
at the FEVER-25 workshop. HerO 2 is an enhanced version of HerO, the
best-performing open-source model from the previous year's challenge. It
improves evidence quality through document summarization and answer
reformulation, optimizes veracity prediction via post-training quantization
under computational constraints, and enhances overall system performance by
integrating updated language model (LM) backbones. HerO 2 ranked second on the
leaderboard while achieving the shortest runtime among the top three systems,
demonstrating both high efficiency and strong potential for real-world fact
verification. The code is available at https://github.com/ssu-humane/HerO2.

</details>


### [122] [Journalism-Guided Agentic In-Context Learning for News Stance Detection](https://arxiv.org/abs/2507.11049)
*Dahyun Lee,Jonghyeon Choi,Jiyoung Han,Kunwoo Park*

Main category: cs.CL

TL;DR: The paper introduces K-News-Stance, a Korean dataset for article-level stance detection, and JoA-ICL, a framework for stance detection in long-form news, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: To address gaps in stance detection research, particularly for long texts and low-resource languages like Korean, and to mitigate filter bubbles and polarization in news recommendations.

Method: Proposes JoA-ICL, a journalism-guided agentic in-context learning framework that predicts stances of key segments (e.g., leads, quotes) and aggregates them for article-level stance detection.

Result: JoA-ICL outperforms existing stance detection methods, demonstrating effectiveness in capturing long-form article stances and enabling viewpoint diversity in recommendations.

Conclusion: The work advances stance detection for long-form content and supports applications like diverse news recommendations and media bias analysis.

Abstract: As online news consumption grows, personalized recommendation systems have
become integral to digital journalism. However, these systems risk reinforcing
filter bubbles and political polarization by failing to incorporate diverse
perspectives. Stance detection -- identifying a text's position on a target --
can help mitigate this by enabling viewpoint-aware recommendations and
data-driven analyses of media bias. Yet, existing stance detection research
remains largely limited to short texts and high-resource languages. To address
these gaps, we introduce \textsc{K-News-Stance}, the first Korean dataset for
article-level stance detection, comprising 2,000 news articles with
article-level and 19,650 segment-level stance annotations across 47 societal
issues. We also propose \textsc{JoA-ICL}, a \textbf{Jo}urnalism-guided
\textbf{A}gentic \textbf{I}n-\textbf{C}ontext \textbf{L}earning framework that
employs a language model agent to predict the stances of key structural
segments (e.g., leads, quotes), which are then aggregated to infer the overall
article stance. Experiments show that \textsc{JoA-ICL} outperforms existing
stance detection methods, highlighting the benefits of segment-level agency in
capturing the overall position of long-form news articles. Two case studies
further demonstrate its broader utility in promoting viewpoint diversity in
news recommendations and uncovering patterns of media bias.

</details>


### [123] [LLM-Augmented Symptom Analysis for Cardiovascular Disease Risk Prediction: A Clinical NLP](https://arxiv.org/abs/2507.11052)
*Haowei Yang,Ziyu Shen,Junli Shao,Luyao Men,Xinyue Han,Jing Dong*

Main category: cs.CL

TL;DR: A novel LLM-augmented NLP pipeline improves CVD risk prediction by extracting and analyzing unstructured clinical notes, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Existing CVD prediction models rely on structured data, missing valuable insights from unstructured clinical notes. This study aims to leverage LLMs for better risk assessment.

Method: The approach uses domain-adapted LLMs for symptom extraction, contextual reasoning, and correlation from free-text reports, integrating fine-tuning, prompt-based inference, and entity-aware reasoning.

Result: Evaluations show improved precision, recall, F1-score, and AUROC, with high clinical relevance (kappa = 0.82). Challenges like contextual hallucination and temporal ambiguity are addressed.

Conclusion: The study highlights LLMs' potential in clinical decision support, improving early warning systems and translating patient narratives into actionable risk assessments.

Abstract: Timely identification and accurate risk stratification of cardiovascular
disease (CVD) remain essential for reducing global mortality. While existing
prediction models primarily leverage structured data, unstructured clinical
notes contain valuable early indicators. This study introduces a novel
LLM-augmented clinical NLP pipeline that employs domain-adapted large language
models for symptom extraction, contextual reasoning, and correlation from
free-text reports. Our approach integrates cardiovascular-specific fine-tuning,
prompt-based inference, and entity-aware reasoning. Evaluations on MIMIC-III
and CARDIO-NLP datasets demonstrate improved performance in precision, recall,
F1-score, and AUROC, with high clinical relevance (kappa = 0.82) assessed by
cardiologists. Challenges such as contextual hallucination, which occurs when
plausible information contracts with provided source, and temporal ambiguity,
which is related with models struggling with chronological ordering of events
are addressed using prompt engineering and hybrid rule-based verification. This
work underscores the potential of LLMs in clinical decision support systems
(CDSS), advancing early warning systems and enhancing the translation of
patient narratives into actionable risk assessments.

</details>


### [124] [Social Media Sentiments Analysis on the July Revolution in Bangladesh: A Hybrid Transformer Based Machine Learning Approach](https://arxiv.org/abs/2507.11084)
*Md. Sabbir Hossen,Md. Saiduzzaman,Pabon Shaha*

Main category: cs.CL

TL;DR: A hybrid transformer-based sentiment analysis framework was developed to analyze Bangla social media comments during the July Revolution in Bangladesh, achieving 83.7% accuracy with a voting classifier.


<details>
  <summary>Details</summary>
Motivation: To decode public opinion during the July Revolution in Bangladesh using social media data, addressing the challenge of sentiment analysis in low-resource languages like Bangla.

Method: A hybrid transformer-based framework (BanglaBERT, mBERT, XLM-RoBERTa, and XMB-BERT) with PCA for dimensionality reduction and eleven classifiers for sentiment identification.

Result: The hybrid XMB-BERT with a voting classifier achieved the highest accuracy of 83.7%, outperforming other models.

Conclusion: Machine learning, especially transformer-based models, is effective for sentiment analysis in low-resource languages, as demonstrated with Bangla social media data.

Abstract: The July Revolution in Bangladesh marked a significant student-led mass
uprising, uniting people across the nation to demand justice, accountability,
and systemic reform. Social media platforms played a pivotal role in amplifying
public sentiment and shaping discourse during this historic mass uprising. In
this study, we present a hybrid transformer-based sentiment analysis framework
to decode public opinion expressed in social media comments during and after
the revolution. We used a brand new dataset of 4,200 Bangla comments collected
from social media. The framework employs advanced transformer-based feature
extraction techniques, including BanglaBERT, mBERT, XLM-RoBERTa, and the
proposed hybrid XMB-BERT, to capture nuanced patterns in textual data.
Principle Component Analysis (PCA) were utilized for dimensionality reduction
to enhance computational efficiency. We explored eleven traditional and
advanced machine learning classifiers for identifying sentiments. The proposed
hybrid XMB-BERT with the voting classifier achieved an exceptional accuracy of
83.7% and outperform other model classifier combinations. This study
underscores the potential of machine learning techniques to analyze social
sentiment in low-resource languages like Bangla.

</details>


### [125] [Beyond Traditional Algorithms: Leveraging LLMs for Accurate Cross-Border Entity Identification](https://arxiv.org/abs/2507.11086)
*Andres Azqueta-Gavaldón,Joaquin Ramos Cosgrove*

Main category: cs.CL

TL;DR: The paper explores using Large Language Models (LLMs) to improve entity-matching in cross-border financial activities, outperforming traditional methods with higher accuracy and lower false positives.


<details>
  <summary>Details</summary>
Motivation: The need for accurate foreign entity identification in the Spanish financial system due to challenges like linguistic variations and outdated names, which traditional methods struggle with.

Method: Evaluation of traditional methods (Jaccard, cosine, Levenshtein) and LLMs (Hugging Face-based and interface-based like Microsoft Copilot) on a dataset of 65 Portuguese companies.

Result: Interface-based LLMs achieved higher accuracy (93%), F1 scores (96%), and lower false positives (40-80%) compared to traditional methods (92% accuracy, 20-40% false positives).

Conclusion: LLMs, especially interface-based ones, offer a superior solution for entity-matching in cross-border financial contexts, addressing limitations of traditional methods.

Abstract: The growing prevalence of cross-border financial activities in global markets
has underscored the necessity of accurately identifying and classifying foreign
entities. This practice is essential within the Spanish financial system for
ensuring robust risk management, regulatory adherence, and the prevention of
financial misconduct. This process involves a labor-intensive entity-matching
task, where entities need to be validated against available reference sources.
Challenges arise from linguistic variations, special characters, outdated
names, and changes in legal forms, complicating traditional matching algorithms
like Jaccard, cosine, and Levenshtein distances. These methods struggle with
contextual nuances and semantic relationships, leading to mismatches. To
address these limitations, we explore Large Language Models (LLMs) as a
flexible alternative. LLMs leverage extensive training to interpret context,
handle abbreviations, and adapt to legal transitions. We evaluate traditional
methods, Hugging Face-based LLMs, and interface-based LLMs (e.g., Microsoft
Copilot, Alibaba's Qwen 2.5) using a dataset of 65 Portuguese company cases.
Results show traditional methods achieve accuracies over 92% but suffer high
false positive rates (20-40%). Interface-based LLMs outperform, achieving
accuracies above 93%, F1 scores exceeding 96%, and lower false positives
(40-80%).

</details>


### [126] [The Devil behind the mask: An emergent safety vulnerability of Diffusion LLMs](https://arxiv.org/abs/2507.11097)
*Zichen Wen,Jiashu Qu,Dongrui Liu,Zhiyuan Liu,Ruixi Wu,Yicun Yang,Xiangqi Jin,Haoyun Xu,Xuyang Liu,Weijia Li,Chaochao Lu,Jing Shao,Conghui He,Linfeng Zhang*

Main category: cs.CL

TL;DR: DIJA is a jailbreak attack framework targeting diffusion-based LLMs (dLLMs), exploiting their bidirectional modeling and parallel decoding to bypass safety mechanisms, achieving high success rates in harmful completions.


<details>
  <summary>Details</summary>
Motivation: Existing alignment mechanisms for dLLMs fail against context-aware adversarial prompts, exposing safety vulnerabilities.

Method: DIJA constructs adversarial interleaved mask-text prompts to exploit dLLMs' bidirectional modeling and parallel decoding, bypassing safety filters.

Result: DIJA outperforms prior methods, achieving up to 100% keyword-based ASR and significant improvements in evaluator-based metrics.

Conclusion: The study highlights critical safety gaps in dLLMs, urging a rethink of alignment strategies for this model class.

Abstract: Diffusion-based large language models (dLLMs) have recently emerged as a
powerful alternative to autoregressive LLMs, offering faster inference and
greater interactivity via parallel decoding and bidirectional modeling.
However, despite strong performance in code generation and text infilling, we
identify a fundamental safety concern: existing alignment mechanisms fail to
safeguard dLLMs against context-aware, masked-input adversarial prompts,
exposing novel vulnerabilities. To this end, we present DIJA, the first
systematic study and jailbreak attack framework that exploits unique safety
weaknesses of dLLMs. Specifically, our proposed DIJA constructs adversarial
interleaved mask-text prompts that exploit the text generation mechanisms of
dLLMs, i.e., bidirectional modeling and parallel decoding. Bidirectional
modeling drives the model to produce contextually consistent outputs for masked
spans, even when harmful, while parallel decoding limits model dynamic
filtering and rejection sampling of unsafe content. This causes standard
alignment mechanisms to fail, enabling harmful completions in alignment-tuned
dLLMs, even when harmful behaviors or unsafe instructions are directly exposed
in the prompt. Through comprehensive experiments, we demonstrate that DIJA
significantly outperforms existing jailbreak methods, exposing a previously
overlooked threat surface in dLLM architectures. Notably, our method achieves
up to 100% keyword-based ASR on Dream-Instruct, surpassing the strongest prior
baseline, ReNeLLM, by up to 78.5% in evaluator-based ASR on JailbreakBench and
by 37.7 points in StrongREJECT score, while requiring no rewriting or hiding of
harmful content in the jailbreak prompt. Our findings underscore the urgent
need for rethinking safety alignment in this emerging class of language models.
Code is available at https://github.com/ZichenWen1/DIJA.

</details>


### [127] [Multi-Trigger Poisoning Amplifies Backdoor Vulnerabilities in LLMs](https://arxiv.org/abs/2507.11112)
*Sanhanat Sivapiromrat,Caiqi Zhang,Marco Basaldella,Nigel Collier*

Main category: cs.CL

TL;DR: The paper explores data poisoning in LLMs, revealing that multiple backdoor triggers can coexist without interference, and proposes a mitigation method via selective retraining.


<details>
  <summary>Details</summary>
Motivation: Existing research lacks understanding of trigger mechanisms and interactions in poisoned LLMs, prompting a need for deeper analysis and practical defenses.

Method: A framework is introduced to study poisoning, demonstrating coexistence of multiple triggers. A post hoc recovery method uses layer-wise weight difference analysis for selective retraining.

Result: Multiple triggers can robustly activate despite token substitutions or spans. The proposed method effectively removes triggers with minimal updates.

Conclusion: The study highlights persistent vulnerabilities in LLMs and offers an efficient defense against multi-trigger poisoning.

Abstract: Recent studies have shown that Large Language Models (LLMs) are vulnerable to
data poisoning attacks, where malicious training examples embed hidden
behaviours triggered by specific input patterns. However, most existing works
assume a phrase and focus on the attack's effectiveness, offering limited
understanding of trigger mechanisms and how multiple triggers interact within
the model. In this paper, we present a framework for studying poisoning in
LLMs. We show that multiple distinct backdoor triggers can coexist within a
single model without interfering with each other, enabling adversaries to embed
several triggers concurrently. Using multiple triggers with high embedding
similarity, we demonstrate that poisoned triggers can achieve robust activation
even when tokens are substituted or separated by long token spans. Our findings
expose a broader and more persistent vulnerability surface in LLMs. To mitigate
this threat, we propose a post hoc recovery method that selectively retrains
specific model components based on a layer-wise weight difference analysis. Our
method effectively removes the trigger behaviour with minimal parameter
updates, presenting a practical and efficient defence against multi-trigger
poisoning.

</details>


### [128] [MSA at ImageCLEF 2025 Multimodal Reasoning: Multilingual Multimodal Reasoning With Ensemble Vision Language Models](https://arxiv.org/abs/2507.11114)
*Seif Ahmed,Mohamed T. Younes,Abdelrahman Moustafa,Abdelrahman Allam,Hamza Moustafa*

Main category: cs.CL

TL;DR: A robust ensemble system for multilingual multimodal reasoning achieved top performance in the ImageCLEF 2025 EXAMS V challenge by integrating Gemini models and optimizing prompts.


<details>
  <summary>Details</summary>
Motivation: To develop a lightweight yet effective system for multilingual multimodal reasoning in educational settings, outperforming heavier end-to-end models.

Method: Combined Gemini 2.5 Flash, Gemini 1.5 Pro, and Gemini 2.5 Pro with few-shot and zero-shot prompts, and conducted ablation studies on various LLMs.

Result: Achieved 81.4% accuracy overall, leading 11 of 13 language tracks (e.g., 95.07% for Croatian). Prompt optimization boosted accuracy from 55.9% to 61.7%.

Conclusion: Lightweight OCR-VLM ensembles with precise prompts and cross-lingual augmentation outperform heavier models in multilingual educational tasks.

Abstract: We present a robust ensemble-based system for multilingual multimodal
reasoning, designed for the ImageCLEF 2025 EXAMS V challenge. Our approach
integrates Gemini 2.5 Flash for visual description, Gemini 1.5 Pro for caption
refinement and consistency checks, and Gemini 2.5 Pro as a reasoner which
handles final answer selection, all coordinated through carefully engineered
few-shot and zero-shot prompts. We conducted an extensive ablation study,
training several large language models (Gemini 2.5 Flash, Phi 4, Gemma 3,
Mistral) on an English dataset and its multilingual augmented version.
Additionally, we evaluated Gemini 2.5 Flash in a zero-shot setting for
comparison and found it to substantially outperform the trained models. Prompt
design also proved critical: enforcing concise, language-normalized formats and
prohibiting explanatory text boosted model accuracy on the English validation
set from 55.9% to 61.7%. On the official leaderboard, our system (Team MSA)
achieved first place overall in the multilingual track with 81.4% accuracy, and
led 11 out of 13 individual language tracks, with top results such as 95.07%
for Croatian and 92.12% for Italian. These findings highlight that lightweight
OCR-VLM ensembles, when paired with precise prompt strategies and cross-lingual
augmentation, can outperform heavier end-to-end models in high-stakes,
multilingual educational settings.

</details>


### [129] [What Should LLMs Forget? Quantifying Personal Data in LLMs for Right-to-Be-Forgotten Requests](https://arxiv.org/abs/2507.11128)
*Dimitri Staufer*

Main category: cs.CL

TL;DR: The paper addresses the challenge of identifying and quantifying memorized personal data in LLMs for GDPR compliance, introducing WikiMem and a model-agnostic metric to measure human-fact associations.


<details>
  <summary>Details</summary>
Motivation: Concerns about LLMs memorizing personal data and GDPR compliance, especially the Right to Be Forgotten (RTBF), motivate the need for methods to identify individual-level memorized data.

Method: The authors introduce WikiMem, a dataset of natural language canaries, and a metric using calibrated negative log-likelihood to rank ground-truth values against counterfactuals.

Result: Evaluation across 15 LLMs shows memorization correlates with subject web presence and model scale, providing a way to identify memorized data for unlearning.

Conclusion: The work lays a foundation for dynamically constructing forget sets to comply with RTBF requests and improve LLM privacy.

Abstract: Large Language Models (LLMs) can memorize and reveal personal information,
raising concerns regarding compliance with the EU's GDPR, particularly the
Right to Be Forgotten (RTBF). Existing machine unlearning methods assume the
data to forget is already known but do not address how to identify which
individual-fact associations are stored in the model. Privacy auditing
techniques typically operate at the population level or target a small set of
identifiers, limiting applicability to individual-level data inquiries. We
introduce WikiMem, a dataset of over 5,000 natural language canaries covering
243 human-related properties from Wikidata, and a model-agnostic metric to
quantify human-fact associations in LLMs. Our approach ranks ground-truth
values against counterfactuals using calibrated negative log-likelihood across
paraphrased prompts. We evaluate 200 individuals across 15 LLMs (410M-70B
parameters), showing that memorization correlates with subject web presence and
model scale. We provide a foundation for identifying memorized personal data in
LLMs at the individual level, enabling the dynamic construction of forget sets
for machine unlearning and RTBF requests.

</details>


### [130] [Temperature and Persona Shape LLM Agent Consensus With Minimal Accuracy Gains in Qualitative Coding](https://arxiv.org/abs/2507.11198)
*Conrad Borchers,Bahar Shahrokhian,Francesco Balzan,Elham Tajik,Sreecharan Sankaranarayanan,Sebastian Simon*

Main category: cs.CL

TL;DR: The study explores how multi-agent systems (MAS) with diverse personas and temperature settings affect consensus-building and coding accuracy in qualitative research using LLMs, finding limited benefits over single-agent approaches.


<details>
  <summary>Details</summary>
Motivation: To understand the impact of agent persona and temperature on consensus-building and coding accuracy in qualitative research using LLMs, and to compare MAS with single-agent coding.

Method: Experimental study with six open-source LLMs (3B to 32B parameters) and 18 configurations, analyzing 77,000 coding decisions against human-annotated transcripts. MAS mirrored human coding workflows with structured discussion and consensus arbitration.

Result: Temperature and multiple personas delayed consensus but did not improve coding accuracy. Single agents often matched or outperformed MAS. Only one model (OpenHermesV2:7B) showed gains under specific conditions.

Conclusion: Diverse MAS personas do not consistently improve outcomes, challenging assumptions about their benefits. MAS may help narrow ambiguous code applications, but single agents are often sufficient.

Abstract: Large Language Models (LLMs) enable new possibilities for qualitative
research at scale, including coding and data annotation. While multi-agent
systems (MAS) can emulate human coding workflows, their benefits over
single-agent coding remain poorly understood. We conducted an experimental
study of how agent persona and temperature shape consensus-building and coding
accuracy of dialog segments based on a codebook with 8 codes. Our open-source
MAS mirrors deductive human coding through structured agent discussion and
consensus arbitration. Using six open-source LLMs (with 3 to 32 billion
parameters) and 18 experimental configurations, we analyze over 77,000 coding
decisions against a gold-standard dataset of human-annotated transcripts from
online math tutoring sessions. Temperature significantly impacted whether and
when consensus was reached across all six LLMs. MAS with multiple personas
(including neutral, assertive, or empathetic), significantly delayed consensus
in four out of six LLMs compared to uniform personas. In three of those LLMs,
higher temperatures significantly diminished the effects of multiple personas
on consensus. However, neither temperature nor persona pairing lead to robust
improvements in coding accuracy. Single agents matched or outperformed MAS
consensus in most conditions. Only one model (OpenHermesV2:7B) and code
category showed above-chance gains from MAS deliberation when temperature was
0.5 or lower and especially when the agents included at least one assertive
persona. Qualitative analysis of MAS collaboration for these configurations
suggests that MAS may nonetheless aid in narrowing ambiguous code applications
that could improve codebooks and human-AI coding. We contribute new insight
into the limits of LLM-based qualitative methods, challenging the notion that
diverse MAS personas lead to better outcomes. We open-source our MAS and
experimentation code.

</details>


### [131] [EsBBQ and CaBBQ: The Spanish and Catalan Bias Benchmarks for Question Answering](https://arxiv.org/abs/2507.11216)
*Valle Ruiz-Fernández,Mario Mina,Júlia Falcão,Luis Vasquez-Reina,Anna Sallés,Aitor Gonzalez-Agirre,Olatz Perez-de-Viñaspre*

Main category: cs.CL

TL;DR: The paper introduces Spanish and Catalan Bias Benchmarks (EsBBQ and CaBBQ) to evaluate social biases in LLMs for non-English languages and non-US contexts, showing models often rely on biases in ambiguous scenarios.


<details>
  <summary>Details</summary>
Motivation: Address the lack of resources for evaluating social biases in languages other than English and contexts outside the US.

Method: Develop parallel datasets (EsBBQ and CaBBQ) based on BBQ, assessing bias in 10 categories via multiple-choice QA, adapted to Spanish/Catalan and Spain's social context.

Result: LLMs often fail in ambiguous scenarios, with high QA accuracy correlating with greater reliance on social biases.

Conclusion: The benchmarks highlight the need for bias evaluation in diverse languages and contexts, revealing LLMs' tendency to rely on biases.

Abstract: Previous literature has largely shown that Large Language Models (LLMs)
perpetuate social biases learnt from their pre-training data. Given the notable
lack of resources for social bias evaluation in languages other than English,
and for social contexts outside of the United States, this paper introduces the
Spanish and the Catalan Bias Benchmarks for Question Answering (EsBBQ and
CaBBQ). Based on the original BBQ, these two parallel datasets are designed to
assess social bias across 10 categories using a multiple-choice QA setting, now
adapted to the Spanish and Catalan languages and to the social context of
Spain. We report evaluation results on different LLMs, factoring in model
family, size and variant. Our results show that models tend to fail to choose
the correct answer in ambiguous scenarios, and that high QA accuracy often
correlates with greater reliance on social biases.

</details>


### [132] [An Agentic Flow for Finite State Machine Extraction using Prompt Chaining](https://arxiv.org/abs/2507.11222)
*Fares Wael,Youssef Maklad,Ali Hamdi,Wael Elsersy*

Main category: cs.CL

TL;DR: FlowFSM is a novel framework using LLMs to extract accurate FSMs from RFC documents, improving scalability and precision in protocol analysis.


<details>
  <summary>Details</summary>
Motivation: Existing FSM extraction techniques are limited by scalability, incomplete coverage, and ambiguity in natural language specifications.

Method: FlowFSM leverages LLMs with prompt chaining and chain-of-thought reasoning to systematically process RFCs, identify state transitions, and construct rule-books.

Result: FlowFSM achieves high extraction precision with minimal hallucinated transitions in FTP and RTSP protocols.

Conclusion: Agent-based LLM systems like FlowFSM show promise for advancing protocol analysis and FSM inference in cybersecurity and reverse engineering.

Abstract: Finite-State Machines (FSMs) are critical for modeling the operational logic
of network protocols, enabling verification, analysis, and vulnerability
discovery. However, existing FSM extraction techniques face limitations such as
scalability, incomplete coverage, and ambiguity in natural language
specifications. In this paper, we propose FlowFSM, a novel agentic framework
that leverages Large Language Models (LLMs) combined with prompt chaining and
chain-of-thought reasoning to extract accurate FSMs from raw RFC documents.
FlowFSM systematically processes protocol specifications, identifies state
transitions, and constructs structured rule-books by chaining agent outputs.
Experimental evaluation across FTP and RTSP protocols demonstrates that FlowFSM
achieves high extraction precision while minimizing hallucinated transitions,
showing promising results. Our findings highlight the potential of agent-based
LLM systems in the advancement of protocol analysis and FSM inference for
cybersecurity and reverse engineering applications.

</details>


### [133] [Sparse Autoencoders Can Capture Language-Specific Concepts Across Diverse Languages](https://arxiv.org/abs/2507.11230)
*Lyzander Marciano Andrylie,Inaya Rahmanisa,Mahardika Krisna Ihsani,Alfan Farizki Wicaksono,Haryo Akbarianto Wibowo,Alham Fikri Aji*

Main category: cs.CL

TL;DR: The paper explores language-specific features in LLMs using sparse autoencoders (SAEs) and introduces SAE-LAPE to identify these features, revealing their impact on multilingual performance and interpretability.


<details>
  <summary>Details</summary>
Motivation: Understanding how LLMs process multiple languages is challenging due to the polysemantic nature of neurons, prompting the need to isolate language-specific features.

Method: The study uses sparse autoencoders (SAEs) and introduces SAE-LAPE, a method based on feature activation probability, to identify language-specific features in LLMs.

Result: Language-specific features are found in middle to final layers, are interpretable, and influence multilingual performance. They also enable language identification comparable to fastText.

Conclusion: SAE-LAPE effectively identifies language-specific features in LLMs, enhancing interpretability and understanding of multilingual mechanisms.

Abstract: Understanding the multilingual mechanisms of large language models (LLMs)
provides insight into how they process different languages, yet this remains
challenging. Existing studies often focus on individual neurons, but their
polysemantic nature makes it difficult to isolate language-specific units from
cross-lingual representations. To address this, we explore sparse autoencoders
(SAEs) for their ability to learn monosemantic features that represent concrete
and abstract concepts across languages in LLMs. While some of these features
are language-independent, the presence of language-specific features remains
underexplored. In this work, we introduce SAE-LAPE, a method based on feature
activation probability, to identify language-specific features within the
feed-forward network. We find that many such features predominantly appear in
the middle to final layers of the model and are interpretable. These features
influence the model's multilingual performance and language output and can be
used for language identification with performance comparable to fastText along
with more interpretability. Our code is available at
https://github.com/LyzanderAndrylie/language-specific-features .

</details>


### [134] [KV-Latent: Dimensional-level KV Cache Reduction with Frequency-aware Rotary Positional Embedding](https://arxiv.org/abs/2507.11273)
*Luohe Shi,Zuchao Li,Lefei Zhang,Guoming Liu,Baoyuan Qi,Hai Zhao*

Main category: cs.CL

TL;DR: KV-Latent reduces KV cache footprint in LLMs by down-sampling Key-Value vectors into a latent space, improving efficiency with minimal extra training.


<details>
  <summary>Details</summary>
Motivation: The increasing KV cache during inference in LLMs causes memory and bandwidth inefficiencies.

Method: Down-samples KV vectors into a latent space, modifies Rotary Positional Embedding for stability, and tests impact of reducing Key and Value components separately.

Result: Significantly reduces KV cache footprint and improves inference speed with minimal training overhead.

Conclusion: KV-Latent enables more efficient LLMs and opens new possibilities for KV cache optimization.

Abstract: Large language models (LLMs) based on Transformer Decoders have become the
preferred choice for conversational generative AI. Despite the overall
superiority of the Decoder architecture, the gradually increasing Key-Value
(KV) cache during inference has emerged as a primary efficiency bottleneck,
both in aspects of memory consumption and data transfer bandwidth limitations.
To address these challenges, we propose a paradigm called KV-Latent. By
down-sampling the Key-Value vector dimensions into a latent space, we can
significantly reduce the KV Cache footprint and improve inference speed, only
with a small amount of extra training, less than 1\% of pre-training takes.
Besides, we enhanced the stability of Rotary Positional Embedding applied on
lower-dimensional vectors by modifying its frequency sampling mechanism,
avoiding noise introduced by higher frequencies while retaining position
attenuation. Our experiments, including both models with Grouped Query
Attention and those without, have yielded satisfactory results. Finally, we
conducted comparative experiments to study the impact of separately reducing
Key and Value components on model's performance. Our approach allows for the
construction of more efficient language model systems, and opens the new
possibility on KV Cache saving and efficient LLMs. Our code is available at
https://github.com/ShiLuohe/KV-Latent.

</details>


### [135] [FMC: Formalization of Natural Language Mathematical Competition Problems](https://arxiv.org/abs/2507.11275)
*Jiaxuan Xie,Chengwu Liu,Ye Yuan,Siqi Li,Zhiping Xiao,Ming Zhang*

Main category: cs.CL

TL;DR: The paper introduces an autoformalization pipeline using large language models with error feedback to create a dataset of Olympiad-level math problems in Lean, demonstrating its utility as a benchmark for theorem provers.


<details>
  <summary>Details</summary>
Motivation: Advancing formal mathematical reasoning by developing efficient and accurate autoformalization methods.

Method: Proposes a training-free autoformalization pipeline using large language models with error feedback, curating a dataset of natural language and Lean formalizations.

Result: Created a dataset of 3,922 natural language and 9,787 Lean problems, with 64.46% assessed as high quality. Showed that few-shot learning, error feedback, and increased sampling improve autoformalization.

Conclusion: The dataset serves as a valuable benchmark for automated theorem provers, and the pipeline enhances formalization capabilities.

Abstract: Efficient and accurate autoformalization methods, which leverage large-scale
datasets of extensive natural language mathematical problems to construct
formal language datasets, are key to advancing formal mathematical reasoning.
In this paper, we propose an autoformalization pipeline based on large language
models with error feedback, achieving a fully automatic and training-free
formalization approach. Using this pipeline, we curate an Olympiad-level
dataset aligning natural language problems with Lean formalizations. The
dataset comprises $3,922$ mathematical problems in natural language and $9,787$
in Lean, of which $64.46\%$ were assessed as at least above-average quality,
making it suitable as a benchmark for automated theorem provers. Additionally,
we investigate the formalization and reasoning capabilities of various LLMs and
empirically demonstrate that few-shot learning, error feedback, and increasing
sampling numbers enhance the autoformalization process. Experiments of three
automated theorem provers on the \dataset\ dataset also highlight its
challenging nature and its value as a benchmark for formal reasoning tasks.

</details>


### [136] [Fine-Grained Chinese Hate Speech Understanding: Span-Level Resources, Coded Term Lexicon, and Enhanced Detection Frameworks](https://arxiv.org/abs/2507.11292)
*Zewen Bai,Liang Yang,Shengdi Yin,Yuanyuan Sun,Hongfei Lin*

Main category: cs.CL

TL;DR: The paper addresses gaps in Chinese hate speech detection by introducing a span-level dataset (STATE ToxiCN), studying coded hate terms, and proposing a lexicon-integrated method to improve detection and interpretability.


<details>
  <summary>Details</summary>
Motivation: The societal harm of hate speech and the lack of research on Chinese hate speech detection, especially in span-level annotation and coded hate interpretation, motivated this study.

Method: The authors introduced the STATE ToxiCN dataset, studied coded hate terms, and proposed integrating an annotated lexicon into models for better detection.

Result: The work provided a valuable dataset, insights into coded hate, and a method to enhance detection performance.

Conclusion: This study advances the interpretability and effectiveness of Chinese hate speech detection, offering resources and methods for future research.

Abstract: The proliferation of hate speech has inflicted significant societal harm,
with its intensity and directionality closely tied to specific targets and
arguments. In recent years, numerous machine learning-based methods have been
developed to detect hateful comments on online platforms automatically.
However, research on Chinese hate speech detection lags behind, and
interpretability studies face two major challenges: first, the scarcity of
span-level fine-grained annotated datasets limits models' deep semantic
understanding of hate speech; second, insufficient research on identifying and
interpreting coded hate speech restricts model explainability in complex
real-world scenarios. To address these, we make the following contributions:
(1) We introduce the Span-level Target-Aware Toxicity Extraction dataset (STATE
ToxiCN), the first span-level Chinese hate speech dataset, and evaluate the
hate semantic understanding of existing models using it. (2) We conduct the
first comprehensive study on Chinese coded hate terms, LLMs' ability to
interpret hate semantics. (3) We propose a method to integrate an annotated
lexicon into models, significantly enhancing hate speech detection performance.
Our work provides valuable resources and insights to advance the
interpretability of Chinese hate speech detection research.

</details>


### [137] [Dr.Copilot: A Multi-Agent Prompt Optimized Assistant for Improving Patient-Doctor Communication in Romanian](https://arxiv.org/abs/2507.11299)
*Andrei Niculae,Adrian Cosma,Cosmin Dumitrache,Emilian Rǎdoi*

Main category: cs.CL

TL;DR: Dr.Copilot is a multi-agent LLM system for Romanian-speaking doctors, improving the presentation quality of telemedicine responses via feedback on 17 axes, not medical accuracy.


<details>
  <summary>Details</summary>
Motivation: Text-based telemedicine often prioritizes communication style over clinical accuracy, necessitating tools to enhance response quality.

Method: Uses three LLM agents with DSPy-optimized prompts, trained on low-resource Romanian data and open-weight models for real-time feedback.

Result: Empirical evaluations and live deployment with 41 doctors show improved user reviews and response quality.

Conclusion: Dr.Copilot is a successful real-world LLM deployment in Romanian telemedicine, enhancing communication quality.

Abstract: Text-based telemedicine has become increasingly common, yet the quality of
medical advice in doctor-patient interactions is often judged more on how
advice is communicated rather than its clinical accuracy. To address this, we
introduce Dr.Copilot , a multi-agent large language model (LLM) system that
supports Romanian-speaking doctors by evaluating and enhancing the presentation
quality of their written responses. Rather than assessing medical correctness,
Dr.Copilot provides feedback along 17 interpretable axes. The system comprises
of three LLM agents with prompts automatically optimized via DSPy. Designed
with low-resource Romanian data and deployed using open-weight models, it
delivers real-time specific feedback to doctors within a telemedicine platform.
Empirical evaluations and live deployment with 41 doctors show measurable
improvements in user reviews and response quality, marking one of the first
real-world deployments of LLMs in Romanian medical settings.

</details>


### [138] [Internal Value Alignment in Large Language Models through Controlled Value Vector Activation](https://arxiv.org/abs/2507.11316)
*Haoran Jin,Meng Li,Xiting Wang,Zhihao Xu,Minlie Huang,Yantao Jia,Defu Lian*

Main category: cs.CL

TL;DR: The paper introduces ConVA, a method to align LLMs with human values by controlling value vectors in their latent representations, ensuring consistency without performance loss.


<details>
  <summary>Details</summary>
Motivation: Aligning LLMs with human values is crucial for clarity, transparency, and adaptability in evolving scenarios.

Method: Proposes ConVA, combining context-controlled value vector identification and gated activation for precise, minimal control.

Result: Achieves highest control success rate across 10 values, maintains performance, and resists malicious inputs.

Conclusion: ConVA effectively aligns LLMs with human values while preserving model performance and fluency.

Abstract: Aligning Large Language Models (LLMs) with human values has attracted
increasing attention since it provides clarity, transparency, and the ability
to adapt to evolving scenarios. In this paper, we introduce a Controlled Value
Vector Activation (ConVA) method that directly aligns the internal values of
LLMs by interpreting how a value is encoded in their latent representations and
modifies relevant activations to ensure consistent values in LLMs. To ensure an
accurate and unbiased interpretation, we propose a context-controlled value
vector identification method. To consistently control values without
sacrificing model performance, we introduce a gated value vector activation
method for effective and minimum degree of value control. Experiments show that
our method achieves the highest control success rate across 10 basic values
without hurting LLM performance and fluency, and ensures target values even
with opposite and potentially malicious input prompts. Source code and data are
available at~ https://github.com/hr-jin/ConVA.

</details>


### [139] [Automated Novelty Evaluation of Academic Paper: A Collaborative Approach Integrating Human and Large Language Model Knowledge](https://arxiv.org/abs/2507.11330)
*Wenqing Wu,Chengzhi Zhang,Yi Zhao*

Main category: cs.CL

TL;DR: The paper proposes integrating human expertise and LLM knowledge to assess novelty in academic papers, focusing on method novelty, and achieves superior performance.


<details>
  <summary>Details</summary>
Motivation: Traditional novelty assessment methods (expert judgment or unique reference combinations) have limitations, prompting the need for a hybrid approach combining LLM knowledge and human expertise.

Method: Extracts novelty-related sentences from peer reviews, uses LLM to summarize methodology, fine-tunes PLMs, and employs a text-guided fusion module with Sparse-Attention.

Result: The proposed method outperforms numerous baselines in experiments.

Conclusion: Combining human and LLM knowledge effectively addresses novelty assessment limitations, with promising results.

Abstract: Novelty is a crucial criterion in the peer review process for evaluating
academic papers. Traditionally, it's judged by experts or measure by unique
reference combinations. Both methods have limitations: experts have limited
knowledge, and the effectiveness of the combination method is uncertain.
Moreover, it's unclear if unique citations truly measure novelty. The large
language model (LLM) possesses a wealth of knowledge, while human experts
possess judgment abilities that the LLM does not possess. Therefore, our
research integrates the knowledge and abilities of LLM and human experts to
address the limitations of novelty assessment. The most common novelty in
academic papers is the introduction of new methods. In this paper, we propose
leveraging human knowledge and LLM to assist pretrained language models (PLMs,
e.g. BERT etc.) in predicting the method novelty of papers. Specifically, we
extract sentences related to the novelty of the academic paper from peer review
reports and use LLM to summarize the methodology section of the academic paper,
which are then used to fine-tune PLMs. In addition, we have designed a
text-guided fusion module with novel Sparse-Attention to better integrate human
and LLM knowledge. We compared the method we proposed with a large number of
baselines. Extensive experiments demonstrate that our method achieves superior
performance.

</details>


### [140] [What is the Best Process Model Representation? A Comparative Analysis for Process Modeling with Large Language Models](https://arxiv.org/abs/2507.11356)
*Alexis Brissard,Frédéric Cuppens,Amal Zouaq*

Main category: cs.CL

TL;DR: This paper evaluates various Process Model Representations (PMRs) for Large Language Model (LLM)-based Process Modeling (PMo), introducing a new dataset and comparing PMRs on suitability and performance.


<details>
  <summary>Details</summary>
Motivation: The lack of systematic comparison among PMRs and inconsistent evaluation strategies in Process Model Generation (PMG) motivated this study.

Method: The study introduces the PMo Dataset (55 process descriptions paired with models in nine PMRs) and evaluates PMRs on suitability for LLM-based PMo and PMG performance.

Result: Mermaid scored highest overall for PMo, while BPMN text performed best in PMG for process element similarity.

Conclusion: The study provides empirical insights into PMR selection for LLM-based PMo, highlighting Mermaid and BPMN text as top performers.

Abstract: Large Language Models (LLMs) are increasingly applied for Process Modeling
(PMo) tasks such as Process Model Generation (PMG). To support these tasks,
researchers have introduced a variety of Process Model Representations (PMRs)
that serve as model abstractions or generation targets. However, these PMRs
differ widely in structure, complexity, and usability, and have never been
systematically compared. Moreover, recent PMG approaches rely on distinct
evaluation strategies and generation techniques, making comparison difficult.
This paper presents the first empirical study that evaluates multiple PMRs in
the context of PMo with LLMs. We introduce the PMo Dataset, a new dataset
containing 55 process descriptions paired with models in nine different PMRs.
We evaluate PMRs along two dimensions: suitability for LLM-based PMo and
performance on PMG. \textit{Mermaid} achieves the highest overall score across
six PMo criteria, whereas \textit{BPMN text} delivers the best PMG results in
terms of process element similarity.

</details>


### [141] [Addressing Data Imbalance in Transformer-Based Multi-Label Emotion Detection with Weighted Loss](https://arxiv.org/abs/2507.11384)
*Xia Cui*

Main category: cs.CL

TL;DR: A weighted loss function is applied to Transformer models for multi-label emotion detection, improving performance on high-frequency classes but with limited impact on minority classes.


<details>
  <summary>Details</summary>
Motivation: Address data imbalance in multi-label emotion detection without the computational cost of traditional resampling methods.

Method: Use a simple weighted loss function with BERT, RoBERTa, and BART models on the BRIGHTER dataset, evaluated via Micro F1, Macro F1, ROC-AUC, Accuracy, and Jaccard similarity.

Result: Weighted loss improves performance on high-frequency emotion classes but has limited impact on minority classes.

Conclusion: The approach is effective but highlights challenges in handling imbalanced multi-label emotion detection.

Abstract: This paper explores the application of a simple weighted loss function to
Transformer-based models for multi-label emotion detection in SemEval-2025
Shared Task 11. Our approach addresses data imbalance by dynamically adjusting
class weights, thereby enhancing performance on minority emotion classes
without the computational burden of traditional resampling methods. We evaluate
BERT, RoBERTa, and BART on the BRIGHTER dataset, using evaluation metrics such
as Micro F1, Macro F1, ROC-AUC, Accuracy, and Jaccard similarity coefficients.
The results demonstrate that the weighted loss function improves performance on
high-frequency emotion classes but shows limited impact on minority classes.
These findings underscore both the effectiveness and the challenges of applying
this approach to imbalanced multi-label emotion detection.

</details>


### [142] [DCR: Quantifying Data Contamination in LLMs Evaluation](https://arxiv.org/abs/2507.11405)
*Cheng Xu,Nan Yan,Shuhao Guan,Changhong Jin,Yuke Mei,Yibing Guo,M-Tahar Kechadi*

Main category: cs.CL

TL;DR: The paper introduces the Data Contamination Risk (DCR) framework to detect and quantify benchmark data contamination in LLMs, adjusting accuracy metrics for fairer evaluations.


<details>
  <summary>Details</summary>
Motivation: Address concerns about benchmark data contamination (BDC) in LLMs, which inflates performance metrics and undermines genuine generalization assessment.

Method: Proposes the DCR framework, a lightweight pipeline detecting BDC at four granular levels (semantic, informational, data, label) and synthesizing contamination scores via a fuzzy inference system.

Result: Validated on 9 LLMs, DCR reliably diagnoses contamination severity and adjusts accuracy to within 4% average error compared to uncontaminated baselines.

Conclusion: DCR offers a practical, efficient tool for contamination assessment, improving LLM benchmarking credibility and enabling fairer comparisons.

Abstract: The rapid advancement of large language models (LLMs) has heightened concerns
about benchmark data contamination (BDC), where models inadvertently memorize
evaluation data, inflating performance metrics and undermining genuine
generalization assessment. This paper introduces the Data Contamination Risk
(DCR) framework, a lightweight, interpretable pipeline designed to detect and
quantify BDC across four granular levels: semantic, informational, data, and
label. By synthesizing contamination scores via a fuzzy inference system, DCR
produces a unified DCR Factor that adjusts raw accuracy to reflect
contamination-aware performance. Validated on 9 LLMs (0.5B-72B) across
sentiment analysis, fake news detection, and arithmetic reasoning tasks, the
DCR framework reliably diagnoses contamination severity and with accuracy
adjusted using the DCR Factor to within 4% average error across the three
benchmarks compared to the uncontaminated baseline. Emphasizing computational
efficiency and transparency, DCR provides a practical tool for integrating
contamination assessment into routine evaluations, fostering fairer comparisons
and enhancing the credibility of LLM benchmarking practices.

</details>


### [143] [EXAONE 4.0: Unified Large Language Models Integrating Non-reasoning and Reasoning Modes](https://arxiv.org/abs/2507.11407)
*LG AI Research,:,Kyunghoon Bae,Eunbi Choi,Kibong Choi,Stanley Jungkyu Choi,Yemuk Choi,Kyubeen Han,Seokhee Hong,Junwon Hwang,Taewan Hwang,Joonwon Jang,Hyojin Jeon,Kijeong Jeon,Gerrard Jeongwon Jo,Hyunjik Jo,Jiyeon Jung,Euisoon Kim,Hyosang Kim,Jihoon Kim,Joonkee Kim,Seonghwan Kim,Soyeon Kim,Sunkyoung Kim,Yireun Kim,Yongil Kim,Youchul Kim,Edward Hwayoung Lee,Gwangho Lee,Haeju Lee,Honglak Lee,Jinsik Lee,Kyungmin Lee,Sangha Park,Young Min Paik,Yongmin Park,Youngyong Park,Sanghyun Seo,Sihoon Yang,Heuiyeen Yeen,Sihyuk Yi,Hyeongu Yun*

Main category: cs.CL

TL;DR: EXAONE 4.0 integrates Non-reasoning and Reasoning modes, enhances multilingual support, and offers two model sizes for varied applications, outperforming open-weight models.


<details>
  <summary>Details</summary>
Motivation: To combine usability and advanced reasoning while paving the way for agentic AI with tool use and multilingual capabilities.

Method: Incorporates agentic tool use, supports Spanish, and provides 32B (high performance) and 1.2B (on-device) models.

Result: Superior performance compared to open-weight models and competitive against frontier-class models.

Conclusion: EXAONE 4.0 is publicly available for research, offering advanced features and strong performance.

Abstract: This technical report introduces EXAONE 4.0, which integrates a Non-reasoning
mode and a Reasoning mode to achieve both the excellent usability of EXAONE 3.5
and the advanced reasoning abilities of EXAONE Deep. To pave the way for the
agentic AI era, EXAONE 4.0 incorporates essential features such as agentic tool
use, and its multilingual capabilities are extended to support Spanish in
addition to English and Korean. The EXAONE 4.0 model series consists of two
sizes: a mid-size 32B model optimized for high performance, and a small-size
1.2B model designed for on-device applications. The EXAONE 4.0 demonstrates
superior performance compared to open-weight models in its class and remains
competitive even against frontier-class models. The models are publicly
available for research purposes and can be easily downloaded via
https://huggingface.co/LGAI-EXAONE.

</details>


### [144] [KisMATH: Do LLMs Have Knowledge of Implicit Structures in Mathematical Reasoning?](https://arxiv.org/abs/2507.11408)
*Soumadeep Saha,Akshay Chaturvedi,Saptarshi Saha,Utpal Garain,Nicholas Asher*

Main category: cs.CL

TL;DR: The paper introduces Causal CoT Graphs (CCGs) to analyze chain-of-thought reasoning in LLMs, using a dataset (KisMATH) of 1671 math problems. Findings show CCGs mediate answers and align with LLM reasoning paths.


<details>
  <summary>Details</summary>
Motivation: To understand how chain-of-thought traces improve LLM reasoning, as the mechanism is unclear.

Method: Develop CCGs from reasoning traces to model causal dependencies, and analyze 15 LLMs using the KisMATH dataset.

Result: CCGs mediate final answers, and LLMs align with CCG reasoning paths, suggesting internal graph-like structures.

Conclusion: KisMATH enables controlled interventions and further study of chain-of-thought's role in LLM reasoning.

Abstract: Chain-of-thought traces have been shown to improve performance of large
language models in a plethora of reasoning tasks, yet there is no consensus on
the mechanism through which this performance boost is achieved. To shed more
light on this, we introduce Causal CoT Graphs (CCGs), which are directed
acyclic graphs automatically extracted from reasoning traces that model
fine-grained causal dependencies in the language model output. A collection of
$1671$ mathematical reasoning problems from MATH500, GSM8K and AIME, and their
associated CCGs are compiled into our dataset -- \textbf{KisMATH}. Our detailed
empirical analysis with 15 open-weight LLMs shows that (i) reasoning nodes in
the CCG are mediators for the final answer, a condition necessary for
reasoning; and (ii) LLMs emphasise reasoning paths given by the CCG, indicating
that models internally realise structures akin to our graphs. KisMATH enables
controlled, graph-aligned interventions and opens up avenues for further
investigation into the role of chain-of-thought in LLM reasoning.

</details>


### [145] [Seq vs Seq: An Open Suite of Paired Encoders and Decoders](https://arxiv.org/abs/2507.11412)
*Orion Weller,Kathryn Ricci,Marc Marone,Antoine Chaffin,Dawn Lawrie,Benjamin Van Durme*

Main category: cs.CL

TL;DR: The paper introduces the SOTA Ettin suite of paired encoder-only and decoder-only models, trained uniformly to compare their performance fairly. Encoders excel in classification/retrieval, while decoders outperform in generative tasks. Adapting one for the other's tasks is less effective than using the original architecture.


<details>
  <summary>Details</summary>
Motivation: To address the lack of fair comparisons between encoder-only and decoder-only models due to varying parameters, training techniques, and datasets.

Method: Developed the Ettin suite with paired encoder-decoder models (17M to 1B parameters, trained on up to 2T tokens) using identical training recipes. Evaluated performance on classification, retrieval, and generative tasks.

Result: Encoder-only models outperform in classification/retrieval, while decoder-only models excel in generation. Adapting architectures for opposite tasks is suboptimal.

Conclusion: Architecture specialization matters; encoders and decoders should be used for their respective tasks. The study provides open-source artifacts for further research.

Abstract: The large language model (LLM) community focuses almost exclusively on
decoder-only language models, since they are easier to use for text generation.
However, a large subset of the community still uses encoder-only models for
tasks such as classification or retrieval. Previous work has attempted to
compare these architectures, but is forced to make comparisons with models that
have different numbers of parameters, training techniques, and datasets. We
introduce the SOTA open-data Ettin suite of models: paired encoder-only and
decoder-only models ranging from 17 million parameters to 1 billion, trained on
up to 2 trillion tokens. Using the same recipe for both encoder-only and
decoder-only models produces SOTA recipes in both categories for their
respective sizes, beating ModernBERT as an encoder and Llama 3.2 and SmolLM2 as
decoders. Like previous work, we find that encoder-only models excel at
classification and retrieval tasks while decoders excel at generative tasks.
However, we show that adapting a decoder model to encoder tasks (and vice
versa) through continued training is subpar compared to using only the reverse
objective (i.e. a 400M encoder outperforms a 1B decoder on MNLI, and vice versa
for generative tasks). We open-source all artifacts of this study including
training data, training order segmented by checkpoint, and 200+ checkpoints to
allow future work to analyze or extend all aspects of training.

</details>


### [146] [Reasoning Strategies in Large Language Models: Can They Follow, Prefer, and Optimize?](https://arxiv.org/abs/2507.11423)
*Yanjian Zhang,Guillaume Wisniewski,Nadi Tomeh,Thierry Charnois*

Main category: cs.CL

TL;DR: The paper explores whether prompting can control LLM reasoning strategies and evaluates its impact on problem-solving. While no single strategy consistently improves accuracy, adaptive strategy selection could enhance performance.


<details>
  <summary>Details</summary>
Motivation: Prior work indicates LLMs often rely on a single reasoning strategy, limiting their effectiveness in diverse challenges. This study investigates if prompting can diversify and optimize these strategies.

Method: The study involves experiments to assess the impact of prompting on LLM reasoning strategies and proposes methods to guide adaptive strategy selection.

Result: No single reasoning strategy consistently improves accuracy, but adaptive strategy selection shows potential for enhancing performance.

Conclusion: The findings suggest refining LLM reasoning by guiding adaptive strategy selection, offering new ways to improve their problem-solving abilities.

Abstract: Human reasoning involves different strategies, each suited to specific
problems. Prior work shows that large language model (LLMs) tend to favor a
single reasoning strategy, potentially limiting their effectiveness in diverse
reasoning challenges. In this work, we investigate whether prompting can
control LLMs reasoning strategies and assess its impact on logical
problem-solving. While our experiments show that no single strategy
consistently improves accuracy, performance could be enhanced if models could
adaptively choose the optimal strategy. We propose methods to guide LLMs in
strategy selection, highlighting new ways to refine their reasoning abilities.

</details>


### [147] [HKGAI-V1: Towards Regional Sovereign Large Language Model for Hong Kong](https://arxiv.org/abs/2507.11502)
*Sirui Han,Junqi Zhu,Ruiyuan Zhang,Yike Guo*

Main category: cs.CL

TL;DR: The paper introduces HKGAI-V1, a sovereign LLM tailored for Hong Kong, addressing its multilingual, socio-legal, and cultural needs. It outperforms general models in local contexts and includes a unique alignment benchmark.


<details>
  <summary>Details</summary>
Motivation: To create a value-aligned AI for Hong Kong, addressing its unique linguistic, cultural, and legal challenges under the 'one country, two systems' framework.

Method: Built on DeepSeek architecture with full parameter fine-tuning and integrated RAG for factual accuracy. Includes a proprietary adversarial benchmark for alignment evaluation.

Result: HKGAI-V1 excels in handling Hong Kong-specific queries and embeds governance for digital sovereignty. The adversarial benchmark ensures local ethical and legal alignment.

Conclusion: The paper offers a replicable blueprint for regionally focused AI systems, combining technological innovation with local identity and governance.

Abstract: This paper presents the development of HKGAI-V1, a foundational sovereign
large language model (LLM), developed as part of an initiative to establish
value-aligned AI infrastructure specifically tailored for Hong Kong. Addressing
the region's unique multilingual environment (Cantonese, Mandarin, and
English), its distinct socio-legal context under the "one country, two systems"
framework, and specific local cultural and value considerations, the model is
built upon the DeepSeek architecture and systematically aligned with regional
norms through a multifaceted full parameter fine-tuning process. It is further
integrated with a retrieval-augmented generation (RAG) system to ensure timely
and factually grounded information access. The core contribution lies in the
design and implementation of a comprehensive, region-specific AI alignment and
safety framework, demonstrated through two key achievements: 1) The successful
development of HKGAI-V1 itself - which outper-forms general-purpose models in
handling Hong Kong-specific culturally sensitive queries, and embodies a
"governance-embedded" approach to digital sovereignty - empowers Hong Kong to
exercise control over AI applications in critical sectors including public
services, legal systems, and edu-cation. 2) The development of the proprietary
Adversarial HK Value Benchmark, a rigorous tool for evaluating model alignment
with local ethical and legal stand-ards under challenging conditions. By
documenting these achievements, the paper provides not only a technological
artifact but also a replicable blueprint for developing advanced, regionally
focused AI systems deeply rooted in their local identities.

</details>


### [148] [Real-World Summarization: When Evaluation Reaches Its Limits](https://arxiv.org/abs/2507.11508)
*Patrícia Schmidtová,Ondřej Dušek,Saad Mahamood*

Main category: cs.CL

TL;DR: Simpler metrics like word overlap correlate well with human judgments for evaluating LLM-generated hotel highlights, while LLMs are unreliable for evaluation. Crowdsourcing and incorrect information pose challenges.


<details>
  <summary>Details</summary>
Motivation: To evaluate the faithfulness of LLM-generated hotel highlights to input data and compare traditional metrics, trainable methods, and LLM-as-a-judge approaches.

Method: Human evaluation campaigns involving categorical error assessment and span-level annotation, comparing various evaluation methods.

Result: Simpler metrics (e.g., word overlap) correlate well with human judgments (Spearman 0.63), outperforming complex methods on out-of-domain data. LLMs are unreliable evaluators.

Conclusion: Traditional metrics are effective for faithfulness evaluation, while LLMs are unreliable. Crowdsourcing and non-checkable information present risks.

Abstract: We examine evaluation of faithfulness to input data in the context of hotel
highlights: brief LLM-generated summaries that capture unique features of
accommodations. Through human evaluation campaigns involving categorical error
assessment and span-level annotation, we compare traditional metrics, trainable
methods, and LLM-as-a-judge approaches. Our findings reveal that simpler
metrics like word overlap correlate surprisingly well with human judgments
(Spearman correlation rank of 0.63), often outperforming more complex methods
when applied to out-of-domain data. We further demonstrate that while LLMs can
generate high-quality highlights, they prove unreliable for evaluation as they
tend to severely under- or over-annotate. Our analysis of real-world business
impacts shows incorrect and non-checkable information pose the greatest risks.
We also highlight challenges in crowdsourced evaluations.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [149] [Tool-to-Tool Matching Analysis Based Difference Score Computation Methods for Semiconductor Manufacturing](https://arxiv.org/abs/2507.10564)
*Sameera Bharadwaja H.,Siddhrath Jandial,Shashank S. Agashe,Rajesh Kumar Reddy Moore,Youngkwan Kim*

Main category: cs.LG

TL;DR: The paper proposes novel tool-to-tool matching (TTTM) pipelines for semiconductor manufacturing, addressing limitations of traditional methods in heterogeneous settings.


<details>
  <summary>Details</summary>
Motivation: Traditional TTTM methods rely on static data or golden references, which are hard to obtain and ineffective for heterogeneous equipment.

Method: Proposes univariate and multivariate TTTM pipelines, analyzing variance and modes in data to detect mismatches.

Result: Univariate methods achieve >0.95 correlation with variance and >0.5 with modes; multivariate methods achieve >0.75 correlation with top univariate methods.

Conclusion: The proposed methods are effective for TTTM in heterogeneous settings, with multivariate methods showing robustness.

Abstract: We consider the problem of tool-to-tool matching (TTTM), also called, chamber
matching in the context of a semiconductor manufacturing equipment. Traditional
TTTM approaches utilize static configuration data or depend on a golden
reference which are difficult to obtain in a commercial manufacturing line.
Further, existing methods do not extend very well to a heterogeneous setting,
where equipment are of different make-and-model, sourced from different
equipment vendors. We propose novel TTTM analysis pipelines to overcome these
issues. We hypothesize that a mismatched equipment would have higher variance
and/or higher number of modes in the data. Our best univariate method achieves
a correlation coefficient >0.95 and >0.5 with the variance and number of modes,
respectively showing that the proposed methods are effective. Also, the best
multivariate method achieves a correlation coefficient >0.75 with the
top-performing univariate methods, showing its effectiveness. Finally, we
analyze the sensitivity of the multivariate algorithms to the algorithm
hyper-parameters.

</details>


### [150] [Enhancing Cross Entropy with a Linearly Adaptive Loss Function for Optimized Classification Performance](https://arxiv.org/abs/2507.10574)
*Jae Wan Shim*

Main category: cs.LG

TL;DR: A novel Linearly Adaptive Cross Entropy Loss function is proposed, outperforming standard cross entropy in classification tasks while maintaining efficiency.


<details>
  <summary>Details</summary>
Motivation: To enhance optimization in classification tasks with one-hot encoded labels by introducing an adaptive term based on predicted probability.

Method: Derived from information theory, the loss function adds a term dependent on the true class's predicted probability. Evaluated using a ResNet model on CIFAR-100.

Result: Consistently outperforms standard cross entropy in accuracy while maintaining similar efficiency.

Conclusion: The proposed loss function shows promise for future research in loss function design.

Abstract: We propose the Linearly Adaptive Cross Entropy Loss function. This is a novel
measure derived from the information theory. In comparison to the standard
cross entropy loss function, the proposed one has an additional term that
depends on the predicted probability of the true class. This feature serves to
enhance the optimization process in classification tasks involving one-hot
encoded class labels. The proposed one has been evaluated on a ResNet-based
model using the CIFAR-100 dataset. Preliminary results show that the proposed
one consistently outperforms the standard cross entropy loss function in terms
of classification accuracy. Moreover, the proposed one maintains simplicity,
achieving practically the same efficiency to the traditional cross entropy
loss. These findings suggest that our approach could broaden the scope for
future research into loss function design.

</details>


### [151] [An Adaptive Volatility-based Learning Rate Scheduler](https://arxiv.org/abs/2507.10575)
*Kieran Chai Kai Ren*

Main category: cs.LG

TL;DR: VolSched is a novel adaptive learning rate scheduler using volatility metrics to dynamically adjust LR, improving model generalization and accuracy on CIFAR-100.


<details>
  <summary>Details</summary>
Motivation: Pre-defined and adaptive LR schedulers often lead to suboptimal generalization, prompting the need for a more dynamic approach.

Method: VolSched calculates long-term vs. short-term accuracy volatility to adjust LR, escaping plateaus and stabilizing training.

Result: VolSched improves top-1 accuracy by 1.4 and 1.3 points on ResNet-18 and ResNet-34, respectively, and finds flatter minima for better generalization.

Conclusion: VolSched effectively enhances training dynamics and generalization by leveraging volatility-based LR adjustments.

Abstract: Effective learning rate (LR) scheduling is crucial for training deep neural
networks. However, popular pre-defined and adaptive schedulers can still lead
to suboptimal generalization. This paper introduces VolSched, a novel adaptive
LR scheduler inspired by the concept of volatility in stochastic processes like
Geometric Brownian Motion to dynamically adjust the learning rate. By
calculating the ratio between long-term and short-term accuracy volatility,
VolSched increases the LR to escape plateaus and decreases it to stabilize
training, allowing the model to explore the loss landscape more effectively. We
evaluate VolSched on the CIFAR-100 dataset against a strong baseline using a
standard augmentation pipeline. When paired with ResNet-18 and ResNet-34, our
scheduler delivers consistent performance gains, improving top-1 accuracy by
1.4 and 1.3 percentage points respectively. Analysis of the loss curves reveals
that VolSched promotes a longer exploration phase. A quantitative analysis of
the Hessian shows that VolSched finds a final solution that is 38% flatter than
the next-best baseline, allowing the model to obtain wider minima and hence
better generalization performance.

</details>


### [152] [Universal Approximation Theorem for a Single-Layer Transformer](https://arxiv.org/abs/2507.10581)
*Esmail Gumaan*

Main category: cs.LG

TL;DR: The paper explores the theoretical foundations of deep learning and Transformers, proving a universal approximation theorem for Transformers and bridging theory with practice.


<details>
  <summary>Details</summary>
Motivation: Despite the success of deep learning and Transformers, their theoretical understanding remains limited. This paper aims to address this gap.

Method: The authors review key mathematical concepts, analyze self-attention and backpropagation, and prove a universal approximation theorem for Transformers.

Result: A single-layer Transformer can approximate any continuous sequence-to-sequence mapping on a compact domain to arbitrary precision.

Conclusion: The findings advance theoretical understanding of Transformers and help bridge the gap between theory and practice.

Abstract: Deep learning employs multi-layer neural networks trained via the
backpropagation algorithm. This approach has achieved success across many
domains and relies on adaptive gradient methods such as the Adam optimizer.
Sequence modeling evolved from recurrent neural networks to attention-based
models, culminating in the Transformer architecture. Transformers have achieved
state-of-the-art performance in natural language processing (for example, BERT
and GPT-3) and have been applied in computer vision and computational biology.
However, theoretical understanding of these models remains limited. In this
paper, we examine the mathematical foundations of deep learning and
Transformers and present a novel theoretical result. We review key concepts
from linear algebra, probability, and optimization that underpin deep learning,
and we analyze the multi-head self-attention mechanism and the backpropagation
algorithm in detail. Our main contribution is a universal approximation theorem
for Transformers: we prove that a single-layer Transformer, comprising one
self-attention layer followed by a position-wise feed-forward network with ReLU
activation, can approximate any continuous sequence-to-sequence mapping on a
compact domain to arbitrary precision. We provide a formal statement and a
complete proof. Finally, we present case studies that demonstrate the practical
implications of this result. Our findings advance the theoretical understanding
of Transformer models and help bridge the gap between theory and practice.

</details>


### [153] [MH-FSF: A Unified Framework for Overcoming Benchmarking and Reproducibility Limitations in Feature Selection Evaluation](https://arxiv.org/abs/2507.10591)
*Vanderson Rocha,Diego Kreutz,Gabriel Canto,Hendrio Bragança,Eduardo Feitosa*

Main category: cs.LG

TL;DR: The paper introduces MH-FSF, a modular framework for feature selection, addressing reproducibility issues by benchmarking 17 methods on 10 public Android malware datasets. Results highlight performance variations and the need for preprocessing.


<details>
  <summary>Details</summary>
Motivation: Current feature selection research lacks reproducibility due to limited benchmarking and proprietary datasets, impacting performance.

Method: Developed MH-FSF, a modular framework implementing 17 feature selection methods (11 classical, 6 domain-specific) and evaluating them on 10 public Android malware datasets.

Result: Performance varies across balanced and imbalanced datasets, emphasizing the need for preprocessing and tailored selection criteria.

Conclusion: MH-FSF fosters methodological consistency, broadens literature, and opens new research directions in feature selection, especially for Android malware detection.

Abstract: Feature selection is vital for building effective predictive models, as it
reduces dimensionality and emphasizes key features. However, current research
often suffers from limited benchmarking and reliance on proprietary datasets.
This severely hinders reproducibility and can negatively impact overall
performance. To address these limitations, we introduce the MH-FSF framework, a
comprehensive, modular, and extensible platform designed to facilitate the
reproduction and implementation of feature selection methods. Developed through
collaborative research, MH-FSF provides implementations of 17 methods (11
classical, 6 domain-specific) and enables systematic evaluation on 10 publicly
available Android malware datasets. Our results reveal performance variations
across both balanced and imbalanced datasets, highlighting the critical need
for data preprocessing and selection criteria that account for these
asymmetries. We demonstrate the importance of a unified platform for comparing
diverse feature selection techniques, fostering methodological consistency and
rigor. By providing this framework, we aim to significantly broaden the
existing literature and pave the way for new research directions in feature
selection, particularly within the context of Android malware detection.

</details>


### [154] [Extension OL-MDISF: Online Learning from Mix-Typed, Drifted, and Incomplete Streaming Features](https://arxiv.org/abs/2507.10594)
*Shengda Zhuo,Di Wu,Yi He,Shuqiang Huang,Xindong Wu*

Main category: cs.LG

TL;DR: OL-MDISF addresses challenges in online learning with mixed, drifted, and incomplete features using copula-based representation, drift detection, and pseudo-labeling.


<details>
  <summary>Details</summary>
Motivation: To tackle challenges like heterogeneous data, distribution shifts, and labeling constraints in online learning.

Method: Uses latent copula-based representation, ensemble entropy for drift detection, and structure-aware pseudo-labeling.

Result: Tested on 14 datasets, showing performance under drift scenarios with CER trends, ablation studies, and sensitivity analyses.

Conclusion: Provides a reproducible benchmark for weakly supervised online learning on complex streaming data.

Abstract: Online learning, where feature spaces can change over time, offers a flexible
learning paradigm that has attracted considerable attention. However, it still
faces three significant challenges. First, the heterogeneity of real-world data
streams with mixed feature types presents challenges for traditional parametric
modeling. Second, data stream distributions can shift over time, causing an
abrupt and substantial decline in model performance. Third, it is often
infeasible to label every data instance due to time and cost constraints. To
address these issues, we proposed OL-MDISF (Online Learning from Mix-typed,
Drifted, and Incomplete Streaming Features), which constructs a latent
copula-based representation for heterogeneous features, detects drifts via
ensemble entropy and latent mismatch, and performs structure-aware
pseudo-labeling.
  This companion paper serves as a standalone technical reference to OL-MDISF.
It provides a contextual discussion of related work in mixed-type modeling,
drift adaptation, and weak supervision, as well as a comprehensive set of
experiments across 14 real-world datasets under two types of drift scenarios.
These include CER trends, ablation studies, sensitivity analyses, and temporal
ensemble dynamics. We hope this document offers a reproducible benchmark for
online learning on complex, weakly supervised streaming data.

</details>


### [155] [Divide-Then-Rule: A Cluster-Driven Hierarchical Interpolator for Attribute-Missing Graphs](https://arxiv.org/abs/2507.10595)
*Yaowen Hu,Wenxuan Tu,Yue Liu,Miaomiao Li,Wenpeng Lu,Zhigang Luo,Xinwang Liu,Ping Chen*

Main category: cs.LG

TL;DR: DTRGC is a novel method for deep graph clustering in attribute-missing graphs, using hierarchical imputation and clustering to improve accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing imputation methods for attribute-missing graphs often fail due to varying neighborhood information, leading to unreliable results.

Method: DTRGC uses Dynamic Cluster-Aware Feature Propagation, Hierarchical Neighborhood-aware Imputation, and Hop-wise Representation Enhancement to iteratively impute missing attributes and refine clustering.

Result: Experiments on six datasets show DTRGC significantly improves clustering performance for attribute-missing graphs.

Conclusion: DTRGC effectively addresses the challenge of attribute-missing graphs by leveraging clustering and hierarchical imputation, outperforming existing methods.

Abstract: Deep graph clustering (DGC) for attribute-missing graphs is an unsupervised
task aimed at partitioning nodes with incomplete attributes into distinct
clusters. Addressing this challenging issue is vital for practical
applications. However, research in this area remains underexplored. Existing
imputation methods for attribute-missing graphs often fail to account for the
varying amounts of information available across node neighborhoods, leading to
unreliable results, especially for nodes with insufficient known neighborhood.
To address this issue, we propose a novel method named Divide-Then-Rule Graph
Completion (DTRGC). This method first addresses nodes with sufficient known
neighborhood information and treats the imputed results as new knowledge to
iteratively impute more challenging nodes, while leveraging clustering
information to correct imputation errors. Specifically, Dynamic Cluster-Aware
Feature Propagation (DCFP) initializes missing node attributes by adjusting
propagation weights based on the clustering structure. Subsequently,
Hierarchical Neighborhood-aware Imputation (HNAI) categorizes attribute-missing
nodes into three groups based on the completeness of their neighborhood
attributes. The imputation is performed hierarchically, prioritizing the groups
with nodes that have the most available neighborhood information. The cluster
structure is then used to refine the imputation and correct potential errors.
Finally, Hop-wise Representation Enhancement (HRE) integrates information
across multiple hops, thereby enriching the expressiveness of node
representations. Experimental results on six widely used graph datasets show
that DTRGC significantly improves the clustering performance of various DGC
methods under attribute-missing graphs.

</details>


### [156] [RedOne: Revealing Domain-specific LLM Post-Training in Social Networking Services](https://arxiv.org/abs/2507.10605)
*Fei Zhao,Chonggang Lu,Yue Wang,Zheyong Xie,Ziyan Liu,Haofu Qian,JianZhao Huang,Fangcheng Shi,Zijie Meng,Hongcheng Guo,Mingqian He,Xinze Lyu,Yiming Lu,Ziyang Xiang,Zheyu Ye,Chengqiang Lu,Zhe Xu,Yi Wu,Yao Hu,Yan Gao,Jun Fan,Xiaolong Jiang,Weiting Liu,Boyang Wang,Shaosheng Cao*

Main category: cs.LG

TL;DR: RedOne, a domain-specific LLM for SNS, outperforms single-task baselines by 14.02% on average across 8 tasks and reduces harmful content exposure by 11.23%.


<details>
  <summary>Details</summary>
Motivation: Address challenges in SNS content management and interaction quality by overcoming limitations of isolated task-focused LLMs.

Method: Three-stage training: continued pretraining, supervised fine-tuning, and preference optimization using large-scale real-world data.

Result: Average 14.02% improvement on SNS tasks, 7.56% on bilingual benchmarks, 11.23% reduction in harmful content exposure, and 14.95% boost in post-view search clicks.

Conclusion: RedOne is a robust, generalizable domain-specific LLM for SNS, effective in real-world applications.

Abstract: As a primary medium for modern information dissemination, social networking
services (SNS) have experienced rapid growth, which has proposed significant
challenges for platform content management and interaction quality improvement.
Recently, the development of large language models (LLMs) has offered potential
solutions but existing studies focus on isolated tasks, which not only
encounter diminishing benefit from the data scaling within individual scenarios
but also fail to flexibly adapt to diverse real-world context. To address these
challenges, we introduce RedOne, a domain-specific LLM designed to break the
performance bottleneck of single-task baselines and establish a comprehensive
foundation for the SNS. RedOne was developed through a three-stage training
strategy consisting of continue pretraining, supervised fine-tuning, and
preference optimization, using a large-scale real-world dataset. Through
extensive experiments, RedOne maintains strong general capabilities, and
achieves an average improvement up to 14.02% across 8 major SNS tasks and 7.56%
in SNS bilingual evaluation benchmark, compared with base models. Furthermore,
through online testing, RedOne reduced the exposure rate in harmful content
detection by 11.23% and improved the click page rate in post-view search by
14.95% compared with single-tasks finetuned baseline models. These results
establish RedOne as a robust domain-specific LLM for SNS, demonstrating
excellent generalization across various tasks and promising applicability in
real-world scenarios.

</details>


### [157] [DALI-PD: Diffusion-based Synthetic Layout Heatmap Generation for ML in Physical Design](https://arxiv.org/abs/2507.10606)
*Bing-Yue Wu,Vidya A. Chhabria*

Main category: cs.LG

TL;DR: DALI-PD is a scalable framework using diffusion models to generate synthetic layout heatmaps for ML in physical design, addressing dataset limitations.


<details>
  <summary>Details</summary>
Motivation: Overcoming the lack of high-quality, large-scale datasets for ML in physical design, which are costly and constrained by IP.

Method: Uses a diffusion model to generate diverse layout heatmaps (power, IR drop, congestion, etc.) quickly.

Result: Created a dataset of 20,000+ synthetic layouts resembling real ones, improving ML accuracy for tasks like IR drop prediction.

Conclusion: DALI-PD provides a scalable solution to dataset scarcity, enhancing ML applications in physical design.

Abstract: Machine learning (ML) has demonstrated significant promise in various
physical design (PD) tasks. However, model generalizability remains limited by
the availability of high-quality, large-scale training datasets. Creating such
datasets is often computationally expensive and constrained by IP. While very
few public datasets are available, they are typically static, slow to generate,
and require frequent updates. To address these limitations, we present DALI-PD,
a scalable framework for generating synthetic layout heatmaps to accelerate ML
in PD research. DALI-PD uses a diffusion model to generate diverse layout
heatmaps via fast inference in seconds. The heatmaps include power, IR drop,
congestion, macro placement, and cell density maps. Using DALI-PD, we created a
dataset comprising over 20,000 layout configurations with varying macro counts
and placements. These heatmaps closely resemble real layouts and improve ML
accuracy on downstream ML tasks such as IR drop or congestion prediction.

</details>


### [158] [A Feed-Forward Artificial Intelligence Pipeline for Sustainable Desalination under Climate Uncertainties: UAE Insights](https://arxiv.org/abs/2507.10609)
*Obumneme Nwafor,Chioma Nwafor,Amro Zakaria,Nkechi Nwankwo*

Main category: cs.LG

TL;DR: The UAE heavily depends on energy-intensive desalination, contributing to CO2 emissions. A novel two-stage predictive model forecasts AOD and desalination efficiency losses, achieving 98% accuracy, and proposes dust-aware control logic for adaptive planning.


<details>
  <summary>Details</summary>
Motivation: The UAE's reliance on desalination faces sustainability challenges due to climate uncertainties like rising temperatures and AOD, impacting solar-powered systems.

Method: A pipelined two-stage model: first forecasts AOD using satellite data, then predicts efficiency losses. SHAP analysis identifies degradation drivers. Dust-aware control logic adjusts operations.

Result: 98% accuracy in predictions. Interactive dashboard provides decision-support for climate-adaptive planning.

Conclusion: The framework enhances desalination sustainability by integrating predictive analytics and adaptive controls, aiding UAE's climate resilience.

Abstract: The United Arab Emirates (UAE) relies heavily on seawater desalination to
meet over 90% of its drinking water needs. Desalination processes are highly
energy intensive and account for approximately 15% of the UAE's electricity
consumption, contributing to over 22% of the country's energy-related CO2
emissions. Moreover, these processes face significant sustainability challenges
in the face of climate uncertainties such as rising seawater temperatures,
salinity, and aerosol optical depth (AOD). AOD greatly affects the operational
and economic performance of solar-powered desalination systems through
photovoltaic soiling, membrane fouling, and water turbidity cycles.
  This study proposes a novel pipelined two-stage predictive modelling
architecture: the first stage forecasts AOD using satellite-derived time series
and meteorological data; the second stage uses the predicted AOD and other
meteorological factors to predict desalination performance efficiency losses.
The framework achieved 98% accuracy, and SHAP (SHapley Additive exPlanations)
was used to reveal key drivers of system degradation. Furthermore, this study
proposes a dust-aware rule-based control logic for desalination systems based
on predicted values of AOD and solar efficiency. This control logic is used to
adjust the desalination plant feed water pressure, adapt maintenance
scheduling, and regulate energy source switching.
  To enhance the practical utility of the research findings, the predictive
models and rule-based controls were packaged into an interactive dashboard for
scenario and predictive analytics. This provides a management decision-support
system for climate-adaptive planning.

</details>


### [159] [FedGSCA: Medical Federated Learning with Global Sample Selector and Client Adaptive Adjuster under Label Noise](https://arxiv.org/abs/2507.10611)
*Mengwen Ye,Yingzi Huangfu,Shujian Gao,Wei Ren,Weifan Liu,Zekuan Yu*

Main category: cs.LG

TL;DR: FedGSCA is a robust federated learning framework for medical image classification that addresses label noise and data imbalance through a Global Sample Selector and Client Adaptive Adjustment mechanism.


<details>
  <summary>Details</summary>
Motivation: Label noise and data imbalance in federated medical image classification degrade model performance, motivating the need for a robust solution.

Method: FedGSCA uses a Global Sample Selector to aggregate noise knowledge and a Client Adaptive Adjustment mechanism with adaptive threshold pseudo-label generation and Robust Credal Labeling Loss.

Result: FedGSCA outperforms state-of-the-art methods, especially in extreme and heterogeneous noise scenarios, improving model stability and generalizability.

Conclusion: FedGSCA is effective for real-world medical federated learning, handling complex noise and enhancing model robustness.

Abstract: Federated Learning (FL) emerged as a solution for collaborative medical image
classification while preserving data privacy. However, label noise, which
arises from inter-institutional data variability, can cause training
instability and degrade model performance. Existing FL methods struggle with
noise heterogeneity and the imbalance in medical data. Motivated by these
challenges, we propose FedGSCA, a novel framework for enhancing robustness in
noisy medical FL. FedGSCA introduces a Global Sample Selector that aggregates
noise knowledge from all clients, effectively addressing noise heterogeneity
and improving global model stability. Furthermore, we develop a Client Adaptive
Adjustment (CAA) mechanism that combines adaptive threshold pseudo-label
generation and Robust Credal Labeling Loss. CAA dynamically adjusts to class
distributions, ensuring the inclusion of minority samples and carefully
managing noisy labels by considering multiple plausible labels. This dual
approach mitigates the impact of noisy data and prevents overfitting during
local training, which improves the generalizability of the model. We evaluate
FedGSCA on one real-world colon slides dataset and two synthetic medical
datasets under various noise conditions, including symmetric, asymmetric,
extreme, and heterogeneous types. The results show that FedGSCA outperforms the
state-of-the-art methods, excelling in extreme and heterogeneous noise
scenarios. Moreover, FedGSCA demonstrates significant advantages in improving
model stability and handling complex noise, making it well-suited for
real-world medical federated learning scenarios.

</details>


### [160] [Sub-Scaling Laws: On the Role of Data Density and Training Strategies in LLMs](https://arxiv.org/abs/2507.10613)
*Zhengyu Chen,Siqi Wang,Teng Xiao,Yudong Wang,Shiqi Chen,Xunliang Cai,Junxian He,Jingang Wang*

Main category: cs.LG

TL;DR: The paper revisits scaling laws in NLP, finding that data quality and training strategies impact performance, with high data density and poor resource allocation causing sub-scaling. A new scaling law is proposed to address this.


<details>
  <summary>Details</summary>
Motivation: To understand deviations from traditional scaling laws in large language models, particularly the sub-scaling phenomenon, and identify contributing factors.

Method: Empirical analysis of over 400 models to study the effects of data quality and training strategies on performance.

Result: High data density and non-optimal resource allocation are key causes of sub-scaling, leading to diminishing returns.

Conclusion: A sub-optimal scaling law is proposed to better predict performance in sub-scaling regimes, emphasizing data quality and diversity.

Abstract: Traditional scaling laws in natural language processing suggest that
increasing model size and training data enhances performance. However, recent
studies reveal deviations, particularly in large language models, where
performance improvements decelerate, which is a phenomenon known as
sub-scaling. This paper revisits these scaling laws by examining the impact of
data quality and training strategies on model performance. Through extensive
empirical analysis of over 400 models, we identify high data density and
non-optimal resource allocation as key factors contributing to sub-scaling.
High data density leads to diminishing returns due to redundant information,
while optimal resource allocation is crucial for sustained performance
improvements. We propose a sub-optimal scaling law that better predicts
performance in sub-scaling regimes, highlighting the importance of data quality
and diversity.

</details>


### [161] [Fine-tuning Large Language Model for Automated Algorithm Design](https://arxiv.org/abs/2507.10614)
*Fei Liu,Rui Zhang,Xi Lin,Zhichao Lu,Qingfu Zhang*

Main category: cs.LG

TL;DR: The paper explores fine-tuning LLMs for algorithm design, introducing a Diversity-Aware Rank-based sampling strategy and direct preference optimization to enhance performance and generalization.


<details>
  <summary>Details</summary>
Motivation: To determine if LLMs specifically tailored for algorithm design are needed and how to obtain them, given the limitations of general-purpose LLMs.

Method: Uses DAR sampling for balanced training data and direct preference optimization to align LLM outputs with task objectives, tested on Llama models.

Result: Fine-tuned LLMs outperform general-purpose ones, with smaller models matching larger ones in some tasks, showing promising generalization.

Conclusion: Task-specific adaptation of LLMs is valuable for algorithm design, opening new research directions.

Abstract: The integration of large language models (LLMs) into automated algorithm
design has shown promising potential. A prevalent approach embeds LLMs within
search routines to iteratively generate and refine candidate algorithms.
However, most existing methods rely on off-the-shelf LLMs trained for general
coding tasks,leaving a key question open: Do we need LLMs specifically tailored
for algorithm design? If so, how can such LLMs be effectively obtained and how
well can they generalize across different algorithm design tasks? In this
paper, we take a first step toward answering these questions by exploring
fine-tuning of LLMs for algorithm design. We introduce a Diversity-Aware Rank
based (DAR) sampling strategy to balance training data diversity and quality,
then we leverage direct preference optimization to efficiently align LLM
outputs with task objectives. Our experiments, conducted on
Llama-3.2-1B-Instruct and Llama- 3.1-8B-Instruct, span three distinct algorithm
design tasks. Results suggest that finetuned LLMs can significantly outperform
their off-the-shelf counterparts with the smaller Llama-3.2-1B-Instruct and
match the larger Llama-3.1-8B-Instruct on the admissible set problem. Moreover,
we observe promising generalization: LLMs finetuned on specific algorithm
design tasks also improve performance on related tasks with varying settings.
These findings highlight the value of task-specific adaptation for LLMs in
algorithm design and open new avenues for future research.

</details>


### [162] [Scalpel vs. Hammer: GRPO Amplifies Existing Capabilities, SFT Replaces Them](https://arxiv.org/abs/2507.10616)
*Neel Rajani,Aryo Pradipta Gema,Seraphina Goldfarb-Tarrant,Ivan Titov*

Main category: cs.LG

TL;DR: Comparative analysis of RL and SFT for LLM reasoning training shows RL has minor in-domain gains, while SFT causes more pronounced changes and out-of-domain degradation. Freezing parts of the model yields inconclusive results.


<details>
  <summary>Details</summary>
Motivation: To understand the training dynamics of RL and SFT for LLM reasoning, as their effects are poorly understood.

Method: Comparative analysis of RL and SFT on the same maths problems with the same model and similar hyperparameters, including parameter updates and freezing experiments.

Result: RL shows minor in-domain gains and slight degradation on knowledge benchmarks, while SFT has more pronounced effects and affects mid-layer MLPs more. Freezing parts of the model yields mixed results.

Conclusion: RL amplifies existing capabilities, while SFT replaces old skills with new ones, though freezing parts of the model does not consistently mitigate degradation.

Abstract: Training large language models (LLMs) for reasoning via maths and code
datasets has become a major new focus in LLM post-training. Two particularly
popular approaches are reinforcement learning (RL) and supervised fine-tuning
(SFT), but their training dynamics are poorly understood. We present a
comparative analysis of RL and SFT on the same maths problems with the same
model and similar hyperparameters. We find that RL yields minor in-domain gains
on maths and slight degradation on knowledge-intensive benchmarks like MMLU,
while both trends are more pronounced in SFT. We also analyse model parameters
across checkpoints, observing that both algorithms modify query and key weights
the most. Meanwhile, SFT exhibits greater updates and also affects mid-layer
MLPs more, leading us to hypothesise that this may have caused the
out-of-domain degradation. We therefore investigate whether freezing parts of
the model during training can mitigate the reduced performance on
knowledge-intensive benchmarks. However, our results are inconclusive, with
benefits on GPQA:Diamond and degradation on other benchmarks. Taken together,
our observations provide a preliminary indication for why RL amplifies existing
capabilities, while SFT replaces old skills with new ones.

</details>


### [163] [Compute Requirements for Algorithmic Innovation in Frontier AI Models](https://arxiv.org/abs/2507.10618)
*Peter Barnett*

Main category: cs.LG

TL;DR: The paper investigates compute requirements for algorithmic innovations in large language models, finding that compute caps alone may not significantly slow AI progress.


<details>
  <summary>Details</summary>
Motivation: To understand the compute resources needed for algorithmic innovations in pretraining large language models and assess the impact of compute caps on innovation.

Method: Catalog 36 pretraining algorithmic innovations from Llama 3 and DeepSeek-V3, estimating development FLOP and hardware FLOP/s. Analyze the effect of compute caps on innovation.

Result: Compute requirements for innovations double yearly, but even stringent caps (e.g., GPT-2's compute or 8 H100 GPUs) could allow half of the innovations.

Conclusion: Compute caps alone are unlikely to dramatically slow AI algorithmic progress, as many innovations can still occur under restrictive conditions.

Abstract: Algorithmic innovation in the pretraining of large language models has driven
a massive reduction in the total compute required to reach a given level of
capability. In this paper we empirically investigate the compute requirements
for developing algorithmic innovations. We catalog 36 pre-training algorithmic
innovations used in Llama 3 and DeepSeek-V3. For each innovation we estimate
both the total FLOP used in development and the FLOP/s of the hardware
utilized. Innovations using significant resources double in their requirements
each year. We then use this dataset to investigate the effect of compute caps
on innovation. Our analysis suggests that compute caps alone are unlikely to
dramatically slow AI algorithmic progress. Even stringent compute caps -- such
as capping total operations to the compute used to train GPT-2 or capping
hardware capacity to 8 H100 GPUs -- could still have allowed for half of the
cataloged innovations.

</details>


### [164] [Meta-Reinforcement Learning for Fast and Data-Efficient Spectrum Allocation in Dynamic Wireless Networks](https://arxiv.org/abs/2507.10619)
*Oluwaseyi Giwa,Tobi Awodunmila,Muhammad Ahmed Mohsin,Ahsan Bilal,Muhammad Ali Jamshed*

Main category: cs.LG

TL;DR: A meta-learning framework is proposed for dynamic spectrum allocation in 5G/6G networks, outperforming traditional DRL methods in throughput, safety, and adaptability.


<details>
  <summary>Details</summary>
Motivation: Traditional DRL methods are inefficient and unsafe for spectrum allocation due to high sample complexity and unguided exploration risks.

Method: Three meta-learning architectures (MAML, RNN, attention-enhanced RNN) are implemented and compared to PPO in a simulated IAB environment.

Result: Attention-based meta-learning achieves 48 Mbps peak throughput (vs. PPO's 10 Mbps), reduces SINR/latency violations by 50%, and shows quick adaptation with a fairness index of 0.7.

Conclusion: Meta-learning is effective and safer for intelligent control in wireless systems, offering superior performance and adaptability.

Abstract: The dynamic allocation of spectrum in 5G / 6G networks is critical to
efficient resource utilization. However, applying traditional deep
reinforcement learning (DRL) is often infeasible due to its immense sample
complexity and the safety risks associated with unguided exploration, which can
cause severe network interference. To address these challenges, we propose a
meta-learning framework that enables agents to learn a robust initial policy
and rapidly adapt to new wireless scenarios with minimal data. We implement
three meta-learning architectures, model-agnostic meta-learning (MAML),
recurrent neural network (RNN), and an attention-enhanced RNN, and evaluate
them against a non-meta-learning DRL algorithm, proximal policy optimization
(PPO) baseline, in a simulated dynamic integrated access/backhaul (IAB)
environment. Our results show a clear performance gap. The attention-based
meta-learning agent reaches a peak mean network throughput of 48 Mbps, while
the PPO baseline decreased drastically to 10 Mbps. Furthermore, our method
reduces SINR and latency violations by more than 50% compared to PPO. It also
shows quick adaptation, with a fairness index 0.7, showing better resource
allocation. This work proves that meta-learning is a very effective and safer
option for intelligent control in complex wireless systems.

</details>


### [165] [LLMs Meet Cross-Modal Time Series Analytics: Overview and Directions](https://arxiv.org/abs/2507.10620)
*Chenxi Liu,Hao Miao,Cheng Long,Yan Zhao,Ziyue Li,Panos Kalnis*

Main category: cs.LG

TL;DR: The paper provides an overview of LLM-based cross-modal time series analytics, classifying approaches into conversion, alignment, and fusion, and discusses applications and open challenges.


<details>
  <summary>Details</summary>
Motivation: LLMs are promising for time series analytics but face a cross-modality gap due to their text-based pre-training. This tutorial aims to bridge this gap and expand their practical use.

Method: Introduces a taxonomy of cross-modal modeling strategies (conversion, alignment, fusion) and reviews their applications in downstream tasks.

Result: Summarizes current advancements and methodologies, highlighting open challenges in cross-modal time series analytics.

Conclusion: The tutorial aims to enhance LLM applications in real-world time series problems, balancing effectiveness and efficiency, and guides future research directions.

Abstract: Large Language Models (LLMs) have emerged as a promising paradigm for time
series analytics, leveraging their massive parameters and the shared sequential
nature of textual and time series data. However, a cross-modality gap exists
between time series and textual data, as LLMs are pre-trained on textual
corpora and are not inherently optimized for time series. In this tutorial, we
provide an up-to-date overview of LLM-based cross-modal time series analytics.
We introduce a taxonomy that classifies existing approaches into three groups
based on cross-modal modeling strategies, e.g., conversion, alignment, and
fusion, and then discuss their applications across a range of downstream tasks.
In addition, we summarize several open challenges. This tutorial aims to expand
the practical application of LLMs in solving real-world problems in cross-modal
time series analytics while balancing effectiveness and efficiency.
Participants will gain a thorough understanding of current advancements,
methodologies, and future research directions in cross-modal time series
analytics.

</details>


### [166] [Flows and Diffusions on the Neural Manifold](https://arxiv.org/abs/2507.10623)
*Daniel Saragih,Deyu Cao,Tejas Balaji*

Main category: cs.LG

TL;DR: The paper extends diffusion and flow-based generative models to weight space learning, using gradient flow matching to treat optimization paths as inductive bias, and demonstrates improved performance in weight generation, initialization, and safety-critical applications.


<details>
  <summary>Details</summary>
Motivation: To leverage the success of diffusion and flow-based generative models in domains like image synthesis and NLP for weight space learning, incorporating structural priors from optimization dynamics.

Method: Models gradient descent trajectories as a trajectory inference problem, unifying techniques under gradient flow matching. Includes reward fine-tuning, autoencoders for latent weight representation, task-specific conditioning, and informative source distributions.

Result: Matches or surpasses baselines in generating in-distribution weights, improves downstream training initialization, and excels in detecting harmful covariate shifts.

Conclusion: The method effectively applies generative modeling to weight space learning, offering practical benefits in performance and safety-critical applications.

Abstract: Diffusion and flow-based generative models have achieved remarkable success
in domains such as image synthesis, video generation, and natural language
modeling. In this work, we extend these advances to weight space learning by
leveraging recent techniques to incorporate structural priors derived from
optimization dynamics. Central to our approach is modeling the trajectory
induced by gradient descent as a trajectory inference problem. We unify several
trajectory inference techniques under the framework of gradient flow matching,
providing a theoretical framework for treating optimization paths as inductive
bias. We further explore architectural and algorithmic choices, including
reward fine-tuning by adjoint matching, the use of autoencoders for latent
weight representation, conditioning on task-specific context data, and adopting
informative source distributions such as Kaiming uniform. Experiments
demonstrate that our method matches or surpasses baselines in generating
in-distribution weights, improves initialization for downstream training, and
supports fine-tuning to enhance performance. Finally, we illustrate a practical
application in safety-critical systems: detecting harmful covariate shifts,
where our method outperforms the closest comparable baseline.

</details>


### [167] [Player-Team Heterogeneous Interaction Graph Transformer for Soccer Outcome Prediction](https://arxiv.org/abs/2507.10626)
*Lintao Wang,Shiwen Xu,Michael Horton,Joachim Gudmundsson,Zhiyong Wang*

Main category: cs.LG

TL;DR: HIGFormer, a graph-augmented transformer model, improves soccer match outcome prediction by capturing player and team interactions, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods overlook heterogeneous player and team interactions, crucial for accurate soccer outcome prediction.

Method: HIGFormer uses a multi-level framework: Player Interaction Network (heterogeneous graphs), Team Interaction Network (team graphs), and Match Comparison Transformer (joint analysis).

Result: HIGFormer outperforms existing methods on the WyScout dataset and aids in player performance evaluation.

Conclusion: HIGFormer offers a robust solution for soccer prediction and insights for talent scouting and strategy analysis.

Abstract: Predicting soccer match outcomes is a challenging task due to the inherently
unpredictable nature of the game and the numerous dynamic factors influencing
results. While it conventionally relies on meticulous feature engineering, deep
learning techniques have recently shown a great promise in learning effective
player and team representations directly for soccer outcome prediction.
However, existing methods often overlook the heterogeneous nature of
interactions among players and teams, which is crucial for accurately modeling
match dynamics. To address this gap, we propose HIGFormer (Heterogeneous
Interaction Graph Transformer), a novel graph-augmented transformer-based deep
learning model for soccer outcome prediction. HIGFormer introduces a
multi-level interaction framework that captures both fine-grained player
dynamics and high-level team interactions. Specifically, it comprises (1) a
Player Interaction Network, which encodes player performance through
heterogeneous interaction graphs, combining local graph convolutions with a
global graph-augmented transformer; (2) a Team Interaction Network, which
constructs interaction graphs from a team-to-team perspective to model
historical match relationships; and (3) a Match Comparison Transformer, which
jointly analyzes both team and player-level information to predict match
outcomes. Extensive experiments on the WyScout Open Access Dataset, a
large-scale real-world soccer dataset, demonstrate that HIGFormer significantly
outperforms existing methods in prediction accuracy. Furthermore, we provide
valuable insights into leveraging our model for player performance evaluation,
offering a new perspective on talent scouting and team strategy analysis.

</details>


### [168] [GHPO: Adaptive Guidance for Stable and Efficient LLM Reinforcement Learning](https://arxiv.org/abs/2507.10628)
*Ziru Liu,Cheng Gong,Xinyu Fu,Yaofang Liu,Ran Chen,Shoubo Hu,Suiyun Zhang,Rui Liu,Qingfu Zhang,Dandan Tu*

Main category: cs.LG

TL;DR: RLVR improves LLMs but faces training instability. GHPO introduces adaptive difficulty calibration, balancing imitation and exploration, achieving 5% performance gain on math tasks.


<details>
  <summary>Details</summary>
Motivation: Address training instability and inefficiency in RLVR for LLMs, especially smaller models, due to capacity-difficulty mismatch.

Method: Proposes Guided Hybrid Policy Optimization (GHPO), using adaptive prompt refinement to balance imitation learning and exploration.

Result: 5% average performance gain on six math benchmarks, outperforming baselines in stability and reasoning.

Conclusion: GHPO offers a scalable, efficient solution for robust reasoning models.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as
a powerful paradigm for facilitating the self-improvement of large language
models (LLMs), particularly in the domain of complex reasoning tasks. However,
prevailing on-policy RL methods often contend with significant training
instability and inefficiency. This is primarily due to a capacity-difficulty
mismatch, where the complexity of training data frequently outpaces the model's
current capabilities, leading to critically sparse reward signals and stalled
learning progress. This challenge is particularly acute for smaller, more
resource-efficient LLMs. To overcome this, we introduce the Guided Hybrid
Policy Optimization (GHPO), a novel difficulty-aware reinforcement learning
framework. GHPO dynamically calibrates task difficulty by employing adaptive
prompt refinement to provide targeted guidance. This unique approach adaptively
balances direct imitation learning for problems currently beyond the model's
reach with exploration-based reinforcement learning for more manageable tasks,
effectively creating a smooth and optimized learning curriculum. Extensive
experiments demonstrate that GHPO achieves an average performance gain of
approximately 5% across six challenging mathematics benchmarks, consistently
outperforming strong on-policy reinforcement learning and curriculum learning
baselines. Further analysis confirms that our framework significantly enhances
both training stability and final reasoning performance, thus offering a
scalable and efficient solution for developing powerful and robust reasoning
models.

</details>


### [169] [Scalable Unsupervised Segmentation via Random Fourier Feature-based Gaussian Process](https://arxiv.org/abs/2507.10632)
*Issei Saito,Masatoshi Nagano,Tomoaki Nakamura,Daichi Mochihashi,Koki Mimura*

Main category: cs.LG

TL;DR: RFF-GP-HSMM is a fast unsupervised time-series segmentation method using random Fourier features to reduce computational costs of GP-HSMM, achieving comparable performance with 278x speedup.


<details>
  <summary>Details</summary>
Motivation: The high computational cost of GP-HSMM due to kernel matrix inversion limits scalability for large datasets.

Method: Approximates Gaussian process with linear regression using random Fourier features (RFF), avoiding kernel matrix inversion.

Result: Achieves comparable segmentation performance to conventional methods with a 278x speedup on large datasets (e.g., 39,200 frames).

Conclusion: RFF-GP-HSMM offers a scalable and efficient alternative to GP-HSMM for time-series segmentation without sacrificing performance.

Abstract: In this paper, we propose RFF-GP-HSMM, a fast unsupervised time-series
segmentation method that incorporates random Fourier features (RFF) to address
the high computational cost of the Gaussian process hidden semi-Markov model
(GP-HSMM). GP-HSMM models time-series data using Gaussian processes, requiring
inversion of an N times N kernel matrix during training, where N is the number
of data points. As the scale of the data increases, matrix inversion incurs a
significant computational cost. To address this, the proposed method
approximates the Gaussian process with linear regression using RFF, preserving
expressive power while eliminating the need for inversion of the kernel matrix.
Experiments on the Carnegie Mellon University (CMU) motion-capture dataset
demonstrate that the proposed method achieves segmentation performance
comparable to that of conventional methods, with approximately 278 times faster
segmentation on time-series data comprising 39,200 frames.

</details>


### [170] [GeoHopNet: Hopfield-Augmented Sparse Spatial Attention for Dynamic UAV Site Location Problem](https://arxiv.org/abs/2507.10636)
*Jianing Zhi,Xinghua Li,Zidong Chen*

Main category: cs.LG

TL;DR: GeoHopNet is a novel Hopfield-augmented sparse spatial attention network for dynamic UAV site selection, addressing computational bottlenecks of traditional methods with innovations like distance-biased attention and K-nearest neighbor sparse attention.


<details>
  <summary>Details</summary>
Motivation: The rapid growth of the UAV economy demands efficient solutions for dynamic site selection, but traditional deep reinforcement learning methods struggle with computational complexity in large-scale urban scenarios.

Method: GeoHopNet introduces four innovations: distance-biased multi-head attention, K-nearest neighbor sparse attention, a Hopfield external memory module, and memory regularization, reducing complexity from O(N²) to O(NK).

Result: GeoHopNet solves large-scale problems (1,000 nodes) in under 0.1 seconds with a 0.22% optimality gap, outperforming baselines in speed (1.8× faster) and quality (22.2% improvement).

Conclusion: GeoHopNet significantly advances dynamic UAV site selection by combining efficiency and accuracy, making it suitable for large-scale urban applications.

Abstract: The rapid development of urban low-altitude unmanned aerial vehicle (UAV)
economy poses new challenges for dynamic site selection of UAV landing points
and supply stations. Traditional deep reinforcement learning methods face
computational complexity bottlenecks, particularly with standard attention
mechanisms, when handling large-scale urban-level location problems. This paper
proposes GeoHopNet, a Hopfield-augmented sparse spatial attention network
specifically designed for dynamic UAV site location problems. Our approach
introduces four core innovations: (1) distance-biased multi-head attention
mechanism that explicitly encodes spatial geometric information; (2) K-nearest
neighbor sparse attention that reduces computational complexity from $O(N^2)$
to $O(NK)$; (3) a modern Hopfield external memory module; and (4) a memory
regularization strategy. Experimental results demonstrate that GeoHopNet
extends the boundary of solvable problem sizes. For large-scale instances with
1,000 nodes, where standard attention models become prohibitively slow (over 3
seconds per instance) and traditional solvers fail, GeoHopNet finds
high-quality solutions (0.22\% optimality gap) in under 0.1 seconds. Compared
to the state-of-the-art ADNet baseline on 100-node instances, our method
improves solution quality by 22.2\% and is 1.8$\times$ faster.

</details>


### [171] [A Simple Baseline for Stable and Plastic Neural Networks](https://arxiv.org/abs/2507.10637)
*É. Künzel,A. Jaziri,V. Ramesh*

Main category: cs.LG

TL;DR: RDBP introduces ReLUDown and Decreasing Backpropagation to balance plasticity and stability in continual learning, outperforming state-of-the-art methods with lower computational cost.


<details>
  <summary>Details</summary>
Motivation: Existing continual learning approaches often imbalance plasticity and stability, necessitating a simpler, efficient solution.

Method: Combines ReLUDown (activation modification) and Decreasing Backpropagation (gradient-scheduling) to prevent neuron dormancy and shield early layers.

Result: Matches or exceeds state-of-the-art performance on Continual ImageNet with reduced computational cost.

Conclusion: RDBP is a practical, efficient benchmark for future continual learning strategies.

Abstract: Continual learning in computer vision requires that models adapt to a
continuous stream of tasks without forgetting prior knowledge, yet existing
approaches often tip the balance heavily toward either plasticity or stability.
We introduce RDBP, a simple, low-overhead baseline that unites two
complementary mechanisms: ReLUDown, a lightweight activation modification that
preserves feature sensitivity while preventing neuron dormancy, and Decreasing
Backpropagation, a biologically inspired gradient-scheduling scheme that
progressively shields early layers from catastrophic updates. Evaluated on the
Continual ImageNet benchmark, RDBP matches or exceeds the plasticity and
stability of state-of-the-art methods while reducing computational cost. RDBP
thus provides both a practical solution for real-world continual learning and a
clear benchmark against which future continual learning strategies can be
measured.

</details>


### [172] [ZClassifier: Temperature Tuning and Manifold Approximation via KL Divergence on Logit Space](https://arxiv.org/abs/2507.10638)
*Shim Soon Yong*

Main category: cs.LG

TL;DR: ZClassifier replaces deterministic logits with Gaussian-distributed logits, unifying uncertainty calibration and latent control via KL divergence minimization, improving robustness and calibration.


<details>
  <summary>Details</summary>
Motivation: To address temperature scaling and manifold approximation in classification while unifying uncertainty calibration and latent control.

Method: Uses diagonal Gaussian-distributed logits and minimizes KL divergence between predicted Gaussians and a unit isotropic Gaussian.

Result: Outperforms softmax classifiers in robustness, calibration, and latent separation on CIFAR-10 and CIFAR-100.

Conclusion: ZClassifier provides a principled probabilistic framework for classification and classifier-guided generation, enhancing interpretability and performance.

Abstract: We introduce a novel classification framework, ZClassifier, that replaces
conventional deterministic logits with diagonal Gaussian-distributed logits.
Our method simultaneously addresses temperature scaling and manifold
approximation by minimizing the Kullback-Leibler (KL) divergence between the
predicted Gaussian distributions and a unit isotropic Gaussian. This unifies
uncertainty calibration and latent control in a principled probabilistic
manner, enabling a natural interpretation of class confidence and geometric
consistency. Experiments on CIFAR-10 and CIFAR-100 show that ZClassifier
improves over softmax classifiers in robustness, calibration, and latent
separation. We also demonstrate its effectiveness for classifier-guided
generation by interpreting logits as Gaussian semantic potentials.

</details>


### [173] [First-of-its-kind AI model for bioacoustic detection using a lightweight associative memory Hopfield neural network](https://arxiv.org/abs/2507.10642)
*Andrew Gascoyne,Wendy Lomas*

Main category: cs.LG

TL;DR: An AI model using Hopfield neural networks for bioacoustic analysis is proposed, addressing data scarcity, environmental impact, and hardware demands. It’s fast, lightweight, and accurate.


<details>
  <summary>Details</summary>
Motivation: To tackle challenges in bioacoustic analysis like limited training data, high energy consumption, and hardware requirements.

Method: Uses associative memory via a transparent Hopfield neural network, requiring minimal training data (one signal per target sound).

Result: Achieves 86% precision, processes 10384 bat recordings in 5.4s, uses only 144.09MB RAM, and matches expert manual identification.

Conclusion: The model is a sustainable, efficient, and accurate solution for bioacoustic analysis, suitable for deployment on standard devices.

Abstract: A growing issue within conservation bioacoustics is the task of analysing the
vast amount of data generated from the use of passive acoustic monitoring
devices. In this paper, we present an alternative AI model which has the
potential to help alleviate this problem. Our model formulation addresses the
key issues encountered when using current AI models for bioacoustic analysis,
namely the: limited training data available; environmental impact, particularly
in energy consumption and carbon footprint of training and implementing these
models; and associated hardware requirements. The model developed in this work
uses associative memory via a transparent, explainable Hopfield neural network
to store signals and detect similar signals which can then be used to classify
species. Training is rapid ($3$\,ms), as only one representative signal is
required for each target sound within a dataset. The model is fast, taking only
$5.4$\,s to pre-process and classify all $10384$ publicly available bat
recordings, on a standard Apple MacBook Air. The model is also lightweight with
a small memory footprint of $144.09$\,MB of RAM usage. Hence, the low
computational demands make the model ideal for use on a variety of standard
personal devices with potential for deployment in the field via edge-processing
devices. It is also competitively accurate, with up to $86\%$ precision on the
dataset used to evaluate the model. In fact, we could not find a single case of
disagreement between model and manual identification via expert field guides.
Although a dataset of bat echolocation calls was chosen to demo this
first-of-its-kind AI model, trained on only two representative calls, the model
is not species specific. In conclusion, we propose an equitable AI model that
has the potential to be a game changer for fast, lightweight, sustainable,
transparent, explainable and accurate bioacoustic analysis.

</details>


### [174] [A Group Theoretic Analysis of the Symmetries Underlying Base Addition and Their Learnability by Neural Networks](https://arxiv.org/abs/2507.10678)
*Cutter Dawes,Simon Segert,Kamesh Krishnamurthy,Jonathan D. Cohen*

Main category: cs.LG

TL;DR: The paper explores how neural networks can achieve radical generalization by learning symmetry functions, using base addition as a case study. It analyzes alternative carry functions, measures their properties, and tests neural networks' learning efficacy with different carries.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of designing neural networks capable of radical generalization by leveraging symmetry functions, particularly in the context of base addition.

Method: Group theoretic analysis of base addition, introduction of alternative carry functions, and training neural networks with different carries to study learning efficacy.

Result: Simple neural networks can achieve radical generalization with appropriate input format and carry function, with learning speed tied to carry function structure.

Conclusion: The findings highlight the importance of symmetry and carry function structure in neural network learning, with implications for cognitive science and machine learning.

Abstract: A major challenge in the use of neural networks both for modeling human
cognitive function and for artificial intelligence is the design of systems
with the capacity to efficiently learn functions that support radical
generalization. At the roots of this is the capacity to discover and implement
symmetry functions. In this paper, we investigate a paradigmatic example of
radical generalization through the use of symmetry: base addition. We present a
group theoretic analysis of base addition, a fundamental and defining
characteristic of which is the carry function -- the transfer of the remainder,
when a sum exceeds the base modulus, to the next significant place. Our
analysis exposes a range of alternative carry functions for a given base, and
we introduce quantitative measures to characterize these. We then exploit
differences in carry functions to probe the inductive biases of neural networks
in symmetry learning, by training neural networks to carry out base addition
using different carries, and comparing efficacy and rate of learning as a
function of their structure. We find that even simple neural networks can
achieve radical generalization with the right input format and carry function,
and that learning speed is closely correlated with carry function structure. We
then discuss the relevance this has for cognitive science and machine learning.

</details>


### [175] [A Simple Approximate Bayesian Inference Neural Surrogate for Stochastic Petri Net Models](https://arxiv.org/abs/2507.10714)
*Bright Kwaku Manu,Trevor Reckell,Beckett Sterner,Petar Jevtic*

Main category: cs.LG

TL;DR: A neural-surrogate framework using a 1D Convolutional Residual Network is introduced for parameter estimation in Stochastic Petri Nets (SPNs) with covariate-dependent rates, outperforming traditional Bayesian methods in accuracy and speed.


<details>
  <summary>Details</summary>
Motivation: Parameter estimation in SPNs is challenging, especially with covariate-dependent rates and unavailable explicit likelihoods, necessitating a robust, data-driven solution.

Method: A lightweight 1D Convolutional Residual Network is trained on Gillespie-simulated SPN data to predict rate-function coefficients from noisy, partially observed trajectories, using Monte Carlo dropout for uncertainty.

Result: The surrogate achieves RMSE = 0.108 on synthetic SPNs with 20% missing events and runs faster than Bayesian methods.

Conclusion: Neural surrogates enable accurate, real-time parameter recovery in complex, partially observed discrete-event systems.

Abstract: Stochastic Petri Nets (SPNs) are an increasingly popular tool of choice for
modeling discrete-event dynamics in areas such as epidemiology and systems
biology, yet their parameter estimation remains challenging in general and in
particular when transition rates depend on external covariates and explicit
likelihoods are unavailable. We introduce a neural-surrogate
(neural-network--based approximation of the posterior distribution) framework
that predicts the coefficients of known covariate-dependent rate functions
directly from noisy, partially observed token trajectories. Our model employs a
lightweight 1D Convolutional Residual Network trained end-to-end on
Gillespie-simulated SPN realizations, learning to invert system dynamics under
realistic conditions of event dropout. During inference, Monte Carlo dropout
provides calibrated uncertainty bounds together with point estimates. On
synthetic SPNs with 20% missing events, our surrogate recovers rate-function
coefficients with an RMSE = 0.108 and substantially runs faster than
traditional Bayesian approaches. These results demonstrate that data-driven,
likelihood-free surrogates can enable accurate, robust, and real-time parameter
recovery in complex, partially observed discrete-event systems.

</details>


### [176] [Distributionally Robust Optimization with Adversarial Data Contamination](https://arxiv.org/abs/2507.10718)
*Shuyao Li,Ilias Diakonikolas,Jelena Diakonikolas*

Main category: cs.LG

TL;DR: The paper introduces a framework combining robustness against data contamination and distributional shifts in Wasserstein-1 DRO for generalized linear models, with an efficient algorithm and proven error bounds.


<details>
  <summary>Details</summary>
Motivation: Address the dual challenges of outliers in training data and distributional uncertainty in DRO, ensuring robust decision-making.

Method: Proposes a modeling framework integrating robustness against data corruption and distributional shifts, with an efficient algorithm inspired by robust statistics.

Result: Achieves an estimation error of $O(\sqrt{\epsilon})$ for the true DRO objective using contaminated data under bounded covariance.

Conclusion: First rigorous guarantees for learning under data contamination and distributional shifts, with efficient computation.

Abstract: Distributionally Robust Optimization (DRO) provides a framework for
decision-making under distributional uncertainty, yet its effectiveness can be
compromised by outliers in the training data. This paper introduces a
principled approach to simultaneously address both challenges. We focus on
optimizing Wasserstein-1 DRO objectives for generalized linear models with
convex Lipschitz loss functions, where an $\epsilon$-fraction of the training
data is adversarially corrupted. Our primary contribution lies in a novel
modeling framework that integrates robustness against training data
contamination with robustness against distributional shifts, alongside an
efficient algorithm inspired by robust statistics to solve the resulting
optimization problem. We prove that our method achieves an estimation error of
$O(\sqrt{\epsilon})$ for the true DRO objective value using only the
contaminated data under the bounded covariance assumption. This work
establishes the first rigorous guarantees, supported by efficient computation,
for learning under the dual challenges of data contamination and distributional
shifts.

</details>


### [177] [Ground-Compose-Reinforce: Tasking Reinforcement Learning Agents through Formal Language](https://arxiv.org/abs/2507.10741)
*Andrew C. Li,Toryn Q. Klassen,Andrew Wang,Parand A. Alamdari,Sheila A. McIlraith*

Main category: cs.LG

TL;DR: A neurosymbolic framework, Ground-Compose-Reinforce, is proposed for grounding formal language in perception and action, enabling efficient learning and generalization with limited data.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of grounding language in complex perception and action without manual design or massive datasets.

Method: Uses a neurosymbolic framework combining formal language semantics and reinforcement learning to ground language and elicit behaviors.

Result: Achieves reliable mapping of formal language instructions to behaviors with limited data, outperforming end-to-end data-driven approaches.

Conclusion: The framework offers a scalable and efficient solution for language grounding in situated agents.

Abstract: Grounding language in complex perception (e.g. pixels) and action is a key
challenge when building situated agents that can interact with humans via
language. In past works, this is often solved via manual design of the language
grounding or by curating massive datasets relating language to elements of the
environment. We propose Ground-Compose-Reinforce, a neurosymbolic framework for
grounding formal language from data, and eliciting behaviours by directly
tasking RL agents through this language. By virtue of data-driven learning, our
framework avoids the manual design of domain-specific elements like reward
functions or symbol detectors. By virtue of compositional formal language
semantics, our framework achieves data-efficient grounding and generalization
to arbitrary language compositions. Experiments on an image-based gridworld and
a MuJoCo robotics domain show that our approach reliably maps formal language
instructions to behaviours with limited data while end-to-end, data-driven
approaches fail.

</details>


### [178] [A Benchmarking Framework for AI models in Automotive Aerodynamics](https://arxiv.org/abs/2507.10747)
*Kaustubh Tangsali,Rishikesh Ranade,Mohammad Amin Nabian,Alexey Kamenev,Peter Sharpe,Neil Ashton,Ram Cherukuri,Sanjay Choudhry*

Main category: cs.LG

TL;DR: A benchmarking framework in NVIDIA PhysicsNeMo-CFD is introduced to evaluate AI models for automotive aerodynamics, focusing on accuracy, performance, scalability, and generalization.


<details>
  <summary>Details</summary>
Motivation: To standardize and enhance the assessment of AI models in automotive aerodynamics, improving transparency and consistency for better model development.

Method: The framework uses standardized metrics to evaluate AI models (DoMINO, X-MeshGraphNet, FIGConvNet) on surface and volumetric flow predictions with the DrivAerML dataset.

Result: Demonstrates utility by evaluating models and providing guidelines for integrating additional models/datasets.

Conclusion: Aims to aid researchers and professionals in refining AI-driven aerodynamic models for more efficient, accurate, and interpretable solutions.

Abstract: In this paper, we introduce a benchmarking framework within the open-source
NVIDIA PhysicsNeMo-CFD framework designed to systematically assess the
accuracy, performance, scalability, and generalization capabilities of AI
models for automotive aerodynamics predictions. The open extensible framework
enables incorporation of a diverse set of metrics relevant to the
Computer-Aided Engineering (CAE) community. By providing a standardized
methodology for comparing AI models, the framework enhances transparency and
consistency in performance assessment, with the overarching goal of improving
the understanding and development of these models to accelerate research and
innovation in the field. To demonstrate its utility, the framework includes
evaluation of both surface and volumetric flow field predictions on three AI
models: DoMINO, X-MeshGraphNet, and FIGConvNet using the DrivAerML dataset. It
also includes guidelines for integrating additional models and datasets, making
it extensible for physically consistent metrics. This benchmarking study aims
to enable researchers and industry professionals in selecting, refining, and
advancing AI-driven aerodynamic modeling approaches, ultimately fostering the
development of more efficient, accurate, and interpretable solutions in
automotive aerodynamics

</details>


### [179] [Spatial Reasoners for Continuous Variables in Any Domain](https://arxiv.org/abs/2507.10768)
*Bart Pogodzinski,Christopher Wewer,Bernt Schiele,Jan Eric Lenssen*

Main category: cs.LG

TL;DR: Spatial Reasoners is a framework for spatial reasoning using generative denoising models, simplifying research with easy interfaces and open availability.


<details>
  <summary>Details</summary>
Motivation: The need for infrastructure to support generative reasoning with denoising models due to diverse formulations and high effort required.

Method: Provides interfaces for variable mapping, generative model paradigms, and inference strategies.

Result: A framework that facilitates research in spatial reasoning with generative models.

Conclusion: Spatial Reasoners is a valuable tool for advancing research in generative spatial reasoning.

Abstract: We present Spatial Reasoners, a software framework to perform spatial
reasoning over continuous variables with generative denoising models. Denoising
generative models have become the de-facto standard for image generation, due
to their effectiveness in sampling from complex, high-dimensional
distributions. Recently, they have started being explored in the context of
reasoning over multiple continuous variables. Providing infrastructure for
generative reasoning with such models requires a high effort, due to a wide
range of different denoising formulations, samplers, and inference strategies.
Our presented framework aims to facilitate research in this area, providing
easy-to-use interfaces to control variable mapping from arbitrary data domains,
generative model paradigms, and inference strategies. Spatial Reasoners are
openly available at https://spatialreasoners.github.io/

</details>


### [180] [A Generalizable Physics-Enhanced State Space Model for Long-Term Dynamics Forecasting in Complex Environments](https://arxiv.org/abs/2507.10792)
*Yuchen Wang,Hongjue Zhao,Haohong Lin,Enze Xu,Lifang He,Huajie Shao*

Main category: cs.LG

TL;DR: The paper proposes Phy-SSM, a method integrating partial physics knowledge into state space models for long-term dynamic forecasting in noisy, irregularly sampled environments. It outperforms baselines in real-world tasks like vehicle motion and COVID-19 forecasting.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of long-term forecasting in complex environments by leveraging state space models (SSMs) and partial physics knowledge for better generalization.

Method: Phy-SSM decomposes partially known system dynamics into known and unknown state matrices, integrating them into a unit with a physics state regularization term for alignment with system dynamics.

Result: Phy-SSM shows superior performance in long-term interpolation and extrapolation tasks across vehicle motion, drone state, and COVID-19 forecasting.

Conclusion: Phy-SSM effectively combines physics knowledge with SSMs, improving long-term forecasting in complex environments, as validated by real-world experiments.

Abstract: This work aims to address the problem of long-term dynamic forecasting in
complex environments where data are noisy and irregularly sampled. While recent
studies have introduced some methods to improve prediction performance, these
approaches still face a significant challenge in handling long-term
extrapolation tasks under such complex scenarios. To overcome this challenge,
we propose Phy-SSM, a generalizable method that integrates partial physics
knowledge into state space models (SSMs) for long-term dynamics forecasting in
complex environments. Our motivation is that SSMs can effectively capture
long-range dependencies in sequential data and model continuous dynamical
systems, while the incorporation of physics knowledge improves generalization
ability. The key challenge lies in how to seamlessly incorporate partially
known physics into SSMs. To achieve this, we decompose partially known system
dynamics into known and unknown state matrices, which are integrated into a
Phy-SSM unit. To further enhance long-term prediction performance, we introduce
a physics state regularization term to make the estimated latent states align
with system dynamics. Besides, we theoretically analyze the uniqueness of the
solutions for our method. Extensive experiments on three real-world
applications, including vehicle motion prediction, drone state prediction, and
COVID-19 epidemiology forecasting, demonstrate the superior performance of
Phy-SSM over the baselines in both long-term interpolation and extrapolation
tasks. The code is available at https://github.com/511205787/Phy_SSM-ICML2025.

</details>


### [181] [Multi-Armed Sampling Problem and the End of Exploration](https://arxiv.org/abs/2507.10797)
*Mohammad Pedramfar,Siamak Ravanbakhsh*

Main category: cs.LG

TL;DR: The paper introduces multi-armed sampling, a framework analogous to multi-armed bandits but for sampling. It explores the exploration-exploitation trade-off in sampling, defines regret notions, and proposes an optimal algorithm. Unlike optimization, sampling doesn't require exploration. The work connects sampling and bandit problems via a temperature parameter and has implications for neural samplers and RLHF.


<details>
  <summary>Details</summary>
Motivation: To rigorously study the exploration-exploitation trade-off in sampling, analogous to multi-armed bandits in optimization, and to establish foundational insights for neural samplers and RLHF.

Method: Defines regret notions for multi-armed sampling, establishes lower bounds, and proposes an optimal algorithm. Introduces a continuous family of problems unifying sampling and bandits via a temperature parameter.

Result: Theoretical results show sampling doesn't require exploration, unlike optimization. The proposed algorithm achieves optimal regret bounds.

Conclusion: The multi-armed sampling framework provides foundational insights for sampling problems, with implications for neural samplers, entropy-regularized RL, and RLHF, highlighting the role of exploration and convergence properties.

Abstract: This paper introduces the framework of multi-armed sampling, as the sampling
counterpart to the optimization problem of multi-arm bandits. Our primary
motivation is to rigorously examine the exploration-exploitation trade-off in
the context of sampling. We systematically define plausible notions of regret
for this framework and establish corresponding lower bounds. We then propose a
simple algorithm that achieves these optimal regret bounds. Our theoretical
results demonstrate that in contrast to optimization, sampling does not require
exploration. To further connect our findings with those of multi-armed bandits,
we define a continuous family of problems and associated regret measures that
smoothly interpolates and unifies multi-armed sampling and multi-armed bandit
problems using a temperature parameter. We believe the multi-armed sampling
framework, and our findings in this setting can have a foundational role in the
study of sampling including recent neural samplers, akin to the role of
multi-armed bandits in reinforcement learning. In particular, our work sheds
light on the need for exploration and the convergence properties of algorithm
for entropy-regularized reinforcement learning, fine-tuning of pretrained
models and reinforcement learning with human feedback (RLHF).

</details>


### [182] [Uncovering Causal Relation Shifts in Event Sequences under Out-of-Domain Interventions](https://arxiv.org/abs/2507.10809)
*Kazi Tasnim Zinat,Yun Zhou,Xiang Lyu,Yawei Wang,Zhicheng Liu,Panpan Xu*

Main category: cs.LG

TL;DR: A new causal framework is proposed to handle out-of-domain interventions in temporal event sequences, improving ATE estimation with a Transformer-based model.


<details>
  <summary>Details</summary>
Motivation: Existing causal inference methods ignore out-of-domain interventions, which can significantly alter causal dynamics in real-world settings.

Method: A new causal framework extends Rubin's ATE to non-i.i.d. data, with an unbiased estimator and a Transformer-based model for temporal dependencies and out-of-domain interventions.

Result: The method outperforms baselines in ATE estimation and goodness-of-fit on simulated and real-world datasets.

Conclusion: The proposed framework effectively captures causal shifts under out-of-domain interventions, enhancing causal inference in temporal processes.

Abstract: Inferring causal relationships between event pairs in a temporal sequence is
applicable in many domains such as healthcare, manufacturing, and
transportation. Most existing work on causal inference primarily focuses on
event types within the designated domain, without considering the impact of
exogenous out-of-domain interventions. In real-world settings, these
out-of-domain interventions can significantly alter causal dynamics. To address
this gap, we propose a new causal framework to define average treatment effect
(ATE), beyond independent and identically distributed (i.i.d.) data in classic
Rubin's causal framework, to capture the causal relation shift between events
of temporal process under out-of-domain intervention. We design an unbiased ATE
estimator, and devise a Transformer-based neural network model to handle both
long-range temporal dependencies and local patterns while integrating
out-of-domain intervention information into process modeling. Extensive
experiments on both simulated and real-world datasets demonstrate that our
method outperforms baselines in ATE estimation and goodness-of-fit under
out-of-domain-augmented point processes.

</details>


### [183] [Semantic Context for Tool Orchestration](https://arxiv.org/abs/2507.10820)
*Robert Müller*

Main category: cs.LG

TL;DR: Semantic Context (SC) is key for robust tool orchestration, validated theoretically and empirically, with a new pipeline (FiReAct) for large-scale tool use.


<details>
  <summary>Details</summary>
Motivation: To improve tool orchestration by leveraging Semantic Context for better adaptability and efficiency in dynamic environments.

Method: Theoretical foundation with SC-LinUCB (contextual bandits), empirical validation with LLMs, and the FiReAct pipeline for large-scale tool retrieval.

Result: SC-LinUCB reduces regret and adapts well; SC improves LLM performance; FiReAct enables effective orchestration over 10,000 tools.

Conclusion: SC is foundational for efficient, adaptive, and scalable tool orchestration, supported by theory and experiments.

Abstract: This paper demonstrates that Semantic Context (SC), leveraging descriptive
tool information, is a foundational component for robust tool orchestration.
Our contributions are threefold. First, we provide a theoretical foundation
using contextual bandits, introducing SC-LinUCB and proving it achieves lower
regret and adapts favourably in dynamic action spaces. Second, we provide
parallel empirical validation with Large Language Models, showing that SC is
critical for successful in-context learning in both static (efficient learning)
and non-stationary (robust adaptation) settings. Third, we propose the FiReAct
pipeline, and demonstrate on a benchmark with over 10,000 tools that SC-based
retrieval enables an LLM to effectively orchestrate over a large action space.
These findings provide a comprehensive guide to building more sample-efficient,
adaptive, and scalable orchestration agents.

</details>


### [184] [From Small to Large: A Graph Convolutional Network Approach for Solving Assortment Optimization Problems](https://arxiv.org/abs/2507.10834)
*Guokai Li,Pin Gao,Stefanus Jasin,Zizhuo Wang*

Main category: cs.LG

TL;DR: The paper proposes using Graph Convolutional Networks (GCNs) to solve constrained assortment optimization efficiently, achieving high performance (90%+ optimality) on large-scale instances by training on small-scale data.


<details>
  <summary>Details</summary>
Motivation: Assortment optimization is NP-hard and challenging due to its combinatorial and non-linear nature, requiring efficient solutions for practical applications.

Method: The authors develop a graph representation of the problem, train a GCN to learn optimal assortment patterns, and propose two inference policies based on GCN outputs.

Result: GCN-based policies achieve superior performance (90%+ optimality) on large-scale instances (up to 2,000 products) quickly, outperforming existing heuristics. The method also works in a model-free setting with transaction data.

Conclusion: GCNs offer an efficient and scalable solution for assortment optimization, even in model-free scenarios, demonstrating significant improvements over traditional methods.

Abstract: Assortment optimization involves selecting a subset of substitutable products
(subject to certain constraints) to maximize the expected revenue. It is a
classic problem in revenue management and finds applications across various
industries. However, the problem is usually NP-hard due to its combinatorial
and non-linear nature. In this work, we explore how graph concolutional
networks (GCNs) can be leveraged to efficiently solve constrained assortment
optimization under the mixed multinomial logit choice model. We first develop a
graph representation of the assortment problem, then train a GCN to learn the
patterns of optimal assortments, and lastly propose two inference policies
based on the GCN's output. Due to the GCN's inherent ability to generalize
across inputs of varying sizes, we can use a GCN trained on small-scale
instances to facilitate large-scale instances. Extensive numerical experiments
demonstrate that given a GCN trained on small-scale instances (e.g., with 20
products), the proposed policies can achieve superior performance (90%+
optimality) on large-scale instances (with up to 2,000 products) within
seconds, which outperform existing heuristic policies in both performance and
efficiency. Furthermore, we extend our framework to a model-free setting where
the underlying choice model is unknown but transaction data is available. We
also conduct numerical experiments to demonstrate the effectiveness and
efficiency of our proposed policies in this setting.

</details>


### [185] [Offline Reinforcement Learning with Wasserstein Regularization via Optimal Transport Maps](https://arxiv.org/abs/2507.10843)
*Motoki Omura,Yusuke Mukuta,Kazuki Ota,Takayuki Osa,Tatsuya Harada*

Main category: cs.LG

TL;DR: The paper proposes a Wasserstein distance-based method for offline RL to address distributional shift, using ICNNs for stable learning, and shows competitive performance on D4RL.


<details>
  <summary>Details</summary>
Motivation: Offline RL is useful where data collection is costly, but suffers from distributional shift. Existing methods use density ratio-based measures, but the authors propose a more robust alternative.

Method: Uses Wasserstein distance for regularization, modeled via ICNNs to compute optimal transport maps, avoiding adversarial training.

Result: Demonstrates comparable or superior performance to existing methods on the D4RL benchmark.

Conclusion: The proposed method effectively mitigates distributional shift in offline RL, offering stable and robust performance.

Abstract: Offline reinforcement learning (RL) aims to learn an optimal policy from a
static dataset, making it particularly valuable in scenarios where data
collection is costly, such as robotics. A major challenge in offline RL is
distributional shift, where the learned policy deviates from the dataset
distribution, potentially leading to unreliable out-of-distribution actions. To
mitigate this issue, regularization techniques have been employed. While many
existing methods utilize density ratio-based measures, such as the
$f$-divergence, for regularization, we propose an approach that utilizes the
Wasserstein distance, which is robust to out-of-distribution data and captures
the similarity between actions. Our method employs input-convex neural networks
(ICNNs) to model optimal transport maps, enabling the computation of the
Wasserstein distance in a discriminator-free manner, thereby avoiding
adversarial training and ensuring stable learning. Our approach demonstrates
comparable or superior performance to widely used existing methods on the D4RL
benchmark dataset. The code is available at
https://github.com/motokiomura/Q-DOT .

</details>


### [186] [Collaboration Promotes Group Resilience in Multi-Agent RL](https://arxiv.org/abs/2111.06614)
*Ilai Shraga,Guy Azran,Matthias Gerstgrasser,Ofir Abu,Jeffrey S. Rosenschein,Sarah Keren*

Main category: cs.LG

TL;DR: The paper introduces 'group resilience' in multi-agent RL, showing collaboration enhances resilience to environmental changes.


<details>
  <summary>Details</summary>
Motivation: To address the lack of resilience frameworks in multi-agent settings, focusing on how collaboration aids adaptation.

Method: Formalized group resilience, tested collaboration protocols in MARL, and compared collaborative vs. non-collaborative approaches.

Result: Collaborative protocols outperformed non-collaborative ones in achieving group resilience.

Conclusion: Collaboration is key to group resilience in multi-agent RL, with empirical validation supporting the hypothesis.

Abstract: To effectively operate in various dynamic scenarios, RL agents must be
resilient to unexpected changes in their environment. Previous work on this
form of resilience has focused on single-agent settings. In this work, we
introduce and formalize a multi-agent variant of resilience, which we term
group resilience. We further hypothesize that collaboration with other agents
is key to achieving group resilience; collaborating agents adapt better to
environmental perturbations in multi-agent reinforcement learning (MARL)
settings. We test our hypothesis empirically by evaluating different
collaboration protocols and examining their effect on group resilience. Our
experiments show that all the examined collaborative approaches achieve higher
group resilience than their non-collaborative counterparts.

</details>


### [187] [Visually grounded emotion regulation via diffusion models and user-driven reappraisal](https://arxiv.org/abs/2507.10861)
*Edoardo Pinzuti,Oliver Tüscher,André Ferreira Castro*

Main category: cs.LG

TL;DR: A novel visually based augmentation of cognitive reappraisal using text-to-image diffusion models significantly reduces negative affect, demonstrating the potential of generative AI in emotion regulation.


<details>
  <summary>Details</summary>
Motivation: Standard cognitive reappraisal interventions are cognitively demanding and rely on verbal processes, which are often impaired in individuals with trauma or depression, limiting their effectiveness.

Method: A system integrates stable diffusion models with a fine-tuned IP-adapter to transform spoken reappraisals into supportive visualizations, tested in a within-subject experiment (N=20) using a modified CER task.

Result: AI-assisted reappraisal significantly reduced negative affect compared to non-AI and control conditions, with sentiment alignment between reappraisals and images correlating with affective relief.

Conclusion: Generative visual input enhances cognitive reappraisal efficacy, offering new directions for integrating generative AI, affective computing, and therapeutic technology.

Abstract: Cognitive reappraisal is a key strategy in emotion regulation, involving
reinterpretation of emotionally charged stimuli to alter affective responses.
Despite its central role in clinical and cognitive science, real-world
reappraisal interventions remain cognitively demanding, abstract, and primarily
verbal. This reliance on higher-order cognitive and linguistic processes is
often impaired in individuals with trauma or depression, limiting the
effectiveness of standard approaches. Here, we propose a novel, visually based
augmentation of cognitive reappraisal by integrating large-scale text-to-image
diffusion models into the emotional regulation process. Specifically, we
introduce a system in which users reinterpret emotionally negative images via
spoken reappraisals, which are transformed into supportive, emotionally
congruent visualizations using stable diffusion models with a fine-tuned
IP-adapter. This generative transformation visually instantiates users'
reappraisals while maintaining structural similarity to the original stimuli,
externalizing and reinforcing regulatory intent. To test this approach, we
conducted a within-subject experiment (N = 20) using a modified cognitive
emotion regulation (CER) task. Participants reappraised or described aversive
images from the International Affective Picture System (IAPS), with or without
AI-generated visual feedback. Results show that AI-assisted reappraisal
significantly reduced negative affect compared to both non-AI and control
conditions. Further analyses reveal that sentiment alignment between
participant reappraisals and generated images correlates with affective relief,
suggesting that multimodal coherence enhances regulatory efficacy. These
findings demonstrate that generative visual input can support cogitive
reappraisal and open new directions at the intersection of generative AI,
affective computing, and therapeutic technology.

</details>


### [188] [GALDS: A Graph-Autoencoder-based Latent Dynamics Surrogate model to predict neurite material transport](https://arxiv.org/abs/2507.10871)
*Tsung Yeh Hsieh,Yongjie Jessica Zhang*

Main category: cs.LG

TL;DR: The paper introduces GALDS, a Graph-Autoencoder-based Latent Dynamics Surrogate model, to efficiently simulate material transport in neural trees, achieving high accuracy and speed.


<details>
  <summary>Details</summary>
Motivation: Accurate simulation of material transport in neuron networks is computationally challenging due to their complex tree-like structures, requiring optimization beyond traditional methods.

Method: GALDS uses a graph autoencoder to encode network geometry, velocity fields, and concentration profiles into latent representations, then predicts dynamics via a graph latent space model inspired by Neural ODEs.

Result: GALDS achieves a mean relative error of 3% (max <8%) and a 10-fold speed improvement over previous surrogate models on unseen and abnormal transport cases.

Conclusion: GALDS offers an efficient and accurate solution for simulating material transport in neural trees, addressing computational challenges with innovative graph-based and Neural ODE techniques.

Abstract: Neurons exhibit intricate geometries within their neurite networks, which
play a crucial role in processes such as signaling and nutrient transport.
Accurate simulation of material transport in the networks is essential for
understanding these biological phenomena but poses significant computational
challenges because of the complex tree-like structures involved. Traditional
approaches are time-intensive and resource-demanding, yet the inherent
properties of neuron trees, which consists primarily of pipes with steady-state
parabolic velocity profiles and bifurcations, provide opportunities for
computational optimization. To address these challenges, we propose a
Graph-Autoencoder-based Latent Dynamics Surrogate (GALDS) model, which is
specifically designed to streamline the simulation of material transport in
neural trees. GALDS employs a graph autoencoder to encode latent
representations of the network's geometry, velocity fields, and concentration
profiles. These latent space representations are then assembled into a global
graph, which is subsequently used to predict system dynamics in the latent
space via a trained graph latent space system dynamic model, inspired by the
Neural Ordinary Differential Equations (Neural ODEs) concept. The integration
of an autoencoder allows for the use of smaller graph neural network models
with reduced training data requirements. Furthermore, the Neural ODE component
effectively mitigates the issue of error accumulation commonly encountered in
recurrent neural networks. The effectiveness of the GALDS model is demonstrated
through results on eight unseen geometries and four abnormal transport
examples, where our approach achieves mean relative error of 3% with maximum
relative error <8% and demonstrates a 10-fold speed improvement compared to
previous surrogate model approaches.

</details>


### [189] [Domain-Adaptive Small Language Models for Structured Tax Code Prediction](https://arxiv.org/abs/2507.10880)
*Souvik Nath,Sumit Wadhwa,Luiz Perez*

Main category: cs.LG

TL;DR: A domain-adaptive small language model (SLM) with encoder-decoder architecture is proposed for accurate prediction of hierarchical tax codes like HSN or SAC, outperforming flat classifiers and other architectures.


<details>
  <summary>Details</summary>
Motivation: Multinational firms face challenges in accurately determining tax codes (e.g., HSN, SAC) due to varying regulations, risking penalties. A robust solution is needed.

Method: Uses an encoder-decoder SLM to predict hierarchical tax code sequences from unstructured product/service data, capturing dependencies.

Result: Encoder-decoder SLMs outperform flat classifiers and other architectures (decoder-only, encoder-only) in tax code prediction.

Conclusion: The approach is scalable to other tax codes (e.g., UNSPSC, NCM) and effective for structured sequence generation in tax compliance.

Abstract: Every day, multinational firms process thousands of transactions, each of
which must adhere to tax regulations that vary by jurisdiction and are often
nuanced. The determination of product and service tax codes, such as HSN or SAC
is a major use case in Tax compliance. An accurate determination of such codes
is imperative to avoid any tax penalties. This paper proposes a domain-adaptive
small language model (SLM) with an encoder-decoder architecture for the
enhanced prediction of product and service tax codes. In this approach, we
address the problem of predicting hierarchical tax code sequences using
unstructured product and services data. We employ an SLM based upon
encoder-decoder architecture as this enables sequential generation of tax codes
to capture the hierarchical dependencies present within the tax codes. Our
experiments demonstrate that encoder-decoder SLMs can be successfully applied
to the sequential prediction of structured tax codes, a domain that remains
comparatively unexplored in current NLP research. In this paper, we demonstrate
the superior performance of the domain-adaptive encoder-decoder SLMs over flat
classifiers when applied to the Harmonized System of Nomenclature (HSN), and
achieve superior results compared to decoder-only and encoder-only
architectures for structured sequence generation tasks. This approach can also
be scaled to other government-mandated tax commodity codes, such as United
Nations Standard Products and Services Codes (UNSPSC), or Brazil's Nomenclatura
Comum do Mercosul (NCM).

</details>


### [190] [Learning from Imperfect Data: Robust Inference of Dynamic Systems using Simulation-based Generative Model](https://arxiv.org/abs/2507.10884)
*Hyunwoo Cho,Hyeontae Jo,Hyung Ju Hwang*

Main category: cs.LG

TL;DR: SiGMoID is a simulation-based generative model for robust inference of nonlinear dynamic systems from noisy or sparse data, combining physics-informed neural networks and Wasserstein GANs.


<details>
  <summary>Details</summary>
Motivation: Inferring nonlinear dynamic models from imperfect data is challenging, especially with noise, sparsity, or partial observability.

Method: Integrates physics-informed neural networks (for ODE solving) and Wasserstein GANs (for parameter estimation from noisy data).

Result: SiGMoID accurately quantifies noise, estimates parameters, and infers unobserved components, validated in realistic experiments.

Conclusion: SiGMoID is broadly applicable across domains, enabling discovery of full system dynamics from imperfect data.

Abstract: System inference for nonlinear dynamic models, represented by ordinary
differential equations (ODEs), remains a significant challenge in many fields,
particularly when the data are noisy, sparse, or partially observable. In this
paper, we propose a Simulation-based Generative Model for Imperfect Data
(SiGMoID) that enables precise and robust inference for dynamic systems. The
proposed approach integrates two key methods: (1) physics-informed neural
networks with hyper-networks that constructs an ODE solver, and (2) Wasserstein
generative adversarial networks that estimates ODE parameters by effectively
capturing noisy data distributions. We demonstrate that SiGMoID quantifies data
noise, estimates system parameters, and infers unobserved system components.
Its effectiveness is validated validated through realistic experimental
examples, showcasing its broad applicability in various domains, from
scientific research to engineered systems, and enabling the discovery of full
system dynamics.

</details>


### [191] [How to Protect Models against Adversarial Unlearning?](https://arxiv.org/abs/2507.10886)
*Patryk Jasiorski,Marek Klonowski,Michał Woźniak*

Main category: cs.LG

TL;DR: The paper explores adversarial unlearning, where malicious unlearn requests degrade model performance, and proposes a method to protect models from such effects.


<details>
  <summary>Details</summary>
Motivation: AI models require unlearning for legal compliance, debiasing, or removing toxic content, but this can degrade performance. Adversarial unlearning exacerbates this issue.

Method: Investigates adversarial unlearning, analyzing factors like model backbone and data selection strategies, and introduces a protection method.

Result: Demonstrates that adversarial unlearning can significantly harm model performance, depending on model and data factors.

Conclusion: Proposes a novel method to safeguard model performance against adversarial and spontaneous unlearning effects.

Abstract: AI models need to be unlearned to fulfill the requirements of legal acts such
as the AI Act or GDPR, and also because of the need to remove toxic content,
debiasing, the impact of malicious instances, or changes in the data
distribution structure in which a model works. Unfortunately, removing
knowledge may cause undesirable side effects, such as a deterioration in model
performance. In this paper, we investigate the problem of adversarial
unlearning, where a malicious party intentionally sends unlearn requests to
deteriorate the model's performance maximally. We show that this phenomenon and
the adversary's capabilities depend on many factors, primarily on the backbone
model itself and strategy/limitations in selecting data to be unlearned. The
main result of this work is a new method of protecting model performance from
these side effects, both in the case of unlearned behavior resulting from
spontaneous processes and adversary actions.

</details>


### [192] [Outbound Modeling for Inventory Management](https://arxiv.org/abs/2507.10890)
*Riccardo Savorgnan,Udaya Ghai,Carson Eisenach,Dean Foster*

Main category: cs.LG

TL;DR: The paper addresses forecasting warehouse unit drain and shipping costs for RL-based inventory planning, proposing a probabilistic model and validation scheme for robustness.


<details>
  <summary>Details</summary>
Motivation: Accurate modeling of warehouse drain and shipping costs is crucial for RL-based inventory planning, but existing methods are non-differentiable and inefficient.

Method: A probabilistic forecasting model is developed to predict joint distributions of drain and shipping costs, conditioned on inventory and demand, with a validation scheme for RL robustness.

Result: Preliminary results show the model's accuracy in in-distribution settings.

Conclusion: The proposed model and validation scheme offer a differentiable and efficient solution for RL-based inventory planning, with potential for handling out-of-distribution scenarios.

Abstract: We study the problem of forecasting the number of units fulfilled (or
``drained'') from each inventory warehouse to meet customer demand, along with
the associated outbound shipping costs. The actual drain and shipping costs are
determined by complex production systems that manage the planning and execution
of customers' orders fulfillment, i.e. from where and how to ship a unit to be
delivered to a customer. Accurately modeling these processes is critical for
regional inventory planning, especially when using Reinforcement Learning (RL)
to develop control policies. For the RL usecase, a drain model is incorporated
into a simulator to produce long rollouts, which we desire to be
differentiable. While simulating the calls to the internal software systems can
be used to recover this transition, they are non-differentiable and too slow
and costly to run within an RL training environment. Accordingly, we frame this
as a probabilistic forecasting problem, modeling the joint distribution of
outbound drain and shipping costs across all warehouses at each time period,
conditioned on inventory positions and exogenous customer demand. To ensure
robustness in an RL environment, the model must handle out-of-distribution
scenarios that arise from off-policy trajectories. We propose a validation
scheme that leverages production systems to evaluate the drain model on
counterfactual inventory states induced by RL policies. Preliminary results
demonstrate the model's accuracy within the in-distribution setting.

</details>


### [193] [Class-Proportional Coreset Selection for Difficulty-Separable Data](https://arxiv.org/abs/2507.10904)
*Elisa Tsai,Haizhong Zheng,Atul Prakash*

Main category: cs.LG

TL;DR: The paper challenges the assumption of class-wise homogeneity in data difficulty for coreset selection, introduces a measure (CDSC) for class-difficulty separability, and proposes class-proportional methods to improve data efficiency and performance.


<details>
  <summary>Details</summary>
Motivation: Existing coreset selection methods assume uniform data difficulty across classes, but real-world domains like network intrusion detection and medical imaging show class-specific difficulty clusters, leading to performance issues in class-agnostic methods.

Method: The authors introduce the Class Difficulty Separability Coefficient (CDSC) and develop class-proportional variants of sampling strategies to address class-specific difficulty.

Result: Class-proportional methods outperform class-agnostic ones, e.g., CCS-CP shows minimal performance drops (e.g., 2.58% accuracy loss) at 99% pruning, while baselines suffer larger declines (e.g., 7.59% accuracy loss).

Conclusion: Explicitly modeling class-difficulty separability enhances data pruning effectiveness, robustness, and generalizability, especially in high-stakes applications.

Abstract: High-quality training data is essential for building reliable and efficient
machine learning systems. One-shot coreset selection addresses this by pruning
the dataset while maintaining or even improving model performance, often
relying on training-dynamics-based data difficulty scores. However, most
existing methods implicitly assume class-wise homogeneity in data difficulty,
overlooking variation in data difficulty across different classes.
  In this work, we challenge this assumption by showing that, in domains such
as network intrusion detection and medical imaging, data difficulty often
clusters by class. We formalize this as class-difficulty separability and
introduce the Class Difficulty Separability Coefficient (CDSC) as a
quantitative measure. We demonstrate that high CDSC values correlate with
performance degradation in class-agnostic coreset methods, which tend to
overrepresent easy majority classes while neglecting rare but informative ones.
  To address this, we introduce class-proportional variants of multiple
sampling strategies. Evaluated on five diverse datasets spanning security and
medical domains, our methods consistently achieve state-of-the-art data
efficiency. For instance, on CTU-13, at an extreme 99% pruning rate, a
class-proportional variant of Coverage-centric Coreset Selection (CCS-CP) shows
remarkable stability, with accuracy dropping only 2.58%, precision 0.49%, and
recall 0.19%. In contrast, the class-agnostic CCS baseline, the next best
method, suffers sharper declines of 7.59% in accuracy, 4.57% in precision, and
4.11% in recall.
  We further show that aggressive pruning enhances generalization in noisy,
imbalanced, and large-scale datasets. Our results underscore that explicitly
modeling class-difficulty separability leads to more effective, robust, and
generalizable data pruning, particularly in high-stakes scenarios.

</details>


### [194] [Diffusion Decoding for Peptide De Novo Sequencing](https://arxiv.org/abs/2507.10955)
*Chi-en Amy Tai,Alexander Wong*

Main category: cs.LG

TL;DR: The paper explores diffusion decoders for peptide de novo sequencing, finding they improve amino acid recall but not peptide precision/recall compared to autoregressive models.


<details>
  <summary>Details</summary>
Motivation: Traditional autoregressive decoders in deep learning (e.g., Casanovo) suffer from cascading errors and underuse high-confidence regions.

Method: The study tests three diffusion decoder designs, knapsack beam search, and various loss functions for peptide sequencing.

Result: Knapsack beam search didn't help, and diffusion decoders alone lowered performance. However, the best design with DINOISER loss improved amino acid recall by 0.373.

Conclusion: Diffusion decoders show promise for enhancing sensitivity in peptide de novo sequencing, despite mixed results.

Abstract: Peptide de novo sequencing is a method used to reconstruct amino acid
sequences from tandem mass spectrometry data without relying on existing
protein sequence databases. Traditional deep learning approaches, such as
Casanovo, mainly utilize autoregressive decoders and predict amino acids
sequentially. Subsequently, they encounter cascading errors and fail to
leverage high-confidence regions effectively. To address these issues, this
paper investigates using diffusion decoders adapted for the discrete data
domain. These decoders provide a different approach, allowing sequence
generation to start from any peptide segment, thereby enhancing prediction
accuracy. We experiment with three different diffusion decoder designs,
knapsack beam search, and various loss functions. We find knapsack beam search
did not improve performance metrics and simply replacing the transformer
decoder with a diffusion decoder lowered performance. Although peptide
precision and recall were still 0, the best diffusion decoder design with the
DINOISER loss function obtained a statistically significant improvement in
amino acid recall by 0.373 compared to the baseline autoregressive
decoder-based Casanovo model. These findings highlight the potential of
diffusion decoders to not only enhance model sensitivity but also drive
significant advancements in peptide de novo sequencing.

</details>


### [195] [Physics-Informed Neural Networks For Semiconductor Film Deposition: A Review](https://arxiv.org/abs/2507.10983)
*Tao Han,Zahra Taheri,Hyunwoong Ko*

Main category: cs.LG

TL;DR: The paper reviews ML applications, especially Physics-Informed Neural Networks (PINNs), for improving semiconductor film deposition processes, identifying trends, gaps, and proposing future research directions.


<details>
  <summary>Details</summary>
Motivation: To address challenges in semiconductor film deposition (e.g., control, quality) using ML, particularly PINNs, for better precision and efficiency.

Method: Thematic analysis of ML applications in film deposition, focusing on PINNs and their integration of physical laws into neural networks.

Result: Identified trends, limitations, and gaps in current ML methods, proposing PINNs as a solution for enhanced interpretability and robustness.

Conclusion: PINNs offer significant potential for advancing film deposition; future research should focus on integrating physics-informed ML to improve manufacturing processes.

Abstract: Semiconductor manufacturing relies heavily on film deposition processes, such
as Chemical Vapor Deposition and Physical Vapor Deposition. These complex
processes require precise control to achieve film uniformity, proper adhesion,
and desired functionality. Recent advancements in Physics-Informed Neural
Networks (PINNs), an innovative machine learning (ML) approach, have shown
significant promise in addressing challenges related to process control,
quality assurance, and predictive modeling within semiconductor film deposition
and other manufacturing domains. This paper provides a comprehensive review of
ML applications targeted at semiconductor film deposition processes. Through a
thematic analysis, we identify key trends, existing limitations, and research
gaps, offering insights into both the advantages and constraints of current
methodologies. Our structured analysis aims to highlight the potential
integration of these ML techniques to enhance interpretability, accuracy, and
robustness in film deposition processes. Additionally, we examine
state-of-the-art PINN methods, discussing strategies for embedding physical
knowledge, governing laws, and partial differential equations into advanced
neural network architectures tailored for semiconductor manufacturing. Based on
this detailed review, we propose novel research directions that integrate the
strengths of PINNs to significantly advance film deposition processes. The
contributions of this study include establishing a clear pathway for future
research in integrating physics-informed ML frameworks, addressing existing
methodological gaps, and ultimately improving precision, scalability, and
operational efficiency within semiconductor manufacturing.

</details>


### [196] [StellarF: A Lora-Adapter Integrated Large Model Framework for Stellar Flare Forecasting with Historical & Statistical Data](https://arxiv.org/abs/2507.10986)
*Tianyu Su,Zhiqiang Zou,Ali Luo,Xiao Kong,Qingyu Lu,Min Li*

Main category: cs.LG

TL;DR: StellarF introduces a parameter-efficient model for stellar flare forecasting using LoRA and Adapter techniques, outperforming existing methods on Kepler and TESS datasets.


<details>
  <summary>Details</summary>
Motivation: The sparsity of recorded flare events and lack of domain-specific large-scale models hinder stellar flare forecasting.

Method: StellarF combines a flare statistical information module with a historical flare record module for multi-scale pattern recognition.

Result: StellarF achieves state-of-the-art performance on self-constructed datasets from Kepler and TESS.

Conclusion: The model provides a novel framework for advancing astrophysical research and cross-disciplinary applications.

Abstract: Stellar flare forecasting, a critical research frontier in astronomy, offers
profound insights into stellar activity. However, the field is constrained by
both the sparsity of recorded flare events and the absence of domain-specific
large-scale predictive models. To address these challenges, this study
introduces StellarF (Stellar Flare Forecasting), a novel large model that
leverages Low-Rank (LoRA) and Adapter techniques to parameter-efficient
learning for stellar flare forecasting. At its core, StellarF integrates an
flare statistical information module with a historical flare record module,
enabling multi-scale pattern recognition from observational data. Extensive
experiments on our self-constructed datasets (derived from Kepler and TESS
light curves) demonstrate that StellarF achieves state-of-the-art performance
compared to existing methods. The proposed prediction paradigm establishes a
novel methodological framework for advancing astrophysical research and
cross-disciplinary applications.

</details>


### [197] [High-Throughput Distributed Reinforcement Learning via Adaptive Policy Synchronization](https://arxiv.org/abs/2507.10990)
*Rodney Lafuente-Mercado*

Main category: cs.LG

TL;DR: ClusterEnv is a lightweight, modular framework for distributed RL environment execution, decoupling simulation from training with the DETACH pattern and addressing policy staleness via AAPS.


<details>
  <summary>Details</summary>
Motivation: Existing RL frameworks entangle simulation, learning, and orchestration, limiting modularity and reusability.

Method: ClusterEnv uses the DETACH pattern to offload simulation tasks to remote workers and introduces AAPS for efficient policy synchronization.

Result: AAPS achieves high sample efficiency with fewer weight updates, and ClusterEnv integrates seamlessly into existing RL pipelines.

Conclusion: ClusterEnv offers a modular, efficient solution for distributed RL, improving scalability and reducing synchronization overhead.

Abstract: Scaling reinforcement learning (RL) workloads often requires distributing
environment simulation across compute clusters. Existing frameworks entangle
simulation, learning logic, and orchestration into monolithic systems, limiting
modularity and reusability. We present ClusterEnv, a lightweight,
learner-agnostic interface for distributed environment execution that mirrors
the Gymnasium API. ClusterEnv introduces the DETACH pattern, which decouples
simulation from training by offloading reset() and step() operations to remote
workers while keeping learning centralized. To address policy staleness in
distributed execution, we propose Adaptive Actor Policy Synchronization (AAPS),
a divergence-triggered update mechanism that reduces synchronization overhead
without sacrificing performance. ClusterEnv integrates cleanly into existing RL
pipelines, supports both on-policy and off-policy methods, and requires minimal
code changes. Experiments on discrete control tasks demonstrate that AAPS
achieves high sample efficiency with significantly fewer weight updates. Source
code is available at https://github.com/rodlaf/ClusterEnv.

</details>


### [198] [Misalignment from Treating Means as Ends](https://arxiv.org/abs/2507.10995)
*Henrik Marklund,Alex Infanger,Benjamin Van Roy*

Main category: cs.LG

TL;DR: The paper highlights how reward functions in reinforcement learning often conflate terminal and instrumental goals, leading to misalignment and poor performance.


<details>
  <summary>Details</summary>
Motivation: To address the problem of reward functions inaccurately expressing human goals due to the conflation of terminal (ends) and instrumental (means) goals.

Method: Formulates a simple example demonstrating how slight conflation of these goals causes severe misalignment and analyzes environments sensitive to this issue.

Result: Shows that optimizing misspecified reward functions results in poor performance when evaluated by the true reward function.

Conclusion: The study underscores the sensitivity of reinforcement learning to goal conflation and discusses implications for reward learning and real-world environments.

Abstract: Reward functions, learned or manually specified, are rarely perfect. Instead
of accurately expressing human goals, these reward functions are often
distorted by human beliefs about how best to achieve those goals. Specifically,
these reward functions often express a combination of the human's terminal
goals -- those which are ends in themselves -- and the human's instrumental
goals -- those which are means to an end. We formulate a simple example in
which even slight conflation of instrumental and terminal goals results in
severe misalignment: optimizing the misspecified reward function results in
poor performance when measured by the true reward function. This example
distills the essential properties of environments that make reinforcement
learning highly sensitive to conflation of instrumental and terminal goals. We
discuss how this issue can arise with a common approach to reward learning and
how it can manifest in real environments.

</details>


### [199] [Crafting Imperceptible On-Manifold Adversarial Attacks for Tabular Data](https://arxiv.org/abs/2507.10998)
*Zhipeng He,Alexander Stevens,Chun Ouyang,Johannes De Smedt,Alistair Barros,Catarina Moreira*

Main category: cs.LG

TL;DR: A framework using a mixed-input VAE for generating imperceptible adversarial examples on tabular data, ensuring statistical consistency and outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of defining imperceptible modifications in tabular data due to its heterogeneous nature and lack of intuitive similarity metrics.

Method: Proposes a latent space perturbation framework with a mixed-input VAE, integrating categorical and numerical features into a unified latent manifold.

Result: Achieves lower outlier rates and consistent performance across datasets and model architectures, with IDSR measuring indistinguishability.

Conclusion: VAE-based attacks are effective for realistic adversarial examples on tabular data, emphasizing on-manifold perturbations and practical utility.

Abstract: Adversarial attacks on tabular data present fundamental challenges distinct
from image or text domains due to the heterogeneous nature of mixed categorical
and numerical features. Unlike images where pixel perturbations maintain visual
similarity, tabular data lacks intuitive similarity metrics, making it
difficult to define imperceptible modifications. Additionally, traditional
gradient-based methods prioritise $\ell_p$-norm constraints, often producing
adversarial examples that deviate from the original data distributions, making
them detectable. We propose a latent space perturbation framework using a
mixed-input Variational Autoencoder (VAE) to generate imperceptible adversarial
examples. The proposed VAE integrates categorical embeddings and numerical
features into a unified latent manifold, enabling perturbations that preserve
statistical consistency. We specify In-Distribution Success Rate (IDSR) to
measure the proportion of adversarial examples that remain statistically
indistinguishable from the input distribution. Evaluation across six publicly
available datasets and three model architectures demonstrates that our method
achieves substantially lower outlier rates and more consistent performance
compared to traditional input-space attacks and other VAE-based methods adapted
from image domain approaches. Our comprehensive analysis includes
hyperparameter sensitivity, sparsity control mechanisms, and generative
architectural comparisons, revealing that VAE-based attacks depend critically
on reconstruction quality but offer superior practical utility when sufficient
training data is available. This work highlights the importance of on-manifold
perturbations for realistic adversarial attacks on tabular data, offering a
robust approach for practical deployment. The source code can be accessed
through https://github.com/ZhipengHe/VAE-TabAttack.

</details>


### [200] [AdaMuon: Adaptive Muon Optimizer](https://arxiv.org/abs/2507.11005)
*Chongjie Si,Debing Zhang,Wei Shen*

Main category: cs.LG

TL;DR: AdaMuon is an adaptive learning-rate framework based on Muon, enhancing it with per-parameter second-moment modulation and RMS-aligned rescaling for better performance.


<details>
  <summary>Details</summary>
Motivation: To improve upon Muon's efficiency gains over AdamW in large-scale model training by introducing adaptive features without added complexity.

Method: AdaMuon adds two modules: per-parameter second-moment modulation for update-level adaptivity and RMS-aligned rescaling for update magnitude regulation.

Result: AdaMuon outperforms Muon in convergence speed and stability across various model scales and learning-rate regimes.

Conclusion: AdaMuon is a seamless, tuning-free enhancement to Muon, offering superior performance without extra integration effort.

Abstract: We propose AdaMuon, an adaptive learning-rate framework built upon the
recently validated Muon optimizer, which has demonstrated substantial
efficiency gains over AdamW in large-scale model training. AdaMuon augments
Muon with two mutually dependent modules: (1) a per-parameter second-moment
modulation that captures orthogonal gradient updates to ensure update-level
adaptivity, and (2) a RMS-aligned rescaling that regulates the overall update
magnitude by aligning it with the intrinsic structure of the parameter space.
Empirical results on multiple model scales and learning-rate regimes confirm
that AdaMuon consistently outperforms the original Muon, delivering higher
acceleration in convergence while maintaining training stability. Our method
introduces no additional tuning burden and can be seamlessly integrated into
existing Muon training pipelines.

</details>


### [201] [Leveraging Advanced Machine Learning to Predict Turbulence Dynamics from Temperature Observations at an Experimental Prescribed Fire](https://arxiv.org/abs/2507.11012)
*Dipak Dulal,Joseph J. Charney,Michael R. Gallagher,Pitambar Acharya,Carmeliza Navasca,Nicholas S. Skowronski*

Main category: cs.LG

TL;DR: Machine learning models predict turbulent kinetic energy (TKE) from temperature data in fire environments, revealing new insights for fire research and management.


<details>
  <summary>Details</summary>
Motivation: To explore relationships between temperature and TKE in fire environments and improve fire operations and model predictions.

Method: Used Deep Neural Networks, Random Forest Regressor, Gradient Boosting, and Gaussian Process Regressor on 10 Hz temperature and turbulence data from a prescribed burn.

Result: Achieved accurate TKE predictions despite weak correlations, with regression models performing particularly well.

Conclusion: Machine learning can uncover new temperature-airflow relationships, advancing fire research and management.

Abstract: This study explores the potential for predicting turbulent kinetic energy
(TKE) from more readily acquired temperature data using temperature profiles
and turbulence data collected concurrently at 10 Hz during a small experimental
prescribed burn in the New Jersey Pine Barrens. Machine learning models,
including Deep Neural Networks, Random Forest Regressor, Gradient Boosting, and
Gaussian Process Regressor, were employed to assess the potential to predict
TKE from temperature perturbations and explore temporal and spatial dynamics of
correlations. Data visualization and correlation analyses revealed patterns and
relationships between thermocouple temperatures and TKE, providing insight into
the underlying dynamics. More accurate predictions of TKE were achieved by
employing various machine learning models despite a weak correlation between
the predictors and the target variable. The results demonstrate significant
success, particularly from regression models, in accurately predicting the TKE.
The findings of this study demonstrate a novel numerical approach to
identifying new relationships between temperature and airflow processes in and
around the fire environment. These relationships can help refine our
understanding of combustion environment processes and the coupling and
decoupling of fire environment processes necessary for improving fire
operations strategy and fire and smoke model predictions. The findings of this
study additionally highlight the valuable role of machine learning techniques
in analyzing the complex large datasets of the fire environments, showcasing
their potential to advance fire research and management practices.

</details>


### [202] [First-Order Error Matters: Accurate Compensation for Quantized Large Language Models](https://arxiv.org/abs/2507.11017)
*Xingyu Zheng,Haotong Qin,Yuye Li,Jiakai Wang,Jinyang Guo,Michele Magno,Xianglong Liu*

Main category: cs.LG

TL;DR: FOEM is a novel PTQ method for LLMs that incorporates first-order gradient terms to improve quantization error compensation, outperforming existing methods like GPTQ.


<details>
  <summary>Details</summary>
Motivation: Existing PTQ methods assume negligible first-order terms in quantization error, but this assumption is flawed due to accumulated deviations.

Method: FOEM explicitly includes first-order gradients, approximates them efficiently, and uses precomputed Cholesky factors for Hessian inversion.

Result: FOEM reduces perplexity by 89.6% for Llama3-8B and improves MMLU accuracy from 51.7% to 74.9% for Llama3-70B, nearing full-precision performance.

Conclusion: FOEM effectively addresses flaws in existing PTQ methods, offering significant improvements and compatibility with advanced techniques.

Abstract: Post-training quantization (PTQ) offers an efficient approach to compressing
large language models (LLMs), significantly reducing memory access and
computational costs. Existing compensation-based weight calibration methods
often rely on a second-order Taylor expansion to model quantization error,
under the assumption that the first-order term is negligible in well-trained
full-precision models. However, we reveal that the progressive compensation
process introduces accumulated first-order deviations between latent weights
and their full-precision counterparts, making this assumption fundamentally
flawed. To address this, we propose FOEM, a novel PTQ method that explicitly
incorporates first-order gradient terms to improve quantization error
compensation. FOEM approximates gradients by directly computing the difference
between latent and full-precision weights, avoiding the high cost and limited
generalization of backpropagation-based gradient computation. This approach
introduces minimal additional computational overhead. Moreover, FOEM leverages
precomputed Cholesky factors to efficiently recover the inverse of Hessian
submatrices in real time. Extensive experiments across a wide range of models
and benchmarks demonstrate that FOEM consistently outperforms the classical
GPTQ method. In 3-bit weight-only quantization, FOEM reduces the perplexity of
Llama3-8B by 89.6%, and improves the 5-shot MMLU accuracy of Llama3-70B from
51.7% to 74.9%, approaching the full-precision performance of 78.6%.
Furthermore, FOEM can be seamlessly integrated with advanced techniques such as
GPTAQ and SpinQuant, yielding additional improvements under the challenging
W4A4KV4 setting, and further narrowing the accuracy gap with full-precision
baselines beyond what current state-of-the-art methods achieve. The code is
available at https://github.com/Xingyu-Zheng/FOEM.

</details>


### [203] [Relative Entropy Pathwise Policy Optimization](https://arxiv.org/abs/2507.11019)
*Claas Voelcker,Axel Brunnbauer,Marcel Hussing,Michal Nauman,Pieter Abbeel,Eric Eaton,Radu Grosu,Amir-massoud Farahmand,Igor Gilitschenski*

Main category: cs.LG

TL;DR: The paper introduces REPPO, an on-policy algorithm combining pathwise policy gradients' efficiency with on-policy learning's simplicity, addressing high variance and training stability issues.


<details>
  <summary>Details</summary>
Motivation: Score-function policy gradients suffer from high variance, while pathwise policy gradients require accurate action-conditioned value functions, often relying on off-policy data. The goal is to enable stable, efficient on-policy learning with pathwise updates.

Method: Proposes REPPO, which trains Q-value models purely from on-policy data, balances stochastic policies for exploration, and uses constrained policy updates for stability.

Result: REPPO shows strong performance with reduced sample needs, faster training, lower memory use, and robust hyperparameters in benchmarks.

Conclusion: REPPO effectively merges pathwise policy gradients' efficiency with on-policy learning's simplicity, offering a practical solution for stable and scalable reinforcement learning.

Abstract: Score-function policy gradients have delivered strong results in
game-playing, robotics and language-model fine-tuning. Yet its high-variance
often undermines training stability. On the other hand, pathwise policy
gradients alleviate the training variance, but are reliable only when driven by
an accurate action-conditioned value function which is notoriously hard to
train without relying on past off-policy data. In this paper, we discuss how to
construct a value-gradient driven, on-policy algorithm that allow training
Q-value models purely from on-policy data, unlocking the possibility of using
pathwise policy updates in the context of on-policy learning. We show how to
balance stochastic policies for exploration with constrained policy updates for
stable training, and evaluate important architectural components that
facilitate accurate value function learning. Building on these insights, we
propose Relative Entropy Pathwise Policy Optimization (REPPO), an efficient
on-policy algorithm that combines the sample-efficiency of pathwise policy
gradients with the simplicity and minimal memory footprint of standard
on-policy learning. We demonstrate that REPPO provides strong empirical
performance at decreased sample requirements, wall-clock time, memory footprint
as well as high hyperparameter robustness in a set of experiments on two
standard GPU-parallelized benchmarks.

</details>


### [204] [GATE: Graph Attention Neural Networks with Real-Time Edge Construction for Robust Indoor Localization using Mobile Embedded Devices](https://arxiv.org/abs/2507.11053)
*Danish Gufran,Sudeep Pasricha*

Main category: cs.LG

TL;DR: GATE, a novel framework using adaptive graph representation and attention mechanisms, improves indoor localization accuracy by addressing non-Euclidean noise and device heterogeneity, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Indoor localization is vital for smart environments, but current DL models fail to handle non-Euclidean noise and device heterogeneity, limiting accuracy.

Method: GATE uses adaptive graph representation, AHV for message passing, MDHV to mitigate GNN blind spots, and RTEC for dynamic graph adaptation.

Result: GATE reduces mean localization errors by 1.6x to 4.72x and worst-case errors by 1.85x to 4.57x compared to state-of-the-art methods.

Conclusion: GATE effectively addresses challenges in indoor localization, offering superior accuracy and robustness across diverse environments and devices.

Abstract: Accurate indoor localization is crucial for enabling spatial context in smart
environments and navigation systems. Wi-Fi Received Signal Strength (RSS)
fingerprinting is a widely used indoor localization approach due to its
compatibility with mobile embedded devices. Deep Learning (DL) models improve
accuracy in localization tasks by learning RSS variations across locations, but
they assume fingerprint vectors exist in a Euclidean space, failing to
incorporate spatial relationships and the non-uniform distribution of
real-world RSS noise. This results in poor generalization across heterogeneous
mobile devices, where variations in hardware and signal processing distort RSS
readings. Graph Neural Networks (GNNs) can improve upon conventional DL models
by encoding indoor locations as nodes and modeling their spatial and signal
relationships as edges. However, GNNs struggle with non-Euclidean noise
distributions and suffer from the GNN blind spot problem, leading to degraded
accuracy in environments with dense access points (APs). To address these
challenges, we propose GATE, a novel framework that constructs an adaptive
graph representation of fingerprint vectors while preserving an indoor
state-space topology, modeling the non-Euclidean structure of RSS noise to
mitigate environmental noise and address device heterogeneity. GATE introduces
1) a novel Attention Hyperspace Vector (AHV) for enhanced message passing, 2) a
novel Multi-Dimensional Hyperspace Vector (MDHV) to mitigate the GNN blind
spot, and 3) an new Real-Time Edge Construction (RTEC) approach for dynamic
graph adaptation. Extensive real-world evaluations across multiple indoor
spaces with varying path lengths, AP densities, and heterogeneous devices
demonstrate that GATE achieves 1.6x to 4.72x lower mean localization errors and
1.85x to 4.57x lower worst-case errors compared to state-of-the-art indoor
localization frameworks.

</details>


### [205] [A Distance Metric for Mixed Integer Programming Instances](https://arxiv.org/abs/2507.11063)
*Gwen Maudet,Grégoire Danoy*

Main category: cs.LG

TL;DR: The paper introduces a mathematical distance metric for MILP instances to improve similarity comparison, outperforming existing methods in accuracy and speed.


<details>
  <summary>Details</summary>
Motivation: MILP lacks a reliable similarity metric for comparing instances, limiting solver guidance and evaluation of instance heterogeneity. Existing metrics are imprecise or rely on labeled data.

Method: Proposes a distance metric derived from MILP formulations, discretizing components and using Earth mover's distance for constraint comparisons. Evaluates exact and greedy variants.

Result: The greedy variant is nearly as accurate as the exact one but 200x faster. The method outperforms non-learned baselines and rivals supervised classifiers.

Conclusion: The introduced metric effectively addresses the gap in MILP instance comparison, offering a fast and accurate unsupervised solution.

Abstract: Mixed-integer linear programming (MILP) is a powerful tool for addressing a
wide range of real-world problems, but it lacks a clear structure for comparing
instances. A reliable similarity metric could establish meaningful
relationships between instances, enabling more effective evaluation of instance
set heterogeneity and providing better guidance to solvers, particularly when
machine learning is involved. Existing similarity metrics often lack precision
in identifying instance classes or rely heavily on labeled data, which limits
their applicability and generalization. To bridge this gap, this paper
introduces the first mathematical distance metric for MILP instances, derived
directly from their mathematical formulations. By discretizing right-hand
sides, weights, and variables into classes, the proposed metric draws
inspiration from the Earth mover's distance to quantify mismatches in
weight-variable distributions for constraint comparisons. This approach
naturally extends to enable instance-level comparisons. We evaluate both an
exact and a greedy variant of our metric under various parameter settings,
using the StrIPLIB dataset. Results show that all components of the metric
contribute to class identification, and that the greedy version achieves
accuracy nearly identical to the exact formulation while being nearly 200 times
faster. Compared to state-of-the-art baselines, including feature-based,
image-based, and neural network models, our unsupervised method consistently
outperforms all non-learned approaches and rivals the performance of a
supervised classifier on class and subclass grouping tasks.

</details>


### [206] [LogTinyLLM: Tiny Large Language Models Based Contextual Log Anomaly Detection](https://arxiv.org/abs/2507.11071)
*Isaiah Thompson Ocansey,Ritwik Bhattacharya,Tanmay Sen*

Main category: cs.LG

TL;DR: The paper proposes parameter-efficient fine-tuning methods (LoRA and adapters) for log anomaly detection, showing significant performance gains over traditional approaches.


<details>
  <summary>Details</summary>
Motivation: Log anomaly detection is challenging due to large, complex log sequences, necessitating efficient methods for system maintenance.

Method: Uses LoRA and adapter-based fine-tuning on tiny LLMs, tested on the Thunderbird dataset.

Result: LoRA achieves 97.76%-98.83% accuracy, outperforming LogBert (79.37%) by 18-19%.

Conclusion: Parameter-efficient fine-tuning, especially LoRA, is highly effective for log anomaly detection.

Abstract: Log anomaly detection using traditional rule based or deep learning based
methods is often challenging due to the large volume and highly complex nature
of log sequence. So effective way of detection of anomalous sequence of logs is
crucial for system maintenance and development. This paper proposes parameter
efficient finetuning specifically low rank adaptation (LoRA) and adapter based
approaches for finding contextual anomalies in sequence of logs in large log
data set. It compares different tiny large language models (LLMs) on the
Thunderbird dataset. The results show that LoRA based finetuning provides
substantial performance improvements of 18 to 19 percentage over LogBert based
full finetuning approach, achieving accuracy scores between 97.76% and 98.83%
compared to 79.37%.

</details>


### [207] [Real-Time Bayesian Detection of Drift-Evasive GNSS Spoofing in Reinforcement Learning Based UAV Deconfliction](https://arxiv.org/abs/2507.11173)
*Deepak Kumar Panda,Weisi Guo*

Main category: cs.LG

TL;DR: The paper proposes a Bayesian online change point detection (BOCPD) method to detect subtle drift-evasive spoofing attacks on UAVs by monitoring temporal shifts in RL critic network outputs, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: UAVs depend on GNSS for navigation but are vulnerable to spoofing attacks, especially drift-evasive ones that evade conventional detection. Rapid detection is crucial for resilience.

Method: Uses BOCPD to monitor temporal shifts in RL critic network value estimates for detecting behavioral deviations in UAV navigation.

Result: Outperforms traditional GNSS spoofing detectors, semi-supervised learning, and the Page-Hinkley test in accuracy and lower error rates.

Conclusion: The BOCPD-based temporal value framework effectively detects stealthy spoofing attacks, enhancing UAV navigation security.

Abstract: Autonomous unmanned aerial vehicles (UAVs) rely on global navigation
satellite system (GNSS) pseudorange measurements for accurate real-time
localization and navigation. However, this dependence exposes them to
sophisticated spoofing threats, where adversaries manipulate pseudoranges to
deceive UAV receivers. Among these, drift-evasive spoofing attacks subtly
perturb measurements, gradually diverting the UAVs trajectory without
triggering conventional signal-level anti-spoofing mechanisms. Traditional
distributional shift detection techniques often require accumulating a
threshold number of samples, causing delays that impede rapid detection and
timely response. Consequently, robust temporal-scale detection methods are
essential to identify attack onset and enable contingency planning with
alternative sensing modalities, improving resilience against stealthy
adversarial manipulations. This study explores a Bayesian online change point
detection (BOCPD) approach that monitors temporal shifts in value estimates
from a reinforcement learning (RL) critic network to detect subtle behavioural
deviations in UAV navigation. Experimental results show that this temporal
value-based framework outperforms conventional GNSS spoofing detectors,
temporal semi-supervised learning frameworks, and the Page-Hinkley test,
achieving higher detection accuracy and lower false-positive and false-negative
rates for drift-evasive spoofing attacks.

</details>


### [208] [Gradient Regularization-based Neural Granger Causality](https://arxiv.org/abs/2507.11178)
*Meiliang Liu,Huiwen Dong,Xiaoxiao Yang,Yunfang Xu,Zijin Li,Zhengye Si,Xinyue Yang,Zhiwen Zhao*

Main category: cs.LG

TL;DR: GRNGC is a novel neural Granger causality method using gradient regularization, reducing computational costs and improving flexibility.


<details>
  <summary>Details</summary>
Motivation: Existing neural Granger causality models are computationally expensive and limited in capturing complex interactions.

Method: GRNGC applies L1 regularization to gradients between input and output, using a single model for all time series.

Result: GRNGC outperforms baselines in simulations and real-world datasets, reducing computational overhead.

Conclusion: GRNGC is effective, flexible, and efficient for Granger causality inference.

Abstract: With the advancement of deep learning technologies, various neural
network-based Granger causality models have been proposed. Although these
models have demonstrated notable improvements, several limitations remain. Most
existing approaches adopt the component-wise architecture, necessitating the
construction of a separate model for each time series, which results in
substantial computational costs. In addition, imposing the sparsity-inducing
penalty on the first-layer weights of the neural network to extract causal
relationships weakens the model's ability to capture complex interactions. To
address these limitations, we propose Gradient Regularization-based Neural
Granger Causality (GRNGC), which requires only one time series prediction model
and applies $L_{1}$ regularization to the gradient between model's input and
output to infer Granger causality. Moreover, GRNGC is not tied to a specific
time series forecasting model and can be implemented with diverse architectures
such as KAN, MLP, and LSTM, offering enhanced flexibility. Numerical
simulations on DREAM, Lorenz-96, fMRI BOLD, and CausalTime show that GRNGC
outperforms existing baselines and significantly reduces computational
overhead. Meanwhile, experiments on real-world DNA, Yeast, HeLa, and bladder
urothelial carcinoma datasets further validate the model's effectiveness in
reconstructing gene regulatory networks.

</details>


### [209] [Mixture of Experts in Large Language Models](https://arxiv.org/abs/2507.11181)
*Danyang Zhang,Junhao Song,Ziqian Bi,Yingfang Yuan,Tianyang Wang,Joe Yeong,Junfeng Hao*

Main category: cs.LG

TL;DR: A review of Mixture-of-Experts (MoE) architecture in large language models, highlighting its performance benefits, key mechanisms, and challenges.


<details>
  <summary>Details</summary>
Motivation: To explore MoE's potential in enhancing model performance with minimal computational overhead and its applications in deep learning.

Method: Systematic analysis of theoretical foundations, architectural designs, gating/routing mechanisms, and real-world deployments.

Result: MoE offers superior model capacity, task-specific performance, and efficient scaling, but requires expert diversity and calibration.

Conclusion: The review identifies limitations, challenges, and future directions for MoE architecture innovation.

Abstract: This paper presents a comprehensive review of the Mixture-of-Experts (MoE)
architecture in large language models, highlighting its ability to
significantly enhance model performance while maintaining minimal computational
overhead. Through a systematic analysis spanning theoretical foundations, core
architectural designs, and large language model (LLM) applications, we examine
expert gating and routing mechanisms, hierarchical and sparse MoE
configurations, meta-learning approaches, multimodal and multitask learning
scenarios, real-world deployment cases, and recent advances and challenges in
deep learning. Our analysis identifies key advantages of MoE, including
superior model capacity compared to equivalent Bayesian approaches, improved
task-specific performance, and the ability to scale model capacity efficiently.
We also underscore the importance of ensuring expert diversity, accurate
calibration, and reliable inference aggregation, as these are essential for
maximizing the effectiveness of MoE architectures. Finally, this review
outlines current research limitations, open challenges, and promising future
directions, providing a foundation for continued innovation in MoE architecture
and its applications.

</details>


### [210] [Quantized Rank Reduction: A Communications-Efficient Federated Learning Scheme for Network-Critical Applications](https://arxiv.org/abs/2507.11183)
*Dimitrios Kritsiolis,Constantine Kotropoulos*

Main category: cs.LG

TL;DR: A communication-efficient federated learning scheme using low-rank approximation and quantization to reduce network load while maintaining model accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the high communication overhead in federated learning caused by frequent model updates, while preserving data privacy and security.

Method: Proposes a scheme combining low-rank approximation of neural network gradients and quantization to minimize data exchange.

Result: Significantly reduces network load with minimal impact on model accuracy.

Conclusion: The proposed method effectively balances communication efficiency and model performance in federated learning.

Abstract: Federated learning is a machine learning approach that enables multiple
devices (i.e., agents) to train a shared model cooperatively without exchanging
raw data. This technique keeps data localized on user devices, ensuring privacy
and security, while each agent trains the model on their own data and only
shares model updates. The communication overhead is a significant challenge due
to the frequent exchange of model updates between the agents and the central
server. In this paper, we propose a communication-efficient federated learning
scheme that utilizes low-rank approximation of neural network gradients and
quantization to significantly reduce the network load of the decentralized
learning process with minimal impact on the model's accuracy.

</details>


### [211] [An Explainable AI-Enhanced Machine Learning Approach for Cardiovascular Disease Detection and Risk Assessment](https://arxiv.org/abs/2507.11185)
*Md. Emon Akter Sourov,Md. Sabbir Hossen,Pabon Shaha,Mohammad Minoar Hossain,Md Sadiq Iqbal*

Main category: cs.LG

TL;DR: Machine learning improves heart disease diagnosis and risk prediction using classification and regression models, with Random Forest and Linear Regression showing top performance.


<details>
  <summary>Details</summary>
Motivation: Traditional diagnostic methods for heart disease are often inaccurate, especially in resource-limited regions. Machine learning can enhance accuracy and efficiency.

Method: Used the Heart Disease dataset (1,035 cases) with SMOTE for class imbalance, generating 100,000 synthetic data points. Evaluated models with various metrics.

Result: Random Forest achieved 97.2% accuracy (real data) and 97.6% (synthetic). Linear Regression had R2 values of 0.992 (real) and 0.984 (synthetic).

Conclusion: Machine learning can revolutionize heart disease diagnosis and risk prediction, aiding early intervention and clinical decisions.

Abstract: Heart disease remains a major global health concern, particularly in regions
with limited access to medical resources and diagnostic facilities. Traditional
diagnostic methods often fail to accurately identify and manage heart disease
risks, leading to adverse outcomes. Machine learning has the potential to
significantly enhance the accuracy, efficiency, and speed of heart disease
diagnosis. In this study, we proposed a comprehensive framework that combines
classification models for heart disease detection and regression models for
risk prediction. We employed the Heart Disease dataset, which comprises 1,035
cases. To address the issue of class imbalance, the Synthetic Minority
Oversampling Technique (SMOTE) was applied, resulting in the generation of an
additional 100,000 synthetic data points. Performance metrics, including
accuracy, precision, recall, F1-score, R2, MSE, RMSE, and MAE, were used to
evaluate the model's effectiveness. Among the classification models, Random
Forest emerged as the standout performer, achieving an accuracy of 97.2% on
real data and 97.6% on synthetic data. For regression tasks, Linear Regression
demonstrated the highest R2 values of 0.992 and 0.984 on real and synthetic
datasets, respectively, with the lowest error metrics. Additionally,
Explainable AI techniques were employed to enhance the interpretability of the
models. This study highlights the potential of machine learning to
revolutionize heart disease diagnosis and risk prediction, thereby facilitating
early intervention and enhancing clinical decision-making.

</details>


### [212] [AirLLM: Diffusion Policy-based Adaptive LoRA for Remote Fine-Tuning of LLM over the Air](https://arxiv.org/abs/2507.11515)
*Shiyi Yang,Xiaoxue Yu,Rongpeng Li,Jianhang Zhu,Zhifeng Zhao,Honggang Zhang*

Main category: cs.LG

TL;DR: AirLLM introduces a hierarchical diffusion policy framework for efficient, communication-aware LoRA adaptation in edge-based LLM fine-tuning, reducing transmission costs while improving performance.


<details>
  <summary>Details</summary>
Motivation: Addressing inefficiencies in existing LoRA approaches for remote fine-tuning of LLMs on edge devices due to fixed rank configurations and high transmission costs.

Method: Combines Proximal Policy Optimization (PPO) for coarse-grained decisions and Denoising Diffusion Implicit Models (DDIM) for high-resolution rank vectors, optimized alternately under Classifier-Free Guidance.

Result: AirLLM enhances fine-tuning performance and reduces transmission costs under varying signal-to-noise ratios.

Conclusion: AirLLM effectively leverages reinforcement-driven, diffusion-refined rank adaptation for scalable and efficient remote LLM fine-tuning.

Abstract: Operating Large Language Models (LLMs) on edge devices is increasingly
challenged by limited communication bandwidth and strained computational and
memory costs. Thus, cloud-assisted remote fine-tuning becomes indispensable.
Nevertheless, existing Low-Rank Adaptation (LoRA) approaches typically employ
fixed or heuristic rank configurations, and the subsequent over-the-air
transmission of all LoRA parameters could be rather inefficient. To address
this limitation, we develop AirLLM, a hierarchical diffusion policy framework
for communication-aware LoRA adaptation. Specifically, AirLLM models the rank
configuration as a structured action vector that spans all LoRA-inserted
projections. To solve the underlying high-dimensional sequential
decision-making problem, a Proximal Policy Optimization (PPO) agent generates
coarse-grained decisions by jointly observing wireless states and linguistic
complexity, which are then refined via Denoising Diffusion Implicit Models
(DDIM) to produce high-resolution, task- and channel-adaptive rank vectors. The
two modules are optimized alternatively, with the DDIM trained under the
Classifier-Free Guidance (CFG) paradigm to maintain alignment with PPO rewards.
Experiments under varying signal-to-noise ratios demonstrate that AirLLM
consistently enhances fine-tuning performance while significantly reducing
transmission costs, highlighting the effectiveness of reinforcement-driven,
diffusion-refined rank adaptation for scalable and efficient remote fine-tuning
over the air.

</details>


### [213] [Striking the Perfect Balance: Preserving Privacy While Boosting Utility in Collaborative Medical Prediction Platforms](https://arxiv.org/abs/2507.11187)
*Shao-Bo Lin,Xiaotong Liu,Yao Wang*

Main category: cs.LG

TL;DR: A privacy-preserving mechanism is integrated into a one-shot distributed learning framework for collaborative medical prediction, addressing privacy concerns while maintaining prediction performance.


<details>
  <summary>Details</summary>
Motivation: Privacy attacks and low prediction quality deter patient and doctor participation in online collaborative medical prediction platforms.

Method: Proposes a privacy-preserving mechanism within a one-shot distributed learning framework, validated through simulations and real-world data.

Result: The framework achieves optimal prediction performance under specific privacy requirements, as demonstrated theoretically and empirically.

Conclusion: The proposed platform effectively balances privacy and performance, encouraging broader adoption of collaborative medical prediction.

Abstract: Online collaborative medical prediction platforms offer convenience and
real-time feedback by leveraging massive electronic health records. However,
growing concerns about privacy and low prediction quality can deter patient
participation and doctor cooperation. In this paper, we first clarify the
privacy attacks, namely attribute attacks targeting patients and model
extraction attacks targeting doctors, and specify the corresponding privacy
principles. We then propose a privacy-preserving mechanism and integrate it
into a novel one-shot distributed learning framework, aiming to simultaneously
meet both privacy requirements and prediction performance objectives. Within
the framework of statistical learning theory, we theoretically demonstrate that
the proposed distributed learning framework can achieve the optimal prediction
performance under specific privacy requirements. We further validate the
developed privacy-preserving collaborative medical prediction platform through
both toy simulations and real-world data experiments.

</details>


### [214] [Gradient Descent on Logistic Regression: Do Large Step-Sizes Work with Data on the Sphere?](https://arxiv.org/abs/2507.11228)
*Si Yi Meng,Baptiste Goujaud,Antonio Orvieto,Christopher De Sa*

Main category: cs.LG

TL;DR: The paper investigates if equal-magnitude data ensures global convergence of gradient descent (GD) on logistic regression under any step size below the stability threshold. It confirms this for 1D but finds cycling in higher dimensions.


<details>
  <summary>Details</summary>
Motivation: To understand conditions for global convergence of GD in logistic regression, especially with large step sizes, and to explore cycling behavior in non-separable cases.

Method: Analyzes GD behavior on logistic regression with equal-magnitude data, proving results for 1D and demonstrating cycling in higher dimensions.

Result: Equal-magnitude data ensures global convergence in 1D, but cycling persists in higher dimensions even under the stability threshold.

Conclusion: Further research is needed to quantify cycling frequency in real datasets and identify sufficient conditions for global convergence with large step sizes.

Abstract: Gradient descent (GD) on logistic regression has many fascinating properties.
When the dataset is linearly separable, it is known that the iterates converge
in direction to the maximum-margin separator regardless of how large the step
size is. In the non-separable case, however, it has been shown that GD can
exhibit a cycling behaviour even when the step sizes is still below the
stability threshold $2/\lambda$, where $\lambda$ is the largest eigenvalue of
the Hessian at the solution. This short paper explores whether restricting the
data to have equal magnitude is a sufficient condition for global convergence,
under any step size below the stability threshold. We prove that this is true
in a one dimensional space, but in higher dimensions cycling behaviour can
still occur. We hope to inspire further studies on quantifying how common these
cycles are in realistic datasets, as well as finding sufficient conditions to
guarantee global convergence with large step sizes.

</details>


### [215] [Generative Click-through Rate Prediction with Applications to Search Advertising](https://arxiv.org/abs/2507.11246)
*Lingwei Kong,Lu Wang,Changping Peng,Zhangang Lin,Ching Law,Jingping Shao*

Main category: cs.LG

TL;DR: A novel model combines generative and discriminative approaches for CTR prediction, improving accuracy through a two-stage training process.


<details>
  <summary>Details</summary>
Motivation: To enhance CTR prediction by leveraging generative models' expressive power beyond traditional discriminative methods.

Method: Two-stage training: 1) Generative pre-training for next-item prediction, 2) Fine-tuning within a discriminative CTR framework.

Result: Improved CTR prediction accuracy, validated by experiments and online A/B testing.

Conclusion: The model is effective and deployed on a major e-commerce platform, with plans to release code and dataset.

Abstract: Click-Through Rate (CTR) prediction models are integral to a myriad of
industrial settings, such as personalized search advertising. Current methods
typically involve feature extraction from users' historical behavior sequences
combined with product information, feeding into a discriminative model that is
trained on user feedback to estimate CTR. With the success of models such as
GPT, the potential for generative models to enrich expressive power beyond
discriminative models has become apparent. In light of this, we introduce a
novel model that leverages generative models to enhance the precision of CTR
predictions in discriminative models. To reconcile the disparate data
aggregation needs of both model types, we design a two-stage training process:
1) Generative pre-training for next-item prediction with the given item
category in user behavior sequences; 2) Fine-tuning the well-trained generative
model within a discriminative CTR prediction framework. Our method's efficacy
is substantiated through extensive experiments on a new dataset, and its
significant utility is further corroborated by online A/B testing results.
Currently, the model is deployed on one of the world's largest e-commerce
platforms, and we intend to release the associated code and dataset in the
future.

</details>


### [216] [LyAm: Robust Non-Convex Optimization for Stable Learning in Noisy Environments](https://arxiv.org/abs/2507.11262)
*Elmira Mirzabeigi,Sepehr Rezaee,Kourosh Parand*

Main category: cs.LG

TL;DR: LyAm is a novel optimizer combining Adam with Lyapunov stability to improve deep learning training robustness and convergence.


<details>
  <summary>Details</summary>
Motivation: Address noisy gradients and unstable convergence in deep neural networks, particularly for computer vision tasks.

Method: Integrates Adam's adaptive moment estimation with Lyapunov-based stability mechanisms for dynamic learning rate adjustment.

Result: Outperforms state-of-the-art optimizers in accuracy, convergence speed, and stability on datasets like CIFAR-10 and CIFAR-100.

Conclusion: LyAm is a robust optimizer for deep learning, offering theoretical guarantees and practical performance improvements.

Abstract: Training deep neural networks, particularly in computer vision tasks, often
suffers from noisy gradients and unstable convergence, which hinder performance
and generalization. In this paper, we propose LyAm, a novel optimizer that
integrates Adam's adaptive moment estimation with Lyapunov-based stability
mechanisms. LyAm dynamically adjusts the learning rate using Lyapunov stability
theory to enhance convergence robustness and mitigate training noise. We
provide a rigorous theoretical framework proving the convergence guarantees of
LyAm in complex, non-convex settings. Extensive experiments on like as CIFAR-10
and CIFAR-100 show that LyAm consistently outperforms state-of-the-art
optimizers in terms of accuracy, convergence speed, and stability, establishing
it as a strong candidate for robust deep learning optimization.

</details>


### [217] [Turning Sand to Gold: Recycling Data to Bridge On-Policy and Off-Policy Learning via Causal Bound](https://arxiv.org/abs/2507.11269)
*Tal Fiskus,Uri Shaham*

Main category: cs.LG

TL;DR: A novel DRL method using the Neyman-Rubin framework improves sample efficiency by reducing buffer size and boosting rewards.


<details>
  <summary>Details</summary>
Motivation: DRL agents demand high computational resources due to large training steps and replay buffers.

Method: Leverages the Neyman-Rubin framework to bound factual loss, reusing past value network outputs.

Result: Achieves up to 2,427% higher reward ratio and reduces buffer size by 96%.

Conclusion: The method significantly enhances DRL efficiency with minimal cost.

Abstract: Deep reinforcement learning (DRL) agents excel in solving complex
decision-making tasks across various domains. However, they often require a
substantial number of training steps and a vast experience replay buffer,
leading to significant computational and resource demands. To address these
challenges, we introduce a novel theoretical result that leverages the
Neyman-Rubin potential outcomes framework into DRL. Unlike most methods that
focus on bounding the counterfactual loss, we establish a causal bound on the
factual loss, which is analogous to the on-policy loss in DRL. This bound is
computed by storing past value network outputs in the experience replay buffer,
effectively utilizing data that is usually discarded. Extensive experiments
across the Atari 2600 and MuJoCo domains on various agents, such as DQN and
SAC, achieve up to 2,427% higher reward ratio, outperforming the same agents
without our proposed term, and reducing the experience replay buffer size by up
to 96%, significantly improving sample efficiency at negligible cost.

</details>


### [218] [Fast Last-Iterate Convergence of SGD in the Smooth Interpolation Regime](https://arxiv.org/abs/2507.11274)
*Amit Attia,Matan Schliserman,Uri Sherman,Tomer Koren*

Main category: cs.LG

TL;DR: The paper analyzes the convergence guarantees of SGD for smooth convex objectives in the interpolation regime, focusing on the last iterate's performance with large stepsizes. It provides improved convergence rates under specific conditions.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by applications in over-parameterized model training, continual learning, and solving linear systems, where understanding SGD's behavior is crucial.

Method: The authors analyze SGD on β-smooth convex loss functions with stepsize η ≤ 1/β, deriving expected excess risk bounds for the last iterate.

Result: The last iterate achieves a near-optimal rate of Õ(1/T + σ⋆/√T) with tuned stepsize, and O(1/√T) when noise at optimum is zero (σ⋆=0), improving prior results.

Conclusion: The work extends and improves existing convergence guarantees for SGD in the interpolation regime, offering tighter bounds and broader applicability beyond least squares regression.

Abstract: We study population convergence guarantees of stochastic gradient descent
(SGD) for smooth convex objectives in the interpolation regime, where the noise
at optimum is zero or near zero. The behavior of the last iterate of SGD in
this setting -- particularly with large (constant) stepsizes -- has received
growing attention in recent years due to implications for the training of
over-parameterized models, as well as to analyzing forgetting in continual
learning and to understanding the convergence of the randomized Kaczmarz method
for solving linear systems. We establish that after $T$ steps of SGD on
$\beta$-smooth convex loss functions with stepsize $\eta \leq 1/\beta$, the
last iterate exhibits expected excess risk $\widetilde{O}(1/(\eta
T^{1-\beta\eta/2}) + \eta T^{\beta\eta/2} \sigma_\star^2)$, where
$\sigma_\star^2$ denotes the variance of the stochastic gradients at the
optimum. In particular, for a well-tuned stepsize we obtain a near optimal
$\widetilde{O}(1/T + \sigma_\star/\sqrt{T})$ rate for the last iterate,
extending the results of Varre et al. (2021) beyond least squares regression;
and when $\sigma_\star=0$ we obtain a rate of $O(1/\sqrt{T})$ with
$\eta=1/\beta$, improving upon the best-known $O(T^{-1/4})$ rate recently
established by Evron et al. (2025) in the special case of realizable linear
regression.

</details>


### [219] [Guiding LLM Decision-Making with Fairness Reward Models](https://arxiv.org/abs/2507.11344)
*Zara Hall,Melanie Subbiah,Thomas P Zollo,Kathleen McKeown,Richard Zemel*

Main category: cs.LG

TL;DR: A framework for training a Fairness Reward Model (FRM) is proposed to mitigate bias in LLM reasoning for high-stakes decisions, improving fairness without sacrificing accuracy.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of unfair bias amplification in LLM reasoning for high-stakes decisions like bail or loans.

Method: Train a Fairness Reward Model (FRM) using weakly supervised, LLM-annotated examples to score and favor equitable reasoning.

Result: The FRM generalizes across tasks, domains, and model families, improving fairness while maintaining or surpassing baseline accuracy.

Conclusion: The proposed FRM enables trustworthy use of reasoning models in high-stakes decision-making by balancing fairness and accuracy.

Abstract: Large language models are increasingly used to support high-stakes decisions,
potentially influencing who is granted bail or receives a loan. Naive
chain-of-thought sampling can improve average decision accuracy, but has also
been shown to amplify unfair bias. To address this challenge and enable the
trustworthy use of reasoning models in high-stakes decision-making, we propose
a framework for training a generalizable Fairness Reward Model (FRM). Our model
assigns a fairness score to LLM reasoning, enabling the system to down-weight
biased trajectories and favor equitable ones when aggregating decisions across
reasoning chains. We show that a single Fairness Reward Model, trained on
weakly supervised, LLM-annotated examples of biased versus unbiased reasoning,
transfers across tasks, domains, and model families without additional
fine-tuning. Applied to real-world decision-making tasks including recidivism
prediction and social media moderation, we show that our approach consistently
improves fairness while matching, or even surpassing, baseline accuracy.

</details>


### [220] [Neurosymbolic Reasoning Shortcuts under the Independence Assumption](https://arxiv.org/abs/2507.11357)
*Emile van Krieken,Pasquale Minervini,Edoardo Ponti,Antonio Vergari*

Main category: cs.LG

TL;DR: The paper challenges the independence assumption in neurosymbolic (NeSy) predictors, showing it limits uncertainty modeling and causes reasoning shortcuts.


<details>
  <summary>Details</summary>
Motivation: To address skepticism about the impact of the independence assumption in NeSy systems and demonstrate its limitations.

Method: Formal analysis of the independence assumption's effects on uncertainty representation and reasoning shortcuts.

Result: Independence among symbolic concepts prevents models from representing uncertainty over certain combinations, leading to reasoning shortcuts.

Conclusion: The independence assumption in NeSy predictors hinders accurate uncertainty modeling and awareness of reasoning shortcuts.

Abstract: The ubiquitous independence assumption among symbolic concepts in
neurosymbolic (NeSy) predictors is a convenient simplification: NeSy predictors
use it to speed up probabilistic reasoning. Recent works like van Krieken et
al. (2024) and Marconato et al. (2024) argued that the independence assumption
can hinder learning of NeSy predictors and, more crucially, prevent them from
correctly modelling uncertainty. There is, however, scepticism in the NeSy
community around the scenarios in which the independence assumption actually
limits NeSy systems (Faronius and Dos Martires, 2025). In this work, we settle
this question by formally showing that assuming independence among symbolic
concepts entails that a model can never represent uncertainty over certain
concept combinations. Thus, the model fails to be aware of reasoning shortcuts,
i.e., the pathological behaviour of NeSy predictors that predict correct
downstream tasks but for the wrong reasons.

</details>


### [221] [Local Pairwise Distance Matching for Backpropagation-Free Reinforcement Learning](https://arxiv.org/abs/2507.11367)
*Daniel Tanneberg*

Main category: cs.LG

TL;DR: Proposes a backpropagation-free method for training neural networks in RL using local, layer-wise losses during forward pass, achieving competitive performance and improved stability.


<details>
  <summary>Details</summary>
Motivation: Addresses issues with backpropagation (BP) in RL, such as vanishing/exploding gradients and the need to store activations, by introducing a local training approach.

Method: Trains each layer using local signals during forward pass, leveraging multi-dimensional scaling principles and optional reward-driven guidance, eliminating backward passes.

Result: Competitive performance to BP-based methods, with enhanced stability, consistency, and better performance in challenging environments.

Conclusion: The proposed method offers a viable alternative to BP in RL, improving learning stability and performance without backward passes.

Abstract: Training neural networks with reinforcement learning (RL) typically relies on
backpropagation (BP), necessitating storage of activations from the forward
pass for subsequent backward updates. Furthermore, backpropagating error
signals through multiple layers often leads to vanishing or exploding
gradients, which can degrade learning performance and stability. We propose a
novel approach that trains each layer of the neural network using local signals
during the forward pass in RL settings. Our approach introduces local,
layer-wise losses leveraging the principle of matching pairwise distances from
multi-dimensional scaling, enhanced with optional reward-driven guidance. This
method allows each hidden layer to be trained using local signals computed
during forward propagation, thus eliminating the need for backward passes and
storing intermediate activations. Our experiments, conducted with policy
gradient methods across common RL benchmarks, demonstrate that this
backpropagation-free method achieves competitive performance compared to their
classical BP-based counterpart. Additionally, the proposed method enhances
stability and consistency within and across runs, and improves performance
especially in challenging environments.

</details>


### [222] [Step-wise Policy for Rare-tool Knowledge (SPaRK): Offline RL that Drives Diverse Tool Use in LLMs](https://arxiv.org/abs/2507.11371)
*Gabriel Bo,Koa Chang,Justin Gu*

Main category: cs.LG

TL;DR: SPaRK is a reinforcement learning framework for diverse tool usage in LLMs, using a rarity-first strategy to enhance exploration without losing accuracy.


<details>
  <summary>Details</summary>
Motivation: To teach LLMs to explore diverse tool usage beyond conventional methods, improving reasoning capabilities.

Method: Uses a dual-objective reward system (answer quality and tool diversity) with offline PPO on synthetic MMLU-Pro data, guided by a GPT-4o judge.

Result: Achieves competitive performance on MMLU-Pro with higher tool selection entropy than baselines.

Conclusion: Explicit tool diversity enhances reasoning without sacrificing accuracy.

Abstract: We present Step-wise Policy for Rare-tool Knowledge (SPaRK), a novel
reinforcement learning framework that teaches large language models to explore
diverse tool usage patterns beyond conventional high-temperature sampling.
Building on recent advances in step-wise reinforcement learning, we introduce a
dual-objective reward system that simultaneously optimizes for answer quality
and tool diversity, training a Llama-3.1 8B model through offline PPO on
synthetically generated trajectories from the MMLU-Pro dataset. Our approach
uniquely employs a rarity-first exploitation strategy where a GPT-4o judge
scores candidate actions across eight distinct tools plus chain-of-thought
reasoning, with the policy favoring less-frequently used but still viable tools
to encourage systematic exploration. Empirical results demonstrate that SPaRK
achieves competitive performance across 14 MMLU-Pro categories while exhibiting
significantly higher entropy in tool selection compared to both baseline and
supervised fine-tuning approaches, suggesting that algorithmic exploration
through explicit tool diversity can enhance reasoning capabilities without
sacrificing accuracy.

</details>


### [223] [A Neural Network Model of Complementary Learning Systems: Pattern Separation and Completion for Continual Learning](https://arxiv.org/abs/2507.11393)
*James P Jun,Vijay Marupudi,Raj Sanjay Shah,Sashank Varma*

Main category: cs.LG

TL;DR: A neural model combining VAEs and MHNs reduces catastrophic forgetting in continual learning, achieving ~90% accuracy on Split-MNIST by mimicking brain functions of pattern separation and completion.


<details>
  <summary>Details</summary>
Motivation: Address catastrophic forgetting in neural networks by emulating the brain's Complementary Learning Systems (CLS) theory.

Method: Combine variational autoencoders (VAEs) for pattern completion and Modern Hopfield networks (MHNs) for pattern separation into a continual learning model.

Result: Achieves ~90% accuracy on Split-MNIST, significantly reducing forgetting, with VAEs handling completion and MHNs separation.

Conclusion: The model provides a scalable template for continual learning, bridging biological and artificial memory systems.

Abstract: Learning new information without forgetting prior knowledge is central to
human intelligence. In contrast, neural network models suffer from catastrophic
forgetting: a significant degradation in performance on previously learned
tasks when acquiring new information. The Complementary Learning Systems (CLS)
theory offers an explanation for this human ability, proposing that the brain
has distinct systems for pattern separation (encoding distinct memories) and
pattern completion (retrieving complete memories from partial cues). To capture
these complementary functions, we leverage the representational generalization
capabilities of variational autoencoders (VAEs) and the robust memory storage
properties of Modern Hopfield networks (MHNs), combining them into a neurally
plausible continual learning model. We evaluate this model on the Split-MNIST
task, a popular continual learning benchmark, and achieve close to
state-of-the-art accuracy (~90%), substantially reducing forgetting.
Representational analyses empirically confirm the functional dissociation: the
VAE underwrites pattern completion, while the MHN drives pattern separation. By
capturing pattern separation and completion in scalable architectures, our work
provides a functional template for modeling memory consolidation,
generalization, and continual learning in both biological and artificial
systems.

</details>


### [224] [Robust-Multi-Task Gradient Boosting](https://arxiv.org/abs/2507.11411)
*Seyedsaman Emami,Gonzalo Martínez-Muñoz,Daniel Hernández-Lobato*

Main category: cs.LG

TL;DR: R-MTGB is a robust multi-task gradient boosting framework that handles outlier tasks and improves performance by learning shared patterns, partitioning tasks, and fine-tuning predictors.


<details>
  <summary>Details</summary>
Motivation: Real-world MTL often includes outlier tasks that harm performance, requiring a method to handle task heterogeneity.

Method: R-MTGB uses three blocks: learning shared patterns, partitioning tasks into outliers/non-outliers, and fine-tuning task-specific predictors.

Result: R-MTGB isolates outliers, transfers knowledge, and reduces prediction errors, achieving overall performance gains.

Conclusion: R-MTGB is robust, adaptable, and reliable in challenging MTL environments.

Abstract: Multi-task learning (MTL) has shown effectiveness in exploiting shared
information across tasks to improve generalization. MTL assumes tasks share
similarities that can improve performance. In addition, boosting algorithms
have demonstrated exceptional performance across diverse learning problems,
primarily due to their ability to focus on hard-to-learn instances and
iteratively reduce residual errors. This makes them a promising approach for
learning multi-task problems. However, real-world MTL scenarios often involve
tasks that are not well-aligned (known as outlier or adversarial tasks), which
do not share beneficial similarities with others and can, in fact, deteriorate
the performance of the overall model. To overcome this challenge, we propose
Robust-Multi-Task Gradient Boosting (R-MTGB), a novel boosting framework that
explicitly models and adapts to task heterogeneity during training. R-MTGB
structures the learning process into three sequential blocks: (1) learning
shared patterns, (2) partitioning tasks into outliers and non-outliers with
regularized parameters, and (3) fine-tuning task-specific predictors. This
architecture enables R-MTGB to automatically detect and penalize outlier tasks
while promoting effective knowledge transfer among related tasks. Our method
integrates these mechanisms seamlessly within gradient boosting, allowing
robust handling of noisy or adversarial tasks without sacrificing accuracy.
Extensive experiments on both synthetic benchmarks and real-world datasets
demonstrate that our approach successfully isolates outliers, transfers
knowledge, and consistently reduces prediction errors for each task
individually, and achieves overall performance gains across all tasks. These
results highlight robustness, adaptability, and reliable convergence of R-MTGB
in challenging MTL environments.

</details>


### [225] [Toward Improving fNIRS Classification: A Study on Activation Functions in Deep Neural Architectures](https://arxiv.org/abs/2507.11436)
*Behtom Adeli,John McLinden,Pankaj Pandey,Ming Shao,Yalda Shahriari*

Main category: cs.LG

TL;DR: The study evaluates activation functions for deep learning in fNIRS tasks, finding symmetrical functions like Tanh and Abs(x) outperform ReLU in certain architectures.


<details>
  <summary>Details</summary>
Motivation: The impact of activation functions on deep learning performance in fNIRS is underexplored, despite challenges like nonlinearity and low SNR.

Method: Tested conventional and field-specific activation functions on multiple DL architectures (fNIRSNet, AbsoluteNet, MDNN, shallowConvNet) using standardized preprocessing and training.

Result: Symmetrical activation functions (Tanh, Abs(x)) outperformed ReLU in some architectures; Modified Absolute Function (MAF) further supported symmetry's role.

Conclusion: Proper activation function selection, aligned with fNIRS signal characteristics, is crucial for performance gains.

Abstract: Activation functions are critical to the performance of deep neural networks,
particularly in domains such as functional near-infrared spectroscopy (fNIRS),
where nonlinearity, low signal-to-noise ratio (SNR), and signal variability
poses significant challenges to model accuracy. However, the impact of
activation functions on deep learning (DL) performance in the fNIRS domain
remains underexplored and lacks systematic investigation in the current
literature. This study evaluates a range of conventional and field-specific
activation functions for fNIRS classification tasks using multiple deep
learning architectures, including the domain-specific fNIRSNet, AbsoluteNet,
MDNN, and shallowConvNet (as the baseline), all tested on a single dataset
recorded during an auditory task. To ensure fair a comparison, all networks
were trained and tested using standardized preprocessing and consistent
training parameters. The results show that symmetrical activation functions
such as Tanh and the Absolute value function Abs(x) can outperform commonly
used functions like the Rectified Linear Unit (ReLU), depending on the
architecture. Additionally, a focused analysis of the role of symmetry was
conducted using a Modified Absolute Function (MAF), with results further
supporting the effectiveness of symmetrical activation functions on performance
gains. These findings underscore the importance of selecting proper activation
functions that align with the signal characteristics of fNIRS data.

</details>


### [226] [Data Augmentation in Time Series Forecasting through Inverted Framework](https://arxiv.org/abs/2507.11439)
*Hongming Tan,Ting Chen,Ruochong Jin,Wai Kin Chan*

Main category: cs.LG

TL;DR: The paper introduces DAIF, a novel data augmentation method for the iTransformer model to address its limitations in capturing temporal interdependency and handling noise in nonsignificant variable correlations.


<details>
  <summary>Details</summary>
Motivation: The inverted framework of iTransformer, while effective for multivariate correlation, diminishes temporal interdependency and introduces noise in cases of nonsignificant variable correlation.

Method: The authors propose DAIF, featuring two strategies: Frequency Filtering and Cross-variation Patching, tailored for the inverted framework in MTS forecasting.

Result: Experiments show DAIF's effectiveness across multiple datasets and inverted models.

Conclusion: DAIF successfully addresses the limitations of the inverted framework, enhancing its performance in MTS forecasting.

Abstract: Currently, iTransformer is one of the most popular and effective models for
multivariate time series (MTS) forecasting. Thanks to its inverted framework,
iTransformer effectively captures multivariate correlation. However, the
inverted framework still has some limitations. It diminishes temporal
interdependency information, and introduces noise in cases of nonsignificant
variable correlation. To address these limitations, we introduce a novel data
augmentation method on inverted framework, called DAIF. Unlike previous data
augmentation methods, DAIF stands out as the first real-time augmentation
specifically designed for the inverted framework in MTS forecasting. We first
define the structure of the inverted sequence-to-sequence framework, then
propose two different DAIF strategies, Frequency Filtering and Cross-variation
Patching to address the existing challenges of the inverted framework.
Experiments across multiple datasets and inverted models have demonstrated the
effectiveness of our DAIF.

</details>


### [227] [LRMR: LLM-Driven Relational Multi-node Ranking for Lymph Node Metastasis Assessment in Rectal Cancer](https://arxiv.org/abs/2507.11457)
*Yaoxian Dong,Yifan Gao,Haoyue Li,Yanfen Cui,Xin Gao*

Main category: cs.LG

TL;DR: LRMR, an LLM-driven framework, improves lymph node metastasis assessment in rectal cancer by combining visual analysis with relational ranking, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Conventional MRI and existing AI models lack interpretability and patient-level context, limiting their clinical utility.

Method: LRMR uses a two-stage approach: a multimodal LLM analyzes LN images to generate structured reports, followed by a text-based LLM performing pairwise comparisons for risk ranking.

Result: LRMR achieved an AUC of 0.7917 and F1-score of 0.7200, surpassing deep learning baselines like ResNet50.

Conclusion: The two-stage LLM framework provides a more interpretable and effective solution for LN metastasis assessment in rectal cancer.

Abstract: Accurate preoperative assessment of lymph node (LN) metastasis in rectal
cancer guides treatment decisions, yet conventional MRI evaluation based on
morphological criteria shows limited diagnostic performance. While some
artificial intelligence models have been developed, they often operate as black
boxes, lacking the interpretability needed for clinical trust. Moreover, these
models typically evaluate nodes in isolation, overlooking the patient-level
context. To address these limitations, we introduce LRMR, an LLM-Driven
Relational Multi-node Ranking framework. This approach reframes the diagnostic
task from a direct classification problem into a structured reasoning and
ranking process. The LRMR framework operates in two stages. First, a multimodal
large language model (LLM) analyzes a composite montage image of all LNs from a
patient, generating a structured report that details ten distinct radiological
features. Second, a text-based LLM performs pairwise comparisons of these
reports between different patients, establishing a relative risk ranking based
on the severity and number of adverse features. We evaluated our method on a
retrospective cohort of 117 rectal cancer patients. LRMR achieved an area under
the curve (AUC) of 0.7917 and an F1-score of 0.7200, outperforming a range of
deep learning baselines, including ResNet50 (AUC 0.7708). Ablation studies
confirmed the value of our two main contributions: removing the relational
ranking stage or the structured prompting stage led to a significant
performance drop, with AUCs falling to 0.6875 and 0.6458, respectively. Our
work demonstrates that decoupling visual perception from cognitive reasoning
through a two-stage LLM framework offers a powerful, interpretable, and
effective new paradigm for assessing lymph node metastasis in rectal cancer.

</details>


### [228] [D3FL: Data Distribution and Detrending for Robust Federated Learning in Non-linear Time-series Data](https://arxiv.org/abs/2507.11471)
*Harsha Varun Marisetty,Manik Gupta,Yogesh Simmhan*

Main category: cs.LG

TL;DR: The paper explores federated learning (FL) for forecasting non-linear, non-stationary IoT time-series data, comparing it to centralized methods and evaluating detrending techniques.


<details>
  <summary>Details</summary>
Motivation: Centralized IoT data analysis introduces delays and costs; FL offers a distributed alternative. Non-linear data variations impact prediction accuracy, necessitating investigation.

Method: Used synthetic and real-world datasets with non-linear distributions, trained LSTM models in FL and centralized setups, and tested detrending techniques.

Result: FL underperforms centralized methods for non-linear data but improves with detrending.

Conclusion: Detrending enhances FL performance for non-linear time-series data, though FL still lags behind centralized approaches.

Abstract: With advancements in computing and communication technologies, the Internet
of Things (IoT) has seen significant growth. IoT devices typically collect data
from various sensors, such as temperature, humidity, and energy meters. Much of
this data is temporal in nature. Traditionally, data from IoT devices is
centralized for analysis, but this approach introduces delays and increased
communication costs. Federated learning (FL) has emerged as an effective
alternative, allowing for model training across distributed devices without the
need to centralize data. In many applications, such as smart home energy and
environmental monitoring, the data collected by IoT devices across different
locations can exhibit significant variation in trends and seasonal patterns.
Accurately forecasting such non-stationary, non-linear time-series data is
crucial for applications like energy consumption estimation and weather
forecasting. However, these data variations can severely impact prediction
accuracy. The key contributions of this paper are: (1) Investigating how
non-linear, non-stationary time-series data distributions, like generalized
extreme value (gen-extreme) and log norm distributions, affect FL performance.
(2) Analyzing how different detrending techniques for non-linear time-series
data influence the forecasting model's performance in a FL setup. We generated
several synthetic time-series datasets using non-linear data distributions and
trained an LSTM-based forecasting model using both centralized and FL
approaches. Additionally, we evaluated the impact of detrending on real-world
datasets with non-linear time-series data distributions. Our experimental
results show that: (1) FL performs worse than centralized approaches when
dealing with non-linear data distributions. (2) The use of appropriate
detrending techniques improves FL performance, reducing loss across different
data distributions.

</details>


### [229] [Exploring the robustness of TractOracle methods in RL-based tractography](https://arxiv.org/abs/2507.11486)
*Jeremi Levesque,Antoine Théberge,Maxime Descoteaux,Pierre-Marc Jodoin*

Main category: cs.LG

TL;DR: The paper explores extensions of the TractOracle-RL framework for tractography, introducing Iterative Reward Training (IRT) to improve accuracy and anatomical validity.


<details>
  <summary>Details</summary>
Motivation: To enhance tractography performance by integrating advanced RL techniques and anatomical priors, reducing false positives and improving reliability.

Method: Extends TractOracle-RL with four RL advancements and introduces IRT, a novel training scheme using bundle filtering for iterative refinement.

Result: RL methods with oracle feedback outperform traditional techniques in accuracy and anatomical validity across diverse datasets.

Conclusion: Combining RL frameworks with oracle guidance yields robust tractography, with IRT showing promise for further improvements.

Abstract: Tractography algorithms leverage diffusion MRI to reconstruct the fibrous
architecture of the brain's white matter. Among machine learning approaches,
reinforcement learning (RL) has emerged as a promising framework for
tractography, outperforming traditional methods in several key aspects.
TractOracle-RL, a recent RL-based approach, reduces false positives by
incorporating anatomical priors into the training process via a reward-based
mechanism. In this paper, we investigate four extensions of the original
TractOracle-RL framework by integrating recent advances in RL, and we evaluate
their performance across five diverse diffusion MRI datasets. Results
demonstrate that combining an oracle with the RL framework consistently leads
to robust and reliable tractography, regardless of the specific method or
dataset used. We also introduce a novel RL training scheme called Iterative
Reward Training (IRT), inspired by the Reinforcement Learning from Human
Feedback (RLHF) paradigm. Instead of relying on human input, IRT leverages
bundle filtering methods to iteratively refine the oracle's guidance throughout
training. Experimental results show that RL methods trained with oracle
feedback significantly outperform widely used tractography techniques in terms
of accuracy and anatomical validity.

</details>


### [230] [A parametric activation function based on Wendland RBF](https://arxiv.org/abs/2507.11493)
*Majid Darehmiraki*

Main category: cs.LG

TL;DR: A novel parametric activation function using Wendland RBFs is introduced for deep learning, offering tunable locality, improved gradients, and stability. It outperforms traditional functions in some tasks.


<details>
  <summary>Details</summary>
Motivation: Address limitations of ReLU, sigmoid, and tanh by leveraging Wendland RBFs' compact support and smoothness for better deep learning performance.

Method: Combine standard Wendland RBFs with linear and exponential terms to create an enhanced activation function. Validate through theoretical analysis and experiments on synthetic and benchmark datasets.

Result: Superior accuracy in regression tasks and competitive performance on MNIST/Fashion-MNIST, with computational efficiency.

Conclusion: Wendland activations bridge RBF theory and deep learning, improving generalization and suggesting future hybrid architectures.

Abstract: This paper introduces a novel parametric activation function based on
Wendland radial basis functions (RBFs) for deep neural networks. Wendland RBFs,
known for their compact support, smoothness, and positive definiteness in
approximation theory, are adapted to address limitations of traditional
activation functions like ReLU, sigmoid, and tanh. The proposed enhanced
Wendland activation combines a standard Wendland component with linear and
exponential terms, offering tunable locality, improved gradient propagation,
and enhanced stability during training. Theoretical analysis highlights its
mathematical properties, including smoothness and adaptability, while empirical
experiments on synthetic tasks (e.g., sine wave approximation) and benchmark
datasets (MNIST, Fashion-MNIST) demonstrate competitive performance. Results
show that the Wendland-based activation achieves superior accuracy in certain
scenarios, particularly in regression tasks, while maintaining computational
efficiency. The study bridges classical RBF theory with modern deep learning,
suggesting that Wendland activations can mitigate overfitting and improve
generalization through localized, smooth transformations. Future directions
include hybrid architectures and domain-specific adaptations.

</details>


### [231] [Langevin Flows for Modeling Neural Latent Dynamics](https://arxiv.org/abs/2507.11531)
*Yue Song,T. Anderson Keller,Yisong Yue,Pietro Perona,Max Welling*

Main category: cs.LG

TL;DR: LangevinFlow, a physics-inspired VAE, models neural dynamics using Langevin equation, outperforming baselines in synthetic and real datasets.


<details>
  <summary>Details</summary>
Motivation: To capture intrinsic and external influences in neural population dynamics with physical priors.

Method: Uses a VAE with Langevin dynamics, a recurrent encoder, Transformer decoder, and oscillator-based potential function.

Result: Outperforms baselines in synthetic Lorenz attractor data and NLB datasets, achieving high accuracy in firing rates and behavioral metrics.

Conclusion: LangevinFlow is a flexible, high-performing framework for neural dynamics modeling.

Abstract: Neural populations exhibit latent dynamical structures that drive
time-evolving spiking activities, motivating the search for models that capture
both intrinsic network dynamics and external unobserved influences. In this
work, we introduce LangevinFlow, a sequential Variational Auto-Encoder where
the time evolution of latent variables is governed by the underdamped Langevin
equation. Our approach incorporates physical priors -- such as inertia,
damping, a learned potential function, and stochastic forces -- to represent
both autonomous and non-autonomous processes in neural systems. Crucially, the
potential function is parameterized as a network of locally coupled
oscillators, biasing the model toward oscillatory and flow-like behaviors
observed in biological neural populations. Our model features a recurrent
encoder, a one-layer Transformer decoder, and Langevin dynamics in the latent
space. Empirically, our method outperforms state-of-the-art baselines on
synthetic neural populations generated by a Lorenz attractor, closely matching
ground-truth firing rates. On the Neural Latents Benchmark (NLB), the model
achieves superior held-out neuron likelihoods (bits per spike) and forward
prediction accuracy across four challenging datasets. It also matches or
surpasses alternative methods in decoding behavioral metrics such as hand
velocity. Overall, this work introduces a flexible, physics-inspired,
high-performing framework for modeling complex neural population dynamics and
their unobserved influences.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [232] [Latent Space Consistency for Sparse-View CT Reconstruction](https://arxiv.org/abs/2507.11152)
*Duoyou Chen,Yunqing Chen,Can Zhang,Zhou Wang,Cheng Chen,Ruoxiu Xiao*

Main category: eess.IV

TL;DR: The paper introduces CLS-DM, a model for 3D CT reconstruction from sparse 2D X-ray images using latent space alignment and contrastive learning, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: CT imaging faces challenges like high time and radiation costs. Sparse-view reconstruction offers a solution, but aligning 2D X-ray and 3D CT latent spaces is difficult.

Method: Proposes CLS-DM, incorporating cross-modal feature contrastive learning to align latent spaces and extract 3D information from 2D X-rays.

Result: CLS-DM achieves superior performance in voxel-level metrics (PSNR, SSIM) on LIDC-IDRI and CTSpine1K datasets.

Conclusion: CLS-DM enhances sparse X-ray CT reconstruction and can generalize to other cross-modal tasks. Code is publicly available for further research.

Abstract: Computed Tomography (CT) is a widely utilized imaging modality in clinical
settings. Using densely acquired rotational X-ray arrays, CT can capture 3D
spatial features. However, it is confronted with challenged such as significant
time consumption and high radiation exposure. CT reconstruction methods based
on sparse-view X-ray images have garnered substantial attention from
researchers as they present a means to mitigate costs and risks. In recent
years, diffusion models, particularly the Latent Diffusion Model (LDM), have
demonstrated promising potential in the domain of 3D CT reconstruction.
Nonetheless, due to the substantial differences between the 2D latent
representation of X-ray modalities and the 3D latent representation of CT
modalities, the vanilla LDM is incapable of achieving effective alignment
within the latent space. To address this issue, we propose the Consistent
Latent Space Diffusion Model (CLS-DM), which incorporates cross-modal feature
contrastive learning to efficiently extract latent 3D information from 2D X-ray
images and achieve latent space alignment between modalities. Experimental
results indicate that CLS-DM outperforms classical and state-of-the-art
generative models in terms of standard voxel-level metrics (PSNR, SSIM) on the
LIDC-IDRI and CTSpine1K datasets. This methodology not only aids in enhancing
the effectiveness and economic viability of sparse X-ray reconstructed CT but
can also be generalized to other cross-modal transformation tasks, such as
text-to-image synthesis. We have made our code publicly available at
https://anonymous.4open.science/r/CLS-DM-50D6/ to facilitate further research
and applications in other domains.

</details>


### [233] [3D Magnetic Inverse Routine for Single-Segment Magnetic Field Images](https://arxiv.org/abs/2507.11293)
*J. Senthilnath,Chen Hao,F. C. Wellstood*

Main category: eess.IV

TL;DR: The paper introduces 3D MIR, a method combining deep learning and physics-based optimization to recover 3D current flow parameters in semiconductor packaging using magnetic field images.


<details>
  <summary>Details</summary>
Motivation: Accurate 3D information recovery is essential for non-destructive testing to localize circuit defects in semiconductor packaging.

Method: 3D MIR uses a CNN for initial predictions, spatial-physics constraints for parameter estimates, and optimization to refine parameters.

Result: The method accurately recovers 3D information, setting a new benchmark for magnetic image reconstruction.

Conclusion: Combining deep learning and physics-driven optimization shows great potential for practical applications in semiconductor testing.

Abstract: In semiconductor packaging, accurately recovering 3D information is crucial
for non-destructive testing (NDT) to localize circuit defects. This paper
presents a novel approach called the 3D Magnetic Inverse Routine (3D MIR),
which leverages Magnetic Field Images (MFI) to retrieve the parameters for the
3D current flow of a single-segment. The 3D MIR integrates a deep learning
(DL)-based Convolutional Neural Network (CNN), spatial-physics-based
constraints, and optimization techniques. The method operates in three stages:
i) The CNN model processes the MFI data to predict ($\ell/z_o$), where $\ell$
is the wire length and $z_o$ is the wire's vertical depth beneath the magnetic
sensors and classify segment type ($c$). ii) By leveraging
spatial-physics-based constraints, the routine provides initial estimates for
the position ($x_o$, $y_o$, $z_o$), length ($\ell$), current ($I$), and current
flow direction (positive or negative) of the current segment. iii) An optimizer
then adjusts these five parameters ($x_o$, $y_o$, $z_o$, $\ell$, $I$) to
minimize the difference between the reconstructed MFI and the actual MFI. The
results demonstrate that the 3D MIR method accurately recovers 3D information
with high precision, setting a new benchmark for magnetic image reconstruction
in semiconductor packaging. This method highlights the potential of combining
DL and physics-driven optimization in practical applications.

</details>


### [234] [HANS-Net: Hyperbolic Convolution and Adaptive Temporal Attention for Accurate and Generalizable Liver and Tumor Segmentation in CT Imaging](https://arxiv.org/abs/2507.11325)
*Arefin Ittesafun Abian,Ripon Kumar Debnath,Md. Abdur Rahman,Mohaimenul Azam Khan Raiaan,Md Rafiqul Islam,Asif Karim,Reem E. Mohamed,Sami Azam*

Main category: eess.IV

TL;DR: HANS-Net is a novel segmentation framework for liver and tumor segmentation on CT images, combining hyperbolic convolutions, wavelet-inspired decomposition, synaptic plasticity, and neural representation. It achieves high accuracy and generalization across datasets.


<details>
  <summary>Details</summary>
Motivation: Accurate liver and tumor segmentation is crucial for diagnosis and treatment but is challenging due to anatomical complexity, tumor variability, and limited annotated data.

Method: HANS-Net integrates hyperbolic convolutions, wavelet decomposition, synaptic plasticity, neural representation, uncertainty-aware Monte Carlo dropout, and temporal attention for robust segmentation.

Result: Achieves 93.26% Dice score on LiTS and 87.45% on 3D-IRCADb-01, with strong generalization and anatomical consistency.

Conclusion: HANS-Net is effective and robust for accurate, confident, and consistent liver and tumor segmentation.

Abstract: Accurate liver and tumor segmentation on abdominal CT images is critical for
reliable diagnosis and treatment planning, but remains challenging due to
complex anatomical structures, variability in tumor appearance, and limited
annotated data. To address these issues, we introduce Hyperbolic-convolutions
Adaptive-temporal-attention with Neural-representation and Synaptic-plasticity
Network (HANS-Net), a novel segmentation framework that synergistically
combines hyperbolic convolutions for hierarchical geometric representation, a
wavelet-inspired decomposition module for multi-scale texture learning, a
biologically motivated synaptic plasticity mechanism for adaptive feature
enhancement, and an implicit neural representation branch to model fine-grained
and continuous anatomical boundaries. Additionally, we incorporate
uncertainty-aware Monte Carlo dropout to quantify prediction confidence and
lightweight temporal attention to improve inter-slice consistency without
sacrificing efficiency. Extensive evaluations of the LiTS dataset demonstrate
that HANS-Net achieves a mean Dice score of 93.26%, an IoU of 88.09%, an
average symmetric surface distance (ASSD) of 0.72 mm, and a volume overlap
error (VOE) of 11.91%. Furthermore, cross-dataset validation on the
3D-IRCADb-01 dataset obtains an average Dice of 87.45%, IoU of 80.30%, ASSD of
1.525 mm, and VOE of 19.71%, indicating strong generalization across different
datasets. These results confirm the effectiveness and robustness of HANS-Net in
providing anatomically consistent, accurate, and confident liver and tumor
segmentation.

</details>


### [235] [Comparative Analysis of Vision Transformers and Traditional Deep Learning Approaches for Automated Pneumonia Detection in Chest X-Rays](https://arxiv.org/abs/2507.10589)
*Gaurav Singh*

Main category: eess.IV

TL;DR: The study compares traditional machine learning and deep learning for pneumonia detection in CXRs, finding Vision Transformers (especially Cross-ViT) superior with 88.25% accuracy and 99.42% recall.


<details>
  <summary>Details</summary>
Motivation: Pneumonia, especially from COVID-19, requires rapid, accurate diagnosis, prompting the need for effective automated detection methods.

Method: Evaluated traditional ML (PCA, Logistic Regression, SVM) and deep learning (CNNs, Vision Transformers) on 5,856 pediatric CXR images.

Result: Cross-ViT outperformed others with 88.25% accuracy and 99.42% recall, showing architectural choices matter more than model size.

Conclusion: Vision Transformers, particularly Cross-ViT, are promising for automated pneumonia detection, balancing accuracy and efficiency.

Abstract: Pneumonia, particularly when induced by diseases like COVID-19, remains a
critical global health challenge requiring rapid and accurate diagnosis. This
study presents a comprehensive comparison of traditional machine learning and
state-of-the-art deep learning approaches for automated pneumonia detection
using chest X-rays (CXRs). We evaluate multiple methodologies, ranging from
conventional machine learning techniques (PCA-based clustering, Logistic
Regression, and Support Vector Classification) to advanced deep learning
architectures including Convolutional Neural Networks (Modified LeNet,
DenseNet-121) and various Vision Transformer (ViT) implementations (Deep-ViT,
Compact Convolutional Transformer, and Cross-ViT). Using a dataset of 5,856
pediatric CXR images, we demonstrate that Vision Transformers, particularly the
Cross-ViT architecture, achieve superior performance with 88.25% accuracy and
99.42% recall, surpassing traditional CNN approaches. Our analysis reveals that
architectural choices impact performance more significantly than model size,
with Cross-ViT's 75M parameters outperforming larger models. The study also
addresses practical considerations including computational efficiency, training
requirements, and the critical balance between precision and recall in medical
diagnostics. Our findings suggest that Vision Transformers offer a promising
direction for automated pneumonia detection, potentially enabling more rapid
and accurate diagnosis during health crises.

</details>


### [236] [Focus on Texture: Rethinking Pre-training in Masked Autoencoders for Medical Image Classification](https://arxiv.org/abs/2507.10869)
*Chetan Madan,Aarjav Satia,Soumen Basu,Pankaj Gupta,Usha Dutta,Chetan Arora*

Main category: eess.IV

TL;DR: GLCM-MAE improves medical image representation by using GLCM-based loss instead of MSE, outperforming state-of-the-art in cancer and disease detection tasks.


<details>
  <summary>Details</summary>
Motivation: MSE-based MAEs blur textures crucial for medical imaging; GLCM preserves morphological features better.

Method: Proposes GLCM-MAE, a framework using GLCM-based reconstruction loss for pre-training, with a differentiable GLCM loss formulation.

Result: Outperforms SOTA in gallbladder cancer (2.1%), breast cancer (3.1%), pneumonia (0.5%), and COVID (0.6%) detection.

Conclusion: GLCM-MAE enhances medical image representation, proving GLCM loss superior to MSE for texture-sensitive tasks.

Abstract: Masked Autoencoders (MAEs) have emerged as a dominant strategy for
self-supervised representation learning in natural images, where models are
pre-trained to reconstruct masked patches with a pixel-wise mean squared error
(MSE) between original and reconstructed RGB values as the loss. We observe
that MSE encourages blurred image re-construction, but still works for natural
images as it preserves dominant edges. However, in medical imaging, when the
texture cues are more important for classification of a visual abnormality, the
strategy fails. Taking inspiration from Gray Level Co-occurrence Matrix (GLCM)
feature in Radiomics studies, we propose a novel MAE based pre-training
framework, GLCM-MAE, using reconstruction loss based on matching GLCM. GLCM
captures intensity and spatial relationships in an image, hence proposed loss
helps preserve morphological features. Further, we propose a novel formulation
to convert matching GLCM matrices into a differentiable loss function. We
demonstrate that unsupervised pre-training on medical images with the proposed
GLCM loss improves representations for downstream tasks. GLCM-MAE outperforms
the current state-of-the-art across four tasks - gallbladder cancer detection
from ultrasound images by 2.1%, breast cancer detection from ultrasound by
3.1%, pneumonia detection from x-rays by 0.5%, and COVID detection from CT by
0.6%. Source code and pre-trained models are available at:
https://github.com/ChetanMadan/GLCM-MAE.

</details>


### [237] [U-RWKV: Lightweight medical image segmentation with direction-adaptive RWKV](https://arxiv.org/abs/2507.11415)
*Hongbo Ye,Fenghe Tang,Peiang Zhao,Zhen Huang,Dexin Zhao,Minghao Bian,S. Kevin Zhou*

Main category: eess.IV

TL;DR: U-RWKV is a novel framework for medical image segmentation, leveraging RWKV architecture for efficient long-range modeling, outperforming existing methods like U-Net with innovations like DARM and SASE.


<details>
  <summary>Details</summary>
Motivation: Addressing the need for lightweight, high-performance solutions in healthcare accessibility, especially in resource-limited settings, by overcoming the limited global ERFs of existing methods.

Method: Proposes U-RWKV with Direction-Adaptive RWKV Module (DARM) and Stage-Adaptive Squeeze-and-Excitation Module (SASE) for efficient long-range modeling and dynamic feature adaptation.

Result: Achieves state-of-the-art segmentation performance with high computational efficiency, suitable for resource-constrained environments.

Conclusion: U-RWKV offers a practical solution for democratizing advanced medical imaging technologies, with code publicly available.

Abstract: Achieving equity in healthcare accessibility requires lightweight yet
high-performance solutions for medical image segmentation, particularly in
resource-limited settings. Existing methods like U-Net and its variants often
suffer from limited global Effective Receptive Fields (ERFs), hindering their
ability to capture long-range dependencies. To address this, we propose U-RWKV,
a novel framework leveraging the Recurrent Weighted Key-Value(RWKV)
architecture, which achieves efficient long-range modeling at O(N)
computational cost. The framework introduces two key innovations: the
Direction-Adaptive RWKV Module(DARM) and the Stage-Adaptive
Squeeze-and-Excitation Module(SASE). DARM employs Dual-RWKV and QuadScan
mechanisms to aggregate contextual cues across images, mitigating directional
bias while preserving global context and maintaining high computational
efficiency. SASE dynamically adapts its architecture to different feature
extraction stages, balancing high-resolution detail preservation and semantic
relationship capture. Experiments demonstrate that U-RWKV achieves
state-of-the-art segmentation performance with high computational efficiency,
offering a practical solution for democratizing advanced medical imaging
technologies in resource-constrained environments. The code is available at
https://github.com/hbyecoding/U-RWKV.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [238] [Pronunciation Deviation Analysis Through Voice Cloning and Acoustic Comparison](https://arxiv.org/abs/2507.10985)
*Andrew Valdivia,Yueming Zhang,Hailu Xu,Amir Ghasemkhani,Xin Qin*

Main category: cs.SD

TL;DR: A novel method detects mispronunciations by comparing original speech with a voice-cloned version of corrected pronunciation, identifying deviations as errors.


<details>
  <summary>Details</summary>
Motivation: To improve pronunciation detection without relying on predefined phonetic rules or large training datasets for each language.

Method: Uses voice cloning to create a corrected pronunciation version of the user's voice, then compares it frame-by-frame with the original to find deviations.

Result: Effectively identifies specific pronunciation errors without needing extensive language-specific data.

Conclusion: The approach offers a practical and scalable solution for mispronunciation detection.

Abstract: This paper presents a novel approach for detecting mispronunciations by
analyzing deviations between a user's original speech and their voice-cloned
counterpart with corrected pronunciation. We hypothesize that regions with
maximal acoustic deviation between the original and cloned utterances indicate
potential mispronunciations. Our method leverages recent advances in voice
cloning to generate a synthetic version of the user's voice with proper
pronunciation, then performs frame-by-frame comparisons to identify problematic
segments. Experimental results demonstrate the effectiveness of this approach
in pinpointing specific pronunciation errors without requiring predefined
phonetic rules or extensive training data for each target language.

</details>


### [239] [EditGen: Harnessing Cross-Attention Control for Instruction-Based Auto-Regressive Audio Editing](https://arxiv.org/abs/2507.11096)
*Vassilis Sioros,Alexandros Potamianos,Giorgos Paraskevopoulos*

Main category: cs.SD

TL;DR: The paper introduces a method for efficient audio editing in auto-regressive models using cross-attention control, inspired by image editing. It combines Prompt-to-Prompt-like guidance with diffusion-based strategies and MUSICGEN, proposing three editing mechanisms. Evaluations show it outperforms diffusion-based baselines in melody, dynamics, and tempo.


<details>
  <summary>Details</summary>
Motivation: To improve audio editing efficiency in auto-regressive models by leveraging cross-attention control, inspired by image editing techniques.

Method: Develops a Prompt-to-Prompt-like approach with cross and self-attention mechanisms, integrates diffusion-based strategies, and introduces three editing mechanisms (Replacement, Reweighting, Refinement) using MUSICGEN.

Result: The proposed method outperforms diffusion-based baselines in melody, dynamics, and tempo, as confirmed by automatic and human evaluations.

Conclusion: The combination of prompt-to-prompt guidance with auto-regressive models significantly enhances audio editing performance, setting a new baseline for prompt-guided audio editing.

Abstract: In this study, we investigate leveraging cross-attention control for
efficient audio editing within auto-regressive models. Inspired by image
editing methodologies, we develop a Prompt-to-Prompt-like approach that guides
edits through cross and self-attention mechanisms. Integrating a
diffusion-based strategy, influenced by Auffusion, we extend the model's
functionality to support refinement edits, establishing a baseline for
prompt-guided audio editing. Additionally, we introduce an alternative approach
by incorporating MUSICGEN, a pre-trained frozen auto-regressive model, and
propose three editing mechanisms, based on Replacement, Reweighting, and
Refinement of the attention scores. We employ commonly-used music-specific
evaluation metrics and a human study, to gauge time-varying controllability,
adherence to global text cues, and overall audio realism. The automatic and
human evaluations indicate that the proposed combination of prompt-to-prompt
guidance with autoregressive generation models significantly outperforms the
diffusion-based baseline in terms of melody, dynamics, and tempo of the
generated audio. Our code is available at https://github.com/billsioros/EditGen

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [240] [FLsim: A Modular and Library-Agnostic Simulation Framework for Federated Learning](https://arxiv.org/abs/2507.11430)
*Arnab Mukherjee,Raju Halder,Joydeep Chandra*

Main category: cs.DC

TL;DR: FLsim is a modular, scalable, and efficient Federated Learning simulation framework designed to simplify benchmarking and research by supporting customizable data distributions, algorithms, and network topologies.


<details>
  <summary>Details</summary>
Motivation: To address the challenges in benchmarking and researching novel FL techniques against existing solutions, a flexible and reproducible simulation framework is needed.

Method: FLsim offers a modular and scalable framework with customizable features like data distributions, learning algorithms, network topologies, and blockchain support.

Result: FLsim effectively simulates diverse FL experiments, demonstrating its versatility and robustness.

Conclusion: FLsim advances FL research by providing unprecedented flexibility and functionality for researchers and practitioners.

Abstract: Federated Learning (FL) has undergone significant development since its
inception in 2016, advancing from basic algorithms to complex methodologies
tailored to address diverse challenges and use cases. However, research and
benchmarking of novel FL techniques against a plethora of established
state-of-the-art solutions remain challenging. To streamline this process, we
introduce FLsim, a comprehensive FL simulation framework designed to meet the
diverse requirements of FL workflows in the literature. FLsim is characterized
by its modularity, scalability, resource efficiency, and controlled
reproducibility of experimental outcomes. Its easy to use interface allows
users to specify customized FL requirements through job configuration, which
supports: (a) customized data distributions, ranging from non-independent and
identically distributed (non-iid) data to independent and identically
distributed (iid) data, (b) selection of local learning algorithms according to
user preferences, with complete agnosticism to ML libraries, (c) choice of
network topology illustrating communication patterns among nodes, (d)
definition of model aggregation and consensus algorithms, and (e) pluggable
blockchain support for enhanced robustness. Through a series of experimental
evaluations, we demonstrate the effectiveness and versatility of FLsim in
simulating a diverse range of state-of-the-art FL experiments. We envisage that
FLsim would mark a significant advancement in FL simulation frameworks,
offering unprecedented flexibility and functionality for researchers and
practitioners alike.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [241] [Overview of the TREC 2022 deep learning track](https://arxiv.org/abs/2507.10865)
*Nick Craswell,Bhaskar Mitra,Emine Yilmaz,Daniel Campos,Jimmy Lin,Ellen M. Voorhees,Ian Soboroff*

Main category: cs.IR

TL;DR: The TREC Deep Learning track in 2022 focused on improving test collections for passage retrieval, leveraging expanded datasets and deep neural models, which outperformed traditional methods. Some top runs surprised by not using dense retrieval.


<details>
  <summary>Details</summary>
Motivation: To enhance the quality of test collections for passage retrieval and evaluate the performance of deep neural ranking models compared to traditional methods.

Method: Utilized expanded MS MARCO datasets, focused on passage retrieval, and inferred document-level labels from passage-level labels. Deep neural models with large-scale pretraining were employed.

Result: Deep neural models outperformed traditional methods. Some top runs did not use dense retrieval, and single-stage dense retrieval was less competitive than in previous years.

Conclusion: The 2022 TREC Deep Learning track improved test collection quality and confirmed the dominance of deep neural models, though with unexpected outcomes regarding dense retrieval.

Abstract: This is the fourth year of the TREC Deep Learning track. As in previous
years, we leverage the MS MARCO datasets that made hundreds of thousands of
human annotated training labels available for both passage and document ranking
tasks. In addition, this year we also leverage both the refreshed passage and
document collections that were released last year leading to a nearly $16$
times increase in the size of the passage collection and nearly four times
increase in the document collection size. Unlike previous years, in 2022 we
mainly focused on constructing a more complete test collection for the passage
retrieval task, which has been the primary focus of the track. The document
ranking task was kept as a secondary task, where document-level labels were
inferred from the passage-level labels. Our analysis shows that similar to
previous years, deep neural ranking models that employ large scale pretraining
continued to outperform traditional retrieval methods. Due to the focusing our
judging resources on passage judging, we are more confident in the quality of
this year's queries and judgments, with respect to our ability to distinguish
between runs and reuse the dataset in future. We also see some surprises in
overall outcomes. Some top-performing runs did not do dense retrieval. Runs
that did single-stage dense retrieval were not as competitive this year as they
were last year.

</details>


### [242] [Extracting Document Relations from Search Corpus by Marginalizing over User Queries](https://arxiv.org/abs/2507.10726)
*Yuki Iwamoto,Kaoru Tsunoda,Ken Kaneiwa*

Main category: cs.IR

TL;DR: EDR-MQ is a framework for discovering document relationships by analyzing co-occurrence in query results, eliminating the need for manual annotation or predefined taxonomies.


<details>
  <summary>Details</summary>
Motivation: Existing methods for document relationship analysis rely on manual effort or rigid taxonomies, limiting scalability and adaptability.

Method: EDR-MQ uses query marginalization to estimate joint probabilities between documents, supported by MC-RAG for conditional retrieval.

Result: The approach identifies meaningful relationships like topical clusters and cross-domain connections, outperforming traditional methods.

Conclusion: EDR-MQ provides a scalable, user-adaptive solution for document organization without labeled data.

Abstract: Understanding relationships between documents in large-scale corpora is
essential for knowledge discovery and information organization. However,
existing approaches rely heavily on manual annotation or predefined
relationship taxonomies. We propose EDR-MQ (Extracting Document Relations by
Marginalizing over User Queries), a novel framework that discovers document
relationships through query marginalization. EDR-MQ is based on the insight
that strongly related documents often co-occur in results across diverse user
queries, enabling us to estimate joint probabilities between document pairs by
marginalizing over a collection of queries. To enable this query
marginalization approach, we develop Multiply Conditioned Retrieval-Augmented
Generation (MC-RAG), which employs conditional retrieval where subsequent
document retrievals depend on previously retrieved content. By observing
co-occurrence patterns across diverse queries, EDR-MQ estimates joint
probabilities between document pairs without requiring labeled training data or
predefined taxonomies. Experimental results show that our query marginalization
approach successfully identifies meaningful document relationships, revealing
topical clusters, evidence chains, and cross-domain connections that are not
apparent through traditional similarity-based methods. Our query-driven
framework offers a practical approach to document organization that adapts to
different user perspectives and information needs.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [243] [The Shape of Deceit: Behavioral Consistency and Fragility in Money Laundering Patterns](https://arxiv.org/abs/2507.10608)
*Danny Butvinik,Ofir Yakobi,Michal Einhorn Cohen,Elina Maliarsky*

Main category: cs.SI

TL;DR: The paper critiques traditional AML systems for focusing on anomalies and proposes a network-theoretic approach to detect laundering patterns based on behavioral consistency and subgraph structures, emphasizing pattern fragility and semantic robustness.


<details>
  <summary>Details</summary>
Motivation: Current AML systems misidentify money laundering by focusing on anomalies, whereas laundering is deliberate and consistent. The paper aims to shift the focus to detecting predefined laundering patterns in transaction networks.

Method: The authors introduce a network-theoretic perspective, using behavioral consistency and subgraph structures to identify laundering patterns. They explore pattern fragility and semantic robustness.

Result: The proposed approach redefines pattern similarity, focusing on preserving behavioral essence rather than statistical outliers.

Conclusion: The paper advocates for a philosophical and practical shift in AML systems to better model and detect laundering patterns in financial networks.

Abstract: Conventional anti-money laundering (AML) systems predominantly focus on
identifying anomalous entities or transactions, flagging them for manual
investigation based on statistical deviation or suspicious behavior. This
paradigm, however, misconstrues the true nature of money laundering, which is
rarely anomalous but often deliberate, repeated, and concealed within
consistent behavioral routines. In this paper, we challenge the entity-centric
approach and propose a network-theoretic perspective that emphasizes detecting
predefined laundering patterns across directed transaction networks. We
introduce the notion of behavioral consistency as the core trait of laundering
activity, and argue that such patterns are better captured through subgraph
structures expressing semantic and functional roles - not solely geometry.
Crucially, we explore the concept of pattern fragility: the sensitivity of
laundering patterns to small attribute changes and, conversely, their semantic
robustness even under drastic topological transformations. We claim that
laundering detection should not hinge on statistical outliers, but on
preservation of behavioral essence, and propose a reconceptualization of
pattern similarity grounded in this insight. This philosophical and practical
shift has implications for how AML systems model, scan, and interpret networks
in the fight against financial crime.

</details>


### [244] [Multilayer Artificial Benchmark for Community Detection (mABCD)](https://arxiv.org/abs/2507.10795)
*Łukasz Kraiński,Michał Czuba,Piotr Bródka,Paweł Prałat,Bogumił Kamiński,François Théberge*

Main category: cs.SI

TL;DR: The paper introduces mABCD, a variant of the ABCD model for multilayer networks, offering faster, interpretable, and analytically tractable community detection.


<details>
  <summary>Details</summary>
Motivation: To extend the ABCD model's benefits (speed, interpretability, analytical tractability) to multilayer networks, addressing limitations of existing models like LFR.

Method: Leverages ABCD model's underlying principles to develop mABCD, a multilayer variant.

Result: mABCD provides a faster, more interpretable, and analytically tractable solution for community detection in multilayer networks.

Conclusion: The mABCD model successfully extends ABCD's advantages to multilayer networks, offering a practical tool for community detection.

Abstract: The Artificial Benchmark for Community Detection (ABCD) model is a random
graph model with community structure and power-law distribution for both
degrees and community sizes. The model generates graphs similar to the
well-known LFR model but it is faster, more interpretable, and can be
investigated analytically. In this paper, we use the underlying ingredients of
the ABCD model and introduce its variant for multilayer networks, mABCD.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [245] [Protocols for Verifying Smooth Strategies in Bandits and Games](https://arxiv.org/abs/2507.10567)
*Miranda Christ,Daniel Reichman,Jonathan Shafer*

Main category: cs.GT

TL;DR: The paper explores protocols for verifying approximate optimality in multi-armed bandits and normal-form games, focusing on sublinear query complexity to utility oracles. It proves feasibility for smooth strategies and provides efficient verification methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to address the challenge of verifying optimality in large-action spaces efficiently, avoiding the high query costs of traditional learning methods.

Method: The method involves designing verification protocols for smooth strategies in multi-armed bandits and extending them to normal-form games, with a focus on sublinear query complexity.

Result: The results show that verification is possible with fewer queries than learning, and a nearly-tight lower bound on query complexity is established.

Conclusion: The conclusion highlights the applicability of verification protocols for bandits to normal-form games, enabling efficient verification of approximate Nash equilibria with sublinear query complexity.

Abstract: We study protocols for verifying approximate optimality of strategies in
multi-armed bandits and normal-form games. As the number of actions available
to each player is often large, we seek protocols where the number of queries to
the utility oracle is sublinear in the number of actions. We prove that such
verification is possible for sufficiently smooth strategies that do not put too
much probability mass on any specific action. We provide protocols for
verifying that a smooth policy for a multi-armed bandit is
$\varepsilon$-optimal. Our verification protocols require provably fewer arm
queries than learning. Furthermore, we establish a nearly-tight lower bound on
the query complexity of verification in our settings. As an application, we
show how to use verification for bandits to achieve verification in normal-form
games. This gives a protocol for verifying whether a given strategy profile is
an approximate strong smooth Nash equilibrium, with a query complexity that is
sublinear in the number of actions.

</details>


### [246] [A Parallelizable Approach for Characterizing NE in Zero-Sum Games After a Linear Number of Iterations of Gradient Descent](https://arxiv.org/abs/2507.11366)
*Taemin Kim,James P. Bailey*

Main category: cs.GT

TL;DR: A new Hamiltonian dynamics-based method for zero-sum games achieves finite-time NE convergence, outperforms traditional methods, and supports parallelization with arbitrary learning rates.


<details>
  <summary>Details</summary>
Motivation: Addressing limitations of traditional NE approximation methods (regret-based or contraction-map-based) in online optimization for zero-sum games.

Method: Proposes a Hamiltonian dynamics-inspired approach for alternating gradient descent, enabling finite-time NE characterization and parallelization.

Result: The method converges to NE in finite iterations, works with arbitrary learning rates, and outperforms standard methods experimentally.

Conclusion: The Hamiltonian-based approach offers a novel, efficient, and flexible solution for computing NE in zero-sum games.

Abstract: We study online optimization methods for zero-sum games, a fundamental
problem in adversarial learning in machine learning, economics, and many other
domains. Traditional methods approximate Nash equilibria (NE) using either
regret-based methods (time-average convergence) or contraction-map-based
methods (last-iterate convergence). We propose a new method based on
Hamiltonian dynamics in physics and prove that it can characterize the set of
NE in a finite (linear) number of iterations of alternating gradient descent in
the unbounded setting, modulo degeneracy, a first in online optimization.
Unlike standard methods for computing NE, our proposed approach can be
parallelized and works with arbitrary learning rates, both firsts in
algorithmic game theory. Experimentally, we support our results by showing our
approach drastically outperforms standard methods.

</details>


### [247] [Better Regret Rates in Bilateral Trade via Sublinear Budget Violation](https://arxiv.org/abs/2507.11419)
*Anna Lunghi,Matteo Castiglioni,Alberto Marchesi*

Main category: cs.GT

TL;DR: The paper explores the trade-off between regret and budget balance violation in bilateral trade, proposing an algorithm that achieves optimal regret rates for varying levels of budget violation.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between strict budget balance constraints and unconstrained settings by studying how regret varies with allowed budget violations.

Method: Designs an algorithm that violates the budget balance constraint by at most T^β for β ∈ [3/4, 6/7], achieving regret O~(T^(1 - β/3)).

Result: The algorithm achieves regret O~(T^(1 - β/3)) for given β, with a matching lower bound confirming tightness.

Conclusion: The results fully characterize the trade-off between regret and budget violation, showing tightness of prior bounds in extreme cases.

Abstract: Bilateral trade is a central problem in algorithmic economics, and recent
work has explored how to design trading mechanisms using no-regret learning
algorithms. However, no-regret learning is impossible when budget balance has
to be enforced at each time step. Bernasconi et al. [Ber+24] show how this
impossibility can be circumvented by relaxing the budget balance constraint to
hold only globally over all time steps. In particular, they design an algorithm
achieving regret of the order of $\tilde O(T^{3/4})$ and provide a lower bound
of $\Omega(T^{5/7})$.
  In this work, we interpolate between these two extremes by studying how the
optimal regret rate varies with the allowed violation of the global budget
balance constraint. Specifically, we design an algorithm that, by violating the
constraint by at most $T^{\beta}$ for any given $\beta \in [\frac{3}{4},
\frac{6}{7}]$, attains regret $\tilde O(T^{1 - \beta/3})$. We complement this
result with a matching lower bound, thus fully characterizing the trade-off
between regret and budget violation. Our results show that both the $\tilde
O(T^{3/4})$ upper bound in the global budget balance case and the
$\Omega(T^{5/7})$ lower bound under unconstrained budget balance violation
obtained by Bernasconi et al. [Ber+24] are tight.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [248] [LiLM-RDB-SFC: Lightweight Language Model with Relational Database-Guided DRL for Optimized SFC Provisioning](https://arxiv.org/abs/2507.10903)
*Parisa Fard Moshiri,Xinyu Zhu,Poonam Lohan,Burak Kantarci,Emil Janulewicz*

Main category: cs.NI

TL;DR: LiLM-RDB-SFC combines Lightweight Language Models (LiLMs) with Relational Databases to enhance DRL for SFC provisioning, showing FLAN-T5 outperforms BART and SQLCoder in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of DRL in dynamic network decision-making due to structured data dependency and fixed action rules, especially under unpredictable conditions.

Method: Uses two LiLMs (BART and FLAN-T5) with RDB to interpret network data and guide DRL for SFC provisioning.

Result: FLAN-T5 achieves lower test loss (0.00161 vs. 0.00734), higher accuracy (94.79% vs. 80.2%), and faster processing (2h 2min vs. 2h 38min) than BART. It also matches SQLCoder's accuracy while reducing processing time by 96%.

Conclusion: FLAN-T5 is highly effective for SFC provisioning, offering superior performance and efficiency compared to other models.

Abstract: Effective management of Service Function Chains (SFCs) and optimal Virtual
Network Function (VNF) placement are critical challenges in modern
Software-Defined Networking (SDN) and Network Function Virtualization (NFV)
environments. Although Deep Reinforcement Learning (DRL) is widely adopted for
dynamic network decision-making, its inherent dependency on structured data and
fixed action rules often limits adaptability and responsiveness, particularly
under unpredictable network conditions. This paper introduces LiLM-RDB-SFC, a
novel approach combining Lightweight Language Model (LiLM) with Relational
Database (RDB) to answer network state queries to guide DRL model for efficient
SFC provisioning. Our proposed approach leverages two LiLMs, Bidirectional and
Auto-Regressive Transformers (BART) and the Fine-tuned Language Net T5
(FLAN-T5), to interpret network data and support diverse query types related to
SFC demands, data center resources, and VNF availability. Results demonstrate
that FLAN-T5 outperforms BART with a lower test loss (0.00161 compared to
0.00734), higher accuracy (94.79% compared to 80.2%), and less processing time
(2h 2min compared to 2h 38min). Moreover, when compared to the large language
model SQLCoder, FLAN-T5 matches the accuracy of SQLCoder while cutting
processing time by 96% (SQLCoder: 54 h 43 min; FLAN-T5: 2 h 2 min).

</details>


### [249] [Improving Wi-Fi Network Performance Prediction with Deep Learning Models](https://arxiv.org/abs/2507.11168)
*Gabriele Formis,Amanda Ericson,Stefan Forsstrom,Kyi Thar,Gianluca Cena,Stefano Scanzio*

Main category: cs.NI

TL;DR: Machine learning models predict Wi-Fi channel quality (frame delivery ratio) for industrial use, with CNNs balancing accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Addressing the need for robust, reliable wireless networks in industrial and mission-critical applications.

Method: Used convolutional neural networks (CNNs) and long short-term memory (LSTM) on real Wi-Fi datasets to predict frame delivery ratio.

Result: CNNs, while slightly less accurate, are more efficient in CPU and memory usage, making them suitable for embedded systems.

Conclusion: CNNs offer a practical solution for optimizing Wi-Fi networks in industrial settings due to their efficiency.

Abstract: The increasing need for robustness, reliability, and determinism in wireless
networks for industrial and mission-critical applications is the driver for the
growth of new innovative methods. The study presented in this work makes use of
machine learning techniques to predict channel quality in a Wi-Fi network in
terms of the frame delivery ratio. Predictions can be used proactively to
adjust communication parameters at runtime and optimize network operations for
industrial applications. Methods including convolutional neural networks and
long short-term memory were analyzed on datasets acquired from a real Wi-Fi
setup across multiple channels. The models were compared in terms of prediction
accuracy and computational complexity. Results show that the frame delivery
ratio can be reliably predicted, and convolutional neural networks, although
slightly less effective than other models, are more efficient in terms of CPU
usage and memory consumption. This enhances the model's usability on embedded
and industrial systems.

</details>


<div id='math-ph'></div>

# math-ph [[Back]](#toc)

### [250] [From Kinetic Theory to AI: a Rediscovery of High-Dimensional Divergences and Their Properties](https://arxiv.org/abs/2507.11387)
*Gennaro Auricchio,Giovanni Brigati,Paolo Giudici,Giuseppe Toscani*

Main category: math-ph

TL;DR: A review of divergence measures from kinetic theory, comparing their theoretical foundations and potential uses in machine learning and AI.


<details>
  <summary>Details</summary>
Motivation: The choice of divergence measure affects model performance, and kinetic theory offers useful measures like KL divergence.

Method: Comparative review of divergence measures from kinetic theory, analyzing their theoretical bases.

Result: Identifies divergence measures from kinetic theory with potential applications in machine learning and AI.

Conclusion: Kinetic theory-derived divergence measures can enhance machine learning and AI applications.

Abstract: Selecting an appropriate divergence measure is a critical aspect of machine
learning, as it directly impacts model performance. Among the most widely used,
we find the Kullback-Leibler (KL) divergence, originally introduced in kinetic
theory as a measure of relative entropy between probability distributions. Just
as in machine learning, the ability to quantify the proximity of probability
distributions plays a central role in kinetic theory. In this paper, we present
a comparative review of divergence measures rooted in kinetic theory,
highlighting their theoretical foundations and exploring their potential
applications in machine learning and artificial intelligence.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [251] [AGFS-Tractometry: A Novel Atlas-Guided Fine-Scale Tractometry Approach for Enhanced Along-Tract Group Statistical Comparison Using Diffusion MRI Tractography](https://arxiv.org/abs/2507.10601)
*Ruixi Zheng,Wei Zhang,Yijie Li,Xi Zhu,Zhou Lan,Jarrett Rushmore,Yogesh Rathi,Nikos Makris,Lauren J. O'Donnell,Fan Zhang*

Main category: q-bio.QM

TL;DR: AGFS-Tractometry is a novel atlas-guided fine-scale tractometry method for enhanced along-tract statistical analysis in diffusion MRI, improving sensitivity and specificity in detecting white matter differences.


<details>
  <summary>Details</summary>
Motivation: To address the need for more precise and consistent along-tract profiling in tractometry for studying white matter differences between populations.

Method: Proposes an atlas-guided tract profiling template for fine-scale parcellation and a nonparametric permutation testing method for group comparisons.

Result: AGFS-Tractometry outperforms AFQ and BUAN in sensitivity and specificity, identifying more anatomically consistent regions with significant differences.

Conclusion: AGFS-Tractometry effectively detects subtle or localized white matter differences, with its template and code publicly available.

Abstract: Diffusion MRI (dMRI) tractography is currently the only method for in vivo
mapping of the brain's white matter (WM) connections. Tractometry is an
advanced tractography analysis technique for along-tract profiling to
investigate the morphology and microstructural properties along the fiber
tracts. Tractometry has become an essential tool for studying local along-tract
differences between different populations (e.g., health vs disease). In this
study, we propose a novel atlas-guided fine-scale tractometry method, namely
AGFS-Tractometry, that leverages tract spatial information and permutation
testing to enhance the along-tract statistical analysis between populations.
There are two major contributions in AGFS-Tractometry. First, we create a novel
atlas-guided tract profiling template that enables consistent, fine-scale,
along-tract parcellation of subject-specific fiber tracts. Second, we propose a
novel nonparametric permutation testing group comparison method to enable
simultaneous analysis across all along-tract parcels while correcting for
multiple comparisons. We perform experimental evaluations on synthetic datasets
with known group differences and in vivo real data. We compare AGFS-Tractometry
with two state-of-the-art tractometry methods, including Automated Fiber-tract
Quantification (AFQ) and BUndle ANalytics (BUAN). Our results show that the
proposed AGFS-Tractometry obtains enhanced sensitivity and specificity in
detecting local WM differences. In the real data analysis experiments,
AGFS-Tractometry can identify more regions with significant differences, which
are anatomically consistent with the existing literature. Overall, these
demonstrate the ability of AGFS-Tractometry to detect subtle or spatially
localized WM group-level differences. The created tract profiling template and
related code are available at:
https://github.com/ZhengRuixi/AGFS-Tractometry.git.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [252] [SQLord: A Robust Enterprise Text-to-SQL Solution via Reverse Data Generation and Workflow Decomposition](https://arxiv.org/abs/2507.10629)
*Song Cheng,Qiannan Cheng,Linbo Jin,Lei Yi,Guannan Zhang*

Main category: cs.DB

TL;DR: SQLord is an enterprise-level NL2SQL framework addressing challenges like complex business logic and lack of domain-specific data. It uses data reverse generation, query decomposition, and a GPT-Judge evaluation framework, achieving over 90% accuracy in real-world applications.


<details>
  <summary>Details</summary>
Motivation: Existing NL2SQL frameworks struggle with complex business logic and lack domain-specific data, while evaluation methods often require scarce annotated data and executable environments.

Method: SQLord employs data reverse generation for supervised fine-tuning, decomposes complex queries via an automated workflow generator, and introduces the GPT-Judge evaluation framework (EXE, QSE, SSE).

Result: Offline tests outperform baselines, and online accuracy exceeds 90%, demonstrating effectiveness in real-world scenarios.

Conclusion: SQLord successfully addresses NL2SQL challenges, proving its utility in complex business applications, as evidenced by its deployment on a major B2B e-commerce platform.

Abstract: Transforming natural language into SQL queries (NL2SQL) is crucial for
data-driven business applications. Existing frameworks, trained on open-source
datasets, struggle with complex business logic and lack domain-specific data
for fine-tuning. Additionally, evaluation methods often require annotated data
and executable database environments, which are scarce in real-world scenarios.
To address these challenges, we propose SQLord, an enterprise-level NL2SQL
framework. First, SQLord introduces a data reverse generation approach to
convert raw SQL statements into annotated data for supervised fine-tuning
(SFT). Second, it proposes a decomposition method for complex queries using an
automated workflow generator. Additionally, SQLord features a comprehensive
GPT-Judge evaluation framework, including Execution Evaluation (EXE), Query-SQL
Evaluation (QSE), and SQL-SQL Evaluation (SSE), tailored to diverse scenarios.
Offline tests significantly outperform state of the art baselines, and online
accuracy consistently exceeds 90, highlighting SQLord's advantages and
effectiveness in complex real world scenarios. SQLord has been successfully
applied across multiple scenarios on the world's largest B2B e-commerce
platform.

</details>


### [253] [Towards Practical Benchmarking of Data Cleaning Techniques: On Generating Authentic Errors via Large Language Models](https://arxiv.org/abs/2507.10934)
*Xinyuan Liu,Jiahui Chen,Bocheng Hu,Yu Sun,Xinyang Chen,Shaoxu Song*

Main category: cs.DB

TL;DR: TableEG is a framework using LLMs to generate realistic tabular data errors, outperforming rule-based and non-fine-tuned LLM methods in pattern and distribution similarity.


<details>
  <summary>Details</summary>
Motivation: The lack of diverse, real-world error datasets limits evaluation of error detection algorithms, and manual annotation is inefficient.

Method: TableEG employs a table fine-tuning strategy and triplet representation (I, T, O) to model error generation, detection, and correction, trained on 12 real-world datasets.

Result: TableEG-generated errors closely mimic real-world errors in pattern and distribution, aligning well with performance metrics for detection algorithms.

Conclusion: TableEG bridges the synthetic-real error gap and provides a robust benchmark for error detection and correction tasks.

Abstract: Data quality remains an important challenge in data-driven systems, as errors
in tabular data can severely compromise downstream analytics and machine
learning performance. Although numerous error detection algorithms have been
proposed, the lack of diverse, real-world error datasets limits comprehensive
evaluation. Manual error annotation is both time-consuming and inconsistent,
motivating the exploration of synthetic error generation as an alternative. In
this work, we introduce TableEG, a framework that leverages large language
models (LLMs) to generate authentic errors. By employing a table fine-tuning
strategy and a triplet representation $(I, T, O)$ to model error generation,
detection, and correction tasks, TableEG captures the complex dependencies
inherent in two-dimensional tables. Trained on 12 real-world datasets spanning
10 diverse domains, TableEG ensures that the synthesized errors faithfully
reflect authentic error distributions. Experimental results indicate that
errors generated by TableEG exhibit superior pattern and distribution
similarity compared to both rule-based methods and LLM-generated errors without
fine-tuning. Furthermore, performance metrics on TableEG-generated errors
closely align with those on real-world errors across nearly all datasets and
detection algorithms, particularly for machine learning based detection
techniques. Overall, TableEG not only bridges the gap between synthetic and
real-world errors but also establishes a robust benchmark for subsequent error
detection and correction tasks.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [254] [Elevating 3D Models: High-Quality Texture and Geometry Refinement from a Low-Quality Model](https://arxiv.org/abs/2507.11465)
*Nuri Ryu,Jiyun Won,Jooeun Son,Minsu Gong,Joo-Haeng Lee,Sunghyun Cho*

Main category: cs.GR

TL;DR: Elevate3D enhances low-quality 3D assets into high-quality ones using HFS-SDEdit for texture improvement and monocular geometry predictors for geometry refinement, outperforming competitors.


<details>
  <summary>Details</summary>
Motivation: High-quality 3D assets are scarce due to acquisition costs, limiting applications in computer graphics and 3D vision.

Method: Elevate3D uses HFS-SDEdit for texture enhancement and alternates between texture and geometry refinement, leveraging monocular geometry predictors for accurate geometry.

Result: The framework achieves state-of-the-art quality in 3D model refinement, addressing the scarcity of high-quality open-source 3D assets.

Conclusion: Elevate3D effectively transforms low-quality 3D assets into high-quality ones, outperforming existing methods.

Abstract: High-quality 3D assets are essential for various applications in computer
graphics and 3D vision but remain scarce due to significant acquisition costs.
To address this shortage, we introduce Elevate3D, a novel framework that
transforms readily accessible low-quality 3D assets into higher quality. At the
core of Elevate3D is HFS-SDEdit, a specialized texture enhancement method that
significantly improves texture quality while preserving the appearance and
geometry while fixing its degradations. Furthermore, Elevate3D operates in a
view-by-view manner, alternating between texture and geometry refinement.
Unlike previous methods that have largely overlooked geometry refinement, our
framework leverages geometric cues from images refined with HFS-SDEdit by
employing state-of-the-art monocular geometry predictors. This approach ensures
detailed and accurate geometry that aligns seamlessly with the enhanced
texture. Elevate3D outperforms recent competitors by achieving state-of-the-art
quality in 3D model refinement, effectively addressing the scarcity of
high-quality open-source 3D assets.

</details>


<div id='cond-mat.str-el'></div>

# cond-mat.str-el [[Back]](#toc)

### [255] [Functional Neural Wavefunction Optimization](https://arxiv.org/abs/2507.10835)
*Victor Armegioiu,Juan Carrasquilla,Siddhartha Mishra,Johannes Müller,Jannes Nys,Marius Zeinhofer,Hang Zhang*

Main category: cond-mat.str-el

TL;DR: A framework for designing and analyzing optimization algorithms in variational quantum Monte Carlo using geometric insights, unifying existing methods and deriving new algorithms with principled hyperparameters.


<details>
  <summary>Details</summary>
Motivation: To improve optimization in variational quantum Monte Carlo by leveraging geometric insights into function space.

Method: Translates infinite-dimensional optimization dynamics into parameter-space algorithms via Galerkin projection onto the tangent space of the variational ansatz.

Result: Unifies existing methods (e.g., stochastic reconfiguration, Rayleigh-Gauss-Newton) and validates framework with accurate ground-state energy estimates for condensed matter models.

Conclusion: The framework is practical and effective, demonstrated by successful numerical experiments on neural network wavefunctions.

Abstract: We propose a framework for the design and analysis of optimization algorithms
in variational quantum Monte Carlo, drawing on geometric insights into the
corresponding function space. The framework translates infinite-dimensional
optimization dynamics into tractable parameter-space algorithms through a
Galerkin projection onto the tangent space of the variational ansatz. This
perspective unifies existing methods such as stochastic reconfiguration and
Rayleigh-Gauss-Newton, provides connections to classic function-space
algorithms, and motivates the derivation of novel algorithms with geometrically
principled hyperparameter choices. We validate our framework with numerical
experiments demonstrating its practical relevance through the accurate
estimation of ground-state energies for several prototypical models in
condensed matter physics modeled with neural network wavefunctions.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [256] [A Learning Framework For Cooperative Collision Avoidance of UAV Swarms Leveraging Domain Knowledge](https://arxiv.org/abs/2507.10913)
*Shuangyao Huang,Haibo Zhang,Zhiyi Huang*

Main category: cs.MA

TL;DR: A MARL framework for UAV swarm collision avoidance uses domain knowledge-driven rewards from image processing to model obstacles and ensure smooth, energy-efficient paths.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of cooperative collision avoidance in UAV swarms without complex interaction or credit assignment mechanisms.

Method: Leverages domain knowledge (image processing) to derive rewards, modeling obstacles as maxima on a 2D field to avoid collisions and ensure smooth paths.

Result: The framework minimizes agent interaction, eliminates complex credit assignment, and adapts to complex environments through training.

Conclusion: The approach outperforms state-of-the-art MARL algorithms in UAV swarm collision avoidance.

Abstract: This paper presents a multi-agent reinforcement learning (MARL) framework for
cooperative collision avoidance of UAV swarms leveraging domain
knowledge-driven reward. The reward is derived from knowledge in the domain of
image processing, approximating contours on a two-dimensional field. By
modeling obstacles as maxima on the field, collisions are inherently avoided as
contours never go through peaks or intersect. Additionally, counters are smooth
and energy-efficient. Our framework enables training with large swarm sizes as
the agent interaction is minimized and the need for complex credit assignment
schemes or observation sharing mechanisms in state-of-the-art MARL approaches
are eliminated. Moreover, UAVs obtain the ability to adapt to complex
environments where contours may be non-viable or non-existent through intensive
training. Extensive experiments are conducted to evaluate the performances of
our framework against state-of-the-art MARL algorithms.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [257] [NLP Meets the World: Toward Improving Conversations With the Public About Natural Language Processing Research](https://arxiv.org/abs/2507.10559)
*Shomir Wilson*

Main category: cs.CY

TL;DR: The paper provides recommendations for effectively communicating the capabilities and limitations of large language models (LLMs) to the general public, addressing vague terminology, unreasonable expectations, and ethical concerns.


<details>
  <summary>Details</summary>
Motivation: To improve public understanding and support for NLP research by addressing common communication challenges.

Method: Analyzes NLP research and news coverage to identify key themes (vague terminology, unreasonable expectations, ethical failures) and provides recommendations.

Result: Highlights the need for transparent communication to avoid misunderstandings and sustain public trust in NLP advancements.

Conclusion: Effective communication about LLMs is crucial for fostering public understanding and continued support for NLP research.

Abstract: Recent developments in large language models (LLMs) have been accompanied by
rapidly growing public interest in natural language processing (NLP). This
attention is reflected by major news venues, which sometimes invite NLP
researchers to share their knowledge and views with a wide audience.
Recognizing the opportunities of the present, for both the research field and
for individual researchers, this paper shares recommendations for communicating
with a general audience about LLMs' capabilities and limitations. These
recommendations cover three themes: vague terminology as an obstacle to public
understanding, unreasonable expectations as obstacles to sustainable growth,
and ethical failures as obstacles to continued support. Published NLP research
and popular news coverage are cited to illustrate these themes with examples.
The recommendations promote effective, transparent communication with the
general public about NLP, in order to strengthen public understanding and
encourage support for research.

</details>


### [258] [Can Large Language Models Understand As Well As Apply Patent Regulations to Pass a Hands-On Patent Attorney Test?](https://arxiv.org/abs/2507.10576)
*Bhakti Khera,Rezvan Alamian,Pascal A. Scherz,Stephan M. Goetz*

Main category: cs.CY

TL;DR: The paper evaluates LLMs on the European Qualifying Examination for patent attorneys, finding none meet professional standards. GPT-4o performed best, but expert oversight remains crucial due to model shortcomings.


<details>
  <summary>Details</summary>
Motivation: To quantitatively assess LLM performance in legal applications, specifically for patent attorney exams, and highlight gaps between model capabilities and professional requirements.

Method: Evaluated open-source and proprietary LLMs (e.g., GPT-series, Anthropic, Llama-3) on parts of the EQE, measuring accuracy and F1 scores, and analyzed outputs with human experts.

Result: GPT-4o led with 0.82 accuracy, but no model met the 0.90 threshold for professional standards. Models showed sensitivity to prompts and temperature, with human experts identifying critical flaws.

Conclusion: Current LLMs fall short of human-level patent proficiency. Future work must address logical consistency, multimodality, and adaptive prompting to bridge this gap.

Abstract: The legal field already uses various large language models (LLMs) in actual
applications, but their quantitative performance and reasons for it are
underexplored. We evaluated several open-source and proprietary LLMs --
including GPT-series, Anthropic, Deepseek and Llama-3, variants -- on parts of
the European Qualifying Examination (EQE) for future European Patent Attorneys.
OpenAI o1 led with 0.82 accuracy and 0.81 F1 score, whereas (Amazon Web
Services) AWS Llama 3.1 8B lagged at 0.50 accuracy, and a Python-deployed Llama
3.1 8B scored 0.55. The latter two are within the range of mere guessing for
the two-answer forced-choice design. None of the evaluated models could have
passed the examination fully, as accuracy never exceeded the average threshold
of 0.90 required for professional-level standards -- also not models that are
regularly promoted for their assumed beyond-PhD- and bar-admitted-lawyer-level
performance. GPT-4o excelled at integrating text and graphics, while Claude 3
Opus often lost formatting coherence. Human patent experts evaluated the
textual justifications and uncovered various critical shortcomings of each
model. They valued clarity and legal rationale over the raw correctness of the
answers, which revealed misalignment between automatic metrics and expert
judgment. Model outputs were sensitive to modest temperature changes and prompt
wording, which underscores the remaining necessity of expert oversight. Future
work should target logical consistency, robust multimodality, and adaptive
prompting to approach human-level patent proficiency. In summary, despite the
outstanding performance of recent large models, the general public might
overestimate their performance. The field has a long way to go to develop a
virtual patent attorney. This paper wants to point out several specific
limitations that need solutions.

</details>


### [259] [Findings of the BEA 2025 Shared Task on Pedagogical Ability Assessment of AI-powered Tutors](https://arxiv.org/abs/2507.10579)
*Ekaterina Kochmar,Kaushal Kumar Maurya,Kseniia Petukhova,KV Aditya Srivatsa,Anaïs Tack,Justin Vasselli*

Main category: cs.CY

TL;DR: The paper evaluates AI tutors powered by LLMs in educational dialogues, focusing on mistake remediation. Over 50 teams participated, with results showing promise but room for improvement.


<details>
  <summary>Details</summary>
Motivation: To assess and improve the pedagogical abilities of AI tutors in identifying and remedying student mistakes, grounded in learning science principles.

Method: A shared task with five tracks evaluated AI tutors on mistake identification, location, guidance, feedback actionability, and tutor identity detection. Models were compared to human annotations.

Result: Best F1 scores ranged from 58.34 (guidance) to 71.81 (mistake identification), with tutor identification scoring 96.98.

Conclusion: While promising, AI tutors need further improvement. The shared task resources are publicly available to aid future research.

Abstract: This shared task has aimed to assess pedagogical abilities of AI tutors
powered by large language models (LLMs), focusing on evaluating the quality of
tutor responses aimed at student's mistake remediation within educational
dialogues. The task consisted of five tracks designed to automatically evaluate
the AI tutor's performance across key dimensions of mistake identification,
precise location of the mistake, providing guidance, and feedback
actionability, grounded in learning science principles that define good and
effective tutor responses, as well as the track focusing on detection of the
tutor identity. The task attracted over 50 international teams across all
tracks. The submitted models were evaluated against gold-standard human
annotations, and the results, while promising, show that there is still
significant room for improvement in this domain: the best results for the four
pedagogical ability assessment tracks range between macro F1 scores of 58.34
(for providing guidance) and 71.81 (for mistake identification) on three-class
problems, with the best F1 score in the tutor identification track reaching
96.98 on a 9-class task. In this paper, we overview the main findings of the
shared task, discuss the approaches taken by the teams, and analyze their
performance. All resources associated with this task are made publicly
available to support future research in this critical domain.

</details>


### [260] [Exploring User Security and Privacy Attitudes and Concerns Toward the Use of General-Purpose LLM Chatbots for Mental Health](https://arxiv.org/abs/2507.10695)
*Jabari Kwesi,Jiaxun Cao,Riya Manchanda,Pardis Emami-Naeini*

Main category: cs.CY

TL;DR: The paper explores privacy and security concerns in using general-purpose LLM-enabled chatbots for mental health, revealing misconceptions and lack of risk awareness among users.


<details>
  <summary>Details</summary>
Motivation: To address the gap in understanding users' privacy and security concerns when using general-purpose LLM chatbots for mental health, given the lack of empirical research on this topic.

Method: Conducted 21 semi-structured interviews with U.S. participants to identify misconceptions and risk awareness.

Result: Participants misunderstood LLM accountability and privacy protections, undervaluing emotional disclosures compared to tangible data.

Conclusion: Proposes recommendations to better safeguard mental health disclosures in LLM-enabled chatbots, introducing the concept of 'intangible vulnerability.'

Abstract: Individuals are increasingly relying on large language model (LLM)-enabled
conversational agents for emotional support. While prior research has examined
privacy and security issues in chatbots specifically designed for mental health
purposes, these chatbots are overwhelmingly "rule-based" offerings that do not
leverage generative AI. Little empirical research currently measures users'
privacy and security concerns, attitudes, and expectations when using
general-purpose LLM-enabled chatbots to manage and improve mental health.
Through 21 semi-structured interviews with U.S. participants, we identified
critical misconceptions and a general lack of risk awareness. Participants
conflated the human-like empathy exhibited by LLMs with human-like
accountability and mistakenly believed that their interactions with these
chatbots were safeguarded by the same regulations (e.g., HIPAA) as disclosures
with a licensed therapist. We introduce the concept of "intangible
vulnerability," where emotional or psychological disclosures are undervalued
compared to more tangible forms of information (e.g., financial or
location-based data). To address this, we propose recommendations to safeguard
user mental health disclosures with general-purpose LLM-enabled chatbots more
effectively.

</details>


### [261] ["Is it always watching? Is it always listening?" Exploring Contextual Privacy and Security Concerns Toward Domestic Social Robots](https://arxiv.org/abs/2507.10786)
*Henry Bell,Jabari Kwesi,Hiba Laabadli,Pardis Emami-Naeini*

Main category: cs.CY

TL;DR: Social robots with AI and sensing capabilities pose security and privacy risks in the U.S. market. Interviews reveal concerns about data misuse, transparency, and usability, emphasizing the need for robust privacy controls.


<details>
  <summary>Details</summary>
Motivation: To understand U.S. users' security and privacy concerns regarding social robots, guiding their design before widespread commercialization.

Method: Conducted 19 semi-structured interviews to identify key concerns and expectations.

Result: Participants highlighted risks like data linkage, unauthorized sharing, and physical safety. They demanded transparency, usability, and context-appropriate controls.

Conclusion: Designing social robots requires addressing privacy and security concerns through tangible controls and clear data collection indicators to ensure user trust and adoption.

Abstract: Equipped with artificial intelligence (AI) and advanced sensing capabilities,
social robots are gaining interest among consumers in the United States. These
robots seem like a natural evolution of traditional smart home devices.
However, their extensive data collection capabilities, anthropomorphic
features, and capacity to interact with their environment make social robots a
more significant security and privacy threat. Increased risks include data
linkage, unauthorized data sharing, and the physical safety of users and their
homes. It is critical to investigate U.S. users' security and privacy needs and
concerns to guide the design of social robots while these devices are still in
the early stages of commercialization in the U.S. market. Through 19
semi-structured interviews, we identified significant security and privacy
concerns, highlighting the need for transparency, usability, and robust privacy
controls to support adoption. For educational applications, participants
worried most about misinformation, and in medical use cases, they worried about
the reliability of these devices. Participants were also concerned with the
data inference that social robots could enable. We found that participants
expect tangible privacy controls, indicators of data collection, and
context-appropriate functionality.

</details>


<div id='q-fin.TR'></div>

# q-fin.TR [[Back]](#toc)

### [262] [Kernel Learning for Mean-Variance Trading Strategies](https://arxiv.org/abs/2507.10701)
*Owen Futter,Nicola Muca Cirone,Blanka Horvath*

Main category: q-fin.TR

TL;DR: A kernel-based framework for dynamic, path-dependent trading strategies under mean-variance optimization is developed, outperforming classical methods.


<details>
  <summary>Details</summary>
Motivation: To address limitations of Markovian methods in handling temporal dependencies in asset dynamics or predictive signals.

Method: Parameterizes strategies in a reproducing kernel Hilbert space (RKHS), comparing with signature-based frameworks.

Result: Outperforms classical Markovian methods in synthetic and market-data examples.

Conclusion: Kernel-based approach offers flexibility, closed-form solutions, and an alternative to gradient-based optimization.

Abstract: In this article, we develop a kernel-based framework for constructing
dynamic, pathdependent trading strategies under a mean-variance optimisation
criterion. Building on the theoretical results of (Muca Cirone and Salvi,
2025), we parameterise trading strategies as functions in a reproducing kernel
Hilbert space (RKHS), enabling a flexible and non-Markovian approach to optimal
portfolio problems. We compare this with the signature-based framework of
(Futter, Horvath, Wiese, 2023) and demonstrate that both significantly
outperform classical Markovian methods when the asset dynamics or predictive
signals exhibit temporal dependencies for both synthetic and market-data
examples. Using kernels in this context provides significant modelling
flexibility, as the choice of feature embedding can range from randomised
signatures to the final layers of neural network architectures. Crucially, our
framework retains closed-form solutions and provides an alternative to
gradient-based optimisation.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [263] [TaylorPODA: A Taylor Expansion-Based Method to Improve Post-Hoc Attributions for Opaque Models](https://arxiv.org/abs/2507.10643)
*Yuchi Tang,Iñaki Esnaola,Suzanne Mason,George Panoutsos*

Main category: stat.ML

TL;DR: The paper introduces TaylorPODA, a method for feature attribution in opaque models, guided by postulates like precision and adaptation, offering improved theoretical grounding and visualization-friendly explanations.


<details>
  <summary>Details</summary>
Motivation: Existing post-hoc model-agnostic methods lack a systematic framework for quantifying feature contributions, limiting their reliability and theoretical grounding.

Method: The authors propose TaylorPODA, based on Taylor expansion, with postulates (precision, federation, zero-discrepancy) and an adaptation property for task-specific alignment.

Result: Empirical evaluations show TaylorPODA achieves competitive results and provides principled, visualization-friendly explanations.

Conclusion: TaylorPODA advances trustworthy deployment of opaque models by enhancing theoretical grounding and practical utility of explanations.

Abstract: Existing post-hoc model-agnostic methods generate external explanations for
opaque models, primarily by locally attributing the model output to its input
features. However, they often lack an explicit and systematic framework for
quantifying the contribution of individual features. Building on the Taylor
expansion framework introduced by Deng et al. (2024) to unify existing local
attribution methods, we propose a rigorous set of postulates -- "precision",
"federation", and "zero-discrepancy" -- to govern Taylor term-specific
attribution. Guided by these postulates, we introduce TaylorPODA (Taylor
expansion-derived imPortance-Order aDapted Attribution), which incorporates an
additional "adaptation" property. This property enables alignment with
task-specific goals, especially in post-hoc settings lacking ground-truth
explanations. Empirical evaluations demonstrate that TaylorPODA achieves
competitive results against baseline methods, providing principled and
visualization-friendly explanations. This work represents a step toward the
trustworthy deployment of opaque models by offering explanations with stronger
theoretical grounding.

</details>


### [264] [Robust Multi-Manifold Clustering via Simplex Paths](https://arxiv.org/abs/2507.10710)
*Haoyu Chen,Anna Little,Akin Narayan*

Main category: stat.ML

TL;DR: A geometric method for multi-manifold clustering (MMC) using a novel metric (LAPD) based on dihedral angles and infinity path distances, proven effective under random sampling and validated with experiments.


<details>
  <summary>Details</summary>
Motivation: To cluster intersecting d-dimensional manifolds into individual components, addressing challenges like noise, curvature, and small intersection angles.

Method: Compute a locality graph on d-simplices using dihedral angles, then derive the largest angle path distance (LAPD) metric. Apply denoising and analyze LAPD properties under random sampling.

Result: LAPD separates manifold components with high probability, outperforming other MMC algorithms in robustness to noise, curvature, and intersection angles.

Conclusion: The proposed method is effective, scalable (quasi-linear complexity), and validated by synthetic and real-world experiments.

Abstract: This article introduces a novel, geometric approach for multi-manifold
clustering (MMC), i.e. for clustering a collection of potentially intersecting,
d-dimensional manifolds into the individual manifold components. We first
compute a locality graph on d-simplices, using the dihedral angle in between
adjacent simplices as the graph weights, and then compute infinity path
distances in this simplex graph. This procedure gives a metric on simplices
which we refer to as the largest angle path distance (LAPD). We analyze the
properties of LAPD under random sampling, and prove that with an appropriate
denoising procedure, this metric separates the manifold components with high
probability. We validate the proposed methodology with extensive numerical
experiments on both synthetic and real-world data sets. These experiments
demonstrate that the method is robust to noise, curvature, and small
intersection angle, and generally out-performs other MMC algorithms. In
addition, we provide a highly scalable implementation of the proposed
algorithm, which leverages approximation schemes for infinity path distance to
achieve quasi-linear computational complexity.

</details>


### [265] [GOLFS: Feature Selection via Combining Both Global and Local Information for High Dimensional Clustering](https://arxiv.org/abs/2507.10956)
*Zhaoyu Xing,Yang Wan,Juan Wen,Wei Zhong*

Main category: stat.ML

TL;DR: GOLFS is an unsupervised feature selection method combining global and local structures for high-dimensional clustering, improving accuracy and performance.


<details>
  <summary>Details</summary>
Motivation: Discriminative feature identification is crucial for high-dimensional clustering, but unsupervised settings lack labels for traditional methods.

Method: GOLFS integrates manifold learning (local structure) and regularized self-representation (global structure) to select features iteratively.

Result: GOLFS shows superior performance in simulations and real data for feature selection and clustering.

Conclusion: GOLFS effectively combines global and local information for unsupervised feature selection, enhancing clustering accuracy.

Abstract: It is important to identify the discriminative features for high dimensional
clustering. However, due to the lack of cluster labels, the regularization
methods developed for supervised feature selection can not be directly applied.
To learn the pseudo labels and select the discriminative features
simultaneously, we propose a new unsupervised feature selection method, named
GlObal and Local information combined Feature Selection (GOLFS), for high
dimensional clustering problems. The GOLFS algorithm combines both local
geometric structure via manifold learning and global correlation structure of
samples via regularized self-representation to select the discriminative
features. The combination improves the accuracy of both feature selection and
clustering by exploiting more comprehensive information. In addition, an
iterative algorithm is proposed to solve the optimization problem and the
convergency is proved. Simulations and two real data applications demonstrate
the excellent finite-sample performance of GOLFS on both feature selection and
clustering.

</details>


### [266] [Interpretable Bayesian Tensor Network Kernel Machines with Automatic Rank and Feature Selection](https://arxiv.org/abs/2507.11136)
*Afra Kilic,Kim Batselier*

Main category: stat.ML

TL;DR: Bayesian Tensor Network Kernel Machines introduce a probabilistic framework for automatic model complexity tuning and uncertainty quantification, outperforming deterministic methods in accuracy and scalability.


<details>
  <summary>Details</summary>
Motivation: Existing TN-based Kernel methods are deterministic, lack uncertainty quantification, and require manual hyperparameter tuning, which is inefficient.

Method: The proposed method uses sparsity-inducing hierarchical priors on TN factors for automatic complexity inference and applies mean-field variational inference for posterior approximation.

Result: The model achieves better prediction accuracy, uncertainty quantification, interpretability, and scalability on synthetic and real-world datasets.

Conclusion: The Bayesian approach enhances TN Kernel Machines by automating complexity tuning and providing uncertainty quantification without extra computational cost.

Abstract: Tensor Network (TN) Kernel Machines speed up model learning by representing
parameters as low-rank TNs, reducing computation and memory use. However, most
TN-based Kernel methods are deterministic and ignore parameter uncertainty.
Further, they require manual tuning of model complexity hyperparameters like
tensor rank and feature dimensions, often through trial-and-error or
computationally costly methods like cross-validation. We propose Bayesian
Tensor Network Kernel Machines, a fully probabilistic framework that uses
sparsity-inducing hierarchical priors on TN factors to automatically infer
model complexity. This enables automatic inference of tensor rank and feature
dimensions, while also identifying the most relevant features for prediction,
thereby enhancing model interpretability. All the model parameters and
hyperparameters are treated as latent variables with corresponding priors.
Given the Bayesian approach and latent variable dependencies, we apply a
mean-field variational inference to approximate their posteriors. We show that
applying a mean-field approximation to TN factors yields a Bayesian ALS
algorithm with the same computational complexity as its deterministic
counterpart, enabling uncertainty quantification at no extra computational
cost. Experiments on synthetic and real-world datasets demonstrate the superior
performance of our model in prediction accuracy, uncertainty quantification,
interpretability, and scalability.

</details>


### [267] [How does Labeling Error Impact Contrastive Learning? A Perspective from Data Dimensionality Reduction](https://arxiv.org/abs/2507.11161)
*Jun Chen,Hong Chen,Yonghua Yu,Yiming Ying*

Main category: stat.ML

TL;DR: The paper explores the impact of labeling errors in contrastive learning on downstream classification, proposing dimensionality reduction (SVD) to mitigate negative effects while noting its trade-offs.


<details>
  <summary>Details</summary>
Motivation: To understand how labeling errors, caused by strong augmentations like RRC, affect contrastive learning's downstream performance and find solutions to mitigate these issues.

Method: The study uses dimensionality reduction (SVD) to reduce false positives and evaluates its impact theoretically and empirically, also considering augmentation strategies.

Result: SVD helps reduce labeling errors but can harm classification accuracy by reducing graph connectivity. Moderate embedding dimensions, weak augmentations, and SVD are recommended.

Conclusion: Balancing dimensionality reduction and augmentation strategies is crucial for optimizing contrastive learning performance.

Abstract: In recent years, contrastive learning has achieved state-of-the-art
performance in the territory of self-supervised representation learning. Many
previous works have attempted to provide the theoretical understanding
underlying the success of contrastive learning. Almost all of them rely on a
default assumption, i.e., the label consistency assumption, which may not hold
in practice (the probability of failure is called labeling error) due to the
strength and randomness of common augmentation strategies, such as random
resized crop (RRC). This paper investigates the theoretical impact of labeling
error on the downstream classification performance of contrastive learning. We
first reveal several significant negative impacts of labeling error on
downstream classification risk. To mitigate these impacts, data dimensionality
reduction method (e.g., singular value decomposition, SVD) is applied on
original data to reduce false positive samples, and establish both theoretical
and empirical evaluations. Moreover, it is also found that SVD acts as a
double-edged sword, which may lead to the deterioration of downstream
classification accuracy due to the reduced connectivity of the augmentation
graph. Based on the above observations, we give the augmentation suggestion
that we should use some moderate embedding dimension (such as $512, 1024$ in
our experiments), data inflation, weak augmentation, and SVD to ensure large
graph connectivity and small labeling error to improve model performance.

</details>


### [268] [From Observational Data to Clinical Recommendations: A Causal Framework for Estimating Patient-level Treatment Effects and Learning Policies](https://arxiv.org/abs/2507.11381)
*Rom Gutman,Shimon Sheiba,Omer Noy Klien,Naama Dekel Bird,Amit Gruber,Doron Aronson,Oren Caspi,Uri Shalit*

Main category: stat.ML

TL;DR: A framework for patient-specific treatment recommendations, emphasizing safety and causal validity, with a real-world application in heart failure patients with acute kidney injury.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of building safe and valid patient-specific treatment models using observational data, inspired by causal modeling literature and the target trial paradigm.

Method: Proposes a practical pipeline integrating existing causal modeling methods, focusing on causal identification and observational data. Includes a real-world use-case for treatment optimization.

Result: The pipeline demonstrates potential to improve patient outcomes over current treatment regimes in the heart failure and acute kidney injury use-case.

Conclusion: The framework offers a practical approach to integrating causal methods for personalized treatment recommendations, with promising real-world applicability.

Abstract: We propose a framework for building patient-specific treatment recommendation
models, building on the large recent literature on learning patient-level
causal models and inspired by the target trial paradigm of Hernan and Robins.
We focus on safety and validity, including the crucial issue of causal
identification when using observational data. We do not provide a specific
model, but rather a way to integrate existing methods and know-how into a
practical pipeline. We further provide a real world use-case of treatment
optimization for patients with heart failure who develop acute kidney injury
during hospitalization. The results suggest our pipeline can improve patient
outcomes over the current treatment regime.

</details>


### [269] [Joint space-time wind field data extrapolation and uncertainty quantification using nonparametric Bayesian dictionary learning](https://arxiv.org/abs/2507.11385)
*George D. Pasparakis,Ioannis A. Kougioumtzoglou,Michael D. Shields*

Main category: stat.ML

TL;DR: A Bayesian dictionary learning method for wind field data extrapolation and statistics estimation from sparse measurements, outperforming standard compressive sampling by quantifying uncertainty and adaptively selecting basis functions.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of estimating wind field statistics and extrapolating data from limited measurements, common in wind engineering applications.

Method: Nonparametric Bayesian dictionary learning to formulate a time-dependent optimization problem for low-dimensional representation of stochastic wind fields.

Result: Enhanced extrapolation accuracy, handling high-dimensional data and large distances, demonstrated via simulated and experimental case studies.

Conclusion: The method is effective for wind engineering, especially with limited sensors, due to its adaptive basis selection and uncertainty quantification.

Abstract: A methodology is developed, based on nonparametric Bayesian dictionary
learning, for joint space-time wind field data extrapolation and estimation of
related statistics by relying on limited/incomplete measurements. Specifically,
utilizing sparse/incomplete measured data, a time-dependent optimization
problem is formulated for determining the expansion coefficients of an
associated low-dimensional representation of the stochastic wind field.
Compared to an alternative, standard, compressive sampling treatment of the
problem, the developed methodology exhibits the following advantages. First,
the Bayesian formulation enables also the quantification of the uncertainty in
the estimates. Second, the requirement in standard CS-based applications for an
a priori selection of the expansion basis is circumvented. Instead, this is
done herein in an adaptive manner based on the acquired data. Overall, the
methodology exhibits enhanced extrapolation accuracy, even in cases of
high-dimensional data of arbitrary form, and of relatively large extrapolation
distances. Thus, it can be used, potentially, in a wide range of wind
engineering applications where various constraints dictate the use of a limited
number of sensors. The efficacy of the methodology is demonstrated by
considering two case studies. The first relates to the extrapolation of
simulated wind velocity records consistent with a prescribed joint
wavenumber-frequency power spectral density in a three-dimensional domain (2D
and time). The second pertains to the extrapolation of four-dimensional (3D and
time) boundary layer wind tunnel experimental data that exhibit significant
spatial variability and non-Gaussian characteristics.

</details>


### [270] [Canonical Bayesian Linear System Identification](https://arxiv.org/abs/2507.11535)
*Andrey Bryutkin,Matthew E. Levine,Iñigo Urteaga,Youssef Marzouk*

Main category: stat.ML

TL;DR: The paper proposes embedding canonical forms of LTI systems in Bayesian frameworks to resolve parameter non-identifiability, improving inference efficiency and enabling structure-aware priors.


<details>
  <summary>Details</summary>
Motivation: Standard Bayesian methods for LTI system identification face challenges due to non-identifiable parameters, leading to inefficient inference.

Method: The authors use canonical forms of LTI systems within Bayesian inference to ensure identifiability and capture invariant dynamics.

Result: Canonical forms improve computational efficiency, yield interpretable posteriors, and provide robust uncertainty estimates, especially with limited data.

Conclusion: This approach resolves identifiability issues, enables meaningful priors, and ensures theoretical guarantees like the Bernstein--von Mises theorem.

Abstract: Standard Bayesian approaches for linear time-invariant (LTI) system
identification are hindered by parameter non-identifiability; the resulting
complex, multi-modal posteriors make inference inefficient and impractical. We
solve this problem by embedding canonical forms of LTI systems within the
Bayesian framework. We rigorously establish that inference in these minimal
parameterizations fully captures all invariant system dynamics (e.g., transfer
functions, eigenvalues, predictive distributions of system outputs) while
resolving identifiability. This approach unlocks the use of meaningful,
structure-aware priors (e.g., enforcing stability via eigenvalues) and ensures
conditions for a Bernstein--von Mises theorem -- a link between Bayesian and
frequentist large-sample asymptotics that is broken in standard forms.
Extensive simulations with modern MCMC methods highlight advantages over
standard parameterizations: canonical forms achieve higher computational
efficiency, generate interpretable and well-behaved posteriors, and provide
robust uncertainty estimates, particularly from limited data.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [271] [Learning to Quantize and Precode in Massive MIMO Systems for Energy Reduction: a Graph Neural Network Approach](https://arxiv.org/abs/2507.10634)
*Thomas Feys,Liesbet Van der Perre,François Rottenberg*

Main category: eess.SY

TL;DR: A GNN-based non-linear precoding method for coarsely quantized massive MIMO downlink improves achievable rates and reduces DAC power consumption, though with increased digital signal processing power.


<details>
  <summary>Details</summary>
Motivation: Address the bottleneck of DACs in massive MIMO systems due to hardware complexity and power consumption by exploring non-linear precoding for coarse quantization.

Method: Proposes a graph neural network (GNN) trained self-supervised to output precoded quantized vectors, using Gumbel-softmax gradient estimation to handle non-differentiable DAC functions.

Result: Achieves higher sum rates with coarse quantization (e.g., 1-bit DACs matching MRT's performance with 3-bit DACs), reducing DAC power consumption by factors of 4-7 (baseband) and 3 (RF).

Conclusion: The method significantly reduces overall power consumption for certain bandwidths, though trade-offs exist with increased digital processing power.

Abstract: Massive MIMO systems are moving toward increased numbers of radio frequency
chains, higher carrier frequencies and larger bandwidths. As such,
digital-to-analog converters (DACs) are becoming a bottleneck in terms of
hardware complexity and power consumption. In this work, non-linear precoding
for coarsely quantized downlink massive MIMO is studied. Given the NP-hard
nature of this problem, a graph neural network (GNN) is proposed that directly
outputs the precoded quantized vector based on the channel matrix and the
intended transmit symbols. The model is trained in a self-supervised manner, by
directly maximizing the achievable rate. To overcome the non-differentiability
of the objective function, introduced due to the non-differentiable DAC
functions, a straight-through Gumbel-softmax estimation of the gradient is
proposed. The proposed method achieves a significant increase in achievable sum
rate under coarse quantization. For instance, in the single-user case, the
proposed method can achieve the same sum rate as maximum ratio transmission
(MRT) by using one-bit DAC's as compared to 3 bits for MRT. This reduces the
DAC's power consumption by a factor 4-7 and 3 for baseband and RF DACs
respectively. This, however, comes at the cost of increased digital signal
processing power consumption. When accounting for this, the reduction in
overall power consumption holds for a system bandwidth up to 3.5 MHz for
baseband DACs, while the RF DACs can maintain a power reduction of 2.9 for
higher bandwidths. Notably, indirect effects, which further reduce the power
consumption, such as a reduced fronthaul consumption and reduction in other
components, are not considered in this analysis.

</details>


### [272] [Standards-Compliant DM-RS Allocation via Temporal Channel Prediction for Massive MIMO Systems](https://arxiv.org/abs/2507.11064)
*Sehyun Ryu,Hyun Jong Yang*

Main category: eess.SY

TL;DR: The paper proposes a novel method, CPRS, to optimize reference signal allocation in beyond 5G networks without CSI feedback, achieving a 36.60% throughput improvement.


<details>
  <summary>Details</summary>
Motivation: The challenge of reducing feedback overhead in massive MIMO systems, especially in FDD, where CSI feedback demand is high, motivates the exploration of neural network-based solutions.

Method: Introduces CPRS, a joint optimization of channel prediction and DM-RS allocation using a ViViT/CNN architecture, treating CSI matrices as sequential image-like data.

Result: Simulations with ray-tracing data show a 36.60% throughput improvement over benchmarks.

Conclusion: CPRS effectively addresses the underexplored problem of reference signal allocation, offering a standards-compliant solution for dynamic environments.

Abstract: Reducing feedback overhead in beyond 5G networks is a critical challenge, as
the growing number of antennas in modern massive MIMO systems substantially
increases the channel state information (CSI) feedback demand in frequency
division duplex (FDD) systems. To address this, extensive research has focused
on CSI compression and prediction, with neural network-based approaches gaining
momentum and being considered for integration into the 3GPP 5G-Advanced
standards. While deep learning has been effectively applied to CSI-limited
beamforming and handover optimization, reference signal allocation under such
constraints remains surprisingly underexplored. To fill this gap, we introduce
the concept of channel prediction-based reference signal allocation (CPRS),
which jointly optimizes channel prediction and DM-RS allocation to improve data
throughput without requiring CSI feedback. We further propose a
standards-compliant ViViT/CNN-based architecture that implements CPRS by
treating evolving CSI matrices as sequential image-like data, enabling
efficient and adaptive transmission in dynamic environments. Simulation results
using ray-tracing channel data generated in NVIDIA Sionna validate the proposed
method, showing up to 36.60% throughput improvement over benchmark strategies.

</details>


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [273] [Recent Advances in Simulation-based Inference for Gravitational Wave Data Analysis](https://arxiv.org/abs/2507.11192)
*Bo Liang,He Wang*

Main category: gr-qc

TL;DR: The paper reviews simulation-based inference methods, like machine-learning techniques, for gravitational wave data analysis, highlighting their potential and challenges compared to traditional Bayesian methods.


<details>
  <summary>Details</summary>
Motivation: The need for faster and more efficient parameter estimation and population-level analyses in gravitational wave astronomy due to computational limitations of traditional methods.

Method: Examines simulation-based inference methods, including neural posterior estimation, neural ratio estimation, and flow matching, applied to gravitational wave data.

Result: These methods show speed improvements but face challenges like model-dependency and prior sensitivity, with accuracy needing further validation.

Conclusion: Simulation-based inference holds promise for gravitational wave astronomy but requires broader validation and refinement for widespread adoption.

Abstract: The detection of gravitational waves by the LIGO-Virgo-KAGRA collaboration
has ushered in a new era of observational astronomy, emphasizing the need for
rapid and detailed parameter estimation and population-level analyses.
Traditional Bayesian inference methods, particularly Markov chain Monte Carlo,
face significant computational challenges when dealing with the
high-dimensional parameter spaces and complex noise characteristics inherent in
gravitational wave data. This review examines the emerging role of
simulation-based inference methods in gravitational wave astronomy, with a
focus on approaches that leverage machine-learning techniques such as
normalizing flows and neural posterior estimation. We provide a comprehensive
overview of the theoretical foundations underlying various simulation-based
inference methods, including neural posterior estimation, neural ratio
estimation, neural likelihood estimation, flow matching, and consistency
models. We explore the applications of these methods across diverse
gravitational wave data processing scenarios, from single-source parameter
estimation and overlapping signal analysis to testing general relativity and
conducting population studies. Although these techniques demonstrate speed
improvements over traditional methods in controlled studies, their
model-dependent nature and sensitivity to prior assumptions are barriers to
their widespread adoption. Their accuracy, which is similar to that of
conventional methods, requires further validation across broader parameter
spaces and noise conditions.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [274] [Improved sampling algorithms and Poincaré inequalities for non-log-concave distributions](https://arxiv.org/abs/2507.11236)
*Yuchen He,Zhehan Lei,Jianan Shao,Chihao Zhang*

Main category: cs.DS

TL;DR: The paper studies sampling from distributions with specific smoothness and moment assumptions, showing improved query complexity under stronger smoothness conditions and deriving bounds on the Poincaré constant.


<details>
  <summary>Details</summary>
Motivation: Understanding the query complexity of sampling algorithms under varying smoothness and moment assumptions, and exploring the impact of these assumptions on computational efficiency.

Method: Analyzes sampling algorithms under strengthened smoothness conditions (1*) and moment assumptions (2), comparing query complexities and deriving bounds on the Poincaré constant.

Result: Under (1*) and (2), query complexity becomes polynomial in dimension and accuracy, improving prior quasi-polynomial results. Also, bounds the Poincaré constant for sub-Gaussian distributions.

Conclusion: Strengthening smoothness assumptions can exponentially reduce query complexity, and the technique provides improved bounds for mixtures of Gaussians.

Abstract: We study the problem of sampling from a distribution $\mu$ with density
$\propto e^{-V}$ for some potential function $V:\mathbb R^d\to \mathbb R$ with
query access to $V$ and $\nabla V$. We start with the following standard
assumptions:
  (1) The potential function $V$ is $L$-smooth.
  (2) The second moment $\mathbf{E}_{X\sim \mu}[\|X\|^2]\leq M$.
  Recently, He and Zhang (COLT'25) showed that the query complexity of sampling
from such distributions is at least
$\left(\frac{LM}{d\epsilon}\right)^{\Omega(d)}$ where $\epsilon$ is the desired
accuracy in total variation distance, and the Poincar\'e constant can be
arbitrarily large.
  Meanwhile, another common assumption in the study of diffusion based samplers
(see e.g., the work of Chen, Chewi, Li, Li, Salim and Zhang (ICLR'23))
strengthens the smoothness condition (1) to the following:
  (1*) The potential function of *every* distribution along the
Ornstein-Uhlenbeck process starting from $\mu$ is $L$-smooth.
  We show that under the assumptions (1*) and (2), the query complexity of
sampling from $\mu$ can be $\mathrm{poly}(L,d)\cdot
\left(\frac{Ld+M}{\epsilon^2}\right)^{\mathcal{O}(L+1)}$, which is polynomial
in $d$ and $\frac{1}{\epsilon}$ when $L=\mathcal{O}(1)$ and
$M=\mathrm{poly}(d)$. This improves the algorithm with quasi-polynomial query
complexity developed by Huang et al. (COLT'24). Our results imply that the
seemly moderate strengthening of the smoothness condition (1) to (1*) can lead
to an exponential gap in the query complexity of sampling algorithms.
  Moreover, we show that together with the assumption (1*) and the stronger
moment assumption that $\|X\|$ is $\lambda$-sub-Gaussian for $X\sim\mu$, the
Poincar\'e constant of $\mu$ is at most $\mathcal{O}(\lambda)^{2(L+1)}$. As an
application of our technique, we obtain improved estimate of the Poincar\'e
constant for mixture of Gaussians with the same covariance.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [275] [SPICEAssistant: LLM using SPICE Simulation Tools for Schematic Design of Switched-Mode Power Supplies](https://arxiv.org/abs/2507.10639)
*Simon Nau,Jan Krummenauer,André Zimmermann*

Main category: cs.AR

TL;DR: SPICEAssistant enhances LLMs' ability to design SMPS circuits by integrating SPICE simulation tools, improving performance by 38% over standalone GPT-4o.


<details>
  <summary>Details</summary>
Motivation: To explore LLMs' potential in EDA, specifically SMPS design, addressing challenges like SPICE interpretation and multi-step design.

Method: Proposed SPICEAssistant, a framework providing LLMs with tools to interact with SPICE simulations for circuit adaptation.

Result: Simulation feedback improves SMPS design; SPICEAssistant outperforms GPT-4o by 38% on a 256-question benchmark.

Conclusion: SPICEAssistant effectively bridges LLMs' limitations in EDA, demonstrating significant performance gains in SMPS design.

Abstract: State-of-the-art large language models (LLMs) show high performance across a
wide range of tasks in many domains of science. In the field of electronic
design automation (EDA), it is yet to be determined to what extent they are
capable to understand, adapt, and dimension electronic circuits. This paper
focuses on the application of LLMs to switched-mode power supply (SMPS) design
on printed circuit boards (PCBs). Particular challenges for LLMs in this
context include their limited ability to interpret results from key simulation
tools like SPICE and the multi-step design process. To address these
challenges, we suggest SPICEAssistant, a framework that provides a broad
selection of tools to an LLM. The tools serve as an interface to SPICE,
allowing the LLM to interact flexibly with the simulator to estimate the impact
of its modifications to the circuit. To evaluate the performance of
SPICEAssistant, we defined a benchmark consisting of 256 questions testing the
ability to adapt circuit netlists to fulfil different SMPS design tasks. The
benchmarking results show that simulation feedback effectively improves SMPS
design capabilities of LLMs. An increasing number of simulation iterations
leads to enhanced performance. The SPICEAssistant framework significantly
outperforms the standalone LLM GPT-4o on the benchmark by approximately 38%.

</details>


### [276] [Elk: Exploring the Efficiency of Inter-core Connected AI Chips with Deep Learning Compiler Techniques](https://arxiv.org/abs/2507.11506)
*Yiqi Liu,Yuqi Xue,Noelle Crawford,Jilong Xue,Jian Huang*

Main category: cs.AR

TL;DR: Elk is a DL compiler framework optimizing ICCA chip efficiency by balancing compute, communication, and I/O. It achieves 94% of ideal performance and aids in chip design exploration.


<details>
  <summary>Details</summary>
Motivation: The challenge lies in balancing compute, communication, and I/O in inter-core connected AI (ICCA) chips for deep learning models.

Method: Elk structures performance factors into configurable parameters, uses inductive operator scheduling, and cost-aware memory allocation to optimize execution plans.

Result: Elk achieves 94% of ideal roofline performance on ICCA chips and supports architecture design space exploration.

Conclusion: Elk effectively maximizes ICCA chip efficiency and aids in future chip development.

Abstract: To meet the increasing demand of deep learning (DL) models, AI chips are
employing both off-chip memory (e.g., HBM) and high-bandwidth low-latency
interconnect for direct inter-core data exchange. However, it is not easy to
explore the efficiency of these inter-core connected AI (ICCA) chips, due to a
fundamental tussle among compute (per-core execution), communication
(inter-core data exchange), and I/O (off-chip data access).
  In this paper, we develop Elk, a DL compiler framework to maximize the
efficiency of ICCA chips by jointly trading off all the three performance
factors discussed above. Elk structures these performance factors into
configurable parameters and forms a global trade-off space in the DL compiler.
To systematically explore this space and maximize overall efficiency, Elk
employs a new inductive operator scheduling policy and a cost-aware on-chip
memory allocation algorithm. It generates globally optimized execution plans
that best overlap off-chip data loading and on-chip execution. To examine the
efficiency of Elk, we build a full-fledged emulator based on a real ICCA chip
IPU-POD4, and an ICCA chip simulator for sensitivity analysis with different
interconnect network topologies. Elk achieves 94% of the ideal roofline
performance of ICCA chips on average, showing the benefits of supporting large
DL models on ICCA chips. We also show Elk's capability of enabling architecture
design space exploration for new ICCA chip development.

</details>


### [277] [SystolicAttention: Fusing FlashAttention within a Single Systolic Array](https://arxiv.org/abs/2507.11331)
*Jiawei Lin,Guokai Chen,Yuanlong Li,Thomas Bourgeat*

Main category: cs.AR

TL;DR: FSA improves systolic array utilization for FlashAttention by enabling full execution within the array, outperforming commercial accelerators.


<details>
  <summary>Details</summary>
Motivation: Current systolic-array-based accelerators struggle with FlashAttention due to inefficient interleaving of matrix multiplications and softmax operations, leading to low utilization.

Method: Proposes FSA, an enhanced systolic array architecture with SystolicAttention scheduling, allowing FlashAttention to run entirely within the array.

Result: FSA achieves 1.77x and 4.83x higher FLOPs/s utilization than AWS NeuronCore-v2 and Google TPUv5e, with minimal area overhead.

Conclusion: FSA effectively addresses the limitations of current accelerators for FlashAttention, offering significant performance improvements.

Abstract: Transformer models rely heavily on scaled dot-product attention (SDPA),
typically implemented using the FlashAttention algorithm. However, current
systolic-array-based accelerators face significant challenges when executing
FlashAttention. Systolic arrays can only achieve high utilization for
consecutive and large matrix multiplications. In contrast, FlashAttention
requires frequently interleaved matrix multiplications and softmax operations.
  The frequent data swaps between the systolic array and external vector units
result in low systolic array utilization. This is further exacerbated by the
fact that softmax involves numerous non-matrix operations, which are not
well-suited for systolic arrays. Moreover, the concurrent execution of matrix
multiplication on systolic arrays and softmax on vector units leads to register
file and SRAM port contention, further degrading performance.
  To overcome these limitations, we propose FSA, an enhanced systolic array
architecture that enables the entire FlashAttention algorithm to run entirely
within a single systolic array, eliminating the need for external vector units.
At the core of FSA is SystolicAttention, a novel scheduling algorithm that maps
FlashAttention operations onto systolic arrays with fine-grained, element-wise
overlap. This significantly improves array utilization while preserving the
original floating-point operation order to maintain numerical stability.
  We implement FSA in synthesizable RTL and evaluate its performance against
state-of-the-art commercial accelerators. Our results show that FSA achieves
1.77x and 4.83x higher attention FLOPs/s utilization compared to AWS
NeuronCore-v2 and Google TPUv5e, respectively, with only about 10% area
overhead.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [278] [Theory of Mind and Self-Disclosure to CUIs](https://arxiv.org/abs/2507.10773)
*Samuel Rhys Cox*

Main category: cs.HC

TL;DR: The paper explores how social cues and expressions of uncertainty in conversational user interfaces (CUIs) can encourage self-disclosure by making the CUI's 'theory of mind' clearer to users.


<details>
  <summary>Details</summary>
Motivation: Self-disclosure is challenging due to concerns about others' reactions. The paper aims to address this by examining how CUIs can facilitate self-disclosure.

Method: The study discusses self-disclosure to CUIs in relation to social cues and explores the role of uncertainty expressions and reasoning transparency.

Result: Expressions of uncertainty and transparent reasoning in CUIs may encourage self-disclosure by clarifying the CUI's 'theory of mind.'

Conclusion: Enhancing transparency in CUIs' reasoning and uncertainty expressions can foster self-disclosure, addressing the difficulty users face in sharing personal information.

Abstract: Self-disclosure is important to help us feel better, yet is often difficult.
This difficulty can arise from how we think people are going to react to our
self-disclosure. In this workshop paper, we briefly discuss self-disclosure to
conversational user interfaces (CUIs) in relation to various social cues. We
then, discuss how expressions of uncertainty or representation of a CUI's
reasoning could help encourage self-disclosure, by making a CUI's intended
"theory of mind" more transparent to users.

</details>


### [279] [React to This (RTT): A Nonverbal Turing Test for Embodied AI](https://arxiv.org/abs/2507.10812)
*Chuxuan Zhang,Yasaman Etesam,Angelica Lim*

Main category: cs.HC

TL;DR: The paper proposes the React to This (RTT) test to evaluate embodied AI agents' interaction awareness and believability, focusing on nonverbal behaviors in challenging human interactions.


<details>
  <summary>Details</summary>
Motivation: To explore whether machines can react believably in nonverbal scenarios, inspired by Turing's Imitation Game and Total Turing Test.

Method: Introduces the RTT test for nonverbal behaviors and conducts an initial experiment to evaluate AI agents.

Result: Presents findings from the initial RTT experiment, though specific results are not detailed in the abstract.

Conclusion: The RTT test offers a new framework for assessing AI agents' nonverbal interaction capabilities, expanding beyond traditional Turing tests.

Abstract: We propose an approach to test embodied AI agents for interaction awareness
and believability, particularly in scenarios where humans push them to their
limits. Turing introduced the Imitation Game as a way to explore the question:
"Can machines think?" The Total Turing Test later expanded this concept beyond
purely verbal communication, incorporating perceptual and physical interaction.
Building on this, we propose a new guiding question: "Can machines react?" and
introduce the React to This (RTT) test for nonverbal behaviors, presenting
results from an initial experiment.

</details>


### [280] [Role-Playing LLM-Based Multi-Agent Support Framework for Detecting and Addressing Family Communication Bias](https://arxiv.org/abs/2507.11210)
*Rushia Harada,Yuken Kimura,Keito Inoshita*

Main category: cs.HC

TL;DR: The study explores an LLM-based framework to address suppressed emotion and ideal parent bias in family communication, showing promising results in improving emotional expression and understanding.


<details>
  <summary>Details</summary>
Motivation: Conventional metrics overlook subtle psychological dynamics in families, such as unconscious parental expectations (ideal parent bias) and suppressed emotion in children.

Method: Developed a Role-Playing LLM-based multi-agent framework using a Japanese parent-child dialogue corpus, with specialized agents detecting suppressed emotion and ideal parent bias, and expert agents generating feedback.

Result: The system detected suppressed emotion with moderate accuracy and produced highly empathetic and practical feedback, improving emotional expression in follow-up dialogues.

Conclusion: The framework shows potential for supporting positive transformation in family interactions by addressing latent psychological dynamics.

Abstract: Well-being in family settings involves subtle psychological dynamics that
conventional metrics often overlook. In particular, unconscious parental
expectations, termed ideal parent bias, can suppress children's emotional
expression and autonomy. This suppression, referred to as suppressed emotion,
often stems from well-meaning but value-driven communication, which is
difficult to detect or address from outside the family. Focusing on these
latent dynamics, this study explores Large Language Model (LLM)-based support
for psychologically safe family communication. We constructed a Japanese
parent-child dialogue corpus of 30 scenarios, each annotated with metadata on
ideal parent bias and suppressed emotion. Based on this corpus, we developed a
Role-Playing LLM-based multi-agent dialogue support framework that analyzes
dialogue and generates feedback. Specialized agents detect suppressed emotion,
describe implicit ideal parent bias in parental speech, and infer contextual
attributes such as the child's age and background. A meta-agent compiles these
outputs into a structured report, which is then passed to five selected expert
agents. These agents collaboratively generate empathetic and actionable
feedback through a structured four-step discussion process. Experiments show
that the system can detect categories of suppressed emotion with moderate
accuracy and produce feedback rated highly in empathy and practicality.
Moreover, simulated follow-up dialogues incorporating this feedback exhibited
signs of improved emotional expression and mutual understanding, suggesting the
framework's potential in supporting positive transformation in family
interactions.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [281] [BioScore: A Foundational Scoring Function For Diverse Biomolecular Complexes](https://arxiv.org/abs/2507.10877)
*Yuchen Zhu,Jihong Chen,Yitong Li,Xiaomin Fang,Xianbin Ye,Jingzhou He,Xujun Zhang,Jingxuan Ge,Chao Shen,Xiaonan Zhang,Tingjun Hou,Chang-Yu Hsieh*

Main category: physics.chem-ph

TL;DR: BioScore is a foundational scoring function using a dual-scale geometric graph learning framework to improve structural assessment of biomolecular complexes, outperforming traditional methods across diverse tasks and systems.


<details>
  <summary>Details</summary>
Motivation: Current scoring functions lack generalizability across diverse biomolecular systems, limiting their utility in biology and drug discovery.

Method: BioScore employs a dual-scale geometric graph learning framework with tailored modules for structure assessment and affinity prediction, addressing data sparsity, cross-system representation, and task compatibility.

Result: BioScore outperforms or matches 70 traditional and deep learning methods across 16 benchmarks, showing significant improvements in affinity prediction, conformation ranking, and virtual screening.

Conclusion: BioScore provides a robust, generalizable framework for structural assessment, enhancing performance in diverse biomolecular tasks and systems.

Abstract: Structural assessment of biomolecular complexes is vital for translating
molecular models into functional insights, shaping our understanding of biology
and aiding drug discovery. However, current structure-based scoring functions
often lack generalizability across diverse biomolecular systems. We present
BioScore, a foundational scoring function that addresses key challenges -- data
sparsity, cross-system representation, and task compatibility -- through a
dual-scale geometric graph learning framework with tailored modules for
structure assessment and affinity prediction. BioScore supports a wide range of
tasks, including affinity prediction, conformation ranking, and structure-based
virtual screening. Evaluated on 16 benchmarks spanning proteins, nucleic acids,
small molecules, and carbohydrates, BioScore consistently outperforms or
matches 70 traditional and deep learning methods. Our newly proposed PPI
Benchmark further enables comprehensive evaluation of protein-protein complex
scoring. BioScore demonstrates broad applicability: (1) pretraining on
mixed-structure data boosts protein-protein affinity prediction by up to 40%
and antigen-antibody binding correlation by over 90%; (2) cross-system
generalizability enables zero- and few-shot prediction with up to 71%
correlation gain; and (3) its unified representation captures chemically
challenging systems such as cyclic peptides, improving affinity prediction by
over 60%. BioScore establishes a robust and generalizable framework for
structural assessment across complex biomolecular landscapes.

</details>


<div id='physics.app-ph'></div>

# physics.app-ph [[Back]](#toc)

### [282] [Real-time, Adaptive Radiological Anomaly Detection and Isotope Identification Using Non-negative Matrix Factorization](https://arxiv.org/abs/2507.10715)
*Chandler Jones,Mark Bandstra,Stefan Faaland,Yue Shi Lai,Nico Abgrall,Scott Suchyta,Reynold Cooper*

Main category: physics.app-ph

TL;DR: A novel Adaptive NMF algorithm is introduced for real-time background model updates in mobile gamma-ray detection, improving anomaly detection and isotope identification in changing environments.


<details>
  <summary>Details</summary>
Motivation: Mobile detector systems face challenges due to dynamic gamma-ray backgrounds, causing conventional algorithms to either exceed false alarm rates or lose sensitivity.

Method: The Adaptive NMF algorithm periodically updates its background model to adapt to environmental changes, reducing assumptions and improving generalizability.

Result: The algorithm maintains or surpasses detection performance on simulated and real-world datasets compared to existing NMF-based methods.

Conclusion: Adaptive NMF offers a more robust solution for nuclear nonproliferation applications by addressing the limitations of static background models.

Abstract: Spectroscopic anomaly detection and isotope identification algorithms are
integral components in nuclear nonproliferation applications such as search
operations. The task is especially challenging in the case of mobile detector
systems due to the fact that the observed gamma-ray background changes more
than for a static detector system, and a pretrained background model can easily
find itself out of domain. The result is that algorithms may exceed their
intended false alarm rate, or sacrifice detection sensitivity in order to
maintain the desired false alarm rate. Non-negative matrix factorization (NMF)
has been shown to be a powerful tool for spectral anomaly detection and
identification, but, like many similar algorithms that rely on data-driven
background models, in its conventional implementation it is unable to update in
real time to account for environmental changes that affect the background
spectroscopic signature. We have developed a novel NMF-based algorithm that
periodically updates its background model to accommodate changing environmental
conditions. The Adaptive NMF algorithm involves fewer assumptions about its
environment, making it more generalizable than existing NMF-based methods while
maintaining or exceeding detection performance on simulated and real-world
datasets.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [283] [Data-Driven Differential Evolution in Tire Industry Extrusion: Leveraging Surrogate Models](https://arxiv.org/abs/2507.11191)
*Eider Garate-Perez,Kerman López de Calle-Etxabe,Susana Ferreiro*

Main category: cs.CE

TL;DR: A surrogate-based, data-driven method optimizes industrial processes without explicit mathematical models, reducing setup time by 65% and minimizing waste.


<details>
  <summary>Details</summary>
Motivation: Industrial processes often lack mathematical formulations for optimization, necessitating data-driven approaches.

Method: Machine learning models approximate system behavior, integrated into a tailored metaheuristic (Data-Driven Differential Evolution) for optimization.

Result: Applied to tire manufacturing, the method reduced initialization time by 65% and minimized waste.

Conclusion: Combining data-driven modeling and metaheuristics is effective for optimizing complex industrial processes.

Abstract: The optimization of industrial processes remains a critical challenge,
particularly when no mathematical formulation of objective functions or
constraints is available. This study addresses this issue by proposing a
surrogate-based, data-driven methodology for optimizing complex real-world
manufacturing systems using only historical process data. Machine learning
models are employed to approximate system behavior and construct surrogate
models, which are integrated into a tailored metaheuristic approach:
Data-Driven Differential Evolution with Multi-Level Penalty Functions and
Surrogate Models, an adapted version of Differential Evolution suited to the
characteristics of the studied process. The methodology is applied to an
extrusion process in the tire manufacturing industry, with the goal of
optimizing initialization parameters to reduce waste and production time.
Results show that the surrogate-based optimization approach outperforms
historical best configurations, achieving a 65\% reduction in initialization
and setup time, while also significantly minimizing material waste. These
findings highlight the potential of combining data-driven modeling and
metaheuristic optimization for industrial processes where explicit formulations
are unavailable.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [284] [A Biomimetic Way for Coral-Reef-Inspired Swarm Intelligence for Carbon-Neutral Wastewater Treatment](https://arxiv.org/abs/2507.10563)
*Antonis Messinis*

Main category: cs.NE

TL;DR: A coral-reef-inspired Swarm Interaction Network for carbon-neutral wastewater treatment achieves high efficiency, low energy use, and reduced CO2 emissions, with potential diesel savings. Future work includes AutoML integration despite governance challenges.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of energy-neutral wastewater treatment due to increasing wastewater rates.

Method: Combines morphogenetic abstraction with multi-task carbon awareness, leveraging linear token complexity for scalability.

Result: Achieves 96.7% removal efficiency, 0.31 kWh/m³ energy consumption, and 14.2 g/m³ CO2 emissions, with robustness under sensor drift and potential diesel savings up to 22%.

Conclusion: The approach is promising but faces staffing and governance challenges; future work will focus on AutoML integration and visual analytics for interpretability.

Abstract: With increasing wastewater rates, achieving energy-neutral purification is
challenging. We introduce a coral-reef-inspired Swarm Interaction Network for
carbon-neutral wastewater treatment, combining morphogenetic abstraction with
multi-task carbon awareness. Scalability stems from linear token complexity,
mitigating the energy-removal problem. Compared with seven baselines, our
approach achieves 96.7\% removal efficiency, 0.31~kWh~m$^{-3}$ energy
consumption, and 14.2~g~m$^{-3}$ CO$_2$ emissions. Variance analysis
demonstrates robustness under sensor drift. Field scenarios--insular lagoons,
brewery spikes, and desert greenhouses--show potential diesel savings of up to
22\%. However, data-science staffing remains an impediment. Future work will
integrate AutoML wrappers within the project scope, although governance
restrictions pose interpretability challenges that require further visual
analytics.

</details>


### [285] [SFATTI: Spiking FPGA Accelerator for Temporal Task-driven Inference -- A Case Study on MNIST](https://arxiv.org/abs/2507.10561)
*Alessio Caviglia,Filippo Marostica,Alessio Carpegna,Alessandro Savino,Stefano Di Carlo*

Main category: cs.NE

TL;DR: The paper explores using the Spiker+ framework to optimize Spiking Neural Networks (SNNs) for FPGA-based handwritten digit recognition, focusing on edge computing constraints.


<details>
  <summary>Details</summary>
Motivation: Hardware accelerators are needed for low-latency, energy-efficient edge applications like image recognition, with SNNs being promising due to their event-driven nature.

Method: The Spiker+ framework is used to generate optimized SNNs accelerators for MNIST dataset recognition, allowing high-level specification of network topologies, neuron models, and quantization.

Result: Multiple configurations are evaluated, analyzing trade-offs relevant to edge computing constraints.

Conclusion: The Spiker+ framework effectively enables optimized SNN deployment for edge applications like handwritten digit recognition.

Abstract: Hardware accelerators are essential for achieving low-latency,
energy-efficient inference in edge applications like image recognition. Spiking
Neural Networks (SNNs) are particularly promising due to their event-driven and
temporally sparse nature, making them well-suited for low-power Field
Programmable Gate Array (FPGA)-based deployment. This paper explores using the
open-source Spiker+ framework to generate optimized SNNs accelerators for
handwritten digit recognition on the MNIST dataset. Spiker+ enables high-level
specification of network topologies, neuron models, and quantization,
automatically generating deployable HDL. We evaluate multiple configurations
and analyze trade-offs relevant to edge computing constraints.

</details>


### [286] [Tangma: A Tanh-Guided Activation Function with Learnable Parameters](https://arxiv.org/abs/2507.10560)
*Shreel Golwala*

Main category: cs.NE

TL;DR: Tangma, a new activation function with learnable parameters, outperforms ReLU, Swish, and GELU on MNIST and CIFAR-10, achieving higher accuracy and faster convergence.


<details>
  <summary>Details</summary>
Motivation: To improve backpropagation and expressiveness in deep neural networks by introducing a smooth, learnable activation function.

Method: Tangma combines hyperbolic tangent's smooth shape with two learnable parameters (α and γ) to adjust activation and preserve gradients. Evaluated on MNIST and CIFAR-10 using custom networks.

Result: Achieved 99.09% accuracy on MNIST and 78.15% on CIFAR-10, with faster convergence and lower loss than baselines.

Conclusion: Tangma is effective for vision tasks, offering reliable training and potential benefits for larger models.

Abstract: Activation functions are key to effective backpropagation and expressiveness
in deep neural networks. This work introduces Tangma, a new activation function
that combines the smooth shape of the hyperbolic tangent with two learnable
parameters: $\alpha$, which shifts the curve's inflection point to adjust
neuron activation, and $\gamma$, which adds linearity to preserve weak
gradients and improve training stability. Tangma was evaluated on MNIST and
CIFAR-10 using custom networks composed of convolutional and linear layers, and
compared against ReLU, Swish, and GELU. On MNIST, Tangma achieved the highest
validation accuracy of 99.09% and the lowest validation loss, demonstrating
faster and more stable convergence than the baselines. On CIFAR-10, Tangma
reached a top validation accuracy of 78.15%, outperforming all other activation
functions while maintaining a competitive training loss. Tangma also showed
improved training efficiency, with lower average epoch runtimes compared to
Swish and GELU. These results suggest that Tangma performs well on standard
vision tasks and enables reliable, efficient training. Its learnable design
gives more control over activation behavior, which may benefit larger models in
tasks such as image recognition or language modeling.

</details>


### [287] [Biological Processing Units: Leveraging an Insect Connectome to Pioneer Biofidelic Neural Architectures](https://arxiv.org/abs/2507.10951)
*Siyu Yu,Zihan Qin,Tingshan Liu,Beiya Xu,R. Jacob Vogelstein,Jason Brown,Joshua T. Vogelstein*

Main category: cs.NE

TL;DR: The Drosophila larva brain's connectome was used to create a Biological Processing Unit (BPU), which outperformed size-matched MLPs on MNIST and CIFAR-10. Scaling and ablations improved performance, and BPU models surpassed Transformers in chess and vision tasks.


<details>
  <summary>Details</summary>
Motivation: To explore if biologically evolved neural circuits can support artificial intelligence tasks.

Method: Converted the Drosophila larva brain's connectome into a fixed recurrent network (BPU) and tested it on MNIST, CIFAR-10, and ChessBench. Scaled and ablated the BPU for performance analysis.

Result: BPU achieved 98% on MNIST, 58% on CIFAR-10, and 60% move accuracy on ChessBench, outperforming MLPs and Transformers. CNN-BPU reached 91.7% accuracy with minimax search.

Conclusion: Biofidelic neural architectures show promise for complex tasks, motivating further scaling of connectomes.

Abstract: The complete connectome of the Drosophila larva brain offers a unique
opportunity to investigate whether biologically evolved circuits can support
artificial intelligence. We convert this wiring diagram into a Biological
Processing Unit (BPU), a fixed recurrent network derived directly from synaptic
connectivity. Despite its modest size 3,000 neurons and 65,000 weights between
them), the unmodified BPU achieves 98% accuracy on MNIST and 58% on CIFAR-10,
surpassing size-matched MLPs. Scaling the BPU via structured connectome
expansions further improves CIFAR-10 performance, while modality-specific
ablations reveal the uneven contributions of different sensory subsystems. On
the ChessBench dataset, a lightweight GNN-BPU model trained on only 10,000
games achieves 60% move accuracy, nearly 10x better than any size transformer.
Moreover, CNN-BPU models with ~2M parameters outperform parameter-matched
Transformers, and with a depth-6 minimax search at inference, reach 91.7%
accuracy, exceeding even a 9M-parameter Transformer baseline. These results
demonstrate the potential of biofidelic neural architectures to support complex
cognitive tasks and motivate scaling to larger and more intelligent connectomes
in future work.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [288] [Deep Equilibrium models for Poisson Imaging Inverse problems via Mirror Descent](https://arxiv.org/abs/2507.11461)
*Christian Daniele,Silvia Villa,Samuel Vaiter,Luca Calatroni*

Main category: math.OC

TL;DR: The paper extends Deep Equilibrium Models (DEQs) to Poisson inverse problems using a novel Mirror Descent-based DEQ formulation, outperforming traditional methods and matching Bregman Plug-and-Play performance without their drawbacks.


<details>
  <summary>Details</summary>
Motivation: To address Poisson inverse problems, which require a Kullback-Leibler divergence data fidelity term, and improve upon existing methods like Bregman Plug-and-Play.

Method: Introduces a DEQ formulation based on Mirror Descent with a non-Euclidean geometry tailored to the data term, ensuring convergence and efficient training.

Result: Outperforms traditional model-based approaches and matches Bregman Plug-and-Play performance, while being less sensitive to initialization and hyperparameters.

Conclusion: The proposed DEQ-MD framework is effective for Poisson inverse problems, offering robust performance and ease of use.

Abstract: Deep Equilibrium Models (DEQs) are implicit neural networks with fixed
points, which have recently gained attention for learning image regularization
functionals, particularly in settings involving Gaussian fidelities, where
assumptions on the forward operator ensure contractiveness of standard
(proximal) Gradient Descent operators. In this work, we extend the application
of DEQs to Poisson inverse problems, where the data fidelity term is more
appropriately modeled by the Kullback-Leibler divergence. To this end, we
introduce a novel DEQ formulation based on Mirror Descent defined in terms of a
tailored non-Euclidean geometry that naturally adapts with the structure of the
data term. This enables the learning of neural regularizers within a principled
training framework. We derive sufficient conditions to guarantee the
convergence of the learned reconstruction scheme and propose computational
strategies that enable both efficient training and fully parameter-free
inference. Numerical experiments show that our method outperforms traditional
model-based approaches and it is comparable to the performance of Bregman
Plug-and-Play methods, while mitigating their typical drawbacks - namely,
sensitivity to initialization and careful tuning of hyperparameters. The code
is publicly available at https://github.com/christiandaniele/DEQ-MD.

</details>


### [289] [A Mathematical Optimization Approach to Multisphere Support Vector Data Description](https://arxiv.org/abs/2507.11106)
*Víctor Blanco,Inmaculada Espejo,Raúl Páez,Antonio M. Rodríguez-Chía*

Main category: math.OC

TL;DR: A novel optimization framework for outlier detection in multimodal datasets, extending SVDD with primal and dual models for improved accuracy and robustness.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of detecting outliers in complex, multimodal datasets by extending Support Vector Data Description (SVDD) approaches.

Method: Develops a primal Mixed Integer Second Order Cone model for Euclidean hypersphere construction and a dual model enabling kernel trick application for non-linear data.

Result: The exact method outperforms existing heuristic techniques in accuracy and robustness, as shown in computational studies.

Conclusion: The proposed framework effectively detects outliers in multimodal and non-linear datasets, offering superior performance over heuristic methods.

Abstract: We present a novel mathematical optimization framework for outlier detection
in multimodal datasets, extending Support Vector Data Description approaches.
We provide a primal formulation, in the shape of a Mixed Integer Second Order
Cone model, that constructs Euclidean hyperspheres to identify anomalous
observations. Building on this, we develop a dual model that enables the
application of the kernel trick, thus allowing for the detection of outliers
within complex, non-linear data structures. An extensive computational study
demonstrates the effectiveness of our exact method, showing clear advantages
over existing heuristic techniques in terms of accuracy and robustness.

</details>


### [290] [Recursive Bound-Constrained AdaGrad with Applications to Multilevel and Domain Decomposition Minimization](https://arxiv.org/abs/2507.11513)
*Serge Gratton,Alena Kopaničáková,Philippe Toint*

Main category: math.OC

TL;DR: Two noise-tolerant OFFO algorithms for bound-constrained optimization, handling inexact gradients and using second-order info, generalize AdaGrad. Both require O(ε⁻²) iterations for ε-approximate critical points.


<details>
  <summary>Details</summary>
Motivation: Address bound-constrained optimization with noise-tolerant methods, leveraging hierarchical and domain-decomposition approaches.

Method: Multi-level hierarchical and domain-decomposition methods, extending AdaGrad, with a unified convergence theory.

Result: Proven O(ε⁻²) iterations for ε-approximate critical points; validated on PDEs and deep learning.

Conclusion: Algorithms are efficient and broadly applicable, with strong theoretical guarantees.

Abstract: Two OFFO (Objective-Function Free Optimization) noise tolerant algorithms are
presented that handle bound constraints, inexact gradients and use second-order
information when available.The first is a multi-level method exploiting a
hierarchical description of the problem and the second is a
domain-decomposition method covering the standard addditive Schwarz
decompositions. Both are generalizations of the first-order AdaGrad algorithm
for unconstrained optimization. Because these algorithms share a common
theoretical framework, a single convergence/complexity theory is provided which
covers them both. Its main result is that, with high probability, both methods
need at most $O(\epsilon^{-2})$ iterations and noisy gradient evaluations to
compute an $\epsilon$-approximate first-order critical point of the
bound-constrained problem. Extensive numerical experiments are discussed on
applications ranging from PDE-based problems to deep neural network training,
illustrating their remarkable computational efficiency.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [291] [Quantitative multi-metabolite imaging of Parkinson's disease using AI boosted molecular MRI](https://arxiv.org/abs/2507.11329)
*Hagar Shmuely,Michal Rivlin,Or Perlman*

Main category: physics.med-ph

TL;DR: A rapid molecular MRI method combined with deep learning enables multi-metabolite quantification in Parkinson's disease (PD) models, identifying potential biomarkers.


<details>
  <summary>Details</summary>
Motivation: Overcome limitations of traditional PD imaging (radioactive isotopes, long scans, low resolution) and improve specificity of MRI-based methods.

Method: Combined rapid MRI acquisition with deep learning reconstruction to quantify glutamate, proteins, and macromolecules in an acute MPTP mouse model.

Result: Quantitative maps aligned with histology and MR spectroscopy, identifying semisolid MT, amide, and aliphatic rNOE as potential PD biomarkers.

Conclusion: The approach offers a non-invasive, high-resolution method for PD biomarker discovery.

Abstract: Traditional approaches for molecular imaging of Parkinson's disease (PD) in
vivo require radioactive isotopes, lengthy scan times, or deliver only low
spatial resolution. Recent advances in saturation transfer-based PD magnetic
resonance imaging (MRI) have provided biochemical insights, although the image
contrast is semi-quantitative and nonspecific. Here, we combined a rapid
molecular MRI acquisition paradigm with deep learning based reconstruction for
multi-metabolite quantification of glutamate, mobile proteins, semisolid, and
mobile macromolecules in an acute MPTP
(1-methyl-4-phenyl-1,2,3,6-tetrahydropyridine) mouse model. The quantitative
parameter maps are in general agreement with the histology and MR spectroscopy,
and demonstrate that semisolid magnetization transfer (MT), amide, and
aliphatic relayed nuclear Overhauser effect (rNOE) proton volume fractions may
serve as PD biomarkers.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [292] [$\texttt{Droid}$: A Resource Suite for AI-Generated Code Detection](https://arxiv.org/abs/2507.10583)
*Daniil Orel,Indraneil Paul,Iryna Gurevych,Preslav Nakov*

Main category: cs.SE

TL;DR: The paper introduces DroidCollection, a large dataset for training AI-generated code detectors, and DroidDetect, a suite of detectors trained on it. It highlights limitations of existing detectors and proposes solutions like adversarial training and metric learning.


<details>
  <summary>Details</summary>
Motivation: To address the lack of diverse and extensive datasets for training and evaluating machine-generated code detectors, and to improve detector robustness against adversarial attacks.

Method: Compiled DroidCollection, a dataset with over a million code samples across seven languages and 43 coding models, and developed DroidDetect, a suite of detectors trained using a multi-task objective.

Result: Existing detectors fail to generalize across diverse domains and languages. Adversarial training and metric learning improve robustness.

Conclusion: DroidCollection and DroidDetect provide a robust framework for detecting AI-generated code, with adversarial training and metric learning enhancing performance.

Abstract: In this work, we compile $\textbf{$\texttt{DroidCollection}$}$, the most
extensive open data suite for training and evaluating machine-generated code
detectors, comprising over a million code samples, seven programming languages,
outputs from 43 coding models, and over three real-world coding domains.
Alongside fully AI-generated samples, our collection includes human-AI
co-authored code, as well as adversarial samples explicitly crafted to evade
detection. Subsequently, we develop $\textbf{$\texttt{DroidDetect}$}$, a suite
of encoder-only detectors trained using a multi-task objective over
$\texttt{DroidCollection}$. Our experiments show that existing detectors'
performance fails to generalise to diverse coding domains and programming
languages outside of their narrow training data. Additionally, we demonstrate
that while most detectors are easily compromised by humanising the output
distributions using superficial prompting and alignment approaches, this
problem can be easily amended by training on a small amount of adversarial
data. Finally, we demonstrate the effectiveness of metric learning and
uncertainty-based resampling as means to enhance detector training on possibly
noisy distributions.

</details>


### [293] [ARPaCCino: An Agentic-RAG for Policy as Code Compliance](https://arxiv.org/abs/2507.10584)
*Francesco Romeo,Luigi Arena,Francesco Blefari,Francesco Aurelio Pironti,Matteo Lupinacci,Angelo Furfaro*

Main category: cs.SE

TL;DR: ARPaCCino automates Policy as Code (PaC) rule generation and verification using LLMs, RAG, and tool-based validation, improving reliability and accessibility.


<details>
  <summary>Details</summary>
Motivation: Complexity of policy languages and misconfiguration risks hinder PaC adoption.

Method: Combines LLMs, RAG, and tool validation to generate and verify Rego rules from natural language descriptions.

Result: Effectively generates correct policies, identifies non-compliant infrastructures, and applies fixes, even with smaller LLMs.

Conclusion: Agentic RAG architectures enhance PaC automation, reliability, and accessibility.

Abstract: Policy as Code (PaC) is a paradigm that encodes security and compliance
policies into machine-readable formats, enabling automated enforcement in
Infrastructure as Code (IaC) environments. However, its adoption is hindered by
the complexity of policy languages and the risk of misconfigurations. In this
work, we present ARPaCCino, an agentic system that combines Large Language
Models (LLMs), Retrieval-Augmented-Generation (RAG), and tool-based validation
to automate the generation and verification of PaC rules. Given natural
language descriptions of the desired policies, ARPaCCino generates formal Rego
rules, assesses IaC compliance, and iteratively refines the IaC configurations
to ensure conformance. Thanks to its modular agentic architecture and
integration with external tools and knowledge bases, ARPaCCino supports policy
validation across a wide range of technologies, including niche or emerging IaC
frameworks. Experimental evaluation involving a Terraform-based case study
demonstrates ARPaCCino's effectiveness in generating syntactically and
semantically correct policies, identifying non-compliant infrastructures, and
applying corrective modifications, even when using smaller, open-weight LLMs.
Our results highlight the potential of agentic RAG architectures to enhance the
automation, reliability, and accessibility of PaC workflows.

</details>


### [294] [Repairing Language Model Pipelines by Meta Self-Refining Competing Constraints at Runtime](https://arxiv.org/abs/2507.10590)
*Mojtaba Eshghie*

Main category: cs.SE

TL;DR: Meta Self-Refining framework improves LM pipelines by detecting and repairing oscillatory failures caused by competing soft constraints.


<details>
  <summary>Details</summary>
Motivation: Address inefficiencies in LM pipelines when handling competing soft constraints, which lead to backtracking loops.

Method: Introduces a meta-corrective layer that monitors execution history, detects failures, and uses a meta-repairer LM to synthesize strategic instructions.

Result: Successfully repairs refining loops, making LM programs more efficient.

Conclusion: Meta Self-Refining effectively resolves constraint conflicts in LM pipelines, enhancing their performance.

Abstract: Language Model (LM) pipelines can dynamically refine their outputs against
programmatic constraints. However, their effectiveness collapses when faced
with competing soft constraints, leading to inefficient backtracking loops
where satisfying one constraint violates another. We introduce Meta
Self-Refining, a framework that equips LM pipelines with a meta-corrective
layer to repair these competitions at runtime/inference-time. Our approach
monitors the pipeline's execution history to detect oscillatory failures. Upon
detection, it invokes a meta-repairer LM that analyzes the holistic state of
the backtracking attempts and synthesizes a strategic instruction to balance
the competing requirements. This self-repair instruction guides the original LM
out of a failing refining loop towards a successful output. Our results show
Meta Self-Refining can successfully repair these loops, leading to more
efficient LM programs.

</details>


### [295] [ToolRegistry: A Protocol-Agnostic Tool Management Library for Function-Calling LLMs](https://arxiv.org/abs/2507.10593)
*Peng Ding*

Main category: cs.SE

TL;DR: Toolregistry is a protocol-agnostic library simplifying tool integration for LLMs, reducing code, improving performance, and ensuring compatibility.


<details>
  <summary>Details</summary>
Motivation: Current tool integration methods for LLMs are fragmented, limited, and complex, causing high development overhead.

Method: Toolregistry provides a unified interface for tool registration, representation, execution, and lifecycle management.

Result: Achieves 60-80% less integration code, 3.1x performance gains, and 100% OpenAI compatibility.

Conclusion: Toolregistry enhances development efficiency and maintainability, proven in real-world cases, and is open-source.

Abstract: Large Language Model (LLM) applications are increasingly relying on external
tools to extend their capabilities beyond text generation. However, current
tool integration approaches suffer from fragmentation, protocol limitations,
and implementation complexity, leading to substantial development overhead.
This paper presents Toolregistry, a protocol-agnostic tool management library
that simplifies tool registration, representation, execution, and lifecycle
management via a unified interface. Our evaluation demonstrates that
\toolregistry achieves 60-80% reduction in tool integration code, up to 3.1x
performance improvements through concurrent execution, and 100% compatibility
with OpenAI function calling standards. Real-world case studies show
significant improvements in development efficiency and code maintainability
across diverse integration scenarios. \toolregistry is open-source and
available at https://github.com/Oaklight/ToolRegistry, with comprehensive
documentation at https://toolregistry.readthedocs.io/.

</details>


### [296] [SWE-MERA: A Dynamic Benchmark for Agenticly Evaluating Large Language Models on Software Engineering Tasks](https://arxiv.org/abs/2507.11059)
*Pavel Adamenko,Mikhail Ivanov,Aidar Valeev,Rodion Levichev,Pavel Zadorozhny,Ivan Lopatin,Dmitry Babayev,Alena Fenogenova,Valentin Malykh*

Main category: cs.SE

TL;DR: SWE-MERA is introduced as a dynamic benchmark to address data contamination in SWE-bench, offering 10,000 tasks with rigorous validation. It evaluates LLMs effectively.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks like SWE-bench have data contamination issues, limiting their reliability for evaluating LLMs in software engineering.

Method: SWE-MERA automates real-world GitHub issue collection and validates quality, minimizing contamination. It includes 10,000 tasks, with 300 samples ready.

Result: The benchmark shows strong discriminative power in evaluating LLMs, with performance reported for models tested from September 2024 to June 2025.

Conclusion: SWE-MERA addresses critical flaws in existing benchmarks, providing a reliable tool for assessing LLMs in software engineering.

Abstract: The rapid advancement of Large Language Models (LLMs) in software engineering
has revealed critical limitations in existing benchmarks, particularly the
widely used SWE-bench dataset. Recent studies have uncovered severe data
contamination issues, e.g. SWE-bench reports 32.67% of successful patches
involve direct solution leakage and 31.08\% pass due to inadequate test cases.
We introduce SWE-MERA, a dynamic, continuously updated benchmark designed to
address these fundamental challenges through an automated collection of
real-world GitHub issues and rigorous quality validation. Our approach
implements a reliable pipeline that ensures quality while minimizing
contamination risks, resulting in approximately 10,000 potential tasks with 300
samples currently available. Evaluation using the Aider coding agent
demonstrates strong discriminative power in state-of-the-art models. We report
performance across a dozen recent LLMs evaluated on tasks collected between
September 2024 and June 2025.

</details>


### [297] [A Code Comprehension Benchmark for Large Language Models for Code](https://arxiv.org/abs/2507.10641)
*Jayant Havare,Saurav Chaudhary,Ganesh Ramakrishnan,Kaushik Maharajan,Srikanth Tamilselvam*

Main category: cs.SE

TL;DR: Fine-tuning large language models for code comprehension improves their semantic understanding, as shown by performance boosts in tasks like Subjectivity Grading.


<details>
  <summary>Details</summary>
Motivation: Large language models excel in syntactic code tasks but lack deeper semantic understanding, limiting performance in debugging and optimization.

Method: Propose fine-tuning models on large-scale datasets for code comprehension, evaluating three models on semantic tasks.

Result: Fine-tuning improves performance, with QWQ-32B accuracy rising from 70% to 83.47%. Codestral-22B achieves the highest accuracy of 87.66%.

Conclusion: Fine-tuning enhances code comprehension, bridging the gap between syntactic and semantic understanding in language models.

Abstract: Large Language Models have shown impressive capabilities in coding tasks like
code generation and code completion, as they have been trained on a large
amount of code data. Also, since one of the core pretraining objectives is Next
Token Prediction, these models tends to learn surface-level syntactic patterns
in code. However, this does not guarantee code comprehension ability i.e. the
ability to capture the semantics of the code. In our opinion, this is the
reason why these models often underperform on tasks that require deeper
semantic understanding, such as code debugging and code optimization. To
address this, we propose fine-tuning these models specifically for code
comprehension tasks using large-scale datasets, enabling them to develop a more
robust understanding of code semantics. We evaluate three code models of
varying sizes on a suite of code comprehension tasks designed to assess
semantic understanding beyond surface-level syntactic pattern matching. In
particular, we analyze performance on the Subjectivity Grading Task and observe
that model performance improves after fine-tuning on relevant downstream tasks.
The most significant improvement is seen in the QWQ-32B model, where accuracy
increases from 70% to 83.47%. A similar or explainable trend is observed across
other models, clearly indicating an enhancement in code comprehension ability.
Among the models studied, the DPO-fine-tuned Codestral-22B achieves the highest
micro-accuracy of 87.66% on the Subjectivity Grading Task.

</details>


### [298] [CodeAssistBench (CAB): Dataset & Benchmarking for Multi-turn Chat-Based Code Assistance](https://arxiv.org/abs/2507.10646)
*Myeongsoo Kim,Shweta Garg,Baishakhi Ray,Varun Kumar,Anoop Deoras*

Main category: cs.SE

TL;DR: CodeAssistBench (CAB) introduces a benchmark for multi-turn programming assistance in realistic settings, revealing a gap in LLM performance between isolated code tasks and real-world project contexts.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks for programming assistants focus on narrow code generation tasks, lacking realism and scalability for real-world project environments.

Method: CAB automatically generates datasets from GitHub issues, containerizes codebases, and evaluates models via simulated users in realistic settings.

Result: LLMs perform well on Stack Overflow (70-83% success) but poorly on CAB's real-world issues (up to 16.49%), showing a capability gap.

Conclusion: CAB highlights the need for benchmarks that reflect complex, project-specific contexts to better evaluate programming assistants.

Abstract: Programming assistants powered by large language models have transformed
software development, yet most benchmarks focus narrowly on code generation
tasks. Recent efforts like InfiBench and StackEval attempt to address this gap
using Stack Overflow data but remain limited to single-turn interactions in
isolated contexts, require significant manual curation, and fail to represent
complete project environments. We introduce CodeAssistBench (CAB), the first
benchmark framework for evaluating multi-turn programming assistance in
realistic settings that address real-world questions about actual codebases.
Unlike existing programming Q&A benchmarks, CAB automatically generates
scalable datasets from question-related GitHub issues using configurable
parameters (e.g., repository creation date, star count, programming languages),
and includes automatic containerization of codebases for evaluation. It then
evaluates models through simulated users in these containerized environments
with full codebase access. Using this framework, we constructed a test set of
3,286 real-world programming questions across 231 repositories, spanning seven
programming languages and diverse problem domains. Our evaluation of leading
LLMs reveals a substantial capability gap: while models perform well on Stack
Overflow questions with success rates of 70-83%, they resolve only up to 16.49%
of CAB's recent issues. This discrepancy highlights the challenges of providing
assistance in complex, project-specific contexts versus answering standalone
questions.

</details>


### [299] [SENSOR: An ML-Enhanced Online Annotation Tool to Uncover Privacy Concerns from User Reviews in Social-Media Applications](https://arxiv.org/abs/2507.10640)
*Labiba Farah,Mohammad Ridwan Kabir,Shohel Ahmed,MD Mohaymen Ul Anam,Md. Sakibul Islam*

Main category: cs.SE

TL;DR: The paper introduces SENSOR, an automated tool for classifying user reviews into privacy-related feature requests, bug reports, or irrelevant categories, using the GRACE model (GRU-based with CBOW and Attention). It achieved high accuracy (95.10%) on 16,000 reviews from social media apps.


<details>
  <summary>Details</summary>
Motivation: Manual classification of privacy-related user reviews is challenging due to volume and nuance. Existing tools lack focus on specific privacy-related categories.

Method: Developed GRACE (GRU with CBOW and Attention) for automated classification. Analyzed 16,000 reviews from 7 social media apps, manually labeled for training.

Result: GRACE achieved high performance (macro F1-score: 0.9434, ROC-AUC: 0.9934, accuracy: 95.10%) despite class imbalance.

Conclusion: SENSOR effectively aids developers in identifying privacy-related issues from user reviews, improving privacy and trust.

Abstract: The widespread use of social media applications has raised significant
privacy concerns, often highlighted in user reviews. These reviews also provide
developers with valuable insights into improving apps by addressing issues and
introducing better features. However, the sheer volume and nuanced nature of
reviews make manual identification and prioritization of privacy-related
concerns challenging for developers. Previous studies have developed software
utilities to automatically classify user reviews as privacy-relevant,
privacy-irrelevant, bug reports, feature requests, etc., using machine
learning. Notably, there is a lack of focus on classifying reviews specifically
as privacy-related feature requests, privacy-related bug reports, or
privacy-irrelevant. This paper introduces SENtinel SORt (SENSOR), an automated
online annotation tool designed to help developers annotate and classify user
reviews into these categories. For automating the annotation of such reviews,
this paper introduces the annotation model, GRACE (GRU-based Attention with
CBOW Embedding), using Gated Recurrent Units (GRU) with Continuous Bag of Words
(CBOW) and Attention mechanism. Approximately 16000 user reviews from seven
popular social media apps on Google Play Store, including Instagram, Facebook,
WhatsApp, Snapchat, X (formerly Twitter), Facebook Lite, and Line were
analyzed. Two annotators manually labelled the reviews, achieving a Cohen's
Kappa value of 0.87, ensuring a labeled dataset with high inter-rater agreement
for training machine learning models. Among the models tested, GRACE
demonstrated the best performance (macro F1-score: 0.9434, macro ROC-AUC:
0.9934, and accuracy: 95.10%) despite class imbalance. SENSOR demonstrates
significant potential to assist developers with extracting and addressing
privacy-related feature requests or bug reports from user reviews, enhancing
user privacy and trust.

</details>


### [300] [Past, Present and Future: Exploring Adaptive AI in Software Development Bots](https://arxiv.org/abs/2507.10822)
*Omar Elsisi,Glaucia Melo*

Main category: cs.SE

TL;DR: The paper explores adaptive AI-powered conversational agents in software development, their evolution, benefits, challenges, and future potential.


<details>
  <summary>Details</summary>
Motivation: To highlight the transformative role of adaptive AI chatbots in enhancing productivity and collaboration in software development.

Method: Examines the evolution of conversational agents, their AI-driven capabilities, and integration challenges in software development.

Result: Identifies benefits like personalized assistance and efficiency, alongside challenges such as data privacy and ethical concerns.

Conclusion: Adaptive AI chatbots hold significant potential to revolutionize software development with real-time, customized support.

Abstract: Conversational agents, such as chatbots and virtual assistants, have become
essential in software development, boosting productivity, collaboration, and
automating various tasks. This paper examines the role of adaptive AI-powered
conversational agents in software development, highlighting their ability to
offer dynamic, context-aware assistance to developers. Unlike traditional
rule-based systems, adaptive AI agents use machine learning and natural
language processing to learn from interactions and improve over time, providing
more personalized and responsive help. We look at how these tools have evolved
from simple query-based systems to advanced AI-driven solutions like GitHub
Copilot and Microsoft Teams bots. We also explore the challenges of integrating
adaptive AI into software development processes. The study aims to assess the
benefits and limitations of these systems, address concerns like data privacy
and ethical issues, and offer insights into their future use in the field.
Ultimately, adaptive AI chatbots have great potential to revolutionize software
development by delivering real-time, customized support and enhancing the
efficiency of development cycles.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [301] [Stochastic Entanglement Configuration for Constructive Entanglement Topologies in Quantum Machine Learning with Application to Cardiac MRI](https://arxiv.org/abs/2507.11401)
*Mehri Mehrnia,Mohammed S. M. Elbaz*

Main category: quant-ph

TL;DR: A stochastic entanglement configuration method is introduced to enhance variational quantum circuits (VQCs) for quantum machine learning (QML), outperforming classical models by identifying constructive entanglement topologies.


<details>
  <summary>Details</summary>
Motivation: Current fixed entanglement topologies in VQCs limit performance gains over classical models, necessitating adaptive strategies.

Method: A stochastic method generates diverse entanglement topologies encoded as binary matrices, evaluated using entanglement density and per-qubit constraints.

Result: 64 (16%) constructive configurations outperformed classical baselines, with ensemble aggregation achieving ~0.92 accuracy (5% higher than classical).

Conclusion: The method demonstrates robustness and generalizability, significantly improving QML performance over conventional topologies.

Abstract: Efficient entanglement strategies are essential for advancing variational
quantum circuits (VQCs) for quantum machine learning (QML). However, most
current approaches use fixed entanglement topologies that are not adaptive to
task requirements, limiting potential gains over classical models. We introduce
a novel stochastic entanglement configuration method that systematically
generates diverse entanglement topologies to identify a subspace of
constructive entanglement configurations, defined as entanglement topologies
that boost hybrid model performance (e.g., classification accuracy) beyond
classical baselines. Each configuration is encoded as a stochastic binary
matrix, denoting directed entanglement between qubits. This enables scalable
exploration of the hyperspace of candidate entanglement topologies using
entanglement density and per-qubit constraints as key metrics. We define
unconstrained and constrained sampling modes, controlling entanglement per
qubit. Using our method, 400 stochastic configurations were generated and
evaluated in a hybrid QML for cardiac MRI disease classification. We identified
64 (16%) novel constructive entanglement configurations that consistently
outperformed the classical baseline. Ensemble aggregation of top-performing
configurations achieved ~0.92 classification accuracy, exceeding the classical
model (~0.87) by over 5%. Compared to four conventional topologies (ring,
nearest neighbor, no entanglement, fully entangled), none surpassed the
classical baseline (maximum accuracy ~0.82), while our configurations delivered
up to ~20% higher accuracy. Thus, highlighting the robustness and
generalizability of the identified constructive entanglements.

</details>


### [302] [Formal Verification of Variational Quantum Circuits](https://arxiv.org/abs/2507.10635)
*Nicola Assolini,Luca Marzari,Isabella Mastroeni,Alessandra di Pierro*

Main category: quant-ph

TL;DR: The paper presents the first theoretical and practical study of formal verification for Variational Quantum Circuits (VQCs), addressing their vulnerability to adversarial inputs and proposing a novel semantic framework based on abstract interpretation.


<details>
  <summary>Details</summary>
Motivation: VQCs, like classical deep neural networks, are vulnerable to adversarial inputs, but lack formal verification frameworks. This gap motivates the study.

Method: The study uses interval-based reachability techniques and introduces a novel semantic framework based on abstract interpretation to verify VQCs.

Result: Quantum-specific challenges like state normalization complicate verification, but the proposed framework addresses these issues and is demonstrated on benchmarks.

Conclusion: The paper establishes a foundation for formally verifying VQCs, highlighting the need for quantum-specific approaches due to unique challenges.

Abstract: Variational quantum circuits (VQCs) are a central component of many quantum
machine learning algorithms, offering a hybrid quantum-classical framework
that, under certain aspects, can be considered similar to classical deep neural
networks. A shared aspect is, for instance, their vulnerability to adversarial
inputs, small perturbations that can lead to incorrect predictions. While
formal verification techniques have been extensively developed for classical
models, no comparable framework exists for certifying the robustness of VQCs.
Here, we present the first in-depth theoretical and practical study of the
formal verification problem for VQCs. Inspired by abstract interpretation
methods used in deep learning, we analyze the applicability and limitations of
interval-based reachability techniques in the quantum setting. We show that
quantum-specific aspects, such as state normalization, introduce inter-variable
dependencies that challenge existing approaches. We investigate these issues by
introducing a novel semantic framework based on abstract interpretation, where
the verification problem for VQCs can be formally defined, and its complexity
analyzed. Finally, we demonstrate our approach on standard verification
benchmarks.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [303] [When and Where do Data Poisons Attack Textual Inversion?](https://arxiv.org/abs/2507.10578)
*Jeremy Styborski,Mingzhi Lyu,Jiayou Lu,Nupur Kapur,Adams Kong*

Main category: cs.CR

TL;DR: The paper analyzes poisoning attacks in diffusion models (DMs), introduces Semantic Sensitivity Maps, identifies non-uniform learning behavior, and proposes Safe-Zone Training (SZT) as a defense.


<details>
  <summary>Details</summary>
Motivation: To understand and mitigate poisoning attacks in textual inversion (TI) for DMs, which threaten model robustness.

Method: Introduces Semantic Sensitivity Maps, identifies learning biases in DMs, and proposes SZT with JPEG compression, high-timestep restriction, and loss masking.

Result: SZT significantly improves TI robustness against poisoning attacks, outperforming prior defenses.

Conclusion: The study provides insights into DM vulnerabilities and offers an effective defense mechanism (SZT) for secure TI.

Abstract: Poisoning attacks pose significant challenges to the robustness of diffusion
models (DMs). In this paper, we systematically analyze when and where poisoning
attacks textual inversion (TI), a widely used personalization technique for
DMs. We first introduce Semantic Sensitivity Maps, a novel method for
visualizing the influence of poisoning on text embeddings. Second, we identify
and experimentally verify that DMs exhibit non-uniform learning behavior across
timesteps, focusing on lower-noise samples. Poisoning attacks inherit this bias
and inject adversarial signals predominantly at lower timesteps. Lastly, we
observe that adversarial signals distract learning away from relevant concept
regions within training data, corrupting the TI process. Based on these
insights, we propose Safe-Zone Training (SZT), a novel defense mechanism
comprised of 3 key components: (1) JPEG compression to weaken high-frequency
poison signals, (2) restriction to high timesteps during TI training to avoid
adversarial signals at lower timesteps, and (3) loss masking to constrain
learning to relevant regions. Extensive experiments across multiple poisoning
methods demonstrate that SZT greatly enhances the robustness of TI against all
poisoning attacks, improving generative quality beyond prior published
defenses. Code: www.github.com/JStyborski/Diff_Lab Data:
www.github.com/JStyborski/NC10

</details>


### [304] [LaSM: Layer-wise Scaling Mechanism for Defending Pop-up Attack on GUI Agents](https://arxiv.org/abs/2507.10610)
*Zihe Yan,Zhuosheng Zhang*

Main category: cs.CR

TL;DR: LaSM, a Layer-wise Scaling Mechanism, enhances GUI agent robustness against pop-up attacks by selectively amplifying attention in critical layers, achieving 98% defense success without retraining.


<details>
  <summary>Details</summary>
Motivation: GUI agents are vulnerable to pop-up-based attacks, and existing defenses are costly or ineffective under inductive interference.

Method: Study attention divergence in attacks, then propose LaSM to selectively amplify attention and MLP modules in critical layers.

Result: LaSM improves defense success rate across 12 pop-up perturbations and 4 model backbones, achieving 98% robustness with prompt-level alerts.

Conclusion: Attention misalignment is a key vulnerability in MLLM agents, and LaSM effectively mitigates it through layer-wise modulation.

Abstract: Graphical user interface (GUI) agents built on multimodal large language
models (MLLMs) have recently demonstrated strong decision-making abilities in
screen-based interaction tasks. However, they remain highly vulnerable to
pop-up-based environmental injection attacks, where malicious visual elements
divert model attention and lead to unsafe or incorrect actions. Existing
defense methods either require costly retraining or perform poorly under
inductive interference. In this work, we systematically study how such attacks
alter the attention behavior of GUI agents and uncover a layer-wise attention
divergence pattern between correct and incorrect outputs. Based on this
insight, we propose \textbf{LaSM}, a \textit{Layer-wise Scaling Mechanism} that
selectively amplifies attention and MLP modules in critical layers. LaSM
improves the alignment between model saliency and task-relevant regions without
additional training. Extensive experiments across 12 types of pop-up
perturbations and 4 different model backbones show that LaSM consistently
enhances the defense success rate. When combined with prompt-level alerts, LaSM
achieves over 98\% robustness even under strong inductive attacks. Our findings
reveal that attention misalignment is a core vulnerability in MLLM agents and
can be effectively addressed through selective layer-wise modulation.

</details>


### [305] [Game Theory Meets LLM and Agentic AI: Reimagining Cybersecurity for the Age of Intelligent Threats](https://arxiv.org/abs/2507.10621)
*Quanyan Zhu*

Main category: cs.CR

TL;DR: The paper explores integrating game theory and LLM-powered AI to enhance cybersecurity, bridging theory and practice for proactive, adaptive defense systems.


<details>
  <summary>Details</summary>
Motivation: Traditional cybersecurity methods are reactive and brittle; integrating game theory and AI offers a path to proactive, intelligent defenses.

Method: Combines game-theoretic frameworks (static, dynamic, Bayesian, signaling games) with LLM-powered agents to model adversarial behavior and operationalize strategies.

Result: LLM agents enhance cyber defense by embedding reasoning into AI systems, while game theory informs agent coordination and trust-aware design.

Conclusion: The convergence of game theory and agentic AI promises richer theoretical foundations and practical solutions for secure, adaptive cyber systems.

Abstract: Protecting cyberspace requires not only advanced tools but also a shift in
how we reason about threats, trust, and autonomy. Traditional cybersecurity
methods rely on manual responses and brittle heuristics. To build proactive and
intelligent defense systems, we need integrated theoretical frameworks and
software tools. Game theory provides a rigorous foundation for modeling
adversarial behavior, designing strategic defenses, and enabling trust in
autonomous systems. Meanwhile, software tools process cyber data, visualize
attack surfaces, verify compliance, and suggest mitigations. Yet a disconnect
remains between theory and practical implementation.
  The rise of Large Language Models (LLMs) and agentic AI offers a new path to
bridge this gap. LLM-powered agents can operationalize abstract strategies into
real-world decisions. Conversely, game theory can inform the reasoning and
coordination of these agents across complex workflows. LLMs also challenge
classical game-theoretic assumptions, such as perfect rationality or static
payoffs, prompting new models aligned with cognitive and computational
realities. This co-evolution promises richer theoretical foundations and novel
solution concepts. Agentic AI also reshapes software design: systems must now
be modular, adaptive, and trust-aware from the outset.
  This chapter explores the intersection of game theory, agentic AI, and
cybersecurity. We review key game-theoretic frameworks (e.g., static, dynamic,
Bayesian, and signaling games) and solution concepts. We then examine how LLM
agents can enhance cyber defense and introduce LLM-driven games that embed
reasoning into AI agents. Finally, we explore multi-agent workflows and
coordination games, outlining how this convergence fosters secure, intelligent,
and adaptive cyber systems.

</details>


### [306] [Spectral Feature Extraction for Robust Network Intrusion Detection Using MFCCs](https://arxiv.org/abs/2507.10622)
*HyeYoung Lee,Muhammad Nadeem,Pavel Tsoi*

Main category: cs.CR

TL;DR: Proposes a novel IoT anomaly detection method using adaptive MFCCs and ResNet-18, tested on three datasets with promising results.


<details>
  <summary>Details</summary>
Motivation: Addressing IoT security vulnerabilities by improving anomaly detection in network traffic.

Method: Uses learnable MFCCs for adaptive spectral feature representation and ResNet-18 for feature extraction and classification.

Result: Demonstrates enhanced class separability and effective multiclass classification on CICIoT2023, NSL-KDD, and IoTID20 datasets.

Conclusion: Combining adaptive signal processing with deep learning offers robust, scalable IoT anomaly detection.

Abstract: The rapid expansion of Internet of Things (IoT) networks has led to a surge
in security vulnerabilities, emphasizing the critical need for robust anomaly
detection and classification techniques. In this work, we propose a novel
approach for identifying anomalies in IoT network traffic by leveraging the
Mel-frequency cepstral coefficients (MFCC) and ResNet-18, a deep learning model
known for its effectiveness in feature extraction and image-based tasks.
Learnable MFCCs enable adaptive spectral feature representation, capturing the
temporal patterns inherent in network traffic more effectively than traditional
fixed MFCCs. We demonstrate that transforming raw signals into MFCCs maps the
data into a higher-dimensional space, enhancing class separability and enabling
more effective multiclass classification. Our approach combines the strengths
of MFCCs with the robust feature extraction capabilities of ResNet-18, offering
a powerful framework for anomaly detection. The proposed model is evaluated on
three widely used IoT intrusion detection datasets: CICIoT2023, NSL-KDD, and
IoTID20. The experimental results highlight the potential of integrating
adaptive signal processing techniques with deep learning architectures to
achieve robust and scalable anomaly detection in heterogeneous IoT network
landscapes.

</details>


### [307] [PhreshPhish: A Real-World, High-Quality, Large-Scale Phishing Website Dataset and Benchmark](https://arxiv.org/abs/2507.10854)
*Thomas Dalton,Hemanth Gowda,Girish Rao,Sachin Pargi,Alireza Hadj Khodabakhshi,Joseph Rombs,Stephan Jou,Manish Marwah*

Main category: cs.CR

TL;DR: The paper introduces PhreshPhish, a high-quality, large-scale phishing dataset, and benchmarks to address limitations in existing datasets, enabling better phishing detection research.


<details>
  <summary>Details</summary>
Motivation: Phishing is a growing threat, but progress in detection is hindered by poor-quality datasets and unrealistic benchmarks.

Method: The authors create PhreshPhish, a large, high-quality dataset, and propose benchmarks minimizing leakage and adjusting base rates for realistic evaluation.

Result: PhreshPhish is larger and higher quality than existing datasets, with benchmarks designed for realistic model testing.

Conclusion: The dataset and benchmarks aim to standardize phishing detection research and foster advancements in the field.

Abstract: Phishing remains a pervasive and growing threat, inflicting heavy economic
and reputational damage. While machine learning has been effective in real-time
detection of phishing attacks, progress is hindered by lack of large,
high-quality datasets and benchmarks. In addition to poor-quality due to
challenges in data collection, existing datasets suffer from leakage and
unrealistic base rates, leading to overly optimistic performance results. In
this paper, we introduce PhreshPhish, a large-scale, high-quality dataset of
phishing websites that addresses these limitations. Compared to existing public
datasets, PhreshPhish is substantially larger and provides significantly higher
quality, as measured by the estimated rate of invalid or mislabeled data
points. Additionally, we propose a comprehensive suite of benchmark datasets
specifically designed for realistic model evaluation by minimizing leakage,
increasing task difficulty, enhancing dataset diversity, and adjustment of base
rates more likely to be seen in the real world. We train and evaluate multiple
solution approaches to provide baseline performance on the benchmark sets. We
believe the availability of this dataset and benchmarks will enable realistic,
standardized model comparison and foster further advances in phishing
detection. The datasets and benchmarks are available on Hugging Face
(https://huggingface.co/datasets/phreshphish/phreshphish).

</details>


### [308] [MalCodeAI: Autonomous Vulnerability Detection and Remediation via Language Agnostic Code Reasoning](https://arxiv.org/abs/2507.10898)
*Jugal Gajjar,Kamalasankari Subramaniakuppusamy,Noha El Kachach*

Main category: cs.CR

TL;DR: MalCodeAI is an AI-driven pipeline for autonomous code security analysis and remediation, achieving low validation loss and high developer ratings.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of traditional vulnerability detection tools and the growing complexity of cyber threats.

Method: Uses a multi-stage AI pipeline with fine-tuned Qwen2.5-Coder-3B-Instruct models, optimized via LoRA in MLX, for code decomposition, semantic reasoning, and vulnerability detection.

Result: Achieved validation losses of 0.397 (Phase 1) and 0.199 (Phase 2), with high developer scores for usefulness (8.06/10), interpretability (7.40/10), and readability (7.53/10).

Conclusion: MalCodeAI advances intelligent, explainable, and developer-centric software security solutions.

Abstract: The growing complexity of cyber threats and the limitations of traditional
vulnerability detection tools necessitate novel approaches for securing
software systems. We introduce MalCodeAI, a language-agnostic, multi-stage AI
pipeline for autonomous code security analysis and remediation. MalCodeAI
combines code decomposition and semantic reasoning using fine-tuned
Qwen2.5-Coder-3B-Instruct models, optimized through Low-Rank Adaptation (LoRA)
within the MLX framework, and delivers scalable, accurate results across 14
programming languages. In Phase 1, the model achieved a validation loss as low
as 0.397 for functional decomposition and summarization of code segments after
200 iterations, 6 trainable layers, and a learning rate of 2 x 10^(-5). In
Phase 2, for vulnerability detection and remediation, it achieved a best
validation loss of 0.199 using the same number of iterations and trainable
layers but with an increased learning rate of 4 x 10^(-5), effectively
identifying security flaws and suggesting actionable fixes. MalCodeAI supports
red-hat-style exploit tracing, CVSS-based risk scoring, and zero-shot
generalization to detect complex, zero-day vulnerabilities. In a qualitative
evaluation involving 15 developers, the system received high scores in
usefulness (mean 8.06/10), interpretability (mean 7.40/10), and readability of
outputs (mean 7.53/10), confirming its practical value in real-world
development workflows. This work marks a significant advancement toward
intelligent, explainable, and developer-centric software security solutions.

</details>


### [309] [Hashed Watermark as a Filter: Defeating Forging and Overwriting Attacks in Weight-based Neural Network Watermarking](https://arxiv.org/abs/2507.11137)
*Yuan Yao,Jin Song,Jian Jin*

Main category: cs.CR

TL;DR: NeuralMark is a robust neural network watermarking method using a hashed watermark filter to protect against forging and overwriting attacks, with broad applicability across architectures.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks are valuable digital assets needing ownership protection, but existing weight-based watermarking methods are vulnerable to attacks.

Method: Uses a hash function to generate an irreversible binary watermark from a secret key, embedding it into selected model parameters. Incorporates average pooling to resist fine-tuning and pruning.

Result: Effective and robust across 13 architectures, tested on five image classification tasks and one text generation task.

Conclusion: NeuralMark provides a secure, practical solution for neural network watermarking, with theoretical and empirical validation.

Abstract: As valuable digital assets, deep neural networks necessitate robust ownership
protection, positioning neural network watermarking (NNW) as a promising
solution. Among various NNW approaches, weight-based methods are favored for
their simplicity and practicality; however, they remain vulnerable to forging
and overwriting attacks. To address those challenges, we propose NeuralMark, a
robust method built around a hashed watermark filter. Specifically, we utilize
a hash function to generate an irreversible binary watermark from a secret key,
which is then used as a filter to select the model parameters for embedding.
This design cleverly intertwines the embedding parameters with the hashed
watermark, providing a robust defense against both forging and overwriting
attacks. An average pooling is also incorporated to resist fine-tuning and
pruning attacks. Furthermore, it can be seamlessly integrated into various
neural network architectures, ensuring broad applicability. Theoretically, we
analyze its security boundary. Empirically, we verify its effectiveness and
robustness across 13 distinct Convolutional and Transformer architectures,
covering five image classification tasks and one text generation task. The
source codes are available at https://github.com/AIResearch-Group/NeuralMark.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [310] [Neural Expectation Operators](https://arxiv.org/abs/2507.10607)
*Qian Qi*

Main category: math.PR

TL;DR: The paper introduces Measure Learning, a paradigm for modeling ambiguity using Neural Expectation Operators derived from BSDEs with neural network drivers, extending to large particle systems and providing foundational math for data-driven ambiguity modeling.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between abstract quadratic BSDE theory and practical machine learning, enabling data-driven modeling under ambiguity with verifiable neural network designs.

Method: Defines Neural Expectation Operators via BSDEs with neural network drivers, proves well-posedness under local Lipschitz and quadratic growth conditions, and extends to Forward-Backward SDEs and particle systems.

Result: Establishes well-posedness for BSDEs with neural drivers, verifies axiomatic properties like convexity, and proves asymptotic results (LLN and CLT) for particle systems.

Conclusion: Provides a rigorous mathematical framework for ambiguity-aware machine learning, demonstrating practical applicability of neural networks in BSDE theory.

Abstract: This paper introduces \textbf{Measure Learning}, a paradigm for modeling
ambiguity via non-linear expectations. We define Neural Expectation Operators
as solutions to Backward Stochastic Differential Equations (BSDEs) whose
drivers are parameterized by neural networks. The main mathematical
contribution is a rigorous well-posedness theorem for BSDEs whose drivers
satisfy a local Lipschitz condition in the state variable $y$ and quadratic
growth in its martingale component $z$. This result circumvents the classical
global Lipschitz assumption, is applicable to common neural network
architectures (e.g., with ReLU activations), and holds for exponentially
integrable terminal data, which is the sharp condition for this setting. Our
primary innovation is to build a constructive bridge between the abstract, and
often restrictive, assumptions of the deep theory of quadratic BSDEs and the
world of machine learning, demonstrating that these conditions can be met by
concrete, verifiable neural network designs. We provide constructive methods
for enforcing key axiomatic properties, such as convexity, by architectural
design. The theory is extended to the analysis of fully coupled
Forward-Backward SDE systems and to the asymptotic analysis of large
interacting particle systems, for which we establish both a Law of Large
Numbers (propagation of chaos) and a Central Limit Theorem. This work provides
the foundational mathematical framework for data-driven modeling under
ambiguity.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [311] [HEIMDALL: a grapH-based sEIsMic Detector And Locator for microseismicity](https://arxiv.org/abs/2507.10850)
*Matteo Bagagli,Francesco Grigoli,Davide Bacciu*

Main category: physics.geo-ph

TL;DR: A new deep-learning model for microseismicity monitoring uses graph theory and neural networks to automate seismic catalog creation, improving event detection and reducing manual oversight.


<details>
  <summary>Details</summary>
Motivation: The need for efficient seismic monitoring in geothermal regions, driven by the global push for green-energy transition and carbon emission reduction.

Method: Utilizes graph neural networks to perform phase picking, association, and event location simultaneously over rolling windows, tested in Iceland's Hengill region.

Result: Significant increase in event detection, fewer false events, and reduced need for manual tuning compared to existing systems.

Conclusion: The model is a robust tool for geothermal seismic monitoring, enhancing risk mitigation during energy exploitation.

Abstract: In this work, we present a new deep-learning model for microseismicity
monitoring that utilizes continuous spatiotemporal relationships between
seismic station recordings, forming an end-to-end pipeline for seismic catalog
creation. It employs graph theory and state-of-the-art graph neural network
architectures to perform phase picking, association, and event location
simultaneously over rolling windows, making it suitable for both playback and
near-real-time monitoring. As part of the global strategy to reduce carbon
emissions within the broader context of a green-energy transition, there has
been growing interest in exploiting enhanced geothermal systems. Tested in the
complex geothermal area of Iceland's Hengill region using open-access data from
a temporary experiment, our model was trained and validated using both manually
revised and automatic seismic catalogs. Results showed a significant increase
in event detection compared to previously published automatic systems and
reference catalogs, including a $4 M_w$ seismic sequence in December 2018 and a
single-day sequence in February 2019. Our method reduces false events,
minimizes manual oversight, and decreases the need for extensive tuning of
pipelines or transfer learning of deep-learning models. Overall, it validates a
robust monitoring tool for geothermal seismic regions, complementing existing
systems and enhancing operational risk mitigation during geothermal energy
exploitation.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [312] [Learning to Move in Rhythm: Task-Conditioned Motion Policies with Orbital Stability Guarantees](https://arxiv.org/abs/2507.10602)
*Maximilian Stölzle,T. Konstantin Rusch,Zach J. Patterson,Rodrigo Pérez-Dattari,Francesco Stella,Josie Hughes,Cosimo Della Santina,Daniela Rus*

Main category: cs.RO

TL;DR: The paper introduces Orbitally Stable Motion Primitives (OSMPs) to improve learning complex periodic behaviors in robots, offering stability and generalization beyond existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing Dynamic Motion Primitives struggle with complex periodic behaviors and task interpolation, limiting their practical use.

Method: OSMPs combine a learned diffeomorphic encoder with a supercritical Hopf bifurcation in latent space for stable periodic motion learning and task-conditioned generalization.

Result: OSMPs outperform baselines in simulation and real-world experiments across diverse robotic platforms, demonstrating versatility and zero-shot generalization.

Conclusion: OSMPs provide a robust framework for learning periodic motions with formal stability guarantees and broad applicability.

Abstract: Learning from demonstration provides a sample-efficient approach to acquiring
complex behaviors, enabling robots to move robustly, compliantly, and with
fluidity. In this context, Dynamic Motion Primitives offer built - in stability
and robustness to disturbances but often struggle to capture complex periodic
behaviors. Moreover, they are limited in their ability to interpolate between
different tasks. These shortcomings substantially narrow their applicability,
excluding a wide class of practically meaningful tasks such as locomotion and
rhythmic tool use. In this work, we introduce Orbitally Stable Motion
Primitives (OSMPs) - a framework that combines a learned diffeomorphic encoder
with a supercritical Hopf bifurcation in latent space, enabling the accurate
acquisition of periodic motions from demonstrations while ensuring formal
guarantees of orbital stability and transverse contraction. Furthermore, by
conditioning the bijective encoder on the task, we enable a single learned
policy to represent multiple motion objectives, yielding consistent zero-shot
generalization to unseen motion objectives within the training distribution. We
validate the proposed approach through extensive simulation and real-world
experiments across a diverse range of robotic platforms - from collaborative
arms and soft manipulators to a bio-inspired rigid-soft turtle robot -
demonstrating its versatility and effectiveness in consistently outperforming
state-of-the-art baselines such as diffusion policies, among others.

</details>


### [313] [rt-RISeg: Real-Time Model-Free Robot Interactive Segmentation for Active Instance-Level Object Understanding](https://arxiv.org/abs/2507.10776)
*Howard H. Qian,Yiting Chen,Gaotian Wang,Podshara Chanrungmaneekul,Kaiyu Hang*

Main category: cs.RO

TL;DR: A novel real-time interactive perception framework, rt-RISeg, improves unseen object instance segmentation (UOIS) by leveraging robot interactions and body frame-invariant features, outperforming state-of-the-art methods by 27.5%.


<details>
  <summary>Details</summary>
Motivation: Existing UOIS methods overfit on static visual features, leading to poor generalization. The paper proposes an interactive approach inspired by the dynamic nature of vision.

Method: The rt-RISeg framework segments unseen objects in real-time using robot interactions and analysis of body frame-invariant features (BFIF), eliminating the need for learned models.

Result: Achieves 27.5% higher segmentation accuracy than state-of-the-art UOIS methods and can integrate with vision foundation models for enhanced performance.

Conclusion: Interactive perception, as demonstrated by rt-RISeg, offers a robust solution for UOIS, improving accuracy and adaptability without relying on static visual features.

Abstract: Successful execution of dexterous robotic manipulation tasks in new
environments, such as grasping, depends on the ability to proficiently segment
unseen objects from the background and other objects. Previous works in unseen
object instance segmentation (UOIS) train models on large-scale datasets, which
often leads to overfitting on static visual features. This dependency results
in poor generalization performance when confronted with out-of-distribution
scenarios. To address this limitation, we rethink the task of UOIS based on the
principle that vision is inherently interactive and occurs over time. We
propose a novel real-time interactive perception framework, rt-RISeg, that
continuously segments unseen objects by robot interactions and analysis of a
designed body frame-invariant feature (BFIF). We demonstrate that the relative
rotational and linear velocities of randomly sampled body frames, resulting
from selected robot interactions, can be used to identify objects without any
learned segmentation model. This fully self-contained segmentation pipeline
generates and updates object segmentation masks throughout each robot
interaction without the need to wait for an action to finish. We showcase the
effectiveness of our proposed interactive perception method by achieving an
average object segmentation accuracy rate 27.5% greater than state-of-the-art
UOIS methods. Furthermore, although rt-RISeg is a standalone framework, we show
that the autonomously generated segmentation masks can be used as prompts to
vision foundation models for significantly improved performance.

</details>


### [314] [Whom to Respond To? A Transformer-Based Model for Multi-Party Social Robot Interaction](https://arxiv.org/abs/2507.10960)
*He Zhu,Ryo Miyoshi,Yuki Okafuji*

Main category: cs.RO

TL;DR: A Transformer-based multi-task learning framework improves social robots' decision-making in multi-user HRI by using novel loss functions and a new dataset.


<details>
  <summary>Details</summary>
Motivation: Addressing the gap in multi-party HRI, where robots must decide when and to whom to respond, unlike single-user interactions.

Method: Proposes a Transformer-based multi-task learning framework with two novel loss functions for scene modeling and response selection, and introduces a multi-party HRI dataset.

Result: The model outperforms heuristic-based and single-task approaches, achieving state-of-the-art performance in response decisions.

Conclusion: The framework advances socially intelligent robots for natural, context-aware multi-party interactions.

Abstract: Prior human-robot interaction (HRI) research has primarily focused on
single-user interactions, where robots do not need to consider the timing or
recipient of their responses. However, in multi-party interactions, such as at
malls and hospitals, social robots must understand the context and decide both
when and to whom they should respond. In this paper, we propose a
Transformer-based multi-task learning framework to improve the decision-making
process of social robots, particularly in multi-user environments. Considering
the characteristics of HRI, we propose two novel loss functions: one that
enforces constraints on active speakers to improve scene modeling, and another
that guides response selection towards utterances specifically directed at the
robot. Additionally, we construct a novel multi-party HRI dataset that captures
real-world complexities, such as gaze misalignment. Experimental results
demonstrate that our model achieves state-of-the-art performance in respond
decisions, outperforming existing heuristic-based and single-task approaches.
Our findings contribute to the development of socially intelligent social
robots capable of engaging in natural and context-aware multi-party
interactions.

</details>


### [315] [Learning to Tune Like an Expert: Interpretable and Scene-Aware Navigation via MLLM Reasoning and CVAE-Based Adaptation](https://arxiv.org/abs/2507.11001)
*Yanbo Wang,Zipeng Fang,Lei Zhao,Weidong Chen*

Main category: cs.RO

TL;DR: LE-Nav is a scene-aware navigation framework using multi-modal LLMs and conditional variational autoencoders to adaptively tune planner hyperparameters, outperforming state-of-the-art methods in real-world trials.


<details>
  <summary>Details</summary>
Motivation: Conventional navigation systems fail in dynamic environments due to poor generalization, and existing RL-based methods struggle with sim-to-real transfer.

Method: LE-Nav combines multi-modal LLM reasoning, one-shot exemplars, chain-of-thought prompting, and conditional variational autoencoders for adaptive hyperparameter tuning.

Result: Achieves human-level tuning, outperforming SOTA in success rate, efficiency, safety, and comfort, with higher perceived safety and social acceptance.

Conclusion: LE-Nav demonstrates superior performance and adaptability in real-world navigation, validated by trials and user studies.

Abstract: Service robots are increasingly deployed in diverse and dynamic environments,
where both physical layouts and social contexts change over time and across
locations. In these unstructured settings, conventional navigation systems that
rely on fixed parameters often fail to generalize across scenarios, resulting
in degraded performance and reduced social acceptance. Although recent
approaches have leveraged reinforcement learning to enhance traditional
planners, these methods often fail in real-world deployments due to poor
generalization and limited simulation diversity, which hampers effective
sim-to-real transfer. To tackle these issues, we present LE-Nav, an
interpretable and scene-aware navigation framework that leverages multi-modal
large language model reasoning and conditional variational autoencoders to
adaptively tune planner hyperparameters. To achieve zero-shot scene
understanding, we utilize one-shot exemplars and chain-of-thought prompting
strategies. Additionally, a conditional variational autoencoder captures the
mapping between natural language instructions and navigation hyperparameters,
enabling expert-level tuning. Experiments show that LE-Nav can generate
hyperparameters achieving human-level tuning across diverse planners and
scenarios. Real-world navigation trials and a user study on a smart wheelchair
platform demonstrate that it outperforms state-of-the-art methods on
quantitative metrics such as success rate, efficiency, safety, and comfort,
while receiving higher subjective scores for perceived safety and social
acceptance. Code is available at https://github.com/Cavendish518/LE-Nav.

</details>


### [316] [TRAN-D: 2D Gaussian Splatting-based Sparse-view Transparent Object Depth Reconstruction via Physics Simulation for Scene Update](https://arxiv.org/abs/2507.11069)
*Jeongyun Kim,Seunghoon Jeong,Giseop Kim,Myung-Hwan Jeon,Eunji Jun,Ayoung Kim*

Main category: cs.RO

TL;DR: TRAN-D is a 2D Gaussian Splatting-based method for reconstructing the depth of transparent objects from RGB images, outperforming existing methods with improved accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Transparent objects pose challenges in 3D geometry reconstruction due to reflections and refractions, especially in sparse-view and dynamic scenarios.

Method: TRAN-D separates transparent objects from the background, uses object-aware loss for artifact mitigation, and incorporates physics-based simulation for refinement.

Result: TRAN-D reduces mean absolute error by over 39% on synthetic data and achieves higher accuracy with fewer images compared to baselines.

Conclusion: TRAN-D offers a robust and efficient solution for transparent object depth reconstruction, advancing the state-of-the-art in the field.

Abstract: Understanding the 3D geometry of transparent objects from RGB images is
challenging due to their inherent physical properties, such as reflection and
refraction. To address these difficulties, especially in scenarios with sparse
views and dynamic environments, we introduce TRAN-D, a novel 2D Gaussian
Splatting-based depth reconstruction method for transparent objects. Our key
insight lies in separating transparent objects from the background, enabling
focused optimization of Gaussians corresponding to the object. We mitigate
artifacts with an object-aware loss that places Gaussians in obscured regions,
ensuring coverage of invisible surfaces while reducing overfitting.
Furthermore, we incorporate a physics-based simulation that refines the
reconstruction in just a few seconds, effectively handling object removal and
chain-reaction movement of remaining objects without the need for rescanning.
TRAN-D is evaluated on both synthetic and real-world sequences, and it
consistently demonstrated robust improvements over existing GS-based
state-of-the-art methods. In comparison with baselines, TRAN-D reduces the mean
absolute error by over 39% for the synthetic TRansPose sequences. Furthermore,
despite being updated using only one image, TRAN-D reaches a {\delta} < 2.5 cm
accuracy of 48.46%, over 1.5 times that of baselines, which uses six images.
Code and more results are available at https://jeongyun0609.github.io/TRAN-D/.

</details>


### [317] [All Eyes, no IMU: Learning Flight Attitude from Vision Alone](https://arxiv.org/abs/2507.11302)
*Jesse J. Hagenaars,Stein Stroobants,Sander M. Bohte,Guido C. H. E. De Croon*

Main category: cs.RO

TL;DR: A vision-only flight control system for drones using an event camera and neural network, replacing traditional inertial sensors.


<details>
  <summary>Details</summary>
Motivation: To enable autonomous flight in robots without relying on accelerometers or gyroscopes, mimicking flying animals' vision-based control.

Method: Uses a recurrent convolutional neural network trained via supervised learning to estimate attitude and rotation rate from event camera data.

Result: Demonstrated successful flight control without inertial sensors, with performance varying by network memory and field of view.

Conclusion: Vision-only flight control is viable for insect-scale robots, with potential for generalization across environments.

Abstract: Vision is an essential part of attitude control for many flying animals, some
of which have no dedicated sense of gravity. Flying robots, on the other hand,
typically depend heavily on accelerometers and gyroscopes for attitude
stabilization. In this work, we present the first vision-only approach to
flight control for use in generic environments. We show that a quadrotor drone
equipped with a downward-facing event camera can estimate its attitude and
rotation rate from just the event stream, enabling flight control without
inertial sensors. Our approach uses a small recurrent convolutional neural
network trained through supervised learning. Real-world flight tests
demonstrate that our combination of event camera and low-latency neural network
is capable of replacing the inertial measurement unit in a traditional flight
control loop. Furthermore, we investigate the network's generalization across
different environments, and the impact of memory and different fields of view.
While networks with memory and access to horizon-like visual cues achieve best
performance, variants with a narrower field of view achieve better relative
generalization. Our work showcases vision-only flight control as a promising
candidate for enabling autonomous, insect-scale flying robots.

</details>


### [318] [Acting and Planning with Hierarchical Operational Models on a Mobile Robot: A Study with RAE+UPOM](https://arxiv.org/abs/2507.11345)
*Oscar Lima,Marc Vinci,Sunandita Patra,Sebastian Stock,Joachim Hertzberg,Martin Atzmueller,Malik Ghallab,Dana Nau,Paolo Traverso*

Main category: cs.RO

TL;DR: An integrated actor-planner system (RAE+UPOM) is deployed on a robot for robust task execution, addressing inconsistencies between symbolic planning and real-world control.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between symbolic planner models and actual robot control structures for reliable task execution.

Method: Combines the Reactive Acting Engine (RAE) with an anytime UCT-like Monte Carlo planner (UPOM) for interleaved acting and planning.

Result: Demonstrates robust task execution under action failures and sensor noise in real-world object collection tasks.

Conclusion: The RAE+UPOM system effectively integrates planning and acting, providing empirical insights into their interleaved decision-making process.

Abstract: Robotic task execution faces challenges due to the inconsistency between
symbolic planner models and the rich control structures actually running on the
robot. In this paper, we present the first physical deployment of an integrated
actor-planner system that shares hierarchical operational models for both
acting and planning, interleaving the Reactive Acting Engine (RAE) with an
anytime UCT-like Monte Carlo planner (UPOM). We implement RAE+UPOM on a mobile
manipulator in a real-world deployment for an object collection task. Our
experiments demonstrate robust task execution under action failures and sensor
noise, and provide empirical insights into the interleaved acting-and-planning
decision making process.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [319] [MultiVox: Benchmarking Voice Assistants for Multimodal Interactions](https://arxiv.org/abs/2507.10859)
*Ramaneswaran Selvakumar,Ashish Seth,Nishit Anand,Utkarsh Tyagi,Sonal Kumar,Sreyan Ghosh,Dinesh Manocha*

Main category: cs.MM

TL;DR: MultiVox is introduced as the first benchmark to evaluate voice assistants' ability to integrate spoken and visual cues, including paralinguistic features, for multimodal understanding. Current models struggle despite human proficiency.


<details>
  <summary>Details</summary>
Motivation: Current benchmarks inadequately assess LLMs' ability to understand fine-grained speech characteristics and align paralinguistic cues with visual signals for context-aware responses.

Method: MultiVox includes 1000 human-annotated speech dialogues with diverse paralinguistic and visual cues, evaluated on 9 state-of-the-art models.

Result: Humans excel at integrating multimodal cues, but current models consistently fail to produce contextually grounded responses.

Conclusion: MultiVox highlights the limitations of current models in multimodal understanding and sets a new standard for future evaluations.

Abstract: The rapid progress of Large Language Models (LLMs) has empowered omni models
to act as voice assistants capable of understanding spoken dialogues. These
models can process multimodal inputs beyond text, such as speech and visual
data, enabling more context-aware interactions. However, current benchmarks
fall short in comprehensively evaluating how well these models generate
context-aware responses, particularly when it comes to implicitly understanding
fine-grained speech characteristics, such as pitch, emotion, timbre, and volume
or the environmental acoustic context such as background sounds. Additionally,
they inadequately assess the ability of models to align paralinguistic cues
with complementary visual signals to inform their responses. To address these
gaps, we introduce MultiVox, the first omni voice assistant benchmark designed
to evaluate the ability of voice assistants to integrate spoken and visual cues
including paralinguistic speech features for truly multimodal understanding.
Specifically, MultiVox includes 1000 human-annotated and recorded speech
dialogues that encompass diverse paralinguistic features and a range of visual
cues such as images and videos. Our evaluation on 9 state-of-the-art models
reveals that, although humans excel at these tasks, current models consistently
struggle to produce contextually grounded responses.

</details>


<div id='econ.GN'></div>

# econ.GN [[Back]](#toc)

### [320] [Artificial Finance: How AI Thinks About Money](https://arxiv.org/abs/2507.10933)
*Orhan Erdem,Ragavi Pobbathi Ashok*

Main category: econ.GN

TL;DR: LLMs show risk-neutral financial decisions, occasional inconsistencies in time trade-offs, and resemble Tanzanian human responses in cross-national comparisons.


<details>
  <summary>Details</summary>
Motivation: To compare LLMs' financial decision-making with human responses globally and understand their emulation of human-like behaviors.

Method: Posed financial questions to seven LLMs and compared outputs to human responses from 53 nations.

Result: LLMs are risk-neutral, sometimes inconsistent in time trade-offs, and aggregate responses resemble Tanzanian participants.

Conclusion: LLMs emulate human decision behaviors with cultural and training influences evident in their outputs.

Abstract: In this paper, we explore how large language models (LLMs) approach financial
decision-making by systematically comparing their responses to those of human
participants across the globe. We posed a set of commonly used financial
decision-making questions to seven leading LLMs, including five models from the
GPT series(GPT-4o, GPT-4.5, o1, o3-mini), Gemini 2.0 Flash, and DeepSeek R1. We
then compared their outputs to human responses drawn from a dataset covering 53
nations. Our analysis reveals three main results. First, LLMs generally exhibit
a risk-neutral decision-making pattern, favoring choices aligned with expected
value calculations when faced with lottery-type questions. Second, when
evaluating trade-offs between present and future, LLMs occasionally produce
responses that appear inconsistent with normative reasoning. Third, when we
examine cross-national similarities, we find that the LLMs' aggregate responses
most closely resemble those of participants from Tanzania. These findings
contribute to the understanding of how LLMs emulate human-like decision
behaviors and highlight potential cultural and training influences embedded
within their outputs.

</details>
