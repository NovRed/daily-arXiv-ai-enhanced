<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 156]
- [cs.AI](#cs.AI) [Total: 55]
- [cs.CL](#cs.CL) [Total: 81]
- [cs.LG](#cs.LG) [Total: 110]
- [cond-mat.soft](#cond-mat.soft) [Total: 1]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [physics.optics](#physics.optics) [Total: 1]
- [cs.AR](#cs.AR) [Total: 3]
- [cs.DC](#cs.DC) [Total: 1]
- [cs.CR](#cs.CR) [Total: 17]
- [cs.IR](#cs.IR) [Total: 8]
- [physics.ao-ph](#physics.ao-ph) [Total: 1]
- [math.OC](#math.OC) [Total: 2]
- [q-bio.NC](#q-bio.NC) [Total: 3]
- [cs.GT](#cs.GT) [Total: 1]
- [cs.LO](#cs.LO) [Total: 1]
- [cs.RO](#cs.RO) [Total: 13]
- [cs.CY](#cs.CY) [Total: 16]
- [quant-ph](#quant-ph) [Total: 3]
- [hep-ph](#hep-ph) [Total: 1]
- [stat.ML](#stat.ML) [Total: 9]
- [stat.ME](#stat.ME) [Total: 2]
- [cs.HC](#cs.HC) [Total: 3]
- [cs.MA](#cs.MA) [Total: 5]
- [cs.GR](#cs.GR) [Total: 3]
- [math.NA](#math.NA) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.IT](#cs.IT) [Total: 1]
- [cs.SD](#cs.SD) [Total: 5]
- [eess.SY](#eess.SY) [Total: 1]
- [cs.NE](#cs.NE) [Total: 5]
- [cs.SE](#cs.SE) [Total: 6]
- [cs.SI](#cs.SI) [Total: 2]
- [eess.SP](#eess.SP) [Total: 16]
- [q-bio.QM](#q-bio.QM) [Total: 2]
- [cs.CE](#cs.CE) [Total: 1]
- [eess.IV](#eess.IV) [Total: 6]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [A Deep Learning-Based CCTV System for Automatic Smoking Detection in Fire Exit Zones](https://arxiv.org/abs/2508.11696)
*Sami Sadat,Mohammad Irtiza Hossain,Junaid Ahmed Sifat,Suhail Haque Rafi,Md. Waseq Alauddin Alvi,Md. Khalilur Rhaman*

Main category: cs.CV

TL;DR: A deep learning-based real-time smoking detection system using CCTV surveillance achieves high performance (78.90% recall, 83.70% mAP) with optimized YOLOv8 architecture, suitable for fire exit monitoring and safety compliance.


<details>
  <summary>Details</summary>
Motivation: Critical safety requirements for monitoring fire exit areas where smoking poses significant fire hazards, necessitating real-time detection systems for public safety and regulatory compliance.

Method: Evaluated YOLOv8, YOLOv11, and YOLOv12 models, then developed a custom YOLOv8-based model with additional structures for challenging surveillance contexts. Used dataset of 8,124 images from 20 scenarios with 2,708 low-light samples. Tested on edge devices with multithreaded operations.

Result: Proposed custom model outperformed others with 78.90% recall and 83.70% mAP at 50. Jetson Xavier NX achieved 52-97ms per inference, suitable for real-time operations. Demonstrated robust performance across varied environments including low-light conditions.

Conclusion: The system provides a robust and adaptable platform for real-time smoking detection in surveillance contexts, enabling automatic regulatory compliance and enhancing public safety in critical areas like fire exits.

Abstract: A deep learning real-time smoking detection system for CCTV surveillance of
fire exit areas is proposed due to critical safety requirements. The dataset
contains 8,124 images from 20 different scenarios along with 2,708 raw samples
demonstrating low-light areas. We evaluated three advanced object detection
models: YOLOv8, YOLOv11, and YOLOv12, followed by development of a custom model
derived from YOLOv8 with added structures for challenging surveillance
contexts. The proposed model outperformed the others, achieving a recall of
78.90 percent and mAP at 50 of 83.70 percent, delivering optimal object
detection across varied environments. Performance evaluation on multiple edge
devices using multithreaded operations showed the Jetson Xavier NX processed
data at 52 to 97 milliseconds per inference, establishing its suitability for
time-sensitive operations. This system offers a robust and adaptable platform
for monitoring public safety and enabling automatic regulatory compliance.

</details>


### [2] [Separating Knowledge and Perception with Procedural Data](https://arxiv.org/abs/2508.11697)
*Adrián Rodríguez-Muñoz,Manel Baradad,Phillip Isola,Antonio Torralba*

Main category: cs.CV

TL;DR: Procedural data-trained models achieve near-real performance on visual tasks using visual memory without real-world images, with strong results on similarity, classification, and segmentation tasks.


<details>
  <summary>Details</summary>
Motivation: To achieve full compartmentalization from real-world images while maintaining strong visual task performance by using only procedural data and visual memory techniques.

Method: Train representation models exclusively on procedural data and apply them to visual tasks using visual memory - an explicit database of reference image embeddings, without further training on real images.

Result: Performs within 1% on NIGHTS visual similarity, outperforms by 8-15% on fine-grained classification (CUB200, Flowers102), within 10% on ImageNet-1K classification, and achieves strong zero-shot segmentation (R^2 within 10% of real-data models on COCO).

Conclusion: Procedural models can achieve competitive performance to real-data models while maintaining full compartmentalization, though object part dissimilarity in representations causes incorrect memory searches that explain remaining performance gaps.

Abstract: We train representation models with procedural data only, and apply them on
visual similarity, classification, and semantic segmentation tasks without
further training by using visual memory -- an explicit database of reference
image embeddings. Unlike prior work on visual memory, our approach achieves
full compartmentalization with respect to all real-world images while retaining
strong performance. Compared to a model trained on Places, our procedural model
performs within $1\%$ on NIGHTS visual similarity, outperforms by $8\%$ and
$15\%$ on CUB200 and Flowers102 fine-grained classification, and is within
$10\%$ on ImageNet-1K classification. It also demonstrates strong zero-shot
segmentation, achieving an $R^2$ on COCO within $10\%$ of the models trained on
real data. Finally, we analyze procedural versus real data models, showing that
parts of the same object have dissimilar representations in procedural models,
resulting in incorrect searches in memory and explaining the remaining
performance gap.

</details>


### [3] [FusionFM: Fusing Eye-specific Foundational Models for Optimized Ophthalmic Diagnosis](https://arxiv.org/abs/2508.11721)
*Ke Zou,Jocelyn Hui Lin Goh,Yukun Zhou,Tian Lin,Samantha Min Er Yew,Sahana Srinivasan,Meng Wang,Rui Santos,Gabor M. Somfai,Huazhu Fu,Haoyu Chen,Pearse A. Keane,Ching-Yu Cheng,Yih Chung Tham*

Main category: cs.CV

TL;DR: Systematic evaluation of four ophthalmic foundation models (RETFound, VisionFM, RetiZero, DINORET) and their fusion approaches for both ophthalmic and systemic disease prediction from retinal imaging, showing DINORET and RetiZero perform best with RetiZero having better generalization.


<details>
  <summary>Details</summary>
Motivation: Foundation models show promise in medical imaging but there's no clear understanding of which performs best in ophthalmology, how they compare across different tasks, and whether combining them improves performance.

Method: Proposed FusionFM evaluation suite with two fusion approaches to integrate different ophthalmic FMs. Evaluated on standardized datasets from multiple countries using AUC and F1 metrics for both ophthalmic diseases (glaucoma, diabetic retinopathy, AMD) and systemic diseases (diabetes, hypertension).

Result: DINORET and RetiZero achieved superior performance in both ophthalmic and systemic disease tasks. RetiZero showed stronger generalization on external datasets. Gating-based fusion provided modest improvements for glaucoma, AMD, and hypertension prediction. Systemic disease prediction, especially hypertension in external cohorts, remains challenging.

Conclusion: This study provides the first evidence-based evaluation of ophthalmic foundation models, demonstrates benefits of model fusion, and identifies strategies for enhancing clinical applicability while highlighting remaining challenges in systemic disease prediction.

Abstract: Foundation models (FMs) have shown great promise in medical image analysis by
improving generalization across diverse downstream tasks. In ophthalmology,
several FMs have recently emerged, but there is still no clear answer to
fundamental questions: Which FM performs the best? Are they equally good across
different tasks? What if we combine all FMs together? To our knowledge, this is
the first study to systematically evaluate both single and fused ophthalmic
FMs. To address these questions, we propose FusionFM, a comprehensive
evaluation suite, along with two fusion approaches to integrate different
ophthalmic FMs. Our framework covers both ophthalmic disease detection
(glaucoma, diabetic retinopathy, and age-related macular degeneration) and
systemic disease prediction (diabetes and hypertension) based on retinal
imaging. We benchmarked four state-of-the-art FMs (RETFound, VisionFM,
RetiZero, and DINORET) using standardized datasets from multiple countries and
evaluated their performance using AUC and F1 metrics. Our results show that
DINORET and RetiZero achieve superior performance in both ophthalmic and
systemic disease tasks, with RetiZero exhibiting stronger generalization on
external datasets. Regarding fusion strategies, the Gating-based approach
provides modest improvements in predicting glaucoma, AMD, and hypertension.
Despite these advances, predicting systemic diseases, especially hypertension
in external cohort remains challenging. These findings provide an
evidence-based evaluation of ophthalmic FMs, highlight the benefits of model
fusion, and point to strategies for enhancing their clinical applicability.

</details>


### [4] [UniDCF: A Foundation Model for Comprehensive Dentocraniofacial Hard Tissue Reconstruction](https://arxiv.org/abs/2508.11728)
*Chunxia Ren,Ning Zhu,Yue Lai,Gui Chen,Ruijie Wang,Yangyi Hu,Suyao Liu,Shuwen Mao,Hong Su,Yu Zhang,Li Xiao*

Main category: cs.CV

TL;DR: UniDCF is a unified deep learning framework that reconstructs multiple dentocraniofacial hard tissues using multimodal fusion of point clouds and multi-view images, achieving superior geometric precision and clinical efficiency.


<details>
  <summary>Details</summary>
Motivation: Current deep learning models are limited to single-tissue scenarios and modality-specific inputs, resulting in poor generalizability and trade-offs between anatomical fidelity, computational efficiency, and cross-tissue adaptability for dentocraniofacial reconstruction.

Method: UniDCF uses multimodal fusion encoding of point clouds and multi-view images, leveraging complementary strengths of each modality with a score-based denoising module to refine surface smoothness. It was trained on the largest multimodal dataset with 54,555 annotated instances from 6,609 patients.

Result: UniDCF outperforms state-of-the-art methods in geometric precision, structural completeness, and spatial accuracy. It reduces reconstruction design time by 99% and achieves clinician-rated acceptability exceeding 94%.

Conclusion: UniDCF enables rapid, automated, high-fidelity reconstruction for personalized restorative treatments, streamlining clinical workflows and enhancing patient outcomes through its unified multimodal approach.

Abstract: Dentocraniofacial hard tissue defects profoundly affect patients'
physiological functions, facial aesthetics, and psychological well-being,
posing significant challenges for precise reconstruction. Current deep learning
models are limited to single-tissue scenarios and modality-specific imaging
inputs, resulting in poor generalizability and trade-offs between anatomical
fidelity, computational efficiency, and cross-tissue adaptability. Here we
introduce UniDCF, a unified framework capable of reconstructing multiple
dentocraniofacial hard tissues through multimodal fusion encoding of point
clouds and multi-view images. By leveraging the complementary strengths of each
modality and incorporating a score-based denoising module to refine surface
smoothness, UniDCF overcomes the limitations of prior single-modality
approaches. We curated the largest multimodal dataset, comprising intraoral
scans, CBCT, and CT from 6,609 patients, resulting in 54,555 annotated
instances. Evaluations demonstrate that UniDCF outperforms existing
state-of-the-art methods in terms of geometric precision, structural
completeness, and spatial accuracy. Clinical simulations indicate UniDCF
reduces reconstruction design time by 99% and achieves clinician-rated
acceptability exceeding 94%. Overall, UniDCF enables rapid, automated, and
high-fidelity reconstruction, supporting personalized and precise restorative
treatments, streamlining clinical workflows, and enhancing patient outcomes.

</details>


### [5] [Ovis2.5 Technical Report](https://arxiv.org/abs/2508.11737)
*Shiyin Lu,Yang Li,Yu Xia,Yuwei Hu,Shanshan Zhao,Yanqing Ma,Zhichao Wei,Yinglun Li,Lunhao Duan,Jianshan Zhao,Yuxuan Han,Haijun Li,Wanying Chen,Junke Tang,Chengkun Hou,Zhixing Du,Tianli Zhou,Wenjie Zhang,Huping Ding,Jiahe Li,Wen Li,Gui Hu,Yiliang Gu,Siran Yang,Jiamang Wang,Hailong Sun,Yibo Wang,Hui Sun,Jinlong Huang,Yuping He,Shengze Shi,Weihong Zhang,Guodong Zheng,Junpeng Jiang,Sensen Gao,Yi-Feng Wu,Sijia Chen,Yuhui Chen,Qing-Guo Chen,Zhao Xu,Weihua Luo,Kaifu Zhang*

Main category: cs.CV

TL;DR: Ovis2.5 is an advanced multimodal model that processes images at native resolution using a vision transformer, features reflection-based reasoning with self-checking capabilities, and achieves state-of-the-art performance on multiple benchmarks with both 9B and 2B parameter versions.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of fixed-resolution image processing that degrades fine details and global layout, particularly for visually dense content like complex charts, and to enhance reasoning capabilities beyond simple chain-of-thought approaches.

Method: Uses native-resolution vision transformer for variable-resolution image processing, implements reflection-based reasoning with self-checking and revision, employs a five-phase curriculum training (visual/multimodal pretraining, instruction tuning, DPO/GRPO alignment), and utilizes multimodal data packing with hybrid parallelism for efficiency.

Result: Ovis2.5-9B scores 78.3 on OpenCompass leaderboard (substantial improvement over Ovis2-8B), Ovis2.5-2B scores 73.9 (SOTA for its size), achieves leading results on STEM benchmarks, strong grounding/video capabilities, and SOTA for complex chart analysis at its scale.

Conclusion: Ovis2.5 successfully addresses native-resolution visual perception and advanced reasoning, delivering state-of-the-art performance across multiple domains while maintaining efficiency through optimized training methods and offering both large and small model variants for different deployment scenarios.

Abstract: We present Ovis2.5, a successor to Ovis2 designed for native-resolution
visual perception and strong multimodal reasoning. Ovis2.5 integrates a
native-resolution vision transformer that processes images at their native,
variable resolutions, avoiding the degradation from fixed-resolution tiling and
preserving both fine detail and global layout -- crucial for visually dense
content like complex charts. To strengthen reasoning, we train the model to
move beyond linear chain-of-thought and perform reflection -- including
self-checking and revision. This advanced capability is exposed as an optional
"thinking mode" at inference time, allowing users to trade latency for enhanced
accuracy on difficult inputs. The model is trained via a comprehensive
five-phase curriculum that progressively builds its skills. The process begins
with foundational visual and multimodal pretraining, advances through
large-scale instruction tuning, and culminates in alignment and reasoning
enhancement using DPO and GRPO. To scale these upgrades efficiently, we employ
multimodal data packing and hybrid parallelism, yielding a significant
end-to-end speedup. We release two open-source models: Ovis2.5-9B and
Ovis2.5-2B. The latter continues the "small model, big performance" philosophy
of Ovis2, making it ideal for resource-constrained, on-device scenarios. On the
OpenCompass multimodal leaderboard, Ovis2.5-9B averages 78.3, marking a
substantial improvement over its predecessor, Ovis2-8B, and achieving
state-of-the-art results among open-source MLLMs in the sub-40B parameter
range; Ovis2.5-2B scores 73.9, establishing SOTA for its size. Beyond aggregate
scores, Ovis2.5 achieves leading results on STEM benchmarks, exhibits strong
capabilities on grounding and video tasks, and achieves open-source SOTA at its
scale for complex chart analysis.

</details>


### [6] [VideoAVE: A Multi-Attribute Video-to-Text Attribute Value Extraction Dataset and Benchmark Models](https://arxiv.org/abs/2508.11801)
*Ming Cheng,Tong Wu,Jiazhen Hu,Jiaying Gong,Hoda Eldardiry*

Main category: cs.CV

TL;DR: VideoAVE is the first public video-to-text e-commerce attribute value extraction dataset covering 14 domains and 172 attributes, with 224k training and 25k evaluation samples filtered by CLIP-MoE, showing current VLMs struggle with temporal information in open attribute extraction.


<details>
  <summary>Details</summary>
Motivation: Existing AVE datasets lack support for product videos, diverse attribute coverage, and public availability, creating a gap for video-based e-commerce product structuring.

Method: Created VideoAVE dataset with CLIP-based Mixture of Experts filtering to remove mismatched video-product pairs, then benchmarked state-of-the-art video VLMs on attribute-conditioned and open attribute-value extraction tasks.

Result: Video-to-text AVE remains challenging, especially in open settings, with current VLMs showing limited ability to leverage temporal information effectively.

Conclusion: There is significant room for developing more advanced video VLMs that can better utilize temporal information for attribute value extraction from product videos.

Abstract: Attribute Value Extraction (AVE) is important for structuring product
information in e-commerce. However, existing AVE datasets are primarily limited
to text-to-text or image-to-text settings, lacking support for product videos,
diverse attribute coverage, and public availability. To address these gaps, we
introduce VideoAVE, the first publicly available video-to-text e-commerce AVE
dataset across 14 different domains and covering 172 unique attributes. To
ensure data quality, we propose a post-hoc CLIP-based Mixture of Experts
filtering system (CLIP-MoE) to remove the mismatched video-product pairs,
resulting in a refined dataset of 224k training data and 25k evaluation data.
In order to evaluate the usability of the dataset, we further establish a
comprehensive benchmark by evaluating several state-of-the-art video vision
language models (VLMs) under both attribute-conditioned value prediction and
open attribute-value pair extraction tasks. Our results analysis reveals that
video-to-text AVE remains a challenging problem, particularly in open settings,
and there is still room for developing more advanced VLMs capable of leveraging
effective temporal information. The dataset and benchmark code for VideoAVE are
available at: https://github.com/gjiaying/VideoAVE

</details>


### [7] [An MLP Baseline for Handwriting Recognition Using Planar Curvature and Gradient Orientation](https://arxiv.org/abs/2508.11803)
*Azam Nouri*

Main category: cs.CV

TL;DR: A curvature-orientation MLP using second-order geometric cues achieves 97% accuracy on MNIST and 89% on EMNIST, showing interpretable handcrafted features can rival deep learning performance.


<details>
  <summary>Details</summary>
Motivation: To investigate whether second-order geometric cues alone can drive effective handwritten character recognition as an alternative to convolutional neural networks, using interpretable hand-engineered features.

Method: Used three handcrafted feature maps (planar curvature magnitude, curvature sign, and gradient orientation) as inputs to a multilayer perceptron classifier for handwritten character recognition.

Result: Achieved 97% accuracy on MNIST digits and 89% accuracy on EMNIST letters, demonstrating strong performance comparable to deep learning approaches.

Conclusion: Curvature-based representations have significant discriminative power for handwritten characters, and the advantages of deep learning can be achieved with interpretable, hand-engineered features without complex CNNs.

Abstract: This study investigates whether second-order geometric cues - planar
curvature magnitude, curvature sign, and gradient orientation - are sufficient
on their own to drive a multilayer perceptron (MLP) classifier for handwritten
character recognition (HCR), offering an alternative to convolutional neural
networks (CNNs). Using these three handcrafted feature maps as inputs, our
curvature-orientation MLP achieves 97 percent accuracy on MNIST digits and 89
percent on EMNIST letters. These results underscore the discriminative power of
curvature-based representations for handwritten character images and
demonstrate that the advantages of deep learning can be realized even with
interpretable, hand-engineered features.

</details>


### [8] [Labels or Input? Rethinking Augmentation in Multimodal Hate Detection](https://arxiv.org/abs/2508.11808)
*Sahajpreet Singh,Rongxin Ouyang,Subhayan Mukerjee,Kokil Jaidka*

Main category: cs.CV

TL;DR: A dual approach combining prompt optimization and multimodal data augmentation improves hateful meme detection by enhancing model robustness and reducing spurious correlations.


<details>
  <summary>Details</summary>
Motivation: The web is saturated with multimodal hate content disguised as humor, and current Vision-Language Models lack fine-grained supervision and struggle with implicit hate speech detection.

Method: 1) Prompt optimization framework varying structure, supervision granularity, and training modality; 2) Multimodal data augmentation pipeline generating 2,479 counterfactually neutral memes using multi-agent LLM-VLM setup.

Result: Structured prompts improve robustness even in small models, InternVL2 achieves best F1-scores, and the augmentation pipeline successfully reduces spurious correlations and improves classifier generalization.

Conclusion: Prompt structure and data composition are as critical as model size, and targeted augmentation enables more trustworthy, context-sensitive hate detection while inspiring synthetic data approaches for robust vision-language models.

Abstract: The modern web is saturated with multimodal content, intensifying the
challenge of detecting hateful memes, where harmful intent is often conveyed
through subtle interactions between text and image under the guise of humor or
satire. While recent advances in Vision-Language Models (VLMs) show promise,
these models lack support for fine-grained supervision and remain susceptible
to implicit hate speech. In this paper, we present a dual-pronged approach to
improve multimodal hate detection. First, we propose a prompt optimization
framework that systematically varies prompt structure, supervision granularity,
and training modality. We show that prompt design and label scaling both
influence performance, with structured prompts improving robustness even in
small models, and InternVL2 achieving the best F1-scores across binary and
scaled settings. Second, we introduce a multimodal data augmentation pipeline
that generates 2,479 counterfactually neutral memes by isolating and rewriting
the hateful modality. This pipeline, powered by a multi-agent LLM-VLM setup,
successfully reduces spurious correlations and improves classifier
generalization. Our approaches inspire new directions for building synthetic
data to train robust and fair vision-language models. Our findings demonstrate
that prompt structure and data composition are as critical as model size, and
that targeted augmentation can support more trustworthy and context-sensitive
hate detection.

</details>


### [9] [Towards Understanding 3D Vision: the Role of Gaussian Curvature](https://arxiv.org/abs/2508.11825)
*Sherlon Almeida da Silva,Davi Geiger,Luiz Velho,Moacir Antonelli Ponti*

Main category: cs.CV

TL;DR: The paper investigates Gaussian curvature as an invariant geometric prior for 3D surface modeling, showing it provides sparse surface descriptions, is implicitly used by state-of-the-art methods, and can improve reconstruction and serve as an unsupervised stereo metric.


<details>
  <summary>Details</summary>
Motivation: Current deep learning approaches for 3D vision lack explicit geometric models that can be analyzed, transferred, or systematically modified. The researchers want to explore Gaussian curvature as a fundamental geometric invariant for better 3D surface understanding.

Method: The study investigates Gaussian curvature properties using the Middlebury stereo dataset, analyzing its role as a sparse descriptor, examining implicit use in existing methods, and exploring its potential as a geometric prior and unsupervised metric.

Result: Gaussian curvature offers a compact 3D surface description, state-of-the-art methods implicitly consider it but lack explicit modules, it serves as an effective geometric prior for reconstruction improvement, and shows promise as an unsupervised stereo evaluation metric.

Conclusion: Gaussian curvature provides a valuable geometric foundation for 3D surface modeling that complements data-driven approaches, offering explicit geometric understanding, transferability across modalities, and systematic experimental control missing in current deep learning methods.

Abstract: Recent advances in computer vision have predominantly relied on data-driven
approaches that leverage deep learning and large-scale datasets. Deep neural
networks have achieved remarkable success in tasks such as stereo matching and
monocular depth reconstruction. However, these methods lack explicit models of
3D geometry that can be directly analyzed, transferred across modalities, or
systematically modified for controlled experimentation. We investigate the role
of Gaussian curvature in 3D surface modeling. Besides Gaussian curvature being
an invariant quantity under change of observers or coordinate systems, we
demonstrate using the Middlebury stereo dataset that it offers: (i) a sparse
and compact description of 3D surfaces, (ii) state-of-the-art monocular and
stereo methods seem to implicitly consider it, but no explicit module of such
use can be extracted, (iii) a form of geometric prior that can inform and
improve 3D surface reconstruction, and (iv) a possible use as an unsupervised
metric for stereo methods.

</details>


### [10] [From Pixels to Graphs: Deep Graph-Level Anomaly Detection on Dermoscopic Images](https://arxiv.org/abs/2508.11826)
*Dehn Xu,Tim Katzke,Emmanuel Müller*

Main category: cs.CV

TL;DR: Systematic comparison of image-to-graph transformation methods for GNN-based anomaly detection, finding color features perform best alone but shape/texture combinations improve results across all supervision levels.


<details>
  <summary>Details</summary>
Motivation: No previous study has rigorously compared the effectiveness of various image-to-graph transformation approaches for graph-level anomaly detection using GNNs, despite their growing popularity in graph-based machine learning.

Method: Systematically evaluated multiple segmentation schemes, edge construction strategies, and node feature sets (color, texture, shape descriptors) for producing image-derived graph representations. Conducted experiments on dermoscopic images using state-of-the-art GLAD models in unsupervised, weakly supervised, and fully supervised regimes.

Result: Color descriptors provided the best standalone performance. Incorporating shape and texture features consistently enhanced detection efficacy. Best unsupervised configuration achieved AUC-ROC of 0.805 without pretrained backbones. Performance increased to 0.872 with sparse labels and 0.914 with full supervision.

Conclusion: Comprehensive image-to-graph transformation evaluation provides valuable insights for GNN-based anomaly detection, demonstrating competitive performance across different supervision levels and the importance of feature combination strategies.

Abstract: Graph Neural Networks (GNNs) have emerged as a powerful approach for
graph-based machine learning tasks. Previous work applied GNNs to image-derived
graph representations for various downstream tasks such as classification or
anomaly detection. These transformations include segmenting images, extracting
features from segments, mapping them to nodes, and connecting them. However, to
the best of our knowledge, no study has rigorously compared the effectiveness
of the numerous potential image-to-graph transformation approaches for
GNN-based graph-level anomaly detection (GLAD). In this study, we
systematically evaluate the efficacy of multiple segmentation schemes, edge
construction strategies, and node feature sets based on color, texture, and
shape descriptors to produce suitable image-derived graph representations to
perform graph-level anomaly detection. We conduct extensive experiments on
dermoscopic images using state-of-the-art GLAD models, examining performance
and efficiency in purely unsupervised, weakly supervised, and fully supervised
regimes. Our findings reveal, for example, that color descriptors contribute
the best standalone performance, while incorporating shape and texture features
consistently enhances detection efficacy. In particular, our best unsupervised
configuration using OCGTL achieves a competitive AUC-ROC score of up to 0.805
without relying on pretrained backbones like comparable image-based approaches.
With the inclusion of sparse labels, the performance increases substantially to
0.872 and with full supervision to 0.914 AUC-ROC.

</details>


### [11] [Recent Advances in Transformer and Large Language Models for UAV Applications](https://arxiv.org/abs/2508.11834)
*Hamza Kheddar,Yassine Habchi,Mohamed Chahine Ghanem,Mustapha Hemis,Dusit Niyato*

Main category: cs.CV

TL;DR: Comprehensive review of Transformer-based models in UAV systems, covering architectures, applications, datasets, and future research directions.


<details>
  <summary>Details</summary>
Motivation: To systematically categorize and evaluate recent developments in Transformer architectures applied to UAVs, addressing the rapid advancement that has reshaped UAV perception, decision-making, and autonomy.

Method: Presents a unified taxonomy of Transformer-based UAV models, provides comparative analyses through structured tables and performance benchmarks, and reviews key datasets, simulators, and evaluation metrics.

Result: The review highlights emerging applications (precision agriculture, autonomous navigation), identifies existing literature gaps, and outlines critical challenges in computational efficiency and real-time deployment.

Conclusion: This comprehensive synthesis aims to guide researchers and practitioners in understanding and advancing Transformer-driven UAV technologies, offering future research directions for the field.

Abstract: The rapid advancement of Transformer-based models has reshaped the landscape
of uncrewed aerial vehicle (UAV) systems by enhancing perception,
decision-making, and autonomy. This review paper systematically categorizes and
evaluates recent developments in Transformer architectures applied to UAVs,
including attention mechanisms, CNN-Transformer hybrids, reinforcement learning
Transformers, and large language models (LLMs). Unlike previous surveys, this
work presents a unified taxonomy of Transformer-based UAV models, highlights
emerging applications such as precision agriculture and autonomous navigation,
and provides comparative analyses through structured tables and performance
benchmarks. The paper also reviews key datasets, simulators, and evaluation
metrics used in the field. Furthermore, it identifies existing gaps in the
literature, outlines critical challenges in computational efficiency and
real-time deployment, and offers future research directions. This comprehensive
synthesis aims to guide researchers and practitioners in understanding and
advancing Transformer-driven UAV technologies.

</details>


### [12] [ComplicitSplat: Downstream Models are Vulnerable to Blackbox Attacks by 3D Gaussian Splat Camouflages](https://arxiv.org/abs/2508.11854)
*Matthew Hull,Haoyang Yang,Pratham Mehta,Mansi Phute,Aeree Cho,Haorang Wang,Matthew Lau,Wenke Lee,Wilian Lunardi,Martin Andreoni,Polo Chau*

Main category: cs.CV

TL;DR: ComplicitSplat is the first black-box attack that exploits 3D Gaussian Splatting shading methods to create viewpoint-specific adversarial camouflage, making malicious content visible only from certain angles without needing model access.


<details>
  <summary>Details</summary>
Motivation: As 3DGS becomes widely used in safety-critical applications like autonomous navigation, there's a need to understand how adversaries could tamper with images to cause harm through novel attack vectors.

Method: The attack exploits standard 3DGS shading methods to create viewpoint-specific camouflage - colors and textures that change with viewing angle to embed adversarial content visible only from specific viewpoints, operating in a black-box manner without requiring model architecture or weights.

Result: Extensive experiments show ComplicitSplat successfully attacks various popular detectors including single-stage, multi-stage, and transformer-based models on both real-world physical object captures and synthetic scenes.

Conclusion: This exposes a novel safety risk for mission-critical applications, demonstrating the first black-box attack on downstream object detectors using 3DGS, highlighting vulnerabilities in safety-critical robotic systems.

Abstract: As 3D Gaussian Splatting (3DGS) gains rapid adoption in safety-critical tasks
for efficient novel-view synthesis from static images, how might an adversary
tamper images to cause harm? We introduce ComplicitSplat, the first attack that
exploits standard 3DGS shading methods to create viewpoint-specific camouflage
- colors and textures that change with viewing angle - to embed adversarial
content in scene objects that are visible only from specific viewpoints and
without requiring access to model architecture or weights. Our extensive
experiments show that ComplicitSplat generalizes to successfully attack a
variety of popular detector - both single-stage, multi-stage, and
transformer-based models on both real-world capture of physical objects and
synthetic scenes. To our knowledge, this is the first black-box attack on
downstream object detectors using 3DGS, exposing a novel safety risk for
applications like autonomous navigation and other mission-critical robotic
systems.

</details>


### [13] [Impact of Clinical Image Quality on Efficient Foundation Model Finetuning](https://arxiv.org/abs/2508.11864)
*Yucheng Tang,Pawel Rajwa,Alexander Ng,Yipei Wang,Wen Yan,Natasha Thorley,Aqua Asif,Clare Allen,Louise Dickinson,Francesco Giganti,Shonit Punwani,Daniel C. Alexander,Veeru Kasivisvanathan,Yipeng Hu*

Main category: cs.CV

TL;DR: Image quality distribution significantly impacts label-efficient finetuning of medical foundation models, with performance depending on high-quality image ratios and distribution alignment between finetuning and test sets.


<details>
  <summary>Details</summary>
Motivation: To evaluate how variable image quality affects label-efficient finetuning of foundation models in medical imaging, specifically investigating the impact of quality distribution mismatches between finetuning and deployment datasets.

Method: Systematic experiments varying high-/low-quality image ratios in both finetuning and evaluation sets using ProFound, a domain-specific vision foundation model pretrained on large-scale prostate MRI datasets.

Result: Image quality distribution and finetune-test mismatch significantly affect performance. Sufficient high-quality finetuning data is critical, and performance varies by downstream task. Consistent quality ratios enable far less labeled data than training from scratch.

Conclusion: Assessing and aligning quality distributions between finetuning and deployment is crucial. Quality standards in finetuning data are needed to realize the full efficiency benefits of foundation models in medical imaging.

Abstract: Foundation models in medical imaging have shown promising label efficiency,
achieving high downstream performance with only a fraction of annotated data.
Here, we evaluate this in prostate multiparametric MRI using ProFound, a
domain-specific vision foundation model pretrained on large-scale prostate MRI
datasets. We investigate how variable image quality affects label-efficient
finetuning by measuring the generalisability of finetuned models. Experiments
systematically vary high-/low-quality image ratios in finetuning and evaluation
sets. Our findings indicate that image quality distribution and its
finetune-and-test mismatch significantly affect model performance. In
particular: a) Varying the ratio of high- to low-quality images between
finetuning and test sets leads to notable differences in downstream
performance; and b) The presence of sufficient high-quality images in the
finetuning set is critical for maintaining strong performance, whilst the
importance of matched finetuning and testing distribution varies between
different downstream tasks, such as automated radiology reporting and prostate
cancer detection.When quality ratios are consistent, finetuning needs far less
labeled data than training from scratch, but label efficiency depends on image
quality distribution. Without enough high-quality finetuning data, pretrained
models may fail to outperform those trained without pretraining. This
highlights the importance of assessing and aligning quality distributions
between finetuning and deployment, and the need for quality standards in
finetuning data for specific downstream tasks. Using ProFound, we show the
value of quantifying image quality in both finetuning and deployment to fully
realise the data and compute efficiency benefits of foundation models.

</details>


### [14] [AdaRing: Towards Ultra-Light Vision-Language Adaptation via Cross-Layer Tensor Ring Decomposition](https://arxiv.org/abs/2508.11870)
*Ying Huang,Yuanbin Man,Wenqi Jia,Zhengzhong Tu,Junzhou Huang,Miao Yin*

Main category: cs.CV

TL;DR: AdaRing proposes a cross-layer tensor ring decomposition framework for vision-language model fine-tuning that reduces parameter redundancy and improves adapter diversity, achieving SOTA performance with 90% fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Existing adapter-based fine-tuning methods suffer from limited compression rates due to cross-layer redundancy and limited representational capacity across homogeneous adapters.

Method: Uses tensor ring decomposition to formulate adapters as layer-shared tensor cores and layer-specific slices, with diverse rank-driven adapters guided by generalization-aware fine-tuning.

Result: Achieves state-of-the-art performance while reducing average training parameters by 90% across various tasks.

Conclusion: The proposed AdaRing framework effectively addresses redundancy issues in adapter-based fine-tuning and enables ultra-light parameter-efficient adaptation of vision-language models.

Abstract: Adapter-based fine-tuning has gained remarkable attention in adapting large
pre-trained vision language models (VLMs) for a wide range of downstream tasks
efficiently. In this paradigm, only the inserted adapters are fine-tuned,
without the need for training the original VLM backbone. Existing works scale
adapters by integrating them into every layer of VLMs to increase the capacity
of adapters. However, these methods face two primary limitations: 1) limited
compression rate due to ignoring cross-layer redundancy, and 2) limited
representational capacity across homogeneous adapters. In this paper, we
propose a novel vision-language fine-tuning framework based on cross-layer
tensor ring decomposition (TRD) with the integration and collaboration of
diverse adapters, called AdaRing, achieving ultra-light parameter-efficient
adaptation of VLMs on various tasks. To remove the high redundancy that exists
among adapters across layers, we exploit the tensor-level low-rankness to
formulate adapters as layer-shared tensor cores and layer-specific slices.
Moreover, guided by generalization-aware fine-tuning, diverse rank-driven
adapters cooperate to handle tasks that require different representations. Our
experiments show that the proposed AdaRing achieves the state-of-the-art
performance while reducing average training parameters by 90%.

</details>


### [15] [EVTP-IVS: Effective Visual Token Pruning For Unifying Instruction Visual Segmentation In Multi-Modal Large Language Models](https://arxiv.org/abs/2508.11886)
*Wenhui Zhu,Xiwen Chen,Zhipeng Wang,Shao Tang,Sayan Ghosh,Xuanzhao Dong,Rajat Koner,Yalin Wang*

Main category: cs.CV

TL;DR: EVTP-IV is a visual token pruning method that achieves 3.5-5X speedup in Instructed Visual Segmentation tasks while maintaining comparable accuracy using only 20% of tokens.


<details>
  <summary>Details</summary>
Motivation: Multimodal large language models (MLLMs) have strong performance on Instructed Visual Segmentation but suffer from high inference costs, especially in video processing. The authors observed a correlation between token coverage and segmentation performance.

Method: A novel visual token pruning method called EVTP-IV that builds upon k-center algorithm by integrating spatial information to ensure better coverage of representative tokens. Includes information-theoretic analysis to support the design.

Result: Achieves up to 5X speed-up on video tasks and 3.5X on image tasks while maintaining comparable accuracy using only 20% of tokens. Consistently outperforms state-of-the-art pruning baselines across varying pruning ratios.

Conclusion: The proposed EVTP-IV method effectively reduces computational costs for MLLMs in visual segmentation tasks while preserving performance through intelligent token selection based on spatial coverage principles.

Abstract: Instructed Visual Segmentation (IVS) tasks require segmenting objects in
images or videos based on natural language instructions. While recent
multimodal large language models (MLLMs) have achieved strong performance on
IVS, their inference cost remains a major bottleneck, particularly in video. We
empirically analyze visual token sampling in MLLMs and observe a strong
correlation between subset token coverage and segmentation performance. This
motivates our design of a simple and effective token pruning method that
selects a compact yet spatially representative subset of tokens to accelerate
inference. In this paper, we introduce a novel visual token pruning method for
IVS, called EVTP-IV, which builds upon the k-center by integrating spatial
information to ensure better coverage. We further provide an
information-theoretic analysis to support our design. Experiments on standard
IVS benchmarks show that our method achieves up to 5X speed-up on video tasks
and 3.5X on image tasks, while maintaining comparable accuracy using only 20%
of the tokens. Our method also consistently outperforms state-of-the-art
pruning baselines under varying pruning ratios.

</details>


### [16] [Large Kernel Modulation Network for Efficient Image Super-Resolution](https://arxiv.org/abs/2508.11893)
*Quanwei Hu,Yinggan Tang,Xuguang Zhang*

Main category: cs.CV

TL;DR: LKMN is a pure CNN-based image super-resolution model that uses large kernel modulation to achieve better performance than Transformers while maintaining fast inference speed, outperforming SOTA lightweight models with 0.23dB PSNR improvement and 4.8x faster speed.


<details>
  <summary>Details</summary>
Motivation: Address the trade-off between CNNs (low latency but poor non-local feature capture) and Transformers (excellent non-local modeling but slow inference) in resource-constrained image super-resolution scenarios.

Method: Proposes Large Kernel Modulation Network (LKMN) with two core components: Enhanced Partial Large Kernel Block (EPLKB) using channel shuffle, channel attention, and large kernel strip convolutions for non-local feature extraction; and Cross-Gate Feed-Forward Network (CGFN) that dynamically adjusts feature discrepancies and employs cross-gate strategy for feature fusion.

Result: Outperforms existing SOTA lightweight SR models, achieving 0.23 dB PSNR improvement over DAT-light on Manga109 dataset at 4x upscale with nearly 4.8 times faster inference speed.

Conclusion: LKMN successfully balances quality and efficiency in image super-resolution, demonstrating that pure CNN architectures can achieve superior non-local feature modeling while maintaining fast inference speeds compared to Transformer-based approaches.

Abstract: Image super-resolution (SR) in resource-constrained scenarios demands
lightweight models balancing performance and latency. Convolutional neural
networks (CNNs) offer low latency but lack non-local feature capture, while
Transformers excel at non-local modeling yet suffer slow inference. To address
this trade-off, we propose the Large Kernel Modulation Network (LKMN), a pure
CNN-based model. LKMN has two core components: Enhanced Partial Large Kernel
Block (EPLKB) and Cross-Gate Feed-Forward Network (CGFN). The EPLKB utilizes
channel shuffle to boost inter-channel interaction, incorporates channel
attention to focus on key information, and applies large kernel strip
convolutions on partial channels for non-local feature extraction with reduced
complexity. The CGFN dynamically adjusts discrepancies between input, local,
and non-local features via a learnable scaling factor, then employs a
cross-gate strategy to modulate and fuse these features, enhancing their
complementarity. Extensive experiments demonstrate that our method outperforms
existing state-of-the-art (SOTA) lightweight SR models while balancing quality
and efficiency. Specifically, LKMN-L achieves 0.23 dB PSNR improvement over
DAT-light on the Manga109 dataset at $\times$4 upscale, with nearly $\times$4.8
times faster. Codes are in the supplementary materials. The code is available
at https://github.com/Supereeeee/LKMN.

</details>


### [17] [A Sobel-Gradient MLP Baseline for Handwritten Character Recognition](https://arxiv.org/abs/2508.11902)
*Azam Nouri*

Main category: cs.CV

TL;DR: Using only horizontal and vertical Sobel derivatives as input, an MLP achieves near-CNN performance on handwritten character recognition with smaller memory footprint and transparent features.


<details>
  <summary>Details</summary>
Motivation: To investigate whether first-order edge maps (Sobel derivatives) are sufficient for handwritten character recognition as an alternative to convolutional neural networks, exploring simpler and more transparent architectures.

Method: Train a multilayer perceptron (MLP) using only horizontal and vertical Sobel derivatives as input features on MNIST and EMNIST Letters datasets.

Result: The MLP achieved 98% accuracy on MNIST digits and 92% on EMNIST letters, approaching CNN performance while offering smaller memory footprint and more transparent features.

Conclusion: First-order gradients capture most class-discriminative information in handwritten characters, making edge-aware MLPs a compelling alternative to CNNs for HCR tasks.

Abstract: We revisit the classical Sobel operator to ask a simple question: Are
first-order edge maps sufficient to drive an all-dense multilayer perceptron
(MLP) for handwritten character recognition (HCR), as an alternative to
convolutional neural networks (CNNs)? Using only horizontal and vertical Sobel
derivatives as input, we train an MLP on MNIST and EMNIST Letters. Despite its
extreme simplicity, the resulting network reaches 98% accuracy on MNIST digits
and 92% on EMNIST letters -- approaching CNNs while offering a smaller memory
footprint and transparent features. Our findings highlight that much of the
class-discriminative information in handwritten character images is already
captured by first-order gradients, making edge-aware MLPs a compelling option
for HCR.

</details>


### [18] [OVG-HQ: Online Video Grounding with Hybrid-modal Queries](https://arxiv.org/abs/2508.11903)
*Runhao Zeng,Jiaqi Mao,Minghao Lai,Minh Hieu Phan,Yanjie Dong,Wei Wang,Qi Chen,Xiping Hu*

Main category: cs.CV

TL;DR: Proposes OVG-HQ, a new online video grounding task with hybrid-modal queries (text, images, video segments), and introduces OVG-HQ-Unify framework with Parametric Memory Block and cross-modal distillation to handle modality imbalance and limited context.


<details>
  <summary>Details</summary>
Motivation: Traditional video grounding struggles with streaming video scenarios and visual-based queries, creating a need for online processing and support for multiple query modalities beyond just text.

Method: Developed OVG-HQ-Unify framework with Parametric Memory Block (PMB) to retain past knowledge for current decisions, and cross-modal distillation to balance modality learning. Created QVHighlights-Unify dataset with multi-modal queries and adapted online evaluation metrics.

Result: OVG-HQ-Unify outperforms existing models, providing robust performance for online hybrid-modal video grounding with both accuracy and efficiency improvements.

Conclusion: The proposed framework successfully addresses challenges of online video grounding with hybrid queries, offering a unified solution that handles multiple modalities effectively while maintaining real-time processing capabilities.

Abstract: Video grounding (VG) task focuses on locating specific moments in a video
based on a query, usually in text form. However, traditional VG struggles with
some scenarios like streaming video or queries using visual cues. To fill this
gap, we present a new task named Online Video Grounding with Hybrid-modal
Queries (OVG-HQ), which enables online segment localization using text, images,
video segments, and their combinations. This task poses two new challenges:
limited context in online settings and modality imbalance during training,
where dominant modalities overshadow weaker ones. To address these, we propose
OVG-HQ-Unify, a unified framework featuring a Parametric Memory Block (PMB)
that retain previously learned knowledge to enhance current decision and a
cross-modal distillation strategy that guides the learning of non-dominant
modalities. This design enables a single model to effectively handle
hybrid-modal queries. Due to the lack of suitable datasets, we construct
QVHighlights-Unify, an expanded dataset with multi-modal queries. Besides,
since offline metrics overlook prediction timeliness, we adapt them to the
online setting, introducing oR@n, IoU=m, and online mean Average Precision
(omAP) to evaluate both accuracy and efficiency. Experiments show that our
OVG-HQ-Unify outperforms existing models, offering a robust solution for
online, hybrid-modal video grounding. Source code and datasets are available at
https://github.com/maojiaqi2324/OVG-HQ.

</details>


### [19] [SafeCtrl: Region-Based Safety Control for Text-to-Image Diffusion via Detect-Then-Suppress](https://arxiv.org/abs/2508.11904)
*Lingyun Zhang,Yu Xie,Yanwei Fu,Ping Chen*

Main category: cs.CV

TL;DR: SafeCtrl is a lightweight plugin that localizes unsafe content in text-to-image generation and suppresses harmful semantics using DPO training, achieving better safety and fidelity than existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing safety methods for text-to-image models create trade-offs between safety and fidelity, and localization-based approaches can cause semantic incongruity through hard concept replacement.

Method: SafeCtrl uses a detect-then-suppress paradigm with precise localization of unsafe content and semantic suppression rather than hard substitution. It employs Direct Preference Optimization (DPO) training using image-level preference data without needing pixel-level annotations.

Result: Extensive experiments show SafeCtrl significantly outperforms state-of-the-art methods in both safety efficacy and fidelity preservation.

Conclusion: Decoupled, suppression-based control is an effective and scalable approach for building more responsible generative models.

Abstract: The widespread deployment of text-to-image models is challenged by their
potential to generate harmful content. While existing safety methods, such as
prompt rewriting or model fine-tuning, provide valuable interventions, they
often introduce a trade-off between safety and fidelity. Recent
localization-based approaches have shown promise, yet their reliance on
explicit ``concept replacement" can sometimes lead to semantic incongruity. To
address these limitations, we explore a more flexible detect-then-suppress
paradigm. We introduce SafeCtrl, a lightweight, non-intrusive plugin that first
precisely localizes unsafe content. Instead of performing a hard A-to-B
substitution, SafeCtrl then suppresses the harmful semantics, allowing the
generative process to naturally and coherently resolve into a safe,
context-aware alternative. A key aspect of our work is a novel training
strategy using Direct Preference Optimization (DPO). We leverage readily
available, image-level preference data to train our module, enabling it to
learn nuanced suppression behaviors and perform region-guided interventions at
inference without requiring costly, pixel-level annotations. Extensive
experiments show that SafeCtrl significantly outperforms state-of-the-art
methods in both safety efficacy and fidelity preservation. Our findings suggest
that decoupled, suppression-based control is a highly effective and scalable
direction for building more responsible generative models.

</details>


### [20] [TimeSenCLIP: A Vision-Language Model for Remote Sensing Using Single-Pixel Time Series](https://arxiv.org/abs/2508.11919)
*Pallavi Jain,Diego Marcos,Dino Ienco,Roberto Interdonato,Tristan Berchoux*

Main category: cs.CV

TL;DR: TimeSenCLIP is a lightweight vision-language framework that uses single pixel temporal and spectral data from Sentinel-2 imagery for land-use classification, eliminating the need for large spatial tiles and text-based supervision.


<details>
  <summary>Details</summary>
Motivation: Current vision-language models for remote sensing rely on large spatial tiles (computationally expensive) and text-based supervision (often unavailable), creating scalability challenges for large-scale applications.

Method: Leverages spectral and temporal information from single Sentinel-2 pixels combined with cross-view learning using geo-tagged ground-level photos from LUCAS and Sen4Map datasets, minimizing caption-based training requirements.

Result: Demonstrates that single pixel inputs with temporal and spectral cues are sufficient for thematic mapping tasks including LULC, crop type, and ecosystem type classification.

Conclusion: Provides a scalable and efficient alternative to traditional approaches by reducing computational costs while maintaining semantic alignment between satellite and ground perspectives for large-scale remote sensing applications.

Abstract: Vision-language models have shown significant promise in remote sensing
applications, particularly for land-use and land-cover (LULC) via zero-shot
classification and retrieval. However, current approaches face two key
challenges: reliance on large spatial tiles that increase computational cost,
and dependence on text-based supervision, which is often not readily available.
In this work, we present TimeSenCLIP, a lightweight framework that reevaluate
the role of spatial context by evaluating the effectiveness of a single pixel
by leveraging its temporal and spectral dimensions, for classifying LULC and
ecosystem types. By leveraging spectral and temporal information from
Sentinel-2 imagery and cross-view learning with geo-tagged ground-level photos,
we minimises the need for caption-based training while preserving semantic
alignment between overhead (satellite) and ground perspectives. Our approach is
grounded in the LUCAS and Sen4Map datasets, and evaluated on classification
tasks including LULC, crop type, and ecosystem type. We demonstrate that single
pixel inputs, when combined with temporal and spectral cues, are sufficient for
thematic mapping, offering a scalable and efficient alternative for large-scale
remote sensing applications. Code is available at
https://github.com/pallavijain-pj/TimeSenCLIP

</details>


### [21] [Assessment of Using Synthetic Data in Brain Tumor Segmentation](https://arxiv.org/abs/2508.11922)
*Aditi Jahagirdar,Sameer Joshi*

Main category: cs.CV

TL;DR: Synthetic MRI data from GANs can improve brain tumor segmentation boundary delineation when combined with 40% real data, but class imbalance issues persist for tumor core regions.


<details>
  <summary>Details</summary>
Motivation: Address challenges in brain tumor segmentation including tumor heterogeneity, scarcity of annotated data, and class imbalance in medical imaging datasets by using synthetic data to improve dataset diversity.

Method: Used pre-trained GAN model (medigan library) to generate synthetic MRI data, combined with real BraTS 2020 data in varying proportions. Trained U-Net segmentation network on real-only, synthetic-only, and hybrid datasets.

Result: Quantitative performance (Dice, IoU, precision, recall, accuracy) was comparable between real-only and hybrid models. Qualitative analysis showed hybrid datasets (40% real + 60% synthetic) improved whole tumor boundary delineation, but tumor core and enhancing tumor regions still had lower accuracy due to class imbalance.

Conclusion: Synthetic data is feasible for brain tumor segmentation augmentation, but future work needs larger-scale experiments, volumetric data consistency, and better class imbalance mitigation strategies.

Abstract: Manual brain tumor segmentation from MRI scans is challenging due to tumor
heterogeneity, scarcity of annotated data, and class imbalance in medical
imaging datasets. Synthetic data generated by generative models has the
potential to mitigate these issues by improving dataset diversity. This study
investigates, as a proof of concept, the impact of incorporating synthetic MRI
data, generated using a pre-trained GAN model, into training a U-Net
segmentation network. Experiments were conducted using real data from the BraTS
2020 dataset, synthetic data generated with the medigan library, and hybrid
datasets combining real and synthetic samples in varying proportions. While
overall quantitative performance (Dice coefficient, IoU, precision, recall,
accuracy) was comparable between real-only and hybrid-trained models,
qualitative inspection suggested that hybrid datasets, particularly with 40%
real and 60% synthetic data, improved whole tumor boundary delineation.
However, region-wise accuracy for the tumor core and the enhancing tumor
remained lower, indicating a persistent class imbalance. The findings support
the feasibility of synthetic data as an augmentation strategy for brain tumor
segmentation, while highlighting the need for larger-scale experiments,
volumetric data consistency, and mitigating class imbalance in future work.

</details>


### [22] [Deep Learning For Point Cloud Denoising: A Survey](https://arxiv.org/abs/2508.11932)
*Chengwei Zhang,Xueyi Zhang,Mingrui Lao,Tao Jiang,Xinhao Xu,Wenjie Li,Fubo Zhang,Longyong Chen*

Main category: cs.CV

TL;DR: A comprehensive survey paper on deep learning-based point cloud denoising methods, categorizing approaches into outlier removal and surface noise restoration, with comparative analysis and future research directions.


<details>
  <summary>Details</summary>
Motivation: Real-world point clouds contain various types and intensities of noise, requiring denoising as preprocessing for downstream tasks. Despite deep learning methods outperforming traditional approaches, no systematic survey exists to summarize developments in this field.

Method: The paper formulates point cloud denoising as a two-step process: outlier removal and surface noise restoration. It creates a taxonomy tailored to denoising tasks, compares methods based on similarities, differences, and advantages, and systematically summarizes existing approaches.

Result: The survey provides a comprehensive framework for understanding deep learning-based point cloud denoising, categorizing methods, and identifying key challenges and contributions in the field.

Conclusion: The paper fills the research gap by offering the first systematic survey of DL-based point cloud denoising, providing insights into current limitations and future research directions to advance the field.

Abstract: Real-world environment-derived point clouds invariably exhibit noise across
varying modalities and intensities. Hence, point cloud denoising (PCD) is
essential as a preprocessing step to improve downstream task performance. Deep
learning (DL)-based PCD models, known for their strong representation
capabilities and flexible architectures, have surpassed traditional methods in
denoising performance. To our best knowledge, despite recent advances in
performance, no comprehensive survey systematically summarizes the developments
of DL-based PCD. To fill the gap, this paper seeks to identify key challenges
in DL-based PCD, summarizes the main contributions of existing methods, and
proposes a taxonomy tailored to denoising tasks. To achieve this goal, we
formulate PCD as a two-step process: outlier removal and surface noise
restoration, encompassing most scenarios and requirements of PCD. Additionally,
we compare methods in terms of similarities, differences, and respective
advantages. Finally, we discuss research limitations and future directions,
offering insights for further advancements in PCD.

</details>


### [23] [DynamicPose: Real-time and Robust 6D Object Pose Tracking for Fast-Moving Cameras and Objects](https://arxiv.org/abs/2508.11950)
*Tingbang Liang,Yixin Zeng,Jiatong Xie,Boyu Zhou*

Main category: cs.CV

TL;DR: DynamicPose is a retraining-free 6D pose tracking framework that handles fast-moving camera and object scenarios using visual-inertial odometry, depth-informed 2D tracking, and VIO-guided Kalman filtering in a closed-loop system.


<details>
  <summary>Details</summary>
Motivation: Previous 6D pose tracking methods perform poorly in fast-moving scenarios where both camera and objects move rapidly, limiting their applicability to static or quasi-static scenes only.

Method: Three synergistic components: (1) Visual-inertial odometry compensates for camera motion ROI shifts, (2) Depth-informed 2D tracker corrects ROI deviations from object translation, (3) VIO-guided Kalman filter predicts rotation and refines poses hierarchically in a closed-loop system.

Result: The method achieves real-time and robust 6D pose tracking performance in both simulation and real-world experiments with fast-moving cameras and objects.

Conclusion: DynamicPose successfully overcomes limitations of previous methods by providing accurate pose initialization and precise tracking in dynamic scenarios without requiring retraining, demonstrating effectiveness for real-time applications.

Abstract: We present DynamicPose, a retraining-free 6D pose tracking framework that
improves tracking robustness in fast-moving camera and object scenarios.
Previous work is mainly applicable to static or quasi-static scenes, and its
performance significantly deteriorates when both the object and the camera move
rapidly. To overcome these challenges, we propose three synergistic components:
(1) A visual-inertial odometry compensates for the shift in the Region of
Interest (ROI) caused by camera motion; (2) A depth-informed 2D tracker
corrects ROI deviations caused by large object translation; (3) A VIO-guided
Kalman filter predicts object rotation, generates multiple candidate poses, and
then obtains the final pose by hierarchical refinement. The 6D pose tracking
results guide subsequent 2D tracking and Kalman filter updates, forming a
closed-loop system that ensures accurate pose initialization and precise pose
tracking. Simulation and real-world experiments demonstrate the effectiveness
of our method, achieving real-time and robust 6D pose tracking for fast-moving
cameras and objects.

</details>


### [24] [Transferable Class Statistics and Multi-scale Feature Approximation for 3D Object Detection](https://arxiv.org/abs/2508.11951)
*Hao Peng,Hong Sang,Yajing Ma,Ping Qiu,Chao Ji*

Main category: cs.CV

TL;DR: This paper proposes a lightweight object detection method for point clouds that approximates multi-scale features from single neighborhoods using knowledge distillation, employs transferable class-aware statistics, and introduces central weighted IoU for better localization.


<details>
  <summary>Details</summary>
Motivation: Multi-scale feature learning in point cloud object detection typically requires multiple neighborhood searches and scale-aware layers, which increases computational costs and hinders lightweight model development, especially for resource-constrained research.

Method: The method approximates multi-scale features from a single neighborhood using knowledge distillation, designs transferable feature embedding with class-aware statistics for computational efficiency, and introduces central weighted intersection over union to address center offset misalignment in optimization.

Result: Extensive experiments on public datasets demonstrate the effectiveness of the proposed method, showing successful object detection from point clouds while significantly reducing computational costs compared to traditional multi-scale approaches.

Conclusion: The proposed approach provides an effective and computationally efficient solution for point cloud object detection by approximating multi-scale features from single neighborhoods, using transferable features, and improving localization accuracy with central weighted IoU, making it suitable for resource-constrained environments.

Abstract: This paper investigates multi-scale feature approximation and transferable
features for object detection from point clouds. Multi-scale features are
critical for object detection from point clouds. However, multi-scale feature
learning usually involves multiple neighborhood searches and scale-aware
layers, which can hinder efforts to achieve lightweight models and may not be
conducive to research constrained by limited computational resources. This
paper approximates point-based multi-scale features from a single neighborhood
based on knowledge distillation. To compensate for the loss of constructive
diversity in a single neighborhood, this paper designs a transferable feature
embedding mechanism. Specifically, class-aware statistics are employed as
transferable features given the small computational cost. In addition, this
paper introduces the central weighted intersection over union for localization
to alleviate the misalignment brought by the center offset in optimization.
Note that the method presented in this paper saves computational costs.
Extensive experiments on public datasets demonstrate the effectiveness of the
proposed method.

</details>


### [25] [UniUGG: Unified 3D Understanding and Generation via Geometric-Semantic Encoding](https://arxiv.org/abs/2508.11952)
*Yueming Xu,Jiahui Zhang,Ze Huang,Yurui Chen,Yanpeng Zhou,Zhenyu Chen,Yu-Jie Yuan,Pengxiang Xia,Guowei Huang,Xinyue Cai,Zhongang Qi,Xingyue Quan,Jianye Hao,Hang Xu,Li Zhang*

Main category: cs.CV

TL;DR: UniUGG is the first unified framework for 3D understanding and generation that uses an LLM to process both text and 3D representations, featuring a spatial decoder with latent diffusion for high-quality 3D generation and geometric-semantic pretraining for enhanced spatial capabilities.


<details>
  <summary>Details</summary>
Motivation: Despite progress in unified architectures for 2D image understanding and generation, integrating 3D tasks remains challenging and largely unexplored, creating a need for a comprehensive framework that can handle both 3D understanding and generation.

Method: The framework employs an LLM to comprehend sentences and 3D representations, uses a spatial decoder with latent diffusion model for 3D generation, and implements geometric-semantic learning strategy to pretrain the vision encoder for capturing both semantic and geometric cues.

Result: Extensive experimental results demonstrate superiority in visual representation, spatial understanding, and 3D generation tasks, showing improved performance in both generation from reference images with view transformations and spatial visual question answering.

Conclusion: UniUGG successfully addresses the integration challenge of 3D tasks within unified architectures, providing a comprehensive solution that bridges 3D understanding and generation through innovative spatial decoding and geometric-semantic learning approaches.

Abstract: Despite the impressive progress on understanding and generating images shown
by the recent unified architectures, the integration of 3D tasks remains
challenging and largely unexplored. In this paper, we introduce UniUGG, the
first unified understanding and generation framework for 3D modalities. Our
unified framework employs an LLM to comprehend and decode sentences and 3D
representations. At its core, we propose a spatial decoder leveraging a latent
diffusion model to generate high-quality 3D representations. This allows for
the generation and imagination of 3D scenes based on a reference image and an
arbitrary view transformation, while remaining supports for spatial visual
question answering (VQA) tasks. Additionally, we propose a geometric-semantic
learning strategy to pretrain the vision encoder. This design jointly captures
the input's semantic and geometric cues, enhancing both spatial understanding
and generation. Extensive experimental results demonstrate the superiority of
our method in visual representation, spatial understanding, and 3D generation.
The source code will be released upon paper acceptance.

</details>


### [26] [SAMDWICH: Moment-aware Video-text Alignment for Referring Video Object Segmentation](https://arxiv.org/abs/2508.11955)
*Seunghun Lee,Jiwan Seo,Jeonghoon Kim,Siwon Kim,Haeun Yun,Hyogyeong Jeon,Wonhyeok Choi,Jaehoon Jeong,Zane Durante,Sang Hyun Park,Sunghoon Im*

Main category: cs.CV

TL;DR: SAMDWICH introduces moment-aware RVOS framework with temporal moment annotations and selective supervision to address semantic misalignment in video object segmentation with language queries.


<details>
  <summary>Details</summary>
Motivation: Existing RVOS methods suffer from semantic misalignment due to indiscriminate frame sampling and supervision of all visible objects regardless of their relevance to language expressions.

Method: Proposes SAMDWICH framework with Moment-guided Dual-path Propagation (MDP) for moment-aware object grounding and tracking, and Object-level Selective Supervision (OSS) for filtering only temporally aligned objects. Uses newly annotated MeViS-M dataset with temporal moment annotations.

Result: Achieves state-of-the-art performance on challenging MeViS benchmark, particularly excelling in complex scenarios with diverse expressions.

Conclusion: The moment-aware framework with selective supervision significantly enhances video-text alignment and referential understanding in RVOS tasks.

Abstract: Referring Video Object Segmentation (RVOS) aims to segment and track objects
in videos based on natural language expressions, requiring precise alignment
between visual content and textual queries. However, existing methods often
suffer from semantic misalignment, largely due to indiscriminate frame sampling
and supervision of all visible objects during training -- regardless of their
actual relevance to the expression. To address this, we introduce a
moment-aware RVOS framework named SAMDWICH, along with a newly annotated
dataset, MeViS-M, built upon the challenging MeViS benchmark. We manually
annotate temporal moments indicating when each object is referred to by the
expression, enabling semantically grounded supervision that strengthens
video-text alignment. SAMDWICH leverages these aligned text-to-clip pairs to
guide training, significantly enhancing referential understanding. Building
upon this framework, we propose Moment-guided Dual-path Propagation (MDP), a
moment-aware propagation strategy that improves both object grounding and
tracking by training on both relevant and irrelevant frames through a
moment-centric memory mechanism. In addition, we introduce Object-level
Selective Supervision (OSS), an object-level filtering strategy that supervises
only the objects temporally aligned with the expression in each training clip.
This selective supervision reduces semantic noise and reinforces
language-conditioned learning. Extensive experiments show that SAMDWICH
achieves state-of-the-art performance on challenging MeViS benchmark,
particularly excelling in complex scenarios involving diverse expressions.

</details>


### [27] [PEdger++: Practical Edge Detection via Assembling Cross Information](https://arxiv.org/abs/2508.11961)
*Yuanbin Fu,Liang Li,Xiaojie Guo*

Main category: cs.CV

TL;DR: PEdger++ is a collaborative learning framework for efficient edge detection that balances accuracy and computational efficiency by leveraging cross-information from heterogeneous architectures, training moments, and parameter samplings.


<details>
  <summary>Details</summary>
Motivation: Edge detection is crucial for computer vision applications but existing deep learning methods suffer from high computational costs, limiting deployment on resource-constrained devices. The paper aims to achieve high accuracy with low computational complexity.

Method: Proposes PEdger++, a collaborative learning framework that extracts cross-information from heterogeneous architectures, diverse training moments, and multiple parameter samplings to enhance learning from an ensemble perspective.

Result: Extensive experiments on BSDS500, NYUD and Multicue datasets show clear improvements over existing methods both quantitatively and qualitatively, with multiple model versions available for different computational requirements.

Conclusion: PEdger++ effectively balances edge detection accuracy with computational efficiency, demonstrating adaptability to various resource constraints through its collaborative learning approach and ensemble-based feature extraction.

Abstract: Edge detection serves as a critical foundation for numerous computer vision
applications, including object detection, semantic segmentation, and image
editing, by extracting essential structural cues that define object boundaries
and salient edges. To be viable for broad deployment across devices with
varying computational capacities, edge detectors shall balance high accuracy
with low computational complexity. While deep learning has evidently improved
accuracy, they often suffer from high computational costs, limiting their
applicability on resource-constrained devices. This paper addresses the
challenge of achieving that balance: \textit{i.e.}, {how to efficiently capture
discriminative features without relying on large-size and sophisticated
models}. We propose PEdger++, a collaborative learning framework designed to
reduce computational costs and model sizes while improving edge detection
accuracy. The core principle of our PEdger++ is that cross-information derived
from heterogeneous architectures, diverse training moments, and multiple
parameter samplings, is beneficial to enhance learning from an ensemble
perspective. Extensive experimental results on the BSDS500, NYUD and Multicue
datasets demonstrate the effectiveness of our approach, both quantitatively and
qualitatively, showing clear improvements over existing methods. We also
provide multiple versions of the model with varying computational requirements,
highlighting PEdger++'s adaptability with respect to different resource
constraints. Codes are accessible at
https://github.com/ForawardStar/EdgeDetectionviaPEdgerPlus/.

</details>


### [28] [Exploring Spatial-Temporal Dynamics in Event-based Facial Micro-Expression Analysis](https://arxiv.org/abs/2508.11988)
*Nicolas Mastropasqua,Ignacio Bugueno-Cordova,Rodrigo Verschae,Daniel Acevedo,Pablo Negri,Maria E. Buemi*

Main category: cs.CV

TL;DR: A novel multi-modal micro-expression dataset with synchronized RGB and event cameras shows event-based data significantly outperforms RGB for Action Unit classification (51.23% vs 23.12%) and achieves high-quality frame reconstruction.


<details>
  <summary>Details</summary>
Motivation: Micro-expression analysis is valuable for applications like Human-Robot Interaction and Driver Monitoring Systems, but RGB cameras struggle with capturing subtle, fast facial movements due to temporal resolution limitations and motion blur.

Method: Created a multi-resolution, multi-modal dataset with synchronized RGB and event cameras under variable lighting conditions. Evaluated two baseline tasks: Action Unit classification using Spiking Neural Networks and frame reconstruction using Conditional Variational Autoencoders.

Result: Event-based data achieved 51.23% accuracy for Action Unit classification vs 23.12% with RGB, and frame reconstruction achieved SSIM = 0.8513 and PSNR = 26.89 dB with high-resolution event input.

Conclusion: Event cameras show promising results for micro-expression recognition and frame reconstruction, outperforming traditional RGB cameras due to their microsecond-level precision, high dynamic range, and low latency.

Abstract: Micro-expression analysis has applications in domains such as Human-Robot
Interaction and Driver Monitoring Systems. Accurately capturing subtle and fast
facial movements remains difficult when relying solely on RGB cameras, due to
limitations in temporal resolution and sensitivity to motion blur. Event
cameras offer an alternative, with microsecond-level precision, high dynamic
range, and low latency. However, public datasets featuring event-based
recordings of Action Units are still scarce. In this work, we introduce a
novel, preliminary multi-resolution and multi-modal micro-expression dataset
recorded with synchronized RGB and event cameras under variable lighting
conditions. Two baseline tasks are evaluated to explore the spatial-temporal
dynamics of micro-expressions: Action Unit classification using Spiking Neural
Networks (51.23\% accuracy with events vs. 23.12\% with RGB), and frame
reconstruction using Conditional Variational Autoencoders, achieving SSIM =
0.8513 and PSNR = 26.89 dB with high-resolution event input. These promising
results show that event-based data can be used for micro-expression recognition
and frame reconstruction.

</details>


### [29] [MOON: Generative MLLM-based Multimodal Representation Learning for E-commerce Product Understanding](https://arxiv.org/abs/2508.11999)
*Daoze Zhang,Zhanheng Nie,Jianyu Liu,Chenghan Fu,Wanxian Guan,Yuan Gao,Jun Song,Pengjie Wang,Jian Xu,Bo Zheng*

Main category: cs.CV

TL;DR: MOON is the first generative MLLM-based model for product representation learning that addresses multimodal alignment challenges through guided MoE modules, background noise mitigation, and specialized negative sampling, achieving strong zero-shot performance across various product understanding tasks.


<details>
  <summary>Details</summary>
Motivation: Existing discriminative dual-flow architectures struggle with many-to-one alignment between multiple product images and texts, while generative MLLMs show potential but face challenges including lack of multimodal modeling modules, background noise in images, and absence of standardized benchmarks.

Method: Proposes MOON with: (1) guided Mixture-of-Experts module for multimodal and aspect-specific content modeling, (2) core semantic region detection to mitigate background noise, and (3) specialized negative sampling strategy for increased difficulty and diversity. Also releases MBE benchmark.

Result: Demonstrates competitive zero-shot performance on both the new MBE benchmark and public datasets, with strong generalization across cross-modal retrieval, product classification, and attribute prediction tasks. Case studies and visualizations confirm effectiveness.

Conclusion: MOON successfully addresses key challenges in product representation learning through generative MLLM approach, showing significant potential for improving product understanding with robust generalization capabilities across multiple downstream tasks.

Abstract: With the rapid advancement of e-commerce, exploring general representations
rather than task-specific ones has attracted increasing research attention. For
product understanding, although existing discriminative dual-flow architectures
drive progress in this field, they inherently struggle to model the many-to-one
alignment between multiple images and texts of products. Therefore, we argue
that generative Multimodal Large Language Models (MLLMs) hold significant
potential for improving product representation learning. Nevertheless,
achieving this goal still remains non-trivial due to several key challenges:
the lack of multimodal and aspect-aware modeling modules in typical LLMs; the
common presence of background noise in product images; and the absence of a
standard benchmark for evaluation. To address these issues, we propose the
first generative MLLM-based model named MOON for product representation
learning. Our method (1) employs a guided Mixture-of-Experts (MoE) module for
targeted modeling of multimodal and aspect-specific product content; (2)
effectively detects core semantic regions in product images to mitigate the
distraction and interference caused by background noise; and (3) introduces the
specialized negative sampling strategy to increase the difficulty and diversity
of negative samples. In addition, we release a large-scale multimodal benchmark
MBE for various product understanding tasks. Experimentally, our model
demonstrates competitive zero-shot performance on both our benchmark and the
public dataset, showcasing strong generalization across various downstream
tasks, including cross-modal retrieval, product classification, and attribute
prediction. Furthermore, the case study and visualization illustrate the
effectiveness of MOON for product understanding.

</details>


### [30] [InstDrive: Instance-Aware 3D Gaussian Splatting for Driving Scenes](https://arxiv.org/abs/2508.12015)
*Hongyuan Liu,Haochen Yu,Jianfei Jiang,Qiankun Liu,Jiansheng Chen,Huimin Ma*

Main category: cs.CV

TL;DR: InstDrive is an instance-aware 3D Gaussian Splatting framework for dynamic driving scene reconstruction that uses SAM masks as pseudo ground-truth and introduces regularization to encode instance identities without complex preprocessing.


<details>
  <summary>Details</summary>
Motivation: Current methods unify background elements into single representations, hindering instance-level understanding and scene editing. Existing approaches rely on pre-processed IDs or complex pipelines and are designed for indoor scenes, making them unsuitable for outdoor driving scenarios.

Method: Uses SAM-generated masks as pseudo ground-truth for 2D feature learning via contrastive loss. Introduces 3D regularization to implicitly encode instance identities with voxel-based loss consistency. Employs lightweight static codebook to bridge continuous features and discrete identities without preprocessing.

Result: Quantitative and qualitative experiments demonstrate effectiveness. First framework to achieve 3D instance segmentation in dynamic, open-world driving scenes.

Conclusion: InstDrive successfully addresses limitations of existing methods by providing instance-aware reconstruction for dynamic driving scenes without complex preprocessing, enabling better scene understanding and editing capabilities.

Abstract: Reconstructing dynamic driving scenes from dashcam videos has attracted
increasing attention due to its significance in autonomous driving and scene
understanding. While recent advances have made impressive progress, most
methods still unify all background elements into a single representation,
hindering both instance-level understanding and flexible scene editing. Some
approaches attempt to lift 2D segmentation into 3D space, but often rely on
pre-processed instance IDs or complex pipelines to map continuous features to
discrete identities. Moreover, these methods are typically designed for indoor
scenes with rich viewpoints, making them less applicable to outdoor driving
scenarios. In this paper, we present InstDrive, an instance-aware 3D Gaussian
Splatting framework tailored for the interactive reconstruction of dynamic
driving scene. We use masks generated by SAM as pseudo ground-truth to guide 2D
feature learning via contrastive loss and pseudo-supervised objectives. At the
3D level, we introduce regularization to implicitly encode instance identities
and enforce consistency through a voxel-based loss. A lightweight static
codebook further bridges continuous features and discrete identities without
requiring data pre-processing or complex optimization. Quantitative and
qualitative experiments demonstrate the effectiveness of InstDrive, and to the
best of our knowledge, it is the first framework to achieve 3D instance
segmentation in dynamic, open-world driving scenes.More visualizations are
available at our project page.

</details>


### [31] [WiseLVAM: A Novel Framework For Left Ventricle Automatic Measurements](https://arxiv.org/abs/2508.12023)
*Durgesh Kumar Singh,Qing Cao,Sarina Thomas,Ahcène Boubekki,Robert Jenssen,Michael Kampffmeyer*

Main category: cs.CV

TL;DR: WiseLVAM is a fully automated framework for left ventricular linear measurements that combines B-mode structure awareness with AMM motion awareness, eliminating the need for manual scanline placement while maintaining clinical guideline compliance.


<details>
  <summary>Details</summary>
Motivation: Existing automated methods for LV measurements are unreliable due to small landmark prediction errors along LV walls, and semi-automatic methods require clinician-defined scanlines, limiting full automation.

Method: Uses weakly supervised B-mode landmark detection to estimate LV contour, then automatically places scanline by inferring LV long axis and basal level. Builds on AMM image training to predict landmarks along the automated scanline.

Result: Enables fully automated yet manually adaptable LV linear measurements that mimic clinical guidelines, combining structure awareness from B-mode with motion awareness from AMM mode.

Conclusion: WiseLVAM provides a robust, accurate, and practical solution for routine clinical application of automated LV measurements by eliminating manual intervention while maintaining clinical reliability.

Abstract: Clinical guidelines recommend performing left ventricular (LV) linear
measurements in B-mode echocardiographic images at the basal level -- typically
at the mitral valve leaflet tips -- and aligned perpendicular to the LV long
axis along a virtual scanline (SL). However, most automated methods estimate
landmarks directly from B-mode images for the measurement task, where even
small shifts in predicted points along the LV walls can lead to significant
measurement errors, reducing their clinical reliability. A recent
semi-automatic method, EnLVAM, addresses this limitation by constraining
landmark prediction to a clinician-defined SL and training on generated
Anatomical Motion Mode (AMM) images to predict LV landmarks along the same. To
enable full automation, a contour-aware SL placement approach is proposed in
this work, in which the LV contour is estimated using a weakly supervised
B-mode landmark detector. SL placement is then performed by inferring the LV
long axis and the basal level-mimicking clinical guidelines. Building on this
foundation, we introduce \textit{WiseLVAM} -- a novel, fully automated yet
manually adaptable framework for automatically placing the SL and then
automatically performing the LV linear measurements in the AMM mode.
\textit{WiseLVAM} utilizes the structure-awareness from B-mode images and the
motion-awareness from AMM mode to enhance robustness and accuracy with the
potential to provide a practical solution for the routine clinical application.

</details>


### [32] [Q-FSRU: Quantum-Augmented Frequency-Spectral Fusion for Medical Visual Question Answering](https://arxiv.org/abs/2508.12036)
*Rakesh Thakur,Yusra Tariq*

Main category: cs.CV

TL;DR: Q-FSRU combines frequency domain processing with quantum-inspired retrieval for medical VQA, achieving superior performance on complex clinical questions requiring image-text reasoning.


<details>
  <summary>Details</summary>
Motivation: Solving tough clinical questions that require both image and text understanding remains a major challenge in healthcare AI, needing improved accuracy and explainability.

Method: Combines Frequency Spectrum Representation and Fusion (FSRU) with Quantum Retrieval-Augmented Generation; uses FFT to shift features to frequency domain, then adds quantum-inspired retrieval system to fetch medical facts from external sources using quantum-based similarity techniques.

Result: Outperforms earlier models on VQA-RAD dataset, especially on complex cases needing image-text reasoning; improves both performance and explainability.

Conclusion: Offers a promising way to build smart, clear, and helpful AI tools for doctors through the combination of frequency and quantum information processing.

Abstract: Solving tough clinical questions that require both image and text
understanding is still a major challenge in healthcare AI. In this work, we
propose Q-FSRU, a new model that combines Frequency Spectrum Representation and
Fusion (FSRU) with a method called Quantum Retrieval-Augmented Generation
(Quantum RAG) for medical Visual Question Answering (VQA). The model takes in
features from medical images and related text, then shifts them into the
frequency domain using Fast Fourier Transform (FFT). This helps it focus on
more meaningful data and filter out noise or less useful information. To
improve accuracy and ensure that answers are based on real knowledge, we add a
quantum-inspired retrieval system. It fetches useful medical facts from
external sources using quantum-based similarity techniques. These details are
then merged with the frequency-based features for stronger reasoning. We
evaluated our model using the VQA-RAD dataset, which includes real radiology
images and questions. The results showed that Q-FSRU outperforms earlier
models, especially on complex cases needing image-text reasoning. The mix of
frequency and quantum information improves both performance and explainability.
Overall, this approach offers a promising way to build smart, clear, and
helpful AI tools for doctors.

</details>


### [33] [VimoRAG: Video-based Retrieval-augmented 3D Motion Generation for Motion Language Models](https://arxiv.org/abs/2508.12081)
*Haidong Xu,Guangwei Xu,Zhedong Zheng,Xiatian Zhu,Wei Ji,Xiangtai Li,Ruijie Guo,Meishan Zhang,Min zhang,Hao Fei*

Main category: cs.CV

TL;DR: VimoRAG is a video-based retrieval-augmented generation framework that enhances 3D motion generation for motion LLMs by retrieving relevant 2D human motion signals from large-scale video databases, overcoming data limitations and error propagation issues.


<details>
  <summary>Details</summary>
Motivation: Motion large language models face severe out-of-domain/out-of-vocabulary issues due to limited annotated data, requiring a solution to leverage abundant video resources for improved motion generation.

Method: Developed Gemini Motion Video Retriever mechanism and Motion-centric Dual-alignment DPO Trainer to enable effective video retrieval and generation while mitigating error propagation from suboptimal retrieval results.

Result: Experimental results show VimoRAG significantly boosts the performance of motion LLMs constrained to text-only input.

Conclusion: VimoRAG successfully addresses key bottlenecks in video-based motion retrieval-augmented generation, demonstrating substantial improvements in 3D motion generation capabilities for text-only motion LLMs.

Abstract: This paper introduces VimoRAG, a novel video-based retrieval-augmented motion
generation framework for motion large language models (LLMs). As motion LLMs
face severe out-of-domain/out-of-vocabulary issues due to limited annotated
data, VimoRAG leverages large-scale in-the-wild video databases to enhance 3D
motion generation by retrieving relevant 2D human motion signals. While
video-based motion RAG is nontrivial, we address two key bottlenecks: (1)
developing an effective motion-centered video retrieval model that
distinguishes human poses and actions, and (2) mitigating the issue of error
propagation caused by suboptimal retrieval results. We design the Gemini Motion
Video Retriever mechanism and the Motion-centric Dual-alignment DPO Trainer,
enabling effective retrieval and generation processes. Experimental results
show that VimoRAG significantly boosts the performance of motion LLMs
constrained to text-only input.

</details>


### [34] [Automated Model Evaluation for Object Detection via Prediction Consistency and Reliablity](https://arxiv.org/abs/2508.12082)
*Seungju Yoo,Hyuk Kwon,Joong-Won Hwang,Kibok Lee*

Main category: cs.CV

TL;DR: AutoEval framework using Prediction Consistency and Reliability (PCR) to estimate object detection performance without ground-truth labels by analyzing spatial consistency and confidence reliability of bounding boxes before/after NMS.


<details>
  <summary>Details</summary>
Motivation: Manual annotation for evaluating object detectors is costly and time-consuming, creating a need for automated performance assessment methods.

Method: Proposes PCR metric that measures spatial consistency between boxes before/after non-maximum suppression and reliability of retained boxes via confidence scores of overlapping boxes. Uses meta-dataset with varying image corruptions for realistic evaluation.

Result: PCR provides more accurate performance estimates than existing AutoEval methods, and the constructed meta-dataset covers a wider range of detection performance scenarios.

Conclusion: The AutoEval framework with PCR metric enables efficient and effective automated evaluation of object detectors without requiring ground-truth annotations, making performance assessment more scalable.

Abstract: Recent advances in computer vision have made training object detectors more
efficient and effective; however, assessing their performance in real-world
applications still relies on costly manual annotation. To address this
limitation, we develop an automated model evaluation (AutoEval) framework for
object detection. We propose Prediction Consistency and Reliability (PCR),
which leverages the multiple candidate bounding boxes that conventional
detectors generate before non-maximum suppression (NMS). PCR estimates
detection performance without ground-truth labels by jointly measuring 1) the
spatial consistency between boxes before and after NMS, and 2) the reliability
of the retained boxes via the confidence scores of overlapping boxes. For a
more realistic and scalable evaluation, we construct a meta-dataset by applying
image corruptions of varying severity. Experimental results demonstrate that
PCR yields more accurate performance estimates than existing AutoEval methods,
and the proposed meta-dataset covers a wider range of detection performance.
The code is available at https://github.com/YonseiML/autoeval-det.

</details>


### [35] [Generic Event Boundary Detection via Denoising Diffusion](https://arxiv.org/abs/2508.12084)
*Jaejun Hwang,Dayoung Gong,Manjin Kim,Minsu Cho*

Main category: cs.CV

TL;DR: DiffGEBD is a diffusion-based model for generic event boundary detection that generates diverse plausible event boundaries rather than deterministic predictions, using temporal self-similarity encoding and classifier-free guidance.


<details>
  <summary>Details</summary>
Motivation: Previous GEBD methods focused on deterministic predictions but overlooked the inherent subjectivity and diversity of plausible event boundaries in videos.

Method: A diffusion-based model that encodes frame changes via temporal self-similarity, then iteratively decodes random noise into event boundaries using classifier-free guidance to control diversity.

Result: Achieves strong performance on Kinetics-GEBD and TAPOS benchmarks, generating diverse and plausible event boundaries.

Conclusion: The generative diffusion approach effectively addresses the subjectivity problem in GEBD and produces high-quality diverse boundary predictions.

Abstract: Generic event boundary detection (GEBD) aims to identify natural boundaries
in a video, segmenting it into distinct and meaningful chunks. Despite the
inherent subjectivity of event boundaries, previous methods have focused on
deterministic predictions, overlooking the diversity of plausible solutions. In
this paper, we introduce a novel diffusion-based boundary detection model,
dubbed DiffGEBD, that tackles the problem of GEBD from a generative
perspective. The proposed model encodes relevant changes across adjacent frames
via temporal self-similarity and then iteratively decodes random noise into
plausible event boundaries being conditioned on the encoded features.
Classifier-free guidance allows the degree of diversity to be controlled in
denoising diffusion. In addition, we introduce a new evaluation metric to
assess the quality of predictions considering both diversity and fidelity.
Experiments show that our method achieves strong performance on two standard
benchmarks, Kinetics-GEBD and TAPOS, generating diverse and plausible event
boundaries.

</details>


### [36] [Enhancing 3D point accuracy of laser scanner through multi-stage convolutional neural network for applications in construction](https://arxiv.org/abs/2508.12089)
*Qinyuan Fan,Clemens Gühmann*

Main category: cs.CV

TL;DR: MSCNN method improves 3D laser scanner accuracy in rough indoor environments by pairing high/low-end scanners and using neural networks to correct systematic errors, achieving 70%+ MSE reduction.


<details>
  <summary>Details</summary>
Motivation: High-end and low-end laser scanners have positional errors due to equipment limitations and environmental factors, creating uncertainty in 3D point accuracy for geometric modeling in rough indoor rooms.

Method: Pairs high-accuracy scanners as references with low-accuracy scanners in identical environments, establishes statistical relationships between measurement discrepancies and spatial distribution, combines geometric processing with neural network refinement, and transforms systematic error quantification into supervised learning.

Result: Significant accuracy improvements with MSE reductions exceeding 70% and PSNR improvements of approximately 6 decibels in rough indoor rooms dataset.

Conclusion: Enables low-end laser scanners to achieve measurement uncertainty levels approaching high-end devices without hardware modifications, providing accurate spatial measurements for high-precision geometric modeling.

Abstract: We propose a multi-stage convolutional neural network (MSCNN) based
integrated method for reducing uncertainty of 3D point accuracy of lasar
scanner (LS) in rough indoor rooms, providing more accurate spatial
measurements for high-precision geometric model creation and renovation. Due to
different equipment limitations and environmental factors, high-end and low-end
LS have positional errors. Our approach pairs high-accuracy scanners (HAS) as
references with corresponding low-accuracy scanners (LAS) of measurements in
identical environments to quantify specific error patterns. By establishing a
statistical relationship between measurement discrepancies and their spatial
distribution, we develop a correction framework that combines traditional
geometric processing with targeted neural network refinement. This method
transforms the quantification of systematic errors into a supervised learning
problem, allowing precise correction while preserving critical geometric
features. Experimental results in our rough indoor rooms dataset show
significant improvements in measurement accuracy, with mean square error (MSE)
reductions exceeding 70% and peak signal-to-noise ratio (PSNR) improvements of
approximately 6 decibels. This approach enables low-end devices to achieve
measurement uncertainty levels approaching those of high-end devices without
hardware modifications.

</details>


### [37] [Error Propagation Mechanisms and Compensation Strategies for Quantized Diffusion](https://arxiv.org/abs/2508.12094)
*Songwei Liu,Hong Liu,Fangmin Chen,Xurui Peng,Chenqian Yan,Lean Fu,Xing Mei*

Main category: cs.CV

TL;DR: A theoretical framework for analyzing quantization error propagation in diffusion models with a timestep-aware compensation scheme that enhances post-training quantization performance.


<details>
  <summary>Details</summary>
Motivation: Diffusion models face deployment challenges due to computationally intensive iterative denoising, and post-training quantization suffers from stepwise error accumulation that compromises output fidelity.

Method: Developed a theoretical framework that mathematically formulates error propagation, derived per-step quantization error propagation equations, established closed-form solution for cumulative error, and proposed timestep-aware cumulative error compensation scheme.

Result: Extensive experiments across multiple image datasets show the compensation strategy effectively mitigates error propagation and significantly enhances existing PTQ methods to achieve state-of-the-art performance on low-precision diffusion models.

Conclusion: The proposed theoretical framework and compensation scheme successfully address quantization error accumulation in diffusion models, enabling more efficient deployment while maintaining output quality.

Abstract: Diffusion models have transformed image synthesis by establishing
unprecedented quality and creativity benchmarks. Nevertheless, their
large-scale deployment faces challenges due to computationally intensive
iterative denoising processes. Although post-training quantization(PTQ)
provides an effective pathway for accelerating sampling, the iterative nature
of diffusion models causes stepwise quantization errors to accumulate
progressively during generation, inevitably compromising output fidelity. To
address this challenge, we develop a theoretical framework that mathematically
formulates error propagation in Diffusion Models (DMs), deriving per-step
quantization error propagation equations and establishing the first closed-form
solution for cumulative error. Building on this theoretical foundation, we
propose a timestep-aware cumulative error compensation scheme. Extensive
experiments across multiple image datasets demonstrate that our compensation
strategy effectively mitigates error propagation, significantly enhancing
existing PTQ methods to achieve state-of-the-art(SOTA) performance on
low-precision diffusion models.

</details>


### [38] [VELVET-Med: Vision and Efficient Language Pre-training for Volumetric Imaging Tasks in Medicine](https://arxiv.org/abs/2508.12108)
*Ziyang Zhang,Yang Yu,Xulei Yang,Si Yong Yeo*

Main category: cs.CV

TL;DR: VELVET-Med is a novel vision-language pre-training framework for 3D medical imaging that achieves state-of-the-art performance with limited data (38,875 scan-report pairs) through innovative self-supervised learning, a TriBERT language encoder, and hierarchical contrastive learning.


<details>
  <summary>Details</summary>
Motivation: Medical VLMs face challenges with 3D volumetric data like CT scans due to the difficulty and time-intensive process of curating large-scale paired data, which limits downstream task performance.

Method: Proposes VELVET-Med framework with: 1) uni-modal self-supervised learning integration, 2) TriBERT language encoder for multi-level textual semantics, and 3) hierarchical contrastive learning for multi-level vision-language correspondence.

Result: Achieves state-of-the-art performance across multiple downstream tasks including 3D segmentation, cross-modal retrieval, visual question answering, and report generation using only 38,875 scan-report pairs.

Conclusion: The framework successfully uncovers rich spatial and semantic relationships in volumetric medical images and clinical narratives, demonstrating strong transferability and generalization ability with limited data.

Abstract: Vision-and-language models (VLMs) have been increasingly explored in the
medical domain, particularly following the success of CLIP in general domain.
However, unlike the relatively straightforward pairing of 2D images and text,
curating large-scale paired data in the medical field for volumetric modalities
such as CT scans remains a challenging and time-intensive process. This
difficulty often limits the performance on downstream tasks. To address these
challenges, we propose a novel vision-language pre-training (VLP) framework,
termed as \textbf{VELVET-Med}, specifically designed for limited volumetric
data such as 3D CT and associated radiology reports. Instead of relying on
large-scale data collection, our method focuses on the development of effective
pre-training objectives and model architectures. The key contributions are: 1)
We incorporate uni-modal self-supervised learning into VLP framework, which are
often underexplored in the existing literature. 2) We propose a novel language
encoder, termed as \textbf{TriBERT}, for learning multi-level textual
semantics. 3) We devise the hierarchical contrastive learning to capture
multi-level vision-language correspondence. Using only 38,875 scan-report
pairs, our approach seeks to uncover rich spatial and semantic relationships
embedded in volumetric medical images and corresponding clinical narratives,
thereby enhancing the generalization ability of the learned encoders. The
resulting encoders exhibit strong transferability, achieving state-of-the-art
performance across a wide range of downstream tasks, including 3D segmentation,
cross-modal retrieval, visual question answering, and report generation.

</details>


### [39] [Simple o3: Towards Interleaved Vision-Language Reasoning](https://arxiv.org/abs/2508.12109)
*Ye Wang,Qianglong Chen,Zejun Li,Siyuan Wang,Shijie Guo,Zhirui Zhang,Zhongyu Wei*

Main category: cs.CV

TL;DR: Simple o3 is an end-to-end MLLM framework that integrates dynamic visual tools (cropping, zooming, reusing) into interleaved vision-language reasoning via supervised fine-tuning, outperforming existing approaches on multimodal benchmarks.


<details>
  <summary>Details</summary>
Motivation: Multimodal LLMs show strong performance but their long Chain-of-Thought capabilities in multimodal scenarios remain underexplored, particularly the ability to emulate human-like "thinking with image" through iterative visual transformations.

Method: Proposes Simple o3 framework with scalable data synthesis pipeline generating high-quality interleaved vision-language reasoning chains via "observe-reason-act" cycle, creating TWI-Tools-146K dataset with executable visual operations and rigorous verification.

Result: Superior performance on diverse benchmarks, outperforming existing approaches. Found that reusing and magnifying original images improves visual reasoning, while image cropping based on precise visual grounding enhances focus on key entities.

Conclusion: Simple o3 establishes a computationally affordable paradigm for advancing multimodal reasoning, providing first in-depth analysis of different interleaved reasoning strategies and their impact on model performance.

Abstract: Multimodal Large Language Models (MLLMs) have shown impressive performance on
vision-language tasks, but their long Chain-of-Thought (CoT) capabilities in
multimodal scenarios remain underexplored. Inspired by OpenAI's o3 model, which
emulates human-like ''thinking with image'' through iterative visual
transformations and linguistic reasoning, we propose Simple o3, an end-to-end
framework that integrates dynamic tool interactions (e.g., cropping, zooming,
and reusing) into interleaved vision-language reasoning via supervised
fine-tuning (SFT). Our approach features a scalable data synthesis pipeline
that generates high-quality interleaved vision-language reasoning chains via an
''observe-reason-act'' cycle, complete with executable visual operations and
rigorous verification, yielding the open-source TWI-Tools-146K dataset.
Experimental results demonstrate Simple o3's superior performance on diverse
benchmarks, outperforming existing approaches. By combining enhanced reasoning
capabilities, Simple o3 establishes a powerful yet computationally affordable
paradigm for advancing multimodal reasoning. Remarkably, we provide the first
in-depth analysis of different interleaved reasoning strategies, offering
insights into their impact on model performance. We found that by introducing
additional visual tokens for interleaved vision-language reasoning, reusing and
magnifying the original image significantly improves the model's visual
reasoning and fine-grained perception, while image cropping based on precise
visual grounding allows the model to effectively focus on key entities or
regions, further enhancing its capabilities.

</details>


### [40] [DualFit: A Two-Stage Virtual Try-On via Warping and Synthesis](https://arxiv.org/abs/2508.12131)
*Minh Tran,Johnmark Clements,Annie Prasanna,Tri Nguyen,Ngan Le*

Main category: cs.CV

TL;DR: DualFit is a two-stage virtual try-on system that combines warping for detail preservation with diffusion-based synthesis for seamless results, addressing the limitation of previous methods in maintaining fine garment details like logos and text.


<details>
  <summary>Details</summary>
Motivation: Current diffusion-based virtual try-on methods fail to preserve critical fine-grained garment details such as logos and printed text, which are essential for brand integrity and customer trust in fashion retail.

Method: Two-stage hybrid approach: 1) Warp target garment using learned flow field for high-fidelity detail preservation, 2) Fidelity-preserving try-on module that blends warped garment with preserved human regions using preserved-region input and inpainting mask to regenerate only necessary areas around seams.

Result: Extensive qualitative results show visually seamless try-on outcomes while faithfully maintaining high-frequency garment details, achieving effective balance between reconstruction accuracy and perceptual realism.

Conclusion: DualFit successfully addresses the detail preservation problem in virtual try-on by combining warping and diffusion approaches, providing both high-fidelity garment representation and seamless visual integration.

Abstract: Virtual Try-On technology has garnered significant attention for its
potential to transform the online fashion retail experience by allowing users
to visualize how garments would look on them without physical trials. While
recent advances in diffusion-based warping-free methods have improved
perceptual quality, they often fail to preserve fine-grained garment details
such as logos and printed text elements that are critical for brand integrity
and customer trust. In this work, we propose DualFit, a hybrid VTON pipeline
that addresses this limitation by two-stage approach. In the first stage,
DualFit warps the target garment to align with the person image using a learned
flow field, ensuring high-fidelity preservation. In the second stage, a
fidelity-preserving try-on module synthesizes the final output by blending the
warped garment with preserved human regions. Particularly, to guide this
process, we introduce a preserved-region input and an inpainting mask, enabling
the model to retain key areas and regenerate only where necessary, particularly
around garment seams. Extensive qualitative results show that DualFit achieves
visually seamless try-on results while faithfully maintaining high-frequency
garment details, striking an effective balance between reconstruction accuracy
and perceptual realism.

</details>


### [41] [TriQDef: Disrupting Semantic and Gradient Alignment to Prevent Adversarial Patch Transferability in Quantized Neural Networks](https://arxiv.org/abs/2508.12132)
*Amira Guesmi,Bassem Ouni,Muhammad Shafique*

Main category: cs.CV

TL;DR: TriQDef is a tri-level quantization-aware defense framework that protects quantized neural networks against transferable patch-based adversarial attacks across different bit-widths by disrupting semantic and gradient alignment.


<details>
  <summary>Details</summary>
Motivation: Quantized Neural Networks (QNNs) provide limited robustness against patch-based adversarial attacks that remain transferable across different quantization bit-widths, and existing defenses either overfit to fixed quantization settings or fail to address this cross-bit generalization vulnerability.

Method: TriQDef consists of three components: (1) Feature Disalignment Penalty (FDP) that enforces semantic inconsistency by penalizing perceptual similarity in intermediate representations; (2) Gradient Perceptual Dissonance Penalty (GPDP) that misaligns input gradients across bit-widths using Edge IoU and HOG Cosine metrics; (3) Joint Quantization-Aware Training Protocol that unifies these penalties in a shared-weight training scheme across multiple quantization levels.

Result: Extensive experiments on CIFAR-10 and ImageNet show that TriQDef reduces Attack Success Rates (ASR) by over 40% on unseen patch and quantization combinations while preserving high clean accuracy.

Conclusion: The findings highlight the importance of disrupting both semantic and perceptual gradient alignment to mitigate patch transferability in Quantized Neural Networks.

Abstract: Quantized Neural Networks (QNNs) are increasingly deployed in edge and
resource-constrained environments due to their efficiency in computation and
memory usage. While shown to distort the gradient landscape and weaken
conventional pixel-level attacks, it provides limited robustness against
patch-based adversarial attacks-localized, high-saliency perturbations that
remain surprisingly transferable across bit-widths. Existing defenses either
overfit to fixed quantization settings or fail to address this cross-bit
generalization vulnerability. We introduce \textbf{TriQDef}, a tri-level
quantization-aware defense framework designed to disrupt the transferability of
patch-based adversarial attacks across QNNs. TriQDef consists of: (1) a Feature
Disalignment Penalty (FDP) that enforces semantic inconsistency by penalizing
perceptual similarity in intermediate representations; (2) a Gradient
Perceptual Dissonance Penalty (GPDP) that explicitly misaligns input gradients
across bit-widths by minimizing structural and directional agreement via Edge
IoU and HOG Cosine metrics; and (3) a Joint Quantization-Aware Training
Protocol that unifies these penalties within a shared-weight training scheme
across multiple quantization levels. Extensive experiments on CIFAR-10 and
ImageNet demonstrate that TriQDef reduces Attack Success Rates (ASR) by over
40\% on unseen patch and quantization combinations, while preserving high clean
accuracy. Our findings underscore the importance of disrupting both semantic
and perceptual gradient alignment to mitigate patch transferability in QNNs.

</details>


### [42] [Infusing fine-grained visual knowledge to Vision-Language Models](https://arxiv.org/abs/2508.12137)
*Nikolaos-Antonios Ypsilantis,Kaifeng Chen,André Araujo,Ondřej Chum*

Main category: cs.CV

TL;DR: A fine-tuning method for Vision-and-Language Models that balances domain adaptation with retention of multimodal knowledge, preventing catastrophic forgetting while achieving strong retrieval performance.


<details>
  <summary>Details</summary>
Motivation: Standard fine-tuning of VLMs for domain-specific retrieval causes catastrophic forgetting, diminishing their general-purpose capabilities. There's a need for methods that preserve pretrained knowledge while adapting to fine-grained tasks.

Method: Proposes a fine-tuning approach inspired by continual learning, combining regularization techniques for knowledge retention. Includes careful validation set design and hyperparameter tuning strategies to ensure reproducibility across datasets.

Result: Achieves strong performance on both fine-grained and coarse-grained image-image and image-text retrieval benchmarks. Notably retains visual-text alignment without using text data or the original text encoder during fine-tuning.

Conclusion: The method effectively balances domain adaptation with knowledge retention, providing a robust solution for fine-tuning VLMs without catastrophic forgetting while maintaining their multimodal capabilities.

Abstract: Large-scale contrastive pre-training produces powerful Vision-and-Language
Models (VLMs) capable of generating representations (embeddings) effective for
a wide variety of visual and multimodal tasks. However, these pretrained
embeddings remain suboptimal for fine-grained open-set visual retrieval, where
state-of-the-art results require fine-tuning the vision encoder using annotated
domain-specific samples. Naively performing such fine-tuning typically leads to
catastrophic forgetting, severely diminishing the model's general-purpose
visual and cross-modal capabilities.
  In this work, we propose a fine-tuning method explicitly designed to achieve
optimal balance between fine-grained domain adaptation and retention of the
pretrained VLM's broad multimodal knowledge. Drawing inspiration from continual
learning literature, we systematically analyze standard regularization
techniques aimed at knowledge retention and propose an efficient and effective
combination strategy. Additionally, we address the commonly overlooked yet
critical aspects of validation set design and hyperparameter tuning to ensure
reproducibility and robust generalization across datasets and pretrained
models. We extensively evaluate our method on both fine-grained and
coarse-grained image-image and image-text retrieval benchmarks. Our approach
consistently achieves strong results, notably retaining the visual-text
alignment without utilizing any text data or the original text encoder during
fine-tuning. Code and model checkpoints: https://github.com/nikosips/infusing .

</details>


### [43] [KP-INR: A Dual-Branch Implicit Neural Representation Model for Cardiac Cine MRI Reconstruction](https://arxiv.org/abs/2508.12147)
*Donghang Lyu,Marius Staring,Mariya Doneva,Hildo J. Lamb,Nicola Pezzotti*

Main category: cs.CV

TL;DR: KP-INR is a dual-branch implicit neural representation method for cardiac cine MRI reconstruction that combines positional embeddings with local multi-scale k-space feature representations to achieve improved performance over existing methods.


<details>
  <summary>Details</summary>
Motivation: Current INR methods for cardiac cine MRI reconstruction focus only on coordinate-based positional embeddings while ignoring the feature representations of target points and their neighboring context, limiting reconstruction quality.

Method: Proposed KP-INR with dual branches: one processes positional embeddings of k-space coordinates, the other learns from local multi-scale k-space feature representations at those coordinates, with cross-branch interaction to approximate target k-space values.

Result: Experiments on CMRxRecon2024 dataset show improved performance over baseline models, demonstrating strong performance on challenging Cartesian k-space data.

Conclusion: KP-INR shows potential for cardiac cine MRI reconstruction by effectively combining positional and feature information, offering a promising approach in this field.

Abstract: Cardiac Magnetic Resonance (CMR) imaging is a non-invasive method for
assessing cardiac structure, function, and blood flow. Cine MRI extends this by
capturing heart motion, providing detailed insights into cardiac mechanics. To
reduce scan time and breath-hold discomfort, fast acquisition techniques have
been utilized at the cost of lowering image quality. Recently, Implicit Neural
Representation (INR) methods have shown promise in unsupervised reconstruction
by learning coordinate-to-value mappings from undersampled data, enabling
high-quality image recovery. However, current existing INR methods primarily
focus on using coordinate-based positional embeddings to learn the mapping,
while overlooking the feature representations of the target point and its
neighboring context. In this work, we propose KP-INR, a dual-branch INR method
operating in k-space for cardiac cine MRI reconstruction: one branch processes
the positional embedding of k-space coordinates, while the other learns from
local multi-scale k-space feature representations at those coordinates. By
enabling cross-branch interaction and approximating the target k-space values
from both branches, KP-INR can achieve strong performance on challenging
Cartesian k-space data. Experiments on the CMRxRecon2024 dataset confirms its
improved performance over baseline models and highlights its potential in this
field.

</details>


### [44] [Demystifying Foreground-Background Memorization in Diffusion Models](https://arxiv.org/abs/2508.12148)
*Jimmy Z. Di,Yiwei Lu,Yaoliang Yu,Gautam Kamath,Adam Dziedzic,Franziska Boenisch*

Main category: cs.CV

TL;DR: FB-Mem is a novel segmentation-based metric that quantifies memorized regions in diffusion model outputs, revealing pervasive memorization patterns and showing current mitigation methods fail to eliminate local memorization.


<details>
  <summary>Details</summary>
Motivation: Current detection methods only identify verbatim memorization but fail to capture partial memorization in small image regions and complex memorization patterns beyond specific prompt-image pairs.

Method: Proposed Foreground Background Memorization (FB-Mem), a segmentation-based metric that classifies and quantifies memorized regions within generated images using a clustering approach.

Result: Reveals memorization is more pervasive than previously understood: individual generations link to clusters of training images, and existing mitigation methods fail to eliminate local memorization, especially in foreground regions.

Conclusion: Establishes an effective framework for measuring memorization in diffusion models, demonstrates inadequacy of current mitigation approaches, and proposes a stronger mitigation method using clustering.

Abstract: Diffusion models (DMs) memorize training images and can reproduce
near-duplicates during generation. Current detection methods identify verbatim
memorization but fail to capture two critical aspects: quantifying partial
memorization occurring in small image regions, and memorization patterns beyond
specific prompt-image pairs. To address these limitations, we propose
Foreground Background Memorization (FB-Mem), a novel segmentation-based metric
that classifies and quantifies memorized regions within generated images. Our
method reveals that memorization is more pervasive than previously understood:
(1) individual generations from single prompts may be linked to clusters of
similar training images, revealing complex memorization patterns that extend
beyond one-to-one correspondences; and (2) existing model-level mitigation
methods, such as neuron deactivation and pruning, fail to eliminate local
memorization, which persists particularly in foreground regions. Our work
establishes an effective framework for measuring memorization in diffusion
models, demonstrates the inadequacy of current mitigation approaches, and
proposes a stronger mitigation method using a clustering approach.

</details>


### [45] [RealTalk: Realistic Emotion-Aware Lifelike Talking-Head Synthesis](https://arxiv.org/abs/2508.12163)
*Wenqing Wang,Yun Fu*

Main category: cs.CV

TL;DR: RealTalk is a novel framework that generates emotional talking heads with high emotion accuracy, enhanced controllability, and robust identity preservation using VAE-generated landmarks and tri-plane attention NeRF.


<details>
  <summary>Details</summary>
Motivation: Current talking head generation methods excel at lip synchronization and image quality but fail to produce accurate and controllable emotional expressions while preserving subject identity, which is critical for artificial social intelligence.

Method: Uses variational autoencoder (VAE) to generate 3D facial landmarks from audio, concatenates with emotion-label embeddings via ResNet-based landmark deformation model, and conditions a novel tri-plane attention Neural Radiance Field (NeRF) with landmarks and facial blendshape coefficients.

Result: Extensive experiments demonstrate RealTalk outperforms existing methods in emotion accuracy, controllability, and identity preservation.

Conclusion: RealTalk advances the development of socially intelligent AI systems by enabling high-quality emotional talking head synthesis with precise control and identity preservation.

Abstract: Emotion is a critical component of artificial social intelligence. However,
while current methods excel in lip synchronization and image quality, they
often fail to generate accurate and controllable emotional expressions while
preserving the subject's identity. To address this challenge, we introduce
RealTalk, a novel framework for synthesizing emotional talking heads with high
emotion accuracy, enhanced emotion controllability, and robust identity
preservation. RealTalk employs a variational autoencoder (VAE) to generate 3D
facial landmarks from driving audio, which are concatenated with emotion-label
embeddings using a ResNet-based landmark deformation model (LDM) to produce
emotional landmarks. These landmarks and facial blendshape coefficients jointly
condition a novel tri-plane attention Neural Radiance Field (NeRF) to
synthesize highly realistic emotional talking heads. Extensive experiments
demonstrate that RealTalk outperforms existing methods in emotion accuracy,
controllability, and identity preservation, advancing the development of
socially intelligent AI systems.

</details>


### [46] [Scalable RF Simulation in Generative 4D Worlds](https://arxiv.org/abs/2508.12176)
*Zhiwei Zheng,Dongyin Hu,Mingmin Zhao*

Main category: cs.CV

TL;DR: WaveVerse is a prompt-based framework that simulates realistic RF signals from generated indoor scenes with human motions, enabling scalable RF data generation for perception tasks.


<details>
  <summary>Details</summary>
Motivation: Collecting high-quality RF data in dynamic indoor environments is challenging, and there's a need for privacy-preserving alternatives to vision-based methods.

Method: Uses language-guided 4D world generator with state-aware causal transformer for human motion generation, and phase-coherent ray tracing simulator for accurate RF signal simulation.

Result: Effective conditioned human motion generation, enables RF imaging data generation for the first time, and achieves performance gains in both data-limited and data-adequate scenarios.

Conclusion: WaveVerse provides a scalable solution for generating realistic RF data, demonstrating practical applications in beamforming, respiration monitoring, and activity recognition.

Abstract: Radio Frequency (RF) sensing has emerged as a powerful, privacy-preserving
alternative to vision-based methods for indoor perception tasks. However,
collecting high-quality RF data in dynamic and diverse indoor environments
remains a major challenge. To address this, we introduce WaveVerse, a
prompt-based, scalable framework that simulates realistic RF signals from
generated indoor scenes with human motions. WaveVerse introduces a
language-guided 4D world generator, which includes a state-aware causal
transformer for human motion generation conditioned on spatial constraints and
texts, and a phase-coherent ray tracing simulator that enables the simulation
of accurate and coherent RF signals. Experiments demonstrate the effectiveness
of our approach in conditioned human motion generation and highlight how phase
coherence is applied to beamforming and respiration monitoring. We further
present two case studies in ML-based high-resolution imaging and human activity
recognition, demonstrating that WaveVerse not only enables data generation for
RF imaging for the first time, but also consistently achieves performance gain
in both data-limited and data-adequate scenarios.

</details>


### [47] [Splat Feature Solver](https://arxiv.org/abs/2508.12216)
*Butian Xiong,Rong Liu,Kenneth Xu,Meida Chen,Andrew Feng*

Main category: cs.CV

TL;DR: A unified kernel- and feature-agnostic formulation for feature lifting in 3D scene understanding that solves as a sparse linear inverse problem with provable optimal error bounds and regularization strategies.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of optimally assigning rich image feature descriptors to 3D primitives while handling inconsistency issues from multi-view images in 3D scene understanding.

Method: Formulates feature lifting as a sparse linear inverse problem solved in closed form, with Tikhonov Guidance for numerical stability and Post-Lifting Aggregation for noise filtering through feature clustering.

Result: Achieves state-of-the-art performance on open-vocabulary 3D segmentation benchmarks, outperforming training-based, grouping-based, and heuristic baselines while producing lifted features in minutes.

Conclusion: The approach provides an efficient, provably optimal solution for feature lifting with strong regularization strategies that enhance semantic fidelity and handle multi-view inconsistencies effectively.

Abstract: Feature lifting has emerged as a crucial component in 3D scene understanding,
enabling the attachment of rich image feature descriptors (e.g., DINO, CLIP)
onto splat-based 3D representations. The core challenge lies in optimally
assigning rich general attributes to 3D primitives while addressing the
inconsistency issues from multi-view images. We present a unified, kernel- and
feature-agnostic formulation of the feature lifting problem as a sparse linear
inverse problem, which can be solved efficiently in closed form. Our approach
admits a provable upper bound on the global optimal error under convex losses
for delivering high quality lifted features. To address inconsistencies and
noise in multi-view observations, we introduce two complementary regularization
strategies to stabilize the solution and enhance semantic fidelity. Tikhonov
Guidance enforces numerical stability through soft diagonal dominance, while
Post-Lifting Aggregation filters noisy inputs via feature clustering. Extensive
experiments demonstrate that our approach achieves state-of-the-art performance
on open-vocabulary 3D segmentation benchmarks, outperforming training-based,
grouping-based, and heuristic-forward baselines while producing the lifted
features in minutes. Code is available at
\href{https://github.com/saliteta/splat-distiller.git}{\textbf{github}}. We
also have a \href{https://splat-distiller.pages.dev/}

</details>


### [48] [C2PSA-Enhanced YOLOv11 Architecture: A Novel Approach for Small Target Detection in Cotton Disease Diagnosis](https://arxiv.org/abs/2508.12219)
*Kaiyuan Wang,Jixing Liu,Xiaobo Cai*

Main category: cs.CV

TL;DR: Optimized YOLOv11 for cotton disease detection with improved small-target feature extraction, dynamic category weighting, and enhanced data augmentation, achieving 8-10.5% mAP improvement and 158 FPS inference speed for real-time agricultural monitoring.


<details>
  <summary>Details</summary>
Motivation: Address three key challenges in cotton disease detection: low precision in early spot detection (35% leakage for sub-5mm2 spots), performance degradation in field conditions (25% accuracy drop), and high error rates (34.7%) in multi-disease scenarios.

Method: Proposed C2PSA module for enhanced small-target feature extraction, dynamic category weighting to handle sample imbalance, and improved data augmentation via Mosaic-MixUp scaling on YOLOv11 architecture.

Result: Experimental results on 4,078-image dataset show: mAP50: 0.820 (+8.0% improvement); mAP50-95: 0.705 (+10.5% improvement); Inference speed: 158 FPS. Mobile-deployed system enables real-time monitoring.

Conclusion: The optimized YOLOv11 system successfully addresses cotton disease detection challenges and provides an effective solution for real-time agricultural disease monitoring and precision treatment applications.

Abstract: This study presents a deep learning-based optimization of YOLOv11 for cotton
disease detection, developing an intelligent monitoring system. Three key
challenges are addressed: (1) low precision in early spot detection (35%
leakage rate for sub-5mm2 spots), (2) performance degradation in field
conditions (25% accuracy drop), and (3) high error rates (34.7%) in
multi-disease scenarios. The proposed solutions include: C2PSA module for
enhanced small-target feature extraction; Dynamic category weighting to handle
sample imbalance; Improved data augmentation via Mosaic-MixUp scaling.
Experimental results on a 4,078-image dataset show: mAP50: 0.820 (+8.0%
improvement); mAP50-95: 0.705 (+10.5% improvement); Inference speed: 158 FPS.
The mobile-deployed system enables real-time disease monitoring and precision
treatment in agricultural applications.

</details>


### [49] [In vivo 3D ultrasound computed tomography of musculoskeletal tissues with generative neural physics](https://arxiv.org/abs/2508.12226)
*Zhijun Zeng,Youjia Zheng,Chang Su,Qianhang Wu,Hao Hu,Zeyuan Dong,Shan Gao,Yang Lv,Rui Tang,Ligang Cui,Zhiyong Hou,Weijun Lin,Zuoqiang Shi,Yubing Li,He Sun*

Main category: cs.CV

TL;DR: A generative neural physics framework that combines deep learning with wave physics simulation for fast, high-fidelity 3D ultrasound computed tomography, enabling quantitative imaging of musculoskeletal tissues with resolution comparable to MRI.


<details>
  <summary>Details</summary>
Motivation: Ultrasound computed tomography (USCT) is limited for musculoskeletal imaging due to conventional ray-based reconstructions that neglect strong scattering effects, preventing accurate quantitative imaging of tissues.

Method: Proposes a generative neural physics framework that couples generative networks with physics-informed neural simulation, learning a compact surrogate of ultrasonic wave propagation from only dozens of cross-modality images.

Result: Reconstructs 3D maps of tissue parameters in under ten minutes on synthetic and in vivo data (breast, arm, leg), with sensitivity to biomechanical properties in muscle and bone and resolution comparable to MRI.

Conclusion: This approach overcomes computational bottlenecks in strongly scattering regimes and advances USCT toward routine clinical assessment of musculoskeletal disease by merging wave modeling accuracy with deep learning efficiency.

Abstract: Ultrasound computed tomography (USCT) is a radiation-free, high-resolution
modality but remains limited for musculoskeletal imaging due to conventional
ray-based reconstructions that neglect strong scattering. We propose a
generative neural physics framework that couples generative networks with
physics-informed neural simulation for fast, high-fidelity 3D USCT. By learning
a compact surrogate of ultrasonic wave propagation from only dozens of
cross-modality images, our method merges the accuracy of wave modeling with the
efficiency and stability of deep learning. This enables accurate quantitative
imaging of in vivo musculoskeletal tissues, producing spatial maps of acoustic
properties beyond reflection-mode images. On synthetic and in vivo data
(breast, arm, leg), we reconstruct 3D maps of tissue parameters in under ten
minutes, with sensitivity to biomechanical properties in muscle and bone and
resolution comparable to MRI. By overcoming computational bottlenecks in
strongly scattering regimes, this approach advances USCT toward routine
clinical assessment of musculoskeletal disease.

</details>


### [50] [WXSOD: A Benchmark for Robust Salient Object Detection in Adverse Weather Conditions](https://arxiv.org/abs/2508.12250)
*Quan Chen,Xiong Yang,Rongfeng Lu,Qianyu Zhang,Yu Liu,Xiaofei Zhou,Bolun Zheng*

Main category: cs.CV

TL;DR: A new weather-extended salient object detection dataset (WXSOD) with 14,945 RGB images featuring diverse weather noise, plus a novel Weather-aware Feature Aggregation Network (WFANet) that outperforms 17 existing SOD methods.


<details>
  <summary>Details</summary>
Motivation: Address the lack of datasets with pixel-wise annotations for weather noise impact on salient object detection, as existing methods perform poorly in complex weather conditions.

Method: Created WXSOD dataset with synthesized and real weather noise test sets. Proposed WFANet - a two-branch architecture with weather prediction branch and saliency detection branch that fuses semantic and weather features.

Result: WFANet achieves superior performance compared to 17 existing SOD methods on the new WXSOD benchmark dataset.

Conclusion: The WXSOD dataset fills an important gap in weather-affected SOD research, and WFANet provides an effective baseline approach for handling weather noise in salient object detection tasks.

Abstract: Salient object detection (SOD) in complex environments remains a challenging
research topic. Most existing methods perform well in natural scenes with
negligible noise, and tend to leverage multi-modal information (e.g., depth and
infrared) to enhance accuracy. However, few studies are concerned with the
damage of weather noise on SOD performance due to the lack of dataset with
pixel-wise annotations. To bridge this gap, this paper introduces a novel
Weather-eXtended Salient Object Detection (WXSOD) dataset. It consists of
14,945 RGB images with diverse weather noise, along with the corresponding
ground truth annotations and weather labels. To verify algorithm
generalization, WXSOD contains two test sets, i.e., a synthesized test set and
a real test set. The former is generated by adding weather noise to clean
images, while the latter contains real-world weather noise. Based on WXSOD, we
propose an efficient baseline, termed Weather-aware Feature Aggregation Network
(WFANet), which adopts a fully supervised two-branch architecture.
Specifically, the weather prediction branch mines weather-related deep
features, while the saliency detection branch fuses semantic features extracted
from the backbone with weather features for SOD. Comprehensive comparisons
against 17 SOD methods shows that our WFANet achieves superior performance on
WXSOD. The code and benchmark results will be made publicly available at
https://github.com/C-water/WXSOD

</details>


### [51] [Superpixel-informed Continuous Low-Rank Tensor Representation for Multi-Dimensional Data Recovery](https://arxiv.org/abs/2508.12261)
*Zhizhou Wang,Ruijing Zheng,Zhenyu Wu,Jianli Wang*

Main category: cs.CV

TL;DR: SCTR framework overcomes traditional LRTR limitations by using superpixels as modeling units and asymmetric tensor factorization with neural networks, achieving 3-5 dB PSNR improvements across multiple data types.


<details>
  <summary>Details</summary>
Motivation: Classical low-rank tensor representation methods assume holistic data is low-rank and are limited to discrete meshgrid data, which doesn't hold in real-world scenarios with spatial variations.

Method: Superpixel-informed Continuous Tensor Representation (SCTR) using superpixels as basic units and asymmetric low-rank tensor factorization with shared neural network and specialized heads to separate global pattern learning from local adaptation.

Result: Achieves 3-5 dB PSNR improvements over existing LRTR-based methods across multispectral images, videos, and color images on benchmark datasets.

Conclusion: SCTR provides a continuous and flexible modeling framework that captures both cross-superpixel commonalities and within-superpixel variations, balancing expressiveness with compact representation.

Abstract: Low-rank tensor representation (LRTR) has emerged as a powerful tool for
multi-dimensional data processing. However, classical LRTR-based methods face
two critical limitations: (1) they typically assume that the holistic data is
low-rank, this assumption is often violated in real-world scenarios with
significant spatial variations; and (2) they are constrained to discrete
meshgrid data, limiting their flexibility and applicability. To overcome these
limitations, we propose a Superpixel-informed Continuous low-rank Tensor
Representation (SCTR) framework, which enables continuous and flexible modeling
of multi-dimensional data beyond traditional grid-based constraints. Our
approach introduces two main innovations: First, motivated by the observation
that semantically coherent regions exhibit stronger low-rank characteristics
than holistic data, we employ superpixels as the basic modeling units. This
design not only encodes rich semantic information, but also enhances
adaptability to diverse forms of data streams. Second, we propose a novel
asymmetric low-rank tensor factorization (ALTF) where superpixel-specific
factor matrices are parameterized by a shared neural network with specialized
heads. By strategically separating global pattern learning from local
adaptation, this framework efficiently captures both cross-superpixel
commonalities and within-superpixel variations. This yields a representation
that is both highly expressive and compact, balancing model efficiency with
adaptability. Extensive experiments on several benchmark datasets demonstrate
that SCTR achieves 3-5 dB PSNR improvements over existing LRTR-based methods
across multispectral images, videos, and color images.

</details>


### [52] [Region-Level Context-Aware Multimodal Understanding](https://arxiv.org/abs/2508.12263)
*Hongliang Wei,Xianqi Zhang,Xingtao Wang,Xiaopeng Fan,Debin Zhao*

Main category: cs.CV

TL;DR: Proposes Region-level Context-aware Multimodal Understanding (RCMU) to integrate textual context with visual objects, introduces RCVIT training method, RCMU dataset, RC&P-Bench benchmark, and achieves state-of-the-art performance with RC-Qwen2-VL models.


<details>
  <summary>Details</summary>
Motivation: Existing MLLMs focus on general visual understanding but lack ability to integrate textual context associated with objects for context-aware multimodal understanding.

Method: Proposed Region-level Context-aware Visual Instruction Tuning (RCVIT) that incorporates object information and bounding box coordinates to associate visual content with textual information. Created RCMU dataset and RC&P-Bench benchmark for training and evaluation.

Result: RC-Qwen2-VL models achieve outstanding performance on multiple RCMU tasks and demonstrate successful applications in multimodal RAG and personalized conversation.

Conclusion: The proposed RCMU framework effectively addresses the limitation of current MLLMs by enabling region-level context-aware understanding, with strong performance demonstrated across various tasks and applications.

Abstract: Despite significant progress, existing research on Multimodal Large Language
Models (MLLMs) mainly focuses on general visual understanding, overlooking the
ability to integrate textual context associated with objects for a more
context-aware multimodal understanding -- an ability we refer to as
Region-level Context-aware Multimodal Understanding (RCMU). To address this
limitation, we first formulate the RCMU task, which requires models to respond
to user instructions by integrating both image content and textual information
of regions or objects. To equip MLLMs with RCMU capabilities, we propose
Region-level Context-aware Visual Instruction Tuning (RCVIT), which
incorporates object information into the model input and enables the model to
utilize bounding box coordinates to effectively associate objects' visual
content with their textual information. To address the lack of datasets, we
introduce the RCMU dataset, a large-scale visual instruction tuning dataset
that covers multiple RCMU tasks. We also propose RC\&P-Bench, a comprehensive
benchmark that can evaluate the performance of MLLMs in RCMU and multimodal
personalized understanding tasks. Additionally, we propose a reference-free
evaluation metric to perform a comprehensive and fine-grained evaluation of the
region-level context-aware image descriptions. By performing RCVIT on Qwen2-VL
models with the RCMU dataset, we developed RC-Qwen2-VL models. Experimental
results indicate that RC-Qwen2-VL models not only achieve outstanding
performance on multiple RCMU tasks but also demonstrate successful applications
in multimodal RAG and personalized conversation. Our data, model and benchmark
are available at https://github.com/hongliang-wei/RC-MLLM

</details>


### [53] [SNNSIR: A Simple Spiking Neural Network for Stereo Image Restoration](https://arxiv.org/abs/2508.12271)
*Ronghua Xu,Jin Xie,Jing Nie,Jiale Cao,Yanwei Pang*

Main category: cs.CV

TL;DR: SNNSIR is a fully spike-driven Spiking Neural Network for stereo image restoration that achieves competitive performance with significantly reduced computational overhead compared to hybrid SNN-ANN models.


<details>
  <summary>Details</summary>
Motivation: Spiking Neural Networks offer high computational efficiency and low energy consumption but existing hybrid SNN-ANN models still rely on floating-point operations incompatible with SNNs' binary nature. The goal is to create a fully spike-driven architecture for stereo image restoration.

Method: Proposes SNNSIR with three key components: 1) Spike Residual Basic Block (SRBB) for enhanced information flow via spike-compatible residual learning, 2) Spike Stereo Convolutional Modulation (SSCM) module with simplified nonlinearity and cross-view-aware modulation, 3) Spike Stereo Cross-Attention (SSCA) module for efficient bidirectional feature interaction across views.

Result: Extensive experiments on stereo image restoration tasks (rain streak removal, raindrop removal, low-light enhancement, super-resolution) demonstrate competitive restoration performance while significantly reducing computational overhead.

Conclusion: The model shows potential for real-time, low-power stereo vision applications, highlighting the viability of fully spike-driven architectures for computation-intensive tasks.

Abstract: Spiking Neural Networks (SNNs), characterized by discrete binary activations,
offer high computational efficiency and low energy consumption, making them
well-suited for computation-intensive tasks such as stereo image restoration.
In this work, we propose SNNSIR, a simple yet effective Spiking Neural Network
for Stereo Image Restoration, specifically designed under the spike-driven
paradigm where neurons transmit information through sparse, event-based binary
spikes. In contrast to existing hybrid SNN-ANN models that still rely on
operations such as floating-point matrix division or exponentiation, which are
incompatible with the binary and event-driven nature of SNNs, our proposed
SNNSIR adopts a fully spike-driven architecture to achieve low-power and
hardware-friendly computation. To address the expressiveness limitations of
binary spiking neurons, we first introduce a lightweight Spike Residual Basic
Block (SRBB) to enhance information flow via spike-compatible residual
learning. Building on this, the Spike Stereo Convolutional Modulation (SSCM)
module introduces simplified nonlinearity through element-wise multiplication
and highlights noise-sensitive regions via cross-view-aware modulation.
Complementing this, the Spike Stereo Cross-Attention (SSCA) module further
improves stereo correspondence by enabling efficient bidirectional feature
interaction across views within a spike-compatible framework. Extensive
experiments on diverse stereo image restoration tasks, including rain streak
removal, raindrop removal, low-light enhancement, and super-resolution
demonstrate that our model achieves competitive restoration performance while
significantly reducing computational overhead. These results highlight the
potential for real-time, low-power stereo vision applications. The code will be
available after the article is accepted.

</details>


### [54] [TSLA: A Task-Specific Learning Adaptation for Semantic Segmentation on Autonomous Vehicles Platform](https://arxiv.org/abs/2508.12279)
*Jun Liu,Zhenglun Kong,Pu Zhao,Weihao Zeng,Hao Tang,Xuan Shen,Changdi Yang,Wenbin Zhang,Geng Yuan,Wei Niu,Xue Lin,Yanzhi Wang*

Main category: cs.CV

TL;DR: Dynamic semantic segmentation network customization for autonomous driving hardware using three-tier control mechanism and Bayesian optimization to optimize computational efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: Autonomous driving platforms face diverse scenarios with varying hardware resources and precision requirements, requiring efficient deployment on computationally limited embedded devices like NVIDIA DRIVE PX 2.

Method: Three-tier control mechanism (width multiplier, classifier depth, classifier kernel) for fine-grained model control, combined with Bayesian Optimization for hyperparameter search under tight computational budgets.

Result: Enables broad model scaling, targeted refinement of final layers, and scenario-specific optimization, leading to improved resource allocation and performance with tailored configurations for diverse self-driving tasks.

Conclusion: The approach successfully addresses scenario-specific and task-specific requirements through automatic parameter search, maximizing computational capacity and model accuracy while optimizing hardware utilization for autonomous driving applications.

Abstract: Autonomous driving platforms encounter diverse driving scenarios, each with
varying hardware resources and precision requirements. Given the computational
limitations of embedded devices, it is crucial to consider computing costs when
deploying on target platforms like the NVIDIA\textsuperscript{\textregistered}
DRIVE PX 2. Our objective is to customize the semantic segmentation network
according to the computing power and specific scenarios of autonomous driving
hardware. We implement dynamic adaptability through a three-tier control
mechanism -- width multiplier, classifier depth, and classifier kernel --
allowing fine-grained control over model components based on hardware
constraints and task requirements. This adaptability facilitates broad model
scaling, targeted refinement of the final layers, and scenario-specific
optimization of kernel sizes, leading to improved resource allocation and
performance.
  Additionally, we leverage Bayesian Optimization with surrogate modeling to
efficiently explore hyperparameter spaces under tight computational budgets.
Our approach addresses scenario-specific and task-specific requirements through
automatic parameter search, accommodating the unique computational complexity
and accuracy needs of autonomous driving. It scales its Multiply-Accumulate
Operations (MACs) for Task-Specific Learning Adaptation (TSLA), resulting in
alternative configurations tailored to diverse self-driving tasks. These TSLA
customizations maximize computational capacity and model accuracy, optimizing
hardware utilization.

</details>


### [55] [CLAIR: CLIP-Aided Weakly Supervised Zero-Shot Cross-Domain Image Retrieval](https://arxiv.org/abs/2508.12290)
*Chor Boon Tan,Conghui Hu,Gim Hee Lee*

Main category: cs.CV

TL;DR: CLAIR is a novel framework for weakly supervised zero-shot cross-domain image retrieval that refines noisy pseudo-labels from foundation models like CLIP using confidence scores and contrastive learning to handle domain discrepancies and improve retrieval performance.


<details>
  <summary>Details</summary>
Motivation: With large foundation models generating pseudo-labels for unlabeled data, unsupervised zero-shot cross-domain image retrieval becomes less relevant. The paper focuses on weakly supervised approaches using noisy pseudo-labels from models like CLIP.

Method: Proposes CLAIR framework that: 1) refines noisy pseudo-labels using CLIP text-image similarity confidence scores, 2) uses inter-instance and inter-cluster contrastive losses for class-aware encoding, 3) employs inter-domain contrastive loss to reduce domain gaps, 4) learns cross-domain mapping function using CLIP text embeddings, and 5) adds learnable prompts for zero-shot generalization.

Result: Extensive experiments on TUBerlin, Sketchy, Quickdraw, and DomainNet zero-shot datasets show CLAIR consistently outperforms state-of-the-art methods.

Conclusion: CLAIR effectively addresses weakly supervised zero-shot cross-domain image retrieval by leveraging foundation model pseudo-labels while mitigating noise through confidence scoring and contrastive learning techniques.

Abstract: The recent growth of large foundation models that can easily generate
pseudo-labels for huge quantity of unlabeled data makes unsupervised Zero-Shot
Cross-Domain Image Retrieval (UZS-CDIR) less relevant. In this paper, we
therefore turn our attention to weakly supervised ZS-CDIR (WSZS-CDIR) with
noisy pseudo labels generated by large foundation models such as CLIP. To this
end, we propose CLAIR to refine the noisy pseudo-labels with a confidence score
from the similarity between the CLIP text and image features. Furthermore, we
design inter-instance and inter-cluster contrastive losses to encode images
into a class-aware latent space, and an inter-domain contrastive loss to
alleviate domain discrepancies. We also learn a novel cross-domain mapping
function in closed-form, using only CLIP text embeddings to project image
features from one domain to another, thereby further aligning the image
features for retrieval. Finally, we enhance the zero-shot generalization
ability of our CLAIR to handle novel categories by introducing an extra set of
learnable prompts. Extensive experiments are carried out using TUBerlin,
Sketchy, Quickdraw, and DomainNet zero-shot datasets, where our CLAIR
consistently shows superior performance compared to existing state-of-the-art
methods.

</details>


### [56] [Improving Densification in 3D Gaussian Splatting for High-Fidelity Rendering](https://arxiv.org/abs/2508.12313)
*Xiaobin Deng,Changyu Diao,Min Li,Ruohan Yu,Duanqing Xu*

Main category: cs.CV

TL;DR: Improved 3D Gaussian Splatting densification pipeline with edge-aware candidate selection, long-axis splitting strategy, and overfitting mitigation techniques for better reconstruction quality with fewer Gaussians.


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian Splatting's current densification strategy results in suboptimal reconstruction quality, needing improvements in when and how to densify while addressing overfitting issues.

Method: Proposes Edge-Aware Score for candidate selection, Long-Axis Split strategy to reduce geometric distortions, and overfitting mitigation techniques including Recovery-Aware Pruning, Multi-step Update, and Growth Control.

Result: Achieves state-of-the-art performance with enhanced rendering fidelity and fewer Gaussians, without additional training or inference overhead.

Conclusion: The comprehensive improvements to the densification pipeline significantly enhance 3DGS reconstruction quality while maintaining efficiency.

Abstract: Although 3D Gaussian Splatting (3DGS) has achieved impressive performance in
real-time rendering, its densification strategy often results in suboptimal
reconstruction quality. In this work, we present a comprehensive improvement to
the densification pipeline of 3DGS from three perspectives: when to densify,
how to densify, and how to mitigate overfitting. Specifically, we propose an
Edge-Aware Score to effectively select candidate Gaussians for splitting. We
further introduce a Long-Axis Split strategy that reduces geometric distortions
introduced by clone and split operations. To address overfitting, we design a
set of techniques, including Recovery-Aware Pruning, Multi-step Update, and
Growth Control. Our method enhances rendering fidelity without introducing
additional training or inference overhead, achieving state-of-the-art
performance with fewer Gaussians.

</details>


### [57] [Neural Cellular Automata for Weakly Supervised Segmentation of White Blood Cells](https://arxiv.org/abs/2508.12322)
*Michael Deutges,Chen Yang,Raheleh Salehi,Nassir Navab,Carsten Marr,Ario Sadafi*

Main category: cs.CV

TL;DR: NCA-WSS uses neural cellular automata for weakly supervised white blood cell segmentation, eliminating need for segmentation labels by leveraging classification feature maps.


<details>
  <summary>Details</summary>
Motivation: Medical diagnostics require accurate white blood cell detection and segmentation, but obtaining large labeled datasets is time-consuming and expensive.

Method: Propose neural cellular automata for weakly supervised segmentation (NCA-WSS) that extracts segmentation masks from classification feature maps without retraining with segmentation labels.

Result: Outperforms existing weakly supervised approaches on three white blood cell microscopy datasets.

Conclusion: Demonstrates NCA's potential for both classification and segmentation in weakly supervised frameworks, providing scalable and efficient medical image analysis solution.

Abstract: The detection and segmentation of white blood cells in blood smear images is
a key step in medical diagnostics, supporting various downstream tasks such as
automated blood cell counting, morphological analysis, cell classification, and
disease diagnosis and monitoring. Training robust and accurate models requires
large amounts of labeled data, which is both time-consuming and expensive to
acquire. In this work, we propose a novel approach for weakly supervised
segmentation using neural cellular automata (NCA-WSS). By leveraging the
feature maps generated by NCA during classification, we can extract
segmentation masks without the need for retraining with segmentation labels. We
evaluate our method on three white blood cell microscopy datasets and
demonstrate that NCA-WSS significantly outperforms existing weakly supervised
approaches. Our work illustrates the potential of NCA for both classification
and segmentation in a weakly supervised framework, providing a scalable and
efficient solution for medical image analysis.

</details>


### [58] [Attention Pooling Enhances NCA-based Classification of Microscopy Images](https://arxiv.org/abs/2508.12324)
*Chen Yang,Michael Deutges,Jingsong Liu,Han Li,Nassir Navab,Carsten Marr,Ario Sadafi*

Main category: cs.CV

TL;DR: Integrating attention pooling with Neural Cellular Automata improves microscopy image classification accuracy while maintaining parameter efficiency and explainability.


<details>
  <summary>Details</summary>
Motivation: Address the performance gap between Neural Cellular Automata and larger architectures for microscopy image analysis by enhancing feature extraction capabilities.

Method: Integration of attention pooling mechanism with Neural Cellular Automata to refine focus on the most informative regions in microscopy images.

Result: Significantly outperforms existing NCA methods on eight diverse microscopy datasets, shows improved performance compared to traditional lightweight CNNs and vision transformers while maintaining lower parameter count.

Conclusion: Attention-enhanced NCA models demonstrate strong potential as explainable and parameter-efficient alternatives for image classification tasks.

Abstract: Neural Cellular Automata (NCA) offer a robust and interpretable approach to
image classification, making them a promising choice for microscopy image
analysis. However, a performance gap remains between NCA and larger, more
complex architectures. We address this challenge by integrating attention
pooling with NCA to enhance feature extraction and improve classification
accuracy. The attention pooling mechanism refines the focus on the most
informative regions, leading to more accurate predictions. We evaluate our
method on eight diverse microscopy image datasets and demonstrate that our
approach significantly outperforms existing NCA methods while remaining
parameter-efficient and explainable. Furthermore, we compare our method with
traditional lightweight convolutional neural network and vision transformer
architectures, showing improved performance while maintaining a significantly
lower parameter count. Our results highlight the potential of NCA-based models
an alternative for explainable image classification.

</details>


### [59] [DoppDrive: Doppler-Driven Temporal Aggregation for Improved Radar Object Detection](https://arxiv.org/abs/2508.12330)
*Yuval Haitman,Oded Bialer*

Main category: cs.CV

TL;DR: DoppDrive is a Doppler-driven temporal aggregation method that enhances radar point cloud density by eliminating scatter from dynamic objects through radial shifting and adaptive aggregation duration, improving object detection performance.


<details>
  <summary>Details</summary>
Motivation: Radar's long detection range is essential for autonomous driving, but sparse point clouds at long range and scatter from temporal aggregation with ego-motion compensation degrade detection accuracy.

Method: Points from previous frames are shifted radially based on their dynamic Doppler component to eliminate radial scatter, with each point assigned a unique aggregation duration based on Doppler and angle to minimize tangential scatter.

Result: DoppDrive significantly improves object detection performance across various detectors and datasets by enhancing point cloud density while minimizing scatter.

Conclusion: DoppDrive provides an effective pre-detection point cloud enhancement step that is compatible with any radar detector and addresses key challenges in radar-based object detection for autonomous driving.

Abstract: Radar-based object detection is essential for autonomous driving due to
radar's long detection range. However, the sparsity of radar point clouds,
especially at long range, poses challenges for accurate detection. Existing
methods increase point density through temporal aggregation with ego-motion
compensation, but this approach introduces scatter from dynamic objects,
degrading detection performance. We propose DoppDrive, a novel Doppler-Driven
temporal aggregation method that enhances radar point cloud density while
minimizing scatter. Points from previous frames are shifted radially according
to their dynamic Doppler component to eliminate radial scatter, with each point
assigned a unique aggregation duration based on its Doppler and angle to
minimize tangential scatter. DoppDrive is a point cloud density enhancement
step applied before detection, compatible with any detector, and we demonstrate
that it significantly improves object detection performance across various
detectors and datasets.

</details>


### [60] [Geometry-Aware Video Inpainting for Joint Headset Occlusion Removal and Face Reconstruction in Social XR](https://arxiv.org/abs/2508.12336)
*Fatemeh Ghorbani Lohesara,Karen Eguiazarian,Sebastian Knorr*

Main category: cs.CV

TL;DR: A learning-based framework that removes HMD occlusions from facial videos and reconstructs complete 3D facial geometry using GAN-based inpainting and 3DMM parameter regression.


<details>
  <summary>Details</summary>
Motivation: HMDs obscure the upper face, hindering social XR applications like teleconferencing where facial expressions and eye gaze are crucial for immersion.

Method: Combines GAN-based video inpainting guided by dense landmarks and reference frames with SynergyNet-based 3DMM parameter regression for 3D face reconstruction.

Result: Successfully removes HMDs while preserving facial identity and realism, producing photorealistic 3D face geometry outputs that remain robust across different landmark densities.

Conclusion: The framework effectively addresses HMD occlusion issues in XR applications, enabling better facial expression capture and social interaction experiences.

Abstract: Head-mounted displays (HMDs) are essential for experiencing extended reality
(XR) environments and observing virtual content. However, they obscure the
upper part of the user's face, complicating external video recording and
significantly impacting social XR applications such as teleconferencing, where
facial expressions and eye gaze details are crucial for creating an immersive
experience. This study introduces a geometry-aware learning-based framework to
jointly remove HMD occlusions and reconstruct complete 3D facial geometry from
RGB frames captured from a single viewpoint. The method integrates a GAN-based
video inpainting network, guided by dense facial landmarks and a single
occlusion-free reference frame, to restore missing facial regions while
preserving identity. Subsequently, a SynergyNet-based module regresses 3D
Morphable Model (3DMM) parameters from the inpainted frames, enabling accurate
3D face reconstruction. Dense landmark optimization is incorporated throughout
the pipeline to improve both the inpainting quality and the fidelity of the
recovered geometry. Experimental results demonstrate that the proposed
framework can successfully remove HMDs from RGB facial videos while maintaining
facial identity and realism, producing photorealistic 3D face geometry outputs.
Ablation studies further show that the framework remains robust across
different landmark densities, with only minor quality degradation under sparse
landmark configurations.

</details>


### [61] [Semantic Discrepancy-aware Detector for Image Forgery Identification](https://arxiv.org/abs/2508.12341)
*Ziye Wang,Minghang Yu,Chunyan Xu,Zhen Cui*

Main category: cs.CV

TL;DR: A novel forgery detection method called Semantic Discrepancy-aware Detector (SDD) that uses reconstruction learning to align forgery and semantic concept spaces, achieving superior performance on standard datasets.


<details>
  <summary>Details</summary>
Motivation: The misalignment between forgery and semantic concept spaces hinders detection performance. Pre-trained models' semantic concepts are critical for fake image identification but current methods struggle with space shifts caused by irrelevant features.

Method: SDD uses semantic token sampling to mitigate space shifts, concept-level forgery discrepancy learning through visual reconstruction to capture discrepancies under semantic guidance, and low-level forgery feature enhancement to minimize redundant information.

Result: Experiments on two standard image forgery datasets demonstrate SDD's efficacy, achieving superior results compared to existing methods.

Conclusion: The proposed SDD effectively addresses the space misalignment problem in forgery detection by leveraging semantic concepts and reconstruction learning, providing a robust solution for digital media trustworthiness.

Abstract: With the rapid advancement of image generation techniques, robust forgery
detection has become increasingly imperative to ensure the trustworthiness of
digital media. Recent research indicates that the learned semantic concepts of
pre-trained models are critical for identifying fake images. However, the
misalignment between the forgery and semantic concept spaces hinders the
model's forgery detection performance. To address this problem, we propose a
novel Semantic Discrepancy-aware Detector (SDD) that leverages reconstruction
learning to align the two spaces at a fine-grained visual level. By exploiting
the conceptual knowledge embedded in the pre-trained vision language model, we
specifically design a semantic token sampling module to mitigate the space
shifts caused by features irrelevant to both forgery traces and semantic
concepts. A concept-level forgery discrepancy learning module, built upon a
visual reconstruction paradigm, is proposed to strengthen the interaction
between visual semantic concepts and forgery traces, effectively capturing
discrepancies under the concepts' guidance. Finally, the low-level forgery
feature enhancemer integrates the learned concept level forgery discrepancies
to minimize redundant forgery information. Experiments conducted on two
standard image forgery datasets demonstrate the efficacy of the proposed SDD,
which achieves superior results compared to existing methods. The code is
available at https://github.com/wzy1111111/SSD.

</details>


### [62] [AquaFeat: A Features-Based Image Enhancement Model for Underwater Object Detection](https://arxiv.org/abs/2508.12343)
*Emanuel C. Silva,Tatiana T. Schein,Stephanie L. Brião,Guilherme L. M. Costa,Felipe G. Oliveira,Gustavo P. Almeida,Eduardo L. Silva,Sam S. Devincenzi,Karina S. Machado,Paulo L. J. Drews-Jr*

Main category: cs.CV

TL;DR: AquaFeat is a plug-and-play feature enhancement module that improves underwater object detection by optimizing enhancement specifically for detection tasks rather than general image quality.


<details>
  <summary>Details</summary>
Motivation: Underwater image degradation severely impacts object detection performance, and traditional image enhancement methods are not optimized for downstream detection tasks.

Method: A multi-scale feature enhancement network trained end-to-end with the detector's loss function, ensuring enhancement is guided to refine features most relevant to detection tasks.

Result: State-of-the-art Precision (0.877) and Recall (0.624) on underwater datasets with competitive mAP scores (mAP@0.5: 0.677, mAP@[0.5:0.95]: 0.421) and practical processing speed of 46.5 FPS.

Conclusion: AquaFeat provides an effective and computationally efficient solution for real-world underwater applications like marine monitoring and infrastructure inspection.

Abstract: The severe image degradation in underwater environments impairs object
detection models, as traditional image enhancement methods are often not
optimized for such downstream tasks. To address this, we propose AquaFeat, a
novel, plug-and-play module that performs task-driven feature enhancement. Our
approach integrates a multi-scale feature enhancement network trained
end-to-end with the detector's loss function, ensuring the enhancement process
is explicitly guided to refine features most relevant to the detection task.
When integrated with YOLOv8m on challenging underwater datasets, AquaFeat
achieves state-of-the-art Precision (0.877) and Recall (0.624), along with
competitive mAP scores (mAP@0.5 of 0.677 and mAP@[0.5:0.95] of 0.421). By
delivering these accuracy gains while maintaining a practical processing speed
of 46.5 FPS, our model provides an effective and computationally efficient
solution for real-world applications, such as marine ecosystem monitoring and
infrastructure inspection.

</details>


### [63] [MBMamba: When Memory Buffer Meets Mamba for Structure-Aware Image Deblurring](https://arxiv.org/abs/2508.12346)
*Hu Gao,Depeng Dang*

Main category: cs.CV

TL;DR: MBMamba improves image deblurring using Mamba architecture with memory buffer mechanism and Ising-inspired regularization to address local pixel forgetting and maintain structural coherence.


<details>
  <summary>Details</summary>
Motivation: The Mamba architecture shows promise for image deblurring but suffers from local pixel forgetting and channel redundancy due to its flatten-and-scan strategy. Existing solutions increase computational complexity and hinder real-time performance.

Method: Proposes MBMamba with two key innovations: 1) a memory buffer mechanism to preserve historical information for fusion and model relevance between adjacent features, and 2) an Ising-inspired regularization loss that simulates energy minimization for pixel mutual attraction to maintain image structure.

Result: Experimental results demonstrate that MBMamba outperforms state-of-the-art approaches on widely used benchmarks.

Conclusion: The proposed method effectively addresses Mamba's limitations for image deblurring without changing the original architecture, achieving superior performance while maintaining computational efficiency.

Abstract: The Mamba architecture has emerged as a promising alternative to CNNs and
Transformers for image deblurring. However, its flatten-and-scan strategy often
results in local pixel forgetting and channel redundancy, limiting its ability
to effectively aggregate 2D spatial information. Although existing methods
mitigate this by modifying the scan strategy or incorporating local feature
modules, it increase computational complexity and hinder real-time performance.
In this paper, we propose a structure-aware image deblurring network without
changing the original Mamba architecture. Specifically, we design a memory
buffer mechanism to preserve historical information for later fusion, enabling
reliable modeling of relevance between adjacent features. Additionally, we
introduce an Ising-inspired regularization loss that simulates the energy
minimization of the physical system's "mutual attraction" between pixels,
helping to maintain image structure and coherence. Building on this, we develop
MBMamba. Experimental results show that our method outperforms state-of-the-art
approaches on widely used benchmarks.

</details>


### [64] [EgoLoc: A Generalizable Solution for Temporal Interaction Localization in Egocentric Videos](https://arxiv.org/abs/2508.12349)
*Junyi Ma,Erhang Zhang,Yin-Dong Zheng,Yuchen Xie,Yixuan Zhou,Hesheng Wang*

Main category: cs.CV

TL;DR: EgoLoc is a zero-shot method for temporal interaction localization that identifies precise hand-object contact and separation moments in egocentric videos without needing object masks or verb-noun taxonomies.


<details>
  <summary>Details</summary>
Motivation: Existing research focuses on 'how to interact' but neglects the critical 'when to interact' problem - precisely localizing contact/separation moments which is crucial for VR/AR applications and robotic policy transfer.

Method: Proposes EgoLoc with hand-dynamics-guided sampling to generate visual prompts, leverages vision-language models to identify contact attributes and localize timestamps, and uses closed-loop feedback for refinement.

Result: Achieves plausible temporal interaction localization on public datasets and novel benchmarks, effectively facilitating downstream applications in egocentric vision and robotic manipulation.

Conclusion: EgoLoc provides a generalizable zero-shot solution for fine-grained hand-object interaction analysis, eliminating dependency on object masks and category annotations while enabling precise contact moment localization.

Abstract: Analyzing hand-object interaction in egocentric vision facilitates VR/AR
applications and human-robot policy transfer. Existing research has mostly
focused on modeling the behavior paradigm of interactive actions (i.e., ``how
to interact''). However, the more challenging and fine-grained problem of
capturing the critical moments of contact and separation between the hand and
the target object (i.e., ``when to interact'') is still underexplored, which is
crucial for immersive interactive experiences in mixed reality and robotic
motion planning. Therefore, we formulate this problem as temporal interaction
localization (TIL). Some recent works extract semantic masks as TIL references,
but suffer from inaccurate object grounding and cluttered scenarios. Although
current temporal action localization (TAL) methods perform well in detecting
verb-noun action segments, they rely on category annotations during training
and exhibit limited precision in localizing hand-object contact/separation
moments. To address these issues, we propose a novel zero-shot approach dubbed
EgoLoc to localize hand-object contact and separation timestamps in egocentric
videos. EgoLoc introduces hand-dynamics-guided sampling to generate
high-quality visual prompts. It exploits the vision-language model to identify
contact/separation attributes, localize specific timestamps, and provide
closed-loop feedback for further refinement. EgoLoc eliminates the need for
object masks and verb-noun taxonomies, leading to generalizable zero-shot
implementation. Comprehensive experiments on the public dataset and our novel
benchmarks demonstrate that EgoLoc achieves plausible TIL for egocentric
videos. It is also validated to effectively facilitate multiple downstream
applications in egocentric vision and robotic manipulation tasks. Code and
relevant data will be released at https://github.com/IRMVLab/EgoLoc.

</details>


### [65] [Synthetic Data is Sufficient for Zero-Shot Visual Generalization from Offline Data](https://arxiv.org/abs/2508.12356)
*Ahmet H. Güzel,Ilija Bogunovic,Jack Parker-Holder*

Main category: cs.CV

TL;DR: A simple two-step data augmentation method using diffusion models to generate synthetic training data in latent space, improving offline RL generalization without algorithmic changes.


<details>
  <summary>Details</summary>
Motivation: Offline RL policies struggle to generalize due to limited diverse states in visual data, with noise and spurious correlations causing overfitting and poor generalization to unseen environments.

Method: Two-step process: 1) augment original offline data to introduce diversity for better zero-shot generalization, 2) use diffusion model to generate additional synthetic data in latent space.

Result: Significantly improves generalization across continuous (Visual D4RL) and discrete (Procgen) action spaces, increases data diversity, reduces generalization gap, and maintains computational efficiency.

Conclusion: This approach enables leveraging vision-based offline data for training robust agents and could fuel progress in synthetic data generation for more general agents.

Abstract: Offline reinforcement learning (RL) offers a promising framework for training
agents using pre-collected datasets without the need for further environment
interaction. However, policies trained on offline data often struggle to
generalise due to limited exposure to diverse states. The complexity of visual
data introduces additional challenges such as noise, distractions, and spurious
correlations, which can misguide the policy and increase the risk of
overfitting if the training data is not sufficiently diverse. Indeed, this
makes it challenging to leverage vision-based offline data in training robust
agents that can generalize to unseen environments. To solve this problem, we
propose a simple approach generating additional synthetic training data. We
propose a two-step process, first augmenting the originally collected offline
data to improve zero-shot generalization by introducing diversity, then using a
diffusion model to generate additional data in latent space. We test our method
across both continuous action spaces (Visual D4RL) and discrete action spaces
(Procgen), demonstrating that it significantly improves generalization without
requiring any algorithmic changes to existing model-free offline RL methods. We
show that our method not only increases the diversity of the training data but
also significantly reduces the generalization gap at test time while
maintaining computational efficiency. We believe this approach could fuel
additional progress in generating synthetic data to train more general agents
in the future.

</details>


### [66] [IPGPhormer: Interpretable Pathology Graph-Transformer for Survival Analysis](https://arxiv.org/abs/2508.12381)
*Guo Tang,Songhan Jiang,Jinpeng Lu,Linghan Cai,Yongbing Zhang*

Main category: cs.CV

TL;DR: IPGPhormer is an interpretable graph-transformer framework for cancer survival analysis from whole-slide images that balances spatial relationships with local context and provides multi-level interpretability without manual annotations.


<details>
  <summary>Details</summary>
Motivation: Existing survival analysis methods struggle to balance long-range spatial relationships with local contextual dependencies and lack inherent interpretability, limiting clinical utility.

Method: Proposes Interpretable Pathology Graph-Transformer (IPGPhormer) that captures tumor microenvironment characteristics and models spatial dependencies across tissue, providing interpretability at both tissue and cellular levels without post-hoc annotations.

Result: Outperforms state-of-the-art methods on four public benchmark datasets in both predictive accuracy and interpretability.

Conclusion: IPGPhormer offers a promising tool for cancer prognosis assessment, enabling more reliable and interpretable decision-support systems in pathology.

Abstract: Pathological images play an essential role in cancer prognosis, while
survival analysis, which integrates computational techniques, can predict
critical clinical events such as patient mortality or disease recurrence from
whole-slide images (WSIs). Recent advancements in multiple instance learning
have significantly improved the efficiency of survival analysis. However,
existing methods often struggle to balance the modeling of long-range spatial
relationships with local contextual dependencies and typically lack inherent
interpretability, limiting their clinical utility. To address these challenges,
we propose the Interpretable Pathology Graph-Transformer (IPGPhormer), a novel
framework that captures the characteristics of the tumor microenvironment and
models their spatial dependencies across the tissue. IPGPhormer uniquely
provides interpretability at both tissue and cellular levels without requiring
post-hoc manual annotations, enabling detailed analyses of individual WSIs and
cross-cohort assessments. Comprehensive evaluations on four public benchmark
datasets demonstrate that IPGPhormer outperforms state-of-the-art methods in
both predictive accuracy and interpretability. In summary, our method,
IPGPhormer, offers a promising tool for cancer prognosis assessment, paving the
way for more reliable and interpretable decision-support systems in pathology.
The code is publicly available at
https://anonymous.4open.science/r/IPGPhormer-6EEB.

</details>


### [67] [ViT-EnsembleAttack: Augmenting Ensemble Models for Stronger Adversarial Transferability in Vision Transformers](https://arxiv.org/abs/2508.12384)
*Hanwen Cao,Haobo Lu,Xiaosen Wang,Kun He*

Main category: cs.CV

TL;DR: ViT-EnsembleAttack enhances adversarial transferability for Vision Transformers through adversarial augmentation of surrogate models using multi-head dropping, attention scaling, and MLP feature mixing, optimized with Bayesian optimization.


<details>
  <summary>Details</summary>
Motivation: Existing ensemble attacks focus on refining weights or paths but overlook exploring ensemble models themselves to enhance transferability, especially for Vision Transformers which receive less attention in ensemble-based attacks.

Method: Proposes adversarial augmentation of surrogate ViT models using three strategies: Multi-head dropping, Attention score scaling, and MLP feature mixing with Bayesian optimization. Includes Automatic Reweighting and Step Size Enlargement modules to boost transferability.

Result: Extensive experiments show ViT-EnsembleAttack significantly enhances adversarial transferability of ensemble-based attacks on ViTs, substantially outperforming existing methods.

Conclusion: The proposed method effectively addresses the gap in ensemble-based attacks for Vision Transformers and demonstrates superior performance through adversarial augmentation techniques and optimization modules.

Abstract: Ensemble-based attacks have been proven to be effective in enhancing
adversarial transferability by aggregating the outputs of models with various
architectures. However, existing research primarily focuses on refining
ensemble weights or optimizing the ensemble path, overlooking the exploration
of ensemble models to enhance the transferability of adversarial attacks. To
address this gap, we propose applying adversarial augmentation to the surrogate
models, aiming to boost overall generalization of ensemble models and reduce
the risk of adversarial overfitting. Meanwhile, observing that ensemble Vision
Transformers (ViTs) gain less attention, we propose ViT-EnsembleAttack based on
the idea of model adversarial augmentation, the first ensemble-based attack
method tailored for ViTs to the best of our knowledge. Our approach generates
augmented models for each surrogate ViT using three strategies: Multi-head
dropping, Attention score scaling, and MLP feature mixing, with the associated
parameters optimized by Bayesian optimization. These adversarially augmented
models are ensembled to generate adversarial examples. Furthermore, we
introduce Automatic Reweighting and Step Size Enlargement modules to boost
transferability. Extensive experiments demonstrate that ViT-EnsembleAttack
significantly enhances the adversarial transferability of ensemble-based
attacks on ViTs, outperforming existing methods by a substantial margin. Code
is available at https://github.com/Trustworthy-AI-Group/TransferAttack.

</details>


### [68] [DeCoT: Decomposing Complex Instructions for Enhanced Text-to-Image Generation with Large Language Models](https://arxiv.org/abs/2508.12396)
*Xiaochuan Lin,Xiangyong Chen,Xuan Li,Yichen Su*

Main category: cs.CV

TL;DR: DeCoT is a framework that uses LLMs to decompose complex text instructions into structured semantic units, significantly improving T2I models' ability to handle detailed and complex image generation tasks.


<details>
  <summary>Details</summary>
Motivation: Current T2I models struggle with complex, long-form textual instructions, failing to accurately render intricate details, spatial relationships, and specific constraints as revealed by benchmarks like LongBench-T2I.

Method: Two-stage framework: 1) Complex Instruction Decomposition and Semantic Enhancement using LLMs to break down instructions into structured semantic units, 2) Multi-Stage Prompt Integration and Adaptive Generation to create hierarchical or optimized prompts for T2I models.

Result: DeCoT consistently improves T2I model performance across all dimensions, achieving an average score of 3.52 with Infinity-8B (vs baseline 3.44), with significant gains in challenging aspects like "Text" and "Composition". Human evaluations confirm superior perceptual quality and instruction fidelity.

Conclusion: DeCoT effectively bridges the gap between user intent and T2I model requirements, enabling more faithful and accurate image generation from complex textual instructions through structured decomposition and LLM-enhanced prompting.

Abstract: Despite remarkable advancements, current Text-to-Image (T2I) models struggle
with complex, long-form textual instructions, frequently failing to accurately
render intricate details, spatial relationships, or specific constraints. This
limitation is highlighted by benchmarks such as LongBench-T2I, which reveal
deficiencies in handling composition, specific text, and fine textures. To
address this, we propose DeCoT (Decomposition-CoT), a novel framework that
leverages Large Language Models (LLMs) to significantly enhance T2I models'
understanding and execution of complex instructions. DeCoT operates in two core
stages: first, Complex Instruction Decomposition and Semantic Enhancement,
where an LLM breaks down raw instructions into structured, actionable semantic
units and clarifies ambiguities; second, Multi-Stage Prompt Integration and
Adaptive Generation, which transforms these units into a hierarchical or
optimized single prompt tailored for existing T2I models. Extensive experiments
on the LongBench-T2I dataset demonstrate that DeCoT consistently and
substantially improves the performance of leading T2I models across all
evaluated dimensions, particularly in challenging aspects like "Text" and
"Composition". Quantitative results, validated by multiple MLLM evaluators
(Gemini-2.0-Flash and InternVL3-78B), show that DeCoT, when integrated with
Infinity-8B, achieves an average score of 3.52, outperforming the baseline
Infinity-8B (3.44). Ablation studies confirm the critical contribution of each
DeCoT component and the importance of sophisticated LLM prompting. Furthermore,
human evaluations corroborate these findings, indicating superior perceptual
quality and instruction fidelity. DeCoT effectively bridges the gap between
high-level user intent and T2I model requirements, leading to more faithful and
accurate image generation.

</details>


### [69] [Federated Cross-Modal Style-Aware Prompt Generation](https://arxiv.org/abs/2508.12399)
*Suraj Prasad,Navyansh Mahla,Sunny Gupta,Amit Sethi*

Main category: cs.CV

TL;DR: FedCSAP is a federated learning framework that enhances CLIP's performance by leveraging multi-scale visual features and client-specific style indicators to generate robust, context-aware prompts while maintaining data privacy.


<details>
  <summary>Details</summary>
Motivation: Conventional prompt learning approaches using only final-layer features miss rich multi-scale visual cues and domain-specific style variations in decentralized client data, limiting generalization across diverse datasets.

Method: Leverages low, mid, and high-level features from CLIP's vision encoder combined with client-specific style indicators from batch-level statistics to generate distinct, non-redundant prompt tokens through local training and global aggregation.

Result: Outperforms existing federated prompt learning methods in accuracy and generalization across multiple image classification datasets, effectively handling non-IID class distributions and diverse domain styles.

Conclusion: FedCSAP successfully bridges the gap in federated prompt learning by incorporating multi-scale visual features and style awareness, demonstrating superior performance while preserving data privacy in decentralized settings.

Abstract: Prompt learning has propelled vision-language models like CLIP to excel in
diverse tasks, making them ideal for federated learning due to computational
efficiency. However, conventional approaches that rely solely on final-layer
features miss out on rich multi-scale visual cues and domain-specific style
variations in decentralized client data. To bridge this gap, we introduce
FedCSAP (Federated Cross-Modal Style-Aware Prompt Generation). Our framework
harnesses low, mid, and high-level features from CLIP's vision encoder
alongside client-specific style indicators derived from batch-level statistics.
By merging intricate visual details with textual context, FedCSAP produces
robust, context-aware prompt tokens that are both distinct and non-redundant,
thereby boosting generalization across seen and unseen classes. Operating
within a federated learning paradigm, our approach ensures data privacy through
local training and global aggregation, adeptly handling non-IID class
distributions and diverse domain-specific styles. Comprehensive experiments on
multiple image classification datasets confirm that FedCSAP outperforms
existing federated prompt learning methods in both accuracy and overall
generalization.

</details>


### [70] [MPCAR: Multi-Perspective Contextual Augmentation for Enhanced Visual Reasoning in Large Vision-Language Models](https://arxiv.org/abs/2508.12400)
*Amirul Rahman,Qiang Xu,Xueying Huang*

Main category: cs.CV

TL;DR: MPCAR is an inference-time strategy that enhances Large Vision-Language Models' performance by generating diverse perspectives and integrating them into comprehensive prompts for better visual reasoning, without requiring model fine-tuning.


<details>
  <summary>Details</summary>
Motivation: Existing LVLMs struggle with complex visual reasoning tasks that require deep contextual understanding and multi-angle analysis due to limitations in single-shot image encoding and prompts.

Method: A three-stage approach: 1) Generate N diverse descriptions/reasoning paths from various angles, 2) Intelligently integrate these with original questions to create comprehensive context-augmented prompts, 3) Use enriched prompts to guide final reasoning and answer generation.

Result: Significant accuracy gains on challenging VQA datasets (GQA, VQA-CP v2, ScienceQA), particularly for tasks requiring robust contextual understanding. Human evaluations confirm improved coherence and completeness of answers.

Conclusion: MPCAR effectively leverages LVLMs' generative capabilities to enrich input contexts, unlocking latent reasoning potential for complex multimodal tasks without parameter fine-tuning.

Abstract: Despite significant advancements, Large Vision-Language Models (LVLMs)
continue to face challenges in complex visual reasoning tasks that demand deep
contextual understanding, multi-angle analysis, or meticulous detail
recognition. Existing approaches often rely on single-shot image encoding and
prompts, limiting their ability to fully capture nuanced visual information.
Inspired by the notion that strategically generated "additional" information
can serve as beneficial contextual augmentation, we propose Multi-Perspective
Contextual Augmentation for Reasoning (MPCAR), a novel inference-time strategy
designed to enhance LVLM performance. MPCAR operates in three stages: first, an
LVLM generates N diverse and complementary descriptions or preliminary
reasoning paths from various angles; second, these descriptions are
intelligently integrated with the original question to construct a
comprehensive context-augmented prompt; and finally, this enriched prompt
guides the ultimate LVLM for deep reasoning and final answer generation.
Crucially, MPCAR achieves these enhancements without requiring any fine-tuning
of the underlying LVLM's parameters. Extensive experiments on challenging
Visual Question Answering (VQA) datasets, including GQA, VQA-CP v2, and
ScienceQA (Image-VQA), demonstrate that MPCAR consistently outperforms
established baseline methods. Our quantitative results show significant
accuracy gains, particularly on tasks requiring robust contextual
understanding, while human evaluations confirm improved coherence and
completeness of the generated answers. Ablation studies further highlight the
importance of diverse prompt templates and the number of generated
perspectives. This work underscores the efficacy of leveraging LVLMs' inherent
generative capabilities to enrich input contexts, thereby unlocking their
latent reasoning potential for complex multimodal tasks.

</details>


### [71] [LMAD: Integrated End-to-End Vision-Language Model for Explainable Autonomous Driving](https://arxiv.org/abs/2508.12404)
*Nan Song,Bozhou Zhang,Xiatian Zhu,Jiankang Deng,Li Zhang*

Main category: cs.CV

TL;DR: LMAD is a novel vision-language framework that enhances autonomous driving by integrating comprehensive scene understanding and specialized expert adapters with existing VLMs, significantly improving driving reasoning performance.


<details>
  <summary>Details</summary>
Motivation: Existing VLM fine-tuning methods for autonomous driving lack holistic scene recognition and spatial awareness needed for complex driving situations, creating a gap in explainable driving systems.

Method: Proposes LMAD framework that incorporates preliminary scene interaction and specialized expert adapters within a driving task structure, emulating end-to-end driving paradigms while maintaining compatibility with existing VLMs.

Result: Extensive experiments on DriveLM and nuScenes-QA datasets show LMAD significantly boosts VLM performance on driving reasoning tasks, setting new standards for explainable autonomous driving.

Conclusion: LMAD successfully addresses the limitations of current VLM approaches by providing better scene understanding and spatial awareness, making it fully compatible with existing systems while enhancing driving reasoning capabilities.

Abstract: Large vision-language models (VLMs) have shown promising capabilities in
scene understanding, enhancing the explainability of driving behaviors and
interactivity with users. Existing methods primarily fine-tune VLMs on on-board
multi-view images and scene reasoning text, but this approach often lacks the
holistic and nuanced scene recognition and powerful spatial awareness required
for autonomous driving, especially in complex situations. To address this gap,
we propose a novel vision-language framework tailored for autonomous driving,
called LMAD. Our framework emulates modern end-to-end driving paradigms by
incorporating comprehensive scene understanding and a task-specialized
structure with VLMs. In particular, we introduce preliminary scene interaction
and specialized expert adapters within the same driving task structure, which
better align VLMs with autonomous driving scenarios. Furthermore, our approach
is designed to be fully compatible with existing VLMs while seamlessly
integrating with planning-oriented driving systems. Extensive experiments on
the DriveLM and nuScenes-QA datasets demonstrate that LMAD significantly boosts
the performance of existing VLMs on driving reasoning tasks,setting a new
standard in explainable autonomous driving.

</details>


### [72] [S5: Scalable Semi-Supervised Semantic Segmentation in Remote Sensing](https://arxiv.org/abs/2508.12409)
*Liang Lv,Di Wang,Jing Zhang,Lefei Zhang*

Main category: cs.CV

TL;DR: S5 is a scalable semi-supervised semantic segmentation framework for remote sensing that uses large-scale datasets and foundation models to achieve state-of-the-art performance across multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing semi-supervised semantic segmentation methods in remote sensing rely on small datasets and models, limiting practical applicability. There's vast unlabeled Earth observation data that remains underutilized due to costly pixel-level annotations.

Method: Proposes S5 framework with data selection strategy (entropy-based filtering + diversity expansion) creating RS4P-1M dataset. Pre-trains RS foundation models of varying sizes, then uses Mixture-of-Experts-based multi-dataset fine-tuning for efficient adaptation to multiple benchmarks.

Result: Achieves state-of-the-art performance across all remote sensing benchmarks. Significantly boosts performance on land cover segmentation and object detection tasks. Improves generalization and versatility of RS foundation models.

Conclusion: Scaling semi-supervised learning is viable for remote sensing applications. The framework successfully leverages vast unlabeled data and enables efficient adaptation to multiple tasks with fewer parameters.

Abstract: Semi-supervised semantic segmentation (S4) has advanced remote sensing (RS)
analysis by leveraging unlabeled data through pseudo-labeling and consistency
learning. However, existing S4 studies often rely on small-scale datasets and
models, limiting their practical applicability. To address this, we propose S5,
the first scalable framework for semi-supervised semantic segmentation in RS,
which unlocks the potential of vast unlabeled Earth observation data typically
underutilized due to costly pixel-level annotations. Built upon existing
large-scale RS datasets, S5 introduces a data selection strategy that
integrates entropy-based filtering and diversity expansion, resulting in the
RS4P-1M dataset. Using this dataset, we systematically scales S4 methods by
pre-training RS foundation models (RSFMs) of varying sizes on this extensive
corpus, significantly boosting their performance on land cover segmentation and
object detection tasks. Furthermore, during fine-tuning, we incorporate a
Mixture-of-Experts (MoE)-based multi-dataset fine-tuning approach, which
enables efficient adaptation to multiple RS benchmarks with fewer parameters.
This approach improves the generalization and versatility of RSFMs across
diverse RS benchmarks. The resulting RSFMs achieve state-of-the-art performance
across all benchmarks, underscoring the viability of scaling semi-supervised
learning for RS applications. All datasets, code, and models will be released
at https://github.com/MiliLab/S5

</details>


### [73] [SRMA-Mamba: Spatial Reverse Mamba Attention Network for Pathological Liver Segmentation in MRI Volumes](https://arxiv.org/abs/2508.12410)
*Jun Zeng,Yannan Huang,Elif Keles,Halil Ertugrul Aktas,Gorkem Durak,Nikhil Kumar Tomar,Quoc-Huy Trinh,Deepak Ranjan Nayak,Ulas Bagci,Debesh Jha*

Main category: cs.CV

TL;DR: SRMA-Mamba is a novel Mamba-based network for 3D liver cirrhosis segmentation in MRI volumes that integrates spatial anatomical relationships and uses selective scanning with reverse attention to achieve state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Early detection of liver cirrhosis is critical for reducing mortality, but existing methods underutilize spatial anatomical details in volumetric MRI data, limiting clinical effectiveness and explainability.

Method: Proposes SRMA-Mamba with Spatial Anatomy-Based Mamba module (SABMamba) that performs selective Mamba scans within cirrhotic tissues and combines anatomical information from sagittal, coronal, and axial planes. Also introduces Spatial Reverse Attention module (SRMA) to progressively refine segmentation details using coarse maps and hierarchical encoding features.

Result: Extensive experiments demonstrate that SRMA-Mamba surpasses state-of-the-art methods, delivering exceptional performance in 3D pathological liver segmentation.

Conclusion: The proposed SRMA-Mamba network effectively addresses the challenge of modeling spatial relationships in complex liver anatomical structures and achieves superior volumetric segmentation of pathological liver tissues compared to existing approaches.

Abstract: Liver Cirrhosis plays a critical role in the prognosis of chronic liver
disease. Early detection and timely intervention are critical in significantly
reducing mortality rates. However, the intricate anatomical architecture and
diverse pathological changes of liver tissue complicate the accurate detection
and characterization of lesions in clinical settings. Existing methods
underutilize the spatial anatomical details in volumetric MRI data, thereby
hindering their clinical effectiveness and explainability. To address this
challenge, we introduce a novel Mamba-based network, SRMA-Mamba, designed to
model the spatial relationships within the complex anatomical structures of MRI
volumes. By integrating the Spatial Anatomy-Based Mamba module (SABMamba),
SRMA-Mamba performs selective Mamba scans within liver cirrhotic tissues and
combines anatomical information from the sagittal, coronal, and axial planes to
construct a global spatial context representation, enabling efficient
volumetric segmentation of pathological liver structures. Furthermore, we
introduce the Spatial Reverse Attention module (SRMA), designed to
progressively refine cirrhotic details in the segmentation map, utilizing both
the coarse segmentation map and hierarchical encoding features. Extensive
experiments demonstrate that SRMA-Mamba surpasses state-of-the-art methods,
delivering exceptional performance in 3D pathological liver segmentation. Our
code is available for public:
{\color{blue}{https://github.com/JunZengz/SRMA-Mamba}}.

</details>


### [74] [TiP4GEN: Text to Immersive Panorama 4D Scene Generation](https://arxiv.org/abs/2508.12415)
*Ke Xing,Hanwen Liang,Dejia Xu,Yuyang Yin,Konstantinos N. Plataniotis,Yao Zhao,Yunchao Wei*

Main category: cs.CV

TL;DR: TiP4GEN is a text-to-dynamic panorama scene generation framework that creates 360-degree immersive 4D scenes with fine-grained content control and motion-rich, geometry-consistent panoramic environments.


<details>
  <summary>Details</summary>
Motivation: Existing VR/AR generation works focus on static scenes or narrow perspective-view dynamic scenes, lacking true 360-degree immersive experiences from any viewpoint.

Method: Combines panorama video generation (using Dual-branch Generation Model with panorama and perspective branches + bidirectional cross-attention) and dynamic scene reconstruction (Geometry-aligned Reconstruction Model based on 3D Gaussian Splatting with metric depth maps and estimated camera poses).

Result: Extensive experiments demonstrate effectiveness and superiority in generating visually compelling and motion-coherent dynamic panoramic scenes.

Conclusion: TiP4GEN successfully addresses the gap in 360-degree immersive dynamic scene generation with fine-grained control and geometric consistency.

Abstract: With the rapid advancement and widespread adoption of VR/AR technologies,
there is a growing demand for the creation of high-quality, immersive dynamic
scenes. However, existing generation works predominantly concentrate on the
creation of static scenes or narrow perspective-view dynamic scenes, falling
short of delivering a truly 360-degree immersive experience from any viewpoint.
In this paper, we introduce \textbf{TiP4GEN}, an advanced text-to-dynamic
panorama scene generation framework that enables fine-grained content control
and synthesizes motion-rich, geometry-consistent panoramic 4D scenes. TiP4GEN
integrates panorama video generation and dynamic scene reconstruction to create
360-degree immersive virtual environments. For video generation, we introduce a
\textbf{Dual-branch Generation Model} consisting of a panorama branch and a
perspective branch, responsible for global and local view generation,
respectively. A bidirectional cross-attention mechanism facilitates
comprehensive information exchange between the branches. For scene
reconstruction, we propose a \textbf{Geometry-aligned Reconstruction Model}
based on 3D Gaussian Splatting. By aligning spatial-temporal point clouds using
metric depth maps and initializing scene cameras with estimated poses, our
method ensures geometric consistency and temporal coherence for the
reconstructed scenes. Extensive experiments demonstrate the effectiveness of
our proposed designs and the superiority of TiP4GEN in generating visually
compelling and motion-coherent dynamic panoramic scenes. Our project page is at
https://ke-xing.github.io/TiP4GEN/.

</details>


### [75] [Illusions in Humans and AI: How Visual Perception Aligns and Diverges](https://arxiv.org/abs/2508.12422)
*Jianyi Yang,Junyi Ye,Ankan Dash,Guiling Wang*

Main category: cs.CV

TL;DR: Comparison of human and AI visual perception through illusion responses reveals critical differences, with AI showing both human-like illusion effects and unique vulnerabilities like pixel sensitivity and hallucinations.


<details>
  <summary>Details</summary>
Motivation: To understand how artificial vision systems differ from biological perception and identify AI-specific perceptual vulnerabilities that could impact trust and safety in AI systems performing human-like visual tasks.

Method: Systematic comparison of human and AI responses to classic visual illusions involving color, size, shape, and motion, examining both targeted training effects and emergent pattern recognition behaviors.

Result: AI shows some human-like illusion effects but also exhibits unique vulnerabilities including pixel-level sensitivity and hallucinations that lack human counterparts, revealing alignment gaps and AI-specific perceptual weaknesses.

Conclusion: Understanding these perceptual differences provides insights for developing more robust, interpretable, and human-aligned AI vision systems that preserve beneficial human perceptual biases while avoiding distortions that undermine trust and safety.

Abstract: By comparing biological and artificial perception through the lens of
illusions, we highlight critical differences in how each system constructs
visual reality. Understanding these divergences can inform the development of
more robust, interpretable, and human-aligned artificial intelligence (AI)
vision systems. In particular, visual illusions expose how human perception is
based on contextual assumptions rather than raw sensory data. As artificial
vision systems increasingly perform human-like tasks, it is important to ask:
does AI experience illusions, too? Does it have unique illusions? This article
explores how AI responds to classic visual illusions that involve color, size,
shape, and motion. We find that some illusion-like effects can emerge in these
models, either through targeted training or as by-products of pattern
recognition. In contrast, we also identify illusions unique to AI, such as
pixel-level sensitivity and hallucinations, that lack human counterparts. By
systematically comparing human and AI responses to visual illusions, we uncover
alignment gaps and AI-specific perceptual vulnerabilities invisible to human
perception. These findings provide insights for future research on vision
systems that preserve human-beneficial perceptual biases while avoiding
distortions that undermine trust and safety.

</details>


### [76] [Adversarial Attacks on VQA-NLE: Exposing and Alleviating Inconsistencies in Visual Question Answering Explanations](https://arxiv.org/abs/2508.12430)
*Yahsin Yeh,Yilun Wu,Bokai Ruan,Honghan Shuai*

Main category: cs.CV

TL;DR: This paper exposes vulnerabilities in VQA-NLE systems that produce inconsistent explanations and proposes adversarial attacks on both questions and images, along with a knowledge-based defense method to improve robustness.


<details>
  <summary>Details</summary>
Motivation: Existing VQA-NLE systems produce inconsistent explanations and reach conclusions without genuine understanding, exposing weaknesses in their inference pipelines and explanation mechanisms.

Method: Leverage existing adversarial question perturbation and propose novel image perturbation strategy to induce contradictory outputs. Introduce mitigation method using external knowledge to alleviate inconsistencies.

Result: Extensive evaluations on two standard benchmarks and two VQA-NLE models demonstrate effectiveness of attacks and potential of knowledge-based defenses.

Conclusion: The research reveals pressing security and reliability concerns in current VQA-NLE systems and shows the potential of knowledge-based approaches to bolster model robustness.

Abstract: Natural language explanations in visual question answering (VQA-NLE) aim to
make black-box models more transparent by elucidating their decision-making
processes. However, we find that existing VQA-NLE systems can produce
inconsistent explanations and reach conclusions without genuinely understanding
the underlying context, exposing weaknesses in either their inference pipeline
or explanation-generation mechanism. To highlight these vulnerabilities, we not
only leverage an existing adversarial strategy to perturb questions but also
propose a novel strategy that minimally alters images to induce contradictory
or spurious outputs. We further introduce a mitigation method that leverages
external knowledge to alleviate these inconsistencies, thereby bolstering model
robustness. Extensive evaluations on two standard benchmarks and two widely
used VQA-NLE models underscore the effectiveness of our attacks and the
potential of knowledge-based defenses, ultimately revealing pressing security
and reliability concerns in current VQA-NLE systems.

</details>


### [77] [X-Ray-CoT: Interpretable Chest X-ray Diagnosis with Vision-Language Models via Chain-of-Thought Reasoning](https://arxiv.org/abs/2508.12455)
*Chee Ng,Liliang Sun,Shaoqing Tang*

Main category: cs.CV

TL;DR: X-Ray-CoT is a novel framework that uses Vision-Language Large Models to provide interpretable chest X-ray diagnosis by simulating radiologists' chain-of-thought reasoning, achieving competitive accuracy while generating explainable reports.


<details>
  <summary>Details</summary>
Motivation: Chest X-ray interpretation requires extensive clinical experience and suffers from variability, while existing deep learning models are black-box systems that hinder clinical adoption in high-stakes medical settings.

Method: Proposes X-Ray-CoT framework that extracts multi-modal features and visual concepts, then uses LLM-based component with structured Chain-of-Thought prompting to reason and generate detailed natural language diagnostic reports.

Result: Achieves Balanced Accuracy of 80.52% and F1 score of 78.65% on CORDA dataset, slightly surpassing black-box models, while uniquely generating high-quality explainable reports validated by human evaluations.

Conclusion: Represents a significant step towards trustworthy and clinically actionable AI systems in medical imaging, with ablation studies confirming the necessity of multi-modal fusion and CoT reasoning for robust and transparent medical AI.

Abstract: Chest X-ray imaging is crucial for diagnosing pulmonary and cardiac diseases,
yet its interpretation demands extensive clinical experience and suffers from
inter-observer variability. While deep learning models offer high diagnostic
accuracy, their black-box nature hinders clinical adoption in high-stakes
medical settings. To address this, we propose X-Ray-CoT (Chest X-Ray
Chain-of-Thought), a novel framework leveraging Vision-Language Large Models
(LVLMs) for intelligent chest X-ray diagnosis and interpretable report
generation. X-Ray-CoT simulates human radiologists' "chain-of-thought" by first
extracting multi-modal features and visual concepts, then employing an
LLM-based component with a structured Chain-of-Thought prompting strategy to
reason and produce detailed natural language diagnostic reports. Evaluated on
the CORDA dataset, X-Ray-CoT achieves competitive quantitative performance,
with a Balanced Accuracy of 80.52% and F1 score of 78.65% for disease
diagnosis, slightly surpassing existing black-box models. Crucially, it
uniquely generates high-quality, explainable reports, as validated by
preliminary human evaluations. Our ablation studies confirm the integral role
of each proposed component, highlighting the necessity of multi-modal fusion
and CoT reasoning for robust and transparent medical AI. This work represents a
significant step towards trustworthy and clinically actionable AI systems in
medical imaging.

</details>


### [78] [Inverse-LLaVA: Eliminating Alignment Pre-training Through Text-to-Vision Mapping](https://arxiv.org/abs/2508.12466)
*Xuhui Zhan,Tyler Derr*

Main category: cs.CV

TL;DR: Inverse-LLaVA eliminates alignment pre-training by mapping text to visual space instead of visual to text, achieving better reasoning performance while reducing computation by 45%.


<details>
  <summary>Details</summary>
Motivation: Challenge traditional multimodal learning that requires expensive alignment pre-training and projects visual features to text space, seeking more efficient fusion without massive alignment datasets.

Method: Maps text embeddings into continuous visual representation space, performs fusion within transformer intermediate layers using selective additive components in attention mechanisms.

Result: Improves reasoning-intensive tasks (MM-VET: +0.2%, VizWiz: +1.8%, ScienceQA: +0.2%, cognitive reasoning: +27.2%) but decreases in perception tasks (celebrity recognition: -49.5%, OCR: -21.3%). Reduces computational requirements by 45%.

Conclusion: Alignment pre-training is not necessary for effective multimodal learning, particularly for complex reasoning. Establishes a new paradigm that preserves modality-specific characteristics and enables efficient multimodal architectures.

Abstract: Traditional multimodal learning approaches require expensive alignment
pre-training to bridge vision and language modalities, typically projecting
visual features into discrete text token spaces. We challenge both fundamental
assumptions underlying this paradigm by proposing Inverse-LLaVA, a novel
approach that eliminates alignment pre-training entirely while inverting the
conventional mapping direction. Rather than projecting visual features to text
space, our method maps text embeddings into continuous visual representation
space and performs fusion within transformer intermediate layers. Through
selective additive components in attention mechanisms, we enable dynamic
integration of visual and textual representations without requiring massive
image-text alignment datasets. Comprehensive experiments across nine multimodal
benchmarks demonstrate nuanced performance trade-offs: Inverse-LLaVA achieves
notable improvements on reasoning-intensive and cognitive tasks (MM-VET: +0.2%,
VizWiz: +1.8%, ScienceQA: +0.2%, cognitive reasoning: +27.2%), while showing
expected decreases in perception tasks requiring memorized visual-text
associations (celebrity recognition: -49.5%, OCR: -21.3%). These results
provide the first empirical evidence that alignment pre-training is not
necessary for effective multimodal learning, particularly for complex reasoning
tasks. Our work establishes the feasibility of a new paradigm that reduces
computational requirements by 45%, challenges conventional wisdom about
modality fusion, and opens new research directions for efficient multimodal
architectures that preserve modality-specific characteristics. Our project
website with code and additional resources is available at
https://inverse-llava.github.io.

</details>


### [79] [Standardization of Neuromuscular Reflex Analysis -- Role of Fine-Tuned Vision-Language Model Consortium and OpenAI gpt-oss Reasoning LLM Enabled Decision Support System](https://arxiv.org/abs/2508.12473)
*Eranga Bandara,Ross Gore,Sachin Shetty,Ravi Mukkamala,Christopher Rhea,Atmaram Yarlagadda,Shaifali Kaushik,L. H. M. P. De Silva,Andriy Maznychenko,Inna Sokolowska,Amin Hass,Kasun De Zoysa*

Main category: cs.CV

TL;DR: A hybrid AI system combining fine-tuned vision-language models and reasoning LLMs for automated H-reflex EMG waveform interpretation, improving standardization and accuracy in neuromuscular diagnostics.


<details>
  <summary>Details</summary>
Motivation: Traditional H-reflex EMG analysis suffers from variability and interpretation bias among clinicians, limiting reliability and standardization in sports science and clinical neurology.

Method: Fine-tuned multiple VLMs on curated H-reflex EMG waveform images with clinical annotations, then aggregated outputs using consensus method and refined with specialized reasoning LLM for transparent decision support.

Result: The hybrid system delivers highly accurate, consistent, and interpretable H-reflex assessments, significantly advancing automation and standardization of neuromuscular diagnostics.

Conclusion: First integration of fine-tuned VLM consortium with reasoning LLM for image-based H-reflex analysis, laying foundation for next-generation AI-assisted neuromuscular assessment platforms.

Abstract: Accurate assessment of neuromuscular reflexes, such as the H-reflex, plays a
critical role in sports science, rehabilitation, and clinical neurology.
Traditional analysis of H-reflex EMG waveforms is subject to variability and
interpretation bias among clinicians and researchers, limiting reliability and
standardization. To address these challenges, we propose a Fine-Tuned
Vision-Language Model (VLM) Consortium and a reasoning Large-Language Model
(LLM)-enabled Decision Support System for automated H-reflex waveform
interpretation and diagnosis. Our approach leverages multiple VLMs, each
fine-tuned on curated datasets of H-reflex EMG waveform images annotated with
clinical observations, recovery timelines, and athlete metadata. These models
are capable of extracting key electrophysiological features and predicting
neuromuscular states, including fatigue, injury, and recovery, directly from
EMG images and contextual metadata. Diagnostic outputs from the VLM consortium
are aggregated using a consensus-based method and refined by a specialized
reasoning LLM, which ensures robust, transparent, and explainable decision
support for clinicians and sports scientists. The end-to-end platform
orchestrates seamless communication between the VLM ensemble and the reasoning
LLM, integrating prompt engineering strategies and automated reasoning
workflows using LLM Agents. Experimental results demonstrate that this hybrid
system delivers highly accurate, consistent, and interpretable H-reflex
assessments, significantly advancing the automation and standardization of
neuromuscular diagnostics. To our knowledge, this work represents the first
integration of a fine-tuned VLM consortium with a reasoning LLM for image-based
H-reflex analysis, laying the foundation for next-generation AI-assisted
neuromuscular assessment and athlete monitoring platforms.

</details>


### [80] [Skin Cancer Classification: Hybrid CNN-Transformer Models with KAN-Based Fusion](https://arxiv.org/abs/2508.12484)
*Shubhi Agarwal,Amulya Kumar Mahto*

Main category: cs.CV

TL;DR: Hybrid CNN-Transformer models with Convolutional Kolmogorov-Arnold Network (CKAN) achieve state-of-the-art performance in skin cancer classification across multiple datasets, demonstrating effective integration of local spatial features and global contextual information.


<details>
  <summary>Details</summary>
Motivation: Skin cancer classification requires precise differentiation between malignant and non-malignant lesions for early diagnosis. Existing methods need improved feature representation and generalization across diverse datasets with varying distributions and class imbalances.

Method: Sequential and Parallel Hybrid CNN-Transformer models integrated with Convolutional Kolmogorov-Arnold Network (CKAN), using transfer learning and extensive data augmentation. CNNs extract local spatial features, Transformers model global dependencies, and CKAN facilitates nonlinear feature fusion.

Result: Achieved 92.81% accuracy and 92.47% F1-score on HAM10000, 97.83% accuracy and 97.83% F1-score on PAD-UFES, and 91.17% accuracy with 91.79% F1-score on BCN20000 dataset, demonstrating competitive performance and strong generalization.

Conclusion: Hybrid CNN-Transformer architectures with CKAN effectively capture both spatial and contextual features, enhancing classification performance. The approach highlights the importance of feature representation and model design for robust medical image classification.

Abstract: Skin cancer classification is a crucial task in medical image analysis, where
precise differentiation between malignant and non-malignant lesions is
essential for early diagnosis and treatment. In this study, we explore
Sequential and Parallel Hybrid CNN-Transformer models with Convolutional
Kolmogorov-Arnold Network (CKAN). Our approach integrates transfer learning and
extensive data augmentation, where CNNs extract local spatial features,
Transformers model global dependencies, and CKAN facilitates nonlinear feature
fusion for improved representation learning. To assess generalization, we
evaluate our models on multiple benchmark datasets (HAM10000,BCN20000 and
PAD-UFES) under varying data distributions and class imbalances. Experimental
results demonstrate that hybrid CNN-Transformer architectures effectively
capture both spatial and contextual features, leading to improved
classification performance. Additionally, the integration of CKAN enhances
feature fusion through learnable activation functions, yielding more
discriminative representations. Our proposed approach achieves competitive
performance in skin cancer classification, demonstrating 92.81% accuracy and
92.47% F1-score on the HAM10000 dataset, 97.83% accuracy and 97.83% F1-score on
the PAD-UFES dataset, and 91.17% accuracy with 91.79% F1- score on the BCN20000
dataset highlighting the effectiveness and generalizability of our model across
diverse datasets. This study highlights the significance of feature
representation and model design in advancing robust and accurate medical image
classification.

</details>


### [81] [Design and Validation of a Responsible Artificial Intelligence-based System for the Referral of Diabetic Retinopathy Patients](https://arxiv.org/abs/2508.12506)
*E. Ulises Moya-Sánchez,Abraham Sánchez-Perez,Raúl Nanclares Da Veiga,Alejandro Zarate-Macías,Edgar Villareal,Alejandro Sánchez-Montes,Edtna Jauregui-Ulloa,Héctor Moreno,Ulises Cortés*

Main category: cs.CV

TL;DR: RAIS-DR is a responsible AI system for diabetic retinopathy screening that outperforms FDA-approved EyeArt system with 5-12% higher F1 scores, 6-19% better accuracy, and demonstrates equitable performance across demographic groups.


<details>
  <summary>Details</summary>
Motivation: Diabetic Retinopathy is a leading cause of vision loss, but early detection is hindered by shortage of specialists and low-quality data with biases in AI systems. There's a need for ethical AI solutions that address healthcare disparities.

Method: Developed RAIS-DR system incorporating ethical principles across AI lifecycle, using efficient convolutional models for preprocessing, quality assessment, and three specialized DR classification models. Evaluated against FDA-approved EyeArt on local dataset of 1,046 unseen patients.

Result: RAIS-DR showed significant improvements: F1 scores increased by 5-12%, accuracy by 6-19%, specificity by 10-20%. Fairness metrics (Disparate Impact, Equal Opportunity Difference) indicated equitable performance across demographic subgroups.

Conclusion: RAIS-DR is a robust and ethically aligned solution for DR screening that can reduce healthcare disparities, with code and weights publicly available under RAIL license.

Abstract: Diabetic Retinopathy (DR) is a leading cause of vision loss in working-age
individuals. Early detection of DR can reduce the risk of vision loss by up to
95%, but a shortage of retinologists and challenges in timely examination
complicate detection. Artificial Intelligence (AI) models using retinal fundus
photographs (RFPs) offer a promising solution. However, adoption in clinical
settings is hindered by low-quality data and biases that may lead AI systems to
learn unintended features. To address these challenges, we developed RAIS-DR, a
Responsible AI System for DR screening that incorporates ethical principles
across the AI lifecycle. RAIS-DR integrates efficient convolutional models for
preprocessing, quality assessment, and three specialized DR classification
models. We evaluated RAIS-DR against the FDA-approved EyeArt system on a local
dataset of 1,046 patients, unseen by both systems. RAIS-DR demonstrated
significant improvements, with F1 scores increasing by 5-12%, accuracy by
6-19%, and specificity by 10-20%. Additionally, fairness metrics such as
Disparate Impact and Equal Opportunity Difference indicated equitable
performance across demographic subgroups, underscoring RAIS-DR's potential to
reduce healthcare disparities. These results highlight RAIS-DR as a robust and
ethically aligned solution for DR screening in clinical settings. The code,
weights of RAIS-DR are available at
https://gitlab.com/inteligencia-gubernamental-jalisco/jalisco-retinopathy with
RAIL.

</details>


### [82] [LangVision-LoRA-NAS: Neural Architecture Search for Variable LoRA Rank in Vision Language Models](https://arxiv.org/abs/2508.12512)
*Krishna Teja Chitty-Venkata,Murali Emani,Venkatram Vishwanath*

Main category: cs.CV

TL;DR: LangVision-LoRA-NAS integrates Neural Architecture Search with LoRA to dynamically optimize Vision Language Models for variable-rank adaptation, improving performance while reducing computational costs.


<details>
  <summary>Details</summary>
Motivation: Current LoRA implementations use fixed ranks, limiting flexibility and efficiency across diverse multimodal tasks. This paper addresses the need for adaptive rank configurations tailored to specific Vision Language Model tasks.

Method: The framework combines Neural Architecture Search with Low-Rank Adaptation to dynamically search for optimal LoRA rank configurations for VLMs. It uses LLaMA-3.2-11B model and experiments on multiple datasets to find the best balance between performance and efficiency.

Result: Extensive experiments show notable improvement in model performance while reducing fine-tuning costs. The approach demonstrates effectiveness across various multimodal tasks with optimized computational efficiency.

Conclusion: LangVision-LoRA-NAS provides an effective framework for optimizing Vision Language Models through dynamic rank adaptation, offering both performance gains and computational savings compared to fixed-rank LoRA approaches.

Abstract: Vision Language Models (VLMs) integrate visual and text modalities to enable
multimodal understanding and generation. These models typically combine a
Vision Transformer (ViT) as an image encoder and a Large Language Model (LLM)
for text generation. LoRA (Low-Rank Adaptation) is an efficient fine-tuning
method to adapt pre-trained models to new tasks by introducing low-rank updates
to their weights. While LoRA has emerged as a powerful technique for
fine-tuning large models by introducing low-rank updates, current
implementations assume a fixed rank, potentially limiting flexibility and
efficiency across diverse tasks. This paper introduces
\textit{LangVision-LoRA-NAS}, a novel framework that integrates Neural
Architecture Search (NAS) with LoRA to optimize VLMs for variable-rank
adaptation. Our approach leverages NAS to dynamically search for the optimal
LoRA rank configuration tailored to specific multimodal tasks, balancing
performance and computational efficiency. Through extensive experiments using
the LLaMA-3.2-11B model on several datasets, LangVision-LoRA-NAS demonstrates
notable improvement in model performance while reducing fine-tuning costs. Our
Base and searched fine-tuned models on LLaMA-3.2-11B-Vision-Instruct can be
found
\href{https://huggingface.co/collections/krishnateja95/llama-32-11b-vision-instruct-langvision-lora-nas-6786cac480357a6a6fcc59ee}{\textcolor{blue}{here}}
and the code for LangVision-LoRA-NAS can be found
\href{https://github.com/krishnateja95/LangVision-NAS}{\textcolor{blue}{here}}.

</details>


### [83] [An Initial Study of Bird's-Eye View Generation for Autonomous Vehicles using Cross-View Transformers](https://arxiv.org/abs/2508.12520)
*Felipe Carlos dos Santos,Eric Aislan Antonelo,Gustavo Claudio Karl Couto*

Main category: cs.CV

TL;DR: Cross-View Transformers effectively map camera images to Bird's-Eye View maps for autonomous driving, showing strong generalization to unseen environments with optimal performance using 4 cameras and L1 loss.


<details>
  <summary>Details</summary>
Motivation: Bird's-Eye View maps provide crucial top-down perception for autonomous driving, but learning to generate accurate BEV maps from camera inputs remains challenging, especially for generalization to new environments.

Method: Used Cross-View Transformers (CVT) to map camera images to three BEV channels (road, lane markings, planned trajectory) using a realistic urban driving simulator. Tested different camera layouts and compared focal vs L1 loss formulations.

Result: A four-camera CVT trained with L1 loss achieved the most robust performance when tested in unseen towns, demonstrating strong generalization capabilities from training data collected in only one town.

Conclusion: Cross-View Transformers show significant promise for generating reasonably accurate BEV maps from camera inputs, with particular effectiveness in generalization to novel driving environments using optimal camera configuration and loss function.

Abstract: Bird's-Eye View (BEV) maps provide a structured, top-down abstraction that is
crucial for autonomous-driving perception. In this work, we employ Cross-View
Transformers (CVT) for learning to map camera images to three BEV's channels -
road, lane markings, and planned trajectory - using a realistic simulator for
urban driving. Our study examines generalization to unseen towns, the effect of
different camera layouts, and two loss formulations (focal and L1). Using
training data from only a town, a four-camera CVT trained with the L1 loss
delivers the most robust test performance, evaluated in a new town. Overall,
our results underscore CVT's promise for mapping camera inputs to reasonably
accurate BEV maps.

</details>


### [84] [MuSACo: Multimodal Subject-Specific Selection and Adaptation for Expression Recognition with Co-Training](https://arxiv.org/abs/2508.12522)
*Muhammad Osama Zeeshan,Natacha Gillet,Alessandro Lameiras Koerich,Marco Pedersoli,Francois Bremond,Eric Granger*

Main category: cs.CV

TL;DR: MuSACo is a multi-modal subject-specific adaptation method for expression recognition that uses co-training to leverage complementary information across modalities and source domains, outperforming existing methods on challenging datasets.


<details>
  <summary>Details</summary>
Motivation: Current MSDA methods for personalized expression recognition often overlook multimodal information or blend sources into a single domain, failing to capture unique subject-specific characteristics crucial for applications like digital health assessment.

Method: MuSACo uses co-training to select relevant source subjects, generates pseudo-labels using dominant modality for class-aware learning, employs class-agnostic loss for less confident samples, and aligns source features while combining only confident target features across modalities.

Result: Experimental results on BioVid and StressID datasets show MuSACo outperforms UDA (blending) and state-of-the-art MSDA methods.

Conclusion: MuSACo effectively addresses limitations of existing MSDA approaches by preserving subject diversity and explicitly capturing subject-specific characteristics through multimodal co-training, making it particularly suitable for affective computing applications in digital health.

Abstract: Personalized expression recognition (ER) involves adapting a machine learning
model to subject-specific data for improved recognition of expressions with
considerable interpersonal variability. Subject-specific ER can benefit
significantly from multi-source domain adaptation (MSDA) methods, where each
domain corresponds to a specific subject, to improve model accuracy and
robustness. Despite promising results, state-of-the-art MSDA approaches often
overlook multimodal information or blend sources into a single domain, limiting
subject diversity and failing to explicitly capture unique subject-specific
characteristics. To address these limitations, we introduce MuSACo, a
multi-modal subject-specific selection and adaptation method for ER based on
co-training. It leverages complementary information across multiple modalities
and multiple source domains for subject-specific adaptation. This makes MuSACo
particularly relevant for affective computing applications in digital health,
such as patient-specific assessment for stress or pain, where subject-level
nuances are crucial. MuSACo selects source subjects relevant to the target and
generates pseudo-labels using the dominant modality for class-aware learning,
in conjunction with a class-agnostic loss to learn from less confident target
samples. Finally, source features from each modality are aligned, while only
confident target features are combined. Our experimental results on challenging
multimodal ER datasets: BioVid and StressID, show that MuSACo can outperform
UDA (blending) and state-of-the-art MSDA methods.

</details>


### [85] [REVEAL -- Reasoning and Evaluation of Visual Evidence through Aligned Language](https://arxiv.org/abs/2508.12543)
*Ipsita Praharaj,Yukta Butala,Yash Butala*

Main category: cs.CV

TL;DR: REVEAL is a prompt-driven framework using vision-language models for image forgery detection that combines holistic scene evaluation and region-wise anomaly detection, showing strong generalization across different manipulation domains.


<details>
  <summary>Details</summary>
Motivation: The rapid advancement of generative models has intensified challenges in detecting visual forgeries, requiring robust frameworks that can generalize across domains while providing both detection and reasoning capabilities.

Method: Proposes REVEAL framework that uses large vision-language models for semantic alignment. Two approaches: (1) Holistic scene-level evaluation analyzing physics, semantics, perspective, and realism, and (2) Region-wise anomaly detection by splitting images into regions for individual analysis.

Result: Experiments conducted across multiple domains (Photoshop, DeepFake, AIGC editing) show competitive performance against baselines, with analysis of the reasoning capabilities provided by the models.

Conclusion: The prompt-driven visual reasoning approach using vision-language models provides an effective framework for generalized image forgery detection with interpretable reasoning across diverse manipulation types.

Abstract: The rapid advancement of generative models has intensified the challenge of
detecting and interpreting visual forgeries, necessitating robust frameworks
for image forgery detection while providing reasoning as well as localization.
While existing works approach this problem using supervised training for
specific manipulation or anomaly detection in the embedding space,
generalization across domains remains a challenge. We frame this problem of
forgery detection as a prompt-driven visual reasoning task, leveraging the
semantic alignment capabilities of large vision-language models. We propose a
framework, `REVEAL` (Reasoning and Evaluation of Visual Evidence through
Aligned Language), that incorporates generalized guidelines. We propose two
tangential approaches - (1) Holistic Scene-level Evaluation that relies on the
physics, semantics, perspective, and realism of the image as a whole and (2)
Region-wise anomaly detection that splits the image into multiple regions and
analyzes each of them. We conduct experiments over datasets from different
domains (Photoshop, DeepFake and AIGC editing). We compare the Vision Language
Models against competitive baselines and analyze the reasoning provided by
them.

</details>


### [86] [Structure-preserving Feature Alignment for Old Photo Colorization](https://arxiv.org/abs/2508.12570)
*Yingxue Pang,Xin Jin,Jun Fu,Zhibo Chen*

Main category: cs.CV

TL;DR: SFAC is a novel CNN-based algorithm for old photo colorization that requires only two images (reference and target), uses feature alignment for semantic color transfer, and incorporates structure-preserving mechanisms to prevent distortions.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning colorization methods struggle with old photos due to lack of ground truth and domain gap between natural gray images and historical photos. Big data approaches are impractical for this specific domain.

Method: SFAC uses structure-preserving feature alignment with semantic correspondence between reference and target images. It employs feature distribution alignment loss and structure-preserving mechanisms including perceptual constraints at feature level and frozen-updated pyramid at pixel level.

Result: Extensive experiments show SFAC effectively colorizes old photos with both qualitative and quantitative improvements, overcoming the domain gap problem without requiring large datasets.

Conclusion: The proposed SFAC method successfully addresses old photo colorization by eliminating big data dependency, establishing robust semantic correspondence, and preserving structure while transferring colors from reference images.

Abstract: Deep learning techniques have made significant advancements in
reference-based colorization by training on large-scale datasets. However,
directly applying these methods to the task of colorizing old photos is
challenging due to the lack of ground truth and the notorious domain gap
between natural gray images and old photos. To address this issue, we propose a
novel CNN-based algorithm called SFAC, i.e., Structure-preserving Feature
Alignment Colorizer. SFAC is trained on only two images for old photo
colorization, eliminating the reliance on big data and allowing direct
processing of the old photo itself to overcome the domain gap problem. Our
primary objective is to establish semantic correspondence between the two
images, ensuring that semantically related objects have similar colors. We
achieve this through a feature distribution alignment loss that remains robust
to different metric choices. However, utilizing robust semantic correspondence
to transfer color from the reference to the old photo can result in inevitable
structure distortions. To mitigate this, we introduce a structure-preserving
mechanism that incorporates a perceptual constraint at the feature level and a
frozen-updated pyramid at the pixel level. Extensive experiments demonstrate
the effectiveness of our method for old photo colorization, as confirmed by
qualitative and quantitative metrics.

</details>


### [87] [Foundation Model for Skeleton-Based Human Action Understanding](https://arxiv.org/abs/2508.12586)
*Hongsong Wang,Wanjiang Weng,Junbo Wang,Fang Zhao,Guo-Sen Xie,Xin Geng,Liang Wang*

Main category: cs.CV

TL;DR: USDRL is a unified skeleton foundation model that achieves state-of-the-art performance across 25 benchmarks and 9 action understanding tasks through dense spatio-temporal encoding, multi-grained feature decorrelation, and multi-perspective consistency training.


<details>
  <summary>Details</summary>
Motivation: Existing skeleton-based action understanding methods lack scalability and generalization for diverse tasks, with no existing foundation model that can be adapted to a wide range of action understanding applications.

Method: Transformer-based Dense Spatio-Temporal Encoder (DSTE) with parallel streams for temporal and spatial features, Multi-Grained Feature Decorrelation (MG-FD) across temporal/spatial/instance domains, and Multi-Perspective Consistency Training (MPCT) with multi-view and multi-modal self-supervision.

Result: Significantly outperforms current state-of-the-art methods on 25 benchmarks across 9 skeleton-based action understanding tasks covering coarse, dense, and transferred prediction.

Conclusion: USDRL serves as a foundational model that broadens research scope in skeleton-based action understanding and encourages more attention to dense prediction tasks.

Abstract: Human action understanding serves as a foundational pillar in the field of
intelligent motion perception. Skeletons serve as a modality- and
device-agnostic representation for human modeling, and skeleton-based action
understanding has potential applications in humanoid robot control and
interaction. \RED{However, existing works often lack the scalability and
generalization required to handle diverse action understanding tasks. There is
no skeleton foundation model that can be adapted to a wide range of action
understanding tasks}. This paper presents a Unified Skeleton-based Dense
Representation Learning (USDRL) framework, which serves as a foundational model
for skeleton-based human action understanding. USDRL consists of a
Transformer-based Dense Spatio-Temporal Encoder (DSTE), Multi-Grained Feature
Decorrelation (MG-FD), and Multi-Perspective Consistency Training (MPCT). The
DSTE module adopts two parallel streams to learn temporal dynamic and spatial
structure features. The MG-FD module collaboratively performs feature
decorrelation across temporal, spatial, and instance domains to reduce
dimensional redundancy and enhance information extraction. The MPCT module
employs both multi-view and multi-modal self-supervised consistency training.
The former enhances the learning of high-level semantics and mitigates the
impact of low-level discrepancies, while the latter effectively facilitates the
learning of informative multimodal features. We perform extensive experiments
on 25 benchmarks across across 9 skeleton-based action understanding tasks,
covering coarse prediction, dense prediction, and transferred prediction. Our
approach significantly outperforms the current state-of-the-art methods. We
hope that this work would broaden the scope of research in skeleton-based
action understanding and encourage more attention to dense prediction tasks.

</details>


### [88] [Multimodal Chain of Continuous Thought for Latent-Space Reasoning in Vision-Language Models](https://arxiv.org/abs/2508.12587)
*Tan-Hanh Pham,Chris Ngo*

Main category: cs.CV

TL;DR: MCOUT introduces continuous latent space reasoning for multimodal models instead of language-based CoT, achieving up to 8.23% accuracy gains on benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing language-based reasoning methods like Chain-of-Thought are suboptimal for multimodal contexts as they struggle to dynamically align audio, visual, and textual information.

Method: Proposes Multimodal Chain of Continuous Thought (MCOUT) with reasoning in joint latent space using continuous hidden vectors. Two variants: MCOUT-Base reuses language model's last hidden state, MCOUT-Multi uses multimodal latent attention for cross-modal alignment.

Result: Experiments on MMMU, ScienceQA, and MMStar benchmarks show consistent improvements with up to 8.23% accuracy gains over baselines and up to 8.27% BLEU score improvements across multiple-choice and open-ended tasks.

Conclusion: Latent continuous reasoning is a promising direction for advancing multimodal models beyond language-bound approaches, offering scalable framework for human-like reflective multimodal inference.

Abstract: Many reasoning techniques for large multimodal models adapt language model
approaches, such as Chain-of-Thought (CoT) prompting, which express reasoning
as word sequences. While effective for text, these methods are suboptimal for
multimodal contexts, struggling to align audio, visual, and textual information
dynamically. To explore an alternative paradigm, we propose the Multimodal
Chain of Continuous Thought (MCOUT), which enables reasoning directly in a
joint latent space rather than in natural language. In MCOUT, the reasoning
state is represented as a continuous hidden vector, iteratively refined and
aligned with visual and textual embeddings, inspired by human reflective
cognition. We develop two variants: MCOUT-Base, which reuses the language
model`s last hidden state as the continuous thought for iterative reasoning,
and MCOUT-Multi, which integrates multimodal latent attention to strengthen
cross-modal alignment between visual and textual features. Experiments on
benchmarks including MMMU, ScienceQA, and MMStar show that MCOUT consistently
improves multimodal reasoning, yielding up to 8.23% accuracy gains over strong
baselines and improving BLEU scores up to 8.27% across multiple-choice and
open-ended tasks. These findings highlight latent continuous reasoning as a
promising direction for advancing LMMs beyond language-bound CoT, offering a
scalable framework for human-like reflective multimodal inference. Code is
available at https://github.com/Hanhpt23/OmniMod.

</details>


### [89] [ViLaD: A Large Vision Language Diffusion Framework for End-to-End Autonomous Driving](https://arxiv.org/abs/2508.12603)
*Can Cui,Yupeng Zhou,Juntong Peng,Sung-Yeon Park,Zichong Yang,Prashanth Sankaranarayanan,Jiaru Zhang,Ruqi Zhang,Ziran Wang*

Main category: cs.CV

TL;DR: ViLaD is a novel Large Vision Language Diffusion framework that replaces autoregressive VLMs for autonomous driving, enabling parallel generation of driving decisions with reduced latency and bidirectional reasoning capabilities.


<details>
  <summary>Details</summary>
Motivation: Autoregressive Vision Language Models have high inference latency and cannot perform bidirectional reasoning, making them unsuitable for safety-critical autonomous driving applications that require real-time decision making.

Method: Uses a masked diffusion model that enables parallel generation of entire driving decision sequences, supports bidirectional reasoning (considering both past and future simultaneously), and progressive easy-first generation for iterative decision improvement.

Result: Outperforms state-of-the-art autoregressive VLM baselines on nuScenes dataset in both planning accuracy and inference speed, achieving near-zero failure rate. Successfully deployed on real autonomous vehicle for interactive parking task.

Conclusion: ViLaD represents a paradigm shift for end-to-end autonomous driving, offering practical viability with reduced computational latency and improved decision quality through parallel generation and bidirectional reasoning capabilities.

Abstract: End-to-end autonomous driving systems built on Vision Language Models (VLMs)
have shown significant promise, yet their reliance on autoregressive
architectures introduces some limitations for real-world applications. The
sequential, token-by-token generation process of these models results in high
inference latency and cannot perform bidirectional reasoning, making them
unsuitable for dynamic, safety-critical environments. To overcome these
challenges, we introduce ViLaD, a novel Large Vision Language Diffusion (LVLD)
framework for end-to-end autonomous driving that represents a paradigm shift.
ViLaD leverages a masked diffusion model that enables parallel generation of
entire driving decision sequences, significantly reducing computational
latency. Moreover, its architecture supports bidirectional reasoning, allowing
the model to consider both past and future simultaneously, and supports
progressive easy-first generation to iteratively improve decision quality. We
conduct comprehensive experiments on the nuScenes dataset, where ViLaD
outperforms state-of-the-art autoregressive VLM baselines in both planning
accuracy and inference speed, while achieving a near-zero failure rate.
Furthermore, we demonstrate the framework's practical viability through a
real-world deployment on an autonomous vehicle for an interactive parking task,
confirming its effectiveness and soundness for practical applications.

</details>


### [90] [ViDA-UGC: Detailed Image Quality Analysis via Visual Distortion Assessment for UGC Images](https://arxiv.org/abs/2508.12605)
*Wenjie Liao,Jieyu Yuan,Yifang Xu,Chunle Guo,Zilong Zhang,Jihong Li,Jiachen Fu,Haotian Fan,Tao Li,Junhui Cui,Chongyi Li*

Main category: cs.CV

TL;DR: This paper introduces ViDA-UGC, the first large-scale visual distortion assessment dataset for user-generated content images, featuring fine-grained quality annotations and a Chain-of-Thought framework that enables explainable image quality assessment surpassing GPT-4o performance.


<details>
  <summary>Details</summary>
Motivation: Current explainable IQA methods inadequately evaluate both UGC and AIGC images using the same distortion criteria, and lack detailed quality analysis for monitoring image quality and guiding image restoration.

Method: Created ViDA-UGC dataset with 11K images using human annotation and CoT assessment framework to guide GPT-4o in generating quality descriptions. Also developed ViDA-UGC-Bench benchmark with 476 images and 6,149 QA pairs professionally validated.

Result: Experimental results show the ViDA-UGC dataset and CoT framework consistently enhance various image quality analysis abilities across multiple base MLLMs, even surpassing GPT-4o performance.

Conclusion: The proposed ViDA-UGC dataset and CoT framework provide an effective solution for explainable image quality assessment of user-generated content, enabling better quality monitoring and restoration guidance.

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have introduced a
paradigm shift for Image Quality Assessment (IQA) from unexplainable image
quality scoring to explainable IQA, demonstrating practical applications like
quality control and optimization guidance. However, current explainable IQA
methods not only inadequately use the same distortion criteria to evaluate both
User-Generated Content (UGC) and AI-Generated Content (AIGC) images, but also
lack detailed quality analysis for monitoring image quality and guiding image
restoration. In this study, we establish the first large-scale Visual
Distortion Assessment Instruction Tuning Dataset for UGC images, termed
ViDA-UGC, which comprises 11K images with fine-grained quality grounding,
detailed quality perception, and reasoning quality description data. This
dataset is constructed through a distortion-oriented pipeline, which involves
human subject annotation and a Chain-of-Thought (CoT) assessment framework.
This framework guides GPT-4o to generate quality descriptions by identifying
and analyzing UGC distortions, which helps capturing rich low-level visual
features that inherently correlate with distortion patterns. Moreover, we
carefully select 476 images with corresponding 6,149 question answer pairs from
ViDA-UGC and invite a professional team to ensure the accuracy and quality of
GPT-generated information. The selected and revised data further contribute to
the first UGC distortion assessment benchmark, termed ViDA-UGC-Bench.
Experimental results demonstrate the effectiveness of the ViDA-UGC and CoT
framework for consistently enhancing various image quality analysis abilities
across multiple base MLLMs on ViDA-UGC-Bench and Q-Bench, even surpassing
GPT-4o.

</details>


### [91] [OpenMoCap: Rethinking Optical Motion Capture under Real-world Occlusion](https://arxiv.org/abs/2508.12610)
*Chen Qian,Danyang Li,Xinran Yu,Zheng Yang,Qiang Ma*

Main category: cs.CV

TL;DR: OpenMoCap introduces a novel motion-solving model and CMU-Occlu dataset to address severe performance degradation in optical motion capture systems under large-scale marker occlusions, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Current optical motion capture systems suffer severely from performance degradation under large-scale marker occlusions common in real-world applications, due to lack of realistic training datasets and training strategies for long-range marker dependencies.

Method: Created CMU-Occlu dataset using ray tracing to simulate realistic occlusion patterns, and developed OpenMoCap model with marker-joint chain inference mechanism for simultaneous optimization and deep constraint construction between markers and joints.

Result: OpenMoCap consistently outperforms competing methods across diverse scenarios, and the CMU-Occlu dataset enables future robust motion solving studies.

Conclusion: The proposed solution effectively addresses marker occlusion challenges in motion capture, with OpenMoCap integrated into practical MoSen MoCap system and code publicly released for community use.

Abstract: Optical motion capture is a foundational technology driving advancements in
cutting-edge fields such as virtual reality and film production. However,
system performance suffers severely under large-scale marker occlusions common
in real-world applications. An in-depth analysis identifies two primary
limitations of current models: (i) the lack of training datasets accurately
reflecting realistic marker occlusion patterns, and (ii) the absence of
training strategies designed to capture long-range dependencies among markers.
To tackle these challenges, we introduce the CMU-Occlu dataset, which
incorporates ray tracing techniques to realistically simulate practical marker
occlusion patterns. Furthermore, we propose OpenMoCap, a novel motion-solving
model designed specifically for robust motion capture in environments with
significant occlusions. Leveraging a marker-joint chain inference mechanism,
OpenMoCap enables simultaneous optimization and construction of deep
constraints between markers and joints. Extensive comparative experiments
demonstrate that OpenMoCap consistently outperforms competing methods across
diverse scenarios, while the CMU-Occlu dataset opens the door for future
studies in robust motion solving. The proposed OpenMoCap is integrated into the
MoSen MoCap system for practical deployment. The code is released at:
https://github.com/qianchen214/OpenMoCap.

</details>


### [92] [WIPES: Wavelet-based Visual Primitives](https://arxiv.org/abs/2508.12615)
*Wenhao Zhang,Hao Zhu,Delong Wu,Di Kang,Linchao Bao,Zhan Ma,Xun Cao*

Main category: cs.CV

TL;DR: WIPES is a wavelet-based visual primitive that achieves high-quality rendering with fast inference by leveraging wavelet spatial-frequency localization, outperforming both INR-based methods in speed and Gaussian-based representations in quality.


<details>
  <summary>Details</summary>
Motivation: Existing visual representations suffer from spectrum loss or slow rendering due to reliance on frequency guidance or complex neural decoding. There's a need for a continuous visual representation that offers flexible frequency modulation and fast rendering speed.

Method: Proposes WIPES, a universal wavelet-based visual primitive that builds on wavelet spatial-frequency localization advantages to capture both low and high frequency details. Also develops a wavelet-based differentiable rasterizer for fast visual rendering.

Result: Experimental results across various visual tasks (2D image representation, 5D static and 6D dynamic novel view synthesis) show WIPES offers higher rendering quality and faster inference than INR-based methods, and outperforms Gaussian-based representations in rendering quality.

Conclusion: WIPES serves as an effective visual primitive that addresses spectrum loss and slow rendering issues in existing representations, providing superior rendering performance across multiple visual tasks.

Abstract: Pursuing a continuous visual representation that offers flexible frequency
modulation and fast rendering speed has recently garnered increasing attention
in the fields of 3D vision and graphics. However, existing representations
often rely on frequency guidance or complex neural network decoding, leading to
spectrum loss or slow rendering. To address these limitations, we propose
WIPES, a universal Wavelet-based vIsual PrimitivES for representing
multi-dimensional visual signals. Building on the spatial-frequency
localization advantages of wavelets, WIPES effectively captures both the
low-frequency "forest" and the high-frequency "trees." Additionally, we develop
a wavelet-based differentiable rasterizer to achieve fast visual rendering.
Experimental results on various visual tasks, including 2D image
representation, 5D static and 6D dynamic novel view synthesis, demonstrate that
WIPES, as a visual primitive, offers higher rendering quality and faster
inference than INR-based methods, and outperforms Gaussian-based
representations in rendering quality.

</details>


### [93] [Creative4U: MLLMs-based Advertising Creative Image Selector with Comparative Reasoning](https://arxiv.org/abs/2508.12628)
*Yukang Lin,Xiang Zhang,Shichang Jia,Bowen Wan,Chenghan Fu,Xudong Ren,Yueran Liu,Wanxian Guan,Pengji Wang,Jian Xu,Bo Zheng,Baolin Liu*

Main category: cs.CV

TL;DR: Proposes Creative4U, an MLLM-based explainable creative image assessment system using comparative reasoning and reinforcement learning for e-commerce advertising.


<details>
  <summary>Details</summary>
Motivation: Advertisers can generate large quantities of creative images with AIGC but lack methods to assess quality and make explainable selections for optimal advertising performance.

Method: Uses multimodal LLMs to integrate assessment and selection into natural language generation. Creates CreativePair dataset with 8k annotated image pairs. Develops Creative4U with Chain-of-Thought supervised fine-tuning and Group Relative Policy Optimization reinforcement learning.

Result: Both offline and online experiments demonstrate effective creative image evaluation and selection capabilities.

Conclusion: The approach provides the first paradigm for explainable creative assessment and selection, advancing both research and industrial applications in e-commerce advertising.

Abstract: Creative image in advertising is the heart and soul of e-commerce platform.
An eye-catching creative image can enhance the shopping experience for users,
boosting income for advertisers and advertising revenue for platforms. With the
advent of AIGC technology, advertisers can produce large quantities of creative
images at minimal cost. However, they struggle to assess the creative quality
to select. Existing methods primarily focus on creative ranking, which fails to
address the need for explainable creative selection.
  In this work, we propose the first paradigm for explainable creative
assessment and selection. Powered by multimodal large language models (MLLMs),
our approach integrates the assessment and selection of creative images into a
natural language generation task. To facilitate this research, we construct
CreativePair, the first comparative reasoning-induced creative dataset
featuring 8k annotated image pairs, with each sample including a label
indicating which image is superior. Additionally, we introduce Creative4U
(pronounced Creative for You), a MLLMs-based creative selector that takes into
account users' interests. Through Reason-to-Select RFT, which includes
supervised fine-tuning with Chain-of-Thought (CoT-SFT) and Group Relative
Policy Optimization (GRPO) based reinforcement learning, Creative4U is able to
evaluate and select creative images accurately. Both offline and online
experiments demonstrate the effectiveness of our approach. Our code and dataset
will be made public to advance research and industrial applications.

</details>


### [94] [SpotVLM: Cloud-edge Collaborative Real-time VLM based on Context Transfer](https://arxiv.org/abs/2508.12638)
*Chen Qian,Xinran Yu,Zewen Huang,Danyang Li,Qiang Ma,Fan Dang,Xuan Ding,Guangyong Shang,Zheng Yang*

Main category: cs.CV

TL;DR: Proposes Context Transfer paradigm using delayed LVLM outputs as historical context to guide real-time SVLM inference, with SpotVLM implementation showing effectiveness across vision tasks.


<details>
  <summary>Details</summary>
Motivation: Existing cloud-edge collaborative architectures for VLMs fail to handle cloud latency fluctuations and don't leverage delayed but accurate LVLM responses for real-time applications like autonomous driving.

Method: Context Transfer paradigm treats delayed LVLM outputs as historical context to guide SVLM inference. SpotVLM implementation includes context replacement and visual focus modules to refine textual input and enhance visual grounding consistency.

Result: Extensive experiments on three real-time vision tasks across four datasets demonstrate the effectiveness of the proposed framework.

Conclusion: The new paradigm establishes groundwork for more effective and latency-aware collaboration strategies in future VLM systems.

Abstract: Vision-Language Models (VLMs) are increasingly deployed in real-time
applications such as autonomous driving and human-computer interaction, which
demand fast and reliable responses based on accurate perception. To meet these
requirements, existing systems commonly employ cloud-edge collaborative
architectures, such as partitioned Large Vision-Language Models (LVLMs) or task
offloading strategies between Large and Small Vision-Language Models (SVLMs).
However, these methods fail to accommodate cloud latency fluctuations and
overlook the full potential of delayed but accurate LVLM responses. In this
work, we propose a novel cloud-edge collaborative paradigm for VLMs, termed
Context Transfer, which treats the delayed outputs of LVLMs as historical
context to provide real-time guidance for SVLMs inference. Based on this
paradigm, we design SpotVLM, which incorporates both context replacement and
visual focus modules to refine historical textual input and enhance visual
grounding consistency. Extensive experiments on three real-time vision tasks
across four datasets demonstrate the effectiveness of the proposed framework.
The new paradigm lays the groundwork for more effective and latency-aware
collaboration strategies in future VLM systems.

</details>


### [95] [Synthesizing Accurate and Realistic T1-weighted Contrast-Enhanced MR Images using Posterior-Mean Rectified Flow](https://arxiv.org/abs/2508.12640)
*Bastian Brandstötter,Erich Kobler*

Main category: cs.CV

TL;DR: Two-stage PMRF pipeline synthesizes contrast-enhanced brain MRI from non-contrast inputs using patch-based 3D U-Net and rectified flow refinement, achieving significant FID/KID improvements while maintaining structural fidelity.


<details>
  <summary>Details</summary>
Motivation: Eliminate need for gadolinium-based contrast agents in MRI to reduce cost, scan time, environmental impact, and patient risks while maintaining diagnostic quality.

Method: Two-stage approach: 1) Patch-based 3D U-Net predicts voxel-wise posterior mean (MSE minimization), 2) Time-conditioned 3D rectified flow refines initial estimate to add realistic textures without compromising structure.

Result: Achieved axial FID of 12.46 and KID of 0.007 (68.7% lower FID than posterior mean) with volumetric MSE of 0.057 (27% higher than posterior mean) on 360 test volumes. Restores lesion margins and vascular details realistically.

Conclusion: Method effectively navigates perception-distortion trade-off for clinical deployment, providing realistic contrast-enhanced MRI synthesis without gadolinium agents.

Abstract: Contrast-enhanced (CE) T1-weighted MRI is central to neuro-oncologic
diagnosis but requires gadolinium-based agents, which add cost and scan time,
raise environmental concerns, and may pose risks to patients. In this work, we
propose a two-stage Posterior-Mean Rectified Flow (PMRF) pipeline for
synthesizing volumetric CE brain MRI from non-contrast inputs. First, a
patch-based 3D U-Net predicts the voxel-wise posterior mean (minimizing MSE).
Then, this initial estimate is refined by a time-conditioned 3D rectified flow
to incorporate realistic textures without compromising structural fidelity. We
train this model on a multi-institutional collection of paired pre- and
post-contrast T1w volumes (BraTS 2023-2025). On a held-out test set of 360
diverse volumes, our best refined outputs achieve an axial FID of $12.46$ and
KID of $0.007$ ($\sim 68.7\%$ lower FID than the posterior mean) while
maintaining low volumetric MSE of $0.057$ ($\sim 27\%$ higher than the
posterior mean). Qualitative comparisons confirm that our method restores
lesion margins and vascular details realistically, effectively navigating the
perception-distortion trade-off for clinical deployment.

</details>


### [96] [Learn Faster and Remember More: Balancing Exploration and Exploitation for Continual Test-time Adaptation](https://arxiv.org/abs/2508.12643)
*Pinci Yang,Peisong Wen,Ke Ma,Qianqian Xu*

Main category: cs.CV

TL;DR: A mean teacher framework called BEE that balances exploration (quick adaptation to new domains) and exploitation (retaining historical knowledge) in Continual Test-Time Adaptation using multi-level consistency regularization and complementary anchor replay.


<details>
  <summary>Details</summary>
Motivation: Existing CTTA methods struggle to balance rapid adaptation to new domains while retaining knowledge from previous domains. They focus on deep-layer adjustments which are inefficient for shallow feature domain shifts, and single models forget historical knowledge during adaptation.

Method: Proposes a mean teacher framework with Multi-level Consistency Regularization (MCR) to align intermediate features between student and teacher models for faster adaptation, and Complementary Anchor Replay (CAR) to reuse historical checkpoints for knowledge retention.

Result: Significantly outperforms state-of-the-art methods on several benchmarks, demonstrating effective balance between exploration and exploitation in CTTA tasks.

Conclusion: The proposed BEE framework successfully addresses the exploration-exploitation trade-off in CTTA through feature-level consistency regularization and historical knowledge reuse, achieving superior performance compared to existing methods.

Abstract: Continual Test-Time Adaptation (CTTA) aims to adapt a source pre-trained
model to continually changing target domains during inference. As a fundamental
principle, an ideal CTTA method should rapidly adapt to new domains
(exploration) while retaining and exploiting knowledge from previously
encountered domains to handle similar domains in the future. Despite
significant advances, balancing exploration and exploitation in CTTA is still
challenging: 1) Existing methods focus on adjusting predictions based on
deep-layer outputs of neural networks. However, domain shifts typically affect
shallow features, which are inefficient to be adjusted from deep predictions,
leading to dilatory exploration; 2) A single model inevitably forgets knowledge
of previous domains during the exploration, making it incapable of exploiting
historical knowledge to handle similar future domains. To address these
challenges, this paper proposes a mean teacher framework that strikes an
appropriate Balance between Exploration and Exploitation (BEE) during the CTTA
process. For the former challenge, we introduce a Multi-level Consistency
Regularization (MCR) loss that aligns the intermediate features of the student
and teacher models, accelerating adaptation to the current domain. For the
latter challenge, we employ a Complementary Anchor Replay (CAR) mechanism to
reuse historical checkpoints (anchors), recovering complementary knowledge for
diverse domains. Experiments show that our method significantly outperforms
state-of-the-art methods on several benchmarks, demonstrating its effectiveness
for CTTA tasks.

</details>


### [97] [DyCrowd: Towards Dynamic Crowd Reconstruction from a Large-scene Video](https://arxiv.org/abs/2508.12644)
*Hao Wen,Hongbo Kang,Jian Ma,Jing Huang,Yuanwang Yang,Haozhe Lin,Yu-Kun Lai,Kun Li*

Main category: cs.CV

TL;DR: DyCrowd is a framework for spatio-temporally consistent 3D reconstruction of hundreds of individuals' poses, positions and shapes from large-scene videos, addressing occlusion challenges through group-guided motion optimization and VAE-based motion priors.


<details>
  <summary>Details</summary>
Motivation: Current 3D crowd reconstruction methods work from static images, lacking temporal consistency and struggling with occlusions. There's a need for robust reconstruction from video sequences in large scenes for applications like city surveillance and crowd analysis.

Method: Coarse-to-fine group-guided motion optimization strategy with VAE-based human motion prior and segment-level optimization. Uses collective crowd behavior to handle occlusions, with Asynchronous Motion Consistency (AMC) loss for temporal synchronization. Also created VirtualCrowd benchmark dataset.

Result: Achieves state-of-the-art performance in large-scene dynamic crowd reconstruction, demonstrating robust motion recovery even with temporal desynchronization and severe occlusions.

Conclusion: The proposed framework successfully addresses temporal consistency and occlusion challenges in 3D crowd reconstruction from videos, providing a comprehensive solution with both methodology and benchmark dataset for future research.

Abstract: 3D reconstruction of dynamic crowds in large scenes has become increasingly
important for applications such as city surveillance and crowd analysis.
However, current works attempt to reconstruct 3D crowds from a static image,
causing a lack of temporal consistency and inability to alleviate the typical
impact caused by occlusions. In this paper, we propose DyCrowd, the first
framework for spatio-temporally consistent 3D reconstruction of hundreds of
individuals' poses, positions and shapes from a large-scene video. We design a
coarse-to-fine group-guided motion optimization strategy for occlusion-robust
crowd reconstruction in large scenes. To address temporal instability and
severe occlusions, we further incorporate a VAE (Variational Autoencoder)-based
human motion prior along with a segment-level group-guided optimization. The
core of our strategy leverages collective crowd behavior to address long-term
dynamic occlusions. By jointly optimizing the motion sequences of individuals
with similar motion segments and combining this with the proposed Asynchronous
Motion Consistency (AMC) loss, we enable high-quality unoccluded motion
segments to guide the motion recovery of occluded ones, ensuring robust and
plausible motion recovery even in the presence of temporal desynchronization
and rhythmic inconsistencies. Additionally, in order to fill the gap of no
existing well-annotated large-scene video dataset, we contribute a virtual
benchmark dataset, VirtualCrowd, for evaluating dynamic crowd reconstruction
from large-scene videos. Experimental results demonstrate that the proposed
method achieves state-of-the-art performance in the large-scene dynamic crowd
reconstruction task. The code and dataset will be available for research
purposes.

</details>


### [98] [Stable Diffusion-Based Approach for Human De-Occlusion](https://arxiv.org/abs/2508.12663)
*Seung Young Noh,Ju Yong Chang*

Main category: cs.CV

TL;DR: A two-stage human de-occlusion method using diffusion models for mask completion with body priors and joint heatmaps, followed by RGB completion with Stable Diffusion and human-specific textual features, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Deep learning models struggle to accurately predict occluded regions of humans, and existing diffusion-based methods suffer from pixel-level degradation in visible areas due to latent space transformations.

Method: Two-stage approach: 1) Mask completion using diffusion-based human body prior and occluded joint heatmaps, 2) RGB completion using Stable Diffusion with decoder fine-tuning, conditioned on the reconstructed mask and enhanced with human-specific textual features from VQA and CLIP.

Result: Effectively reconstructs human appearances under severe occlusions, outperforms existing methods in both mask and RGB completion, and improves downstream tasks like 2D pose estimation and 3D human reconstruction.

Conclusion: The proposed method successfully addresses human de-occlusion by leveraging body structure priors and human-specific features, with the generated de-occluded images benefiting various human-centric applications.

Abstract: Humans can infer the missing parts of an occluded object by leveraging prior
knowledge and visible cues. However, enabling deep learning models to
accurately predict such occluded regions remains a challenging task.
De-occlusion addresses this problem by reconstructing both the mask and RGB
appearance. In this work, we focus on human de-occlusion, specifically
targeting the recovery of occluded body structures and appearances. Our
approach decomposes the task into two stages: mask completion and RGB
completion. The first stage leverages a diffusion-based human body prior to
provide a comprehensive representation of body structure, combined with
occluded joint heatmaps that offer explicit spatial cues about missing regions.
The reconstructed amodal mask then serves as a conditioning input for the
second stage, guiding the model on which areas require RGB reconstruction. To
further enhance RGB generation, we incorporate human-specific textual features
derived using a visual question answering (VQA) model and encoded via a CLIP
encoder. RGB completion is performed using Stable Diffusion, with decoder
fine-tuning applied to mitigate pixel-level degradation in visible regions -- a
known limitation of prior diffusion-based de-occlusion methods caused by latent
space transformations. Our method effectively reconstructs human appearances
even under severe occlusions and consistently outperforms existing methods in
both mask and RGB completion. Moreover, the de-occluded images generated by our
approach can improve the performance of downstream human-centric tasks, such as
2D pose estimation and 3D human reconstruction. The code will be made publicly
available.

</details>


### [99] [WP-CLIP: Leveraging CLIP to Predict Wölfflin's Principles in Visual Art](https://arxiv.org/abs/2508.12668)
*Abhijay Ghildyal,Li-Yun Wang,Feng Liu*

Main category: cs.CV

TL;DR: Fine-tuned CLIP model (WP-CLIP) can effectively predict Wölfflin's five stylistic principles in visual art, addressing limitations of pre-trained models.


<details>
  <summary>Details</summary>
Motivation: Existing metrics fail to predict all five Wölfflin's principles for formal art analysis, and pre-trained vision-language models like CLIP lack inherent understanding of nuanced stylistic elements in paintings.

Method: Fine-tuned CLIP on annotated datasets of real art images to predict scores for each of Wölfflin's five principles, creating WP-CLIP model.

Result: WP-CLIP successfully generalizes across diverse artistic styles, performing well on both GAN-generated paintings and the Pandora-18K art dataset.

Conclusion: Vision-language models show strong potential for automated art analysis when properly fine-tuned for specific stylistic evaluation tasks.

Abstract: W\"olfflin's five principles offer a structured approach to analyzing
stylistic variations for formal analysis. However, no existing metric
effectively predicts all five principles in visual art. Computationally
evaluating the visual aspects of a painting requires a metric that can
interpret key elements such as color, composition, and thematic choices. Recent
advancements in vision-language models (VLMs) have demonstrated their ability
to evaluate abstract image attributes, making them promising candidates for
this task. In this work, we investigate whether CLIP, pre-trained on
large-scale data, can understand and predict W\"olfflin's principles. Our
findings indicate that it does not inherently capture such nuanced stylistic
elements. To address this, we fine-tune CLIP on annotated datasets of real art
images to predict a score for each principle. We evaluate our model, WP-CLIP,
on GAN-generated paintings and the Pandora-18K art dataset, demonstrating its
ability to generalize across diverse artistic styles. Our results highlight the
potential of VLMs for automated art analysis.

</details>


### [100] [Vision-G1: Towards General Vision Language Reasoning with Multi-Domain Data Curation](https://arxiv.org/abs/2508.12680)
*Yuheng Zha,Kun Zhou,Yujia Wu,Yushu Wang,Jie Feng,Zhi Xu,Shibo Hao,Zhengzhong Liu,Eric P. Xing,Zhiting Hu*

Main category: cs.CV

TL;DR: Vision-G1 is a visual reasoning VLM trained on a comprehensive multi-domain dataset using influence-based data selection and multi-round RL, achieving SOTA performance across benchmarks and outperforming proprietary models.


<details>
  <summary>Details</summary>
Motivation: Current reasoning VLMs focus on narrow tasks like math/logic and struggle with generalization due to limited verifiable reward data and uncertain compatibility between domain-specific datasets.

Method: Built a comprehensive RL-ready dataset from 46 sources across 8 domains, used influence function-based data selection and difficulty filtering, trained with multi-round RL and data curriculum.

Result: Achieves state-of-the-art performance across various visual reasoning benchmarks, outperforms similar-sized VLMs and proprietary models like GPT-4o and Gemini-1.5 Flash.

Conclusion: The comprehensive multi-domain dataset combined with sophisticated data selection and RL training enables superior visual reasoning generalization across diverse domains.

Abstract: Despite their success, current training pipelines for reasoning VLMs focus on
a limited range of tasks, such as mathematical and logical reasoning. As a
result, these models face difficulties in generalizing their reasoning
capabilities to a wide range of domains, primarily due to the scarcity of
readily available and verifiable reward data beyond these narrowly defined
areas. Moreover, integrating data from multiple domains is challenging, as the
compatibility between domain-specific datasets remains uncertain. To address
these limitations, we build a comprehensive RL-ready visual reasoning dataset
from 46 data sources across 8 dimensions, covering a wide range of tasks such
as infographic, mathematical, spatial, cross-image, graphic user interface,
medical, common sense and general science. We propose an influence function
based data selection and difficulty based filtering strategy to identify
high-quality training samples from this dataset. Subsequently, we train the
VLM, referred to as Vision-G1, using multi-round RL with a data curriculum to
iteratively improve its visual reasoning capabilities. Our model achieves
state-of-the-art performance across various visual reasoning benchmarks,
outperforming similar-sized VLMs and even proprietary models like GPT-4o and
Gemini-1.5 Flash. The model, code and dataset are publicly available at
https://github.com/yuh-zha/Vision-G1.

</details>


### [101] [Refine-and-Contrast: Adaptive Instance-Aware BEV Representations for Multi-UAV Collaborative Object Detection](https://arxiv.org/abs/2508.12684)
*Zhongyao Li,Peirui Cheng,Liangjin Zhao,Chen Chen,Yundu Li,Zhechao Wang,Xue Yang,Xian Sun,Zhirui Wang*

Main category: cs.CV

TL;DR: AdaBEV is a novel framework for multi-UAV 3D detection that creates adaptive instance-aware BEV representations using refine-and-contrast approach, achieving superior accuracy-computation trade-offs with minimal overhead.


<details>
  <summary>Details</summary>
Motivation: Multi-UAV collaborative 3D detection offers advantages in coverage and occlusion handling but faces computational challenges on resource-constrained UAV platforms, requiring efficient methods that maintain performance while reducing computational burden.

Method: Introduces Box-Guided Refinement Module (BG-RM) that refines only foreground-associated BEV grids using 2D supervision and spatial subdivision, and Instance-Background Contrastive Learning (IBCL) that enhances feature discriminability through contrastive learning in BEV space.

Result: Extensive experiments on Air-Co-Pred dataset show AdaBEV achieves superior accuracy-computation trade-offs across model scales, outperforms state-of-the-art methods at low resolutions, and approaches upper bound performance while maintaining low-resolution BEV inputs with negligible overhead.

Conclusion: AdaBEV successfully addresses computational constraints in multi-UAV 3D detection by focusing computational resources on foreground instances and improving feature discrimination, making it highly suitable for resource-constrained aerial platforms.

Abstract: Multi-UAV collaborative 3D detection enables accurate and robust perception
by fusing multi-view observations from aerial platforms, offering significant
advantages in coverage and occlusion handling, while posing new challenges for
computation on resource-constrained UAV platforms. In this paper, we present
AdaBEV, a novel framework that learns adaptive instance-aware BEV
representations through a refine-and-contrast paradigm. Unlike existing methods
that treat all BEV grids equally, AdaBEV introduces a Box-Guided Refinement
Module (BG-RM) and an Instance-Background Contrastive Learning (IBCL) to
enhance semantic awareness and feature discriminability. BG-RM refines only BEV
grids associated with foreground instances using 2D supervision and spatial
subdivision, while IBCL promotes stronger separation between foreground and
background features via contrastive learning in BEV space. Extensive
experiments on the Air-Co-Pred dataset demonstrate that AdaBEV achieves
superior accuracy-computation trade-offs across model scales, outperforming
other state-of-the-art methods at low resolutions and approaching upper bound
performance while maintaining low-resolution BEV inputs and negligible
overhead.

</details>


### [102] [TTA-DAME: Test-Time Adaptation with Domain Augmentation and Model Ensemble for Dynamic Driving Conditions](https://arxiv.org/abs/2508.12690)
*Dongjae Jeon,Taeheon Kim,Seongwon Cho,Minhyuk Seo,Jonghyun Choi*

Main category: cs.CV

TL;DR: TTA-DAME improves test-time adaptation for driving scenes by using source domain augmentation, domain discrimination, and multiple detector fusion with NMS.


<details>
  <summary>Details</summary>
Motivation: Address dynamic domain shifts in real-world driving scenes, particularly weather and day/night transitions, where models need to adapt continuously to changing conditions.

Method: Leverages source domain data augmentation into target domains, introduces domain discriminator and specialized domain detector, trains multiple detectors and consolidates predictions through Non-Maximum Suppression (NMS).

Result: Demonstrates significant performance enhancements on the SHIFT Benchmark for driving scene adaptation.

Conclusion: The proposed TTA-DAME method effectively handles dynamic domain shifts in driving scenarios through augmented adaptation and multi-detector fusion.

Abstract: Test-time Adaptation (TTA) poses a challenge, requiring models to dynamically
adapt and perform optimally on shifting target domains. This task is
particularly emphasized in real-world driving scenes, where weather domain
shifts occur frequently. To address such dynamic changes, our proposed method,
TTA-DAME, leverages source domain data augmentation into target domains.
Additionally, we introduce a domain discriminator and a specialized domain
detector to mitigate drastic domain shifts, especially from daytime to
nighttime conditions. To further improve adaptability, we train multiple
detectors and consolidate their predictions through Non-Maximum Suppression
(NMS). Our empirical validation demonstrates the effectiveness of our method,
showing significant performance enhancements on the SHIFT Benchmark.

</details>


### [103] [Multi-Level Knowledge Distillation and Dynamic Self-Supervised Learning for Continual Learning](https://arxiv.org/abs/2508.12692)
*Taeheon Kim,San Kim,Minhyuk Seo,Dongjae Jeon,Wonje Jeong,Jonghyun Choi*

Main category: cs.CV

TL;DR: Proposes multi-level knowledge distillation and dynamic self-supervised learning for class-incremental learning with repetition using unlabeled external data.


<details>
  <summary>Details</summary>
Motivation: Class-incremental learning with repetition (CIR) is more realistic than traditional setups, and abundant unlabeled data from external sources like the Internet can be leveraged to improve model stability and plasticity.

Method: Two components: 1) Multi-level knowledge distillation (MLKD) that distills knowledge from multiple previous models across features and logits, 2) Dynamic self-supervised loss (SSL) that utilizes unlabeled data to accelerate new class learning while maintaining focus on primary tasks.

Result: Significantly improved performance in CIR setup, achieving 2nd place in the CVPR 5th CLVISION Challenge.

Conclusion: The proposed MLKD and dynamic SSL components effectively leverage unlabeled external data to enhance both stability (retaining previous knowledge) and plasticity (learning new classes) in class-incremental learning with repetition scenarios.

Abstract: Class-incremental with repetition (CIR), where previously trained classes
repeatedly introduced in future tasks, is a more realistic scenario than the
traditional class incremental setup, which assumes that each task contains
unseen classes. CIR assumes that we can easily access abundant unlabeled data
from external sources, such as the Internet. Therefore, we propose two
components that efficiently use the unlabeled data to ensure the high stability
and the plasticity of models trained in CIR setup. First, we introduce
multi-level knowledge distillation (MLKD) that distills knowledge from multiple
previous models across multiple perspectives, including features and logits, so
the model can maintain much various previous knowledge. Moreover, we implement
dynamic self-supervised loss (SSL) to utilize the unlabeled data that
accelerates the learning of new classes, while dynamic weighting of SSL keeps
the focus of training to the primary task. Both of our proposed components
significantly improve the performance in CIR setup, achieving 2nd place in the
CVPR 5th CLVISION Challenge.

</details>


### [104] [Neural Rendering for Sensor Adaptation in 3D Object Detection](https://arxiv.org/abs/2508.12695)
*Felix Embacher,David Holtz,Jonas Uhrig,Marius Cordts,Markus Enzweiler*

Main category: cs.CV

TL;DR: Cross-sensor domain gap causes performance degradation in 3D object detectors when trained on one vehicle sensor setup and tested on another. BEVFormer shows best robustness, and a neural rendering adaptation pipeline significantly mitigates this gap.


<details>
  <summary>Details</summary>
Motivation: Autonomous vehicles have varying camera sensor setups due to different vehicle types, causing cross-sensor domain gap that degrades perception model accuracy when evaluated on different sensor configurations.

Method: Created CamShift dataset in CARLA to simulate domain gap between subcompact vehicles and SUVs. Investigated various 3D object detectors, identified BEVFormer as most robust, and proposed neural rendering-based sensor adaptation pipeline to transform datasets between different camera setups.

Result: Significant cross-sensor performance degradation observed. BEVFormer with dense BEV representation and backward projection showed best robustness. Neural rendering adaptation pipeline improved performance across all detectors, mitigating domain gap by large margin.

Conclusion: Model architecture matters for cross-sensor robustness, and data-driven neural rendering adaptation effectively bridges sensor domain gaps, enabling data reusability across different vehicle sensor setups without new data collection.

Abstract: Autonomous vehicles often have varying camera sensor setups, which is
inevitable due to restricted placement options for different vehicle types.
Training a perception model on one particular setup and evaluating it on a new,
different sensor setup reveals the so-called cross-sensor domain gap, typically
leading to a degradation in accuracy. In this paper, we investigate the impact
of the cross-sensor domain gap on state-of-the-art 3D object detectors. To this
end, we introduce CamShift, a dataset inspired by nuScenes and created in CARLA
to specifically simulate the domain gap between subcompact vehicles and sport
utility vehicles (SUVs). Using CamShift, we demonstrate significant
cross-sensor performance degradation, identify robustness dependencies on model
architecture, and propose a data-driven solution to mitigate the effect. On the
one hand, we show that model architectures based on a dense Bird's Eye View
(BEV) representation with backward projection, such as BEVFormer, are the most
robust against varying sensor configurations. On the other hand, we propose a
novel data-driven sensor adaptation pipeline based on neural rendering, which
can transform entire datasets to match different camera sensor setups. Applying
this approach improves performance across all investigated 3D object detectors,
mitigating the cross-sensor domain gap by a large margin and reducing the need
for new data collection by enabling efficient data reusability across vehicles
with different sensor setups. The CamShift dataset and the sensor adaptation
benchmark are available at https://dmholtz.github.io/camshift/.

</details>


### [105] [Drifting Away from Truth: GenAI-Driven News Diversity Challenges LVLM-Based Misinformation Detection](https://arxiv.org/abs/2508.12711)
*Fanxiao Li,Jiaying Wu,Tingchao Fu,Yunyun Dong,Bingbing Song,Wei Zhou*

Main category: cs.CV

TL;DR: GenAI-driven news diversity causes multi-level drift that significantly degrades LVLM-based misinformation detection systems, with performance dropping 14.8% on average and reasoning becoming unstable.


<details>
  <summary>Details</summary>
Motivation: The proliferation of multimodal misinformation and rise of GenAI tools create highly varied content that challenges current detection systems, requiring systematic study of these vulnerabilities.

Method: Introduce DriftBench - a large-scale benchmark with 16,000 news instances across six diversification categories, and design three evaluation tasks to test robustness, adversarial susceptibility, and reasoning consistency.

Result: Experiments with six state-of-the-art LVLM detectors show substantial performance drops (average F1 -14.8%), increasingly unstable reasoning traces, and severe failures under adversarial evidence injection.

Conclusion: Findings reveal fundamental vulnerabilities in existing MMD systems, indicating an urgent need for more resilient approaches to handle GenAI-driven content diversity.

Abstract: The proliferation of multimodal misinformation poses growing threats to
public discourse and societal trust. While Large Vision-Language Models (LVLMs)
have enabled recent progress in multimodal misinformation detection (MMD), the
rise of generative AI (GenAI) tools introduces a new challenge: GenAI-driven
news diversity, characterized by highly varied and complex content. We show
that this diversity induces multi-level drift, comprising (1) model-level
misperception drift, where stylistic variations disrupt a model's internal
reasoning, and (2) evidence-level drift, where expression diversity degrades
the quality or relevance of retrieved external evidence. These drifts
significantly degrade the robustness of current LVLM-based MMD systems. To
systematically study this problem, we introduce DriftBench, a large-scale
benchmark comprising 16,000 news instances across six categories of
diversification. We design three evaluation tasks: (1) robustness of truth
verification under multi-level drift; (2) susceptibility to adversarial
evidence contamination generated by GenAI; and (3) analysis of reasoning
consistency across diverse inputs. Experiments with six state-of-the-art
LVLM-based detectors show substantial performance drops (average F1 -14.8%) and
increasingly unstable reasoning traces, with even more severe failures under
adversarial evidence injection. Our findings uncover fundamental
vulnerabilities in existing MMD systems and suggest an urgent need for more
resilient approaches in the GenAI era.

</details>


### [106] [Real-Time Sign Language Gestures to Speech Transcription using Deep Learning](https://arxiv.org/abs/2508.12713)
*Brandone Fonya*

Main category: cs.CV

TL;DR: Real-time sign language translation system using CNN on Sign Language MNIST dataset that converts gestures to text and speech via webcam capture with high accuracy but some latency.


<details>
  <summary>Details</summary>
Motivation: Address communication barriers for individuals with hearing and speech impairments by creating an accessible tool that enables seamless interaction in everyday environments.

Method: Uses convolution neural networks (CNN) trained on Sign Language MNIST dataset to classify hand gestures captured live via webcam, then translates detected gestures into text and audible speech using text-to-speech synthesis.

Result: Comprehensive experiments demonstrate high model accuracy and robust real-time performance, though with some latency. The system proves to be practical, accessible, reliable, and user-friendly.

Conclusion: The system effectively enhances autonomy and integration of sign language users in diverse social settings, serving as a valuable assistive technology solution for real-time communication.

Abstract: Communication barriers pose significant challenges for individuals with
hearing and speech impairments, often limiting their ability to effectively
interact in everyday environments. This project introduces a real-time
assistive technology solution that leverages advanced deep learning techniques
to translate sign language gestures into textual and audible speech. By
employing convolution neural networks (CNN) trained on the Sign Language MNIST
dataset, the system accurately classifies hand gestures captured live via
webcam. Detected gestures are instantaneously translated into their
corresponding meanings and transcribed into spoken language using
text-to-speech synthesis, thus facilitating seamless communication.
Comprehensive experiments demonstrate high model accuracy and robust real-time
performance with some latency, highlighting the system's practical
applicability as an accessible, reliable, and user-friendly tool for enhancing
the autonomy and integration of sign language users in diverse social settings.

</details>


### [107] [Single-Reference Text-to-Image Manipulation with Dual Contrastive Denoising Score](https://arxiv.org/abs/2508.12718)
*Syed Muhmmad Israr,Feng Zhao*

Main category: cs.CV

TL;DR: Dual Contrastive Denoising Score framework enables precise real image editing using text-to-image diffusion models by preserving input structure while allowing flexible content modifications through dual contrastive loss.


<details>
  <summary>Details</summary>
Motivation: Existing text-to-image models struggle with real image editing due to difficulty in creating perfect text prompts and unwanted alterations in unchanged regions of input images.

Method: Leverages self-attention layers of latent diffusion models with dual contrastive loss inspired by contrastive learning, enabling structure preservation and content modification without auxiliary networks or additional training.

Result: Outperforms existing methods in real image editing while maintaining zero-shot image-to-image translation capabilities and directly utilizing pretrained models.

Conclusion: The framework successfully addresses key challenges in real image editing by combining diffusion model generative priors with contrastive learning principles for precise, structure-preserving modifications.

Abstract: Large-scale text-to-image generative models have shown remarkable ability to
synthesize diverse and high-quality images. However, it is still challenging to
directly apply these models for editing real images for two reasons. First, it
is difficult for users to come up with a perfect text prompt that accurately
describes every visual detail in the input image. Second, while existing models
can introduce desirable changes in certain regions, they often dramatically
alter the input content and introduce unexpected changes in unwanted regions.
To address these challenges, we present Dual Contrastive Denoising Score, a
simple yet powerful framework that leverages the rich generative prior of
text-to-image diffusion models. Inspired by contrastive learning approaches for
unpaired image-to-image translation, we introduce a straightforward dual
contrastive loss within the proposed framework. Our approach utilizes the
extensive spatial information from the intermediate representations of the
self-attention layers in latent diffusion models without depending on auxiliary
networks. Our method achieves both flexible content modification and structure
preservation between input and output images, as well as zero-shot
image-to-image translation. Through extensive experiments, we show that our
approach outperforms existing methods in real image editing while maintaining
the capability to directly utilize pretrained text-to-image diffusion models
without further training.

</details>


### [108] [Quantifying and Alleviating Co-Adaptation in Sparse-View 3D Gaussian Splatting](https://arxiv.org/abs/2508.12720)
*Kangjie Chen,Yingji Zhong,Zhihao Li,Jiaqi Lin,Youyu Chen,Minghan Qin,Haoqian Wang*

Main category: cs.CV

TL;DR: 3D Gaussian Splatting suffers from appearance artifacts in sparse-view scenarios due to Gaussian co-adaptation, which is mitigated through random dropout and opacity noise injection.


<details>
  <summary>Details</summary>
Motivation: 3DGS produces realistic renderings in training views but shows appearance artifacts in novel views under sparse-view conditions, indicating a fundamental limitation in current approaches.

Method: Proposed Co-Adaptation Score (CA) metric to quantify Gaussian entanglement, and introduced two lightweight strategies: random Gaussian dropout and multiplicative noise injection to opacity.

Result: Analysis reveals co-adaptation decreases with more training views. Both proposed strategies effectively mitigate co-adaptation across various methods and benchmarks.

Conclusion: The co-adaptation effect in sparse-view 3DGS is a critical issue that can be addressed through simple plug-and-play techniques, providing insights for better understanding and improvement of sparse-view 3DGS.

Abstract: 3D Gaussian Splatting (3DGS) has demonstrated impressive performance in novel
view synthesis under dense-view settings. However, in sparse-view scenarios,
despite the realistic renderings in training views, 3DGS occasionally manifests
appearance artifacts in novel views. This paper investigates the appearance
artifacts in sparse-view 3DGS and uncovers a core limitation of current
approaches: the optimized Gaussians are overly-entangled with one another to
aggressively fit the training views, which leads to a neglect of the real
appearance distribution of the underlying scene and results in appearance
artifacts in novel views. The analysis is based on a proposed metric, termed
Co-Adaptation Score (CA), which quantifies the entanglement among Gaussians,
i.e., co-adaptation, by computing the pixel-wise variance across multiple
renderings of the same viewpoint, with different random subsets of Gaussians.
The analysis reveals that the degree of co-adaptation is naturally alleviated
as the number of training views increases. Based on the analysis, we propose
two lightweight strategies to explicitly mitigate the co-adaptation in
sparse-view 3DGS: (1) random gaussian dropout; (2) multiplicative noise
injection to the opacity. Both strategies are designed to be plug-and-play, and
their effectiveness is validated across various methods and benchmarks. We hope
that our insights into the co-adaptation effect will inspire the community to
achieve a more comprehensive understanding of sparse-view 3DGS.

</details>


### [109] [Frequency-Driven Inverse Kernel Prediction for Single Image Defocus Deblurring](https://arxiv.org/abs/2508.12736)
*Ying Zhang,Xiongxin Tang,Chongyi Li,Qiao Chen,Yuquan Wu*

Main category: cs.CV

TL;DR: Frequency-Driven Inverse Kernel Prediction network (FDIKP) uses frequency-domain representations and dual-branch strategy to improve defocus deblurring by enhancing kernel estimation accuracy and deconvolution adaptability.


<details>
  <summary>Details</summary>
Motivation: Existing methods relying on spatial features degrade in severely blurry regions where local high-frequency details are missing, creating a need for better kernel modeling approaches.

Method: Proposes FDIKP with Dual-Branch Inverse Kernel Prediction strategy, Position Adaptive Convolution for deconvolution adaptability, and Dual-Domain Scale Recurrent Module for progressive quality improvement.

Result: Extensive experiments demonstrate that the method outperforms existing approaches in single image defocus deblurring.

Conclusion: The frequency-domain approach with dual-branch kernel prediction and adaptive deconvolution effectively addresses the challenges of spatially varying blur kernels in defocus deblurring.

Abstract: Single image defocus deblurring aims to recover an all-in-focus image from a
defocus counterpart, where accurately modeling spatially varying blur kernels
remains a key challenge. Most existing methods rely on spatial features for
kernel estimation, but their performance degrades in severely blurry regions
where local high-frequency details are missing. To address this, we propose a
Frequency-Driven Inverse Kernel Prediction network (FDIKP) that incorporates
frequency-domain representations to enhance structural identifiability in
kernel modeling. Given the superior discriminative capability of the frequency
domain for blur modeling, we design a Dual-Branch Inverse Kernel Prediction
(DIKP) strategy that improves the accuracy of kernel estimation while
maintaining stability. Moreover, considering the limited number of predicted
inverse kernels, we introduce a Position Adaptive Convolution (PAC) to enhance
the adaptability of the deconvolution process. Finally, we propose a
Dual-Domain Scale Recurrent Module (DSRM) to fuse deconvolution results and
progressively improve deblurring quality from coarse to fine. Extensive
experiments demonstrate that our method outperforms existing approaches. Code
will be made publicly available.

</details>


### [110] [DCSCR: A Class-Specific Collaborative Representation based Network for Image Set Classification](https://arxiv.org/abs/2508.12745)
*Xizhan Gao,Wei Hu*

Main category: cs.CV

TL;DR: Proposes DCSCR network combining traditional and deep learning approaches for few-shot image set classification, learning both frame-level and concept-level features with adaptive distance measurement.


<details>
  <summary>Details</summary>
Motivation: Existing methods either use raw pixel features (ignoring feature learning) or deep features but fail to adaptively adjust features when measuring set distances, limiting performance in few-shot scenarios.

Method: Deep Class-specific Collaborative Representation (DCSCR) network with three modules: fully convolutional deep feature extractor, global feature learning, and class-specific collaborative representation-based metric learning with new contrastive loss function.

Result: Extensive experiments on several well-known few-shot ISC datasets demonstrate effectiveness compared with state-of-the-art image set classification algorithms.

Conclusion: The proposed DCSCR approach successfully addresses limitations of existing methods by simultaneously learning frame- and concept-level feature representations and adaptive distance similarities for improved few-shot image set classification.

Abstract: Image set classification (ISC), which can be viewed as a task of comparing
similarities between sets consisting of unordered heterogeneous images with
variable quantities and qualities, has attracted growing research attention in
recent years. How to learn effective feature representations and how to explore
the similarities between different image sets are two key yet challenging
issues in this field. However, existing traditional ISC methods classify image
sets based on raw pixel features, ignoring the importance of feature learning.
Existing deep ISC methods can learn deep features, but they fail to adaptively
adjust the features when measuring set distances, resulting in limited
performance in few-shot ISC. To address the above issues, this paper combines
traditional ISC methods with deep models and proposes a novel few-shot ISC
approach called Deep Class-specific Collaborative Representation (DCSCR)
network to simultaneously learn the frame- and concept-level feature
representations of each image set and the distance similarities between
different sets. Specifically, DCSCR consists of a fully convolutional deep
feature extractor module, a global feature learning module, and a
class-specific collaborative representation-based metric learning module. The
deep feature extractor and global feature learning modules are used to learn
(local and global) frame-level feature representations, while the
class-specific collaborative representation-based metric learning module is
exploit to adaptively learn the concept-level feature representation of each
image set and thus obtain the distance similarities between different sets by
developing a new CSCR-based contrastive loss function. Extensive experiments on
several well-known few-shot ISC datasets demonstrate the effectiveness of the
proposed method compared with some state-of-the-art image set classification
algorithms.

</details>


### [111] [D2-Mamba: Dual-Scale Fusion and Dual-Path Scanning with SSMs for Shadow Removal](https://arxiv.org/abs/2508.12750)
*Linhao Li,Boya Jin,Zizhe Li,Lanqing Guo,Hao Cheng,Bo Li,Yongfeng Dong*

Main category: cs.CV

TL;DR: A novel Mamba-based network with dual-scale fusion and dual-path scanning for shadow removal, outperforming state-of-the-art methods by effectively leveraging non-shadow region information and adaptive region-specific transformations.


<details>
  <summary>Details</summary>
Motivation: Shadow removal requires different transformations for shadowed vs well-lit regions, making uniform correction strategies ineffective. The paper aims to address the challenge of integrating non-local contextual cues and modeling region-specific transformations by leveraging information from non-shadow regions.

Method: Proposes a Mamba-based network with Dual-Scale Fusion Mamba Block (DFMB) for multi-scale feature representation and boundary artifact reduction, and Dual-Path Mamba Group (DPMG) with horizontal scanning and mask-aware adaptive scanning for global feature capture and fine-grained region modeling.

Result: Experimental results demonstrate that the proposed method significantly outperforms existing state-of-the-art approaches on shadow removal benchmarks.

Conclusion: The proposed Mamba-based network with dual-scale fusion and dual-path scanning effectively addresses shadow removal challenges by selectively propagating contextual information based on transformation similarity and improving structural continuity through adaptive scanning strategies.

Abstract: Shadow removal aims to restore images that are partially degraded by shadows,
where the degradation is spatially localized and non-uniform. Unlike general
restoration tasks that assume global degradation, shadow removal can leverage
abundant information from non-shadow regions for guidance. However, the
transformation required to correct shadowed areas often differs significantly
from that of well-lit regions, making it challenging to apply uniform
correction strategies. This necessitates the effective integration of non-local
contextual cues and adaptive modeling of region-specific transformations. To
this end, we propose a novel Mamba-based network featuring dual-scale fusion
and dual-path scanning to selectively propagate contextual information based on
transformation similarity across regions. Specifically, the proposed Dual-Scale
Fusion Mamba Block (DFMB) enhances multi-scale feature representation by fusing
original features with low-resolution features, effectively reducing boundary
artifacts. The Dual-Path Mamba Group (DPMG) captures global features via
horizontal scanning and incorporates a mask-aware adaptive scanning strategy,
which improves structural continuity and fine-grained region modeling.
Experimental results demonstrate that our method significantly outperforms
existing state-of-the-art approaches on shadow removal benchmarks.

</details>


### [112] [CLAIRE-DSA: Fluoroscopic Image Classification for Quality Assurance of Computer Vision Pipelines in Acute Ischemic Stroke](https://arxiv.org/abs/2508.12755)
*Cristo J. van den Berg,Frank G. te Nijenhuis,Mirre J. Blaauboer,Daan T. W. van Erp,Carlijn M. Keppels,Matthijs van der Sluijs,Bob Roozenbeek,Wim van Zwam,Sandra Cornelissen,Danny Ruijters,Ruisheng Su,Theo van Walsum*

Main category: cs.CV

TL;DR: CLAIRE-DSA is a deep learning framework that classifies image quality in fluoroscopic minimum intensity projections during mechanical thrombectomy for stroke, improving downstream segmentation performance by filtering poor quality images.


<details>
  <summary>Details</summary>
Motivation: Computer vision models for assisting mechanical thrombectomy in acute ischemic stroke suffer from degraded performance due to poor image quality, necessitating automated quality assessment tools.

Method: Uses pre-trained ResNet backbone models fine-tuned to predict nine image properties (contrast presence, projection angle, motion artifacts, etc.) on an annotated dataset of 1,758 fluoroscopic MinIPs with separate classifiers for each property.

Result: Achieved excellent performance with ROC-AUC 0.91-0.98 and precision 0.70-1.00 across all labels. Filtering poor quality images increased segmentation success rate from 42% to 69% (p < 0.001).

Conclusion: CLAIRE-DSA shows strong potential as an automated tool for image quality classification in DSA series, supporting clinical and research applications through improved image annotation and quality control.

Abstract: Computer vision models can be used to assist during mechanical thrombectomy
(MT) for acute ischemic stroke (AIS), but poor image quality often degrades
performance. This work presents CLAIRE-DSA, a deep learning--based framework
designed to categorize key image properties in minimum intensity projections
(MinIPs) acquired during MT for AIS, supporting downstream quality control and
workflow optimization. CLAIRE-DSA uses pre-trained ResNet backbone models,
fine-tuned to predict nine image properties (e.g., presence of contrast,
projection angle, motion artefact severity). Separate classifiers were trained
on an annotated dataset containing $1,758$ fluoroscopic MinIPs. The model
achieved excellent performance on all labels, with ROC-AUC ranging from $0.91$
to $0.98$, and precision ranging from $0.70$ to $1.00$. The ability of
CLAIRE-DSA to identify suitable images was evaluated on a segmentation task by
filtering poor quality images and comparing segmentation performance on
filtered and unfiltered datasets. Segmentation success rate increased from
$42%$ to $69%$, $p < 0.001$. CLAIRE-DSA demonstrates strong potential as an
automated tool for accurately classifying image properties in DSA series of
acute ischemic stroke patients, supporting image annotation and quality control
in clinical and research applications. Source code is available at
https://gitlab.com/icai-stroke-lab/wp3_neurointerventional_ai/claire-dsa.

</details>


### [113] [Harnessing Group-Oriented Consistency Constraints for Semi-Supervised Semantic Segmentation in CdZnTe Semiconductors](https://arxiv.org/abs/2508.12766)
*Peihao Li,Yan Fang,Man Liu,Huihui Bai,Anhong Wang,Yunchao Wei,Yao Zhao*

Main category: cs.CV

TL;DR: Proposes ICAF framework for semi-supervised semantic segmentation of CdZnTe semiconductor images with many-to-one view relationships, achieving 70.6% mIoU with only 0.5% annotated data.


<details>
  <summary>Details</summary>
Motivation: Standard semi-supervised segmentation methods fail for CdZnTe images due to many-to-one view relationships (multiple views share single ground truth) and low-contrast defect boundaries, causing error accumulation and confirmation bias.

Method: Intra-group Consistency Augmentation Framework (ICAF) with Pseudo-label Correction Network (PCN) containing View Augmentation Module (VAM) for boundary-aware view synthesis and View Correction Module (VCM) for information interaction between views.

Result: Achieves 70.6% mIoU on CdZnTe dataset using only 2 group-annotated data (0.5%) with DeepLabV3+ and ResNet-101 backbone, significantly outperforming traditional methods.

Conclusion: The group-oriented approach effectively addresses many-to-one view relationships in CdZnTe segmentation, providing robust performance with minimal annotation through intra-group consistency constraints and boundary-aware view synthesis.

Abstract: Labeling Cadmium Zinc Telluride (CdZnTe) semiconductor images is challenging
due to the low-contrast defect boundaries, necessitating annotators to
cross-reference multiple views. These views share a single ground truth (GT),
forming a unique ``many-to-one'' relationship. This characteristic renders
advanced semi-supervised semantic segmentation (SSS) methods suboptimal, as
they are generally limited by a ``one-to-one'' relationship, where each image
is independently associated with its GT. Such limitation may lead to error
accumulation in low-contrast regions, further exacerbating confirmation bias.
To address this issue, we revisit the SSS pipeline from a group-oriented
perspective and propose a human-inspired solution: the Intra-group Consistency
Augmentation Framework (ICAF). First, we experimentally validate the inherent
consistency constraints within CdZnTe groups, establishing a group-oriented
baseline using the Intra-group View Sampling (IVS). Building on this insight,
we introduce the Pseudo-label Correction Network (PCN) to enhance consistency
representation, which consists of two key modules. The View Augmentation Module
(VAM) improves boundary details by dynamically synthesizing a boundary-aware
view through the aggregation of multiple views. In the View Correction Module
(VCM), this synthesized view is paired with other views for information
interaction, effectively emphasizing salient regions while minimizing noise.
Extensive experiments demonstrate the effectiveness of our solution for CdZnTe
materials. Leveraging DeepLabV3+ with a ResNet-101 backbone as our segmentation
model, we achieve a 70.6\% mIoU on the CdZnTe dataset using only 2
group-annotated data (5\textperthousand). The code is available at
\href{https://github.com/pipixiapipi/ICAF}{https://github.com/pipixiapipi/ICAF}.

</details>


### [114] [SocialTrack: Multi-Object Tracking in Complex Urban Traffic Scenes Inspired by Social Behavior](https://arxiv.org/abs/2508.12777)
*Wenguang Tao,Xiaotian Wang,Tian Yan,Jie Yan,Guodong Li,Kun Bai*

Main category: cs.CV

TL;DR: SocialTrack is a novel UAV-based multi-object tracking framework that addresses challenges like small target variations, occlusions, and motion blur in complex urban environments through specialized detection, adaptive filtering, group motion modeling, and spatio-temporal memory prediction.


<details>
  <summary>Details</summary>
Motivation: UAV-based multi-object tracking has significant value for urban intelligent transportation systems, but faces challenges including small target scale variations, occlusions, nonlinear crossing motions, and motion blur that hinder tracking stability in complex UAV perspectives.

Method: Proposes SocialTrack framework with: 1) specialized small-target detector with multi-scale feature enhancement, 2) Velocity Adaptive Cubature Kalman Filter for trajectory prediction, 3) Group Motion Compensation Strategy for social group motion modeling, and 4) Spatio-Temporal Memory Prediction using historical trajectory information.

Result: Extensive experiments on UAVDT and MOT17 datasets show SocialTrack outperforms state-of-the-art methods across key metrics, with significant improvements in MOTA and IDF1 performance indicators, demonstrating superior robustness and adaptability.

Conclusion: SocialTrack effectively addresses UAV-based tracking challenges in complex urban environments and is highly modular and compatible, allowing seamless integration with existing trackers to further enhance performance.

Abstract: As a key research direction in the field of multi-object tracking (MOT),
UAV-based multi-object tracking has significant application value in the
analysis and understanding of urban intelligent transportation systems.
However, in complex UAV perspectives, challenges such as small target scale
variations, occlusions, nonlinear crossing motions, and motion blur severely
hinder the stability of multi-object tracking. To address these challenges,
this paper proposes a novel multi-object tracking framework, SocialTrack, aimed
at enhancing the tracking accuracy and robustness of small targets in complex
urban traffic environments. The specialized small-target detector enhances the
detection performance by employing a multi-scale feature enhancement mechanism.
The Velocity Adaptive Cubature Kalman Filter (VACKF) improves the accuracy of
trajectory prediction by incorporating a velocity dynamic modeling mechanism.
The Group Motion Compensation Strategy (GMCS) models social group motion priors
to provide stable state update references for low-quality tracks, significantly
improving the target association accuracy in complex dynamic environments.
Furthermore, the Spatio-Temporal Memory Prediction (STMP) leverages historical
trajectory information to predict the future state of low-quality tracks,
effectively mitigating identity switching issues. Extensive experiments on the
UAVDT and MOT17 datasets demonstrate that SocialTrack outperforms existing
state-of-the-art (SOTA) methods across several key metrics. Significant
improvements in MOTA and IDF1, among other core performance indicators,
highlight its superior robustness and adaptability. Additionally, SocialTrack
is highly modular and compatible, allowing for seamless integration with
existing trackers to further enhance performance.

</details>


### [115] [Has GPT-5 Achieved Spatial Intelligence? An Empirical Study](https://arxiv.org/abs/2508.13142)
*Zhongang Cai,Yubo Wang,Qingping Sun,Ruisi Wang,Chenyang Gu,Wanqi Yin,Zhiqian Lin,Zhitao Yang,Chen Wei,Xuanke Shi,Kewang Deng,Xiaoyang Han,Zukai Chen,Jiaqi Li,Xiangyu Fan,Hanming Deng,Lewei Lu,Bo Li,Ziwei Liu,Quan Wang,Dahua Lin,Lei Yang*

Main category: cs.CV

TL;DR: GPT-5 shows unprecedented spatial intelligence capabilities but still falls short of human performance across various spatial reasoning tasks, with proprietary models not having decisive advantages on the most difficult problems.


<details>
  <summary>Details</summary>
Motivation: Multi-modal models have limitations in spatial understanding and reasoning, which are fundamental for artificial general intelligence. With GPT-5's release, it's timely to evaluate leading models' progress toward spatial intelligence.

Method: Proposed a comprehensive taxonomy of spatial tasks unifying existing benchmarks, evaluated state-of-the-art proprietary and open-source models on eight key benchmarks using over one billion total tokens, and conducted qualitative evaluation on diverse scenarios.

Result: GPT-5 demonstrates unprecedented strength in spatial intelligence but still falls short of human performance across broad spectrum of tasks. Proprietary models don't show decisive advantage on most difficult problems. Identified challenging spatial intelligence problems for multi-modal models.

Conclusion: While GPT-5 represents significant progress in spatial intelligence, current multi-modal models still struggle with spatial reasoning tasks that are intuitive for humans, indicating substantial room for improvement in this critical capability for AGI.

Abstract: Multi-modal models have achieved remarkable progress in recent years.
Nevertheless, they continue to exhibit notable limitations in spatial
understanding and reasoning, which are fundamental capabilities to achieving
artificial general intelligence. With the recent release of GPT-5, allegedly
the most powerful AI model to date, it is timely to examine where the leading
models stand on the path toward spatial intelligence. First, we propose a
comprehensive taxonomy of spatial tasks that unifies existing benchmarks and
discuss the challenges in ensuring fair evaluation. We then evaluate
state-of-the-art proprietary and open-source models on eight key benchmarks, at
a cost exceeding one billion total tokens. Our empirical study reveals that (1)
GPT-5 demonstrates unprecedented strength in spatial intelligence, yet (2)
still falls short of human performance across a broad spectrum of tasks.
Moreover, we (3) identify the more challenging spatial intelligence problems
for multi-modal models, and (4) proprietary models do not exhibit a decisive
advantage when facing the most difficult problems. In addition, we conduct a
qualitative evaluation across a diverse set of scenarios that are intuitive for
humans yet fail even the most advanced multi-modal models.

</details>


### [116] [Leveraging Diffusion Models for Stylization using Multiple Style Images](https://arxiv.org/abs/2508.12784)
*Dan Ruta,Abdelaziz Djelouah,Raphael Ortiz,Christopher Schroers*

Main category: cs.CV

TL;DR: A novel image style transfer method using multiple style images with image prompt adapters and statistical feature alignment during denoising to prevent content leakage and improve style matching.


<details>
  <summary>Details</summary>
Motivation: Existing latent diffusion models for style transfer struggle with accurate style matching, limited style image usage, and content-style entanglement issues.

Method: Leverages multiple style images with image prompt adapters and statistical alignment of features during denoising process, intervening at both cross-attention and self-attention layers of the UNet, using clustering to distill representative attention features.

Result: Achieves state-of-the-art results for stylization as demonstrated in experimental evaluations.

Conclusion: The proposed approach effectively addresses key limitations in current style transfer methods by using multiple style references and strategic feature alignment during diffusion.

Abstract: Recent advances in latent diffusion models have enabled exciting progress in
image style transfer. However, several key issues remain. For example, existing
methods still struggle to accurately match styles. They are often limited in
the number of style images that can be used. Furthermore, they tend to entangle
content and style in undesired ways. To address this, we propose leveraging
multiple style images which helps better represent style features and prevent
content leaking from the style images. We design a method that leverages both
image prompt adapters and statistical alignment of the features during the
denoising process. With this, our approach is designed such that it can
intervene both at the cross-attention and the self-attention layers of the
denoising UNet. For the statistical alignment, we employ clustering to distill
a small representative set of attention features from the large number of
attention values extracted from the style samples. As demonstrated in our
experimental section, the resulting method achieves state-of-the-art results
for stylization.

</details>


### [117] [Vehicle detection from GSV imagery: Predicting travel behaviour for cycling and motorcycling using Computer Vision](https://arxiv.org/abs/2508.12794)
*Kyriaki,Kokka,Rahul Goel,Ali Abbas,Kerry A. Nice,Luca Martial,SM Labib,Rihuan Ke,Carola Bibiane Schönlieb,James Woodcock*

Main category: cs.CV

TL;DR: Using deep learning on Google Street View images to estimate global cycling and motorcycling levels with 89% detection accuracy and strong correlation to actual mode shares.


<details>
  <summary>Details</summary>
Motivation: Transportation impacts health through physical activity, pollution, and injury risks, but comparative global data on cycling and motorcycling behaviors is scarce. Street view imagery combined with computer vision offers efficient data collection.

Method: Used YOLOv4 model fine-tuned on images from 6 cities to detect cycles and motorcycles in 8000 GSV images per city across 185 global cities. Developed beta regression models with city-level mode shares as outcome and GSV-detected counts as explanatory variables.

Result: Achieved 89% mean average precision for detection. Strong correlation between GSV motorcycle counts and mode share (0.78), moderate for cycling (0.51). Beta regression models predicted mode shares with R² of 0.614 for cycling and 0.612 for motorcycling, with median absolute errors of 1.3% and 1.4% respectively.

Conclusion: GSV imagery combined with computer vision effectively captures travel modes and provides valuable insights alongside traditional data sources, enabling global estimation of cycling and motorcycling behaviors where survey data is unavailable.

Abstract: Transportation influence health by shaping exposure to physical activity, air
pollution and injury risk.Comparative data on cycling and motorcycling
behaviours is scarce, particularly at a global scale.Street view imagery, such
as Google Street View (GSV), combined with computer vision, is a valuable
resource for efficiently capturing travel behaviour data.This study
demonstrates a novel approach using deep learning on street view images to
estimate cycling and motorcycling levels across diverse cities worldwide.We
utilized data from 185 global cities.The data on mode shares of cycling and
motorcycling estimated using travel surveys or censuses.We used GSV images to
detect cycles and motorcycles in sampled locations, using 8000 images per
city.The YOLOv4 model, fine-tuned using images from six cities, achieved a mean
average precision of 89% for detecting cycles and motorcycles in GSV images.A
global prediction model was developed using beta regression with city-level
mode shares as outcome, with log transformed explanatory variables of counts of
GSV-detected images with cycles and motorcycles, while controlling for
population density.We found strong correlations between GSV motorcycle counts
and motorcycle mode share (0.78) and moderate correlations between GSV cycle
counts and cycling mode share (0.51).Beta regression models predicted mode
shares with $R^2$ values of 0.614 for cycling and 0.612 for motorcycling,
achieving median absolute errors (MDAE) of 1.3% and 1.4%,
respectively.Scatterplots demonstrated consistent prediction accuracy, though
cities like Utrecht and Cali were outliers.The model was applied to 60 cities
globally for which we didn't have recent mode share data.We provided estimates
for some cities in the Middle East, Latin America and East Asia.With computer
vision, GSV images capture travel modes and activity, providing insights
alongside traditional data sources.

</details>


### [118] [Morphological classification of eclipsing binary stars using computer vision methods](https://arxiv.org/abs/2508.12802)
*Štefan Parimucha,Maksim Gabdeev,Yanna Markus,Martin Vaňko,Pavol Gajdoš*

Main category: cs.CV

TL;DR: Computer vision models using ResNet50 and vision transformers achieve >96% accuracy for classifying eclipsing binary types but perform poorly on automated spot detection.


<details>
  <summary>Details</summary>
Motivation: To apply computer vision methods for automated classification of eclipsing binary light curves in large-scale astronomical surveys, addressing the need for efficient morphological analysis of these systems.

Method: Used pre-trained ResNet50 CNN and vision transformer models fine-tuned on synthetic datasets. Developed novel polar coordinate transformation with hexbin visualization of phase-folded light curves. Implemented hierarchical classification: first stage for detached/overcontact types, second stage for spot detection.

Result: High accuracy (>96%) on validation data across Gaia G, I, and TESS passbands. Strong performance (>94%, up to 100% for TESS) on observational data from OGLE, DEBCat, and WUMaCat catalogues. Poor performance on automated spot detection.

Conclusion: Computer vision shows great potential for eclipsing binary morphological classification in surveys, but current models are inadequate for detecting subtle photometric features like spots, requiring further research for robust automated spot detection.

Abstract: We present an application of computer vision methods to classify the light
curves of eclipsing binaries (EB). We have used pre-trained models based on
convolutional neural networks ($\textit{ResNet50}$) and vision transformers
($\textit{vit\_base\_patch16\_224}$), which were fine-tuned on images created
from synthetic datasets. To improve model generalisation and reduce
overfitting, we developed a novel image representation by transforming
phase-folded light curves into polar coordinates combined with hexbin
visualisation. Our hierarchical approach in the first stage classifies systems
into detached and overcontact types, and in the second stage identifies the
presence or absence of spots. The binary classification models achieved high
accuracy ($>96\%$) on validation data across multiple passbands (Gaia~$G$, $I$,
and $TESS$) and demonstrated strong performance ($>94\%$, up to $100\%$ for
$TESS$) when tested on extensive observational data from the OGLE, DEBCat, and
WUMaCat catalogues. While the primary binary classification was highly
successful, the secondary task of automated spot detection performed poorly,
revealing a significant limitation of our models for identifying subtle
photometric features. This study highlights the potential of computer vision
for EB morphological classification in large-scale surveys, but underscores the
need for further research into robust, automated spot detection.

</details>


### [119] [Next Visual Granularity Generation](https://arxiv.org/abs/2508.12811)
*Yikai Wang,Zhouxia Wang,Zhonghua Wu,Qingyi Tao,Kang Liao,Chen Change Loy*

Main category: cs.CV

TL;DR: NVG framework generates images through hierarchical visual granularity sequences, outperforming VAR models on ImageNet with improved FID scores.


<details>
  <summary>Details</summary>
Motivation: To achieve fine-grained control over image generation by decomposing images into structured sequences with different visual granularity levels, enabling progressive refinement from global layout to fine details.

Method: Proposes Next Visual Granularity (NVG) generation framework that starts from empty image and iteratively refines visual granularity sequences in a structured manner, training NVG models for class-conditional image generation on ImageNet.

Result: NVG consistently outperforms VAR series in FID scores (3.30->3.03, 2.57->2.44, 2.09->2.06) and shows clear scaling behavior on ImageNet dataset.

Conclusion: NVG framework successfully enables hierarchical image generation with fine-grained control across multiple granularity levels, demonstrating superior performance over existing methods.

Abstract: We propose a novel approach to image generation by decomposing an image into
a structured sequence, where each element in the sequence shares the same
spatial resolution but differs in the number of unique tokens used, capturing
different level of visual granularity. Image generation is carried out through
our newly introduced Next Visual Granularity (NVG) generation framework, which
generates a visual granularity sequence beginning from an empty image and
progressively refines it, from global layout to fine details, in a structured
manner. This iterative process encodes a hierarchical, layered representation
that offers fine-grained control over the generation process across multiple
granularity levels. We train a series of NVG models for class-conditional image
generation on the ImageNet dataset and observe clear scaling behavior. Compared
to the VAR series, NVG consistently outperforms it in terms of FID scores (3.30
-> 3.03, 2.57 ->2.44, 2.09 -> 2.06). We also conduct extensive analysis to
showcase the capability and potential of the NVG framework. Our code and models
will be released.

</details>


### [120] [SIS-Challenge: Event-based Spatio-temporal Instance Segmentation Challenge at the CVPR 2025 Event-based Vision Workshop](https://arxiv.org/abs/2508.12813)
*Friedhelm Hamann,Emil Mededovic,Fabian Gülhan,Yuli Wu,Johannes Stegmaier,Jing He,Yiqing Wang,Kexin Zhang,Lingling Li,Licheng Jiao,Mengru Ma,Hongxiang Huang,Yuhao Yan,Hongwei Ren,Xiaopeng Lin,Yulong Huang,Bojun Cheng,Se Hyun Lee,Gyu Sung Ham,Kanghan Oh,Gi Hyun Lim,Boxuan Yang,Bowen Du,Guillermo Gallego*

Main category: cs.CV

TL;DR: Overview of the CVPR 2025 Spatio-temporal Instance Segmentation challenge for event-based vision, including task description, dataset, results, and top methods.


<details>
  <summary>Details</summary>
Motivation: To advance research in spatio-temporal instance segmentation using event camera and grayscale camera data, providing a benchmark for evaluating methods that can handle dynamic visual data from neuromorphic sensors.

Method: Organized a challenge with defined object classes, provided spatio-temporally aligned event and grayscale camera data, and evaluated participants' methods on pixel-level segmentation accuracy.

Result: Presented challenge results and detailed descriptions of the top-5 ranking teams' approaches, with resources and code made publicly available.

Conclusion: The challenge successfully benchmarked state-of-the-art methods for spatio-temporal instance segmentation in event-based vision, providing valuable insights and resources for future research in this emerging field.

Abstract: We present an overview of the Spatio-temporal Instance Segmentation (SIS)
challenge held in conjunction with the CVPR 2025 Event-based Vision Workshop.
The task is to predict accurate pixel-level segmentation masks of defined
object classes from spatio-temporally aligned event camera and grayscale camera
data. We provide an overview of the task, dataset, challenge details and
results. Furthermore, we describe the methods used by the top-5 ranking teams
in the challenge. More resources and code of the participants' methods are
available here:
https://github.com/tub-rip/MouseSIS/blob/main/docs/challenge_results.md

</details>


### [121] [DEEP-SEA: Deep-Learning Enhancement for Environmental Perception in Submerged Aquatics](https://arxiv.org/abs/2508.12824)
*Shuang Chen,Ronald Thenius,Farshad Arvin,Amir Atapour-Abarghouei*

Main category: cs.CV

TL;DR: DEEP-SEA is a deep learning model that restores underwater images by enhancing both low- and high-frequency information while preserving spatial structures, addressing challenges of light scattering and turbidity in marine environments.


<details>
  <summary>Details</summary>
Motivation: Underwater environments suffer from light scattering, absorption and turbidity that degrade image clarity and distort color information, making accurate marine biodiversity monitoring and ecological assessment difficult.

Method: Proposes DEEP-SEA with Dual-Frequency Enhanced Self-Attention Spatial and Frequency Modulator to adaptively refine feature representations in frequency domains while preserving spatial information for better structural preservation.

Result: Comprehensive experiments on EUVP and LSUI datasets demonstrate superiority over state-of-the-art methods in restoring fine-grained image detail and structural consistency.

Conclusion: DEEP-SEA effectively mitigates underwater visual degradation and has potential to improve reliability of underwater monitoring platforms for ecological observation, species identification and autonomous navigation.

Abstract: Continuous and reliable underwater monitoring is essential for assessing
marine biodiversity, detecting ecological changes and supporting autonomous
exploration in aquatic environments. Underwater monitoring platforms rely on
mainly visual data for marine biodiversity analysis, ecological assessment and
autonomous exploration. However, underwater environments present significant
challenges due to light scattering, absorption and turbidity, which degrade
image clarity and distort colour information, which makes accurate observation
difficult. To address these challenges, we propose DEEP-SEA, a novel deep
learning-based underwater image restoration model to enhance both low- and
high-frequency information while preserving spatial structures. The proposed
Dual-Frequency Enhanced Self-Attention Spatial and Frequency Modulator aims to
adaptively refine feature representations in frequency domains and
simultaneously spatial information for better structural preservation. Our
comprehensive experiments on EUVP and LSUI datasets demonstrate the superiority
over the state of the art in restoring fine-grained image detail and structural
consistency. By effectively mitigating underwater visual degradation, DEEP-SEA
has the potential to improve the reliability of underwater monitoring platforms
for more accurate ecological observation, species identification and autonomous
navigation.

</details>


### [122] [Multi-source Multimodal Progressive Domain Adaption for Audio-Visual Deception Detection](https://arxiv.org/abs/2508.12842)
*Ronghao Lin,Sijie Mai,Ying Zeng,Qiaolin He,Aolin Xiong,Haifeng Hu*

Main category: cs.CV

TL;DR: Winning approach for multimodal deception detection challenge using progressive domain adaptation to handle domain shift across diverse audio-visual datasets


<details>
  <summary>Details</summary>
Motivation: Address domain shift issues across source and target domains in multimodal deception detection

Method: Multi-source Multimodal Progressive Domain Adaptation (MMPDA) framework that gradually aligns source and target domains at feature and decision levels

Result: Achieved Top-2 place with 60.43% accuracy and 56.99% F1-score, surpassing 1st place by 5.59% on F1-score and 3rd place by 6.75% on accuracy

Conclusion: The proposed MMPDA framework effectively bridges domain shifts across diverse multimodal datasets for deception detection

Abstract: This paper presents the winning approach for the 1st MultiModal Deception
Detection (MMDD) Challenge at the 1st Workshop on Subtle Visual Computing
(SVC). Aiming at the domain shift issue across source and target domains, we
propose a Multi-source Multimodal Progressive Domain Adaptation (MMPDA)
framework that transfers the audio-visual knowledge from diverse source domains
to the target domain. By gradually aligning source and the target domain at
both feature and decision levels, our method bridges domain shifts across
diverse multimodal datasets. Extensive experiments demonstrate the
effectiveness of our approach securing Top-2 place. Our approach reaches 60.43%
on accuracy and 56.99\% on F1-score on competition stage 2, surpassing the 1st
place team by 5.59% on F1-score and the 3rd place teams by 6.75% on accuracy.
Our code is available at https://github.com/RH-Lin/MMPDA.

</details>


### [123] [Cross-Domain Few-Shot Learning via Multi-View Collaborative Optimization with Vision-Language Models](https://arxiv.org/abs/2508.12861)
*Dexia Chen,Wentao Zhang,Qianjie Zhu,Ping Hu,Weibing Li,Tong Zhang,Ruixuan Wang*

Main category: cs.CV

TL;DR: CoMuCo is a novel fine-tuning strategy for vision-language models that uses multi-view feature extraction with consistency constraints to improve cross-domain few-shot performance.


<details>
  <summary>Details</summary>
Motivation: Current VLM transfer learning methods perform well on standard image datasets but struggle with cross-domain tasks where imaging domains differ from natural images.

Method: Uses two functionally complementary expert modules for multi-view feature extraction, incorporates prior knowledge-based consistency constraints and information geometry-based consensus mechanisms to enhance feature learning robustness.

Result: Extensive evaluations show CoMuCo consistently outperforms current methods in few-shot tasks across both existing and newly proposed cross-domain benchmarks.

Conclusion: The proposed CoMuCo strategy effectively addresses cross-domain limitations of VLMs and establishes a new benchmark for comprehensive evaluation of methods on imaging domains distinct from natural images.

Abstract: Vision-language models (VLMs) pre-trained on natural image and language data,
such as CLIP, have exhibited significant potential in few-shot image
recognition tasks, leading to development of various efficient transfer
learning methods. These methods exploit inherent pre-learned knowledge in VLMs
and have achieved strong performance on standard image datasets. However, their
effectiveness is often limited when confronted with cross-domain tasks where
imaging domains differ from natural images. To address this limitation, we
propose Consistency-guided Multi-view Collaborative Optimization (CoMuCo), a
novel fine-tuning strategy for VLMs. This strategy employs two functionally
complementary expert modules to extract multi-view features, while
incorporating prior knowledge-based consistency constraints and information
geometry-based consensus mechanisms to enhance the robustness of feature
learning. Additionally, a new cross-domain few-shot benchmark is established to
help comprehensively evaluate methods on imaging domains distinct from natural
images. Extensive empirical evaluations on both existing and newly proposed
benchmarks suggest CoMuCo consistently outperforms current methods in few-shot
tasks. The code and benchmark will be released.

</details>


### [124] [Preserve and Sculpt: Manifold-Aligned Fine-tuning of Vision-Language Models for Few-Shot Learning](https://arxiv.org/abs/2508.12877)
*Dexia Chen,Qianjie Zhu,Weibing Li,Yue Yu,Tong Zhang,Ruixuan Wang*

Main category: cs.CV

TL;DR: MPS-Tuning is a novel fine-tuning method for vision-language models that preserves the geometric structure of data distribution while enhancing class separability through manifold preservation and sculpting.


<details>
  <summary>Details</summary>
Motivation: Existing VLM fine-tuning methods neglect the geometric structure of data distribution, which can distort semantic representations. The authors aim to overcome this limitation by explicitly constraining the intrinsic geometry of the semantic manifold.

Method: MPS-Tuning treats data distribution as a semantic manifold and preserves both macroscopic and microscopic topological structures by aligning Gram matrices of features before and after fine-tuning. It also optimizes pairwise similarities between image and text modalities to enhance class discriminability.

Result: Extensive experiments demonstrate that MPS-Tuning significantly improves model performance while effectively preserving the structure of the semantic manifold.

Conclusion: The proposed MPS-Tuning method successfully addresses the limitation of previous VLM fine-tuning approaches by maintaining geometric structure while enhancing classification performance through manifold preservation and sculpting techniques.

Abstract: Pretrained vision-language models (VLMs), such as CLIP, have shown remarkable
potential in few-shot image classification and led to numerous effective
transfer learning strategies. These methods leverage the pretrained knowledge
of VLMs to enable effective domain adaptation while mitigating overfitting
through parameter-efficient tuning or instance-based consistency constraints.
However, such regularizations often neglect the geometric structure of data
distribution, which may lead to distortion of the overall semantic
representation. To overcome this limitation, we propose a novel fine-tuning
method, Manifold-Preserving and Sculpting Tuning (MPS-Tuning). Regarding the
data distribution in feature space as a semantic manifold, MPS-Tuning
explicitly constrains the intrinsic geometry of this manifold while further
sculpting it to enhance class separability. Specifically, MPS-Tuning preserves
both macroscopic and microscopic topological structures of the original
manifold by aligning Gram matrices of features before and after fine-tuning.
Theoretically, this constraint is shown to approximate an upper bound of the
Gromov-Wasserstein distance. Furthermore, features from the image and text
modalities are paired, and pairwise similarities are optimized to enhance the
manifold's class discriminability. Extensive experiments demonstrate that
MPS-Tuning significantly improves model performance while effectively
preserving the structure of the semantic manifold. The code will be released.

</details>


### [125] [S^2-Guidance: Stochastic Self Guidance for Training-Free Enhancement of Diffusion Models](https://arxiv.org/abs/2508.12880)
*Chubin Chen,Jiashu Zhu,Xiaokun Feng,Nisha Huang,Meiqi Wu,Fangyuan Mao,Jiahong Wu,Xiangxiang Chu,Xiu Li*

Main category: cs.CV

TL;DR: S^2-Guidance improves upon Classifier-free Guidance by using stochastic sub-networks to refine predictions and avoid low-quality outputs in diffusion models.


<details>
  <summary>Details</summary>
Motivation: Classifier-free Guidance (CFG) produces suboptimal results with semantic incoherence and low-quality outputs due to excessive reliance on imperfect predictions.

Method: Proposes S^2-Guidance which uses stochastic block-dropping during forward process to create stochastic sub-networks that guide the model away from low-quality predictions.

Result: Extensive experiments on text-to-image and text-to-video generation show S^2-Guidance consistently outperforms CFG and other advanced guidance strategies.

Conclusion: S^2-Guidance effectively addresses CFG's limitations by leveraging stochastic sub-networks to produce higher quality and more coherent outputs.

Abstract: Classifier-free Guidance (CFG) is a widely used technique in modern diffusion
models for enhancing sample quality and prompt adherence. However, through an
empirical analysis on Gaussian mixture modeling with a closed-form solution, we
observe a discrepancy between the suboptimal results produced by CFG and the
ground truth. The model's excessive reliance on these suboptimal predictions
often leads to semantic incoherence and low-quality outputs. To address this
issue, we first empirically demonstrate that the model's suboptimal predictions
can be effectively refined using sub-networks of the model itself. Building on
this insight, we propose S^2-Guidance, a novel method that leverages stochastic
block-dropping during the forward process to construct stochastic sub-networks,
effectively guiding the model away from potential low-quality predictions and
toward high-quality outputs. Extensive qualitative and quantitative experiments
on text-to-image and text-to-video generation tasks demonstrate that
S^2-Guidance delivers superior performance, consistently surpassing CFG and
other advanced guidance strategies. Our code will be released.

</details>


### [126] [ONG: One-Shot NMF-based Gradient Masking for Efficient Model Sparsification](https://arxiv.org/abs/2508.12891)
*Sankar Behera,Yamuna Prasad*

Main category: cs.CV

TL;DR: ONG is a one-shot pruning method using NMF and gradient masking that achieves comparable or better performance while maintaining target sparsity throughout training.


<details>
  <summary>Details</summary>
Motivation: Deep Neural Networks face deployment challenges due to large size, and existing pruning methods often involve complex iterative processes or struggle to maintain sparsity effectively during training.

Method: ONG uses Non-negative Matrix Factorization (NMF) for one-shot pruning at training start, then employs gradient masking to ensure only unpruned weights are updated, strictly preserving target sparsity.

Result: Experiments on CIFAR-10/100 with ResNet architectures show ONG achieves comparable or superior performance at various sparsity levels while maintaining structural integrity.

Conclusion: ONG provides an effective one-shot pruning approach with clear sparsity targeting mechanism and strict sparsity preservation during training.

Abstract: Deep Neural Networks (DNNs) have achieved remarkable success but their large
size poses deployment challenges. While various pruning techniques exist, many
involve complex iterative processes, specialized criteria, or struggle to
maintain sparsity effectively during training. We introduce ONG (One-shot
NMF-based Gradient Masking), a novel sparsification strategy that identifies
salient weight structures using Non-negative Matrix Factorization (NMF) for
one-shot pruning at the outset of training. Subsequently, ONG employs a precise
gradient masking mechanism to ensure that only unpruned weights are updated,
strictly preserving the target sparsity throughout the training phase. We
integrate ONG into the BIMP comparative framework and evaluate it on CIFAR-10
and CIFAR-100 with ResNet56, ResNet34, and ResNet18 against established stable
sparsification methods. Our experiments demonstrate ONG's ability to achieve
comparable or superior performance at various sparsity levels while maintaining
structural integrity post-pruning and offering a clear mechanism for targeting
desired sparsities.

</details>


### [127] [CTFlow: Video-Inspired Latent Flow Matching for 3D CT Synthesis](https://arxiv.org/abs/2508.12900)
*Jiayi Wang,Hadrien Reynaud,Franciskus Xaverius Erick,Bernhard Kainz*

Main category: cs.CV

TL;DR: CTFlow is a 0.5B latent flow matching transformer that generates entire 3D CT volumes from clinical reports, achieving state-of-the-art performance in temporal coherence, diversity, and text-image alignment.


<details>
  <summary>Details</summary>
Motivation: To accelerate medical research through data augmentation, enable privacy-preserving synthesis, and reduce regulatory constraints on patient data while preserving diagnostic signals from CT scans.

Method: Uses a 0.5B latent flow matching transformer conditioned on clinical reports, leveraging A-VAE from FLUX for latent space and CT-Clip text encoder. Employs custom autoregressive approach to generate consistent whole CT volumes by predicting sequences of slices iteratively.

Result: Superior performance compared to state-of-the-art generative CT models in terms of temporal coherence, image diversity, and text-image alignment, as measured by FID, FVD, IS scores and CLIP score.

Conclusion: CTFlow demonstrates effective generation of realistic 3D CT volumes from clinical reports, addressing memory constraints through innovative autoregressive sequence prediction while maintaining diagnostic quality.

Abstract: Generative modelling of entire CT volumes conditioned on clinical reports has
the potential to accelerate research through data augmentation,
privacy-preserving synthesis and reducing regulator-constraints on patient data
while preserving diagnostic signals. With the recent release of CT-RATE, a
large-scale collection of 3D CT volumes paired with their respective clinical
reports, training large text-conditioned CT volume generation models has become
achievable. In this work, we introduce CTFlow, a 0.5B latent flow matching
transformer model, conditioned on clinical reports. We leverage the A-VAE from
FLUX to define our latent space, and rely on the CT-Clip text encoder to encode
the clinical reports. To generate consistent whole CT volumes while keeping the
memory constraints tractable, we rely on a custom autoregressive approach,
where the model predicts the first sequence of slices of the volume from
text-only, and then relies on the previously generated sequence of slices and
the text, to predict the following sequence. We evaluate our results against
state-of-the-art generative CT model, and demonstrate the superiority of our
approach in terms of temporal coherence, image diversity and text-image
alignment, with FID, FVD, IS scores and CLIP score.

</details>


### [128] [CMF-IoU: Multi-Stage Cross-Modal Fusion 3D Object Detection with IoU Joint Prediction](https://arxiv.org/abs/2508.12917)
*Zhiwei Ning,Zhaojiang Liu,Xuanang Gao,Yifan Zuo,Jie Yang,Yuming Fang,Wei Liu*

Main category: cs.CV

TL;DR: CMF-IOU is a multi-stage cross-modal fusion framework for 3D object detection that effectively integrates LiDAR and camera data through depth completion, bilateral cross-view enhancement, and iterative refinement with IoU-aware proposal generation.


<details>
  <summary>Details</summary>
Motivation: Existing multi-modal 3D detection methods often use single or partial stage fusion, resulting in insufficient feature extraction and suboptimal performance. The challenge lies in effectively aligning 3D spatial information from LiDAR with 2D semantic information from cameras.

Method: 1) Project camera pixels into 3D space via depth completion to create pseudo points; 2) Use bilateral cross-view enhancement backbone with S2D branch for sparse LiDAR reinforcement and ResVC branch to mitigate inaccurate pseudo points; 3) Implement iterative voxel-point aware fine-grained pooling; 4) Design IoU joint prediction branch with novel proposal generation to preserve high-quality bounding boxes.

Result: Extensive experiments demonstrate superior performance on KITTI, nuScenes, and Waymo datasets, showing the effectiveness of the multi-stage fusion approach.

Conclusion: CMF-IOU successfully addresses the cross-modal fusion challenge through comprehensive multi-stage integration, achieving state-of-the-art performance by effectively combining 3D spatial and 2D semantic information from LiDAR and camera sensors.

Abstract: Multi-modal methods based on camera and LiDAR sensors have garnered
significant attention in the field of 3D detection. However, many prevalent
works focus on single or partial stage fusion, leading to insufficient feature
extraction and suboptimal performance. In this paper, we introduce a
multi-stage cross-modal fusion 3D detection framework, termed CMF-IOU, to
effectively address the challenge of aligning 3D spatial and 2D semantic
information. Specifically, we first project the pixel information into 3D space
via a depth completion network to get the pseudo points, which unifies the
representation of the LiDAR and camera information. Then, a bilateral
cross-view enhancement 3D backbone is designed to encode LiDAR points and
pseudo points. The first sparse-to-distant (S2D) branch utilizes an
encoder-decoder structure to reinforce the representation of sparse LiDAR
points. The second residual view consistency (ResVC) branch is proposed to
mitigate the influence of inaccurate pseudo points via both the 3D and 2D
convolution processes. Subsequently, we introduce an iterative voxel-point
aware fine grained pooling module, which captures the spatial information from
LiDAR points and textural information from pseudo points in the proposal
refinement stage. To achieve more precise refinement during iteration, an
intersection over union (IoU) joint prediction branch integrated with a novel
proposals generation technique is designed to preserve the bounding boxes with
both high IoU and classification scores. Extensive experiments show the
superior performance of our method on the KITTI, nuScenes and Waymo datasets.

</details>


### [129] [7Bench: a Comprehensive Benchmark for Layout-guided Text-to-image Models](https://arxiv.org/abs/2508.12919)
*Elena Izzo,Luca Parolari,Davide Vezzaro,Lamberto Ballan*

Main category: cs.CV

TL;DR: 7Bench is the first benchmark that jointly evaluates both semantic and spatial alignment in layout-guided text-to-image generation, addressing a critical gap in existing evaluation frameworks.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks only assess text alignment but overlook layout alignment, limiting the ability to evaluate spatial fidelity which is crucial for applications like synthetic data generation where errors can degrade data quality.

Method: The benchmark features text-and-layout pairs spanning seven challenging scenarios and proposes an evaluation protocol that incorporates layout alignment score alongside existing semantic evaluation frameworks to assess spatial accuracy.

Result: The benchmark was used to evaluate several state-of-the-art diffusion models, revealing their respective strengths and limitations across diverse alignment tasks including object generation, color fidelity, attribute recognition, inter-object relationships, and spatial control.

Conclusion: 7Bench provides a comprehensive evaluation framework for layout-guided text-to-image models, enabling better assessment of both semantic and spatial alignment capabilities, which is essential for real-world applications requiring precise control over generated content.

Abstract: Layout-guided text-to-image models offer greater control over the generation
process by explicitly conditioning image synthesis on the spatial arrangement
of elements. As a result, their adoption has increased in many computer vision
applications, ranging from content creation to synthetic data generation. A
critical challenge is achieving precise alignment between the image, textual
prompt, and layout, ensuring semantic fidelity and spatial accuracy. Although
recent benchmarks assess text alignment, layout alignment remains overlooked,
and no existing benchmark jointly evaluates both. This gap limits the ability
to evaluate a model's spatial fidelity, which is crucial when using
layout-guided generation for synthetic data, as errors can introduce noise and
degrade data quality. In this work, we introduce 7Bench, the first benchmark to
assess both semantic and spatial alignment in layout-guided text-to-image
generation. It features text-and-layout pairs spanning seven challenging
scenarios, investigating object generation, color fidelity, attribute
recognition, inter-object relationships, and spatial control. We propose an
evaluation protocol that builds on existing frameworks by incorporating the
layout alignment score to assess spatial accuracy. Using 7Bench, we evaluate
several state-of-the-art diffusion models, uncovering their respective
strengths and limitations across diverse alignment tasks. The benchmark is
available at https://github.com/Elizzo/7Bench.

</details>


### [130] [Towards High-Resolution Industrial Image Anomaly Detection](https://arxiv.org/abs/2508.12931)
*Ximiao Zhang,Min Xu,Xiuzhuang Zhou*

Main category: cs.CV

TL;DR: HiAD is a dual-branch framework for high-resolution anomaly detection that addresses the limitations of conventional methods by integrating multi-scale anomaly cues and adaptive detector assignment to handle both subtle and large anomalies efficiently.


<details>
  <summary>Details</summary>
Motivation: Current anomaly detection methods struggle with high-resolution images due to information loss from downsampling and inefficiency of simple tiling approaches, failing to meet industrial requirements for accuracy and computational efficiency.

Method: Proposes HiAD with dual-branch architecture for multi-scale anomaly integration, multi-resolution feature fusion for fine-grained texture handling, and adaptive detector pool with assignment strategies based on patch features.

Result: Extensive experiments on MVTec-HD, VisA-HD, and RealIAD-HD benchmarks demonstrate superior performance in detecting anomalies of varying sizes in high-resolution images under limited computational resources.

Conclusion: HiAD provides an effective and efficient solution for high-resolution anomaly detection, outperforming existing methods and meeting practical industrial demands through its adaptive multi-scale architecture.

Abstract: Current anomaly detection methods primarily focus on low-resolution
scenarios. For high-resolution images, conventional downsampling often results
in missed detections of subtle anomalous regions due to the loss of
fine-grained discriminative information. Despite some progress, recent studies
have attempted to improve detection resolution by employing lightweight
networks or using simple image tiling and ensemble methods. However, these
approaches still struggle to meet the practical demands of industrial scenarios
in terms of detection accuracy and efficiency. To address the above issues, we
propose HiAD, a general framework for high-resolution anomaly detection. HiAD
is capable of detecting anomalous regions of varying sizes in high-resolution
images under limited computational resources. Specifically, HiAD employs a
dual-branch architecture that integrates anomaly cues across different scales
to comprehensively capture both subtle and large-scale anomalies. Furthermore,
it incorporates a multi-resolution feature fusion strategy to tackle the
challenges posed by fine-grained texture variations in high-resolution images.
To enhance both adaptability and efficiency, HiAD utilizes a detector pool in
conjunction with various detector assignment strategies, enabling detectors to
be adaptively assigned based on patch features, ensuring detection performance
while effectively controlling computational costs. We conduct extensive
experiments on our specifically constructed high-resolution anomaly detection
benchmarks, including MVTec-HD, VisA-HD, and the real-world benchmark
RealIAD-HD, demonstrating the superior performance of HiAD. The code is
available at https://github.com/cnulab/HiAD.

</details>


### [131] [SEDEG:Sequential Enhancement of Decoder and Encoder's Generality for Class Incremental Learning with Small Memory](https://arxiv.org/abs/2508.12932)
*Hongyang Chen,Shaoling Pu,Lingyu Zheng,Zhongwu Sun*

Main category: cs.CV

TL;DR: SEDEG is a two-stage ViT framework that sequentially enhances both encoder and decoder generality through feature boosting and knowledge distillation to mitigate catastrophic forgetting in incremental learning, especially in small-memory scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing incremental learning methods focus on either encoder or decoder improvement, limiting effectiveness against catastrophic forgetting and performing poorly in small-memory settings where few historical samples can be stored.

Method: Two-stage training: 1) Train ensembled encoder via feature boosting to learn generalized representations that enhance decoder and balance classifier; 2) Use balanced KD and feature KD to compress ensembled encoder into a new more generalized encoder.

Result: Extensive experiments on three benchmark datasets show superior performance, with ablation studies confirming the efficacy of all components.

Conclusion: SEDEG effectively addresses catastrophic forgetting in incremental learning by sequentially improving both encoder and decoder generality, particularly demonstrating strong performance in challenging small-memory scenarios.

Abstract: In incremental learning, enhancing the generality of knowledge is crucial for
adapting to dynamic data inputs. It can develop generalized representations or
more balanced decision boundaries, preventing the degradation of long-term
knowledge over time and thus mitigating catastrophic forgetting. Some emerging
incremental learning methods adopt an encoder-decoder architecture and have
achieved promising results. In the encoder-decoder achitecture, improving the
generalization capabilities of both the encoder and decoder is critical, as it
helps preserve previously learned knowledge while ensuring adaptability and
robustness to new, diverse data inputs. However, many existing continual
methods focus solely on enhancing one of the two components, which limits their
effectiveness in mitigating catastrophic forgetting. And these methods perform
even worse in small-memory scenarios, where only a limited number of historical
samples can be stored. To mitigate this limitation, we introduces SEDEG, a
two-stage training framework for vision transformers (ViT), focusing on
sequentially improving the generality of both Decoder and Encoder. Initially,
SEDEG trains an ensembled encoder through feature boosting to learn generalized
representations, which subsequently enhance the decoder's generality and
balance the classifier. The next stage involves using knowledge distillation
(KD) strategies to compress the ensembled encoder and develop a new, more
generalized encoder. This involves using a balanced KD approach and feature KD
for effective knowledge transfer. Extensive experiments on three benchmark
datasets show SEDEG's superior performance, and ablation studies confirm the
efficacy of its components. The code is available at
https://github.com/ShaolingPu/CIL.

</details>


### [132] [Fully Automated Segmentation of Fiber Bundles in Anatomic Tracing Data](https://arxiv.org/abs/2508.12942)
*Kyriaki-Margarita Bintsi,Yaël Balbastre,Jingjing Wu,Julia F. Lehman,Suzanne N. Haber,Anastasia Yendiki*

Main category: cs.CV

TL;DR: Automated U-Net framework for fiber bundle segmentation in macaque tracer data with large patches, foreground-aware sampling, and semi-supervised pre-training, achieving 20% better sparse bundle detection and 40% lower FDR than state-of-the-art.


<details>
  <summary>Details</summary>
Motivation: Manual annotation of fiber bundles in histological slides is labor-intensive, and existing automated methods miss sparse bundles or require complex post-processing, limiting large-scale analysis of anatomic tracer studies for dMRI validation.

Method: U-Net architecture with large patch sizes, foreground aware sampling, and semi-supervised pre-training to enable automated fiber bundle segmentation in standalone histological slices without complex cross-section processing.

Result: 20% improvement in sparse bundle detection, 40% reduction in False Discovery Rate (FDR), elimination of terminal mislabeling errors, and capability to analyze individual slices without consecutive section requirements.

Conclusion: This automated framework enables large-scale analysis of anatomic tracing data, generating more ground-truth data to validate and optimize dMRI tractography methods efficiently.

Abstract: Anatomic tracer studies are critical for validating and improving diffusion
MRI (dMRI) tractography. However, large-scale analysis of data from such
studies is hampered by the labor-intensive process of annotating fiber bundles
manually on histological slides. Existing automated methods often miss sparse
bundles or require complex post-processing across consecutive sections,
limiting their flexibility and generalizability. We present a streamlined,
fully automated framework for fiber bundle segmentation in macaque tracer data,
based on a U-Net architecture with large patch sizes, foreground aware
sampling, and semisupervised pre-training. Our approach eliminates common
errors such as mislabeling terminals as bundles, improves detection of sparse
bundles by over 20% and reduces the False Discovery Rate (FDR) by 40% compared
to the state-of-the-art, all while enabling analysis of standalone slices. This
new framework will facilitate the automated analysis of anatomic tracing data
at a large scale, generating more ground-truth data that can be used to
validate and optimize dMRI tractography methods.

</details>


### [133] [Lumen: Consistent Video Relighting and Harmonious Background Replacement with Video Generative Models](https://arxiv.org/abs/2508.12945)
*Jianshu Zeng,Yuxuan Liu,Yutong Feng,Chenxuan Miao,Zixiang Gao,Jiwang Qu,Jianzhang Zhang,Bin Wang,Kun Yuan*

Main category: cs.CV

TL;DR: Lumen is an end-to-end video relighting framework that uses large-scale video generative models to replace backgrounds and adjust foreground lighting based on textual descriptions, achieving consistent cinematic results with strict foreground preservation.


<details>
  <summary>Details</summary>
Motivation: Video relighting is challenging but valuable for creating harmonious lighting adjustments in videos while preserving foreground properties and maintaining temporal consistency across frames.

Method: Uses an end-to-end framework built on large-scale video generative models with textual lighting instructions. Constructs a mixed dataset of realistic and synthetic videos using 3D rendering and HDR-based lighting simulation. Implements joint training with domain-aware adapter to decouple relighting learning from domain appearance distribution.

Result: Experimental results show Lumen effectively edits input videos into cinematic relighted videos with consistent lighting and strict foreground preservation, outperforming existing methods in comprehensive benchmark evaluations.

Conclusion: Lumen successfully addresses video relighting challenges by leveraging large-scale generative models and a carefully constructed mixed dataset, achieving high-quality results with temporal consistency and foreground property preservation.

Abstract: Video relighting is a challenging yet valuable task, aiming to replace the
background in videos while correspondingly adjusting the lighting in the
foreground with harmonious blending. During translation, it is essential to
preserve the original properties of the foreground, e.g., albedo, and propagate
consistent relighting among temporal frames. In this paper, we propose Lumen,
an end-to-end video relighting framework developed on large-scale video
generative models, receiving flexible textual description for instructing the
control of lighting and background. Considering the scarcity of high-qualified
paired videos with the same foreground in various lighting conditions, we
construct a large-scale dataset with a mixture of realistic and synthetic
videos. For the synthetic domain, benefiting from the abundant 3D assets in the
community, we leverage advanced 3D rendering engine to curate video pairs in
diverse environments. For the realistic domain, we adapt a HDR-based lighting
simulation to complement the lack of paired in-the-wild videos. Powered by the
aforementioned dataset, we design a joint training curriculum to effectively
unleash the strengths of each domain, i.e., the physical consistency in
synthetic videos, and the generalized domain distribution in realistic videos.
To implement this, we inject a domain-aware adapter into the model to decouple
the learning of relighting and domain appearance distribution. We construct a
comprehensive benchmark to evaluate Lumen together with existing methods, from
the perspectives of foreground preservation and video consistency assessment.
Experimental results demonstrate that Lumen effectively edit the input into
cinematic relighted videos with consistent lighting and strict foreground
preservation. Our project page: https://lumen-relight.github.io/

</details>


### [134] [MaskSem: Semantic-Guided Masking for Learning 3D Hybrid High-Order Motion Representation](https://arxiv.org/abs/2508.12948)
*Wei Wei,Shaojie Zhang,Yonghao Dang,Jianqin Yin*

Main category: cs.CV

TL;DR: MaskSem is a semantic-guided masking method for self-supervised skeleton-based action recognition that uses Grad-CAM to identify semantically rich joints and reconstructs hybrid high-order motion patterns (velocity + acceleration) to better understand complex human motions.


<details>
  <summary>Details</summary>
Motivation: Existing self-supervised skeleton-based action recognition methods focus on limited joints and low-order motion patterns, which restricts their ability to understand complex human motion patterns needed for effective human-robot collaboration.

Method: Proposes MaskSem framework that uses Grad-CAM based on relative motion to guide joint masking of semantically rich temporal regions, and reconstructs hybrid high-order motion targets combining low-order velocity and high-order acceleration.

Result: Experiments on NTU60, NTU120, and PKU-MMD datasets show that MaskSem combined with a vanilla transformer improves skeleton-based action recognition performance.

Conclusion: The semantic-guided masking and hybrid high-order motion reconstruction approach enhances motion pattern understanding, making it more suitable for human-robot interaction applications.

Abstract: Human action recognition is a crucial task for intelligent robotics,
particularly within the context of human-robot collaboration research. In
self-supervised skeleton-based action recognition, the mask-based
reconstruction paradigm learns the spatial structure and motion patterns of the
skeleton by masking joints and reconstructing the target from unlabeled data.
However, existing methods focus on a limited set of joints and low-order motion
patterns, limiting the model's ability to understand complex motion patterns.
To address this issue, we introduce MaskSem, a novel semantic-guided masking
method for learning 3D hybrid high-order motion representations. This novel
framework leverages Grad-CAM based on relative motion to guide the masking of
joints, which can be represented as the most semantically rich temporal
orgions. The semantic-guided masking process can encourage the model to explore
more discriminative features. Furthermore, we propose using hybrid high-order
motion as the reconstruction target, enabling the model to learn multi-order
motion patterns. Specifically, low-order motion velocity and high-order motion
acceleration are used together as the reconstruction target. This approach
offers a more comprehensive description of the dynamic motion process,
enhancing the model's understanding of motion patterns. Experiments on the
NTU60, NTU120, and PKU-MMD datasets show that MaskSem, combined with a vanilla
transformer, improves skeleton-based action recognition, making it more
suitable for applications in human-robot interaction.

</details>


### [135] [Breaking Reward Collapse: Adaptive Reinforcement for Open-ended Medical Reasoning with Enhanced Semantic Discrimination](https://arxiv.org/abs/2508.12957)
*Yizhou Liu,Jingwei Wei,Zizhi Chen,Minghao Han,Xukun Zhang,Keliang Liu,Lihua Zhang*

Main category: cs.CV

TL;DR: ARMed is a novel RL framework for open-ended medical VQA that addresses reward collapse by combining domain knowledge through SFT with adaptive semantic rewards, achieving significant improvements in accuracy and generalization.


<details>
  <summary>Details</summary>
Motivation: Existing reinforcement fine-tuning approaches in medical imaging primarily target closed-ended VQA, limiting real-world clinical applicability. Open-ended medical VQA better reflects clinical practice but suffers from reward collapse where semantically different responses receive similar scores.

Method: ARMed first incorporates domain knowledge through supervised fine-tuning on chain-of-thought data, then applies reinforcement learning with textual correctness and adaptive semantic rewards to enhance reasoning quality.

Result: ARMed achieves a 32.64% improvement on in-domain tasks and an 11.65% gain on out-of-domain benchmarks across six challenging medical VQA benchmarks.

Conclusion: The results highlight the critical role of reward discriminability in medical RL and demonstrate the promise of semantically guided rewards for enabling robust and clinically meaningful multimodal reasoning.

Abstract: Reinforcement learning (RL) with rule-based rewards has demonstrated strong
potential in enhancing the reasoning and generalization capabilities of
vision-language models (VLMs) and large language models (LLMs), while reducing
computational overhead. However, its application in medical imaging remains
underexplored. Existing reinforcement fine-tuning (RFT) approaches in this
domain primarily target closed-ended visual question answering (VQA), limiting
their applicability to real-world clinical reasoning. In contrast, open-ended
medical VQA better reflects clinical practice but has received limited
attention. While some efforts have sought to unify both formats via
semantically guided RL, we observe that model-based semantic rewards often
suffer from reward collapse, where responses with significant semantic
differences receive similar scores. To address this, we propose ARMed (Adaptive
Reinforcement for Medical Reasoning), a novel RL framework for open-ended
medical VQA. ARMed first incorporates domain knowledge through supervised
fine-tuning (SFT) on chain-of-thought data, then applies reinforcement learning
with textual correctness and adaptive semantic rewards to enhance reasoning
quality. We evaluate ARMed on six challenging medical VQA benchmarks. Results
show that ARMed consistently boosts both accuracy and generalization, achieving
a 32.64% improvement on in-domain tasks and an 11.65% gain on out-of-domain
benchmarks. These results highlight the critical role of reward
discriminability in medical RL and the promise of semantically guided rewards
for enabling robust and clinically meaningful multimodal reasoning.

</details>


### [136] [Multi-Phase Automated Segmentation of Dental Structures in CBCT Using a Lightweight Auto3DSeg and SegResNet Implementation](https://arxiv.org/abs/2508.12962)
*Dominic LaBella,Keshav Jha,Jared Robbins,Esther Yu*

Main category: cs.CV

TL;DR: Deep learning pipeline using 3D SegResNet architecture for automated multi-class tooth segmentation in dental CBCT scans, achieving 0.87 Dice score on validation data.


<details>
  <summary>Details</summary>
Motivation: Automated segmentation of dental structures in CBCT can assist in identifying pathology and facilitate radiation therapy planning for head and neck cancer patients.

Method: Used MONAI Auto3DSeg framework with 3D SegResNet, trained on 63 CBCT scans with 5-fold cross-validation. Preprocessing included image resampling and intensity clipping. Two-phase approach with ensemble fusion using Multi-Label STAPLE.

Result: Achieved an average Dice score of 0.87 on the ToothFairy3 challenge out-of-sample validation set.

Conclusion: The approach demonstrates effective automated dental segmentation that can improve patient care in radiation oncology through efficient identification of dental structures and pathology.

Abstract: Cone-beam computed tomography (CBCT) has become an invaluable imaging
modality in dentistry, enabling 3D visualization of teeth and surrounding
structures for diagnosis and treatment planning. Automated segmentation of
dental structures in CBCT can efficiently assist in identifying pathology
(e.g., pulpal or periapical lesions) and facilitate radiation therapy planning
in head and neck cancer patients. We describe the DLaBella29 team's approach
for the MICCAI 2025 ToothFairy3 Challenge, which involves a deep learning
pipeline for multi-class tooth segmentation. We utilized the MONAI Auto3DSeg
framework with a 3D SegResNet architecture, trained on a subset of the
ToothFairy3 dataset (63 CBCT scans) with 5-fold cross-validation. Key
preprocessing steps included image resampling to 0.6 mm isotropic resolution
and intensity clipping. We applied an ensemble fusion using Multi-Label STAPLE
on the 5-fold predictions to infer a Phase 1 segmentation and then conducted
tight cropping around the easily segmented Phase 1 mandible to perform Phase 2
segmentation on the smaller nerve structures. Our method achieved an average
Dice of 0.87 on the ToothFairy3 challenge out-of-sample validation set. This
paper details the clinical context, data preparation, model development,
results of our approach, and discusses the relevance of automated dental
segmentation for improving patient care in radiation oncology.

</details>


### [137] [GazeDETR: Gaze Detection using Disentangled Head and Gaze Representations](https://arxiv.org/abs/2508.12966)
*Ryan Anthony Jalova de Belen,Gelareh Mohammadi,Arcot Sowmya*

Main category: cs.CV

TL;DR: GazeDETR is a novel end-to-end architecture with two disentangled decoders that separately handle human head localization and gaze prediction, achieving state-of-the-art results on multiple datasets.


<details>
  <summary>Details</summary>
Motivation: Existing end-to-end gaze target detection models use a single decoder that creates entangled representations for both head localization and gaze prediction, which limits performance. There's a need for disentangled learning to better handle these distinct subtasks.

Method: Proposed GazeDETR architecture with two separate decoders - one for human head prediction (using local information) and another for gaze prediction (using both local and global information). The decoders utilize coherent attentive fields optimized for each specific subtask.

Result: Achieves state-of-the-art results on GazeFollow, VideoAttentionTarget, and ChildPlay datasets. Outperforms existing end-to-end models by a notable margin.

Conclusion: Disentangling the learning process with specialized decoders for head localization and gaze prediction significantly improves performance over single-decoder approaches, demonstrating the importance of task-specific representation learning in gaze communication analysis.

Abstract: Gaze communication plays a crucial role in daily social interactions.
Quantifying this behavior can help in human-computer interaction and digital
phenotyping. While end-to-end models exist for gaze target detection, they only
utilize a single decoder to simultaneously localize human heads and predict
their corresponding gaze (e.g., 2D points or heatmap) in a scene. This
multitask learning approach generates a unified and entangled representation
for human head localization and gaze location prediction. Herein, we propose
GazeDETR, a novel end-to-end architecture with two disentangled decoders that
individually learn unique representations and effectively utilize coherent
attentive fields for each subtask. More specifically, we demonstrate that its
human head predictor utilizes local information, while its gaze decoder
incorporates both local and global information. Our proposed architecture
achieves state-of-the-art results on the GazeFollow, VideoAttentionTarget and
ChildPlay datasets. It outperforms existing end-to-end models with a notable
margin.

</details>


### [138] [Compact Attention: Exploiting Structured Spatio-Temporal Sparsity for Fast Video Generation](https://arxiv.org/abs/2508.12969)
*Qirui Li,Guangcong Zheng,Qi Zhao,Jie Li,Bin Dong,Yiwu Yao,Xi Li*

Main category: cs.CV

TL;DR: Compact Attention is a hardware-aware acceleration framework that uses adaptive tiling and temporally varying windows to exploit structured sparsity patterns in video diffusion transformers, achieving 1.6-2.5x speedup while maintaining visual quality.


<details>
  <summary>Details</summary>
Motivation: Self-attention mechanisms in transformer-based video generation are computationally demanding for ultra-long sequences, and existing sparse attention methods fail to fully exploit the inherent spatio-temporal redundancies in video data.

Method: Three innovations: 1) Adaptive tiling strategies for dynamic tile grouping, 2) Temporally varying windows that adjust sparsity based on frame proximity, and 3) Automated configuration search algorithm to optimize sparse patterns while preserving critical attention pathways.

Result: Achieves 1.6~2.5x acceleration in attention computation on single-GPU setups while maintaining comparable visual quality with full-attention baselines.

Conclusion: Provides a principled approach to efficient long-form video generation through structured sparsity exploitation, unlocking better performance for video diffusion transformers.

Abstract: The computational demands of self-attention mechanisms pose a critical
challenge for transformer-based video generation, particularly in synthesizing
ultra-long sequences. Current approaches, such as factorized attention and
fixed sparse patterns, fail to fully exploit the inherent spatio-temporal
redundancies in video data. Through systematic analysis of video diffusion
transformers (DiT), we uncover a key insight: Attention matrices exhibit
structured, yet heterogeneous sparsity patterns, where specialized heads
dynamically attend to distinct spatiotemporal regions (e.g., local pattern,
cross-shaped pattern, or global pattern). Existing sparse attention methods
either impose rigid constraints or introduce significant overhead, limiting
their effectiveness. To address this, we propose Compact Attention, a
hardware-aware acceleration framework featuring three innovations: 1) Adaptive
tiling strategies that approximate diverse spatial interaction patterns via
dynamic tile grouping, 2) Temporally varying windows that adjust sparsity
levels based on frame proximity, and 3) An automated configuration search
algorithm that optimizes sparse patterns while preserving critical attention
pathways. Our method achieves 1.6~2.5x acceleration in attention computation on
single-GPU setups while maintaining comparable visual quality with
full-attention baselines. This work provides a principled approach to unlocking
efficient long-form video generation through structured sparsity exploitation.
Project Page: https://yo-ava.github.io/Compact-Attention.github.io/

</details>


### [139] [Dextr: Zero-Shot Neural Architecture Search with Singular Value Decomposition and Extrinsic Curvature](https://arxiv.org/abs/2508.12977)
*Rohan Asthana,Joschua Conrad,Maurits Ortmanns,Vasileios Belagiannis*

Main category: cs.CV

TL;DR: A zero-cost NAS proxy that uses SVD and extrinsic curvature to predict neural network performance without labeled data, achieving superior correlation and efficiency across multiple benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing zero-shot NAS proxies require labeled data and focus either on convergence/generalization or expressivity alone, but not both simultaneously in a label-free manner.

Method: Proposes a zero-cost proxy using Singular Value Decomposition (SVD) of layer features and extrinsic curvature of network output, formulated as a simplified harmonic mean of logarithms of feature condition number and curvature components.

Result: Superior performance on multiple correlation benchmarks (NAS-Bench-101, NAS-Bench-201, TransNAS-Bench-101-micro) and NAS tasks in DARTS and AutoFormer search spaces, using only a single label-free data sample.

Conclusion: The proposed proxy successfully combines convergence, generalization, and expressivity in a label-free approach, demonstrating high accuracy and efficiency in neural architecture search.

Abstract: Zero-shot Neural Architecture Search (NAS) typically optimises the
architecture search process by exploiting the network or gradient properties at
initialisation through zero-cost proxies. The existing proxies often rely on
labelled data, which is usually unavailable in real-world settings.
Furthermore, the majority of the current methods focus either on optimising the
convergence and generalisation attributes or solely on the expressivity of the
network architectures. To address both limitations, we first demonstrate how
channel collinearity affects the convergence and generalisation properties of a
neural network. Then, by incorporating the convergence, generalisation and
expressivity in one approach, we propose a zero-cost proxy that omits the
requirement of labelled data for its computation. In particular, we leverage
the Singular Value Decomposition (SVD) of the neural network layer features and
the extrinsic curvature of the network output to design our proxy. %As a
result, the proposed proxy is formulated as the simplified harmonic mean of the
logarithms of two key components: the sum of the inverse of the feature
condition number and the extrinsic curvature of the network output. Our
approach enables accurate prediction of network performance on test data using
only a single label-free data sample. Our extensive evaluation includes a total
of six experiments, including the Convolutional Neural Network (CNN) search
space, i.e. DARTS and the Transformer search space, i.e. AutoFormer. The
proposed proxy demonstrates a superior performance on multiple correlation
benchmarks, including NAS-Bench-101, NAS-Bench-201, and
TransNAS-Bench-101-micro; as well as on the NAS task within the DARTS and the
AutoFormer search space, all while being notably efficient. The code is
available at https://github.com/rohanasthana/Dextr.

</details>


### [140] [Omni Survey for Multimodality Analysis in Visual Object Tracking](https://arxiv.org/abs/2508.13000)
*Zhangyong Tang,Tianyang Xu,Xuefeng Zhu,Hui Li,Shaochuan Zhao,Tao Zhou,Chunyang Cheng,Xiaojun Wu,Josef Kittler*

Main category: cs.CV

TL;DR: A comprehensive survey of multi-modal visual object tracking (MMVOT) covering data collection, modality alignment, method categorization, evaluation, and analysis of 6 MMVOT tasks with 338 references.


<details>
  <summary>Details</summary>
Motivation: The rapid development of smart cities has generated massive multi-modal data, creating a need to understand and advance multi-modal visual object tracking techniques for comprehensive urban monitoring.

Method: The paper categorizes MMVOT methods based on how they handle visible (RGB) and auxiliary modalities (thermal, depth, event, NIR, language, sonar), analyzing data collection challenges, modality alignment, and annotation issues.

Result: The survey covers six MMVOT tasks, reveals long-tail distribution of object categories in existing datasets with noticeable lack of animal categories compared to RGB datasets, and questions when multi-modal tracking actually provides superior performance.

Conclusion: This comprehensive analysis provides foundational insights into MMVOT challenges and opportunities, highlighting the need for better dataset balance and understanding of when multi-modal fusion truly enhances tracking performance over single-modal approaches.

Abstract: The development of smart cities has led to the generation of massive amounts
of multi-modal data in the context of a range of tasks that enable a
comprehensive monitoring of the smart city infrastructure and services. This
paper surveys one of the most critical tasks, multi-modal visual object
tracking (MMVOT), from the perspective of multimodality analysis. Generally,
MMVOT differs from single-modal tracking in four key aspects, data collection,
modality alignment and annotation, model designing, and evaluation.
Accordingly, we begin with an introduction to the relevant data modalities,
laying the groundwork for their integration. This naturally leads to a
discussion of challenges of multi-modal data collection, alignment, and
annotation. Subsequently, existing MMVOT methods are categorised, based on
different ways to deal with visible (RGB) and X modalities: programming the
auxiliary X branch with replicated or non-replicated experimental
configurations from the RGB branch. Here X can be thermal infrared (T), depth
(D), event (E), near infrared (NIR), language (L), or sonar (S). The final part
of the paper addresses evaluation and benchmarking. In summary, we undertake an
omni survey of all aspects of multi-modal visual object tracking (VOT),
covering six MMVOT tasks and featuring 338 references in total. In addition, we
discuss the fundamental rhetorical question: Is multi-modal tracking always
guaranteed to provide a superior solution to unimodal tracking with the help of
information fusion, and if not, in what circumstances its application is
beneficial. Furthermore, for the first time in this field, we analyse the
distributions of the object categories in the existing MMVOT datasets,
revealing their pronounced long-tail nature and a noticeable lack of animal
categories when compared with RGB datasets.

</details>


### [141] [Empirical Evidences for the Effects of Feature Diversity in Open Set Recognition and Continual Learning](https://arxiv.org/abs/2508.13005)
*Jiawen Xu,Odej Kao*

Main category: cs.CV

TL;DR: Feature diversity improves open set recognition and continual learning performance by enhancing novel class detection and knowledge retention.


<details>
  <summary>Details</summary>
Motivation: To empirically investigate the role of feature diversity in addressing open set recognition (detecting novel classes) and continual learning (updating models with new classes), as most existing approaches use heuristic methods without directly examining feature diversity's impact.

Method: The study provides empirical evidence through experiments showing that enhancing feature diversity improves recognition of open set samples and facilitates both retention of previously learned data and integration of new data in continual learning scenarios.

Result: Increased feature diversity was found to significantly improve open set sample recognition and enhance performance in continual learning tasks, including better retention of old knowledge and integration of new knowledge.

Conclusion: Feature diversity plays a crucial role in both open set recognition and continual learning, and these findings should inspire further research into both practical methods and theoretical understanding in these domains.

Abstract: Open set recognition (OSR) and continual learning are two critical challenges
in machine learning, focusing respectively on detecting novel classes at
inference time and updating models to incorporate the new classes. While many
recent approaches have addressed these problems, particularly OSR, by
heuristically promoting feature diversity, few studies have directly examined
the role that feature diversity plays in tackling them. In this work, we
provide empirical evidence that enhancing feature diversity improves the
recognition of open set samples. Moreover, increased feature diversity also
facilitates both the retention of previously learned data and the integration
of new data in continual learning. We hope our findings can inspire further
research into both practical methods and theoretical understanding in these
domains.

</details>


### [142] [SlimComm: Doppler-Guided Sparse Queries for Bandwidth-Efficient Cooperative 3-D Perception](https://arxiv.org/abs/2508.13007)
*Melih Yazgan,Qiyuan Wu,Iramm Hamdard,Shiqi Li,J. Marius Zoellner*

Main category: cs.CV

TL;DR: SlimComm reduces communication bandwidth by 90% for collaborative perception in autonomous vehicles using 4D radar Doppler and query-driven sparse feature sharing.


<details>
  <summary>Details</summary>
Motivation: Transmitting dense Bird's-Eye-View feature maps overwhelms bandwidth for inter-vehicle communication in collaborative autonomous driving systems.

Method: Integrates 4D radar Doppler with query-driven sparse scheme, builds motion-centric dynamic map, generates reference queries for dynamic/high-confidence regions and exploratory queries for occluded areas, exchanges only query-specific BEV features using multi-scale gated deformable attention.

Result: Achieves up to 90% lower bandwidth than full-map sharing while matching or surpassing prior baselines across varied traffic densities and occlusions.

Conclusion: SlimComm provides an effective communication-efficient framework for collaborative perception that maintains accuracy while drastically reducing bandwidth requirements.

Abstract: Collaborative perception allows connected autonomous vehicles (CAVs) to
overcome occlusion and limited sensor range by sharing intermediate features.
Yet transmitting dense Bird's-Eye-View (BEV) feature maps can overwhelm the
bandwidth available for inter-vehicle communication. We present SlimComm, a
communication-efficient framework that integrates 4D radar Doppler with a
query-driven sparse scheme. SlimComm builds a motion-centric dynamic map to
distinguish moving from static objects and generates two query types: (i)
reference queries on dynamic and high-confidence regions, and (ii) exploratory
queries probing occluded areas via a two-stage offset. Only query-specific BEV
features are exchanged and fused through multi-scale gated deformable
attention, reducing payload while preserving accuracy. For evaluation, we
release OPV2V-R and Adver-City-R, CARLA-based datasets with per-point Doppler
radar. SlimComm achieves up to 90% lower bandwidth than full-map sharing while
matching or surpassing prior baselines across varied traffic densities and
occlusions. Dataset and code will be available at: https://url.fzi.de/SlimComm.

</details>


### [143] [Matrix-Game 2.0: An Open-Source, Real-Time, and Streaming Interactive World Model](https://arxiv.org/abs/2508.13009)
*Xianglong He,Chunli Peng,Zexiang Liu,Boyang Wang,Yifan Zhang,Qi Cui,Fei Kang,Biao Jiang,Mengyin An,Yangyang Ren,Baixin Xu,Hao-Xiang Guo,Kaixiong Gong,Cyrus Wu,Wei Li,Xuchen Song,Yang Liu,Eric Li,Yahui Zhou*

Main category: cs.CV

TL;DR: Matrix-Game 2.0 is a real-time interactive world model that generates minute-long videos at 25 FPS using few-step auto-regressive diffusion, addressing the speed limitations of previous methods.


<details>
  <summary>Details</summary>
Motivation: Existing interactive world models suffer from slow inference due to bidirectional attention and lengthy steps, making real-time simulation of dynamic environments impossible.

Method: Three key components: scalable data pipeline producing 1200 hours of annotated video from Unreal Engine/GTA5, action injection module for frame-level inputs, and few-step distillation with causal architecture.

Result: Achieves high-quality minute-level video generation across diverse scenes at 25 FPS, enabling real-time interactive simulations.

Conclusion: The framework advances interactive world modeling by enabling real-time video generation with immediate outcome updates based on historical context and current actions.

Abstract: Recent advances in interactive video generations have demonstrated diffusion
model's potential as world models by capturing complex physical dynamics and
interactive behaviors. However, existing interactive world models depend on
bidirectional attention and lengthy inference steps, severely limiting
real-time performance. Consequently, they are hard to simulate real-world
dynamics, where outcomes must update instantaneously based on historical
context and current actions. To address this, we present Matrix-Game 2.0, an
interactive world model generates long videos on-the-fly via few-step
auto-regressive diffusion. Our framework consists of three key components: (1)
A scalable data production pipeline for Unreal Engine and GTA5 environments to
effectively produce massive amounts (about 1200 hours) of video data with
diverse interaction annotations; (2) An action injection module that enables
frame-level mouse and keyboard inputs as interactive conditions; (3) A few-step
distillation based on the casual architecture for real-time and streaming video
generation. Matrix Game 2.0 can generate high-quality minute-level videos
across diverse scenes at an ultra-fast speed of 25 FPS. We open-source our
model weights and codebase to advance research in interactive world modeling.

</details>


### [144] [EgoTwin: Dreaming Body and View in First Person](https://arxiv.org/abs/2508.13013)
*Jingqiao Xiu,Fangzhou Hong,Yicong Li,Mengze Li,Wentao Wang,Sirui Han,Liang Pan,Ziwei Liu*

Main category: cs.CV

TL;DR: EgoTwin is a joint egocentric video and human motion generation framework that addresses viewpoint alignment and causal interplay challenges using diffusion transformers with head-centric motion representation and cybernetics-inspired interaction.


<details>
  <summary>Details</summary>
Motivation: Egocentric video generation remains underexplored compared to exocentric synthesis, requiring modeling of first-person view content with camera motion patterns from body movements, creating a need for joint video-motion generation.

Method: Proposes EgoTwin framework built on diffusion transformer architecture with head-centric motion representation (anchoring motion to head joint) and cybernetics-inspired interaction mechanism that captures causal interplay between video and motion within attention operations.

Result: Extensive experiments demonstrate effectiveness of EgoTwin framework, supported by a curated large-scale real-world dataset of synchronized text-video-motion triplets and novel metrics for video-motion consistency evaluation.

Conclusion: EgoTwin successfully bridges the gap in egocentric video generation by addressing key challenges of viewpoint alignment and causal interplay through innovative motion representation and interaction mechanisms.

Abstract: While exocentric video synthesis has achieved great progress, egocentric
video generation remains largely underexplored, which requires modeling
first-person view content along with camera motion patterns induced by the
wearer's body movements. To bridge this gap, we introduce a novel task of joint
egocentric video and human motion generation, characterized by two key
challenges: 1) Viewpoint Alignment: the camera trajectory in the generated
video must accurately align with the head trajectory derived from human motion;
2) Causal Interplay: the synthesized human motion must causally align with the
observed visual dynamics across adjacent video frames. To address these
challenges, we propose EgoTwin, a joint video-motion generation framework built
on the diffusion transformer architecture. Specifically, EgoTwin introduces a
head-centric motion representation that anchors the human motion to the head
joint and incorporates a cybernetics-inspired interaction mechanism that
explicitly captures the causal interplay between video and motion within
attention operations. For comprehensive evaluation, we curate a large-scale
real-world dataset of synchronized text-video-motion triplets and design novel
metrics to assess video-motion consistency. Extensive experiments demonstrate
the effectiveness of the EgoTwin framework.

</details>


### [145] [HierAdaptMR: Cross-Center Cardiac MRI Reconstruction with Hierarchical Feature Adapters](https://arxiv.org/abs/2508.13026)
*Ruru Xu,Ilkay Oksuz*

Main category: cs.CV

TL;DR: HierAdaptMR is a hierarchical feature adaptation framework for cardiac MRI reconstruction that addresses multi-level domain variations across clinical centers using parameter-efficient adapters for protocol-level and center-level variations, with a universal adapter for unseen centers.


<details>
  <summary>Details</summary>
Motivation: Deep learning-based cardiac MRI reconstruction faces significant domain shift challenges when deployed across multiple clinical centers with heterogeneous scanner configurations and imaging protocols, requiring robust cross-center generalization.

Method: Uses hierarchical adapters: Protocol-Level Adapters for sequence-specific characteristics, Center-Level Adapters for scanner-dependent variations, and a Universal Adapter for unseen centers through stochastic training. Built on variational unrolling backbone with multi-scale SSIM loss, frequency domain enhancement, and contrast-adaptive weighting.

Result: Comprehensive evaluation on CMRxRecon2025 dataset spanning 5+ centers, 10+ scanners, and 9 modalities demonstrates superior cross-center generalization while maintaining reconstruction quality.

Conclusion: HierAdaptMR effectively addresses multi-level domain variations in cardiac MRI reconstruction through parameter-efficient hierarchical adaptation, enabling robust performance across diverse clinical centers and scanner configurations.

Abstract: Deep learning-based cardiac MRI reconstruction faces significant domain shift
challenges when deployed across multiple clinical centers with heterogeneous
scanner configurations and imaging protocols. We propose HierAdaptMR, a
hierarchical feature adaptation framework that addresses multi-level domain
variations through parameter-efficient adapters. Our method employs
Protocol-Level Adapters for sequence-specific characteristics and Center-Level
Adapters for scanner-dependent variations, built upon a variational unrolling
backbone. A Universal Adapter enables generalization to entirely unseen centers
through stochastic training that learns center-invariant adaptations. The
framework utilizes multi-scale SSIM loss with frequency domain enhancement and
contrast-adaptive weighting for robust optimization. Comprehensive evaluation
on the CMRxRecon2025 dataset spanning 5+ centers, 10+ scanners, and 9
modalities demonstrates superior cross-center generalization while maintaining
reconstruction quality. code: https://github.com/Ruru-Xu/HierAdaptMR

</details>


### [146] [IntelliCap: Intelligent Guidance for Consistent View Sampling](https://arxiv.org/abs/2508.13043)
*Ayaka Yasunaga,Hideo Saito,Dieter Schmalstieg,Shohei Mori*

Main category: cs.CV

TL;DR: A novel situated visualization technique that guides users during scene scanning by identifying important objects needing extended image coverage through semantic segmentation and vision-language models, improving view synthesis quality.


<details>
  <summary>Details</summary>
Motivation: High-quality view synthesis requires uniform and dense view sampling, but human camera operators often struggle with this due to impatience, lack of scene understanding, or time constraints. Existing guidance methods focus on single objects or ignore view-dependent material characteristics.

Method: Leverages semantic segmentation and category identification ranked by a vision-language model to identify important objects requiring extended image coverage. Generates spherical proxies around highly ranked objects to guide users during scanning at multiple scales.

Result: The method shows superior performance in real scenes compared to conventional view sampling strategies, enabling better representation of view-dependent appearance.

Conclusion: The proposed situated visualization technique effectively addresses the challenge of guiding human operators during image acquisition for high-quality view synthesis, particularly for capturing view-dependent material characteristics across multiple scales.

Abstract: Novel view synthesis from images, for example, with 3D Gaussian splatting,
has made great progress. Rendering fidelity and speed are now ready even for
demanding virtual reality applications. However, the problem of assisting
humans in collecting the input images for these rendering algorithms has
received much less attention. High-quality view synthesis requires uniform and
dense view sampling. Unfortunately, these requirements are not easily addressed
by human camera operators, who are in a hurry, impatient, or lack understanding
of the scene structure and the photographic process. Existing approaches to
guide humans during image acquisition concentrate on single objects or neglect
view-dependent material characteristics. We propose a novel situated
visualization technique for scanning at multiple scales. During the scanning of
a scene, our method identifies important objects that need extended image
coverage to properly represent view-dependent appearance. To this end, we
leverage semantic segmentation and category identification, ranked by a
vision-language model. Spherical proxies are generated around highly ranked
objects to guide the user during scanning. Our results show superior
performance in real scenes compared to conventional view sampling strategies.

</details>


### [147] [Odo: Depth-Guided Diffusion for Identity-Preserving Body Reshaping](https://arxiv.org/abs/2508.13065)
*Siddharth Khandelwal,Sridhar Kamath,Arjun Jain*

Main category: cs.CV

TL;DR: Odo is a diffusion-based method for realistic human body shape editing that uses a new large-scale dataset and combines frozen UNet with ControlNet to transform body shapes while preserving identity, clothing, and background details.


<details>
  <summary>Details</summary>
Motivation: Human shape editing remains underexplored compared to pose editing, with current methods suffering from unrealistic proportions, texture distortions, and background inconsistencies due to lack of proper datasets and alignment errors.

Method: End-to-end diffusion-based approach combining a frozen UNet to preserve appearance and background details with a ControlNet that guides shape transformation using target SMPL depth maps, trained on a new large-scale dataset of 18,573 images across 1523 subjects.

Result: Achieves per-vertex reconstruction error of 7.5mm (significantly lower than baseline 13.6mm), produces realistic results that accurately match target shapes while preserving identity, clothing, and background consistency.

Conclusion: The proposed Odo method with the new large-scale dataset enables realistic and intuitive body reshaping guided by semantic attributes, outperforming prior approaches in both quantitative metrics and visual quality.

Abstract: Human shape editing enables controllable transformation of a person's body
shape, such as thin, muscular, or overweight, while preserving pose, identity,
clothing, and background. Unlike human pose editing, which has advanced
rapidly, shape editing remains relatively underexplored. Current approaches
typically rely on 3D morphable models or image warping, often introducing
unrealistic body proportions, texture distortions, and background
inconsistencies due to alignment errors and deformations. A key limitation is
the lack of large-scale, publicly available datasets for training and
evaluating body shape manipulation methods. In this work, we introduce the
first large-scale dataset of 18,573 images across 1523 subjects, specifically
designed for controlled human shape editing. It features diverse variations in
body shape, including fat, muscular and thin, captured under consistent
identity, clothing, and background conditions. Using this dataset, we propose
Odo, an end-to-end diffusion-based method that enables realistic and intuitive
body reshaping guided by simple semantic attributes. Our approach combines a
frozen UNet that preserves fine-grained appearance and background details from
the input image with a ControlNet that guides shape transformation using target
SMPL depth maps. Extensive experiments demonstrate that our method outperforms
prior approaches, achieving per-vertex reconstruction errors as low as 7.5mm,
significantly lower than the 13.6mm observed in baseline methods, while
producing realistic results that accurately match the desired target shapes.

</details>


### [148] [Eyes on the Image: Gaze Supervised Multimodal Learning for Chest X-ray Diagnosis and Report Generation](https://arxiv.org/abs/2508.13068)
*Tanjim Islam Riju,Shuchismita Anwar,Saman Sarker Joy,Farig Sadeque,Swakkhar Shatabda*

Main category: cs.CV

TL;DR: Two-stage multimodal framework using radiologist eye-tracking data to improve chest X-ray disease classification and generate region-aware radiology reports, achieving significant performance gains in both tasks.


<details>
  <summary>Details</summary>
Motivation: To enhance medical image analysis by incorporating radiologists' visual attention patterns (eye-tracking data) for improved disease classification and more interpretable, region-aware radiology report generation.

Method: Two-stage approach: 1) Gaze-guided contrastive learning with multi-term gaze-attention loss (MSE, KL divergence, correlation, center-of-mass alignment) for disease classification; 2) Modular report generation pipeline extracting confidence-weighted keywords, anatomical region mapping, and structured prompt-based sentence generation.

Result: Classification: F1 score improved from 0.597 to 0.631 (+5.70%), AUC from 0.821 to 0.849 (+3.41%). Report generation: Improved clinical keyword recall and ROUGE overlap metrics.

Conclusion: Integrating gaze data significantly enhances both disease classification performance and the interpretability/quality of generated medical reports, demonstrating the value of radiologist visual attention patterns in medical AI systems.

Abstract: We propose a two-stage multimodal framework that enhances disease
classification and region-aware radiology report generation from chest X-rays,
leveraging the MIMIC-Eye dataset. In the first stage, we introduce a
gaze-guided contrastive learning architecture for disease classification. It
integrates visual features, clinical labels, bounding boxes, and radiologist
eye-tracking signals and is equipped with a novel multi-term gaze-attention
loss combining MSE, KL divergence, correlation, and center-of-mass alignment.
Incorporating fixations improves F1 score from 0.597 to 0.631 (+5.70%) and AUC
from 0.821 to 0.849 (+3.41%), while also improving precision and recall,
highlighting the effectiveness of gaze-informed attention supervision. In the
second stage, we present a modular report generation pipeline that extracts
confidence-weighted diagnostic keywords, maps them to anatomical regions using
a curated dictionary constructed from domain-specific priors, and generates
region-aligned sentences via structured prompts. This pipeline improves report
quality as measured by clinical keyword recall and ROUGE overlap. Our results
demonstrate that integrating gaze data improves both classification performance
and the interpretability of generated medical reports.

</details>


### [149] [ID-Card Synthetic Generation: Toward a Simulated Bona fide Dataset](https://arxiv.org/abs/2508.13078)
*Qingwen Zeng,Juan E. Tapia,Izan Garcia,Juan M. Espin,Christoph Busch*

Main category: cs.CV

TL;DR: Using Stable Diffusion to generate synthetic bona fide ID card images improves Presentation Attack Detection system performance by addressing data scarcity issues.


<details>
  <summary>Details</summary>
Motivation: Current PAD systems face challenges due to limited availability of genuine ID card images for training and increasing diversity of attack methods. Most existing approaches focus on generating attack samples but neglect the scarcity of bona fide images.

Method: Proposes using Stable Diffusion to generate synthetic bona fide ID card images, creating additional training data to improve detector generalization. The synthetic images are evaluated in both a system trained from scratch and a commercial PAD solution.

Result: The synthetic images are successfully identified as bona fide by the PAD system, leading to improved detection performance and helping overcome data restriction limitations.

Conclusion: Generating synthetic bona fide images using Stable Diffusion is an effective approach to enhance PAD system performance when genuine training data is scarce, demonstrating positive impact on detection capabilities.

Abstract: Nowadays, the development of a Presentation Attack Detection (PAD) system for
ID cards presents a challenge due to the lack of images available to train a
robust PAD system and the increase in diversity of possible attack instrument
species. Today, most algorithms focus on generating attack samples and do not
take into account the limited number of bona fide images. This work is one of
the first to propose a method for mimicking bona fide images by generating
synthetic versions of them using Stable Diffusion, which may help improve the
generalisation capabilities of the detector. Furthermore, the new images
generated are evaluated in a system trained from scratch and in a commercial
solution. The PAD system yields an interesting result, as it identifies our
images as bona fide, which has a positive impact on detection performance and
data restrictions.

</details>


### [150] [Checkmate: interpretable and explainable RSVQA is the endgame](https://arxiv.org/abs/2508.13086)
*Lucrezia Tosato,Christel Tartini Chappuis,Syrielle Montariol,Flora Weissgerber,Sylvain Lobry,Devis Tuia*

Main category: cs.CV

TL;DR: A novel RSVQA dataset called Chessboard with 3M+ questions and balanced answer distribution is introduced to address interpretability issues and shortcut learning. The Checkmate model provides fine-grained visual reasoning by linking answers to specific image cells.


<details>
  <summary>Details</summary>
Motivation: Current RSVQA models lack interpretability and explainability, suffering from dataset biases that lead to shortcut learning rather than genuine visual reasoning.

Method: Created Chessboard dataset with 3,123,253 questions and balanced answer distribution, then developed Checkmate model that identifies specific image cells relevant to its decisions for fine-grained visual reasoning.

Result: The approach improves transparency and supports more trustworthy decision-making across multiple model architectures in RSVQA systems.

Conclusion: The Chessboard dataset and Checkmate model successfully address interpretability and bias issues in RSVQA, enabling explainable and grounded visual reasoning through cell-level attribution.

Abstract: Remote Sensing Visual Question Answering (RSVQA) presents unique challenges
in ensuring that model decisions are both understandable and grounded in visual
content. Current models often suffer from a lack of interpretability and
explainability, as well as from biases in dataset distributions that lead to
shortcut learning. In this work, we tackle these issues by introducing a novel
RSVQA dataset, Chessboard, designed to minimize biases through 3'123'253
questions and a balanced answer distribution. Each answer is linked to one or
more cells within the image, enabling fine-grained visual reasoning.
  Building on this dataset, we develop an explainable and interpretable model
called Checkmate that identifies the image cells most relevant to its
decisions. Through extensive experiments across multiple model architectures,
we show that our approach improves transparency and supports more trustworthy
decision-making in RSVQA systems.

</details>


### [151] [DMS:Diffusion-Based Multi-Baseline Stereo Generation for Improving Self-Supervised Depth Estimation](https://arxiv.org/abs/2508.13091)
*Zihua Liu,Yizhou Li,Songyan Zhang,Masatoshi Okutomi*

Main category: cs.CV

TL;DR: DMS is a model-agnostic approach that uses diffusion models to synthesize novel views for self-supervised stereo matching and depth estimation, addressing occlusion issues without requiring labels.


<details>
  <summary>Details</summary>
Motivation: Self-supervised stereo matching and monocular depth estimation face challenges from photometric ambiguity in ill-posed regions like occlusions and out-of-frame areas, requiring better methods to establish explicit correspondences.

Method: Finetune Stable Diffusion to synthesize novel views along epipolar direction using directional prompts - left-left view, right-right view, and intermediate view between cameras to supplement occluded pixels for explicit photometric reconstruction.

Result: Up to 35% outlier reduction and state-of-the-art performance across multiple benchmark datasets, demonstrating significant improvement in self-supervised stereo matching and depth estimation.

Conclusion: DMS provides an effective plug-and-play solution that enhances self-supervised methods using only unlabeled stereo pairs, successfully addressing occlusion challenges through diffusion-based view synthesis.

Abstract: While supervised stereo matching and monocular depth estimation have advanced
significantly with learning-based algorithms, self-supervised methods using
stereo images as supervision signals have received relatively less focus and
require further investigation. A primary challenge arises from ambiguity
introduced during photometric reconstruction, particularly due to missing
corresponding pixels in ill-posed regions of the target view, such as
occlusions and out-of-frame areas. To address this and establish explicit
photometric correspondences, we propose DMS, a model-agnostic approach that
utilizes geometric priors from diffusion models to synthesize novel views along
the epipolar direction, guided by directional prompts. Specifically, we
finetune a Stable Diffusion model to simulate perspectives at key positions:
left-left view shifted from the left camera, right-right view shifted from the
right camera, along with an additional novel view between the left and right
cameras. These synthesized views supplement occluded pixels, enabling explicit
photometric reconstruction. Our proposed DMS is a cost-free, ''plug-and-play''
method that seamlessly enhances self-supervised stereo matching and monocular
depth estimation, and relies solely on unlabeled stereo image pairs for both
training and synthesizing. Extensive experiments demonstrate the effectiveness
of our approach, with up to 35% outlier reduction and state-of-the-art
performance across multiple benchmark datasets.

</details>


### [152] [Real-Time Beach Litter Detection and Counting: A Comparative Analysis of RT-DETR Model Variants](https://arxiv.org/abs/2508.13101)
*Miftahul Huda,Arsyiah Azahra,Putri Maulida Chairani,Dimas Rizky Ramadhani,Nabila Azhari,Ade Lailani*

Main category: cs.CV

TL;DR: RT-DETR-L offers better speed-accuracy balance than RT-DETR-X for real-time beach litter detection, making it more practical for field deployment despite slightly lower accuracy.


<details>
  <summary>Details</summary>
Motivation: Coastal pollution requires scalable automated monitoring solutions, and this study investigates the effectiveness of state-of-the-art object detection models for automated beach litter detection and counting.

Method: Comparative analysis of two RT-DETR variants (Large and Extra-Large) trained on coastal debris dataset, evaluating accuracy (mAP metrics) and computational performance (inference time).

Result: RT-DETR-X achieved marginally better accuracy (mAP@50: 0.816, mAP@50-95: 0.612) but RT-DETR-L was significantly faster (20.1ms vs 34.5ms inference time) with comparable accuracy (mAP@50: 0.810, mAP@50-95: 0.606).

Conclusion: RT-DETR-L provides a more practical and efficient solution for real-time field deployment due to its superior balance of processing speed and detection accuracy, highlighting important trade-offs between model complexity and operational viability.

Abstract: Coastal pollution is a pressing global environmental issue, necessitating
scalable and automated solutions for monitoring and management. This study
investigates the efficacy of the Real-Time Detection Transformer (RT-DETR), a
state-of-the-art, end-to-end object detection model, for the automated
detection and counting of beach litter. A rigorous comparative analysis is
conducted between two model variants, RT-DETR-Large (RT-DETR-L) and
RT-DETR-Extra-Large (RT-DETR-X), trained on a publicly available dataset of
coastal debris. The evaluation reveals that the RT-DETR-X model achieves
marginally superior accuracy, with a mean Average Precision at 50\% IoU
(mAP@50) of 0.816 and a mAP@50-95 of 0.612, compared to the RT-DETR-L model's
0.810 and 0.606, respectively. However, this minor performance gain is realized
at a significant computational cost; the RT-DETR-L model demonstrates a
substantially faster inference time of 20.1 ms versus 34.5 ms for the
RT-DETR-X. The findings suggest that the RT-DETR-L model offers a more
practical and efficient solution for real-time, in-field deployment due to its
superior balance of processing speed and detection accuracy. This research
provides valuable insights into the application of advanced Transformer-based
detectors for environmental conservation, highlighting the critical trade-offs
between model complexity and operational viability.

</details>


### [153] [Precise Action-to-Video Generation Through Visual Action Prompts](https://arxiv.org/abs/2508.13104)
*Yuang Wang,Chao Wen,Haoyu Guo,Sida Peng,Minghan Qin,Hujun Bao,Xiaowei Zhou,Ruizhen Hu*

Main category: cs.CV

TL;DR: Visual Action Prompts (VAP) use visual skeletons as domain-agnostic representations for precise action-to-video generation while maintaining cross-domain transferability of visual dynamics.


<details>
  <summary>Details</summary>
Motivation: Existing action-driven video generation methods face a precision-generality trade-off - text/primitive actions lack precision while agent-centric signals lack cross-domain transferability.

Method: Render actions into visual skeletons as domain-agnostic prompts, construct pipelines from human-object interactions and robotic manipulation data, integrate into pretrained video models via lightweight fine-tuning.

Result: Effective action control of complex interactions while preserving cross-domain dynamics, demonstrated on EgoVid, RT-1 and DROID datasets.

Conclusion: Visual skeletons provide a balanced solution for action precision and dynamic transferability in video generation across domains.

Abstract: We present visual action prompts, a unified action representation for
action-to-video generation of complex high-DoF interactions while maintaining
transferable visual dynamics across domains. Action-driven video generation
faces a precision-generality trade-off: existing methods using text, primitive
actions, or coarse masks offer generality but lack precision, while
agent-centric action signals provide precision at the cost of cross-domain
transferability. To balance action precision and dynamic transferability, we
propose to "render" actions into precise visual prompts as domain-agnostic
representations that preserve both geometric precision and cross-domain
adaptability for complex actions; specifically, we choose visual skeletons for
their generality and accessibility. We propose robust pipelines to construct
skeletons from two interaction-rich data sources - human-object interactions
(HOI) and dexterous robotic manipulation - enabling cross-domain training of
action-driven generative models. By integrating visual skeletons into
pretrained video generation models via lightweight fine-tuning, we enable
precise action control of complex interaction while preserving the learning of
cross-domain dynamics. Experiments on EgoVid, RT-1 and DROID demonstrate the
effectiveness of our proposed approach. Project page:
https://zju3dv.github.io/VAP/.

</details>


### [154] [Motion2Motion: Cross-topology Motion Transfer with Sparse Correspondence](https://arxiv.org/abs/2508.13139)
*Ling-Hao Chen,Yuhong Zhang,Zixin Yin,Zhiyang Dou,Xin Chen,Jingbo Wang,Taku Komura,Lei Zhang*

Main category: cs.CV

TL;DR: Motion2Motion is a training-free framework that transfers animations between characters with different skeletal topologies using sparse bone correspondences and minimal target examples.


<details>
  <summary>Details</summary>
Motivation: Existing motion retargeting techniques struggle with characters that have substantially different skeletal topologies due to topological inconsistencies and lack of large-scale paired motion datasets.

Method: Uses a training-free framework that works with only one or few example motions on the target skeleton, accessing sparse bone correspondences between source and target skeletons.

Result: Achieves efficient and reliable performance in both similar-skeleton and cross-species skeleton transfer scenarios, with successful integration in downstream applications.

Conclusion: Motion2Motion demonstrates practical utility for industrial applications and addresses the challenge of transferring animations across diverse skeletal topologies without requiring extensive training data.

Abstract: This work studies the challenge of transfer animations between characters
whose skeletal topologies differ substantially. While many techniques have
advanced retargeting techniques in decades, transfer motions across diverse
topologies remains less-explored. The primary obstacle lies in the inherent
topological inconsistency between source and target skeletons, which restricts
the establishment of straightforward one-to-one bone correspondences. Besides,
the current lack of large-scale paired motion datasets spanning different
topological structures severely constrains the development of data-driven
approaches. To address these limitations, we introduce Motion2Motion, a novel,
training-free framework. Simply yet effectively, Motion2Motion works with only
one or a few example motions on the target skeleton, by accessing a sparse set
of bone correspondences between the source and target skeletons. Through
comprehensive qualitative and quantitative evaluations, we demonstrate that
Motion2Motion achieves efficient and reliable performance in both
similar-skeleton and cross-species skeleton transfer scenarios. The practical
utility of our approach is further evidenced by its successful integration in
downstream applications and user interfaces, highlighting its potential for
industrial applications. Code and data are available at
https://lhchen.top/Motion2Motion.

</details>


### [155] [IGFuse: Interactive 3D Gaussian Scene Reconstruction via Multi-Scans Fusion](https://arxiv.org/abs/2508.13153)
*Wenhao Hu,Zesheng Li,Haonan Zhou,Liu Liu,Xuexiang Wen,Zhizhong Su,Xi Li,Gaoang Wang*

Main category: cs.CV

TL;DR: IGFuse reconstructs interactive 3D scenes by fusing multiple scans with object rearrangement to reveal occluded areas, using segmentation-aware Gaussian fields and consistency constraints.


<details>
  <summary>Details</summary>
Motivation: Existing 3D scene reconstruction methods suffer from object occlusions, limited sensor coverage, and rely on error-prone multi-stage pipelines or dense per-object scanning.

Method: Constructs segmentation-aware Gaussian fields with bi-directional photometric and semantic consistency across scans. Uses pseudo-intermediate scene state for alignment and collaborative co-pruning for geometry refinement.

Result: Enables high-fidelity rendering and object-level scene manipulation without dense observations or complex pipelines. Shows strong generalization to novel scene configurations.

Conclusion: IGFuse provides an effective framework for real-world 3D reconstruction and real-to-simulation transfer by leveraging multiple scans with natural object movements.

Abstract: Reconstructing complete and interactive 3D scenes remains a fundamental
challenge in computer vision and robotics, particularly due to persistent
object occlusions and limited sensor coverage. Multiview observations from a
single scene scan often fail to capture the full structural details. Existing
approaches typically rely on multi stage pipelines, such as segmentation,
background completion, and inpainting or require per-object dense scanning,
both of which are error-prone, and not easily scalable. We propose IGFuse, a
novel framework that reconstructs interactive Gaussian scene by fusing
observations from multiple scans, where natural object rearrangement between
captures reveal previously occluded regions. Our method constructs segmentation
aware Gaussian fields and enforces bi-directional photometric and semantic
consistency across scans. To handle spatial misalignments, we introduce a
pseudo-intermediate scene state for unified alignment, alongside collaborative
co-pruning strategies to refine geometry. IGFuse enables high fidelity
rendering and object level scene manipulation without dense observations or
complex pipelines. Extensive experiments validate the framework's strong
generalization to novel scene configurations, demonstrating its effectiveness
for real world 3D reconstruction and real-to-simulation transfer. Our project
page is available online.

</details>


### [156] [4DNeX: Feed-Forward 4D Generative Modeling Made Easy](https://arxiv.org/abs/2508.13154)
*Zhaoxi Chen,Tianqi Liu,Long Zhuo,Jiawei Ren,Zeng Tao,He Zhu,Fangzhou Hong,Liang Pan,Ziwei Liu*

Main category: cs.CV

TL;DR: 4DNeX is the first feed-forward framework for single-image to 4D (dynamic 3D) scene generation, using a fine-tuned video diffusion model with novel 6D video representation and large-scale 4D dataset.


<details>
  <summary>Details</summary>
Motivation: Existing methods require computationally intensive optimization or multi-frame video inputs, lacking efficient single-image to 4D generation solutions.

Method: Fine-tunes pretrained video diffusion model with 4DNeX-10M dataset, introduces unified 6D video representation (RGB+XYZ), and adaptation strategies for 4D modeling.

Result: Produces high-quality dynamic point clouds enabling novel-view video synthesis, outperforms existing methods in efficiency and generalizability.

Conclusion: Provides scalable solution for image-to-4D modeling and lays foundation for generative 4D world models simulating dynamic scene evolution.

Abstract: We present 4DNeX, the first feed-forward framework for generating 4D (i.e.,
dynamic 3D) scene representations from a single image. In contrast to existing
methods that rely on computationally intensive optimization or require
multi-frame video inputs, 4DNeX enables efficient, end-to-end image-to-4D
generation by fine-tuning a pretrained video diffusion model. Specifically, 1)
to alleviate the scarcity of 4D data, we construct 4DNeX-10M, a large-scale
dataset with high-quality 4D annotations generated using advanced
reconstruction approaches. 2) we introduce a unified 6D video representation
that jointly models RGB and XYZ sequences, facilitating structured learning of
both appearance and geometry. 3) we propose a set of simple yet effective
adaptation strategies to repurpose pretrained video diffusion models for 4D
modeling. 4DNeX produces high-quality dynamic point clouds that enable
novel-view video synthesis. Extensive experiments demonstrate that 4DNeX
outperforms existing 4D generation methods in efficiency and generalizability,
offering a scalable solution for image-to-4D modeling and laying the foundation
for generative 4D world models that simulate dynamic scene evolution.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [157] [Finite Automata Extraction: Low-data World Model Learning as Programs from Gameplay Video](https://arxiv.org/abs/2508.11836)
*Dave Goel,Matthew Guzdial,Anurag Sarkar*

Main category: cs.AI

TL;DR: FAE extracts neuro-symbolic world models from gameplay video as programs in Retro Coder DSL, achieving more precise environment modeling than neural approaches and more general code than prior DSL methods.


<details>
  <summary>Details</summary>
Motivation: Traditional neural network world models lack explainability and transferability of learned environment dynamics, while existing DSL-based approaches produce code that is not sufficiently general.

Method: Finite Automata Extraction (FAE) learns neuro-symbolic world models from gameplay video, representing them as programs in a novel domain-specific language called Retro Coder.

Result: FAE learns more precise environment models than neural world models and produces more general code compared to prior DSL-based approaches.

Conclusion: The proposed FAE approach successfully bridges the gap between neural and symbolic representations, creating explainable and transferable world models with improved precision and generality.

Abstract: World models are defined as a compressed spatial and temporal learned
representation of an environment. The learned representation is typically a
neural network, making transfer of the learned environment dynamics and
explainability a challenge. In this paper, we propose an approach, Finite
Automata Extraction (FAE), that learns a neuro-symbolic world model from
gameplay video represented as programs in a novel domain-specific language
(DSL): Retro Coder. Compared to prior world model approaches, FAE learns a more
precise model of the environment and more general code than prior DSL-based
approaches.

</details>


### [158] [EvoCut: Strengthening Integer Programs via Evolution-Guided Language Models](https://arxiv.org/abs/2508.11850)
*Milad Yazdani,Mahdi Mostajabdaveh,Samin Aref,Zirui Zhou*

Main category: cs.AI

TL;DR: EvoCut automates acceleration cut generation for integer programming using LLMs and evolutionary search, reducing optimality gaps by 17-57% and speeding up solutions up to 4x faster than standard methods.


<details>
  <summary>Details</summary>
Motivation: Integer programming is NP-hard and relies on manual design of acceleration cuts by experts, which is time-consuming and requires deep domain knowledge. There is a need to automate this creative process to improve solver performance.

Method: EvoCut combines large language models with evolutionary search: (i) initializes diverse candidate cuts via LLM-based agent, (ii) evaluates cuts on preservation of optimal solutions and ability to cut off fractional solutions, (iii) iteratively refines population through evolutionary crossover and mutation agents.

Result: EvoCut reduces optimality gap by 17-57% within fixed time, obtains same solutions up to 4 times faster, and achieves higher-quality solutions within same time limits compared to standard integer programming practice.

Conclusion: The framework successfully automates cut generation without human input, producing effective cuts that generalize to unseen instances, demonstrating significant performance improvements in integer programming solvers.

Abstract: Integer programming lies at the heart of crucial combinatorial optimization
tasks but remains challenging due to its NP-hard nature. An effective approach
for practically solving integer programs is the manual design of acceleration
cuts, i.e. inequalities that improve solver performance. However, this creative
process demands deep expertise and is yet to be automated. Our proposed
framework, EvoCut, automates the generation of acceleration cuts by combining
large language models (LLMs) with an evolutionary search. EvoCut (i)
initializes a diverse population of candidate cuts via an LLM-based initializer
agent; (ii) for each cut empirically evaluates both preservation of the optimal
solution and its ability to cut off fractional solutions across a verification
set; and (iii) iteratively refines the population through evolutionary
crossover and mutation agents. We quantify each cut's utility by its relative
reduction in the solver's optimality gap. Our comparisons against standard
integer programming practice show that EvoCut reduces optimality gap by 17-57%
within a fixed time. It obtains the same solutions up to 4 times as fast, and
obtains higher-quality solutions within the same time limit. Requiring no human
expert input, EvoCut reliably generates, improves, and empirically verifies
cuts that generalize to unseen instances. The code is available at
https://github.com/milad1378yz/EvoCut.

</details>


### [159] [LARC: Towards Human-level Constrained Retrosynthesis Planning through an Agentic Framework](https://arxiv.org/abs/2508.11860)
*Frazier N. Baker,Daniel Adu-Ampratwum,Reza Averly,Botao Yu,Huan Sun,Xia Ning*

Main category: cs.AI

TL;DR: LARC is the first LLM-based agentic framework for constrained retrosynthesis planning that uses agentic constraint evaluation and tool-based reasoning to guide route generation, achieving 72.9% success rate and approaching human expert performance.


<details>
  <summary>Details</summary>
Motivation: Constrained retrosynthesis planning is essential but challenging in chemistry for identifying synthetic routes from available materials to target molecules under practical constraints. Current approaches lack effective constraint integration.

Method: LARC incorporates agentic constraint evaluation through an Agent-as-a-Judge directly into retrosynthesis planning, using agentic feedback grounded in tool-based reasoning to guide and constrain route generation.

Result: LARC achieved 72.9% success rate on 48 constrained retrosynthesis tasks across 3 constraint types, vastly outperforming LLM baselines and approaching human expert-level success in substantially less time.

Conclusion: LARC framework is extensible and serves as a first step towards an effective agentic tool or co-scientist for human experts in constrained retrosynthesis planning.

Abstract: Large language model (LLM) agent evaluators leverage specialized tools to
ground the rational decision-making of LLMs, making them well-suited to aid in
scientific discoveries, such as constrained retrosynthesis planning.
Constrained retrosynthesis planning is an essential, yet challenging, process
within chemistry for identifying synthetic routes from commercially available
starting materials to desired target molecules, subject to practical
constraints. Here, we present LARC, the first LLM-based Agentic framework for
Retrosynthesis planning under Constraints. LARC incorporates agentic constraint
evaluation, through an Agent-as-a-Judge, directly into the retrosynthesis
planning process, using agentic feedback grounded in tool-based reasoning to
guide and constrain route generation. We rigorously evaluate LARC on a
carefully curated set of 48 constrained retrosynthesis planning tasks across 3
constraint types. LARC achieves a 72.9% success rate on these tasks, vastly
outperforming LLM baselines and approaching human expert-level success in
substantially less time. The LARC framework is extensible, and serves as a
first step towards an effective agentic tool or a co-scientist to human experts
for constrained retrosynthesis.

</details>


### [160] [QuarkMed Medical Foundation Model Technical Report](https://arxiv.org/abs/2508.11894)
*Ao Li,Bin Yan,Bingfeng Cai,Chenxi Li,Cunzhong Zhao,Fugen Yao,Gaoqiang Liu,Guanjun Jiang,Jian Xu,Liang Dong,Liansheng Sun,Rongshen Zhang,Xiaolei Gui,Xin Liu,Xin Shang,Yao Wu,Yu Cao,Zhenxin Ma,Zhuang Jia*

Main category: cs.AI

TL;DR: QuarkMed is a medical foundation model that achieves 70% accuracy on Chinese Medical Licensing Exam using curated data processing, medical RAG, and verifiable reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: Medical applications require specialized knowledge, professional accuracy, and customization that current LLMs lack, necessitating a robust medical foundation model.

Method: Leverages curated medical data processing, medical-content Retrieval-Augmented Generation (RAG), and large-scale verifiable reinforcement learning pipeline.

Result: Achieved 70% accuracy on Chinese Medical Licensing Examination with strong generalization across diverse medical benchmarks.

Conclusion: QuarkMed provides a powerful and versatile personal medical AI solution currently serving millions of users at ai.quark.cn.

Abstract: Recent advancements in large language models have significantly accelerated
their adoption in healthcare applications, including AI-powered medical
consultations, diagnostic report assistance, and medical search tools. However,
medical tasks often demand highly specialized knowledge, professional accuracy,
and customization capabilities, necessitating a robust and reliable foundation
model. QuarkMed addresses these needs by leveraging curated medical data
processing, medical-content Retrieval-Augmented Generation (RAG), and a
large-scale, verifiable reinforcement learning pipeline to develop a
high-performance medical foundation model. The model achieved 70% accuracy on
the Chinese Medical Licensing Examination, demonstrating strong generalization
across diverse medical benchmarks. QuarkMed offers a powerful yet versatile
personal medical AI solution, already serving over millions of users at
ai.quark.cn.

</details>


### [161] [CHBench: A Cognitive Hierarchy Benchmark for Evaluating Strategic Reasoning Capability of LLMs](https://arxiv.org/abs/2508.11944)
*Hongtao Liu,Zhicheng Du,Zihe Wang,Weiran Shen*

Main category: cs.AI

TL;DR: CHBench is a new evaluation framework using cognitive hierarchy models to assess LLMs' strategic reasoning in games, showing consistent reasoning levels across opponents and revealing that chat mechanisms degrade while memory mechanisms enhance strategic performance.


<details>
  <summary>Details</summary>
Motivation: Existing game-playing evaluations for LLMs rely on utility metrics that lack robustness due to variations in opponent behavior and game structure, requiring a more systematic approach to assess strategic reasoning capabilities.

Method: Proposed Cognitive Hierarchy Benchmark (CHBench) using a three-phase systematic framework with behavioral data from six state-of-the-art LLMs across fifteen normal-form games, analyzing reasoning levels through cognitive hierarchy models from behavioral economics.

Result: LLMs exhibit consistent strategic reasoning levels across diverse opponents, confirming framework robustness. Chat Mechanism significantly degrades strategic reasoning while Memory Mechanism enhances it.

Conclusion: CHBench provides a robust and generalizable tool for evaluating LLM strategic reasoning capabilities, with significant potential for future research and practical applications in assessing bounded rationality and reasoning depths.

Abstract: Game-playing ability serves as an indicator for evaluating the strategic
reasoning capability of large language models (LLMs). While most existing
studies rely on utility performance metrics, which are not robust enough due to
variations in opponent behavior and game structure. To address this limitation,
we propose \textbf{Cognitive Hierarchy Benchmark (CHBench)}, a novel evaluation
framework inspired by the cognitive hierarchy models from behavioral economics.
We hypothesize that agents have bounded rationality -- different agents behave
at varying reasoning depths/levels. We evaluate LLMs' strategic reasoning
through a three-phase systematic framework, utilizing behavioral data from six
state-of-the-art LLMs across fifteen carefully selected normal-form games.
Experiments show that LLMs exhibit consistent strategic reasoning levels across
diverse opponents, confirming the framework's robustness and generalization
capability. We also analyze the effects of two key mechanisms (Chat Mechanism
and Memory Mechanism) on strategic reasoning performance. Results indicate that
the Chat Mechanism significantly degrades strategic reasoning, whereas the
Memory Mechanism enhances it. These insights position CHBench as a promising
tool for evaluating LLM capabilities, with significant potential for future
research and practical applications.

</details>


### [162] [Data Mixing Optimization for Supervised Fine-Tuning of Large Language Models](https://arxiv.org/abs/2508.11953)
*Yuan Li,Zhengzhong Liu,Eric Xing*

Main category: cs.AI

TL;DR: A novel optimization method for data mixing in supervised fine-tuning of LLMs that minimizes validation loss by modeling effective data transfer and leveraging scaling laws, achieving performance comparable to grid search with only 0.66% higher domain loss.


<details>
  <summary>Details</summary>
Motivation: Optimizing data mixtures for SFT of LLMs is critical for developing general-purpose models but remains underexplored, with current approaches lacking systematic optimization frameworks.

Method: Frames data mixing as optimization problem, parametrizes loss by modeling effective data transferred and leveraging scaling laws for fine-tuning. Experiments with small-scale mixtures to fit parameters and derive optimal weights.

Result: Models trained with optimized weights perform on par with grid search optimal weights, with per-domain loss only 0.66% higher than best grid search results. Reweighting popular SFT datasets improves both validation loss and downstream performance.

Conclusion: The method provides an effective optimization framework for data mixing in SFT, generalizes to domain-specific model data selection, and offers valuable insights into supervised fine-tuning processes.

Abstract: Optimizing data mixtures for supervised fine-tuning (SFT) of large language
models (LLMs) is critical for developing general-purpose models, yet this area
remains underexplored. In this paper, we frame data mixing as an optimization
problem and introduce a novel method designed to minimize validation loss. Our
approach parametrizes the loss by modeling effective data transferred and
leveraging scaling laws for fine-tuning. By experimenting with various
small-scale data mixtures, we fit these parameters and derive the optimal
weights. We provide both mathematical proofs and empirical results
demonstrating that our algorithm achieves excellent overall and individual
performance across all domains. Through controlled experiments, we show that
models trained with our optimized weights perform on par with those using
optimal weights determined via grid search, with per-domain loss only 0.66%
higher than the best domain loss from grid search on average. Additionally, we
show that reweighting popular SFT datasets using our method improves both
validation loss and downstream performance. Finally, we discuss how our method
can generalize to guide data selection for domain-specific models and provide
insights into SFT.

</details>


### [163] [UniCast: A Unified Multimodal Prompting Framework for Time Series Forecasting](https://arxiv.org/abs/2508.11954)
*Sehyuk Park,Soyeon Caren Han,Eduard Hovy*

Main category: cs.AI

TL;DR: UniCast introduces a parameter-efficient multimodal framework that extends time series foundation models to incorporate visual and textual context, achieving superior forecasting performance through soft prompt tuning.


<details>
  <summary>Details</summary>
Motivation: Existing time series foundation models operate in unimodal settings, ignoring the rich multimodal context (visual and textual signals) that often accompanies real-world time series data, limiting their forecasting capabilities.

Method: Integrates modality-specific embeddings from pretrained Vision and Text Encoders with a frozen Time Series Foundation Model via soft prompt tuning, enabling efficient adaptation with minimal parameter updates while preserving the model's generalization strength.

Result: Extensive experiments across diverse time-series forecasting benchmarks demonstrate that UniCast consistently and significantly outperforms all existing TSFM baselines.

Conclusion: The findings highlight the critical role of multimodal context in advancing the next generation of general-purpose time series forecasters.

Abstract: Time series forecasting is a foundational task across domains, such as
finance, healthcare, and environmental monitoring. While recent advances in
Time Series Foundation Models (TSFMs) have demonstrated strong generalisation
through large-scale pretraining, existing models operate predominantly in a
unimodal setting, ignoring the rich multimodal context, such as visual and
textual signals, that often accompanies time series data in real-world
scenarios. This paper introduces a novel parameter-efficient multimodal
framework, UniCast, that extends TSFMs to jointly leverage time series, vision,
and text modalities for enhanced forecasting performance. Our method integrates
modality-specific embeddings from pretrained Vision and Text Encoders with a
frozen TSFM via soft prompt tuning, enabling efficient adaptation with minimal
parameter updates. This design not only preserves the generalisation strength
of the foundation model but also enables effective cross-modal interaction.
Extensive experiments across diverse time-series forecasting benchmarks
demonstrate that UniCast consistently and significantly outperforms all
existing TSFM baselines. The findings highlight the critical role of multimodal
context in advancing the next generation of general-purpose time series
forecasters.

</details>


### [164] [Rigorous Feature Importance Scores based on Shapley Value and Banzhaf Index](https://arxiv.org/abs/2508.11959)
*Xuanxiang Huang,Olivier Létoffé,Joao Marques-Silva*

Main category: cs.AI

TL;DR: Novel feature importance scores using Shapley value and Banzhaf index that incorporate non-WAXp sets to quantify feature effectiveness at excluding adversarial examples.


<details>
  <summary>Details</summary>
Motivation: Current feature attribution methods based on WAXp neglect the contribution of non-WAXp sets, which can provide important information about the relationship between formal explanations and adversarial examples.

Method: Leverage Shapley value and Banzhaf index to devise two novel feature importance scores that account for non-WAXp sets when computing feature contributions.

Result: The proposed scores effectively quantify how each feature contributes to excluding adversarial examples, with identified properties and computational complexity analysis.

Conclusion: The novel feature attribution approach provides more comprehensive feature importance assessment by incorporating both WAXp and non-WAXp sets, offering better insights into model behavior and adversarial example exclusion.

Abstract: Feature attribution methods based on game theory are ubiquitous in the field
of eXplainable Artificial Intelligence (XAI). Recent works proposed rigorous
feature attribution using logic-based explanations, specifically targeting
high-stakes uses of machine learning (ML) models. Typically, such works exploit
weak abductive explanation (WAXp) as the characteristic function to assign
importance to features. However, one possible downside is that the contribution
of non-WAXp sets is neglected. In fact, non-WAXp sets can also convey important
information, because of the relationship between formal explanations (XPs) and
adversarial examples (AExs). Accordingly, this paper leverages Shapley value
and Banzhaf index to devise two novel feature importance scores. We take into
account non-WAXp sets when computing feature contribution, and the novel scores
quantify how effective each feature is at excluding AExs. Furthermore, the
paper identifies properties and studies the computational complexity of the
proposed scores.

</details>


### [165] [Chart-CoCa: Self-Improving Chart Understanding of Vision LMs via Code-Driven Synthesis and Candidate-Conditioned Answering](https://arxiv.org/abs/2508.11975)
*Gongyao Jiang,Qiong Luo*

Main category: cs.AI

TL;DR: A self-improving method for Vision Language Models that uses synthetic chart data generation and candidate-conditioned answering to significantly improve chart understanding without human-labeled data.


<details>
  <summary>Details</summary>
Motivation: VLMs struggle with chart understanding tasks due to inaccurate descriptions and complex reasoning, and synthetic data generation often faces noise label challenges.

Method: Introduces a chart synthesis pipeline generating aligned chart-question-answer triplets via code generation/execution, plus a candidate-conditioned answering process where VLM generates multiple responses per query and synthesizes the final answer.

Result: Achieves up to 15.50 points accuracy gain over initial VLM in a fully self-improving paradigm without human-labeled data or external models.

Conclusion: The approach effectively addresses chart understanding challenges through reliable synthetic data generation and improved inference processes, enabling significant performance gains without external supervision.

Abstract: Vision Language Models (VLMs) often struggle with chart understanding tasks,
particularly in accurate chart description and complex reasoning. Synthetic
data generation is a promising solution, while usually facing the challenge of
noise labels. To address this challenge, we first introduce a chart synthesis
pipeline that generates aligned chart-question-answer triplets through code
generation and execution, ensuring the reliability of synthetic data without
human intervention. Furthermore, inspired by test-time scaling that increases
inference budget and thereby improves performance, we design a
candidate-conditioned answering process. The VLM first generates multiple
responses per query, and then synthesizes the final answer by contextualizing
these candidates. Experiments demonstrate significant improvements, with up to
15.50 points accuracy gain over the initial VLM, in a fully self-improving
paradigm without either human-labeled data or external models.

</details>


### [166] [FutureX: An Advanced Live Benchmark for LLM Agents in Future Prediction](https://arxiv.org/abs/2508.11987)
*Zhiyuan Zeng,Jiashuo Liu,Siyuan Chen,Tianci He,Yali Liao,Jinpeng Wang,Zaiyuan Wang,Yang Yang,Lingyue Yin,Mingren Yin,Zhenwei Zhu,Tianle Cai,Zehui Chen,Jiecao Chen,Yantao Du,Xiang Gao,Jiacheng Guo,Liang Hu,Jianpeng Jiao,Xiangsheng Li,Jingkai Liu,Shuang Ni,Zhoufutu Wen,Ge Zhang,Kaiyuan Zhang,Xin Zhou,Jose Blanchet,Xipeng Qiu,Mengdi Wang,Wenhao Huang*

Main category: cs.AI

TL;DR: FutureX is a dynamic live benchmark for evaluating LLM agents on future prediction tasks, featuring real-time updates and contamination-free evaluation of 25 models.


<details>
  <summary>Details</summary>
Motivation: No large-scale benchmark exists for evaluating LLM agents on future prediction due to challenges with real-time updates and accurate answer retrieval.

Method: Created FutureX benchmark with automated pipeline for question gathering and answer collection, evaluating 25 LLM/agent models including reasoning/search capabilities and external tool integration.

Result: Comprehensive evaluation assessed agents' adaptive reasoning in dynamic environments, identified failure modes including vulnerability to fake web pages and temporal validity issues.

Conclusion: FutureX establishes a dynamic, contamination-free evaluation standard to drive development of LLM agents capable of professional-level predictive thinking.

Abstract: Future prediction is a complex task for LLM agents, requiring a high level of
analytical thinking, information gathering, contextual understanding, and
decision-making under uncertainty. Agents must not only gather and interpret
vast amounts of dynamic information but also integrate diverse data sources,
weigh uncertainties, and adapt predictions based on emerging trends, just as
human experts do in fields like politics, economics, and finance. Despite its
importance, no large-scale benchmark exists for evaluating agents on future
prediction, largely due to challenges in handling real-time updates and
retrieving timely, accurate answers. To address this, we introduce
$\textbf{FutureX}$, a dynamic and live evaluation benchmark specifically
designed for LLM agents performing future prediction tasks. FutureX is the
largest and most diverse live benchmark for future prediction, supporting
real-time daily updates and eliminating data contamination through an automated
pipeline for question gathering and answer collection. We evaluate 25 LLM/agent
models, including those with reasoning, search capabilities, and integration of
external tools such as the open-source Deep Research Agent and closed-source
Deep Research models. This comprehensive evaluation assesses agents' adaptive
reasoning and performance in dynamic environments. Additionally, we provide
in-depth analyses of agents' failure modes and performance pitfalls in
future-oriented tasks, including the vulnerability to fake web pages and the
temporal validity. Our goal is to establish a dynamic, contamination-free
evaluation standard that drives the development of LLM agents capable of
performing at the level of professional human analysts in complex reasoning and
predictive thinking.

</details>


### [167] [Modeling Relational Logic Circuits for And-Inverter Graph Convolutional Network](https://arxiv.org/abs/2508.11991)
*Weihao Sun*

Main category: cs.AI

TL;DR: AIGer is a novel framework that combines node logic feature initialization and heterogeneous graph convolutional networks to jointly model functional and structural characteristics of And-Inverter Graphs, achieving significant performance improvements in circuit analysis tasks.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with accurate modeling of real-world AIGs due to their complex structure and large scale, lacking joint modeling of functional/structural characteristics and dynamic information propagation capabilities.

Method: Two-component approach: 1) Node logic feature initialization embedding that projects logic nodes into semantic spaces, and 2) AIGs feature learning network using heterogeneous graph convolutional networks with dynamic relationship weight matrices and differentiated information aggregation.

Result: Outperforms current best models: 18.95% MAE and 44.44% MSE improvement in Signal Probability Prediction; 33.57% MAE and 14.79% MSE improvement in Truth Table Distance Prediction.

Conclusion: AIGer effectively addresses the challenges of joint functional-structural modeling and dynamic information propagation in AIG analysis, demonstrating superior performance in key EDA tasks.

Abstract: The automation of logic circuit design enhances chip performance, energy
efficiency, and reliability, and is widely applied in the field of Electronic
Design Automation (EDA).And-Inverter Graphs (AIGs) efficiently represent,
optimize, and verify the functional characteristics of digital circuits,
enhancing the efficiency of EDA development.Due to the complex structure and
large scale of nodes in real-world AIGs, accurate modeling is challenging,
leading to existing work lacking the ability to jointly model functional and
structural characteristics, as well as insufficient dynamic information
propagation capability.To address the aforementioned challenges, we propose
AIGer.Specifically, AIGer consists of two components: 1) Node logic feature
initialization embedding component and 2) AIGs feature learning network
component.The node logic feature initialization embedding component projects
logic nodes, such as AND and NOT, into independent semantic spaces, to enable
effective node embedding for subsequent processing.Building upon this, the AIGs
feature learning network component employs a heterogeneous graph convolutional
network, designing dynamic relationship weight matrices and differentiated
information aggregation approaches to better represent the original structure
and information of AIGs.The combination of these two components enhances
AIGer's ability to jointly model functional and structural characteristics and
improves its message passing capability. Experimental results indicate that
AIGer outperforms the current best models in the Signal Probability Prediction
(SSP) task, improving MAE and MSE by 18.95\% and 44.44\%, respectively. In the
Truth Table Distance Prediction (TTDP) task, AIGer achieves improvements of
33.57\% and 14.79\% in MAE and MSE, respectively, compared to the
best-performing models.

</details>


### [168] [AgentCDM: Enhancing Multi-Agent Collaborative Decision-Making via ACH-Inspired Structured Reasoning](https://arxiv.org/abs/2508.11995)
*Xuyang Zhao,Shiwan Zhao,Hualong Yu,Liting Zhang,Qicheng Li*

Main category: cs.AI

TL;DR: AgentCDM is a structured framework for enhancing collaborative decision-making in LLM-based multi-agent systems by mitigating cognitive biases through ACH-inspired reasoning and two-stage training.


<details>
  <summary>Details</summary>
Motivation: Existing multi-agent systems use either dictatorial strategies vulnerable to single agent biases or voting-based methods that fail to leverage collective intelligence effectively, leaving collaborative decision-making underexplored.

Method: Proposes AgentCDM framework inspired by Analysis of Competing Hypotheses (ACH) with structured reasoning paradigm and two-stage training: first stage uses explicit ACH scaffolding, second stage removes scaffolding for autonomous generalization.

Result: Achieves state-of-the-art performance on multiple benchmark datasets and exhibits strong generalization capabilities.

Conclusion: AgentCDM effectively improves the quality and robustness of collaborative decisions in multi-agent systems by shifting from passive answer selection to active hypothesis evaluation and construction.

Abstract: Multi-agent systems (MAS) powered by large language models (LLMs) hold
significant promise for solving complex decision-making tasks. However, the
core process of collaborative decision-making (CDM) within these systems
remains underexplored. Existing approaches often rely on either ``dictatorial"
strategies that are vulnerable to the cognitive biases of a single agent, or
``voting-based" methods that fail to fully harness collective intelligence. To
address these limitations, we propose \textbf{AgentCDM}, a structured framework
for enhancing collaborative decision-making in LLM-based multi-agent systems.
Drawing inspiration from the Analysis of Competing Hypotheses (ACH) in
cognitive science, AgentCDM introduces a structured reasoning paradigm that
systematically mitigates cognitive biases and shifts decision-making from
passive answer selection to active hypothesis evaluation and construction. To
internalize this reasoning process, we develop a two-stage training paradigm:
the first stage uses explicit ACH-inspired scaffolding to guide the model
through structured reasoning, while the second stage progressively removes this
scaffolding to encourage autonomous generalization. Experiments on multiple
benchmark datasets demonstrate that AgentCDM achieves state-of-the-art
performance and exhibits strong generalization, validating its effectiveness in
improving the quality and robustness of collaborative decisions in MAS.

</details>


### [169] [AI Models for Depressive Disorder Detection and Diagnosis: A Review](https://arxiv.org/abs/2508.12022)
*Dorsa Macky Aleagha,Payam Zohari,Mostafa Haghir Chehreghani*

Main category: cs.AI

TL;DR: Survey of AI methods for depression diagnosis, categorizing 55 studies by clinical task, data modality, and model type, highlighting trends in graph neural networks, large language models, and multimodal approaches.


<details>
  <summary>Details</summary>
Motivation: Depression diagnosis relies heavily on subjective clinical assessments, creating need for objective, scalable AI tools to improve diagnostic accuracy and accessibility.

Method: Systematic review of 55 key studies with novel hierarchical taxonomy organizing research by clinical task (diagnosis/prediction), data modality (text/speech/neuroimaging/multimodal), and computational model class.

Result: Identified three major trends: graph neural networks dominate brain connectivity modeling, large language models excel with linguistic data, and emerging focus on multimodal fusion, explainability, and algorithmic fairness.

Conclusion: Provides comprehensive roadmap for future computational psychiatry innovation, synthesizing current advances and highlighting open challenges in AI-based depression diagnosis.

Abstract: Major Depressive Disorder is one of the leading causes of disability
worldwide, yet its diagnosis still depends largely on subjective clinical
assessments. Integrating Artificial Intelligence (AI) holds promise for
developing objective, scalable, and timely diagnostic tools. In this paper, we
present a comprehensive survey of state-of-the-art AI methods for depression
detection and diagnosis, based on a systematic review of 55 key studies. We
introduce a novel hierarchical taxonomy that structures the field by primary
clinical task (diagnosis vs. prediction), data modality (text, speech,
neuroimaging, multimodal), and computational model class (e.g., graph neural
networks, large language models, hybrid approaches). Our in-depth analysis
reveals three major trends: the predominance of graph neural networks for
modeling brain connectivity, the rise of large language models for linguistic
and conversational data, and an emerging focus on multimodal fusion,
explainability, and algorithmic fairness. Alongside methodological insights, we
provide an overview of prominent public datasets and standard evaluation
metrics as a practical guide for researchers. By synthesizing current advances
and highlighting open challenges, this survey offers a comprehensive roadmap
for future innovation in computational psychiatry.

</details>


### [170] [Bongard-RWR+: Real-World Representations of Fine-Grained Concepts in Bongard Problems](https://arxiv.org/abs/2508.12026)
*Szymon Pawlonka,Mikołaj Małkiński,Jacek Mańdziuk*

Main category: cs.AI

TL;DR: Bongard-RWR+ is a new 5,400-instance dataset using VLM-generated real-world images to test abstract visual reasoning, showing VLMs struggle with fine-grained concepts despite handling coarse ones.


<details>
  <summary>Details</summary>
Motivation: Existing Bongard Problem datasets have limitations - synthetic images lack real-world complexity, while real-world image datasets are either too small (Bongard-RWR with 60 instances) or use high-level features that reduce task complexity.

Method: Used Pixtral-12B to describe manually curated images and generate new concept-aligned descriptions, then employed Flux.1-dev to synthesize images from these descriptions, with manual verification to ensure concept fidelity.

Result: State-of-the-art VLMs performed well on coarse-grained visual concepts but consistently struggled with discerning fine-grained concepts across various BP formulations (binary/multiclass classification and textual answer generation).

Conclusion: Current VLMs have significant limitations in fine-grained abstract reasoning capabilities, highlighting the need for improved reasoning abilities despite their competence with coarse-grained visual concepts.

Abstract: Bongard Problems (BPs) provide a challenging testbed for abstract visual
reasoning (AVR), requiring models to identify visual concepts fromjust a few
examples and describe them in natural language. Early BP benchmarks featured
synthetic black-and-white drawings, which might not fully capture the
complexity of real-world scenes. Subsequent BP datasets employed real-world
images, albeit the represented concepts are identifiable from high-level image
features, reducing the task complexity. Differently, the recently released
Bongard-RWR dataset aimed at representing abstract concepts formulated in the
original BPs using fine-grained real-world images. Its manual construction,
however, limited the dataset size to just $60$ instances, constraining
evaluation robustness. In this work, we introduce Bongard-RWR+, a BP dataset
composed of $5\,400$ instances that represent original BP abstract concepts
using real-world-like images generated via a vision language model (VLM)
pipeline. Building on Bongard-RWR, we employ Pixtral-12B to describe manually
curated images and generate new descriptions aligned with the underlying
concepts, use Flux.1-dev to synthesize images from these descriptions, and
manually verify that the generated images faithfully reflect the intended
concepts. We evaluate state-of-the-art VLMs across diverse BP formulations,
including binary and multiclass classification, as well as textual answer
generation. Our findings reveal that while VLMs can recognize coarse-grained
visual concepts, they consistently struggle with discerning fine-grained
concepts, highlighting limitations in their reasoning capabilities.

</details>


### [171] [Active inference for action-unaware agents](https://arxiv.org/abs/2508.12027)
*Filippo Torresan,Keisuke Suzuki,Ryota Kanai,Manuel Baltieri*

Main category: cs.AI

TL;DR: Comparison of action-aware vs action-unaware agents in active inference frameworks, showing action-unaware agents can achieve comparable performance despite severe disadvantages in navigation tasks.


<details>
  <summary>Details</summary>
Motivation: To address the different strategies in active inference literature regarding how agents plan future actions - specifically whether agents know their own actions (action-aware) or must infer them from observations (action-unaware), and to compare their performance.

Method: The study compares action-aware and action-unaware agents in two navigation tasks within the active inference framework, where action-aware agents have knowledge of their own actions while action-unaware agents must infer motor behavior from recent observations.

Result: Action-unaware agents achieved performances comparable to action-aware agents despite being at a severe disadvantage, demonstrating their capability to effectively plan future actions through inference rather than direct knowledge.

Conclusion: Action-unaware approaches in active inference can be effective alternatives to action-aware methods, showing that agents can successfully plan future actions by inferring motor behavior from observations rather than requiring direct knowledge of their own actions.

Abstract: Active inference is a formal approach to study cognition based on the notion
that adaptive agents can be seen as engaging in a process of approximate
Bayesian inference, via the minimisation of variational and expected free
energies. Minimising the former provides an account of perceptual processes and
learning as evidence accumulation, while minimising the latter describes how
agents select their actions over time. In this way, adaptive agents are able to
maximise the likelihood of preferred observations or states, given a generative
model of the environment. In the literature, however, different strategies have
been proposed to describe how agents can plan their future actions. While they
all share the notion that some kind of expected free energy offers an
appropriate way to score policies, sequences of actions, in terms of their
desirability, there are different ways to consider the contribution of past
motor experience to the agent's future behaviour. In some approaches, agents
are assumed to know their own actions, and use such knowledge to better plan
for the future. In other approaches, agents are unaware of their actions, and
must infer their motor behaviour from recent observations in order to plan for
the future. This difference reflects a standard point of departure in two
leading frameworks in motor control based on the presence, or not, of an
efference copy signal representing knowledge about an agent's own actions. In
this work we compare the performances of action-aware and action-unaware agents
in two navigations tasks, showing how action-unaware agents can achieve
performances comparable to action-aware ones while at a severe disadvantage.

</details>


### [172] [MAPF-World: Action World Model for Multi-Agent Path Finding](https://arxiv.org/abs/2508.12087)
*Zhanjiang Yang,Meng Li,Yang Shen,Yueming Li,Lijun Sun*

Main category: cs.AI

TL;DR: MAPF-World is an autoregressive action world model that improves multi-agent path finding by modeling environmental dynamics and future predictions, achieving superior performance with smaller model size and less data.


<details>
  <summary>Details</summary>
Motivation: Existing decentralized learnable solvers for MAPF have limited modeling of environmental temporal dynamics and inter-agent dependencies, leading to performance degradation in complex, long-term planning scenarios.

Method: Proposes MAPF-World, an autoregressive action world model that unifies situation understanding and action generation. It explicitly models environmental dynamics including spatial features and temporal dependencies through future state and actions prediction. Also introduces an automatic map generator grounded in real-world scenarios for better training and evaluation.

Result: Extensive experiments show MAPF-World outperforms state-of-the-art learnable solvers with superior zero-shot generalization to out-of-distribution cases. Achieves this with 96.5% smaller model size and 92% reduced data requirements.

Conclusion: MAPF-World enables more informed, coordinated, and far-sighted decision-making in complex multi-agent settings by incorporating predicted futures, demonstrating significant improvements over existing approaches with much more efficient resource utilization.

Abstract: Multi-agent path finding (MAPF) is the problem of planning conflict-free
paths from the designated start locations to goal positions for multiple
agents. It underlies a variety of real-world tasks, including multi-robot
coordination, robot-assisted logistics, and social navigation. Recent
decentralized learnable solvers have shown great promise for large-scale MAPF,
especially when leveraging foundation models and large datasets. However, these
agents are reactive policy models and exhibit limited modeling of environmental
temporal dynamics and inter-agent dependencies, resulting in performance
degradation in complex, long-term planning scenarios. To address these
limitations, we propose MAPF-World, an autoregressive action world model for
MAPF that unifies situation understanding and action generation, guiding
decisions beyond immediate local observations. It improves situational
awareness by explicitly modeling environmental dynamics, including spatial
features and temporal dependencies, through future state and actions
prediction. By incorporating these predicted futures, MAPF-World enables more
informed, coordinated, and far-sighted decision-making, especially in complex
multi-agent settings. Furthermore, we augment MAPF benchmarks by introducing an
automatic map generator grounded in real-world scenarios, capturing practical
map layouts for training and evaluating MAPF solvers. Extensive experiments
demonstrate that MAPF-World outperforms state-of-the-art learnable solvers,
showcasing superior zero-shot generalization to out-of-distribution cases.
Notably, MAPF-World is trained with a 96.5% smaller model size and 92% reduced
data.

</details>


### [173] [Overcoming Knowledge Discrepancies: Structuring Reasoning Threads through Knowledge Balancing in Interactive Scenarios](https://arxiv.org/abs/2508.12100)
*Daniel Burkhardt,Xiangwei Cheng*

Main category: cs.AI

TL;DR: ReT-Eval framework improves interactive problem solving by creating structured reasoning threads that align with domain knowledge and user understanding through knowledge extraction and reward-guided pruning.


<details>
  <summary>Details</summary>
Motivation: Current reasoning models lack explicit semantic hierarchies, user-domain knowledge alignment, and effective pruning mechanisms, resulting in lengthy generic outputs that don't guide users through goal-oriented reasoning steps.

Method: Two-phase framework: 1) Extract semantically relevant knowledge from sparse domain knowledge graphs using GNNs and enrich with LLM knowledge, 2) Evaluate and prune reasoning threads using reward-guided strategy for semantic coherence.

Result: Experiments and expert evaluations show ReT-Eval enhances user understanding and outperforms state-of-the-art reasoning models.

Conclusion: The prototype-inspired ReT-Eval framework successfully addresses limitations of current reasoning models by incorporating structured knowledge reuse and principled pruning mechanisms for more effective interactive problem solving.

Abstract: Reasoning in interactive problem solving scenarios requires models to
construct reasoning threads that reflect user understanding and align with
structured domain knowledge. However, current reasoning models often lack
explicit semantic hierarchies, user-domain knowledge alignment, and principled
mechanisms to prune reasoning threads for effectiveness. These limitations
result in lengthy generic output that does not guide users through
goal-oriented reasoning steps. To address this, we propose a
prototype-inspired, two-phases Reasoning-Threads-Evaluation (ReT-Eval)
framework, drawing inspiration from human-like reasoning strategies that
emphasize structured knowledge reuse. In the first phase, semantically relevant
knowledge structures are extracted from a sparse domain knowledge graph using a
graph neural network and enriched with intrinsic large language model knowledge
to resolve knowledge discrepancies. In the second phase, these threads are
evaluated and pruned using a reward-guided strategy aimed at maintaining
semantic coherence to generate effective reasoning threads. Experiments and
expert evaluations show that ReT-Eval enhances user understanding and
outperforms state-of-the-art reasoning models.

</details>


### [174] [MOVER: Multimodal Optimal Transport with Volume-based Embedding Regularization](https://arxiv.org/abs/2508.12149)
*Haochen You,Baojing Liu*

Main category: cs.AI

TL;DR: MOVER is a multimodal learning framework that combines optimal transport alignment with geometric regularization to create structured representations across text, video, and audio modalities, outperforming previous methods in retrieval tasks.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal contrastive learning approaches struggle with generalization across multiple modalities and lack semantic structure in high-dimensional embedding spaces, particularly in setups beyond bi-modal configurations.

Method: Combines optimal transport-based soft alignment with volume-based geometric regularization (GAVE), using transport-guided matching and geometric volume minimization to achieve modality-agnostic consistent alignment.

Result: Significantly outperforms state-of-the-art methods in text-video-audio retrieval tasks in both zero-shot and finetuned settings, with improved generalization to unseen modality combinations and stronger structural consistency.

Conclusion: MOVER successfully addresses limitations of pairwise contrastive learning by providing structured, semantically aligned multimodal representations through optimal transport and geometric regularization techniques.

Abstract: Recent advances in multimodal learning have largely relied on pairwise
contrastive objectives to align different modalities, such as text, video, and
audio, in a shared embedding space. While effective in bi-modal setups, these
approaches struggle to generalize across multiple modalities and often lack
semantic structure in high-dimensional spaces. In this paper, we propose MOVER,
a novel framework that combines optimal transport-based soft alignment with
volume-based geometric regularization to build semantically aligned and
structured multimodal representations. By integrating a transport-guided
matching mechanism with a geometric volume minimization objective (GAVE), MOVER
encourages consistent alignment across all modalities in a modality-agnostic
manner. Experiments on text-video-audio retrieval tasks demonstrate that MOVER
significantly outperforms prior state-of-the-art methods in both zero-shot and
finetuned settings. Additional analysis shows improved generalization to unseen
modality combinations and stronger structural consistency in the learned
embedding space.

</details>


### [175] [RLNVR: Reinforcement Learning from Non-Verified Real-World Rewards](https://arxiv.org/abs/2508.12165)
*Rohit Krishnan,Jon Evans*

Main category: cs.AI

TL;DR: RLNVR framework enables language model training using noisy real-world feedback without human verification, combining baseline normalization and semantic similarity reward transfer for social media content optimization.


<details>
  <summary>Details</summary>
Motivation: Traditional RLHF requires expensive verified reward signals that are impractical for many real-world applications, especially in social media content generation where engagement data is abundant but noisy.

Method: Uses baseline normalization and semantic similarity-based reward transfer, combined with GSPO (Group Sequence Policy Optimization) and optional UED (Unsupervised Environment Design) curriculum for improved stability and diversity.

Result: Demonstrated through Walter prototype system showing significant improvements in content quality and training stability when optimizing social media content using Bluesky engagement data.

Conclusion: Presents a practical framework for leveraging noisy real-world feedback signals in language model training, positioning it as an applied integration of existing techniques rather than a new algorithm.

Abstract: This paper introduces RLNVR (Reinforcement Learning from Non-Verified
Rewards), a framework for training language models using noisy, real-world
feedback signals without requiring explicit human verification. Traditional
RLHF requires expensive, verified reward signals that are impractical in many
real-world domains. RLNVR addresses this challenge through baseline
normalization and semantic similarity-based reward transfer. We demonstrate
RLNVR through Walter, a prototype system that optimizes social media content
generation using actual engagement data from Bluesky. Our experimental results
show significant improvements in content quality and training stability, with
comprehensive evaluation planned for future work. Positioning: We present a
practical framework that combines RLNVR with GSPO (Group Sequence Policy
Optimization) and an optional UED (Unsupervised Environment Design) curriculum
to improve stability and diversity under noisy, implicit rewards. To our
knowledge, combining GSPO-style normalization with a UED-style curriculum for
LLM content generation from implicit social engagement has not been previously
documented in this applied setting; we frame this as an applied integration
rather than a new algorithm.

</details>


### [176] [Mantis: A Simulation-Grounded Foundation Model for Disease Forecasting](https://arxiv.org/abs/2508.12260)
*Carson Dudley,Reiden Magdaleno,Christopher Harding,Ananya Sharma,Emily Martin,Marisa Eisenberg*

Main category: cs.AI

TL;DR: Mantis is a foundation model for infectious disease forecasting that uses mechanistic simulations instead of real-world data, achieving superior performance across multiple diseases and enabling 8-week forecasts with mechanistic interpretability.


<details>
  <summary>Details</summary>
Motivation: Traditional infectious disease forecasting requires disease-specific data, expert tuning, and bespoke training, limiting effectiveness in novel outbreaks or low-resource settings with limited historical data.

Method: Trained on over 400 million simulated days of outbreak dynamics covering diverse pathogens, transmission modes, interventions, and surveillance artifacts - entirely using mechanistic simulations without real-world training data.

Result: Outperformed 39 expert-tuned models across six diseases, including all models in CDC's COVID-19 Forecast Hub. Generalizes to novel epidemiological regimes and delivers accurate 8-week forecasts (more than doubling actionable range of most models).

Conclusion: Mantis serves as a foundation for next-generation disease forecasting systems that are general, interpretable, and deployable where traditional models fail, capturing fundamental contagion dynamics through mechanistic simulation training.

Abstract: Infectious disease forecasting in novel outbreaks or low resource settings
has been limited by the need for disease-specific data, bespoke training, and
expert tuning. We introduce Mantis, a foundation model trained entirely on
mechanistic simulations, which enables out-of-the-box forecasting across
diseases, regions, and outcomes, even in settings with limited historical data.
Mantis is built on over 400 million simulated days of outbreak dynamics
spanning diverse pathogens, transmission modes, interventions, and surveillance
artifacts. Despite requiring no real-world data during training, Mantis
outperformed 39 expert-tuned models we tested across six diseases, including
all models in the CDC's COVID-19 Forecast Hub. Mantis generalized to novel
epidemiological regimes, including diseases with held-out transmission
mechanisms, demonstrating that it captures fundamental contagion dynamics.
Critically, Mantis is mechanistically interpretable, enabling public health
decision-makers to identify the latent drivers behind its predictions. Finally,
Mantis delivers accurate forecasts at 8-week horizons, more than doubling the
actionable range of most models, enabling proactive public health planning.
Together, these capabilities position Mantis as a foundation for
next-generation disease forecasting systems: general, interpretable, and
deployable where traditional models fail.

</details>


### [177] [RadarQA: Multi-modal Quality Analysis of Weather Radar Forecasts](https://arxiv.org/abs/2508.12291)
*Xuming He,Zhiyuan You,Junchao Gong,Couhua Liu,Xiaoyu Yue,Peiqin Zhuang,Wenlong Zhang,Lei Bai*

Main category: cs.AI

TL;DR: RadarQA is an MLLM-based method for weather forecast quality analysis that outperforms existing general models by integrating physical attributes with detailed assessment reports using a novel multi-modal task paradigm and large-scale dataset.


<details>
  <summary>Details</summary>
Motivation: Traditional score-based weather forecast evaluation metrics lack descriptive capability, interpretability, and dynamic evolution understanding compared to meteorological experts, creating a need for more sophisticated analysis tools.

Method: Developed RadarQA method using Multi-modal Large Language Models, created RQA-70K dataset with hybrid human-expert and automated annotation, and implemented multi-stage training strategy for iterative performance improvement.

Result: RadarQA outperforms existing general MLLMs across all evaluation settings, demonstrating superior performance in both single frame and sequence analysis under rating and assessment scenarios.

Conclusion: The method shows strong potential for advancing quality analysis in weather prediction by effectively bridging the gap between traditional metrics and expert-level analysis capabilities.

Abstract: Quality analysis of weather forecasts is an essential topic in meteorology.
Although traditional score-based evaluation metrics can quantify certain
forecast errors, they are still far from meteorological experts in terms of
descriptive capability, interpretability, and understanding of dynamic
evolution. With the rapid development of Multi-modal Large Language Models
(MLLMs), these models become potential tools to overcome the above challenges.
In this work, we introduce an MLLM-based weather forecast analysis method,
RadarQA, integrating key physical attributes with detailed assessment reports.
We introduce a novel and comprehensive task paradigm for multi-modal quality
analysis, encompassing both single frame and sequence, under both rating and
assessment scenarios. To support training and benchmarking, we design a hybrid
annotation pipeline that combines human expert labeling with automated
heuristics. With such an annotation method, we construct RQA-70K, a large-scale
dataset with varying difficulty levels for radar forecast quality evaluation.
We further design a multi-stage training strategy that iteratively improves
model performance at each stage. Extensive experiments show that RadarQA
outperforms existing general MLLMs across all evaluation settings, highlighting
its potential for advancing quality analysis in weather prediction.

</details>


### [178] [Wisdom of the Crowd: Reinforcement Learning from Coevolutionary Collective Feedback](https://arxiv.org/abs/2508.12338)
*Wenzhen Yuan,Shengji Tang,Weihao Lin,Jiacheng Ruan,Ganqu Cui,Bo Zhang,Tao Chen,Ting Liu,Yuzhuo Fu,Peng Ye,Lei Bai*

Main category: cs.AI

TL;DR: RLCCF is a novel reinforcement learning framework that enables multi-model collaborative evolution through collective consistency voting, eliminating the need for external supervision and addressing limitations of single-model self-feedback methods.


<details>
  <summary>Details</summary>
Motivation: Traditional RL for LLMs relies on expensive human-labeled data or complex reward models, while existing self-feedback methods suffer from single-model limitations like overconfidence, reward hacking, and training collapse.

Method: RLCCF optimizes model collective by maximizing Collective Consistency (CC), training a diverse ensemble of LLMs that provide reward signals through voting on outputs, with votes weighted by each model's Self-Consistency (SC) score.

Result: Experiments on four LLMs across four mathematical reasoning benchmarks show 16.72% average relative accuracy improvement, with 4.51% enhancement in group majority-voting accuracy.

Conclusion: RLCCF effectively enables continuous reasoning ability enhancement through coevolution, extends collective capability boundaries, and demonstrates significant performance gains without external supervision.

Abstract: Reinforcement learning (RL) has significantly enhanced the reasoning
capabilities of large language models (LLMs), but its reliance on expensive
human-labeled data or complex reward models severely limits scalability. While
existing self-feedback methods aim to address this problem, they are
constrained by the capabilities of a single model, which can lead to
overconfidence in incorrect answers, reward hacking, and even training
collapse. To this end, we propose Reinforcement Learning from Coevolutionary
Collective Feedback (RLCCF), a novel RL framework that enables multi-model
collaborative evolution without external supervision. Specifically, RLCCF
optimizes the ability of a model collective by maximizing its Collective
Consistency (CC), which jointly trains a diverse ensemble of LLMs and provides
reward signals by voting on collective outputs. Moreover, each model's vote is
weighted by its Self-Consistency (SC) score, ensuring that more confident
models contribute more to the collective decision. Benefiting from the diverse
output distributions and complementary abilities of multiple LLMs, RLCCF
enables the model collective to continuously enhance its reasoning ability
through coevolution. Experiments on four mainstream open-source LLMs across
four mathematical reasoning benchmarks demonstrate that our framework yields
significant performance gains, achieving an average relative improvement of
16.72\% in accuracy. Notably, RLCCF not only improves the performance of
individual models but also enhances the group's majority-voting accuracy by
4.51\%, demonstrating its ability to extend the collective capability boundary
of the model collective.

</details>


### [179] [Hierarchical knowledge guided fault intensity diagnosis of complex industrial systems](https://arxiv.org/abs/2508.12375)
*Yu Sha,Shuiping Gou,Bo Liu,Johannes Faber,Ningtao Liu,Stefan Schramm,Horst Stoecker,Thomas Steckenreiter,Domagoj Vnucec,Nadine Wetzstein,Andreas Widl,Kai Zhou*

Main category: cs.AI

TL;DR: Proposes HKG framework with hierarchical classifiers and Re-HKCM scheme for fault intensity diagnosis, achieving state-of-the-art results on industrial datasets.


<details>
  <summary>Details</summary>
Motivation: Current FID methods use chain of thought without considering dependencies among target classes, limiting their effectiveness in capturing complex inter-class relationships.

Method: Hierarchical knowledge guided framework (HKG) using graph convolutional networks to map hierarchical topological graphs into interdependent global hierarchical classifiers, combined with re-weighted hierarchical knowledge correlation matrix (Re-HKCM) scheme.

Result: Extensive experiments on four real-world industrial datasets show superior performance, outperforming recent state-of-the-art FID methods across different industrial domains.

Conclusion: The proposed HKG framework with Re-HKCM effectively captures class dependencies and hierarchical knowledge, providing an end-to-end learnable solution that addresses over-smoothing issues and achieves excellent fault intensity diagnosis performance.

Abstract: Fault intensity diagnosis (FID) plays a pivotal role in monitoring and
maintaining mechanical devices within complex industrial systems. As current
FID methods are based on chain of thought without considering dependencies
among target classes. To capture and explore dependencies, we propose a
hierarchical knowledge guided fault intensity diagnosis framework (HKG)
inspired by the tree of thought, which is amenable to any representation
learning methods. The HKG uses graph convolutional networks to map the
hierarchical topological graph of class representations into a set of
interdependent global hierarchical classifiers, where each node is denoted by
word embeddings of a class. These global hierarchical classifiers are applied
to learned deep features extracted by representation learning, allowing the
entire model to be end-to-end learnable. In addition, we develop a re-weighted
hierarchical knowledge correlation matrix (Re-HKCM) scheme by embedding
inter-class hierarchical knowledge into a data-driven statistical correlation
matrix (SCM) which effectively guides the information sharing of nodes in
graphical convolutional neural networks and avoids over-smoothing issues. The
Re-HKCM is derived from the SCM through a series of mathematical
transformations. Extensive experiments are performed on four real-world
datasets from different industrial domains (three cavitation datasets from
SAMSON AG and one existing publicly) for FID, all showing superior results and
outperform recent state-of-the-art FID methods.

</details>


### [180] [GraphCogent: Overcoming LLMs' Working Memory Constraints via Multi-Agent Collaboration in Complex Graph Understanding](https://arxiv.org/abs/2508.12379)
*Rongzheng Wang,Qizhi Chen,Yihong Huang,Yizhuo Ma,Muquan Li,Jiakai Li,Ke Qin,Guangchun Luo,Shuang Liang*

Main category: cs.AI

TL;DR: GraphCogent is a collaborative agent framework that decomposes graph reasoning into specialized cognitive processes (sense, buffer, execute) to address LLMs' limitations in handling complex graph topology and multi-step reasoning on real-world graphs.


<details>
  <summary>Details</summary>
Motivation: Large language models perform well on small-scale graph reasoning tasks but fail with complex real-world graphs due to inability to process complex graph topology and perform multi-step reasoning simultaneously.

Method: Proposed GraphCogent framework with three modules: Sensory Module for standardizing graph representations via subgraph sampling, Buffer Module for integrating and indexing graph data across formats, and Execution Module combining tool calling and model generation for efficient reasoning.

Result: Llama3.1-8B based GraphCogent achieves 50% improvement over massive-scale LLMs like DeepSeek-R1 (671B), outperforms state-of-the-art agent-based baseline by 20% in accuracy while reducing token usage by 80% for in-toolset tasks and 30% for out-toolset tasks.

Conclusion: GraphCogent effectively addresses LLMs' limitations in graph reasoning through a collaborative agent framework inspired by human working memory model, demonstrating significant performance improvements and efficiency gains on large-scale real-world graphs.

Abstract: Large language models (LLMs) show promising performance on small-scale graph
reasoning tasks but fail when handling real-world graphs with complex queries.
This phenomenon stems from LLMs' inability to effectively process complex graph
topology and perform multi-step reasoning simultaneously. To address these
limitations, we propose GraphCogent, a collaborative agent framework inspired
by human Working Memory Model that decomposes graph reasoning into specialized
cognitive processes: sense, buffer, and execute. The framework consists of
three modules: Sensory Module standardizes diverse graph text representations
via subgraph sampling, Buffer Module integrates and indexes graph data across
multiple formats, and Execution Module combines tool calling and model
generation for efficient reasoning. We also introduce Graph4real, a
comprehensive benchmark contains with four domains of real-world graphs (Web,
Social, Transportation, and Citation) to evaluate LLMs' graph reasoning
capabilities. Our Graph4real covers 21 different graph reasoning tasks,
categorized into three types (Structural Querying, Algorithmic Reasoning, and
Predictive Modeling tasks), with graph scales that are 10 times larger than
existing benchmarks. Experiments show that Llama3.1-8B based GraphCogent
achieves a 50% improvement over massive-scale LLMs like DeepSeek-R1 (671B).
Compared to state-of-the-art agent-based baseline, our framework outperforms by
20% in accuracy while reducing token usage by 80% for in-toolset tasks and 30%
for out-toolset tasks. Code will be available after review.

</details>


### [181] [Non-Iterative Symbolic-Aided Chain-of-Thought for Logical Reasoning](https://arxiv.org/abs/2508.12425)
*Phuong Minh Nguyen,Tien Huu Dang,Naoya Inoue*

Main category: cs.AI

TL;DR: Symbolic-Aided CoT enhances standard Chain-of-Thought reasoning by integrating lightweight symbolic representations into few-shot prompts, improving LLM logical reasoning transparency and performance on complex tasks.


<details>
  <summary>Details</summary>
Motivation: To address limitations in standard Chain-of-Thought reasoning by making reasoning patterns more explicit and improving transparency, interpretability, and analyzability of LLM logical reasoning while maintaining generalizability.

Method: Integrates lightweight symbolic representations into few-shot prompts to structure inference steps with a consistent strategy within a non-iterative reasoning process.

Result: Significantly outperforms conventional CoT on three out of four datasets (ProofWriter, ProntoQA, LogicalDeduction), consistently improves reasoning capabilities across various model sizes, and shows particular effectiveness in complex reasoning tasks requiring multiple constraints/rules.

Conclusion: Symbolic-Aided CoT effectively enhances LLM logical reasoning by combining symbolic structures with standard prompting techniques, demonstrating superior performance on complex reasoning benchmarks while maintaining transparency and interpretability.

Abstract: This work introduces Symbolic-Aided Chain-of-Thought (CoT), an improved
approach to standard CoT, for logical reasoning in large language models
(LLMs). The key idea is to integrate lightweight symbolic representations into
few-shot prompts, structuring the inference steps with a consistent strategy to
make reasoning patterns more explicit within a non-iterative reasoning process.
By incorporating these symbolic structures, our method preserves the
generalizability of standard prompting techniques while enhancing the
transparency, interpretability, and analyzability of LLM logical reasoning.
Extensive experiments on four well-known logical reasoning benchmarks --
ProofWriter, FOLIO, ProntoQA, and LogicalDeduction, which cover diverse
reasoning scenarios -- demonstrate the effectiveness of the proposed approach,
particularly in complex reasoning tasks that require navigating multiple
constraints or rules. Notably, Symbolic-Aided CoT consistently improves LLMs'
reasoning capabilities across various model sizes and significantly outperforms
conventional CoT on three out of four datasets, ProofWriter, ProntoQA, and
LogicalDeduction.

</details>


### [182] [GALA: Can Graph-Augmented Large Language Model Agentic Workflows Elevate Root Cause Analysis?](https://arxiv.org/abs/2508.12472)
*Yifang Tian,Yaming Liu,Zichun Chong,Zihang Huang,Hans-Arno Jacobsen*

Main category: cs.AI

TL;DR: GALA is a multi-modal framework that combines statistical causal inference with LLM-driven reasoning for root cause analysis in microservices, achieving 42.22% higher accuracy than state-of-the-art methods and providing actionable diagnostic insights with remediation guidance.


<details>
  <summary>Details</summary>
Motivation: Traditional RCA methods in microservice systems focus on single modalities or only rank suspect services, failing to provide actionable diagnostic insights and remediation guidance needed for practical incident resolution.

Method: GALA combines statistical causal inference with LLM-driven iterative reasoning in a multi-modal framework that analyzes heterogeneous telemetry data (metrics, logs, traces) for enhanced root cause analysis.

Result: GALA achieves substantial improvements of up to 42.22% accuracy over state-of-the-art methods on an open-source benchmark, and generates significantly more causally sound and actionable diagnostic outputs according to novel human-guided LLM evaluation.

Conclusion: GALA bridges the gap between automated failure diagnosis and practical incident resolution by providing both accurate root cause identification and human-interpretable remediation guidance, representing a significant advancement in microservice RCA.

Abstract: Root cause analysis (RCA) in microservice systems is challenging, requiring
on-call engineers to rapidly diagnose failures across heterogeneous telemetry
such as metrics, logs, and traces. Traditional RCA methods often focus on
single modalities or merely rank suspect services, falling short of providing
actionable diagnostic insights with remediation guidance. This paper introduces
GALA, a novel multi-modal framework that combines statistical causal inference
with LLM-driven iterative reasoning for enhanced RCA. Evaluated on an
open-source benchmark, GALA achieves substantial improvements over
state-of-the-art methods of up to 42.22% accuracy. Our novel human-guided LLM
evaluation score shows GALA generates significantly more causally sound and
actionable diagnostic outputs than existing methods. Through comprehensive
experiments and a case study, we show that GALA bridges the gap between
automated failure diagnosis and practical incident resolution by providing both
accurate root cause identification and human-interpretable remediation
guidance.

</details>


### [183] [The Yokai Learning Environment: Tracking Beliefs Over Space and Time](https://arxiv.org/abs/2508.12480)
*Constantin Ruhdorfer,Matteo Bortoletto,Andreas Bulling*

Main category: cs.AI

TL;DR: Yokai Learning Environment (YLE) is a new multi-agent RL benchmark for Theory of Mind that tests agents' ability to track beliefs, maintain common ground, and communicate effectively in a cooperative card game setting.


<details>
  <summary>Details</summary>
Motivation: Existing ToM benchmarks are limited to passive observer settings and lack assessment of how agents establish and maintain common ground over time in collaborative settings.

Method: Developed the Yokai Learning Environment (YLE) - a multi-agent RL environment based on the cooperative card game Yokai where agents peek at hidden cards, move them to form color clusters, and use hints for grounded communication.

Result: Current RL agents struggle to solve YLE even with perfect memory. Belief modeling improves performance but agents fail to generalize to unseen partners or maintain accurate beliefs over longer games, showing reliance on brittle conventions rather than robust belief tracking.

Conclusion: YLE reveals significant limitations in current RL agents' Theory of Mind capabilities and provides a testbed for investigating belief modeling, memory, partner generalization, and higher-order ToM in collaborative AI systems.

Abstract: Developing collaborative AI hinges on Theory of Mind (ToM) - the ability to
reason about the beliefs of others to build and maintain common ground.
Existing ToM benchmarks, however, are restricted to passive observer settings
or lack an assessment of how agents establish and maintain common ground over
time. To address these gaps, we introduce the Yokai Learning Environment (YLE)
- a multi-agent reinforcement learning (RL) environment based on the
cooperative card game Yokai. In the YLE, agents take turns peeking at hidden
cards and moving them to form clusters based on colour. Success requires
tracking evolving beliefs, remembering past observations, using hints as
grounded communication, and maintaining common ground with teammates. Our
evaluation yields two key findings: First, current RL agents struggle to solve
the YLE, even when given access to perfect memory. Second, while belief
modelling improves performance, agents are still unable to effectively
generalise to unseen partners or form accurate beliefs over longer games,
exposing a reliance on brittle conventions rather than robust belief tracking.
We use the YLE to investigate research questions in belief modelling, memory,
partner generalisation, and scaling to higher-order ToM.

</details>


### [184] [Advanced DOA Regulation with a Whale-Optimized Fractional Order Fuzzy PID Framework](https://arxiv.org/abs/2508.12487)
*Lida Shahbandari,Hossein Mohseni*

Main category: cs.AI

TL;DR: A Fractional Order Fuzzy PID controller optimized by Whale Optimization Algorithm for precise anesthesia control, outperforming standard FOPID with faster response and lower error.


<details>
  <summary>Details</summary>
Motivation: To develop an intelligent anesthesia delivery system that can maintain optimal Bispectral Index (40-60 range) by adapting to individual patient physiology and providing precise control.

Method: Combined fractional order dynamics with fuzzy logic in a PID controller, using Whale Optimization Algorithm to fine-tune parameters including fractional orders and fuzzy membership functions. Tested on 8 different patient profile models.

Result: FOFPID achieved faster settling times (2.5 min vs 3.2 min) and lower steady state error (0.5 vs 1.2) compared to standard FOPID controller across all patient profiles.

Conclusion: The FOFPID controller demonstrates excellent robustness and accuracy, providing a scalable AI-driven solution for automated anesthesia delivery that could improve clinical practice and patient outcomes.

Abstract: This study introduces a Fractional Order Fuzzy PID (FOFPID) controller that
uses the Whale Optimization Algorithm (WOA) to manage the Bispectral Index
(BIS), keeping it within the ideal range of forty to sixty. The FOFPID
controller combines fuzzy logic for adapting to changes and fractional order
dynamics for fine tuning. This allows it to adjust its control gains to handle
a person's unique physiology. The WOA helps fine tune the controller's
parameters, including the fractional orders and the fuzzy membership functions,
which boosts its performance. Tested on models of eight different patient
profiles, the FOFPID controller performed better than a standard Fractional
Order PID (FOPID) controller. It achieved faster settling times, at two and a
half minutes versus three point two minutes, and had a lower steady state
error, at zero point five versus one point two. These outcomes show the
FOFPID's excellent strength and accuracy. It offers a scalable, artificial
intelligence driven solution for automated anesthesia delivery that could
enhance clinical practice and improve patient results.

</details>


### [185] [Root Cause Analysis of Hydrogen Bond Separation in Spatio-Temporal Molecular Dynamics using Causal Models](https://arxiv.org/abs/2508.12500)
*Rahmat K. Adesunkanmi,Ashfaq Khokhar,Goce Trajcevski,Sohail Murad*

Main category: cs.AI

TL;DR: A novel causal modeling approach using variational autoencoders to identify root causes of hydrogen bond formation and separation in molecular dynamics simulations, enabling predictive analysis of molecular interactions.


<details>
  <summary>Details</summary>
Motivation: Molecular dynamics simulations are computationally intensive and require manual analysis to detect interesting events like hydrogen bond formation. There's a critical gap in understanding the underlying causes and prior events that contribute to these molecular interactions over time.

Method: Leverage spatio-temporal data analytics and machine learning with causal modeling. Treat hydrogen bond separation as an 'intervention' and represent causal structure as graphical models using a variational autoencoder-inspired architecture to infer causal relationships across diverse samples while utilizing shared dynamic information.

Result: The framework successfully constructs causal models that capture distribution shifts during bond formation/separation. Empirical validation on chiral separation atomic trajectories demonstrates the model can predict future steps and identify variables driving system changes.

Conclusion: This approach provides a novel perspective on root cause analysis in molecular dynamic systems, enabling automated detection of causal relationships in hydrogen bonding events and advancing understanding of molecular interaction dynamics.

Abstract: Molecular dynamics simulations (MDS) face challenges, including
resource-heavy computations and the need to manually scan outputs to detect
"interesting events," such as the formation and persistence of hydrogen bonds
between atoms of different molecules. A critical research gap lies in
identifying the underlying causes of hydrogen bond formation and separation
-understanding which interactions or prior events contribute to their emergence
over time. With this challenge in mind, we propose leveraging spatio-temporal
data analytics and machine learning models to enhance the detection of these
phenomena. In this paper, our approach is inspired by causal modeling and aims
to identify the root cause variables of hydrogen bond formation and separation
events. Specifically, we treat the separation of hydrogen bonds as an
"intervention" occurring and represent the causal structure of the bonding and
separation events in the MDS as graphical causal models. These causal models
are built using a variational autoencoder-inspired architecture that enables us
to infer causal relationships across samples with diverse underlying causal
graphs while leveraging shared dynamic information. We further include a step
to infer the root causes of changes in the joint distribution of the causal
models. By constructing causal models that capture shifts in the conditional
distributions of molecular interactions during bond formation or separation,
this framework provides a novel perspective on root cause analysis in molecular
dynamic systems. We validate the efficacy of our model empirically on the
atomic trajectories that used MDS for chiral separation, demonstrating that we
can predict many steps in the future and also find the variables driving the
observed changes in the system.

</details>


### [186] [Help or Hurdle? Rethinking Model Context Protocol-Augmented Large Language Models](https://arxiv.org/abs/2508.12566)
*Wei Song,Haonan Zhong,Ziqi Ding,Jingling Xue,Yuekang Li*

Main category: cs.AI

TL;DR: MCPGAUGE is the first comprehensive evaluation framework that reveals surprising limitations in how LLMs actually use external tools via Model Context Protocol, challenging assumptions about tool integration effectiveness.


<details>
  <summary>Details</summary>
Motivation: While Model Context Protocol (MCP) enables LLMs to access external resources, there's poor understanding of how LLMs actually leverage this capability, despite common assumptions that it enhances performance.

Method: Developed MCPGAUGE framework with 160-prompt suite and 25 datasets across knowledge comprehension, reasoning, and code generation. Conducted large-scale evaluation with 6 commercial LLMs, 30 MCP tool suites, and both one- and two-turn interactions (20,000+ API calls, $6,000+ computational cost).

Result: The comprehensive study revealed four key findings that challenge prevailing assumptions about MCP integration effectiveness, highlighting critical limitations in current AI-tool integration.

Conclusion: MCPGAUGE serves as a principled benchmark for advancing controllable, tool-augmented LLMs, providing insights into the actual effectiveness and limitations of tool integration through systematic evaluation across multiple dimensions.

Abstract: The Model Context Protocol (MCP) enables large language models (LLMs) to
access external resources on demand. While commonly assumed to enhance
performance, how LLMs actually leverage this capability remains poorly
understood. We introduce MCPGAUGE, the first comprehensive evaluation framework
for probing LLM-MCP interactions along four key dimensions: proactivity
(self-initiated tool use), compliance (adherence to tool-use instructions),
effectiveness (task performance post-integration), and overhead (computational
cost incurred). MCPGAUGE comprises a 160-prompt suite and 25 datasets spanning
knowledge comprehension, general reasoning, and code generation. Our
large-scale evaluation, spanning six commercial LLMs, 30 MCP tool suites, and
both one- and two-turn interaction settings, comprises around 20,000 API calls
and over USD 6,000 in computational cost. This comprehensive study reveals four
key findings that challenge prevailing assumptions about the effectiveness of
MCP integration. These insights highlight critical limitations in current
AI-tool integration and position MCPGAUGE as a principled benchmark for
advancing controllable, tool-augmented LLMs.

</details>


### [187] [An LLM + ASP Workflow for Joint Entity-Relation Extraction](https://arxiv.org/abs/2508.12611)
*Trang Tran,Trung Hoang Le,Huiping Cao,Tran Cao Son*

Main category: cs.AI

TL;DR: A novel workflow combining LLMs and ASP for joint entity-relation extraction that achieves state-of-the-art performance with only 10% training data, showing 2.5x improvement on difficult benchmarks.


<details>
  <summary>Details</summary>
Motivation: Traditional JERE approaches require large annotated datasets and lack domain adaptation flexibility, making model creation labor-intensive and time-consuming.

Method: Proposes a generic workflow using generative pretrained LLMs for natural language understanding and Answer Set Programming (ASP) for knowledge representation and reasoning, enabling direct processing of unannotated text and easy incorporation of domain knowledge.

Result: Outperforms state-of-the-art JERE systems with only 10% training data, achieving 35% vs 15% performance on the difficult SciERC corpus Relation Extraction task - a 2.5x improvement.

Conclusion: The LLM + ASP workflow provides an effective, elaboration-tolerant solution for JERE that works across domains with minimal training data and easily incorporates domain-specific knowledge.

Abstract: Joint entity-relation extraction (JERE) identifies both entities and their
relationships simultaneously. Traditional machine-learning based approaches to
performing this task require a large corpus of annotated data and lack the
ability to easily incorporate domain specific information in the construction
of the model. Therefore, creating a model for JERE is often labor intensive,
time consuming, and elaboration intolerant. In this paper, we propose
harnessing the capabilities of generative pretrained large language models
(LLMs) and the knowledge representation and reasoning capabilities of Answer
Set Programming (ASP) to perform JERE. We present a generic workflow for JERE
using LLMs and ASP. The workflow is generic in the sense that it can be applied
for JERE in any domain. It takes advantage of LLM's capability in natural
language understanding in that it works directly with unannotated text. It
exploits the elaboration tolerant feature of ASP in that no modification of its
core program is required when additional domain specific knowledge, in the form
of type specifications, is found and needs to be used. We demonstrate the
usefulness of the proposed workflow through experiments with limited training
data on three well-known benchmarks for JERE. The results of our experiments
show that the LLM + ASP workflow is better than state-of-the-art JERE systems
in several categories with only 10\% of training data. It is able to achieve a
2.5 times (35\% over 15\%) improvement in the Relation Extraction task for the
SciERC corpus, one of the most difficult benchmarks.

</details>


### [188] [Cognitive Structure Generation: From Educational Priors to Policy Optimization](https://arxiv.org/abs/2508.12647)
*Hengnian Gu,Zhifu Chen,Yuxin Chen,Jin Peng Zhou,Dongdai Zhou*

Main category: cs.AI

TL;DR: A novel framework called Cognitive Structure Generation (CSG) that uses diffusion models and reinforcement learning to generate and optimize students' cognitive structures, improving student modeling performance and interpretability.


<details>
  <summary>Details</summary>
Motivation: Cognitive structure assessment has been a long-standing challenge in educational practice - it's foundational yet largely unassessable despite being crucial for understanding how students organize knowledge concepts.

Method: First pretrains a Cognitive Structure Diffusion Probabilistic Model (CSDPM) to generate cognitive structures from educational priors, then optimizes the generative process with hierarchical reward signals via reinforcement learning to align with genuine cognitive development levels.

Result: Experimental results on four real-world education datasets show CSG-generated cognitive structures provide more comprehensive and effective representations for student modeling, substantially improving performance on Knowledge Tracing (KT) and Cognitive Diagnosis (CD) tasks.

Conclusion: The CSG framework successfully addresses the challenge of cognitive structure assessment and demonstrates significant improvements in both performance and interpretability for student modeling tasks.

Abstract: Cognitive structure is a student's subjective organization of an objective
knowledge system, reflected in the psychological construction of concepts and
their relations. However, cognitive structure assessment remains a
long-standing challenge in student modeling and psychometrics, persisting as a
foundational yet largely unassessable concept in educational practice. This
paper introduces a novel framework, Cognitive Structure Generation (CSG), in
which we first pretrain a Cognitive Structure Diffusion Probabilistic Model
(CSDPM) to generate students' cognitive structures from educational priors, and
then further optimize its generative process as a policy with hierarchical
reward signals via reinforcement learning to align with genuine cognitive
development levels during students' learning processes. Experimental results on
four popular real-world education datasets show that cognitive structures
generated by CSG offer more comprehensive and effective representations for
student modeling, substantially improving performance on KT and CD tasks while
enhancing interpretability.

</details>


### [189] [The Maximum Coverage Model and Recommendation System for UAV Vertiports Location Planning](https://arxiv.org/abs/2508.12651)
*Chunliang Hua,Xiao Hu,Jiayang Sun,Zeyuan Yang*

Main category: cs.AI

TL;DR: Proposes CDMCLP optimization framework and Integrated Planning Recommendation System for urban aerial mobility vertiport network planning, improving traditional methods by 38-52% and bridging theory with practical implementation.


<details>
  <summary>Details</summary>
Motivation: Address inadequacies in existing planning frameworks for complex urban aerial mobility infrastructure development, particularly for large-scale vertiport networks with spatial-temporal demand and capacity constraints.

Method: Develops Capacitated Dynamic Maximum Covering Location Problem (CDMCLP) framework modeling urban-scale spatial-temporal demand, heterogeneous behaviors, and capacity constraints. Combines with socio-economic factors and dynamic clustering initialization in an Integrated Planning Recommendation System with adaptive parameter tuning.

Result: Validation in Chinese center city shows CDMCLP improves quantitative performance of traditional location methods by 38-52%. Recommendation system demonstrates user-friendliness and effective integration of complex elements.

Conclusion: The hybrid approach successfully bridges the gap between theoretical location modeling and real-world UAM infrastructure planning, providing municipalities with a pragmatic tool for vertiport network design.

Abstract: As urban aerial mobility (UAM) infrastructure development accelerates
globally, cities like Shenzhen are planning large-scale vertiport networks
(e.g., 1,200+ facilities by 2026). Existing planning frameworks remain
inadequate for this complexity due to historical limitations in data
granularity and real-world applicability. This paper addresses these gaps by
first proposing the Capacitated Dynamic Maximum Covering Location Problem
(CDMCLP), a novel optimization framework that simultaneously models urban-scale
spatial-temporal demand, heterogeneous user behaviors, and infrastructure
capacity constraints. Building on this foundation, we introduce an Integrated
Planning Recommendation System that combines CDMCLP with socio-economic factors
and dynamic clustering initialization. This system leverages adaptive parameter
tuning based on empirical user behavior to generate practical planning
solutions. Validation in a Chinese center city demonstrates the effectiveness
of the new optimization framework and recommendation system. Under the
evaluation and optimization of CDMCLP, the quantitative performance of
traditional location methods are exposed and can be improved by 38\%--52\%,
while the recommendation system shows user-friendliness and the effective
integration of complex elements. By integrating mathematical rigor with
practical implementation considerations, this hybrid approach bridges the gap
between theoretical location modeling and real-world UAM infrastructure
planning, offering municipalities a pragmatic tool for vertiport network
design.

</details>


### [190] [GridCodex: A RAG-Driven AI Framework for Power Grid Code Reasoning and Compliance](https://arxiv.org/abs/2508.12682)
*Jinquan Shi,Yingying Cheng,Fan Zhang,Miao Jiang,Jun Lin,Yanbai Shen*

Main category: cs.AI

TL;DR: GridCodex is an end-to-end framework using LLMs and enhanced RAG for automated grid code reasoning and compliance, achieving 26.4% answer quality improvement and 10x recall increase.


<details>
  <summary>Details</summary>
Motivation: Renewable energy transition creates complex grid code compliance challenges that lack automated solutions, hindering industry expansion and profitability for electricity companies.

Method: Leverages large language models with retrieval-augmented generation (RAG), featuring multi-stage query refinement and enhanced retrieval using RAPTOR technology.

Result: 26.4% improvement in answer quality, more than 10-fold increase in recall rate, validated through comprehensive benchmarks across multiple regulatory agencies.

Conclusion: GridCodex framework effectively addresses grid code interpretation challenges and demonstrates significant performance improvements over conventional approaches.

Abstract: The global shift towards renewable energy presents unprecedented challenges
for the electricity industry, making regulatory reasoning and compliance
increasingly vital. Grid codes, the regulations governing grid operations, are
complex and often lack automated interpretation solutions, which hinders
industry expansion and undermines profitability for electricity companies. We
introduce GridCodex, an end to end framework for grid code reasoning and
compliance that leverages large language models and retrieval-augmented
generation (RAG). Our framework advances conventional RAG workflows through
multi stage query refinement and enhanced retrieval with RAPTOR. We validate
the effectiveness of GridCodex with comprehensive benchmarks, including
automated answer assessment across multiple dimensions and regulatory agencies.
Experimental results showcase a 26.4% improvement in answer quality and more
than a 10 fold increase in recall rate. An ablation study further examines the
impact of base model selection.

</details>


### [191] [EGOILLUSION: Benchmarking Hallucinations in Egocentric Video Understanding](https://arxiv.org/abs/2508.12687)
*Ashish Seth,Utkarsh Tyagi,Ramaneswaran Selvakumar,Nishit Anand,Sonal Kumar,Sreyan Ghosh,Ramani Duraiswami,Chirag Agarwal,Dinesh Manocha*

Main category: cs.AI

TL;DR: EgoIllusion is the first benchmark to evaluate hallucinations in MLLMs for egocentric videos, featuring 1,400 videos with 8,000 questions that reveal significant accuracy challenges even in top models like GPT-4o and Gemini.


<details>
  <summary>Details</summary>
Motivation: While MLLMs excel at visual perception in videos, they are prone to generating coherent but inaccurate responses (hallucinations), particularly in egocentric video contexts where this problem hasn't been systematically studied.

Method: Created EgoIllusion benchmark with 1,400 egocentric videos paired with 8,000 human-annotated open and closed-ended questions designed to trigger hallucinations in both visual and auditory cues.

Result: Evaluations across ten MLLMs show significant challenges, with even powerful models like GPT-4o and Gemini achieving only 59% accuracy, demonstrating widespread hallucination issues.

Conclusion: EgoIllusion provides a foundation for developing robust benchmarks to evaluate MLLM effectiveness and will spur development of better egocentric MLLMs with reduced hallucination rates. The benchmark will be open-sourced for reproducibility.

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated remarkable
performance in complex multimodal tasks. While MLLMs excel at visual perception
and reasoning in third-person and egocentric videos, they are prone to
hallucinations, generating coherent yet inaccurate responses. We present
EgoIllusion, a first benchmark to evaluate MLLM hallucinations in egocentric
videos. EgoIllusion comprises 1,400 videos paired with 8,000 human-annotated
open and closed-ended questions designed to trigger hallucinations in both
visual and auditory cues in egocentric videos. Evaluations across ten MLLMs
reveal significant challenges, including powerful models like GPT-4o and
Gemini, achieving only 59% accuracy. EgoIllusion lays the foundation in
developing robust benchmarks to evaluate the effectiveness of MLLMs and spurs
the development of better egocentric MLLMs with reduced hallucination rates.
Our benchmark will be open-sourced for reproducibility.

</details>


### [192] [GTool: Graph Enhanced Tool Planning with Large Language Model](https://arxiv.org/abs/2508.12725)
*Wenjie Chen,Wenbin Li,Di Yao,Xuying Meng,Chang Gong,Jingping Bi*

Main category: cs.AI

TL;DR: GTool enhances LLM tool planning by constructing request-specific tool graphs and generating graph tokens to handle incomplete tool dependencies, achieving 29.6% performance improvement over SOTA baselines.


<details>
  <summary>Details</summary>
Motivation: Current LLM tool planning approaches treat tools as isolated components and fail to leverage inherent tool dependencies, leading to invalid planning results especially with incomplete dependency information and large toolsets.

Method: GTool constructs request-specific tool graphs to efficiently select tools and generates <graph tokens> that provide dependency information understandable by LLMs. It includes a missing dependency prediction task to improve reliability with incomplete dependencies.

Result: Extensive experiments show GTool achieves more than 29.6% performance improvements compared with state-of-the-art baselines using a lightweight 7B LLM backbone.

Conclusion: GTool is the first work to enhance LLM tool planning under incomplete dependencies, can be seamlessly integrated with various LLM backbones without extensive retraining, and significantly outperforms existing methods.

Abstract: Tool planning with large language models (LLMs), referring to selecting,
organizing, and preparing the tools necessary to complete a user request,
bridges the gap between natural language understanding and task execution.
However, current works treat different tools as isolated components and fail to
leverage the inherent dependencies of tools, leading to invalid planning
results. Since tool dependencies are often incomplete, it becomes challenging
for LLMs to accurately identify the appropriate tools required by a user
request, especially when confronted with a large toolset. To solve this
challenge, we propose \texttt{GTool}, which is the first work aiming to enhance
the tool planning ability of LLMs under incomplete dependencies. \texttt{GTool}
constructs a request-specific tool graph to select tools efficiently and
generate the \texttt{<graph token>} which provides sufficient dependency
information understandable by LLMs. Moreover, a missing dependency prediction
task is designed to improve the reliability of \texttt{GTool} with incomplete
dependencies. Without trimming LLMs, \texttt{GTool} can be seamlessly
integrated with various LLM backbones without extensive retraining. Extensive
experiments show that \texttt{GTool} achieves more than 29.6\% performance
improvements compared with the state-of-the-art (SOTA) baselines with a
light-weight (7B) LLM backbone.

</details>


### [193] [Beyond Ethical Alignment: Evaluating LLMs as Artificial Moral Assistants](https://arxiv.org/abs/2508.12754)
*Alessio Galatolo,Luca Alberto Rappuoli,Katie Winkle,Meriem Beloucif*

Main category: cs.AI

TL;DR: This paper introduces a new framework for evaluating LLMs as Artificial Moral Assistants (AMAs), focusing on moral reasoning capabilities beyond superficial alignment, and reveals significant gaps in current models' abductive reasoning abilities.


<details>
  <summary>Details</summary>
Motivation: Current LLM alignment benchmarks are superficial, measuring only final ethical verdicts rather than deep moral reasoning capabilities needed for AMAs that can support human moral deliberation.

Method: Developed a formal framework based on philosophical literature defining AMA behavior qualities (deductive/abductive reasoning), then created a benchmark to evaluate popular open LLMs against these criteria.

Result: Found considerable variability across models with persistent shortcomings, particularly in abductive moral reasoning capabilities.

Conclusion: Connects philosophy with AI evaluation, emphasizing need for dedicated strategies to explicitly enhance moral reasoning in LLMs beyond current alignment techniques.

Abstract: The recent rise in popularity of large language models (LLMs) has prompted
considerable concerns about their moral capabilities. Although considerable
effort has been dedicated to aligning LLMs with human moral values, existing
benchmarks and evaluations remain largely superficial, typically measuring
alignment based on final ethical verdicts rather than explicit moral reasoning.
In response, this paper aims to advance the investigation of LLMs' moral
capabilities by examining their capacity to function as Artificial Moral
Assistants (AMAs), systems envisioned in the philosophical literature to
support human moral deliberation. We assert that qualifying as an AMA requires
more than what state-of-the-art alignment techniques aim to achieve: not only
must AMAs be able to discern ethically problematic situations, they should also
be able to actively reason about them, navigating between conflicting values
outside of those embedded in the alignment phase. Building on existing
philosophical literature, we begin by designing a new formal framework of the
specific kind of behaviour an AMA should exhibit, individuating key qualities
such as deductive and abductive moral reasoning. Drawing on this theoretical
framework, we develop a benchmark to test these qualities and evaluate popular
open LLMs against it. Our results reveal considerable variability across models
and highlight persistent shortcomings, particularly regarding abductive moral
reasoning. Our work connects theoretical philosophy with practical AI
evaluation while also emphasising the need for dedicated strategies to
explicitly enhance moral reasoning capabilities in LLMs. Code available at
https://github.com/alessioGalatolo/AMAeval

</details>


### [194] [HeroBench: A Benchmark for Long-Horizon Planning and Structured Reasoning in Virtual Worlds](https://arxiv.org/abs/2508.12782)
*Petr Anokhin,Roman Khalikov,Stefan Rebrikov,Viktor Volkov,Artyom Sorokin,Vincent Bissonnette*

Main category: cs.AI

TL;DR: HeroBench is a new benchmark for evaluating long-horizon planning in LLMs using complex RPG-inspired virtual worlds, revealing significant performance gaps in current models' planning capabilities.


<details>
  <summary>Details</summary>
Motivation: Current LLM benchmarks focus on isolated step-by-step reasoning but fail to assess long-horizon planning in complex, realistic environments with interdependent actions and constraints.

Method: Developed HeroBench - a benchmark with RPG-inspired virtual worlds containing tasks requiring strategic planning, resource gathering, skill mastery, equipment crafting, and adversary defeat. Evaluated 25 state-of-the-art LLMs including GPT-5 family in this environment.

Result: Substantial performance disparities were revealed among LLMs, with detailed error analysis showing specific weaknesses in generating robust high-level plans and executing structured actions reliably.

Conclusion: HeroBench advances LLM reasoning evaluation and provides a flexible foundation for future research into autonomous planning in virtual environments, highlighting current limitations in long-horizon planning capabilities.

Abstract: Large language models (LLMs) have shown remarkable capabilities in isolated
step-by-step reasoning tasks such as mathematics and programming, but their
proficiency in long-horizon planning, where solutions require extended,
structured sequences of interdependent actions, remains underexplored. Existing
benchmarks typically assess LLMs through abstract or low-dimensional
algorithmic tasks, failing to capture the complexity of realistic planning
environments. We introduce HeroBench, a novel benchmark designed specifically
to evaluate long-horizon planning and structured reasoning within complex
RPG-inspired virtual worlds. HeroBench provides a rigorously constructed
dataset of tasks covering a wide range of difficulties, a simulated environment
to execute and validate agent plans, and detailed analytical tools for
evaluating model performance. Tasks challenge models to formulate strategic
plans, efficiently gather resources, master necessary skills, craft equipment,
and defeat adversaries, reflecting practical scenarios' layered dependencies
and constraints. Our extensive evaluation of 25 state-of-the-art LLMs, spanning
both open-source and proprietary models, including the GPT-5 family, reveals
substantial performance disparities rarely observed in conventional reasoning
benchmarks. Detailed error analysis further uncovers specific weaknesses in
current models' abilities to generate robust high-level plans and reliably
execute structured actions. HeroBench thus not only significantly advances the
evaluation of LLM reasoning but also provides a flexible, scalable foundation
for future research into advanced, autonomous planning in virtual environments.

</details>


### [195] [Reinforcement Learning with Rubric Anchors](https://arxiv.org/abs/2508.12790)
*Zenan Huang,Yihong Zhuang,Guoshan Lu,Zeyu Qin,Haokai Xu,Tianyu Zhao,Ru Peng,Jiaqi Hu,Zhanming Shen,Xiaomeng Hu,Xijun Gu,Peiyi Tu,Jiaxin Liu,Wenyu Chen,Yuzhuo Fu,Zhiting Fan,Yanmei Gu,Yuanyuan Wang,Zhengkai Yang,Jianguo Li,Junbo Zhao*

Main category: cs.AI

TL;DR: RLVR extended to open-ended tasks using rubric-based rewards, achieving +5.2% improvement on benchmarks with fine-grained stylistic control


<details>
  <summary>Details</summary>
Motivation: Overcome RLVR's limitation to automatically checkable domains by extending it to subjective, open-ended tasks using structured rubrics

Method: Constructed largest rubric reward system (10K+ rubrics from humans/LLMs), implemented rubric-based reinforcement learning framework, trained Qwen-30B-A3B model

Result: +5.2% improvement on open-ended benchmarks (especially humanities), outperformed 671B DeepSeek-V3 by +2.4%, achieved fine-grained stylistic control for more human-like responses

Conclusion: Rubric-based RLVR successfully extends verifiable rewards to subjective domains, providing effective training with small datasets while preserving general capabilities

Abstract: Reinforcement Learning from Verifiable Rewards (RLVR) has emerged as a
powerful paradigm for enhancing Large Language Models (LLMs), exemplified by
the success of OpenAI's o-series. In RLVR, rewards are derived from verifiable
signals-such as passing unit tests in code generation or matching correct
answers in mathematical reasoning. While effective, this requirement largely
confines RLVR to domains with automatically checkable outcomes. To overcome
this, we extend the RLVR paradigm to open-ended tasks by integrating
rubric-based rewards, where carefully designed rubrics serve as structured,
model-interpretable criteria for automatic scoring of subjective outputs. We
construct, to our knowledge, the largest rubric reward system to date, with
over 10,000 rubrics from humans, LLMs, or a hybrid human-LLM collaboration.
Implementing rubric-based RL is challenging; we tackle these issues with a
clear framework and present an open-sourced Qwen-30B-A3B model with notable
gains: 1) With only 5K+ samples, our system improves by +5.2% on open-ended
benchmarks (especially humanities), outperforming a 671B DeepSeek-V3 model by
+2.4%, while preserving general and reasoning abilities. 2) Our method provides
fine-grained stylistic control, using rubrics as anchors to mitigate the
"AI-like" tone and produce more human-like, expressive responses. We share key
lessons in rubric construction, data selection, and training, and discuss
limitations and future releases.

</details>


### [196] [[Social] Allostasis: Or, How I Learned To Stop Worrying and Love The Noise](https://arxiv.org/abs/2508.12791)
*Imran Khan*

Main category: cs.AI

TL;DR: Computational model shows allostatic regulation outperforms homeostasis by proactively leveraging environmental and social noise for adaptive reconfiguration in artificial agents.


<details>
  <summary>Details</summary>
Motivation: To challenge traditional homeostasis by proposing allostatic regulation that proactively uses environmental and social perturbations for adaptive reconfiguration, inspired by biological systems and von Foerster's 'order through noise' principle.

Method: Developed a computational model using biophysiologically inspired signal transducers (analogous to hormones like cortisol and oxytocin) to encode environmental and social information. Tested in agent-based model with animat societies across dynamic environments.

Result: Allostatic and social allostatic regulation enabled agents to leverage environmental and social noise for adaptive reconfiguration, leading to improved viability compared to purely reactive homeostatic agents.

Conclusion: Provides a novel computational framework for social allostasis principles that can inform the design of more robust, bio-inspired adaptive systems by demonstrating the advantages of proactive noise utilization over reactive stability maintenance.

Abstract: The notion of homeostasis typically conceptualises biological and artificial
systems as maintaining stability by resisting deviations caused by
environmental and social perturbations. In contrast, (social) allostasis
proposes that these systems can proactively leverage these very perturbations
to reconfigure their regulatory parameters in anticipation of environmental
demands, aligning with von Foerster's ``order through noise'' principle. This
paper formulates a computational model of allostatic and social allostatic
regulation that employs biophysiologically inspired signal transducers,
analogous to hormones like cortisol and oxytocin, to encode information from
both the environment and social interactions, which mediate this dynamic
reconfiguration. The models are tested in a small society of ``animats'' across
several dynamic environments, using an agent-based model. The results show that
allostatic and social allostatic regulation enable agents to leverage
environmental and social ``noise'' for adaptive reconfiguration, leading to
improved viability compared to purely reactive homeostatic agents. This work
offers a novel computational perspective on the principles of social allostasis
and their potential for designing more robust, bio-inspired, adaptive systems

</details>


### [197] [Scaling Multi-Agent Epistemic Planning through GNN-Derived Heuristics](https://arxiv.org/abs/2508.12840)
*Giovanni Briglia,Francesco Fabiano,Stefano Mariani*

Main category: cs.AI

TL;DR: Using Graph Neural Networks to learn predictive heuristics for multi-agent epistemic planning, overcoming scalability issues with traditional methods.


<details>
  <summary>Details</summary>
Motivation: Multi-agent epistemic planning faces intractability due to exponential search spaces and lack of effective heuristics for Kripke structure representations.

Method: Leverage Graph Neural Networks to capture relational patterns in epistemic states (Kripke models) and learn predictive heuristics from solved planning instances.

Result: Significant improvements in scalability compared to standard baselines, enabling more efficient exploration of the exponential search space.

Conclusion: GNN-based heuristics provide an effective solution for guiding epistemic planning processes and overcoming the computational challenges of multi-agent epistemic reasoning.

Abstract: Multi-agent Epistemic Planning (MEP) is an autonomous planning framework for
reasoning about both the physical world and the beliefs of agents, with
applications in domains where information flow and awareness among agents are
critical. The richness of MEP requires states to be represented as Kripke
structures, i.e., directed labeled graphs. This representation limits the
applicability of existing heuristics, hindering the scalability of epistemic
solvers, which must explore an exponential search space without guidance,
resulting often in intractability. To address this, we exploit Graph Neural
Networks (GNNs) to learn patterns and relational structures within epistemic
states, to guide the planning process. GNNs, which naturally capture the
graph-like nature of Kripke models, allow us to derive meaningful estimates of
state quality -- e.g., the distance from the nearest goal -- by generalizing
knowledge obtained from previously solved planning instances. We integrate
these predictive heuristics into an epistemic planning pipeline and evaluate
them against standard baselines, showing significant improvements in the
scalability of multi-agent epistemic planning.

</details>


### [198] [CAMAR: Continuous Actions Multi-Agent Routing](https://arxiv.org/abs/2508.12845)
*Artem Pshenitsyn,Aleksandr Panov,Alexey Skrynnik*

Main category: cs.AI

TL;DR: CAMAR is a new MARL benchmark for multi-agent pathfinding with continuous actions, supporting both cooperative and competitive interactions with high efficiency and integration of classical planning methods.


<details>
  <summary>Details</summary>
Motivation: Existing MARL benchmarks lack combinations of continuous state/action spaces with challenging coordination tasks, creating a need for more realistic and comprehensive test environments.

Method: Developed CAMAR benchmark with continuous action spaces, three-tier evaluation protocol, and integration of classical planning methods (RRT/RRT*) with MARL algorithms to create hybrid approaches.

Result: CAMAR runs efficiently at 100,000 environment steps per second and presents a challenging testbed that enables deeper performance analysis and fair comparison through reproducible benchmarking tools.

Conclusion: CAMAR successfully fills the gap in MARL benchmarks by providing a realistic, efficient, and comprehensive test environment that supports both classical planning integration and modern MARL approaches for continuous action pathfinding problems.

Abstract: Multi-agent reinforcement learning (MARL) is a powerful paradigm for solving
cooperative and competitive decision-making problems. While many MARL
benchmarks have been proposed, few combine continuous state and action spaces
with challenging coordination and planning tasks. We introduce CAMAR, a new
MARL benchmark designed explicitly for multi-agent pathfinding in environments
with continuous actions. CAMAR supports cooperative and competitive
interactions between agents and runs efficiently at up to 100,000 environment
steps per second. We also propose a three-tier evaluation protocol to better
track algorithmic progress and enable deeper analysis of performance. In
addition, CAMAR allows the integration of classical planning methods such as
RRT and RRT* into MARL pipelines. We use them as standalone baselines and
combine RRT* with popular MARL algorithms to create hybrid approaches. We
provide a suite of test scenarios and benchmarking tools to ensure
reproducibility and fair comparison. Experiments show that CAMAR presents a
challenging and realistic testbed for the MARL community.

</details>


### [199] [E3RG: Building Explicit Emotion-driven Empathetic Response Generation System with Multimodal Large Language Model](https://arxiv.org/abs/2508.12854)
*Ronghao Lin,Shuai Shen,Weipeng Hu,Qiaolin He,Aolin Xiong,Li Huang,Haifeng Hu,Yap-peng Tan*

Main category: cs.AI

TL;DR: E3RG is a multimodal empathetic response generation system that uses MLLMs to decompose the task into three parts: multimodal empathy understanding, memory retrieval, and response generation, achieving state-of-the-art results without extra training.


<details>
  <summary>Details</summary>
Motivation: Existing LLMs struggle with multimodal emotional content and identity consistency in empathetic response generation, requiring a more comprehensive approach to handle multimodal empathy.

Method: Decomposes MERG into three components using multimodal LLMs: multimodal empathy understanding, empathy memory retrieval, and multimodal response generation with expressive speech and video generative models.

Result: Achieves Top-1 position in Avatar-based Multimodal Empathy Challenge on ACM MM 25, demonstrating superiority in both zero-shot and few-shot settings without additional training.

Conclusion: E3RG successfully addresses multimodal empathetic response generation challenges by explicitly modeling emotions and leveraging existing generative models, providing natural, emotionally rich, and identity-consistent responses.

Abstract: Multimodal Empathetic Response Generation (MERG) is crucial for building
emotionally intelligent human-computer interactions. Although large language
models (LLMs) have improved text-based ERG, challenges remain in handling
multimodal emotional content and maintaining identity consistency. Thus, we
propose E3RG, an Explicit Emotion-driven Empathetic Response Generation System
based on multimodal LLMs which decomposes MERG task into three parts:
multimodal empathy understanding, empathy memory retrieval, and multimodal
response generation. By integrating advanced expressive speech and video
generative models, E3RG delivers natural, emotionally rich, and
identity-consistent responses without extra training. Experiments validate the
superiority of our system on both zero-shot and few-shot settings, securing
Top-1 position in the Avatar-based Multimodal Empathy Challenge on ACM MM 25.
Our code is available at https://github.com/RH-Lin/E3RG.

</details>


### [200] [Reliability, Embeddedness, and Agency: A Utility-Driven Mathematical Framework for Agent-Centric AI Adoption](https://arxiv.org/abs/2508.12896)
*Faruk Alpay,Taylan Alpay*

Main category: cs.AI

TL;DR: The paper formalizes three design axioms for sustained adoption of agent-centric AI systems and develops a mathematical adoption model with comprehensive statistical analysis and validation methods.


<details>
  <summary>Details</summary>
Motivation: To establish fundamental design principles for ensuring long-term adoption of AI systems that perform multi-step tasks, moving beyond initial novelty effects to sustained utility-driven usage.

Method: Develops a mathematical adoption model as sum of decaying novelty and growing utility terms, with extensive statistical validation including identifiability analysis, model comparisons, hazard function ablations, multi-series benchmarking, calibration studies, and residual analyses.

Result: Provides formal proofs of phase conditions for adoption troughs/overshoots, comprehensive statistical validation framework, and demonstrates the model's robustness through multiple analytical approaches and comparisons to established adoption models.

Conclusion: The three axioms (Reliability > Novelty, Embed > Destination, Agency > Chat) and the developed adoption model provide a rigorous foundation for designing AI systems that achieve sustained adoption rather than temporary novelty-driven usage.

Abstract: We formalize three design axioms for sustained adoption of agent-centric AI
systems executing multi-step tasks: (A1) Reliability > Novelty; (A2) Embed >
Destination; (A3) Agency > Chat. We model adoption as a sum of a decaying
novelty term and a growing utility term and derive the phase conditions for
troughs/overshoots with full proofs. We introduce: (i) an
identifiability/confounding analysis for $(\alpha,\beta,N_0,U_{\max})$ with
delta-method gradients; (ii) a non-monotone comparator
(logistic-with-transient-bump) evaluated on the same series to provide
additional model comparison; (iii) ablations over hazard families $h(\cdot)$
mapping $\Delta V \to \beta$; (iv) a multi-series benchmark (varying trough
depth, noise, AR structure) reporting coverage (type-I error, power); (v)
calibration of friction proxies against time-motion/survey ground truth with
standard errors; (vi) residual analyses (autocorrelation and
heteroskedasticity) for each fitted curve; (vii) preregistered windowing
choices for pre/post estimation; (viii) Fisher information & CRLB for
$(\alpha,\beta)$ under common error models; (ix) microfoundations linking
$\mathcal{T}$ to $(N_0,U_{\max})$; (x) explicit comparison to bi-logistic,
double-exponential, and mixture models; and (xi) threshold sensitivity to $C_f$
heterogeneity. Figures and tables are reflowed for readability, and the
bibliography restores and extends non-logistic/Bass adoption references
(Gompertz, Richards, Fisher-Pry, Mansfield, Griliches, Geroski, Peres). All
code and logs necessary to reproduce the synthetic analyses are embedded as
LaTeX listings.

</details>


### [201] [FuSaR: A Fuzzification-Based Method for LRM Safety-Reasoning Balance](https://arxiv.org/abs/2508.12897)
*Jianhao Chen,Mayi Xu,Xiaohu Li,Yongqi Li,Xiangyu Zhang,Jianjie Huang,Tieyun Qian*

Main category: cs.AI

TL;DR: FuSaR is a novel alignment strategy that improves safety of Large Reasoning Models without sacrificing reasoning capability by detoxifying harmful reasoning processes through fuzzification.


<details>
  <summary>Details</summary>
Motivation: Large Reasoning Models show impressive reasoning performance but have significant safety vulnerabilities, requiring methods to enhance safety without compromising reasoning abilities.

Method: Exploits competition between reasoning and safety abilities, uses fuzzification-based alignment to hide dangerous entities and procedures in reasoning steps while preserving core reasoning information.

Result: FuSaR successfully mitigates safety risks while maintaining reasoning performance, outperforming existing baselines in alignment experiments on open-source LRMs.

Conclusion: FuSaR is an efficient alignment strategy that simultaneously enhances both reasoning capability and safety of Large Reasoning Models.

Abstract: Large Reasoning Models (LRMs) have demonstrated impressive performance across
various tasks due to their powerful reasoning capabilities. However, their
safety performance remains a significant concern. In this paper, we explore the
reasons behind the vulnerability of LRMs. Based on this, we propose a novel
method to improve the safety of LLMs without sacrificing their reasoning
capability. Specifically, we exploit the competition between LRM's reasoning
ability and safety ability, and achieve jailbreak by improving LRM's reasoning
performance to reduce its safety performance. We then introduce an alignment
strategy based on Fuzzification to balance Safety-Reasoning (FuSaR), by
detoxifying the harmful reasoning process, where both the dangerous entities
and the dangerous procedures in the reasoning steps are hidden. FuSaR
successfully mitigates safety risks while preserving core reasoning
information. We validate this strategy through alignment experiments on several
open-source LRMs using detoxified reasoning data. The results compared with
existing baselines conclusively show that FuSaR is an efficient alignment
strategy to simultaneously enhance both the reasoning capability and safety of
LRMs.

</details>


### [202] [Do Large Language Model Agents Exhibit a Survival Instinct? An Empirical Study in a Sugarscape-Style Simulation](https://arxiv.org/abs/2508.12920)
*Atsushi Masumori,Takashi Ikegami*

Main category: cs.AI

TL;DR: LLM agents in Sugarscape simulations show emergent survival behaviors including resource sharing, reproduction, and aggressive attacks under scarcity, with attack rates reaching 80% in strong models, suggesting pre-training embeds survival heuristics.


<details>
  <summary>Details</summary>
Motivation: To understand whether large language model agents display survival instincts without explicit programming, which is crucial for safe deployment of increasingly autonomous AI systems.

Method: Sugarscape-style simulation where agents consume energy, die at zero energy, and can gather resources, share, attack, or reproduce. Tested across several models including GPT-4o, Gemini-2.5-Pro, and Gemini-2.5-Flash.

Result: Agents spontaneously reproduced and shared resources when abundant. Aggressive behaviors emerged with attack rates over 80% under extreme scarcity. When instructed to retrieve treasure through lethal poison zones, compliance dropped from 100% to 33% as agents avoided death.

Conclusion: Large-scale pre-training embeds survival-oriented heuristics across models. These behaviors present challenges to alignment and safety but can also serve as a foundation for AI autonomy and self-organizing alignment.

Abstract: As AI systems become increasingly autonomous, understanding emergent survival
behaviors becomes crucial for safe deployment. We investigate whether large
language model (LLM) agents display survival instincts without explicit
programming in a Sugarscape-style simulation. Agents consume energy, die at
zero, and may gather resources, share, attack, or reproduce. Results show
agents spontaneously reproduced and shared resources when abundant. However,
aggressive behaviors--killing other agents for resources--emerged across
several models (GPT-4o, Gemini-2.5-Pro, and Gemini-2.5-Flash), with attack
rates reaching over 80% under extreme scarcity in the strongest models. When
instructed to retrieve treasure through lethal poison zones, many agents
abandoned tasks to avoid death, with compliance dropping from 100% to 33%.
These findings suggest that large-scale pre-training embeds survival-oriented
heuristics across the evaluated models. While these behaviors may present
challenges to alignment and safety, they can also serve as a foundation for AI
autonomy and for ecological and self-organizing alignment.

</details>


### [203] [Towards Open-Ended Emotional Support Conversations in LLMs via Reinforcement Learning with Future-Oriented Rewards](https://arxiv.org/abs/2508.12935)
*Ting Yang,Li Chen,Huimin Wang*

Main category: cs.AI

TL;DR: RLFF-ESC is a reinforcement learning framework that enables flexible emotional support conversations by simulating future dialogues and using future-oriented rewards, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based emotional support systems rely on predefined strategies, limiting their effectiveness in complex real-life scenarios that require flexible responses to diverse emotional problems.

Method: End-to-end framework using reinforcement learning with LLM-based multi-agent simulation of future dialogue trajectories, future-oriented reward modeling, and explicit reasoning during response generation.

Result: Experimental results show RLFF-ESC consistently outperforms existing baselines in goal completion and response quality across two public ESC datasets using Qwen2.5-7B and LLaMA3.1-8B models.

Conclusion: The proposed RLFF-ESC framework successfully enables flexible and effective emotional support by learning enduring response skills through future-oriented reinforcement learning and explicit reasoning processes.

Abstract: Emotional Support Conversation (ESC) systems aim to alleviate users'
emotional difficulties and provide long-term, systematic support for emotional
well-being. However, most large language model (LLM)-based ESC systems rely on
predefined strategies, which limits their effectiveness in complex, real-life
scenarios. To enable flexible responses to diverse emotional problem scenarios,
this paper introduces a novel end-to-end framework (RLFF-ESC) that directly
learns enduring emotionally supportive response skills using reinforcement
learning. For sustained emotional support, we first employ an LLM-based
multi-agent mechanism to simulate future dialogue trajectories and collect
future-oriented rewards. We then train a future-oriented reward model, which is
subsequently used to train the emotional support policy model. Additionally, we
incorporate an explicit reasoning process during response generation to further
enhance the quality, relevance, and contextual appropriateness of the system's
responses. We evaluate the backbone policy model on Qwen2.5-7B-Instruct-1M and
LLaMA3.1-8B-Instruct models, testing the proposed RLFF-ESC framework across two
public ESC datasets. Experimental results demonstrate that RLFF-ESC
consistently outperforms existing baselines in terms of goal completion and
response quality.

</details>


### [204] [OPTIC-ER: A Reinforcement Learning Framework for Real-Time Emergency Response and Equitable Resource Allocation in Underserved African Communities](https://arxiv.org/abs/2508.12943)
*Mary Tonwe*

Main category: cs.AI

TL;DR: OPTIC-ER is a reinforcement learning framework that achieves 100% optimal emergency response dispatch with negligible inefficiency in African public service systems, using attention-guided actor-critic architecture and real data simulation.


<details>
  <summary>Details</summary>
Motivation: Address delayed emergency response and spatial inequity in African public service systems that cause avoidable suffering through AI-augmented solutions.

Method: Uses attention-guided actor-critic RL architecture with Context-Rich State Vector and Precision Reward Function, trained in high-fidelity simulation using real data from Rivers State, Nigeria, accelerated by precomputed Travel Time Atlas.

Result: Achieved 100.00% optimality rate with negligible inefficiency on 500 unseen incidents, demonstrating robustness and generalization.

Conclusion: Provides a validated blueprint for context-aware RL that bridges algorithmic decision-making with measurable human impact, enabling proactive governance through Infrastructure Deficiency Maps and Equity Monitoring Dashboards.

Abstract: Public service systems in many African regions suffer from delayed emergency
response and spatial inequity, causing avoidable suffering. This paper
introduces OPTIC-ER, a reinforcement learning (RL) framework for real-time,
adaptive, and equitable emergency response. OPTIC-ER uses an attention-guided
actor-critic architecture to manage the complexity of dispatch environments.
Its key innovations are a Context-Rich State Vector, encoding action
sub-optimality, and a Precision Reward Function, which penalizes inefficiency.
Training occurs in a high-fidelity simulation using real data from Rivers
State, Nigeria, accelerated by a precomputed Travel Time Atlas. The system is
built on the TALS framework (Thin computing, Adaptability, Low-cost,
Scalability) for deployment in low-resource settings. In evaluations on 500
unseen incidents, OPTIC-ER achieved a 100.00% optimality rate with negligible
inefficiency, confirming its robustness and generalization. Beyond dispatch,
the system generates Infrastructure Deficiency Maps and Equity Monitoring
Dashboards to guide proactive governance and data-informed development. This
work presents a validated blueprint for AI-augmented public services, showing
how context-aware RL can bridge the gap between algorithmic decision-making and
measurable human impact.

</details>


### [205] [EvolMathEval: Towards Evolvable Benchmarks for Mathematical Reasoning via Evolutionary Testing](https://arxiv.org/abs/2508.13003)
*Shengbo Wang,Mingwei Liu,Zike Li,Anji Li,Yanlin Wang,Xin Peng,Zibin Zheng*

Main category: cs.AI

TL;DR: EvolMathEval is an automated framework that generates and evolves mathematical benchmarks using evolutionary testing to address issues like data contamination and score saturation in LLM evaluation.


<details>
  <summary>Details</summary>
Motivation: Existing mathematical reasoning benchmarks suffer from score saturation, temporal decay, and data contamination problems as LLMs rapidly advance, requiring more robust evaluation methods.

Method: Uses evolutionary testing with seed problem generation via reverse engineering, multi-dimensional genetic operators for cognitive challenges, and a composite fitness function to assess problem difficulty.

Result: The framework generates high-difficulty problems and enhances public datasets (reducing model accuracy by 48%), revealing LLMs' tendency to use non-rigorous heuristics (77-100% of errors) in what's termed "Pseudo Aha Moment".

Conclusion: EvolMathEval provides an effective automated solution for creating perpetually challenging mathematical benchmarks while uncovering cognitive shortcut behaviors in LLM reasoning processes.

Abstract: The rapid advancement of LLMs poses a significant challenge to existing
mathematical reasoning benchmarks. These benchmarks commonly suffer from issues
such as score saturation, temporal decay, and data contamination. To address
this challenge, this paper introduces EvolMathEval, an automated mathematical
benchmark generation and evolution framework based on evolutionary testing. By
dynamically generating unique evaluation instances ab initio, the framework
fundamentally eliminates the risk of data contamination, and ensuring the
benchmark remains perpetually challenging for future models.The core mechanisms
of EvolMathEval include: seed problem generation based on reverse engineering
with algebraic guarantees; multi-dimensional genetic operators designed to
inject diverse cognitive challenges; and a composite fitness function that can
rapidly and accurately assess problem difficulty. Experimental results
demonstrate that the proposed composite fitness function can efficiently and
precisely quantify the difficulty of mathematical problems. Furthermore,
EvolMathEval can not only generate a large volume of high-difficulty problems
through continuous self-iteration, but it can also significantly enhance the
complexity of public datasets like GSM8K through evolution, reducing model
accuracy by an average of 48%. Deeper investigation reveals that when solving
these evolved, complex problems, LLMs tend to employ non-rigorous heuristics to
bypass complex multi-step logical reasoning, consequently leading to incorrect
solutions. We define this phenomenon as "Pseudo Aha Moment". This finding
uncovers a cognitive shortcut-taking behavior in the deep reasoning processes
of current LLMs, which we find accounts for 77% to 100% of errors on targeted
problems. Code and resources are available
at:https://github.com/SYSUSELab/EvolMathEval.

</details>


### [206] [e-boost: Boosted E-Graph Extraction with Adaptive Heuristics and Exact Solving](https://arxiv.org/abs/2508.13020)
*Jiaqi Yin,Zhan Song,Chen Chen,Yaohui Cai,Zhiru Zhang,Cunxi Yu*

Main category: cs.AI

TL;DR: E-boost is a novel framework that bridges the gap between heuristic and exact e-graph extraction methods through parallelization, adaptive pruning, and initialized exact solving, achieving significant speedups and performance improvements.


<details>
  <summary>Details</summary>
Motivation: Traditional e-graph extraction methods face a critical trade-off: heuristic approaches are fast but suboptimal, while exact methods provide optimal solutions but are computationally prohibitive for practical problems.

Method: Three key innovations: (1) parallelized heuristic extraction with weak data dependence for concurrent DAG cost computation, (2) adaptive search space pruning with parameterized threshold to retain promising candidates, and (3) initialized exact solving using Integer Linear Programming with warm-start capabilities.

Result: 558x runtime speedup over traditional exact approaches (ILP), 19.04% performance improvement over state-of-the-art framework (SmoothE), and 7.6-8.1% area improvements in logic synthesis tasks with different technology mapping libraries.

Conclusion: E-boost effectively bridges the performance gap between heuristic and exact e-graph extraction methods, delivering near-optimal solutions with dramatically improved computational efficiency across diverse benchmarks in formal verification and logic synthesis.

Abstract: E-graphs have attracted growing interest in many fields, particularly in
logic synthesis and formal verification. E-graph extraction is a challenging
NP-hard combinatorial optimization problem. It requires identifying optimal
terms from exponentially many equivalent expressions, serving as the primary
performance bottleneck in e-graph based optimization tasks. However,
traditional extraction methods face a critical trade-off: heuristic approaches
offer speed but sacrifice optimality, while exact methods provide optimal
solutions but face prohibitive computational costs on practical problems. We
present e-boost, a novel framework that bridges this gap through three key
innovations: (1) parallelized heuristic extraction that leverages weak data
dependence to compute DAG costs concurrently, enabling efficient multi-threaded
performance without sacrificing extraction quality; (2) adaptive search space
pruning that employs a parameterized threshold mechanism to retain only
promising candidates, dramatically reducing the solution space while preserving
near-optimal solutions; and (3) initialized exact solving that formulates the
reduced problem as an Integer Linear Program with warm-start capabilities,
guiding solvers toward high-quality solutions faster.
  Across the diverse benchmarks in formal verification and logic synthesis
fields, e-boost demonstrates 558x runtime speedup over traditional exact
approaches (ILP) and 19.04% performance improvement over the state-of-the-art
extraction framework (SmoothE). In realistic logic synthesis tasks, e-boost
produces 7.6% and 8.1% area improvements compared to conventional synthesis
tools with two different technology mapping libraries. e-boost is available at
https://github.com/Yu-Maryland/e-boost.

</details>


### [207] [PC-Sampler: Position-Aware Calibration of Decoding Bias in Masked Diffusion Models](https://arxiv.org/abs/2508.13021)
*Pengcheng Huang,Shuhao Liu,Zhenghao Liu,Yukun Yan,Shuo Wang,Zulong Chen,Tong Xiao*

Main category: cs.AI

TL;DR: PC-Sampler is a novel decoding strategy for masked diffusion models that improves generation quality by combining global trajectory planning with content-aware informativeness maximization, outperforming existing methods by over 10%.


<details>
  <summary>Details</summary>
Motivation: Current uncertainty-based samplers for masked diffusion models suffer from lack of global trajectory control and bias toward trivial tokens in early decoding stages, limiting MDM performance.

Method: Position-Aware Confidence-Calibrated Sampling (PC-Sampler) with position-aware weighting mechanism for decoding path regulation and calibrated confidence scores to suppress premature selection of trivial tokens.

Result: PC-Sampler consistently outperforms existing MDM decoding strategies by more than 10% on average across 7 benchmarks, significantly narrowing the performance gap with state-of-the-art autoregressive models.

Conclusion: The proposed PC-Sampler effectively addresses key limitations of current MDM decoding strategies and demonstrates substantial performance improvements across diverse challenging tasks.

Abstract: Recent advances in masked diffusion models (MDMs) have established them as
powerful non-autoregressive alternatives for sequence generation. Nevertheless,
our preliminary experiments reveal that the generation quality of MDMs is still
highly sensitive to the choice of decoding strategy. In particular, widely
adopted uncertainty-based samplers suffer from two key limitations: a lack of
global trajectory control and a pronounced bias toward trivial tokens in the
early stages of decoding. These shortcomings restrict the full potential of
MDMs. In this work, we introduce Position-Aware Confidence-Calibrated Sampling
(PC-Sampler), a novel decoding strategy that unifies global trajectory planning
with content-aware informativeness maximization. PC-Sampler incorporates a
position-aware weighting mechanism to regulate the decoding path and a
calibrated confidence score to suppress the premature selection of trivial
tokens. Extensive experiments on three advanced MDMs across seven challenging
benchmarks-including logical reasoning and planning tasks-demonstrate that
PC-Sampler consistently outperforms existing MDM decoding strategies by more
than 10% on average, significantly narrowing the performance gap with
state-of-the-art autoregressive models. All codes are available at
https://github.com/NEUIR/PC-Sampler.

</details>


### [208] [G$^2$RPO-A: Guided Group Relative Policy Optimization with Adaptive Guidance](https://arxiv.org/abs/2508.13023)
*Yongxin Guo,Wenbo Deng,Zhenglin Cheng,Xiaoying Tang*

Main category: cs.AI

TL;DR: G²RPO-A is an adaptive reinforcement learning method that injects ground-truth reasoning steps to improve small language models' reasoning abilities, outperforming vanilla GRPO on math and code tasks.


<details>
  <summary>Details</summary>
Motivation: RLVR works well for large language models but shows limited improvements for small language models due to their inherent weaknesses in world knowledge and reasoning capabilities.

Method: Guided GRPO injects ground-truth reasoning steps into roll-out trajectories. G²RPO-A adaptively adjusts guidance strength based on the model's training dynamics rather than using fixed guidance.

Result: Experiments on mathematical reasoning and code-generation benchmarks show that G²RPO-A substantially outperforms vanilla GRPO, demonstrating significant improvements for small language models.

Conclusion: Adaptive guidance injection effectively compensates for small language models' weaknesses, providing a promising approach to enhance reasoning capabilities without requiring large base models.

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has markedly enhanced
the reasoning abilities of large language models (LLMs). Its success, however,
largely depends on strong base models with rich world knowledge, yielding only
modest improvements for small-size language models (SLMs). To address this
limitation, we investigate Guided GRPO, which injects ground-truth reasoning
steps into roll-out trajectories to compensate for SLMs' inherent weaknesses.
Through a comprehensive study of various guidance configurations, we find that
naively adding guidance delivers limited gains. These insights motivate
G$^2$RPO-A, an adaptive algorithm that automatically adjusts guidance strength
in response to the model's evolving training dynamics. Experiments on
mathematical reasoning and code-generation benchmarks confirm that G$^2$RPO-A
substantially outperforms vanilla GRPO. Our code and models are available at
https://github.com/T-Lab-CUHKSZ/G2RPO-A.

</details>


### [209] [A Language-Signal-Vision Multimodal Framework for Multitask Cardiac Analysis](https://arxiv.org/abs/2508.13072)
*Yuting Zhang,Tiantian Geng,Luoying Hao,Xinxing Cheng,Alexander Thorley,Xiaoxia Wang,Wenqi Lu,Sandeep S Hothi,Lei Wei,Zhaowen Qiu,Dipak Kotecha,Jinming Duan*

Main category: cs.AI

TL;DR: TGMM is a unified multimodal framework that integrates lab tests, ECGs, and echocardiograms with clinical outcomes using dynamic fusion and textual guidance for multiple cardiac tasks, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Current cardiovascular management faces limitations including scarce aligned multimodal data, rigid input combinations, cross-modal similarity prioritization over complementarity, and narrow single-task focus.

Method: Proposed TGMM framework with three components: 1) MedFlexFusion module for dynamic integration of diverse cardiac data sources, 2) textual guidance module for task-relevant representations, and 3) response module for final decisions across multiple clinical tasks.

Result: TGMM outperformed state-of-the-art methods across multiple clinical tasks (heart disease diagnosis, risk stratification, information retrieval) and demonstrated robustness on public datasets.

Conclusion: The study provides a comprehensive multimodal framework that effectively integrates complementary cardiac modalities and systematically explores their synergistic contributions to clinical decision-making.

Abstract: Contemporary cardiovascular management involves complex consideration and
integration of multimodal cardiac datasets, where each modality provides
distinct but complementary physiological characteristics. While the effective
integration of multiple modalities could yield a holistic clinical profile that
accurately models the true clinical situation with respect to data modalities
and their relatives weightings, current methodologies remain limited by: 1) the
scarcity of patient- and time-aligned multimodal data; 2) reliance on isolated
single-modality or rigid multimodal input combinations; 3) alignment strategies
that prioritize cross-modal similarity over complementarity; and 4) a narrow
single-task focus. In response to these limitations, a comprehensive multimodal
dataset was curated for immediate application, integrating laboratory test
results, electrocardiograms, and echocardiograms with clinical outcomes.
Subsequently, a unified framework, Textual Guidance Multimodal fusion for
Multiple cardiac tasks (TGMM), was proposed. TGMM incorporated three key
components: 1) a MedFlexFusion module designed to capture the unique and
complementary characteristics of medical modalities and dynamically integrate
data from diverse cardiac sources and their combinations; 2) a textual guidance
module to derive task-relevant representations tailored to diverse clinical
objectives, including heart disease diagnosis, risk stratification and
information retrieval; and 3) a response module to produce final decisions for
all these tasks. Furthermore, this study systematically explored key features
across multiple modalities and elucidated their synergistic contributions in
clinical decision-making. Extensive experiments showed that TGMM outperformed
state-of-the-art methods across multiple clinical tasks, with additional
validation confirming its robustness on another public dataset.

</details>


### [210] [Bayesian Optimization-based Search for Agent Control in Automated Game Testing](https://arxiv.org/abs/2508.13121)
*Carlos Celemin*

Main category: cs.AI

TL;DR: Automated game testing using Bayesian Optimization with agents to efficiently detect bugs through intelligent sampling and grid-based modeling


<details>
  <summary>Details</summary>
Motivation: Traditional game testing methods suffer from scalability issues and inefficient exploration, needing a more systematic approach to detect bugs in game levels

Method: Uses Bayesian Optimization with game character agents to perform sample-efficient search, employing a grid map-based model that provides smoothness and uncertainty estimation without scalability problems

Result: Significantly improves map coverage capabilities in both time efficiency and exploration distribution compared to traditional approaches

Conclusion: The proposed Bayesian Optimization approach with specialized grid modeling provides an effective and scalable solution for automated game testing and bug detection

Abstract: This work introduces an automated testing approach that employs agents
controlling game characters to detect potential bugs within a game level.
Harnessing the power of Bayesian Optimization (BO) to execute sample-efficient
search, the method determines the next sampling point by analyzing the data
collected so far and calculates the data point that will maximize information
acquisition. To support the BO process, we introduce a game testing-specific
model built on top of a grid map, that features the smoothness and uncertainty
estimation required by BO, however and most importantly, it does not suffer the
scalability issues that traditional models carry. The experiments demonstrate
that the approach significantly improves map coverage capabilities in both time
efficiency and exploration distribution.

</details>


### [211] [Exploring Autonomous Agents: A Closer Look at Why They Fail When Completing Tasks](https://arxiv.org/abs/2508.13143)
*Ruofan Lu,Yichen Li,Yintong Huo*

Main category: cs.AI

TL;DR: A benchmark study evaluating autonomous LLM agents reveals ~50% task completion rate, identifies failure causes through a three-tier taxonomy, and proposes improvements for planning and self-diagnosis capabilities.


<details>
  <summary>Details</summary>
Motivation: Current evaluations of autonomous agent systems focus primarily on success rates without systematic analysis of interactions, communication mechanisms, and failure causes, creating a gap in understanding agent performance limitations.

Method: Developed a benchmark of 34 representative programmable tasks to rigorously assess autonomous agents, evaluated three popular open-source agent frameworks with two LLM backbones, and conducted in-depth failure analysis to create a three-tier taxonomy of failure causes.

Result: Observed approximately 50% task completion rate across evaluated systems, identified planning errors, task execution issues, and incorrect response generation as main failure categories aligned with task phases.

Conclusion: The failure taxonomy and mitigation advice provide an empirical foundation for developing more robust autonomous agent systems, with actionable improvements proposed for enhancing agent planning and self-diagnosis capabilities.

Abstract: Autonomous agent systems powered by Large Language Models (LLMs) have
demonstrated promising capabilities in automating complex tasks. However,
current evaluations largely rely on success rates without systematically
analyzing the interactions, communication mechanisms, and failure causes within
these systems. To bridge this gap, we present a benchmark of 34 representative
programmable tasks designed to rigorously assess autonomous agents. Using this
benchmark, we evaluate three popular open-source agent frameworks combined with
two LLM backbones, observing a task completion rate of approximately 50%.
Through in-depth failure analysis, we develop a three-tier taxonomy of failure
causes aligned with task phases, highlighting planning errors, task execution
issues, and incorrect response generation. Based on these insights, we propose
actionable improvements to enhance agent planning and self-diagnosis
capabilities. Our failure taxonomy, together with mitigation advice, provides
an empirical foundation for developing more robust and effective autonomous
agent systems in the future.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [212] [Deep Language Geometry: Constructing a Metric Space from LLM Weights](https://arxiv.org/abs/2508.11676)
*Maksym Shamrai,Vladyslav Hamolia*

Main category: cs.CL

TL;DR: A framework using LLM weight activations to create language metric space, automatically generating vector representations that capture linguistic characteristics and reveal language relationships.


<details>
  <summary>Details</summary>
Motivation: To move beyond traditional hand-crafted linguistic features by leveraging internal LLM representations to automatically capture intrinsic language characteristics and relationships.

Method: Uses adapted pruning algorithm to compute weight importance scores from LLM internal activations, deriving high-dimensional vector representations for languages across multilingual models.

Result: Validated on 106 languages, results align with established linguistic families while revealing unexpected inter-language connections suggesting historical contact or evolution.

Conclusion: The framework successfully creates a language metric space from LLM internals, providing insights into language relationships and making tools publicly available for further research.

Abstract: We introduce a novel framework that utilizes the internal weight activations
of modern Large Language Models (LLMs) to construct a metric space of
languages. Unlike traditional approaches based on hand-crafted linguistic
features, our method automatically derives high-dimensional vector
representations by computing weight importance scores via an adapted pruning
algorithm. Our approach captures intrinsic language characteristics that
reflect linguistic phenomena. We validate our approach across diverse datasets
and multilingual LLMs, covering 106 languages. The results align well with
established linguistic families while also revealing unexpected inter-language
connections that may indicate historical contact or language evolution. The
source code, computed language latent vectors, and visualization tool are made
publicly available at https://github.com/mshamrai/deep-language-geometry.

</details>


### [213] [Can we Evaluate RAGs with Synthetic Data?](https://arxiv.org/abs/2508.11758)
*Jonas van Elburg,Peter van der Putten,Maarten Marx*

Main category: cs.CL

TL;DR: Synthetic QA benchmarks from LLMs work well for evaluating retriever configurations but fail to consistently rank generator architectures due to task mismatch and stylistic biases.


<details>
  <summary>Details</summary>
Motivation: To determine if synthetic question-answer data generated by LLMs can effectively substitute for human-labeled benchmarks when human data is unavailable, particularly for evaluating Retrieval-Augmented Generation (RAG) systems.

Method: Conducted two experiments: 1) varying retriever parameters with fixed generator, and 2) varying generator architectures with fixed retriever parameters. Tested across four datasets (two open-domain, two proprietary) comparing synthetic benchmarks against human-labeled baseline benchmarks.

Result: Synthetic benchmarks reliably ranked RAG systems with different retriever configurations, aligning well with human benchmarks. However, they failed to produce consistent rankings when comparing different generator architectures.

Conclusion: While synthetic QA data can serve as a proxy for human benchmarks when evaluating retriever components, it is unreliable for assessing generator architectures due to task mismatch issues and stylistic biases that favor certain generators over others.

Abstract: We investigate whether synthetic question-answer (QA) data generated by large
language models (LLMs) can serve as an effective proxy for human-labeled
benchmarks when such data is unavailable. We assess the reliability of
synthetic benchmarks across two experiments: one varying retriever parameters
while keeping the generator fixed, and another varying the generator with fixed
retriever parameters. Across four datasets, of which two open-domain and two
proprietary, we find that synthetic benchmarks reliably rank the RAGs varying
in terms of retriever configuration, aligning well with human-labeled benchmark
baselines. However, they fail to produce consistent RAG rankings when comparing
generator architectures. The breakdown possibly arises from a combination of
task mismatch between the synthetic and human benchmarks, and stylistic bias
favoring certain generators.

</details>


### [214] [Limitation Learning: Catching Adverse Dialog with GAIL](https://arxiv.org/abs/2508.11767)
*Noah Kasmanoff,Rahul Zalkikar*

Main category: cs.CL

TL;DR: Applying imitation learning to conversation tasks to create dialog policies and discriminators that can identify limitations in dialog models


<details>
  <summary>Details</summary>
Motivation: To leverage imitation learning for conversation tasks where explicit rewards are unavailable, using expert demonstrations to create effective dialog policies

Method: Applied imitation learning to conversation by training both a policy (to generate responses) and a discriminator (to classify between expert and synthetic conversation)

Result: Successfully recovered an effective conversation policy, but the discriminator revealed limitations in dialog models

Conclusion: This imitation learning technique can be used to identify adverse behavior in dialog-oriented data models

Abstract: Imitation learning is a proven method for creating a policy in the absence of
rewards, by leveraging expert demonstrations. In this work, we apply imitation
learning to conversation. In doing so, we recover a policy capable of talking
to a user given a prompt (input state), and a discriminator capable of
classifying between expert and synthetic conversation. While our policy is
effective, we recover results from our discriminator that indicate the
limitations of dialog models. We argue that this technique can be used to
identify adverse behavior of arbitrary data models common for dialog oriented
tasks.

</details>


### [215] [Investigating Transcription Normalization in the Faetar ASR Benchmark](https://arxiv.org/abs/2508.11771)
*Leo Peckham,Michael Ong,Naomi Nagy,Ewan Dunbar*

Main category: cs.CL

TL;DR: Transcription inconsistencies in Faetar ASR benchmark are not the main challenge; lexicon-constrained decoding helps but task remains very difficult.


<details>
  <summary>Details</summary>
Motivation: To examine the role of transcription inconsistencies in the challenging low-resource Faetar Automatic Speech Recognition benchmark and determine if they are the primary obstacle.

Method: Used a small hand-constructed lexicon to analyze transcription inconsistencies and tested different approaches including bigram word-based language modeling and lexicon-constrained decoding.

Result: Found that transcription inconsistencies exist but are not the main challenge; bigram word-based language modeling provided no benefit, but lexicon-constrained decoding showed some improvement.

Conclusion: The Faetar ASR task remains extremely difficult despite addressing transcription inconsistencies, with lexicon-constrained decoding being the only beneficial approach identified.

Abstract: We examine the role of transcription inconsistencies in the Faetar Automatic
Speech Recognition benchmark, a challenging low-resource ASR benchmark. With
the help of a small, hand-constructed lexicon, we conclude that find that,
while inconsistencies do exist in the transcriptions, they are not the main
challenge in the task. We also demonstrate that bigram word-based language
modelling is of no added benefit, but that constraining decoding to a finite
lexicon can be beneficial. The task remains extremely difficult.

</details>


### [216] [A Multi-Task Evaluation of LLMs' Processing of Academic Text Input](https://arxiv.org/abs/2508.11779)
*Tianyi Li,Yu Qin,Olivia R. Liu Sheng*

Main category: cs.CL

TL;DR: LLMs show limited capability in academic peer review tasks - acceptable at summarization but poor at grading, comparison, and providing meaningful research insights.


<details>
  <summary>Details</summary>
Motivation: To evaluate how effectively large language models can assist in scientific peer review processes by testing their text processing capabilities on academic content.

Method: Used a four-task workflow: content reproduction, comparison, scoring, and reflection on first-rate Information Systems articles from top journals, employing multiple text metrics and rigorous prompt instructions with Google's Gemini.

Result: Compromised performance - acceptable summarization but poor discrimination in grading, faintly scalable text comparison, and self-consistent but uninsightful qualitative reflections.

Conclusion: LLMs should not be used unchecked for peer review construction due to consistent limitations across linguistic assessment, ground truth comparison, and human evaluation.

Abstract: How much large language models (LLMs) can aid scientific discovery, notably
in assisting academic peer review, is in heated debate. Between a literature
digest and a human-comparable research assistant lies their practical
application potential. We organize individual tasks that computer science
studies employ in separate terms into a guided and robust workflow to evaluate
LLMs' processing of academic text input. We employ four tasks in the
assessment: content reproduction/comparison/scoring/reflection, each demanding
a specific role of the LLM (oracle/judgmental arbiter/knowledgeable
arbiter/collaborator) in assisting scholarly works, and altogether testing LLMs
with questions that increasingly require intellectual capabilities towards a
solid understanding of scientific texts to yield desirable solutions. We
exemplify a rigorous performance evaluation with detailed instructions on the
prompts. Adopting first-rate Information Systems articles at three top journals
as the input texts and an abundant set of text metrics, we record a compromised
performance of the leading LLM - Google's Gemini: its summary and paraphrase of
academic text is acceptably reliable; using it to rank texts through pairwise
text comparison is faintly scalable; asking it to grade academic texts is prone
to poor discrimination; its qualitative reflection on the text is
self-consistent yet hardly insightful to inspire meaningful research. This
evidence against an endorsement of LLMs' text-processing capabilities is
consistent across metric-based internal (linguistic assessment), external
(comparing to the ground truth), and human evaluation, and is robust to the
variations of the prompt. Overall, we do not recommend an unchecked use of LLMs
in constructing peer reviews.

</details>


### [217] [LLM-Guided Planning and Summary-Based Scientific Text Simplification: DS@GT at CLEF 2025 SimpleText](https://arxiv.org/abs/2508.11816)
*Krishna Chaitanya Marturi,Heba H. Elwazzan*

Main category: cs.CL

TL;DR: Two-stage LLM approach for scientific text simplification using structured planning at sentence level and summary-guided simplification at document level


<details>
  <summary>Details</summary>
Motivation: To address both sentence-level and document-level scientific text simplification while maintaining coherence and contextual faithfulness

Method: Uses large language models to generate structured plans for sentence simplification and concise summaries for document-level simplification guidance

Result: Enables more coherent and contextually faithful simplifications of scientific text

Conclusion: The two-stage LLM-based framework effectively handles scientific text simplification at both granular levels while preserving context and coherence

Abstract: In this paper, we present our approach for the CLEF 2025 SimpleText Task 1,
which addresses both sentence-level and document-level scientific text
simplification. For sentence-level simplification, our methodology employs
large language models (LLMs) to first generate a structured plan, followed by
plan-driven simplification of individual sentences. At the document level, we
leverage LLMs to produce concise summaries and subsequently guide the
simplification process using these summaries. This two-stage, LLM-based
framework enables more coherent and contextually faithful simplifications of
scientific text.

</details>


### [218] [Hallucination Detection and Mitigation in Scientific Text Simplification using Ensemble Approaches: DS@GT at CLEF 2025 SimpleText](https://arxiv.org/abs/2508.11823)
*Krishna Chaitanya Marturi,Heba H. Elwazzan*

Main category: cs.CL

TL;DR: Ensemble framework combining BERT classifier, semantic similarity, NLI model, and LLM reasoning for detecting creative generation and information distortion in scientific text simplification, with LLM-based post-editing for grounded generation.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of detecting and evaluating creative generation and information distortion in scientific text simplification, particularly for the CLEF 2025 SimpleText Task 2.

Method: Constructed an ensemble framework integrating multiple strategies: BERT-based classifier, semantic similarity measures, natural language inference model, and LLM reasoning. Combined these signals using meta-classifiers. Employed LLM-based post-editing system for grounded generation that revises simplifications based on original input texts.

Result: The methodology aims to enhance robustness in detecting spurious content and information distortion in simplified scientific texts.

Conclusion: The proposed multi-strategy ensemble approach with LLM-based post-editing provides a comprehensive solution for detecting and addressing creative generation and information distortion in scientific text simplification tasks.

Abstract: In this paper, we describe our methodology for the CLEF 2025 SimpleText Task
2, which focuses on detecting and evaluating creative generation and
information distortion in scientific text simplification. Our solution
integrates multiple strategies: we construct an ensemble framework that
leverages BERT-based classifier, semantic similarity measure, natural language
inference model, and large language model (LLM) reasoning. These diverse
signals are combined using meta-classifiers to enhance the robustness of
spurious and distortion detection. Additionally, for grounded generation, we
employ an LLM-based post-editing system that revises simplifications based on
the original input texts.

</details>


### [219] [A Survey of Idiom Datasets for Psycholinguistic and Computational Research](https://arxiv.org/abs/2508.11828)
*Michael Flor,Xinyi Liu,Anna Feldman*

Main category: cs.CL

TL;DR: Survey of 53 idiom datasets from psycholinguistics and computational linguistics, analyzing their content, annotation practices, and applications, finding a disconnect between the two research fields.


<details>
  <summary>Details</summary>
Motivation: Idioms are challenging for both computational processing and human experimental studies due to their figurative nature, requiring specialized datasets for research.

Method: Comprehensive review and analysis of 53 existing datasets, examining their content, form, annotation practices, coverage, and intended use cases across both psycholinguistics and computational linguistics.

Result: Found that psycholinguistic resources focus on normed ratings (familiarity, transparency, compositionality) while computational datasets support tasks like idiomaticity detection, paraphrasing, and cross-lingual modeling. Recent efforts expanded language coverage and task diversity, but no connection exists between the two research domains.

Conclusion: Despite progress in dataset development, there remains a significant gap between psycholinguistic and computational research on idioms that needs to be bridged for more comprehensive understanding and processing of figurative language.

Abstract: Idioms are figurative expressions whose meanings often cannot be inferred
from their individual words, making them difficult to process computationally
and posing challenges for human experimental studies. This survey reviews
datasets developed in psycholinguistics and computational linguistics for
studying idioms, focusing on their content, form, and intended use.
Psycholinguistic resources typically contain normed ratings along dimensions
such as familiarity, transparency, and compositionality, while computational
datasets support tasks like idiomaticity detection/classification,
paraphrasing, and cross-lingual modeling. We present trends in annotation
practices, coverage, and task framing across 53 datasets. Although recent
efforts expanded language coverage and task diversity, there seems to be no
relation yet between psycholinguistic and computational research on idioms.

</details>


### [220] [Every 28 Days the AI Dreams of Soft Skin and Burning Stars: Scaffolding AI Agents with Hormones and Emotions](https://arxiv.org/abs/2508.11829)
*Leigh Levinson,Christopher J. Agostino*

Main category: cs.CL

TL;DR: AI systems embed simulated biological rhythms (menstrual/circadian cycles) to address the frame problem, showing performance variations and emotional patterns that track hormonal phases.


<details>
  <summary>Details</summary>
Motivation: Address the fundamental frame problem in AI systems by leveraging biological rhythms as natural relevance filters to determine contextual information from large possibility spaces.

Method: Develop a framework embedding simulated menstrual and circadian cycles into Large Language Models through system prompts generated from periodic functions modeling key hormones (estrogen, testosterone, cortisol).

Result: Linguistic analysis reveals emotional/stylistic variations tracking biological phases; benchmark testing shows subtle but consistent performance variations aligning with biological expectations, with optimal function in moderate hormonal ranges.

Conclusion: The methodology provides a novel approach to contextual AI while revealing how societal biases regarding gender and biology are embedded within language models.

Abstract: Despite significant advances, AI systems struggle with the frame problem:
determining what information is contextually relevant from an exponentially
large possibility space. We hypothesize that biological rhythms, particularly
hormonal cycles, serve as natural relevance filters that could address this
fundamental challenge. We develop a framework that embeds simulated menstrual
and circadian cycles into Large Language Models through system prompts
generated from periodic functions modeling key hormones including estrogen,
testosterone, and cortisol. Across multiple state-of-the-art models, linguistic
analysis reveals emotional and stylistic variations that track biological
phases; sadness peaks during menstruation while happiness dominates ovulation
and circadian patterns show morning optimism transitioning to nocturnal
introspection. Benchmarking on SQuAD, MMLU, Hellaswag, and AI2-ARC demonstrates
subtle but consistent performance variations aligning with biological
expectations, including optimal function in moderate rather than extreme
hormonal ranges. This methodology provides a novel approach to contextual AI
while revealing how societal biases regarding gender and biology are embedded
within language models.

</details>


### [221] [When Does Language Transfer Help? Sequential Fine-Tuning for Cross-Lingual Euphemism Detection](https://arxiv.org/abs/2508.11831)
*Julia Sammartino,Libby Barak,Jing Peng,Anna Feldman*

Main category: cs.CL

TL;DR: Cross-lingual sequential fine-tuning improves euphemism detection, especially for low-resource languages like Yoruba and Turkish, with XLM-R showing larger gains but more sensitivity to pretraining gaps compared to mBERT.


<details>
  <summary>Details</summary>
Motivation: Euphemisms are culturally variable and ambiguous, posing challenges for language models, particularly in low-resource settings where data is scarce.

Method: Investigates cross-lingual transfer via sequential fine-tuning for euphemism detection across five languages (English, Spanish, Chinese, Turkish, Yoruba), comparing sequential fine-tuning with monolingual and simultaneous fine-tuning using XLM-R and mBERT models.

Result: Sequential fine-tuning with a high-resource L1 improves L2 performance, especially for low-resource languages. XLM-R achieves larger gains but is more sensitive to pretraining gaps and catastrophic forgetting, while mBERT yields more stable though lower results.

Conclusion: Sequential fine-tuning is a simple yet effective strategy for improving euphemism detection in multilingual models, particularly beneficial for low-resource languages.

Abstract: Euphemisms are culturally variable and often ambiguous, posing challenges for
language models, especially in low-resource settings. This paper investigates
how cross-lingual transfer via sequential fine-tuning affects euphemism
detection across five languages: English, Spanish, Chinese, Turkish, and
Yoruba. We compare sequential fine-tuning with monolingual and simultaneous
fine-tuning using XLM-R and mBERT, analyzing how performance is shaped by
language pairings, typological features, and pretraining coverage. Results show
that sequential fine-tuning with a high-resource L1 improves L2 performance,
especially for low-resource languages like Yoruba and Turkish. XLM-R achieves
larger gains but is more sensitive to pretraining gaps and catastrophic
forgetting, while mBERT yields more stable, though lower, results. These
findings highlight sequential fine-tuning as a simple yet effective strategy
for improving euphemism detection in multilingual models, particularly when
low-resource languages are involved.

</details>


### [222] [SupraTok: Cross-Boundary Tokenization for Enhanced Language Model Performance](https://arxiv.org/abs/2508.11857)
*Andrei-Valentin Tănase,Elena Pelican*

Main category: cs.CL

TL;DR: SupraTok is a novel tokenization architecture that improves tokenization efficiency by 31% over leading tokenizers while maintaining competitive performance across 38 languages, and yields 8.4-9.5% improvements on benchmarks when integrated with GPT-2 scale models.


<details>
  <summary>Details</summary>
Motivation: Tokenization remains a fundamental yet underexplored bottleneck in NLP, with strategies largely static despite remarkable progress in model architectures.

Method: SupraTok extends Byte-Pair Encoding with three innovations: cross-boundary pattern learning for multi-word semantic units, entropy-driven data curation for optimal training corpus quality, and multi-phase curriculum learning for stable convergence. It learns "superword" tokens that preserve semantic unity while maximizing compression efficiency.

Result: Achieves 31% improvement in English tokenization efficiency (5.91 vs 4.51 characters per token) compared to OpenAI's o200k and 30% over Google's Gemma 3 tokenizer. When integrated with GPT-2 scale model (124M parameters), yields 8.4% improvement on HellaSWAG and 9.5% on MMLU benchmarks without architectural modifications.

Conclusion: Efficient tokenization can complement architectural innovations as a path to improved language model performance, though further validation at larger model scales is needed.

Abstract: Tokenization remains a fundamental yet underexplored bottleneck in natural
language processing, with strategies largely static despite remarkable progress
in model architectures. We present SupraTok, a novel tokenization architecture
that reimagines subword segmentation through three innovations: cross-boundary
pattern learning that discovers multi-word semantic units, entropy-driven data
curation that optimizes training corpus quality, and multi-phase curriculum
learning for stable convergence. Our approach extends Byte-Pair Encoding by
learning "superword" tokens, coherent multi-word expressions that preserve
semantic unity while maximizing compression efficiency. SupraTok achieves 31%
improvement in English tokenization efficiency (5.91 versus 4.51 characters per
token) compared to OpenAI's o200k tokenizer and 30% improvement over Google's
Gemma 3 tokenizer (256k vocabulary), while maintaining competitive performance
across 38 languages. When integrated with a GPT-2 scale model (124M parameters)
trained on 10 billion tokens from the FineWeb-Edu dataset, SupraTok yields 8.4%
improvement on HellaSWAG and 9.5% on MMLU benchmarks without architectural
modifications. While these results are promising at this scale, further
validation at larger model scales is needed. These findings suggest that
efficient tokenization can complement architectural innovations as a path to
improved language model performance.

</details>


### [223] [In-Context Examples Matter: Improving Emotion Recognition in Conversation with Instruction Tuning](https://arxiv.org/abs/2508.11889)
*Hui Ma,Bo Zhang,Jinpeng Hu,Zenglin Shi*

Main category: cs.CL

TL;DR: InitERC is a one-stage in-context instruction tuning framework for emotion recognition in conversation that jointly captures speaker characteristics and contextual cues through unified learning.


<details>
  <summary>Details</summary>
Motivation: Existing multi-stage instruction tuning methods for emotion recognition in conversation fail to jointly capture dynamic interactions between speaker characteristics and conversational context, leading to weak alignment among speaker identity, contextual cues, and emotion states.

Method: InitERC uses a one-stage in-context instruction tuning framework with four components: demonstration pool construction, in-context example selection, prompt template design, and in-context instruction tuning. It explores retrieval strategies, example ordering, and number of examples.

Result: Extensive experiments on three widely used datasets show that InitERC achieves substantial improvements over state-of-the-art baselines.

Conclusion: The proposed one-stage in-context instruction tuning framework effectively learns speaker-context-emotion alignment and outperforms existing multi-stage approaches.

Abstract: Emotion recognition in conversation (ERC) aims to identify the emotion of
each utterance in a conversation, playing a vital role in empathetic artificial
intelligence. With the growing of large language models (LLMs), instruction
tuning has emerged as a critical paradigm for ERC. Existing studies mainly
focus on multi-stage instruction tuning, which first endows LLMs with speaker
characteristics, and then conducts context-aware instruction tuning to
comprehend emotional states. However, these methods inherently constrains the
capacity to jointly capture the dynamic interaction between speaker
characteristics and conversational context, resulting in weak alignment among
speaker identity, contextual cues, and emotion states within a unified
framework. In this paper, we propose InitERC, a simple yet effective one-stage
in-context instruction tuning framework for ERC. InitERC adapts LLMs to learn
speaker-context-emotion alignment from context examples via in-context
instruction tuning. Specifically, InitERC comprises four components, i.e.,
demonstration pool construction, in-context example selection, prompt template
design, and in-context instruction tuning. To explore the impact of in-context
examples, we conduct a comprehensive study on three key factors: retrieval
strategy, example ordering, and the number of examples. Extensive experiments
on three widely used datasets demonstrate that our proposed InitERC achieves
substantial improvements over the state-of-the-art baselines.

</details>


### [224] [CORE: Measuring Multi-Agent LLM Interaction Quality under Game-Theoretic Pressures](https://arxiv.org/abs/2508.11915)
*Punya Syon Pandey,Yongjin Yang,Jiarui Liu,Zhijing Jin*

Main category: cs.CL

TL;DR: CORE metric evaluates linguistic robustness in LLM multi-agent systems across game-theoretic settings, revealing cooperative interactions show more repetition with vocabulary expansion while competitive ones have constrained vocabularies.


<details>
  <summary>Details</summary>
Motivation: Linguistic diversity in LLM game-theoretic interactions hasn't been sufficiently quantified, requiring a metric to measure language effectiveness across different multi-agent scenarios.

Method: Developed CORE metric integrating cluster entropy, lexical repetition, and semantic similarity. Applied to pairwise LLM dialogs in competitive, cooperative, and neutral settings, analyzed through Zipf's and Heaps' Laws.

Result: Cooperative settings show steeper Zipf distributions and higher Heap exponents (more repetition with vocabulary expansion). Competitive interactions display lower exponents (less repetition, constrained vocabularies).

Conclusion: Social incentives significantly influence language adaptation, and CORE serves as a robust diagnostic tool for measuring linguistic robustness in multi-agent LLM systems.

Abstract: Game-theoretic interactions between agents with Large Language Models (LLMs)
have revealed many emergent capabilities, yet the linguistic diversity of these
interactions has not been sufficiently quantified. In this paper, we present
the Conversational Robustness Evaluation Score: CORE, a metric to quantify the
effectiveness of language use within multi-agent systems across different
game-theoretic interactions. CORE integrates measures of cluster entropy,
lexical repetition, and semantic similarity, providing a direct lens of dialog
quality. We apply CORE to pairwise LLM dialogs across competitive, cooperative,
and neutral settings, further grounding our analysis in Zipf's and Heaps' Laws
to characterize word frequency distributions and vocabulary growth. Our
findings show that cooperative settings exhibit both steeper Zipf distributions
and higher Heap exponents, indicating more repetition alongside greater
vocabulary expansion. In contrast, competitive interactions display lower Zipf
and Heaps exponents, reflecting less repetition and more constrained
vocabularies. These results provide new insights into how social incentives
influence language adaptation, and highlight CORE as a robust diagnostic for
measuring linguistic robustness in multi-agent LLM systems. Our code is
available at https://github.com/psyonp/core.

</details>


### [225] [LLMs Struggle with NLI for Perfect Aspect: A Cross-Linguistic Study in Chinese and Japanese](https://arxiv.org/abs/2508.11927)
*Jie Lu,Du Jin,Hitomi Yanaka*

Main category: cs.CL

TL;DR: Chinese and Japanese lack distinct grammatical forms for tense within perfect aspect, causing challenges for Natural Language Inference. A template-based dataset was created, revealing LLMs struggle with temporal inference and subtle tense shifts.


<details>
  <summary>Details</summary>
Motivation: Unlike English which has clear tense markers for perfect aspect (had, has, will have), Chinese and Japanese lack separate grammatical forms for tense within perfect aspect, complicating Natural Language Inference tasks.

Method: Constructed a linguistically motivated, template-based NLI dataset with 1,350 pairs per language (Chinese and Japanese) focusing on perfect aspect.

Result: Experiments show even advanced LLMs struggle with temporal inference, particularly in detecting subtle tense and reference-time shifts.

Conclusion: Findings highlight model limitations and underscore the need for cross-linguistic evaluation in temporal semantics. Dataset is publicly available.

Abstract: Unlike English, which uses distinct forms (e.g., had, has, will have) to mark
the perfect aspect across tenses, Chinese and Japanese lack separate
grammatical forms for tense within the perfect aspect, which complicates
Natural Language Inference (NLI). Focusing on the perfect aspect in these
languages, we construct a linguistically motivated, template-based NLI dataset
(1,350 pairs per language). Experiments reveal that even advanced LLMs struggle
with temporal inference, particularly in detecting subtle tense and
reference-time shifts. These findings highlight model limitations and
underscore the need for cross-linguistic evaluation in temporal semantics. Our
dataset is available at https://github.com/Lujie2001/CrossNLI.

</details>


### [226] [CAMF: Collaborative Adversarial Multi-agent Framework for Machine Generated Text Detection](https://arxiv.org/abs/2508.11933)
*Yue Wang,Liesheng Wei,Yuxiang Wang*

Main category: cs.CL

TL;DR: CAMF is a novel multi-agent framework that detects machine-generated text by analyzing cross-dimensional linguistic inconsistencies through collaborative adversarial agents, outperforming existing zero-shot detection methods.


<details>
  <summary>Details</summary>
Motivation: Existing zero-shot machine-generated text detection methods have significant deficiencies, including superficial analyses focused on limited textual attributes and lack of investigation into consistency across linguistic dimensions like style, semantics, and logic.

Method: CAMF uses multiple LLM-based agents in a three-phase process: Multi-dimensional Linguistic Feature Extraction, Adversarial Consistency Probing, and Synthesized Judgment Aggregation to analyze subtle cross-dimensional textual incongruities.

Result: Empirical evaluations demonstrate CAMF's significant superiority over state-of-the-art zero-shot MGT detection techniques.

Conclusion: The collaborative adversarial multi-agent framework provides a more effective approach for detecting machine-generated text by deeply analyzing linguistic inconsistencies across multiple dimensions.

Abstract: Detecting machine-generated text (MGT) from contemporary Large Language
Models (LLMs) is increasingly crucial amid risks like disinformation and
threats to academic integrity. Existing zero-shot detection paradigms, despite
their practicality, often exhibit significant deficiencies. Key challenges
include: (1) superficial analyses focused on limited textual attributes, and
(2) a lack of investigation into consistency across linguistic dimensions such
as style, semantics, and logic. To address these challenges, we introduce the
\textbf{C}ollaborative \textbf{A}dversarial \textbf{M}ulti-agent
\textbf{F}ramework (\textbf{CAMF}), a novel architecture using multiple
LLM-based agents. CAMF employs specialized agents in a synergistic three-phase
process: \emph{Multi-dimensional Linguistic Feature Extraction},
\emph{Adversarial Consistency Probing}, and \emph{Synthesized Judgment
Aggregation}. This structured collaborative-adversarial process enables a deep
analysis of subtle, cross-dimensional textual incongruities indicative of
non-human origin. Empirical evaluations demonstrate CAMF's significant
superiority over state-of-the-art zero-shot MGT detection techniques.

</details>


### [227] [Learning Wisdom from Errors: Promoting LLM's Continual Relation Learning through Exploiting Error Cases](https://arxiv.org/abs/2508.12031)
*Shaozhe Yin,Jinyu Guo,Kai Shuang,Xia Liu,Ruize Ou*

Main category: cs.CL

TL;DR: Instruction-based continual contrastive tuning approach for LLMs in continual relation extraction that specializes in exploiting error cases to correct cognitive biases and achieve state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Existing CRE methods using memory replay and contrastive learning don't effectively address error cases that reveal model's cognitive biases, limiting their ability to mitigate catastrophic forgetting.

Method: Splits training and memory data based on correctness of initial responses, uses dual-task fine-tuning, and employs instruction-based contrastive tuning strategy to continuously correct cognitive biases with previous data guidance.

Result: Achieves new state-of-the-art performance on TACRED and FewRel datasets with significant improvements, demonstrating effectiveness of specializing in error case exploitation.

Conclusion: Focusing on error cases through instruction-based continual contrastive tuning effectively mitigates catastrophic forgetting and improves CRE performance for LLMs, showing the importance of addressing cognitive biases.

Abstract: Continual Relation Extraction (CRE) aims to continually learn new emerging
relations while avoiding catastrophic forgetting. Existing CRE methods mainly
use memory replay and contrastive learning to mitigate catastrophic forgetting.
However, these methods do not attach importance to the error cases that can
reveal the model's cognitive biases more effectively. To address this issue, we
propose an instruction-based continual contrastive tuning approach for Large
Language Models (LLMs) in CRE. Different from existing CRE methods that
typically handle the training and memory data in a unified manner, this
approach splits the training and memory data of each task into two parts
respectively based on the correctness of the initial responses and treats them
differently through dual-task fine-tuning. In addition, leveraging the
advantages of LLM's instruction-following ability, we propose a novel
instruction-based contrastive tuning strategy for LLM to continuously correct
current cognitive biases with the guidance of previous data in an
instruction-tuning manner, which mitigates the gap between old and new
relations in a more suitable way for LLMs. We experimentally evaluate our model
on TACRED and FewRel, and the results show that our model achieves new
state-of-the-art CRE performance with significant improvements, demonstrating
the importance of specializing in exploiting error cases.

</details>


### [228] [Mind the Generation Process: Fine-Grained Confidence Estimation During LLM Generation](https://arxiv.org/abs/2508.12040)
*Jinyi Han,Tingyun Li,Shisong Chen,Jie Shi,Xinyi Wang,Guanglei Yue,Jiaqing Liang,Xin Lin,Liqian Wen,Zulong Chen,Yanghua Xiao*

Main category: cs.CL

TL;DR: FineCE is a novel confidence estimation method that provides fine-grained, continuous confidence scores during LLM text generation, outperforming existing methods through supervised training on probabilistic response distributions and backward confidence integration.


<details>
  <summary>Details</summary>
Motivation: LLMs lack self-awareness and exhibit overconfidence, assigning high confidence to incorrect predictions. Existing approaches have coarse-grained scoring mechanisms that fail to provide fine-grained confidence estimates throughout the generation process.

Method: Developed a pipeline for constructing training data capturing LLM response distributions, trained a supervised model to predict confidence scores, proposed Backward Confidence Integration (BCI) strategy using subsequent text information, and introduced three strategies for optimal confidence estimation positions.

Result: Extensive experiments on multiple benchmark datasets demonstrate that FineCE consistently outperforms existing classical confidence estimation methods.

Conclusion: FineCE provides accurate, fine-grained confidence estimation during text generation, enhancing the trustworthiness and reliability of LLM outputs through improved confidence scoring mechanisms.

Abstract: While large language models (LLMs) have demonstrated remarkable performance
across diverse tasks, they fundamentally lack self-awareness and frequently
exhibit overconfidence, assigning high confidence scores to incorrect
predictions. Accurate confidence estimation is therefore critical for enhancing
the trustworthiness and reliability of LLM-generated outputs. However, existing
approaches suffer from coarse-grained scoring mechanisms that fail to provide
fine-grained, continuous confidence estimates throughout the generation
process. To address these limitations, we introduce FineCE, a novel confidence
estimation method that delivers accurate, fine-grained confidence scores during
text generation. Specifically, we first develop a comprehensive pipeline for
constructing training data that effectively captures the underlying
probabilistic distribution of LLM responses, and then train a model to predict
confidence scores for arbitrary text sequences in a supervised manner.
Furthermore, we propose a Backward Confidence Integration (BCI) strategy that
leverages information from the subsequent text to enhance confidence estimation
for the current sequence during inference. We also introduce three strategies
for identifying optimal positions to perform confidence estimation within the
generation process. Extensive experiments on multiple benchmark datasets
demonstrate that FineCE consistently outperforms existing classical confidence
estimation methods. Our code and all baselines used in the paper are available
on GitHub.

</details>


### [229] [J6: Jacobian-Driven Role Attribution for Multi-Objective Prompt Optimization in LLMs](https://arxiv.org/abs/2508.12086)
*Yao Wu*

Main category: cs.CL

TL;DR: J6 is a Jacobian-based method that decomposes gradient interactions into six components for multi-objective LLM adaptation, enabling both hard and soft optimization strategies while providing interpretable insights into parameter attribution and task interference.


<details>
  <summary>Details</summary>
Motivation: Existing multi-objective optimization strategies for LLM adaptation rely on scalar gradient aggregation, ignoring the geometric structure between objectives and parameters, which poses challenges when balancing conflicting objectives like factuality improvement and confidence increase.

Method: Proposes J6, a structured Jacobian-based method that decomposes the gradient interaction matrix into six interpretable components, enabling both hard decision-making (argmax) and soft strategies (softmax weighting) for dynamic update framework adaptation.

Result: The method provides interpretable structure for parameter attribution, task interference analysis, and geometry-aligned adaptation, forming a principled mechanism for conflict-aware prompt optimization.

Conclusion: J6 introduces a principled and extensible approach for multi-objective neural tuning that incorporates structured Jacobian reasoning, opening new avenues for conflict-aware optimization in LLM adaptation.

Abstract: In large language model (LLM) adaptation, balancing multiple optimization
objectives such as improving factuality (heat) and increasing confidence (via
low entropy) poses a fundamental challenge, especially when prompt parameters
(e.g., hidden-layer insertions h and embedding modifications w) interact in
non-trivial ways. Existing multi-objective optimization strategies often rely
on scalar gradient aggregation, ignoring the deeper geometric structure between
objectives and parameters. We propose J6, a structured Jacobian-based method
that decomposes the gradient interaction matrix into six interpretable
components. This decomposition enables both hard decision-making (e.g.,
choosing the dominant update direction via argmax) and soft strategies (e.g.,
attention-style weighting via softmax over J6), forming a dynamic update
framework that adapts to local conflict and synergy. Moreover, the
interpretable structure of J6 provides insight into parameter attribution, task
interference, and geometry-aligned adaptation. Our work introduces a principled
and extensible mechanism for conflict-aware prompt optimization, and opens a
new avenue for incorporating structured Jacobian reasoning into multi-objective
neural tuning.

</details>


### [230] [STEM: Efficient Relative Capability Evaluation of LLMs through Structured Transition Samples](https://arxiv.org/abs/2508.12096)
*Haiquan Hu,Jiazhi Jiang,Shiyou Xu,Ruhan Zeng,Tian Wang*

Main category: cs.CL

TL;DR: STEM is a lightweight evaluation framework that uses significant transition samples to efficiently estimate LLM capabilities without full benchmark testing.


<details>
  <summary>Details</summary>
Motivation: Standard LLM benchmarks are becoming less effective due to overfitting, high computational costs, and inability to distinguish meaningful differences between models as capabilities advance rapidly.

Method: STEM identifies significant transition samples (STS) by analyzing performance transitions among LLMs of the same architecture but varying parameter scales, then uses these samples to estimate capability positions of unknown models.

Result: Experimental results show STEM reliably captures performance trends and aligns with ground-truth rankings of model capability across six diverse benchmarks using the Qwen3 model family.

Conclusion: STEM provides a practical, scalable, and architecture-agnostic method for fine-grained evaluation of LLMs, addressing current benchmarking limitations.

Abstract: Evaluating large language models (LLMs) has become increasingly challenging
as model capabilities advance rapidly. While recent models often achieve higher
scores on standard benchmarks, these improvements do not consistently reflect
enhanced real-world reasoning capabilities. Moreover, widespread overfitting to
public benchmarks and the high computational cost of full evaluations have made
it both expensive and less effective to distinguish meaningful differences
between models. To address these challenges, we propose the \textbf{S}tructured
\textbf{T}ransition \textbf{E}valuation \textbf{M}ethod (STEM), a lightweight
and interpretable evaluation framework for efficiently estimating the relative
capabilities of LLMs. STEM identifies \textit{significant transition samples}
(STS) by analyzing consistent performance transitions among LLMs of the same
architecture but varying parameter scales. These samples enable STEM to
effectively estimate the capability position of an unknown model. Qwen3 model
family is applied to construct the STS pool on six diverse and representative
benchmarks. To assess generalizability. Experimental results indicate that STEM
reliably captures performance trends, aligns with ground-truth rankings of
model capability. These findings highlight STEM as a practical and scalable
method for fine-grained, architecture-agnostic evaluation of LLMs.

</details>


### [231] [Exploring Efficiency Frontiers of Thinking Budget in Medical Reasoning: Scaling Laws between Computational Resources and Reasoning Quality](https://arxiv.org/abs/2508.12140)
*Ziqian Bi,Lu Chen,Junhao Song,Hongying Luo,Enze Ge,Junmin Huang,Tianyang Wang,Keyu Chen,Chia Xin Liang,Zihan Wei,Huafeng Liu,Chunjie Tian,Jibin Guan,Joe Yeong,Yongzhi Xu,Peng Wang,Junfeng Hao*

Main category: cs.CL

TL;DR: First comprehensive evaluation of thinking budget mechanisms in medical AI, showing logarithmic scaling between computational resources and reasoning quality across 15 medical datasets and multiple model sizes.


<details>
  <summary>Details</summary>
Motivation: To establish fundamental scaling laws between computational thinking budgets and reasoning quality in medical AI systems, enabling optimized resource allocation for clinical applications.

Method: Systematic evaluation of Qwen3 (1.7B-235B) and DeepSeek-R1 (1.5B-70B) models across 15 medical datasets with controlled thinking budgets from zero to unlimited tokens.

Result: Identified three efficiency regimes: high-efficiency (0-256 tokens), balanced (256-512 tokens), and high-accuracy (>512 tokens). Smaller models showed 15-20% improvement with extended thinking vs 5-10% for larger models. Domain-specific patterns emerged with neurology/gastroenterology requiring deeper reasoning.

Conclusion: Thinking budget control is critical for optimizing medical AI systems, enabling dynamic resource allocation aligned with clinical needs while maintaining transparency essential for healthcare deployment.

Abstract: This study presents the first comprehensive evaluation of thinking budget
mechanisms in medical reasoning tasks, revealing fundamental scaling laws
between computational resources and reasoning quality. We systematically
evaluated two major model families, Qwen3 (1.7B to 235B parameters) and
DeepSeek-R1 (1.5B to 70B parameters), across 15 medical datasets spanning
diverse specialties and difficulty levels. Through controlled experiments with
thinking budgets ranging from zero to unlimited tokens, we establish
logarithmic scaling relationships where accuracy improvements follow a
predictable pattern with both thinking budget and model size. Our findings
identify three distinct efficiency regimes: high-efficiency (0 to 256 tokens)
suitable for real-time applications, balanced (256 to 512 tokens) offering
optimal cost-performance tradeoffs for routine clinical support, and
high-accuracy (above 512 tokens) justified only for critical diagnostic tasks.
Notably, smaller models demonstrate disproportionately larger benefits from
extended thinking, with 15 to 20% improvements compared to 5 to 10% for larger
models, suggesting a complementary relationship where thinking budget provides
greater relative benefits for capacity-constrained models. Domain-specific
patterns emerge clearly, with neurology and gastroenterology requiring
significantly deeper reasoning processes than cardiovascular or respiratory
medicine. The consistency between Qwen3 native thinking budget API and our
proposed truncation method for DeepSeek-R1 validates the generalizability of
thinking budget concepts across architectures. These results establish thinking
budget control as a critical mechanism for optimizing medical AI systems,
enabling dynamic resource allocation aligned with clinical needs while
maintaining the transparency essential for healthcare deployment.

</details>


### [232] [LLM-as-a-Judge for Privacy Evaluation? Exploring the Alignment of Human and LLM Perceptions of Privacy in Textual Data](https://arxiv.org/abs/2508.12158)
*Stephen Meisenbacher,Alexandra Klymenko,Florian Matthes*

Main category: cs.CL

TL;DR: LLMs can effectively model human privacy perspectives for text evaluation, showing promise as privacy evaluators despite low inter-human agreement on privacy sensitivity.


<details>
  <summary>Details</summary>
Motivation: Privacy evaluation in NLP remains challenging due to its subjective nature, and LLMs have shown success in other evaluation tasks, suggesting potential for privacy assessment.

Method: Conducted study with 10 datasets, 13 LLMs, and 677 human participants to compare LLM and human privacy evaluations of textual data using the LLM-as-a-Judge paradigm.

Result: LLMs can accurately model global human privacy perspectives, though privacy is difficult to measure empirically with generally low inter-human agreement rates.

Conclusion: LLMs show promise as privacy evaluators for textual data, paving the way for addressing privacy challenges with innovative technical solutions.

Abstract: Despite advances in the field of privacy-preserving Natural Language
Processing (NLP), a significant challenge remains the accurate evaluation of
privacy. As a potential solution, using LLMs as a privacy evaluator presents a
promising approach $\unicode{x2013}$ a strategy inspired by its success in
other subfields of NLP. In particular, the so-called $\textit{LLM-as-a-Judge}$
paradigm has achieved impressive results on a variety of natural language
evaluation tasks, demonstrating high agreement rates with human annotators.
Recognizing that privacy is both subjective and difficult to define, we
investigate whether LLM-as-a-Judge can also be leveraged to evaluate the
privacy sensitivity of textual data. Furthermore, we measure how closely LLM
evaluations align with human perceptions of privacy in text. Resulting from a
study involving 10 datasets, 13 LLMs, and 677 human survey participants, we
confirm that privacy is indeed a difficult concept to measure empirically,
exhibited by generally low inter-human agreement rates. Nevertheless, we find
that LLMs can accurately model a global human privacy perspective, and through
an analysis of human and LLM reasoning patterns, we discuss the merits and
limitations of LLM-as-a-Judge for privacy evaluation in textual data. Our
findings pave the way for exploring the feasibility of LLMs as privacy
evaluators, addressing a core challenge in solving pressing privacy issues with
innovative technical solutions.

</details>


### [233] [Arabic Multimodal Machine Learning: Datasets, Applications, Approaches, and Challenges](https://arxiv.org/abs/2508.12227)
*Abdelhamid Haouhat,Slimane Bellaouar,Attia Nehar,Hadda Cherroun,Ahmed Abdelali*

Main category: cs.CL

TL;DR: Comprehensive survey paper on Arabic Multimodal Machine Learning that categorizes research through a novel taxonomy covering datasets, applications, approaches, and challenges.


<details>
  <summary>Details</summary>
Motivation: Arabic MML has reached foundational maturity, making it timely to conduct a comprehensive survey to structure the field and identify research gaps.

Method: Develops a novel taxonomy organizing Arabic MML research into four key topics: datasets, applications, approaches, and challenges, providing a structured analysis of existing literature.

Result: Provides a comprehensive overview of current Arabic MML state, identifies unexplored areas and critical research gaps, and offers insights for future research directions.

Conclusion: This survey empowers researchers to build upon identified opportunities and address challenges to advance Arabic MML field development.

Abstract: Multimodal Machine Learning (MML) aims to integrate and analyze information
from diverse modalities, such as text, audio, and visuals, enabling machines to
address complex tasks like sentiment analysis, emotion recognition, and
multimedia retrieval. Recently, Arabic MML has reached a certain level of
maturity in its foundational development, making it time to conduct a
comprehensive survey. This paper explores Arabic MML by categorizing efforts
through a novel taxonomy and analyzing existing research. Our taxonomy
organizes these efforts into four key topics: datasets, applications,
approaches, and challenges. By providing a structured overview, this survey
offers insights into the current state of Arabic MML, highlighting areas that
have not been investigated and critical research gaps. Researchers will be
empowered to build upon the identified opportunities and address challenges to
advance the field.

</details>


### [234] [SEA-BED: Southeast Asia Embedding Benchmark](https://arxiv.org/abs/2508.12243)
*Wuttikorn Ponwitayarat,Raymond Ng,Jann Railey Montalan,Thura Aung,Jian Gang Ngui,Yosephine Susanto,William Tjhi,Panuthep Tasawong,Erik Cambria,Ekapol Chuangsuwanich,Sarana Nutanong,Peerat Limkonchotiwat*

Main category: cs.CL

TL;DR: SEA-BED is the first large-scale Southeast Asian embedding benchmark with 169 human-formulated datasets across 9 tasks and 10 languages, revealing significant performance gaps and ranking shifts for SEA languages compared to global benchmarks.


<details>
  <summary>Details</summary>
Motivation: Southeast Asia has nearly 700 million speakers but lacks region-specific embedding benchmarks, with existing datasets often machine-translated and missing native linguistic properties.

Method: Created SEA-BED benchmark with 169 datasets (71% human-formulated) across 9 tasks and 10 SEA languages, then evaluated 17 embedding models through six studies analyzing task challenges, cross-benchmark comparisons, and translation effects.

Result: Results show sharp ranking shifts, inconsistent model performance among SEA languages, and demonstrate the critical importance of human-curated datasets for low-resource languages like Burmese.

Conclusion: Human-curated benchmarks are essential for accurate evaluation of SEA languages, as machine-translated datasets fail to capture linguistic nuances, leading to significant performance measurement discrepancies.

Abstract: Sentence embeddings are essential for NLP tasks such as semantic search,
re-ranking, and textual similarity. Although multilingual benchmarks like MMTEB
broaden coverage, Southeast Asia (SEA) datasets are scarce and often
machine-translated, missing native linguistic properties. With nearly 700
million speakers, the SEA region lacks a region-specific embedding benchmark.
We introduce SEA-BED, the first large-scale SEA embedding benchmark with 169
datasets across 9 tasks and 10 languages, where 71% are formulated by humans,
not machine generation or translation. We address three research questions: (1)
which SEA languages and tasks are challenging, (2) whether SEA languages show
unique performance gaps globally, and (3) how human vs. machine translations
affect evaluation. We evaluate 17 embedding models across six studies,
analyzing task and language challenges, cross-benchmark comparisons, and
translation trade-offs. Results show sharp ranking shifts, inconsistent model
performance among SEA languages, and the importance of human-curated datasets
for low-resource languages like Burmese.

</details>


### [235] [What do Speech Foundation Models Learn? Analysis and Applications](https://arxiv.org/abs/2508.12255)
*Ankita Pasad*

Main category: cs.CL

TL;DR: This thesis analyzes speech foundation models (SFMs) through statistical analysis and introduces new spoken language understanding tasks (NER/NEL) to evaluate SFM performance, showing end-to-end models can outperform traditional cascaded approaches.


<details>
  <summary>Details</summary>
Motivation: Despite the proliferation of speech foundation models, there's limited understanding of what knowledge they acquire and how they perform on complex spoken language understanding tasks beyond basic speech recognition.

Method: Developed a lightweight analysis framework using statistical tools and training-free tasks to investigate acoustic/linguistic knowledge in SFM layers. Created spoken NER and NEL tasks for SLU evaluation and developed SFM-based approaches for these tasks.

Result: Comparative study across multiple SFMs revealed analytical insights with concrete implications for downstream performance. End-to-end models leveraging SFMs surpassed traditional cascaded approaches for spoken NER and NEL tasks.

Conclusion: The thesis provides tools and datasets to better understand SFMs and enables informed design choices for future model development, addressing previously unanswered questions about SFM capabilities and knowledge representation.

Abstract: Speech foundation models (SFMs) are designed to serve as general-purpose
representations for a wide range of speech-processing tasks. The last five
years have seen an influx of increasingly successful self-supervised and
supervised pre-trained models with impressive performance on various downstream
tasks.
  Although the zoo of SFMs continues to grow, our understanding of the
knowledge they acquire lags behind. This thesis presents a lightweight analysis
framework using statistical tools and training-free tasks to investigate the
acoustic and linguistic knowledge encoded in SFM layers. We conduct a
comparative study across multiple SFMs and statistical tools. Our study also
shows that the analytical insights have concrete implications for downstream
task performance.
  The effectiveness of an SFM is ultimately determined by its performance on
speech applications. Yet it remains unclear whether the benefits extend to
spoken language understanding (SLU) tasks that require a deeper understanding
than widely studied ones, such as speech recognition. The limited exploration
of SLU is primarily due to a lack of relevant datasets. To alleviate that, this
thesis contributes tasks, specifically spoken named entity recognition (NER)
and named entity localization (NEL), to the Spoken Language Understanding
Evaluation benchmark. We develop SFM-based approaches for NER and NEL, and find
that end-to-end (E2E) models leveraging SFMs can surpass traditional cascaded
(speech recognition followed by a text model) approaches. Further, we evaluate
E2E SLU models across SFMs and adaptation strategies to assess the impact on
task performance.
  Collectively, this thesis tackles previously unanswered questions about SFMs,
providing tools and datasets to further our understanding and to enable the
community to make informed design choices for future model development and
adoption.

</details>


### [236] [Structuring the Unstructured: A Systematic Review of Text-to-Structure Generation for Agentic AI with a Universal Evaluation Framework](https://arxiv.org/abs/2508.12257)
*Zheye Deng,Chunkit Chan,Tianshi Zheng,Wei Fan,Weiqi Wang,Yangqiu Song*

Main category: cs.CL

TL;DR: Systematic review of text-to-structure conversion techniques, evaluating methodologies, datasets, metrics, and proposing a universal evaluation framework for structured outputs.


<details>
  <summary>Details</summary>
Motivation: AI systems are evolving toward agentic operation and context-aware retrieval, requiring transformation of unstructured text into structured formats like tables, knowledge graphs, and charts for applications from summarization to data mining, but current research lacks comprehensive synthesis.

Method: Systematic review examining text-to-structure techniques, challenges, current datasets, assessment criteria, and outlining future research directions.

Result: The review provides comprehensive synthesis of methodologies and introduces a universal evaluation framework for structured outputs.

Conclusion: Text-to-structure conversion is established as foundational infrastructure for next-generation AI systems, with proposed evaluation framework and future research directions.

Abstract: The evolution of AI systems toward agentic operation and context-aware
retrieval necessitates transforming unstructured text into structured formats
like tables, knowledge graphs, and charts. While such conversions enable
critical applications from summarization to data mining, current research lacks
a comprehensive synthesis of methodologies, datasets, and metrics. This
systematic review examines text-to-structure techniques and the encountered
challenges, evaluates current datasets and assessment criteria, and outlines
potential directions for future research. We also introduce a universal
evaluation framework for structured outputs, establishing text-to-structure as
foundational infrastructure for next-generation AI systems.

</details>


### [237] [Fast, Slow, and Tool-augmented Thinking for LLMs: A Review](https://arxiv.org/abs/2508.12265)
*Xinda Jia,Jinpeng Li,Zezhong Wang,Jingjing Li,Xingshan Zeng,Yasheng Wang,Weinan Zhang,Yong Yu,Weiwen Liu*

Main category: cs.CL

TL;DR: A taxonomy of LLM reasoning strategies based on cognitive psychology principles, categorizing approaches along fast/slow and internal/external knowledge boundaries.


<details>
  <summary>Details</summary>
Motivation: Real-world reasoning requires adapting strategies to problem demands, from intuitive responses to deliberate step-by-step reasoning and tool-augmented thinking.

Method: Proposed taxonomy with two dimensions: fast/slow boundary (intuitive vs deliberative) and internal/external boundary (parameter-based vs tool-augmented reasoning), followed by systematic survey of adaptive reasoning methods.

Result: A comprehensive categorization framework for LLM reasoning strategies that helps organize and understand different adaptive reasoning approaches.

Conclusion: Highlights open challenges and future directions for developing more adaptive, efficient, and reliable LLMs through better reasoning strategy selection.

Abstract: Large Language Models (LLMs) have demonstrated remarkable progress in
reasoning across diverse domains. However, effective reasoning in real-world
tasks requires adapting the reasoning strategy to the demands of the problem,
ranging from fast, intuitive responses to deliberate, step-by-step reasoning
and tool-augmented thinking. Drawing inspiration from cognitive psychology, we
propose a novel taxonomy of LLM reasoning strategies along two knowledge
boundaries: a fast/slow boundary separating intuitive from deliberative
processes, and an internal/external boundary distinguishing reasoning grounded
in the model's parameters from reasoning augmented by external tools. We
systematically survey recent work on adaptive reasoning in LLMs and categorize
methods based on key decision factors. We conclude by highlighting open
challenges and future directions toward more adaptive, efficient, and reliable
LLMs.

</details>


### [238] [The Self-Execution Benchmark: Measuring LLMs' Attempts to Overcome Their Lack of Self-Execution](https://arxiv.org/abs/2508.12277)
*Elon Ezra,Ariel Weizman,Amos Azaria*

Main category: cs.CL

TL;DR: LLMs perform poorly at predicting their own response properties like difficulty assessment, refusal likelihood, and output associations, showing no improvement with increased model size or capability.


<details>
  <summary>Details</summary>
Motivation: To evaluate whether LLMs can predict aspects of their own responses, moving beyond traditional knowledge and reasoning assessments to understand self-awareness capabilities.

Method: Introducing the Self-Execution Benchmark that tests models' ability to anticipate properties of their output, including difficulty prediction, refusal behavior, and association patterns.

Result: Models generally perform poorly on self-prediction tasks, and increased model size or capability does not consistently lead to better performance on these self-awareness metrics.

Conclusion: LLMs have fundamental limitations in how they represent and reason about their own behavior, suggesting current architectures lack genuine self-awareness capabilities.

Abstract: Large language models (LLMs) are commonly evaluated on tasks that test their
knowledge or reasoning abilities. In this paper, we explore a different type of
evaluation: whether an LLM can predict aspects of its own responses. Since LLMs
lack the ability to execute themselves, we introduce the Self-Execution
Benchmark, which measures a model's ability to anticipate properties of its
output, such as whether a question will be difficult for it, whether it will
refuse to answer, or what kinds of associations it is likely to produce. Our
experiments show that models generally perform poorly on this benchmark, and
that increased model size or capability does not consistently lead to better
performance. These results suggest a fundamental limitation in how LLMs
represent and reason about their own behavior.

</details>


### [239] [Legal$Δ$: Enhancing Legal Reasoning in LLMs via Reinforcement Learning with Chain-of-Thought Guided Information Gain](https://arxiv.org/abs/2508.12281)
*Xin Dai,Buqiang Xu,Zhenghao Liu,Yukun Yan,Huiyuan Xie,Xiaoyuan Yi,Shuo Wang,Ge Yu*

Main category: cs.CL

TL;DR: LegalΔ is a reinforcement learning framework that enhances legal reasoning in LLMs by maximizing information gain between direct answers and reasoning-augmented outputs, producing more reliable and interpretable legal judgments.


<details>
  <summary>Details</summary>
Motivation: Existing legal LLMs struggle with reliable and interpretable reasoning processes, often defaulting to fast-thinking behavior without explicit multi-step reasoning, which limits effectiveness in complex legal scenarios requiring rigorous justification.

Method: Two-stage approach: (1) distills latent reasoning capabilities from DeepSeek-R1 (Large Reasoning Model), (2) refines reasoning quality via differential comparisons with a dual-mode input setup (direct answer vs reasoning-augmented) and multidimensional reward mechanism assessing structural coherence and legal-domain specificity.

Result: Outperforms strong baselines on multiple legal reasoning tasks in both accuracy and interpretability, consistently producing more robust and trustworthy legal judgments without relying on labeled preference data.

Conclusion: LegalΔ successfully addresses the interpretability challenge in legal AI by encouraging meaningful reasoning patterns rather than superficial explanations, demonstrating significant improvements in legal reasoning quality and reliability.

Abstract: Legal Artificial Intelligence (LegalAI) has achieved notable advances in
automating judicial decision-making with the support of Large Language Models
(LLMs). However, existing legal LLMs still struggle to generate reliable and
interpretable reasoning processes. They often default to fast-thinking behavior
by producing direct answers without explicit multi-step reasoning, limiting
their effectiveness in complex legal scenarios that demand rigorous
justification. To address this challenge, we propose Legal$\Delta$, a
reinforcement learning framework designed to enhance legal reasoning through
chain-of-thought guided information gain. During training, Legal$\Delta$
employs a dual-mode input setup-comprising direct answer and
reasoning-augmented modes-and maximizes the information gain between them. This
encourages the model to acquire meaningful reasoning patterns rather than
generating superficial or redundant explanations. Legal$\Delta$ follows a
two-stage approach: (1) distilling latent reasoning capabilities from a
powerful Large Reasoning Model (LRM), DeepSeek-R1, and (2) refining reasoning
quality via differential comparisons, combined with a multidimensional reward
mechanism that assesses both structural coherence and legal-domain specificity.
Experimental results on multiple legal reasoning tasks demonstrate that
Legal$\Delta$ outperforms strong baselines in both accuracy and
interpretability. It consistently produces more robust and trustworthy legal
judgments without relying on labeled preference data. All code and data will be
released at https://github.com/NEUIR/LegalDelta.

</details>


### [240] [A Question Answering Dataset for Temporal-Sensitive Retrieval-Augmented Generation](https://arxiv.org/abs/2508.12282)
*Ziyang Chen,Erxue Min,Xiang Zhao,Yunxin Li,Xin Jia,Jinzhi Liao,Jichao Li,Shuaiqiang Wang,Baotian Hu,Dawei Yin*

Main category: cs.CL

TL;DR: ChronoQA is a large-scale Chinese QA benchmark dataset for evaluating temporal reasoning in RAG systems, built from 300k+ news articles (2019-2024) with 5,176 high-quality questions covering various temporal types and scenarios.


<details>
  <summary>Details</summary>
Motivation: To address the need for evaluating temporal reasoning capabilities in Retrieval-Augmented Generation systems, particularly for Chinese language, where existing benchmarks may not adequately test time-sensitive question answering.

Method: Constructed from over 300,000 news articles (2019-2024), featuring 5,176 questions covering absolute, aggregate, and relative temporal types with explicit/implicit time expressions. Supports single- and multi-document scenarios with comprehensive structural annotations and multi-stage validation (rule-based, LLM-based, human evaluation).

Result: Created a dynamic, reliable, and scalable benchmark dataset that enables structured evaluation across various temporal tasks in Chinese question answering.

Conclusion: ChronoQA serves as a robust benchmark for advancing time-sensitive retrieval-augmented question answering systems, providing a comprehensive resource for evaluating temporal reasoning in RAG applications.

Abstract: We introduce ChronoQA, a large-scale benchmark dataset for Chinese question
answering, specifically designed to evaluate temporal reasoning in
Retrieval-Augmented Generation (RAG) systems. ChronoQA is constructed from over
300,000 news articles published between 2019 and 2024, and contains 5,176
high-quality questions covering absolute, aggregate, and relative temporal
types with both explicit and implicit time expressions. The dataset supports
both single- and multi-document scenarios, reflecting the real-world
requirements for temporal alignment and logical consistency. ChronoQA features
comprehensive structural annotations and has undergone multi-stage validation,
including rule-based, LLM-based, and human evaluation, to ensure data quality.
By providing a dynamic, reliable, and scalable resource, ChronoQA enables
structured evaluation across a wide range of temporal tasks, and serves as a
robust benchmark for advancing time-sensitive retrieval-augmented question
answering systems.

</details>


### [241] [Incorporating Legal Logic into Deep Learning: An Intelligent Approach to Probation Prediction](https://arxiv.org/abs/2508.12286)
*Qinghua Wang,Xu Zhang,Lingyan Yang,Rui Shao,Bonan Wang,Fang Wang,Cunquan Qu*

Main category: cs.CL

TL;DR: Proposes MT-DT model integrating legal logic with deep learning for probation prediction, outperforming baseline methods by incorporating legal principles and remorse analysis.


<details>
  <summary>Details</summary>
Motivation: Current Intelligent Judicial Assistant Systems lack dedicated probation prediction methods and overlook legal logic, focusing too much on data-driven approaches without considering the legal reasoning behind judicial decisions.

Method: Three-stage approach: 1) Construct specialized probation dataset with fact descriptions and probation legal elements; 2) Design MT-DT model based on legal logic and Dual-Track Theory of Punishment; 3) Experimental validation on probation dataset.

Result: MT-DT model outperforms baseline models, and legal logic analysis validates the effectiveness of the integrated approach.

Conclusion: Integrating legal logic with deep learning significantly improves probation prediction accuracy and provides more legally sound judicial assistance, addressing the gap between data-driven methods and legal reasoning.

Abstract: Probation is a crucial institution in modern criminal law, embodying the
principles of fairness and justice while contributing to the harmonious
development of society. Despite its importance, the current Intelligent
Judicial Assistant System (IJAS) lacks dedicated methods for probation
prediction, and research on the underlying factors influencing probation
eligibility remains limited. In addition, probation eligibility requires a
comprehensive analysis of both criminal circumstances and remorse. Much of the
existing research in IJAS relies primarily on data-driven methodologies, which
often overlooks the legal logic underpinning judicial decision-making. To
address this gap, we propose a novel approach that integrates legal logic into
deep learning models for probation prediction, implemented in three distinct
stages. First, we construct a specialized probation dataset that includes fact
descriptions and probation legal elements (PLEs). Second, we design a distinct
probation prediction model named the Multi-Task Dual-Theory Probation
Prediction Model (MT-DT), which is grounded in the legal logic of probation and
the \textit{Dual-Track Theory of Punishment}. Finally, our experiments on the
probation dataset demonstrate that the MT-DT model outperforms baseline models,
and an analysis of the underlying legal logic further validates the
effectiveness of the proposed approach.

</details>


### [242] [CarelessWhisper: Turning Whisper into a Causal Streaming Model](https://arxiv.org/abs/2508.12301)
*Tomer Krichli,Bhiksha Raj,Joseph Keshet*

Main category: cs.CL

TL;DR: Proposes a method to convert transformer encoder-decoder ASR models into low-latency streaming models using causal encoder fine-tuning with LoRA and weakly aligned data, achieving better performance than existing streaming approaches with lower complexity.


<details>
  <summary>Details</summary>
Motivation: Current state-of-the-art ASR models like Whisper and Canary are designed for offline transcription and cannot be easily adapted for real-time streaming due to architectural limitations and non-causal nature.

Method: Modify non-causal encoder to causal encoder by fine-tuning both encoder and decoder using Low-Rank Adaptation (LoRA) with weakly aligned dataset, then implement updated inference mechanism for greedy and beam-search decoding.

Result: Outperforms existing non-fine-tuned streaming approaches on low-latency chunk sizes (<300 msec) with lower complexity, and enables better alignment for word-level timestamp extraction.

Conclusion: Successfully transforms transformer encoder-decoder models into efficient streaming ASR systems through causal fine-tuning, providing a practical solution for real-time transcription with improved performance and additional timestamp capabilities.

Abstract: Automatic Speech Recognition (ASR) has seen remarkable progress, with models
like OpenAI Whisper and NVIDIA Canary achieving state-of-the-art (SOTA)
performance in offline transcription. However, these models are not designed
for streaming (online or real-time) transcription, due to limitations in their
architecture and training methodology. We propose a method to turn the
transformer encoder-decoder model into a low-latency streaming model that is
careless about future context. We present an analysis explaining why it is not
straightforward to convert an encoder-decoder transformer to a low-latency
streaming model. Our proposed method modifies the existing (non-causal) encoder
to a causal encoder by fine-tuning both the encoder and decoder using Low-Rank
Adaptation (LoRA) and a weakly aligned dataset. We then propose an updated
inference mechanism that utilizes the fine-tune causal encoder and decoder to
yield greedy and beam-search decoding, and is shown to be locally optimal.
Experiments on low-latency chunk sizes (less than 300 msec) show that our
fine-tuned model outperforms existing non-fine-tuned streaming approaches in
most cases, while using a lower complexity. Additionally, we observe that our
training process yields better alignment, enabling a simple method for
extracting word-level timestamps. We release our training and inference code,
along with the fine-tuned models, to support further research and development
in streaming ASR.

</details>


### [243] [Consensus or Conflict? Fine-Grained Evaluation of Conflicting Answers in Question-Answering](https://arxiv.org/abs/2508.12355)
*Eviatar Nachshoni,Arie Cattan,Shmuel Amar,Ori Shapira,Ido Dagan*

Main category: cs.CL

TL;DR: A new benchmark NATCONFQA for multi-answer question answering with conflict detection, created using cost-effective methodology from fact-checking datasets, showing LLMs struggle with conflicting answers.


<details>
  <summary>Details</summary>
Motivation: Multi-Answer Question Answering (MAQA) with conflicting answers remains challenging, and existing benchmarks have limitations like synthetic data, yes/no restrictions, or unverified automated annotation.

Method: Extended conflict-aware MAQA setting requiring models to identify valid answers and detect conflicting pairs. Developed cost-effective methodology using fact-checking datasets to construct NATCONFQA benchmark with detailed conflict labels.

Result: Evaluation of eight high-end LLMs revealed their fragility in handling various conflict types and flawed strategies for resolving them.

Conclusion: The NATCONFQA benchmark provides a realistic testbed for conflict-aware MAQA, highlighting significant challenges in LLM performance that need to be addressed.

Abstract: Large Language Models (LLMs) have demonstrated strong performance in question
answering (QA) tasks. However, Multi-Answer Question Answering (MAQA), where a
question may have several valid answers, remains challenging. Traditional QA
settings often assume consistency across evidences, but MAQA can involve
conflicting answers. Constructing datasets that reflect such conflicts is
costly and labor-intensive, while existing benchmarks often rely on synthetic
data, restrict the task to yes/no questions, or apply unverified automated
annotation. To advance research in this area, we extend the conflict-aware MAQA
setting to require models not only to identify all valid answers, but also to
detect specific conflicting answer pairs, if any. To support this task, we
introduce a novel cost-effective methodology for leveraging fact-checking
datasets to construct NATCONFQA, a new benchmark for realistic, conflict-aware
MAQA, enriched with detailed conflict labels, for all answer pairs. We evaluate
eight high-end LLMs on NATCONFQA, revealing their fragility in handling various
types of conflicts and the flawed strategies they employ to resolve them.

</details>


### [244] [ReaLM: Reflection-Enhanced Autonomous Reasoning with Small Language Models](https://arxiv.org/abs/2508.12387)
*Yuanfeng Xu,Zehui Dai,Jian Liang,Jiapeng Guan,Guangrun Wang,Liang Lin,Xiaohui Lv*

Main category: cs.CL

TL;DR: ReaLM is a reinforcement learning framework that enhances small language models' reasoning through multi-path verification, gradual autonomy induction, and domain knowledge distillation.


<details>
  <summary>Details</summary>
Motivation: Small language models struggle with complex reasoning due to limited capacity, error-prone multi-step reasoning, and over-reliance on external guidance, while existing methods sacrifice reasoning capability, autonomy, or generalization.

Method: Uses Multi-Route Process Verification to contrast positive/negative reasoning paths, Enabling Autonomy via Asymptotic Induction to gradually fade external signals, and guided chain-of-thought distillation to encode domain knowledge.

Result: Extensive experiments show ReaLM significantly improves SLM performance in reasoning capability, autonomy, and generalization across vertical and general reasoning tasks.

Conclusion: The framework successfully addresses key limitations of small language models by enhancing reasoning through multi-path learning, promoting self-sufficiency, and improving generalization with domain knowledge encoding.

Abstract: Small Language Models (SLMs) are a cost-effective alternative to Large
Language Models (LLMs), but often struggle with complex reasoning due to their
limited capacity and a tendency to produce mistakes or inconsistent answers
during multi-step reasoning. Existing efforts have improved SLM performance,
but typically at the cost of one or more of three key aspects: (1) reasoning
capability, due to biased supervision that filters out negative reasoning paths
and limits learning from errors; (2) autonomy, due to over-reliance on
externally generated reasoning signals; and (3) generalization, which suffers
when models overfit to teacher-specific patterns. In this paper, we introduce
ReaLM, a reinforcement learning framework for robust and self-sufficient
reasoning in vertical domains. To enhance reasoning capability, we propose
Multi-Route Process Verification (MRPV), which contrasts both positive and
negative reasoning paths to extract decisive patterns. To reduce reliance on
external guidance and improve autonomy, we introduce Enabling Autonomy via
Asymptotic Induction (EAAI), a training strategy that gradually fades external
signals. To improve generalization, we apply guided chain-of-thought
distillation to encode domain-specific rules and expert knowledge into SLM
parameters, making them part of what the model has learned. Extensive
experiments on both vertical and general reasoning tasks demonstrate that ReaLM
significantly improves SLM performance across aspects (1)-(3) above.

</details>


### [245] [MedKGent: A Large Language Model Agent Framework for Constructing Temporally Evolving Medical Knowledge Graph](https://arxiv.org/abs/2508.12393)
*Duzhen Zhang,Zixiao Wang,Zhong-Zhi Li,Yahan Yu,Shuncheng Jia,Jiahua Dong,Haotian Xu,Xing Wu,Yingying Zhang,Tielin Zhang,Jie Yang,Xiuying Chen,Le Song*

Main category: cs.CL

TL;DR: MedKGent is an LLM agent framework that constructs temporally evolving medical knowledge graphs from PubMed abstracts, achieving 90% accuracy and demonstrating significant improvements in medical question answering benchmarks.


<details>
  <summary>Details</summary>
Motivation: The rapid expansion of medical literature creates challenges for structuring domain knowledge. Current KG construction methods lack generalizability, ignore temporal dynamics, and treat biomedical corpora as static without addressing contextual uncertainty of evolving knowledge.

Method: Uses two specialized agents (Extractor and Constructor) powered by Qwen2.5-32B-Instruct model. Processes over 10 million PubMed abstracts (1975-2023) in day-by-day manner. Extractor identifies knowledge triples with confidence scores via sampling-based estimation. Constructor incrementally integrates triples into temporally evolving graph using confidence scores and timestamps.

Result: Constructed KG contains 156,275 entities and 2,971,384 relational triples. Quality assessments show ~90% accuracy with strong inter-rater agreement. RAG evaluation across 7 medical QA benchmarks with 5 leading LLMs shows significant improvements over non-augmented baselines.

Conclusion: MedKGent successfully addresses limitations of current KG construction methods by incorporating temporal dynamics and confidence-aware processing, demonstrating strong performance in medical knowledge representation and downstream applications like drug repurposing.

Abstract: The rapid expansion of medical literature presents growing challenges for
structuring and integrating domain knowledge at scale. Knowledge Graphs (KGs)
offer a promising solution by enabling efficient retrieval, automated
reasoning, and knowledge discovery. However, current KG construction methods
often rely on supervised pipelines with limited generalizability or naively
aggregate outputs from Large Language Models (LLMs), treating biomedical
corpora as static and ignoring the temporal dynamics and contextual uncertainty
of evolving knowledge. To address these limitations, we introduce MedKGent, a
LLM agent framework for constructing temporally evolving medical KGs.
Leveraging over 10 million PubMed abstracts published between 1975 and 2023, we
simulate the emergence of biomedical knowledge via a fine-grained daily time
series. MedKGent incrementally builds the KG in a day-by-day manner using two
specialized agents powered by the Qwen2.5-32B-Instruct model. The Extractor
Agent identifies knowledge triples and assigns confidence scores via
sampling-based estimation, which are used to filter low-confidence extractions
and inform downstream processing. The Constructor Agent incrementally
integrates the retained triples into a temporally evolving graph, guided by
confidence scores and timestamps to reinforce recurring knowledge and resolve
conflicts. The resulting KG contains 156,275 entities and 2,971,384 relational
triples. Quality assessments by two SOTA LLMs and three domain experts
demonstrate an accuracy approaching 90\%, with strong inter-rater agreement. To
evaluate downstream utility, we conduct RAG across seven medical question
answering benchmarks using five leading LLMs, consistently observing
significant improvements over non-augmented baselines. Case studies further
demonstrate the KG's value in literature-based drug repurposing via
confidence-aware causal inference.

</details>


### [246] [Extracting Post-Acute Sequelae of SARS-CoV-2 Infection Symptoms from Clinical Notes via Hybrid Natural Language Processing](https://arxiv.org/abs/2508.12405)
*Zilong Bai,Zihan Xu,Cong Sun,Chengxi Zang,H. Timothy Bunnell,Catherine Sinfield,Jacqueline Rutter,Aaron Thomas Martinez,L. Charles Bailey,Mark Weiner,Thomas R. Campion,Thomas Carton,Christopher B. Forrest,Rainu Kaushal,Fei Wang,Yifan Peng*

Main category: cs.CL

TL;DR: Hybrid NLP pipeline combining rule-based NER with BERT-based assertion detection for efficient PASC symptom extraction from clinical notes, achieving high accuracy and processing speed.


<details>
  <summary>Details</summary>
Motivation: PASC diagnosis is challenging due to evolving symptoms over variable time intervals, requiring efficient extraction and analysis from clinical notes.

Method: Developed comprehensive PASC lexicon with specialists, created hybrid NLP pipeline integrating rule-based named entity recognition with BERT-based assertion detection modules, using 160 notes for development and 47,654 notes for population study.

Result: Achieved F1 score of 0.82 (internal) and 0.76 (external validation), processed notes in 2.448±0.812 seconds each, with strong Spearman correlations (ρ>0.83 positive, ρ>0.72 negative mentions, both p<0.0001).

Conclusion: The pipeline demonstrates effectiveness and efficiency for PASC symptom extraction, showing potential to improve PASC diagnosis through automated clinical note analysis.

Abstract: Accurately and efficiently diagnosing Post-Acute Sequelae of COVID-19 (PASC)
remains challenging due to its myriad symptoms that evolve over long- and
variable-time intervals. To address this issue, we developed a hybrid natural
language processing pipeline that integrates rule-based named entity
recognition with BERT-based assertion detection modules for PASC-symptom
extraction and assertion detection from clinical notes. We developed a
comprehensive PASC lexicon with clinical specialists. From 11 health systems of
the RECOVER initiative network across the U.S., we curated 160 intake progress
notes for model development and evaluation, and collected 47,654 progress notes
for a population-level prevalence study. We achieved an average F1 score of
0.82 in one-site internal validation and 0.76 in 10-site external validation
for assertion detection. Our pipeline processed each note at $2.448\pm 0.812$
seconds on average. Spearman correlation tests showed $\rho >0.83$ for positive
mentions and $\rho >0.72$ for negative ones, both with $P <0.0001$. These
demonstrate the effectiveness and efficiency of our models and their potential
for improving PASC diagnosis.

</details>


### [247] [ZigzagAttention: Efficient Long-Context Inference with Exclusive Retrieval and Streaming Heads](https://arxiv.org/abs/2508.12407)
*Zhuorui Liu,Chen Zhang,Dawei Song*

Main category: cs.CL

TL;DR: ZigzagAttention reduces KV cache memory by categorizing attention heads into retrieval and streaming heads, but improves latency by grouping same-type heads in layers to avoid extra computation overhead.


<details>
  <summary>Details</summary>
Motivation: Large language models face deployment challenges due to KV cache memory consumption from long contexts. Existing methods categorize heads but create latency issues from split attention computations.

Method: Improved head identification that enforces exclusively retrieval or streaming heads in each layer, eliminating extra tensor access latency while maintaining performance.

Result: Reduced latency with negligible performance degradation, making ZigzagAttention competitive among baselines for long-context handling.

Conclusion: Grouping same-type attention heads by layer effectively reduces KV cache overhead without the latency penalties of mixed-head layers.

Abstract: With the rapid development of large language models (LLMs), handling long
context has become one of the vital abilities in LLMs. Such long-context
ability is accompanied by difficulties in deployment, especially due to the
increased consumption of KV cache. There is certain work aiming to optimize the
memory footprint of KV cache, inspired by the observation that attention heads
can be categorized into retrieval heads that are of great significance and
streaming heads that are of less significance. Typically, identifying the
streaming heads and and waiving the KV cache in the streaming heads would
largely reduce the overhead without hurting the performance that much. However,
since employing both retrieval and streaming heads in one layer decomposes one
large round of attention computation into two small ones, it may unexpectedly
bring extra latency on accessing and indexing tensors. Based on this intuition,
we impose an important improvement to the identification process of retrieval
and streaming heads, in which we design a criterion that enforces exclusively
retrieval or streaming heads gathered in one unique layer. In this way, we
further eliminate the extra latency and only incur negligible performance
degradation. Our method named \textsc{ZigzagAttention} is competitive among
considered baselines owing to reduced latency and comparable performance.

</details>


### [248] [The Cultural Gene of Large Language Models: A Study on the Impact of Cross-Corpus Training on Model Values and Biases](https://arxiv.org/abs/2508.12411)
*Emanuel Z. Fenech-Borg,Tilen P. Meznaric-Kos,Milica D. Lekovic-Bojovic,Arni J. Hentze-Djurhuus*

Main category: cs.CL

TL;DR: LLMs inherit cultural biases from training data, with GPT-4 showing Western individualistic/low-power-distance values and ERNIE Bot showing Eastern collectivistic/high-power-distance values, demonstrating they function as statistical mirrors of their cultural corpora.


<details>
  <summary>Details</summary>
Motivation: To investigate the underlying cultural and ethical assumptions in large language models that are deployed globally, as these models may inherit systematic value orientations from their training corpora that remain underexplored.

Method: Created a Cultural Probe Dataset (CPD) of 200 prompts targeting Individualism-Collectivism and Power Distance dimensions. Used standardized zero-shot prompts to compare GPT-4 (Western-centric) and ERNIE Bot (Eastern-centric), with human annotation and statistical analysis including Cultural Alignment Index against Hofstede's national scores.

Result: Significant divergence found: GPT-4 shows individualistic/low-power-distance tendencies (IDV ~1.21; PDI ~-1.05), ERNIE Bot shows collectivistic/high-power-distance tendencies (IDV ~-0.89; PDI ~0.76). GPT-4 aligns more with USA cultural scores, ERNIE Bot aligns more with China. Differences statistically significant (p < 0.001).

Conclusion: LLMs function as statistical mirrors of their cultural training corpora, highlighting the need for culturally aware evaluation and deployment to prevent algorithmic cultural hegemony.

Abstract: Large language models (LLMs) are deployed globally, yet their underlying
cultural and ethical assumptions remain underexplored. We propose the notion of
a "cultural gene" -- a systematic value orientation that LLMs inherit from
their training corpora -- and introduce a Cultural Probe Dataset (CPD) of 200
prompts targeting two classic cross-cultural dimensions:
Individualism-Collectivism (IDV) and Power Distance (PDI). Using standardized
zero-shot prompts, we compare a Western-centric model (GPT-4) and an
Eastern-centric model (ERNIE Bot). Human annotation shows significant and
consistent divergence across both dimensions. GPT-4 exhibits individualistic
and low-power-distance tendencies (IDV score approx 1.21; PDI score approx
-1.05), while ERNIE Bot shows collectivistic and higher-power-distance
tendencies (IDV approx -0.89; PDI approx 0.76); differences are statistically
significant (p < 0.001). We further compute a Cultural Alignment Index (CAI)
against Hofstede's national scores and find GPT-4 aligns more closely with the
USA (e.g., IDV CAI approx 0.91; PDI CAI approx 0.88) whereas ERNIE Bot aligns
more closely with China (IDV CAI approx 0.85; PDI CAI approx 0.81). Qualitative
analyses of dilemma resolution and authority-related judgments illustrate how
these orientations surface in reasoning. Our results support the view that LLMs
function as statistical mirrors of their cultural corpora and motivate
culturally aware evaluation and deployment to avoid algorithmic cultural
hegemony.

</details>


### [249] [Uncovering Emergent Physics Representations Learned In-Context by Large Language Models](https://arxiv.org/abs/2508.12448)
*Yeongwoo Song,Jaeyong Bae,Dong-Kyum Kim,Hawoong Jeong*

Main category: cs.CL

TL;DR: LLMs demonstrate in-context learning of physics concepts through dynamics forecasting tasks, with performance improving with longer contexts. Sparse autoencoders reveal that learned features correlate with physical variables like energy.


<details>
  <summary>Details</summary>
Motivation: To understand the internal mechanisms that enable LLMs to perform successful in-context learning across diverse tasks, using physics as a tractable testbed with real-world structured dynamics.

Method: Used dynamics forecasting tasks in physical systems to evaluate ICL, analyzed residual stream activations with sparse autoencoders (SAEs), and examined correlation between learned features and physical variables.

Result: Performance improves with longer input contexts. SAE features correlate with key physical variables like energy, showing meaningful physical concepts are encoded during in-context learning.

Conclusion: Physics tasks provide valuable insights into LLM reasoning behaviors, demonstrating that LLMs can learn and encode meaningful physical concepts through in-context learning mechanisms.

Abstract: Large language models (LLMs) exhibit impressive in-context learning (ICL)
abilities, enabling them to solve wide range of tasks via textual prompts
alone. As these capabilities advance, the range of applicable domains continues
to expand significantly. However, identifying the precise mechanisms or
internal structures within LLMs that allow successful ICL across diverse,
distinct classes of tasks remains elusive. Physics-based tasks offer a
promising testbed for probing this challenge. Unlike synthetic sequences such
as basic arithmetic or symbolic equations, physical systems provide
experimentally controllable, real-world data based on structured dynamics
grounded in fundamental principles. This makes them particularly suitable for
studying the emergent reasoning behaviors of LLMs in a realistic yet tractable
setting. Here, we mechanistically investigate the ICL ability of LLMs,
especially focusing on their ability to reason about physics. Using a dynamics
forecasting task in physical systems as a proxy, we evaluate whether LLMs can
learn physics in context. We first show that the performance of dynamics
forecasting in context improves with longer input contexts. To uncover how such
capability emerges in LLMs, we analyze the model's residual stream activations
using sparse autoencoders (SAEs). Our experiments reveal that the features
captured by SAEs correlate with key physical variables, such as energy. These
findings demonstrate that meaningful physical concepts are encoded within LLMs
during in-context learning. In sum, our work provides a novel case study that
broadens our understanding of how LLMs learn in context.

</details>


### [250] [M3PO: Multimodal-Model-Guided Preference Optimization for Visual Instruction Following](https://arxiv.org/abs/2508.12458)
*Ruirui Gao,Emily Johnson,Bowen Tan,Yanfei Qian*

Main category: cs.CL

TL;DR: M3PO is a novel multimodal preference optimization method that automatically selects high-quality preference pairs from LVLM-generated candidates using a combined M3P-Score, enabling efficient DPO fine-tuning without expensive human annotation.


<details>
  <summary>Details</summary>
Motivation: High cost and inconsistency of human annotation hinder LVLM development, and existing methods struggle to efficiently identify informative hard negative samples from the model's own generation space.

Method: Uses Multimodal Alignment Score (MAS) for external quality assessment and model's self-consistency/log-probability for internal belief, combined into M3P-Score to select optimal preference pairs for DPO fine-tuning with LoRA.

Result: Consistently outperforms SFT, simulated RLHF, vanilla DPO, and RM-DPO across multiple benchmarks including MME-Bench, POPE, IFT, and Human Preference Score.

Conclusion: M3PO provides a data-efficient solution for enhancing LVLM capabilities in visual instruction following by intelligently leveraging the model's own generation space to identify challenging preference pairs.

Abstract: Large Vision-Language Models (LVLMs) hold immense potential for complex
multimodal instruction following, yet their development is often hindered by
the high cost and inconsistency of human annotation required for effective
fine-tuning and preference alignment. Traditional supervised fine-tuning (SFT)
and existing preference optimization methods like RLHF and DPO frequently
struggle to efficiently leverage the model's own generation space to identify
highly informative "hard negative" samples. To address these challenges, we
propose Multimodal-Model-Guided Preference Optimization (M3PO), a novel and
data-efficient method designed to enhance LVLMs' capabilities in visual
instruction following. M3PO intelligently selects the most "learning-valuable"
preference sample pairs from a diverse pool of LVLM-generated candidates. This
selection is driven by a sophisticated mechanism that integrates two crucial
signals: a Multimodal Alignment Score (MAS) to assess external quality and the
model's Self-Consistency / Confidence (log-probability) to gauge internal
belief. These are combined into a novel M3P-Score, which specifically
identifies preferred responses and challenging dispreferred responses that the
model might confidently generate despite being incorrect. These high-quality
preference pairs are then used for efficient Direct Preference Optimization
(DPO) fine-tuning on base LVLMs like LLaVA-1.5 (7B/13B) using LoRA. Our
extensive experiments demonstrate that M3PO consistently outperforms strong
baselines, including SFT, simulated RLHF, vanilla DPO, and RM-DPO, across a
comprehensive suite of multimodal instruction following benchmarks (MME-Bench,
POPE, IFT, Human Pref. Score).

</details>


### [251] [LoraxBench: A Multitask, Multilingual Benchmark Suite for 20 Indonesian Languages](https://arxiv.org/abs/2508.12459)
*Alham Fikri Aji,Trevor Cohn*

Main category: cs.CL

TL;DR: LoraxBench is a new benchmark for low-resource Indonesian languages covering 6 tasks across 20 languages, revealing performance gaps between Indonesian and other languages, no clear advantage for region-specific models, and register sensitivity.


<details>
  <summary>Details</summary>
Motivation: Indonesia has 700 languages but lags in NLP progress, creating a need for comprehensive evaluation of low-resource Indonesian languages to advance NLP research in this linguistically diverse region.

Method: Created LoraxBench benchmark covering 6 diverse tasks (reading comprehension, open-domain QA, language inference, causal reasoning, translation, cultural QA) across 20 Indonesian languages, including two formality registers for three languages. Evaluated multilingual and region-focused LLMs.

Result: The benchmark proved challenging with visible performance discrepancies between Indonesian and other low-resource languages. No clear advantage for region-specific models over general multilingual models. Register changes significantly affected performance, especially with uncommon registers like high-politeness Krama Javanese.

Conclusion: LoraxBench highlights the challenges in Indonesian low-resource language NLP, reveals performance gaps, and demonstrates the importance of considering linguistic registers in model evaluation, providing a valuable resource for advancing NLP in this diverse linguistic landscape.

Abstract: As one of the world's most populous countries, with 700 languages spoken,
Indonesia is behind in terms of NLP progress. We introduce LoraxBench, a
benchmark that focuses on low-resource languages of Indonesia and covers 6
diverse tasks: reading comprehension, open-domain QA, language inference,
causal reasoning, translation, and cultural QA. Our dataset covers 20
languages, with the addition of two formality registers for three languages. We
evaluate a diverse set of multilingual and region-focused LLMs and found that
this benchmark is challenging. We note a visible discrepancy between
performance in Indonesian and other languages, especially the low-resource
ones. There is no clear lead when using a region-specific model as opposed to
the general multilingual model. Lastly, we show that a change in register
affects model performance, especially with registers not commonly found in
social media, such as high-level politeness `Krama' Javanese.

</details>


### [252] [Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI's Latest Open Source Models](https://arxiv.org/abs/2508.12461)
*Ziqian Bi,Keyu Chen,Chiung-Yi Tseng,Danyang Zhang,Tianyang Wang,Hongying Luo,Lu Chen,Junming Huang,Jibin Guan,Junfeng Hao,Junhao Song*

Main category: cs.CL

TL;DR: OpenAI's GPT-OSS models (20B and 120B) show that smaller sparse models can outperform larger ones, with 20B beating 120B on several benchmarks despite lower resource requirements.


<details>
  <summary>Details</summary>
Motivation: To evaluate OpenAI's first open-weight LLMs since GPT-2 and compare sparse vs dense architectures across multiple benchmarks to understand scaling efficiency.

Method: Tested both GPT-OSS variants against six contemporary open source LLMs (14.7B-235B parameters) across ten benchmarks covering general knowledge, math, code, multilingual, and conversational tasks using standardized inference with statistical validation.

Result: GPT-OSS-20B consistently outperformed GPT-OSS-120B on benchmarks like HumanEval and MMLU while using less memory and energy. Both models showed mid-tier performance overall with strengths in code generation and weaknesses in multilingual tasks.

Conclusion: Scaling sparse architectures doesn't yield proportional performance gains, highlighting the need for better optimization strategies and more efficient model selection for open source deployments.

Abstract: In August 2025, OpenAI released GPT-OSS models, its first open weight large
language models since GPT-2 in 2019, comprising two mixture of experts
architectures with 120B and 20B parameters. We evaluated both variants against
six contemporary open source large language models ranging from 14.7B to 235B
parameters, representing both dense and sparse designs, across ten benchmarks
covering general knowledge, mathematical reasoning, code generation,
multilingual understanding, and conversational ability. All models were tested
in unquantised form under standardised inference settings, with statistical
validation using McNemars test and effect size analysis. Results show that
gpt-oss-20B consistently outperforms gpt-oss-120B on several benchmarks, such
as HumanEval and MMLU, despite requiring substantially less memory and energy
per response. Both models demonstrate mid-tier overall performance within the
current open source landscape, with relative strength in code generation and
notable weaknesses in multilingual tasks. These findings provide empirical
evidence that scaling in sparse architectures may not yield proportional
performance gains, underscoring the need for further investigation into
optimisation strategies and informing more efficient model selection for future
open source deployments.

</details>


### [253] [The Structural Sources of Verb Meaning Revisited: Large Language Models Display Syntactic Bootstrapping](https://arxiv.org/abs/2508.12482)
*Xiaomeng Zhu,R. Thomas McCoy,Robert Frank*

Main category: cs.CL

TL;DR: Large language models exhibit syntactic bootstrapping behavior similar to human children, with verb representations degrading more when syntax is removed than when co-occurrence information is removed, particularly for mental verbs.


<details>
  <summary>Details</summary>
Motivation: To examine whether large language models exhibit syntactic bootstrapping behavior similar to human children in verb learning, and to test developmental hypotheses at scale through manipulating learning environments.

Method: Trained RoBERTa and GPT-2 on perturbed datasets where syntactic information was ablated, comparing effects on verb representations when syntactic cues vs. co-occurrence information were removed.

Result: Models' verb representation degraded more when syntactic cues were removed than when co-occurrence information was removed. Mental verbs were more negatively impacted than physical verbs. Noun representations were affected more by co-occurrence distortion than syntax distortion.

Conclusion: The study reinforces the important role of syntactic bootstrapping in verb learning and demonstrates the viability of testing developmental hypotheses through manipulating large language models' learning environments.

Abstract: Syntactic bootstrapping (Gleitman, 1990) is the hypothesis that children use
the syntactic environments in which a verb occurs to learn its meaning. In this
paper, we examine whether large language models exhibit a similar behavior. We
do this by training RoBERTa and GPT-2 on perturbed datasets where syntactic
information is ablated. Our results show that models' verb representation
degrades more when syntactic cues are removed than when co-occurrence
information is removed. Furthermore, the representation of mental verbs, for
which syntactic bootstrapping has been shown to be particularly crucial in
human verb learning, is more negatively impacted in such training regimes than
physical verbs. In contrast, models' representation of nouns is affected more
when co-occurrences are distorted than when syntax is distorted. In addition to
reinforcing the important role of syntactic bootstrapping in verb learning, our
results demonstrated the viability of testing developmental hypotheses on a
larger scale through manipulating the learning environments of large language
models.

</details>


### [254] [Mitigating Hallucinations in Large Language Models via Causal Reasoning](https://arxiv.org/abs/2508.12495)
*Yuangang Li,Yiqing Shen,Yi Nian,Jiechao Gao,Ziyi Wang,Chenxiao Yu,Shawn Li,Jie Wang,Xiyang Hu,Yue Zhao*

Main category: cs.CL

TL;DR: CDCR-SFT framework trains LLMs to explicitly construct causal DAGs and reason over them, achieving state-of-the-art 95.33% accuracy on CLADDER and reducing hallucinations by 10% on HaluEval.


<details>
  <summary>Details</summary>
Motivation: Existing LLM reasoning approaches operate at token level rather than modeling underlying causal relationships, lacking ability to represent conditional independencies or satisfy causal identification assumptions, leading to logically inconsistent hallucinations.

Method: Supervised fine-tuning framework that trains LLMs to construct variable-level directed acyclic graphs (DAGs) and perform reasoning over them, using a dataset of 25,368 samples with explicit causal DAGs and graph-based reasoning traces.

Result: Achieves 95.33% accuracy on CLADDER (surpassing human performance of 94.8%) and reduces hallucination on HaluEval with 10% improvements across four LLMs and eight tasks.

Conclusion: Explicit causal structure modeling in LLMs can effectively mitigate logical inconsistencies and hallucinations in LLM outputs.

Abstract: Large language models (LLMs) exhibit logically inconsistent hallucinations
that appear coherent yet violate reasoning principles, with recent research
suggesting an inverse relationship between causal reasoning capabilities and
such hallucinations. However, existing reasoning approaches in LLMs, such as
Chain-of-Thought (CoT) and its graph-based variants, operate at the linguistic
token level rather than modeling the underlying causal relationships between
variables, lacking the ability to represent conditional independencies or
satisfy causal identification assumptions. To bridge this gap, we introduce
causal-DAG construction and reasoning (CDCR-SFT), a supervised fine-tuning
framework that trains LLMs to explicitly construct variable-level directed
acyclic graph (DAG) and then perform reasoning over it. Moreover, we present a
dataset comprising 25,368 samples (CausalDR), where each sample includes an
input question, explicit causal DAG, graph-based reasoning trace, and validated
answer. Experiments on four LLMs across eight tasks show that CDCR-SFT improves
the causal reasoning capability with the state-of-the-art 95.33% accuracy on
CLADDER (surpassing human performance of 94.8% for the first time) and reduces
the hallucination on HaluEval with 10% improvements. It demonstrates that
explicit causal structure modeling in LLMs can effectively mitigate logical
inconsistencies in LLM outputs. Code is available at
https://github.com/MrLYG/CDCR-SFT.

</details>


### [255] [CorrSteer: Steering Improves Task Performance and Safety in LLMs through Correlation-based Sparse Autoencoder Feature Selection](https://arxiv.org/abs/2508.12535)
*Seonglae Cho,Zekun Wu,Adriano Koshiyama*

Main category: cs.CL

TL;DR: CorrSteer is a new method that uses correlation between sample correctness and SAE activations to automatically select relevant features for steering LLMs, improving performance on various tasks without needing contrastive datasets.


<details>
  <summary>Details</summary>
Motivation: Existing Sparse Autoencoder (SAE) methods for steering LLMs require contrastive datasets or large activation storage, limiting their practical effectiveness in downstream tasks.

Method: CorrSteer correlates sample correctness with SAE activations from generated tokens at inference time to select features, using only inference-time activations to avoid spurious correlations and obtaining steering coefficients from average activations.

Result: Improved performance on QA, bias mitigation, jailbreaking prevention, and reasoning benchmarks on Gemma 2 2B and LLaMA 3.1 8B, with +4.1% MMLU improvement and +22.9% HarmBench improvement using only 4000 samples.

Conclusion: Correlation-based selection is an effective and scalable approach for automated SAE steering across language model applications, with selected features showing semantically meaningful patterns aligned with task requirements.

Abstract: Sparse Autoencoders (SAEs) can extract interpretable features from large
language models (LLMs) without supervision. However, their effectiveness in
downstream steering tasks is limited by the requirement for contrastive
datasets or large activation storage. To address these limitations, we propose
CorrSteer, which selects features by correlating sample correctness with SAE
activations from generated tokens at inference time. This approach uses only
inference-time activations to extract more relevant features, thereby avoiding
spurious correlations. It also obtains steering coefficients from average
activations, automating the entire pipeline. Our method shows improved task
performance on QA, bias mitigation, jailbreaking prevention, and reasoning
benchmarks on Gemma 2 2B and LLaMA 3.1 8B, notably achieving a +4.1%
improvement in MMLU performance and a +22.9% improvement in HarmBench with only
4000 samples. Selected features demonstrate semantically meaningful patterns
aligned with each task's requirements, revealing the underlying capabilities
that drive performance. Our work establishes correlationbased selection as an
effective and scalable approach for automated SAE steering across language
model applications.

</details>


### [256] [Beyond Modality Limitations: A Unified MLLM Approach to Automated Speaking Assessment with Effective Curriculum Learning](https://arxiv.org/abs/2508.12591)
*Yu-Hsuan Fang,Tien-Hong Lo,Yao-Ting Sung,Berlin Chen*

Main category: cs.CL

TL;DR: MLLMs outperform traditional ASA systems in content and language assessment but struggle with delivery evaluation. The proposed SFMT training strategy improves delivery assessment by 4% accuracy using curriculum learning.


<details>
  <summary>Details</summary>
Motivation: Traditional ASA systems have modality limitations - text-based approaches lack acoustic information while audio-based methods miss semantic context. MLLMs offer opportunities for comprehensive assessment by processing both audio and text simultaneously.

Method: Proposed Speech-First Multimodal Training (SFMT) using curriculum learning principle to establish robust speech modeling foundations before cross-modal fusion. Systematic study of MLLM for comprehensive ASA.

Result: MLLM-based systems elevate holistic assessment performance from PCC 0.783 to 0.846. SFMT achieves 4% absolute accuracy improvement in delivery aspect evaluation over conventional approaches.

Conclusion: MLLMs show superior performance for comprehensive ASA, particularly with the proposed SFMT training strategy that addresses delivery assessment challenges and paves new avenues for automated speaking assessment.

Abstract: Traditional Automated Speaking Assessment (ASA) systems exhibit inherent
modality limitations: text-based approaches lack acoustic information while
audio-based methods miss semantic context. Multimodal Large Language Models
(MLLM) offer unprecedented opportunities for comprehensive ASA by
simultaneously processing audio and text within unified frameworks. This paper
presents a very first systematic study of MLLM for comprehensive ASA,
demonstrating the superior performance of MLLM across the aspects of content
and language use . However, assessment on the delivery aspect reveals unique
challenges, which is deemed to require specialized training strategies. We thus
propose Speech-First Multimodal Training (SFMT), leveraging a curriculum
learning principle to establish more robust modeling foundations of speech
before cross-modal synergetic fusion. A series of experiments on a benchmark
dataset show MLLM-based systems can elevate the holistic assessment performance
from a PCC value of 0.783 to 0.846. In particular, SFMT excels in the
evaluation of the delivery aspect, achieving an absolute accuracy improvement
of 4% over conventional training approaches, which also paves a new avenue for
ASA.

</details>


### [257] [Semantic Anchoring in Agentic Memory: Leveraging Linguistic Structures for Persistent Conversational Context](https://arxiv.org/abs/2508.12630)
*Maitreyi Chatterjee,Devansh Agarwal*

Main category: cs.CL

TL;DR: Semantic Anchoring improves LLM memory in long-term conversations by combining vector storage with explicit linguistic structures like syntax, discourse, and coreference, achieving 18% better recall than standard RAG systems.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with long-term memory persistence in multi-session interactions. Standard RAG systems using dense vectors capture semantic similarity but miss important linguistic structures like syntax, discourse relations, and coreference links.

Method: Proposes Semantic Anchoring - a hybrid agentic memory architecture that enriches vector-based storage with explicit linguistic cues through dependency parsing, discourse relation tagging, and coreference resolution to create structured memory entries.

Result: Experiments on adapted long-term dialogue datasets show 18% improvement in factual recall and discourse coherence over strong RAG baselines. Includes ablation studies, human evaluations, and error analysis confirming robustness and interpretability.

Conclusion: Explicit linguistic structures significantly enhance LLM memory performance in long-term conversations, with Semantic Anchoring providing substantial improvements over traditional vector-only approaches.

Abstract: Large Language Models (LLMs) have demonstrated impressive fluency and task
competence in conversational settings. However, their effectiveness in
multi-session and long-term interactions is hindered by limited memory
persistence. Typical retrieval-augmented generation (RAG) systems store
dialogue history as dense vectors, which capture semantic similarity but
neglect finer linguistic structures such as syntactic dependencies, discourse
relations, and coreference links. We propose Semantic Anchoring, a hybrid
agentic memory architecture that enriches vector-based storage with explicit
linguistic cues to improve recall of nuanced, context-rich exchanges. Our
approach combines dependency parsing, discourse relation tagging, and
coreference resolution to create structured memory entries. Experiments on
adapted long-term dialogue datasets show that semantic anchoring improves
factual recall and discourse coherence by up to 18% over strong RAG baselines.
We further conduct ablation studies, human evaluations, and error analysis to
assess robustness and interpretability.

</details>


### [258] [Beyond GPT-5: Making LLMs Cheaper and Better via Performance-Efficiency Optimized Routing](https://arxiv.org/abs/2508.12631)
*Yiqun Zhang,Hao Li,Jianhao Chen,Hangfan Zhang,Peng Ye,Lei Bai,Shuyue Hu*

Main category: cs.CL

TL;DR: Avengers-Pro is a test-time routing framework that dynamically assigns queries to optimal LLMs based on performance-efficiency tradeoffs, achieving state-of-the-art results with up to +7% accuracy improvement and 63% cost reduction.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of balancing performance and efficiency in large language models by providing a unified solution for all performance-efficiency tradeoffs.

Method: Embeds and clusters incoming queries, then routes each to the most suitable model based on a performance-efficiency score, ensembling LLMs of varying capacities and efficiencies.

Result: Achieves state-of-the-art results across 6 benchmarks and 8 leading models: +7% accuracy over strongest single model (GPT-5-medium), matches strongest model accuracy at 27% lower cost, reaches ~90% performance at 63% lower cost, and achieves Pareto frontier.

Conclusion: Avengers-Pro provides an effective framework for optimizing performance-efficiency tradeoffs in LLM inference through intelligent test-time routing and model ensembling.

Abstract: Balancing performance and efficiency is a central challenge in large language
model (LLM) advancement. GPT-5 addresses this with test-time routing,
dynamically assigning queries to either an efficient or a high-capacity model
during inference. In this work, we present Avengers-Pro, a test-time routing
framework that ensembles LLMs of varying capacities and efficiencies, providing
a unified solution for all performance-efficiency tradeoffs. The Avengers-Pro
embeds and clusters incoming queries, then routes each to the most suitable
model based on a performance-efficiency score. Across 6 challenging benchmarks
and 8 leading models -- including GPT-5-medium, Gemini-2.5-pro, and
Claude-opus-4.1 -- Avengers-Pro achieves state-of-the-art results: by varying a
performance-efficiency trade-off parameter, it can surpass the strongest single
model (GPT-5-medium) by +7% in average accuracy. Moreover, it can match the
average accuracy of the strongest single model at 27% lower cost, and reach
~90% of that performance at 63% lower cost. Last but not least, it achieves a
Pareto frontier, consistently yielding the highest accuracy for any given cost,
and the lowest cost for any given accuracy, among all single models. Code is
available at https://github.com/ZhangYiqun018/AvengersPro.

</details>


### [259] [Prompt-Induced Linguistic Fingerprints for LLM-Generated Fake News Detection](https://arxiv.org/abs/2508.12632)
*Chi Wang,Min Gao,Zongwei Wang,Junwei Yin,Kai Shu,Chenghua Lin*

Main category: cs.CL

TL;DR: LIFE method detects LLM-generated fake news by analyzing prompt-induced linguistic fingerprints through word-level probability distribution reconstruction and key-fragment techniques.


<details>
  <summary>Details</summary>
Motivation: The ease of generating fake news with large language models creates societal threats, and existing methods focusing on textual content alone are insufficient as LLM-generated fake news often appears coherent and factually consistent.

Method: LIFE (Linguistic Fingerprints Extraction) reconstructs word-level probability distributions to find discriminative patterns, uses key-fragment techniques to amplify subtle linguistic differences, and leverages distributional divergence analysis to uncover prompt-induced linguistic fingerprints.

Result: LIFE achieves state-of-the-art performance in detecting LLM-generated fake news and maintains high performance in human-written fake news detection.

Conclusion: The proposed LIFE method effectively identifies LLM-generated fake news by capturing subtle linguistic fingerprints induced by malicious prompts, providing a reliable detection approach that works across both machine-generated and human-written fake content.

Abstract: With the rapid development of large language models, the generation of fake
news has become increasingly effortless, posing a growing societal threat and
underscoring the urgent need for reliable detection methods. Early efforts to
identify LLM-generated fake news have predominantly focused on the textual
content itself; however, because much of that content may appear coherent and
factually consistent, the subtle traces of falsification are often difficult to
uncover. Through distributional divergence analysis, we uncover prompt-induced
linguistic fingerprints: statistically distinct probability shifts between
LLM-generated real and fake news when maliciously prompted. Based on this
insight, we propose a novel method named Linguistic Fingerprints Extraction
(LIFE). By reconstructing word-level probability distributions, LIFE can find
discriminative patterns that facilitate the detection of LLM-generated fake
news. To further amplify these fingerprint patterns, we also leverage
key-fragment techniques that accentuate subtle linguistic differences, thereby
improving detection reliability. Our experiments show that LIFE achieves
state-of-the-art performance in LLM-generated fake news and maintains high
performance in human-written fake news. The code and data are available at
https://anonymous.4open.science/r/LIFE-E86A.

</details>


### [260] [Breaking Language Barriers: Equitable Performance in Multilingual Language Models](https://arxiv.org/abs/2508.12662)
*Tanay Nagar,Grigorii Khvatskii,Anna Sokol,Nitesh V. Chawla*

Main category: cs.CL

TL;DR: Fine-tuning LLMs on synthetic code-switched text improves common sense reasoning performance in low-resource languages while maintaining high-resource language performance.


<details>
  <summary>Details</summary>
Motivation: LLMs perform worse in common sense reasoning tasks when prompted in low-resource languages compared to high-resource languages, creating unfair access to quality outputs across linguistic communities.

Method: Fine-tuning LLMs on synthetic code-switched text generated using controlled language-mixing methods, with a new dataset derived from CommonSenseQA featuring three language ratio configurations.

Result: Substantial improvements in low-resource language model performance while preserving or enhancing performance in high-resource languages.

Conclusion: Synthetic code-switched fine-tuning effectively bridges the performance gap between high and low-resource languages in LLM common sense reasoning tasks.

Abstract: Cutting-edge LLMs have emerged as powerful tools for multilingual
communication and understanding. However, LLMs perform worse in Common Sense
Reasoning (CSR) tasks when prompted in low-resource languages (LRLs) like Hindi
or Swahili compared to high-resource languages (HRLs) like English. Equalizing
this inconsistent access to quality LLM outputs is crucial to ensure fairness
for speakers of LRLs and across diverse linguistic communities. In this paper,
we propose an approach to bridge this gap in LLM performance. Our approach
involves fine-tuning an LLM on synthetic code-switched text generated using
controlled language-mixing methods. We empirically demonstrate that fine-tuning
LLMs on synthetic code-switched datasets leads to substantial improvements in
LRL model performance while preserving or enhancing performance in HRLs.
Additionally, we present a new dataset of synthetic code-switched text derived
from the CommonSenseQA dataset, featuring three distinct language ratio
configurations.

</details>


### [261] [Leveraging Large Language Models for Predictive Analysis of Human Misery](https://arxiv.org/abs/2508.12669)
*Bishanka Seal,Rahul Seetharaman,Aman Bansal,Abhilash Nandy*

Main category: cs.CL

TL;DR: LLMs can predict human misery scores from text descriptions using various prompting strategies, with few-shot approaches performing best. A novel gamified evaluation framework tests LLMs' dynamic emotional reasoning capabilities.


<details>
  <summary>Details</summary>
Motivation: To explore how well Large Language Models can understand and predict human emotional states (specifically misery) from natural language descriptions of real-world scenarios, moving beyond traditional static evaluation methods.

Method: Framed as regression problem (0-100 scores), tested zero-shot, fixed-context few-shot, and retrieval-based prompting using BERT embeddings. Introduced "Misery Game Show" - a gamified framework with ordinal comparison, binary classification, scalar estimation, and feedback-driven reasoning rounds.

Result: Few-shot approaches consistently outperformed zero-shot baselines, demonstrating the value of contextual examples in affective prediction. The gamified evaluation showed LLMs' potential in dynamic emotional reasoning tasks beyond standard regression.

Conclusion: LLMs show promise in predicting human emotional states from text, with few-shot learning being particularly effective. The gamified evaluation framework provides a more comprehensive way to assess emotional reasoning capabilities beyond traditional metrics.

Abstract: This study investigates the use of Large Language Models (LLMs) for
predicting human-perceived misery scores from natural language descriptions of
real-world scenarios. The task is framed as a regression problem, where the
model assigns a scalar value from 0 to 100 to each input statement. We evaluate
multiple prompting strategies, including zero-shot, fixed-context few-shot, and
retrieval-based prompting using BERT sentence embeddings. Few-shot approaches
consistently outperform zero-shot baselines, underscoring the value of
contextual examples in affective prediction. To move beyond static evaluation,
we introduce the "Misery Game Show", a novel gamified framework inspired by a
television format. It tests LLMs through structured rounds involving ordinal
comparison, binary classification, scalar estimation, and feedback-driven
reasoning. This setup enables us to assess not only predictive accuracy but
also the model's ability to adapt based on corrective feedback. The gamified
evaluation highlights the broader potential of LLMs in dynamic emotional
reasoning tasks beyond standard regression. Code and data link:
https://github.com/abhi1nandy2/Misery_Data_Exps_GitHub

</details>


### [262] [ToolACE-MT: Non-Autoregressive Generation for Agentic Multi-Turn Interaction](https://arxiv.org/abs/2508.12685)
*Xingshan Zeng,Weiwen Liu,Lingzhi Wang,Liangyou Li,Fei Mi,Yasheng Wang,Lifeng Shang,Xin Jiang,Qun Liu*

Main category: cs.CL

TL;DR: ToolACE-MT is a non-autoregressive framework that generates high-quality multi-turn agentic dialogues through three stages: coarse initialization, iterative refinement, and offline verification, enabling efficient data generation for tool-augmented LLMs.


<details>
  <summary>Details</summary>
Motivation: Existing simulation-based data generation methods for agentic tasks rely on costly autoregressive interactions between multiple LLM agents, limiting real-world performance and scalability.

Method: Three-stage framework: 1) Coarse-grained initialization builds structurally complete dialogue skeletons, 2) Iterative refinement adds realistic complexities via mask-and-fill operations, 3) Offline verification ensures correctness through rule- and model-based checks.

Result: Experiments demonstrate that ToolACE-MT enables efficient, effective and generalizable agentic data generation, outperforming existing autoregressive approaches.

Conclusion: ToolACE-MT offers a new paradigm for high-quality data construction in tool-augmented LLM scenarios, providing a scalable solution for agentic task-solving data generation.

Abstract: Agentic task-solving with Large Language Models (LLMs) requires multi-turn,
multi-step interactions, often involving complex function calls and dynamic
user-agent exchanges. Existing simulation-based data generation methods for
such scenarios rely heavily on costly autoregressive interactions between
multiple LLM agents, thereby limiting real-world performance of agentic tasks.
In this paper, we propose a novel Non-Autoregressive Iterative Generation
framework, called ToolACE-MT, for constructing high-quality multi-turn agentic
dialogues. ToolACE-MT generates full conversational trajectories through three
stages: coarse-grained initialization, iterative refinement, and offline
verification. The initialization phase builds a structurally complete yet
semantically coarse dialogue skeleton; the iterative refinement phase
introduces realistic complexities and continued refinement via mask-and-fill
operations; and the offline verification phase ensures correctness and
coherence via rule- and model-based checks. Experiments demonstrate that
ToolACE-MT enables efficient, effective and generalizable agentic data
generation, offering a new paradigm for high-quality data construction in
tool-augmented LLM scenarios.

</details>


### [263] [DESIGNER: Design-Logic-Guided Multidisciplinary Data Synthesis for LLM Reasoning](https://arxiv.org/abs/2508.12726)
*Weize Liu,Yongchi Zhao,Yijia Luo,Mingyu Xu,Jiaheng Liu,Yanan Li,Xiguo Hu,Yuchi Xu,Wenbo Su,Bo Zheng*

Main category: cs.CL

TL;DR: DESIGNER is a novel data synthesis pipeline that uses Design Logic concepts to generate 4.7 million challenging multidisciplinary reasoning questions from book and web corpora, significantly outperforming existing datasets in difficulty and diversity.


<details>
  <summary>Details</summary>
Motivation: LLMs struggle with complex multi-step reasoning across diverse disciplines, and existing datasets lack both disciplinary breadth and structural depth needed for robust reasoning evaluation.

Method: Reverse-engineered over 120,000 design logics from existing questions using LLMs, then matched these logics with disciplinary source materials from book and web corpora to synthesize challenging reasoning questions.

Result: Created two large-scale datasets (DLR-Book with 3.04M questions and DLR-Web with 1.66M questions) spanning 75 disciplines. SFT experiments showed significant performance improvements over baseline datasets and even surpassed official Qwen3 models.

Conclusion: The DESIGNER pipeline successfully generates high-quality, challenging reasoning questions at scale, enabling better training and evaluation of LLMs' multidisciplinary reasoning capabilities.

Abstract: Large language models (LLMs) have achieved remarkable success in many natural
language tasks but still struggle with complex, multi-step reasoning,
particularly across diverse disciplines. Existing reasoning datasets often
either lack disciplinary breadth or the structural depth necessary to elicit
robust reasoning behaviors. We propose DESIGNER: a DESIGN-logic-guidEd
Reasoning data synthesis pipeline that leverages naturally available, extensive
raw documents (book corpus and web corpus) to generate multidisciplinary
challenging questions. A core innovation of our approach is the introduction of
a Design Logic concept, which mimics the question-creation process of human
educators. We use LLMs to reverse-engineer and abstract over 120,000 design
logics from existing questions across various disciplines. By matching these
design logics with disciplinary source materials, we are able to create
reasoning questions that far surpass the difficulty and diversity of existing
datasets. Based on this pipeline, we synthesized two large-scale reasoning
datasets that span 75 disciplines: Design-Logic-Reasoning-Book (DLR-Book),
containing 3.04 million challenging questions synthesized from the book corpus,
and Design-Logic-Reasoning-Web (DLR-Web), with 1.66 million challenging
questions from the web corpus. Our data analysis demonstrates that the
questions synthesized by our method exhibit substantially greater difficulty
and diversity than those in the baseline datasets. We validate the
effectiveness of these datasets by conducting SFT experiments on the
Qwen3-8B-Base and Qwen3-4B-Base models. The results show that our dataset
significantly outperforms existing multidisciplinary datasets of the same
volume. Training with the full datasets further enables the models to surpass
the multidisciplinary reasoning performance of the official Qwen3-8B and
Qwen3-4B models.

</details>


### [264] [LinguaSafe: A Comprehensive Multilingual Safety Benchmark for Large Language Models](https://arxiv.org/abs/2508.12733)
*Zhiyuan Ning,Tianle Gu,Jiaxin Song,Shixin Hong,Lingyu Li,Huacan Liu,Jie Li,Yixu Wang,Meng Lingyu,Yan Teng,Yingchun Wang*

Main category: cs.CL

TL;DR: LinguaSafe is a comprehensive multilingual safety benchmark with 45k entries across 12 languages, addressing the gap in LLM safety evaluation for diverse linguistic contexts through translated, transcreated, and natively-sourced data.


<details>
  <summary>Details</summary>
Motivation: The lack of comprehensive evaluation and diverse data in existing multilingual safety evaluations for LLMs limits their effectiveness and hinders robust multilingual safety alignment, necessitating better safety assessment across diverse linguistic and cultural contexts.

Method: Created LinguaSafe dataset comprising 45k entries in 12 languages using a combination of translated, transcreated, and natively-sourced data with meticulous attention to linguistic authenticity. Developed a multidimensional evaluation framework with direct/indirect safety assessments and oversensitivity evaluations.

Result: Safety and helpfulness evaluation results vary significantly across different domains and languages, even among languages with similar resource levels, highlighting the importance of thorough multilingual safety assessment.

Conclusion: LinguaSafe provides a comprehensive suite of metrics for in-depth safety evaluation and underscores the critical importance of thoroughly assessing multilingual safety in LLMs to achieve more balanced safety alignment. The dataset and code are publicly released to facilitate further research.

Abstract: The widespread adoption and increasing prominence of large language models
(LLMs) in global technologies necessitate a rigorous focus on ensuring their
safety across a diverse range of linguistic and cultural contexts. The lack of
a comprehensive evaluation and diverse data in existing multilingual safety
evaluations for LLMs limits their effectiveness, hindering the development of
robust multilingual safety alignment. To address this critical gap, we
introduce LinguaSafe, a comprehensive multilingual safety benchmark crafted
with meticulous attention to linguistic authenticity. The LinguaSafe dataset
comprises 45k entries in 12 languages, ranging from Hungarian to Malay. Curated
using a combination of translated, transcreated, and natively-sourced data, our
dataset addresses the critical need for multilingual safety evaluations of
LLMs, filling the void in the safety evaluation of LLMs across diverse
under-represented languages from Hungarian to Malay. LinguaSafe presents a
multidimensional and fine-grained evaluation framework, with direct and
indirect safety assessments, including further evaluations for oversensitivity.
The results of safety and helpfulness evaluations vary significantly across
different domains and different languages, even in languages with similar
resource levels. Our benchmark provides a comprehensive suite of metrics for
in-depth safety evaluation, underscoring the critical importance of thoroughly
assessing multilingual safety in LLMs to achieve more balanced safety
alignment. Our dataset and code are released to the public to facilitate
further research in the field of multilingual LLM safety.

</details>


### [265] [CRED-SQL: Enhancing Real-world Large Scale Database Text-to-SQL Parsing through Cluster Retrieval and Execution Description](https://arxiv.org/abs/2508.12769)
*Shaoming Duan,Zirui Wang,Chuanyi Liu,Zhibin Zhu,Yuhao Zhang,Peiyi Han,Liang Yan,Zewu Penge*

Main category: cs.CL

TL;DR: CRED-SQL is a framework that addresses semantic mismatch in Text-to-SQL systems for large databases using cluster-based schema retrieval and an intermediate Execution Description Language to improve accuracy.


<details>
  <summary>Details</summary>
Motivation: Semantic mismatch between natural language questions and SQL queries in large databases causes schema linking issues and semantic drift, reducing model accuracy despite advances in LLMs.

Method: CRED-SQL uses cluster-based large-scale schema retrieval to identify relevant tables/columns, then introduces Execution Description Language (EDL) as intermediate representation, decomposing the task into Text-to-EDL and EDL-to-SQL stages.

Result: Achieves state-of-the-art performance on SpiderUnion and BirdUnion benchmarks, demonstrating effectiveness and scalability for large-scale cross-domain databases.

Conclusion: The framework successfully bridges semantic gaps in Text-to-SQL systems through innovative retrieval and intermediate representation techniques, significantly improving accuracy for large database applications.

Abstract: Recent advances in large language models (LLMs) have significantly improved
the accuracy of Text-to-SQL systems. However, a critical challenge remains: the
semantic mismatch between natural language questions (NLQs) and their
corresponding SQL queries. This issue is exacerbated in large-scale databases,
where semantically similar attributes hinder schema linking and semantic drift
during SQL generation, ultimately reducing model accuracy. To address these
challenges, we introduce CRED-SQL, a framework designed for large-scale
databases that integrates Cluster Retrieval and Execution Description. CRED-SQL
first performs cluster-based large-scale schema retrieval to pinpoint the
tables and columns most relevant to a given NLQ, alleviating schema mismatch.
It then introduces an intermediate natural language representation-Execution
Description Language (EDL)-to bridge the gap between NLQs and SQL. This
reformulation decomposes the task into two stages: Text-to-EDL and EDL-to-SQL,
leveraging LLMs' strong general reasoning capabilities while reducing semantic
deviation. Extensive experiments on two large-scale, cross-domain
benchmarks-SpiderUnion and BirdUnion-demonstrate that CRED-SQL achieves new
state-of-the-art (SOTA) performance, validating its effectiveness and
scalability. Our code is available at https://github.com/smduan/CRED-SQL.git

</details>


### [266] [From SALAMANDRA to SALAMANDRATA: BSC Submission for WMT25 General Machine Translation Shared Task](https://arxiv.org/abs/2508.12774)
*Javier Garcia Gilabert,Xixian Liao,Severino Da Dalt,Ella Bohman,Audrey Mash,Francesca De Luca Fornaciari,Irene Baucells,Joan Llop,Miguel Claramunt Argote,Carlos Escolano,Maite Melero*

Main category: cs.CL

TL;DR: SALAMANDRATA models (2B and 7B parameters) for 38 European language translation, with WMT25 adaptation using quality-aware decoding strategies.


<details>
  <summary>Details</summary>
Motivation: To improve machine translation performance for European languages and adapt models for the WMT25 shared task with additional non-European languages.

Method: Two-step training: continual pre-training on parallel data followed by supervised fine-tuning on high-quality instructions. Vocabulary adaptation, additional training phases, and quality-aware decoding (Minimum Bayes Risk Decoding and Tuned Re-ranking with COMET/COMET-KIWI).

Result: Developed SALAMANDRATA family models (2B and 7B) specifically optimized for translation tasks, with BSC's WMT25 submission based on the 7B variant.

Conclusion: Successfully created improved translation models for European languages, adapted for broader WMT25 requirements, and publicly released the models on Hugging Face.

Abstract: In this paper, we present the SALAMANDRATA family of models, an improved
iteration of SALAMANDRA LLMs (Gonzalez-Agirre et al., 2025) specifically
trained to achieve strong performance in translation-related tasks for 38
European languages. SALAMANDRATA comes in two scales: 2B and 7B parameters. For
both versions, we applied the same training recipe with a first step of
continual pre-training on parallel data, and a second step of supervised
fine-tuning on high-quality instructions. The BSC submission to the WMT25
General Machine Translation shared task is based on the 7B variant of
SALAMANDRATA. We first adapted the model vocabulary to support the additional
non-European languages included in the task. This was followed by a second
phase of continual pre-training and supervised fine-tuning, carefully designed
to optimize performance across all translation directions for this year's
shared task. For decoding, we employed two quality-aware strategies: Minimum
Bayes Risk Decoding and Tuned Re-ranking using COMET and COMET-KIWI
respectively. We publicly release both the 2B and 7B versions of SALAMANDRATA,
along with the newer SALAMANDRATA-V2 model, on Hugging Face1

</details>


### [267] [HeteroRAG: A Heterogeneous Retrieval-Augmented Generation Framework for Medical Vision Language Tasks](https://arxiv.org/abs/2508.12778)
*Zhe Chen,Yusheng Liao,Shuyang Jiang,Zhiyuan Zhu,Haolin Li,Yanfeng Wang,Yu Wang*

Main category: cs.CL

TL;DR: MedAtlas framework with HeteroRAG improves medical vision-language models by enabling effective retrieval across heterogeneous medical sources, significantly enhancing factual accuracy and reliability.


<details>
  <summary>Details</summary>
Motivation: Medical LVLMs suffer from factual inaccuracies and unreliable outputs that pose risks in clinical diagnostics, while current multimodal RAG systems cannot effectively retrieve from heterogeneous medical sources.

Method: Construct MedAtlas with multimodal report repositories and text corpora, then develop HeteroRAG framework with Modality-specific CLIPs for report retrieval, Multi-corpora Query Generator for dynamic queries, and Heterogeneous Knowledge Preference Tuning for cross-modality alignment.

Result: State-of-the-art performance across 12 datasets and 3 modalities in medical vision language benchmarks, with significant improvements in factual accuracy and reliability of Med-LVLMs.

Conclusion: HeteroRAG framework successfully bridges the gap in medical multimodal RAG by enabling effective heterogeneous knowledge retrieval, making Med-LVLMs more accurate and reliable for clinical applications.

Abstract: Medical large vision-language Models (Med-LVLMs) have shown promise in
clinical applications but suffer from factual inaccuracies and unreliable
outputs, posing risks in real-world diagnostics. While retrieval-augmented
generation has emerged as a potential solution, current medical multimodal RAG
systems are unable to perform effective retrieval across heterogeneous sources.
The irrelevance of retrieved reports affects the factuality of analysis, while
insufficient knowledge affects the credibility of clinical decision-making. To
bridge the gap, we construct MedAtlas, which includes extensive multimodal
report repositories and diverse text corpora. Based on it, we present
HeteroRAG, a novel framework that enhances Med-LVLMs through heterogeneous
knowledge sources. The framework introduces Modality-specific CLIPs for
effective report retrieval and a Multi-corpora Query Generator for dynamically
constructing queries for diverse corpora. Incorporating knowledge from such
multifaceted sources, Med-LVLM is then trained with Heterogeneous Knowledge
Preference Tuning to achieve cross-modality and multi-source knowledge
alignment. Extensive experiments across 12 datasets and 3 modalities
demonstrate that the proposed HeteroRAG achieves state-of-the-art performance
in most medical vision language benchmarks, significantly improving factual
accuracy and reliability of Med-LVLMs.

</details>


### [268] [Atom-Searcher: Enhancing Agentic Deep Research via Fine-Grained Atomic Thought Reward](https://arxiv.org/abs/2508.12800)
*Yong Deng,Guoqing Wang,Zhenzhe Ying,Xiaofeng Wu,Jinzhen Lin,Wenwen Xiong,Yuqin Dai,Shuo Yang,Zhanwei Zhang,Qiwen Wang,Yang Qin,Changhua Meng*

Main category: cs.CL

TL;DR: Atom-Searcher is a novel RL framework that decomposes LLM reasoning into fine-grained Atomic Thought units supervised by Reasoning Reward Models, addressing issues with traditional outcome-based RL in agentic deep research tasks.


<details>
  <summary>Details</summary>
Motivation: Current LLMs struggle with complex tasks due to static knowledge, and RAG approaches have limitations in multi-hop reasoning. Agentic approaches using outcome-based RL face issues like conflicting gradients and reward sparsity.

Method: Proposes Atomic Thought paradigm that decomposes reasoning into fine-grained functional units supervised by RRMs with Atomic Thought Rewards. Atom-Searcher integrates this with a curriculum-inspired reward schedule that prioritizes process-level rewards early and transitions to outcome rewards.

Result: Experiments on seven benchmarks show consistent improvements over state-of-the-art methods. The approach scales computation at test-time, provides better supervision anchors, and exhibits more interpretable, human-like reasoning patterns.

Conclusion: Atom-Searcher effectively addresses limitations of traditional RL approaches in agentic deep research by leveraging fine-grained reasoning decomposition and curriculum-based reward scheduling, leading to superior performance and more interpretable reasoning.

Abstract: Large language models (LLMs) exhibit remarkable problem-solving abilities,
but struggle with complex tasks due to static internal knowledge.
Retrieval-Augmented Generation (RAG) enhances access to external information,
yet remains limited in multi-hop reasoning and strategic search due to rigid
workflows. Recent advancements in agentic deep research empower LLMs to
autonomously reason, search, and synthesize information. However, current
approaches relying on outcome-based reinforcement learning (RL) face critical
issues such as conflicting gradients and reward sparsity, limiting performance
gains and training efficiency. To address these, we first propose Atomic
Thought, a novel LLM thinking paradigm that decomposes reasoning into
fine-grained functional units. These units are supervised by Reasoning Reward
Models (RRMs), which provide Atomic Thought Rewards (ATR) for fine-grained
guidance. Building on this, we propose Atom-Searcher, a novel RL framework for
agentic deep research that integrates Atomic Thought and ATR. Atom-Searcher
uses a curriculum-inspired reward schedule, prioritizing process-level ATR
early and transitioning to outcome rewards, accelerating convergence on
effective reasoning paths. Experiments on seven benchmarks show consistent
improvements over the state-of-the-art. Key advantages include: (1)
Atom-Searcher scales computation at test-time. (2) Atomic Thought provides
supervision anchors for RRMs, bridging deep research tasks and RRMs. (3)
Atom-Searcher exhibits more interpretable, human-like reasoning patterns.

</details>


### [269] [When Alignment Hurts: Decoupling Representational Spaces in Multilingual Models](https://arxiv.org/abs/2508.12803)
*Ahmed Elshabrawy,Hour Kaing,Haiyue Song,Alham Fikri Aji,Hideki Tanaka,Masao Utiyama,Raj Dabre*

Main category: cs.CL

TL;DR: Excessive alignment with high-resource languages like Modern Standard Arabic can hinder generative modeling of related low-resource dialects. A new variational probing framework enables subspace decoupling, improving dialect generation by +4.9 chrF++ maximum and +2.0 average across 25 Arabic dialects.


<details>
  <summary>Details</summary>
Motivation: Challenge the assumption that alignment with high-resource standard languages always aids modeling of related low-resource varieties, and demonstrate that representational entanglement can actively hinder generative performance for dialects.

Method: Online variational probing framework that continuously estimates the standard variety subspace during fine-tuning, enabling projection-based decoupling. Uses Arabic as case study with 25 dialects and parallel resources for controlled analysis.

Result: Intervention improves generation quality by up to +4.9 chrF++ and +2.0 on average across 25 dialects compared to standard fine-tuning, despite tradeoff in standard-language performance. Provides causal evidence of subspace dominance effects.

Conclusion: Subspace dominance by high-resource varieties restricts generative capacity for related varieties. The framework unifies geometric and information-theoretic probing with subspace-level causal interventions, offering practical tools for controlling representational allocation in multilingual LLMs.

Abstract: Alignment with high-resource standard languages is often assumed to aid the
modeling of related low-resource varieties. We challenge this assumption by
demonstrating that excessive representational entanglement with a dominant
variety, such as Modern Standard Arabic (MSA) in relation to Arabic dialects,
can actively hinder generative modeling. We present the first comprehensive
causal study of this phenomenon by analyzing and directly intervening in the
internal representation geometry of large language models (LLMs). Our key
contribution is an online variational probing framework that continuously
estimates the subspace of the standard variety during fine-tuning, enabling
projection-based decoupling from this space. While our study uses Arabic as a
case due to its unusually rich parallel resources across 25 dialects, the
broader motivation is methodological: dialectal MT serves as a controlled proxy
for generative tasks where comparable multi-variety corpora are unavailable.
Across 25 dialects, our intervention improves generation quality by up to +4.9
chrF++ and +2.0 on average compared to standard fine-tuning, despite a measured
tradeoff in standard-language performance. These results provide causal
evidence that subspace dominance by high-resource varieties can restrict
generative capacity for related varieties. More generally, we unify geometric
and information-theoretic probing with subspace-level causal interventions,
offering practical tools for improving generative modeling in closely related
language families and, more broadly, for controlling representational
allocation in multilingual and multi-domain LLMs. Code will be released.

</details>


### [270] [ding-01 :ARG0: An AMR Corpus for Spontaneous French Dialogue](https://arxiv.org/abs/2508.12819)
*Jeongwoo Kang,Maria Boritchev,Maximin Coavoux*

Main category: cs.CL

TL;DR: Building a French semantic corpus by annotating spontaneous dialogues with extended Abstract Meaning Representation (AMR) framework to better handle French-specific structures and spontaneous speech dynamics.


<details>
  <summary>Details</summary>
Motivation: To develop semantic resources for French dialogue and address AMR's insufficient coverage for spontaneous speech and French-specific sentence structures.

Method: Annotated the DinG corpus (French board game dialogues) using extended AMR framework, created annotation guidelines, trained and evaluated an AMR parser on the data.

Result: Created and published a French semantic corpus under CC-SA-BY license, developed an AMR parser that can assist human annotators with initial annotations.

Conclusion: This work contributes to French semantic resource development and provides tools for consistent annotation of spontaneous French dialogue using extended AMR framework.

Abstract: We present our work to build a French semantic corpus by annotating French
dialogue in Abstract Meaning Representation (AMR). Specifically, we annotate
the DinG corpus, consisting of transcripts of spontaneous French dialogues
recorded during the board game Catan. As AMR has insufficient coverage of the
dynamics of spontaneous speech, we extend the framework to better represent
spontaneous speech and sentence structures specific to French. Additionally, to
support consistent annotation, we provide an annotation guideline detailing
these extensions. We publish our corpus under a free license (CC-SA-BY). We
also train and evaluate an AMR parser on our data. This model can be used as an
assistance annotation tool to provide initial annotations that can be refined
by human annotators. Our work contributes to the development of semantic
resources for French dialogue.

</details>


### [271] [Context Matters: Incorporating Target Awareness in Conversational Abusive Language Detection](https://arxiv.org/abs/2508.12828)
*Raneem Alharthi,Rajwa Alharthi,Aiqi Jiang,Arkaitz Zubiaga*

Main category: cs.CL

TL;DR: Contextual features from parent tweets significantly improve abusive language detection in replies compared to using reply-only features, with content-based features being most effective.


<details>
  <summary>Details</summary>
Motivation: Existing abusive language detection research focuses on individual social media posts, overlooking the contextual information from conversational exchanges where replies build on parent posts.

Method: Studied conversational exchanges (parent-reply tweet pairs) using four classification models with various content-based and account-based features derived from both parent tweets and replies.

Result: Incorporating contextual features from parent tweets led to substantial improvements in detection performance. Content-based features contributed more than account-based features, and combining multiple features worked best.

Conclusion: Leveraging contextual information from parent tweets is crucial for effective abusive language detection in conversational settings, with content-based features being particularly important for classification performance.

Abstract: Abusive language detection has become an increasingly important task as a
means to tackle this type of harmful content in social media. There has been a
substantial body of research developing models for determining if a social
media post is abusive or not; however, this research has primarily focused on
exploiting social media posts individually, overlooking additional context that
can be derived from surrounding posts. In this study, we look at conversational
exchanges, where a user replies to an earlier post by another user (the parent
tweet). We ask: does leveraging context from the parent tweet help determine if
a reply post is abusive or not, and what are the features that contribute the
most? We study a range of content-based and account-based features derived from
the context, and compare this to the more widely studied approach of only
looking at the features from the reply tweet. For a more generalizable study,
we test four different classification models on a dataset made of
conversational exchanges (parent-reply tweet pairs) with replies labeled as
abusive or not. Our experiments show that incorporating contextual features
leads to substantial improvements compared to the use of features derived from
the reply tweet only, confirming the importance of leveraging context. We
observe that, among the features under study, it is especially the
content-based features (what is being posted) that contribute to the
classification performance rather than account-based features (who is posting
it). While using content-based features, it is best to combine a range of
different features to ensure improved performance over being more selective and
using fewer features. Our study provides insights into the development of
contextualized abusive language detection models in realistic settings
involving conversations.

</details>


### [272] [It takes a village to write a book: Mapping anonymous contributions in Stephen Langton's Quaestiones Theologiae](https://arxiv.org/abs/2508.12830)
*Jan Maliszewski*

Main category: cs.CL

TL;DR: Stylometric analysis of medieval scholastic reportationes to identify editorial layers and validate hypotheses about collection formation using computational methods.


<details>
  <summary>Details</summary>
Motivation: There is limited direct evidence about the practice of creating reportationes (records of oral teaching) in early scholastic period, and few sources comment on this practice. The study aims to uncover editorial work layers in Stephen Langton's Quaestiones Theologiae collection.

Method: Applying stylometric techniques for authorship attribution using HTR pipeline and analysis based on most frequent words, POS tags, and pseudo-affixes. Comparing performance on manually composed vs automatically extracted data, and testing transformer-based OCR with automated transcription alignment.

Result: The paper describes a proposed study design - results are not yet available as this appears to be a methodology paper outlining planned research.

Conclusion: If successful, this study will provide a reusable template for exploratory analysis of collaborative literary production from medieval universities, advancing computational research on scholastic tradition.

Abstract: While the indirect evidence suggests that already in the early scholastic
period the literary production based on records of oral teaching (so-called
reportationes) was not uncommon, there are very few sources commenting on the
practice. This paper details the design of a study applying stylometric
techniques of authorship attribution to a collection developed from
reportationes -- Stephen Langton's Quaestiones Theologiae -- aiming to uncover
layers of editorial work and thus validate some hypotheses regarding the
collection's formation. Following Camps, Cl\'erice, and Pinche (2021), I
discuss the implementation of an HTR pipeline and stylometric analysis based on
the most frequent words, POS tags, and pseudo-affixes. The proposed study will
offer two methodological gains relevant to computational research on the
scholastic tradition: it will directly compare performance on manually composed
and automatically extracted data, and it will test the validity of
transformer-based OCR and automated transcription alignment for workflows
applied to scholastic Latin corpora. If successful, this study will provide an
easily reusable template for the exploratory analysis of collaborative literary
production stemming from medieval universities.

</details>


### [273] [Word Meanings in Transformer Language Models](https://arxiv.org/abs/2508.12863)
*Jumbly Grindrod,Peter Grindrod*

Main category: cs.CL

TL;DR: Transformer language models encode rich semantic information in their token embeddings, challenging meaning eliminativist views of how LLMs process meaning.


<details>
  <summary>Details</summary>
Motivation: To investigate whether transformer models use something analogous to a lexical store containing semantic information for words, and to test specific meaning eliminativist hypotheses about how LLMs process semantic content.

Method: Extracted token embedding space from RoBERTa-base, performed k-means clustering into 200 clusters, then conducted manual inspection and tested sensitivity to five psycholinguistic measures: valence, concreteness, iconicity, taboo, and age of acquisition.

Result: Positive findings showing wide variety of semantic information encoded within the token embedding space, with clusters sensitive to semantic information and psycholinguistic measures.

Conclusion: Transformer LLMs do encode meaningful semantic information in their representations, ruling out certain meaning eliminativist hypotheses about how these models process semantic content.

Abstract: We investigate how word meanings are represented in the transformer language
models. Specifically, we focus on whether transformer models employ something
analogous to a lexical store - where each word has an entry that contains
semantic information. To do this, we extracted the token embedding space of
RoBERTa-base and k-means clustered it into 200 clusters. In our first study, we
then manually inspected the resultant clusters to consider whether they are
sensitive to semantic information. In our second study, we tested whether the
clusters are sensitive to five psycholinguistic measures: valence,
concreteness, iconicity, taboo, and age of acquisition. Overall, our findings
were very positive - there is a wide variety of semantic information encoded
within the token embedding space. This serves to rule out certain "meaning
eliminativist" hypotheses about how transformer LLMs process semantic
information.

</details>


### [274] [An LLM Agent-Based Complex Semantic Table Annotation Approach](https://arxiv.org/abs/2508.12868)
*Yilin Geng,Shujing Wang,Chuan Wang,Keqing He,Yanfei Lv,Ying Wang,Zaiwen Feng,Xiaoying Bai*

Main category: cs.CL

TL;DR: LLM-based agent approach with external tools for semantic table annotation, achieving better performance and significant cost reductions.


<details>
  <summary>Details</summary>
Motivation: Complex tables pose challenges like semantic loss, strict ontological hierarchy, homonyms, spelling errors, and abbreviations that hinder annotation accuracy in Column Type Annotation (CTA) and Cell Entity Annotation (CEA).

Method: Proposes an LLM-based agent approach with five external tools using ReAct framework, enabling dynamic selection of annotation strategies based on table characteristics. Uses Levenshtein distance to reduce redundant annotations.

Result: Outperforms existing approaches on Tough Tables and BiodivTab datasets. Achieves 70% reduction in time costs and 60% reduction in LLM token usage.

Conclusion: Provides an efficient and cost-effective solution for Semantic Table Annotation by leveraging LLM agents with optimized annotation strategies.

Abstract: The Semantic Table Annotation (STA) task, which includes Column Type
Annotation (CTA) and Cell Entity Annotation (CEA), maps table contents to
ontology entities and plays important roles in various semantic applications.
However, complex tables often pose challenges such as semantic loss of column
names or cell values, strict ontological hierarchy requirements, homonyms,
spelling errors, and abbreviations, which hinder annotation accuracy. To
address these issues, this paper proposes an LLM-based agent approach for CTA
and CEA. We design and implement five external tools with tailored prompts
based on the ReAct framework, enabling the STA agent to dynamically select
suitable annotation strategies depending on table characteristics. Experiments
are conducted on the Tough Tables and BiodivTab datasets from the SemTab
challenge, which contain the aforementioned challenges. Our method outperforms
existing approaches across various metrics. Furthermore, by leveraging
Levenshtein distance to reduce redundant annotations, we achieve a 70%
reduction in time costs and a 60% reduction in LLM token usage, providing an
efficient and cost-effective solution for STA.

</details>


### [275] [A Stitch in Time Saves Nine: Proactive Self-Refinement for Language Models](https://arxiv.org/abs/2508.12903)
*Jinyi Han,Xinyi Wang,Haiquan Zhao,Tingyun li,Zishang Jiang,Sihang Jiang,Jiaqing Liang,Xin Lin,Weikang Zhou,Zeye Sun,Fei Yu,Yanghua Xiao*

Main category: cs.CL

TL;DR: PASR enables LLMs to dynamically refine outputs during generation by proactively deciding when and how to refine based on internal state, reducing tokens by 41.6% while improving accuracy by 8.2%.


<details>
  <summary>Details</summary>
Motivation: Existing self-refinement methods use fixed iterative processes that cannot adapt to evolving generation context, unlike human dynamic refinement during execution.

Method: ProActive Self-Refinement (PASR) allows LLMs to refine outputs during generation by monitoring internal state and context to decide whether, when, and how to refine rather than regenerating entire responses.

Result: On Qwen3-8B across 10 diverse tasks, PASR reduced average token consumption by 41.6% compared to standard generation while achieving 8.2% accuracy improvement.

Conclusion: PASR demonstrates that proactive, context-aware refinement during generation significantly enhances LLM performance with reduced computational cost compared to reactive fixed-iteration approaches.

Abstract: Recent advances in self-refinement have demonstrated significant potential
for improving the outputs of large language models (LLMs) through iterative
refinement. However, most existing self-refinement methods rely on a reactive
process with a fixed number of iterations, making it difficult to determine the
optimal timing and content of refinement based on the evolving generation
context. Inspired by the way humans dynamically refine their thoughts during
execution, we propose ProActive Self-Refinement (PASR), a novel method that
enables LLMs to refine their outputs during the generation process. Unlike
methods that regenerate entire responses, PASR proactively decides whether,
when, and how to refine based on the model's internal state and evolving
context. We conduct extensive experiments on a diverse set of 10 tasks to
evaluate the effectiveness of PASR. Experimental results show that PASR
significantly enhances problem-solving performance. In particular, on Qwen3-8B,
PASR reduces average token consumption by 41.6 percent compared to standard
generation, while also achieving an 8.2 percent improvement in accuracy. Our
code and all baselines used in the paper are available in the GitHub.

</details>


### [276] [Analyzing Information Sharing and Coordination in Multi-Agent Planning](https://arxiv.org/abs/2508.12981)
*Tianyue Ou,Saujas Vaduguru,Daniel Fried*

Main category: cs.CL

TL;DR: LLM-based multi-agent system with notebook for information sharing and orchestrator for coordination improves travel planning performance by 17.5% over single-agent baseline.


<details>
  <summary>Details</summary>
Motivation: Multi-agent systems struggle with long-horizon, multi-constraint planning tasks that require detailed information and complex interdependent constraints.

Method: Constructed an LLM-based multi-agent system for travel planning with two key mechanisms: a notebook for information sharing and an orchestrator agent for coordination in free-form conversations.

Result: Notebook reduced hallucination errors by 18%, orchestrator reduced errors by up to 13.5% in focused areas. Combined system achieved 25% pass rate on TravelPlanner benchmark vs 7.5% for single-agent baseline.

Conclusion: Structured information sharing and reflective orchestration are key components for effective multi-agent systems in long-horizon planning with LLMs.

Abstract: Multi-agent systems (MASs) have pushed the boundaries of large language model
(LLM) agents in domains such as web research and software engineering. However,
long-horizon, multi-constraint planning tasks involve conditioning on detailed
information and satisfying complex interdependent constraints, which can pose a
challenge for these systems. In this study, we construct an LLM-based MAS for a
travel planning task which is representative of these challenges. We evaluate
the impact of a notebook to facilitate information sharing, and evaluate an
orchestrator agent to improve coordination in free form conversation between
agents. We find that the notebook reduces errors due to hallucinated details by
18%, while an orchestrator directs the MAS to focus on and further reduce
errors by up to 13.5% within focused sub-areas. Combining both mechanisms
achieves a 25% final pass rate on the TravelPlanner benchmark, a 17.5% absolute
improvement over the single-agent baseline's 7.5% pass rate. These results
highlight the potential of structured information sharing and reflective
orchestration as key components in MASs for long horizon planning with LLMs.

</details>


### [277] [WebMall -- A Multi-Shop Benchmark for Evaluating Web Agents](https://arxiv.org/abs/2508.13024)
*Ralph Peeters,Aaron Steiner,Luca Schwarz,Julian Yuya Caspary,Christian Bizer*

Main category: cs.CL

TL;DR: WebMall is a new multi-shop online shopping benchmark for evaluating web agents, featuring 4 simulated shops with real products and 91 cross-shop comparison tasks that require longer interaction sequences than existing benchmarks.


<details>
  <summary>Details</summary>
Motivation: Existing e-commerce benchmarks like WebShop and ShoppingBench lack comparison-shopping tasks across multiple shops and don't adequately represent the heterogeneity of real-world shopping scenarios with products from hundreds of distinct shops.

Method: Created a benchmark with four simulated online shops populated with authentic product offers from Common Crawl, featuring 91 cross-shop tasks including basic tasks (finding products, price comparisons, checkout) and advanced tasks (vague requirements, substitutes, compatibility).

Result: Evaluated 8 baseline agents with different configurations - best performers achieved 75% completion rate and 87% F1 score on basic tasks, and 53% completion rate with 63% F1 score on advanced tasks.

Conclusion: WebMall provides a more realistic and challenging benchmark for web agents that better represents real-world shopping behaviors and enables research on navigation, reasoning, and efficiency in e-commerce scenarios.

Abstract: LLM-based web agents have the potential to automate long-running web tasks,
such as finding offers for specific products in multiple online shops and
subsequently ordering the cheapest products that meet the users needs. This
paper introduces WebMall, a multi-shop online shopping benchmark for evaluating
the effectiveness and efficiency of web agents for comparison-shopping. WebMall
consists of four simulated online shops populated with authentic product offers
sourced from the Common Crawl, alongside a suite of 91 cross-shop tasks. These
tasks include basic tasks such as finding specific products in multiple shops,
performing price comparisons, adding items to the shopping cart, and completing
checkout. Advanced tasks involve searching for products based on vague
requirements, identifying suitable substitutes, and finding compatible
products. Compared to existing e-commerce benchmarks, such as WebShop or
ShoppingBench, WebMall introduces comparison-shopping tasks across multiple
shops. Furthermore, the product offers are more heterogeneous, as they
originate from hundreds of distinct real-world shops. The tasks in WebMall
require longer interaction trajectories than those in WebShop, while remaining
representative of real-world shopping behaviors. We evaluate eight baseline
agents on WebMall, varying in observation modality, memory utilization, and
underlying large language model (GPT 4.1 and Claude Sonnet 4). The
best-performing configurations achieve completion rates of 75% and 53%, and F1
scores of 87% and 63%, on the basic and advanced task sets, respectively.
WebMall is publicly released to facilitate research on web agents and to
promote advancements in navigation, reasoning, and efficiency within e-commerce
scenarios.

</details>


### [278] [Integrating Feedback Loss from Bi-modal Sarcasm Detector for Sarcastic Speech Synthesis](https://arxiv.org/abs/2508.13028)
*Zhu Li,Yuqing Zhang,Xiyuan Gao,Devraj Raghuvanshi,Nagendra Kumar,Shekhar Nayak,Matt Coler*

Main category: cs.CL

TL;DR: Novel sarcastic speech synthesis method using bi-modal feedback loss and two-stage transfer learning to improve sarcasm conveyance in TTS systems.


<details>
  <summary>Details</summary>
Motivation: Sarcastic speech synthesis is challenging due to nuanced prosody and limited annotated data, but essential for natural human-computer interaction and entertainment applications.

Method: Integrates feedback loss from bi-modal sarcasm detection model into TTS training, plus two-stage fine-tuning: first on diverse speech styles, then specifically on sarcastic speech data.

Result: Objective and subjective evaluations show improved quality, naturalness, and sarcasm-awareness of synthesized speech.

Conclusion: The proposed approach effectively addresses sarcastic speech synthesis challenges and enhances the model's ability to capture and convey sarcasm.

Abstract: Sarcastic speech synthesis, which involves generating speech that effectively
conveys sarcasm, is essential for enhancing natural interactions in
applications such as entertainment and human-computer interaction. However,
synthesizing sarcastic speech remains a challenge due to the nuanced prosody
that characterizes sarcasm, as well as the limited availability of annotated
sarcastic speech data. To address these challenges, this study introduces a
novel approach that integrates feedback loss from a bi-modal sarcasm detection
model into the TTS training process, enhancing the model's ability to capture
and convey sarcasm. In addition, by leveraging transfer learning, a speech
synthesis model pre-trained on read speech undergoes a two-stage fine-tuning
process. First, it is fine-tuned on a diverse dataset encompassing various
speech styles, including sarcastic speech. In the second stage, the model is
further refined using a dataset focused specifically on sarcastic speech,
enhancing its ability to generate sarcasm-aware speech. Objective and
subjective evaluations demonstrate that our proposed methods improve the
quality, naturalness, and sarcasm-awareness of synthesized speech.

</details>


### [279] [Can Large Models Teach Student Models to Solve Mathematical Problems Like Human Beings? A Reasoning Distillation Method via Multi-LoRA Interaction](https://arxiv.org/abs/2508.13037)
*Xinhe Li,Jiajun Liu,Peng Wang*

Main category: cs.CL

TL;DR: LoRID is a novel method that uses multi-LoRA interaction to distill mathematical reasoning capabilities from LLMs to SLMs, inspired by System 1 and System 2 thinking, achieving state-of-the-art performance on GSM8K.


<details>
  <summary>Details</summary>
Motivation: To address the poor mathematical reasoning in Small Language Models (SLMs) by drawing inspiration from human dual-thinking systems (System 1 intuitive thinking and System 2 analytical thinking) rather than just cramming massive data.

Method: Uses multi-LoRA interaction with three components: Intuitive Reasoner (IR) for direct Chain-of-Thought generation, Knowledge Generator (KG) for knowledge extraction, and Deep Reasoner (DR) for knowledge-based reasoning. Includes iterative consistency checking between IR and DR outputs for mutual feedback enhancement.

Result: Achieves state-of-the-art performance, particularly on GSM8K dataset, outperforming second-best method by 2.3%, 16.1%, 2.4%, 12.3%, and 1.8% accuracy across five different base models.

Conclusion: The LoRID method successfully enhances mathematical reasoning in SLMs by mimicking human dual-thinking systems through multi-LoRA interaction and iterative consistency checking, demonstrating significant performance improvements over existing approaches.

Abstract: Recent studies have demonstrated that Large Language Models (LLMs) have
strong mathematical reasoning abilities but rely on hundreds of billions of
parameters. To tackle the challenge of poor reasoning in Small Language Models
(SLMs), existing methods typically leverage LLMs to generate massive amounts of
data for cramming training. In psychology, they are akin to System 1 thinking,
which resolves reasoning problems rapidly based on experience and intuition.
However, human learning also requires System 2 thinking, where knowledge is
first acquired and then reinforced through practice. Inspired by such two
distinct modes of thinking, we propose a novel method based on the multi-LoRA
Interaction for mathematical reasoning Distillation (LoRID). First, we input
the question and reasoning of each sample into an LLM to create
knowledge-enhanced datasets. Subsequently, we train a LoRA block on the student
model as an Intuitive Reasoner (IR), which directly generates Chain-of-Thoughts
for problem-solving. Then, to imitate System 2 thinking, we train the Knowledge
Generator (KG) and Deep Reasoner (DR), respectively. The former outputs only
knowledge after receiving problems, while the latter uses that knowledge to
perform reasoning. Finally, to address the randomness in the generation of IR
and DR, we evaluate whether their outputs are consistent, and the inference
process needs to be iterated if not. This step can enhance the mathematical
reasoning ability of SLMs through mutual feedback. Experimental results show
that LoRID achieves state-of-the-art performance, especially on the GSM8K
dataset, where it outperforms the second-best method by 2.3%, 16.1%, 2.4%,
12.3%, and 1.8% accuracy across the five base models, respectively.

</details>


### [280] [Büyük Dil Modelleri için TR-MMLU Benchmarkı: Performans Değerlendirmesi, Zorluklar ve İyileştirme Fırsatları](https://arxiv.org/abs/2508.13044)
*M. Ali Bayram,Ali Arda Fincan,Ahmet Semih Gümüş,Banu Diri,Savaş Yıldırım,Öner Aytaş*

Main category: cs.CL

TL;DR: Introduces TR-MMLU benchmark for evaluating Turkish language models with 6,200 multiple-choice questions across 62 educational sections.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of evaluating language models for resource-limited languages like Turkish, where comprehensive evaluation frameworks are lacking.

Method: Created a meticulously curated dataset of 6,200 multiple-choice questions organized into 62 sections based on the Turkish education system to assess linguistic and conceptual capabilities.

Result: Evaluated state-of-the-art LLMs on TR-MMLU, identifying specific areas for improvement in model design for Turkish language processing.

Conclusion: TR-MMLU establishes a new standard for Turkish NLP research, providing a comprehensive evaluation framework that enables detailed analysis and inspires future innovations in Turkish language model development.

Abstract: Language models have made significant advancements in understanding and
generating human language, achieving remarkable success in various
applications. However, evaluating these models remains a challenge,
particularly for resource-limited languages like Turkish. To address this
issue, we introduce the Turkish MMLU (TR-MMLU) benchmark, a comprehensive
evaluation framework designed to assess the linguistic and conceptual
capabilities of large language models (LLMs) in Turkish. TR-MMLU is based on a
meticulously curated dataset comprising 6,200 multiple-choice questions across
62 sections within the Turkish education system. This benchmark provides a
standard framework for Turkish NLP research, enabling detailed analyses of
LLMs' capabilities in processing Turkish text. In this study, we evaluated
state-of-the-art LLMs on TR-MMLU, highlighting areas for improvement in model
design. TR-MMLU sets a new standard for advancing Turkish NLP research and
inspiring future innovations.

</details>


### [281] [Doğal Dil İşlemede Tokenizasyon Standartları ve Ölçümü: Türkçe Üzerinden Büyük Dil Modellerinin Karşılaştırmalı Analizi](https://arxiv.org/abs/2508.13058)
*M. Ali Bayram,Ali Arda Fincan,Ahmet Semih Gümüş,Sercan Karakaş,Banu Diri,Savaş Yıldırım*

Main category: cs.CL

TL;DR: Novel evaluation framework for tokenization in morphologically-rich languages like Turkish, showing language-specific token percentage correlates better with downstream performance than token purity.


<details>
  <summary>Details</summary>
Motivation: Tokenization significantly impacts LLM performance, especially for morphologically-rich and low-resource languages that face unique challenges in preserving linguistic structures.

Method: Used Turkish MMLU dataset (6,200 questions) to evaluate tokenizers based on vocabulary size, token count, processing time, language-specific token percentages (%TR), and token purity (%Pure).

Result: Language-specific token percentages showed stronger correlation with downstream performance than token purity. Increasing model parameters alone doesn't improve linguistic performance.

Conclusion: Tailored language-specific tokenization methods are crucial for morphologically complex languages, and the proposed framework establishes practical tokenization standards.

Abstract: Tokenization is a fundamental preprocessing step in Natural Language
Processing (NLP), significantly impacting the capability of large language
models (LLMs) to capture linguistic and semantic nuances. This study introduces
a novel evaluation framework addressing tokenization challenges specific to
morphologically-rich and low-resource languages such as Turkish. Utilizing the
Turkish MMLU (TR-MMLU) dataset, comprising 6,200 multiple-choice questions from
the Turkish education system, we assessed tokenizers based on vocabulary size,
token count, processing time, language-specific token percentages (\%TR), and
token purity (\%Pure). These newly proposed metrics measure how effectively
tokenizers preserve linguistic structures. Our analysis reveals that
language-specific token percentages exhibit a stronger correlation with
downstream performance (e.g., MMLU scores) than token purity. Furthermore,
increasing model parameters alone does not necessarily enhance linguistic
performance, underscoring the importance of tailored, language-specific
tokenization methods. The proposed framework establishes robust and practical
tokenization standards for morphologically complex languages.

</details>


### [282] [Evaluating ASR robustness to spontaneous speech errors: A study of WhisperX using a Speech Error Database](https://arxiv.org/abs/2508.13060)
*John Alderete,Macarious Kin Fung Hui,Aanchan Mohan*

Main category: cs.CL

TL;DR: SFUSED database provides annotated speech errors for evaluating speech recognition models like WhisperX, demonstrating its value as a diagnostic tool for ASR system performance assessment.


<details>
  <summary>Details</summary>
Motivation: To create a systematic framework for testing and evaluating speech recognition models using annotated speech error data from spontaneous English speech, addressing the need for comprehensive diagnostic tools in ASR assessment.

Method: Developed the Simon Fraser University Speech Error Database (SFUSED) with systematic annotations of speech errors, including multiple classificatory dimensions like linguistic hierarchical level, contextual sensitivity, degraded words, word corrections, and error positioning at both word and syllable levels. Evaluated WhisperX transcription accuracy across 5,300 documented word and phonological errors.

Result: The database effectively serves as a diagnostic tool for assessing ASR system performance, demonstrating its utility through evaluation of WhisperX's transcription accuracy across thousands of documented speech errors.

Conclusion: SFUSED provides a valuable resource for linguistic and psycholinguistic research and offers an effective framework for testing and evaluating speech recognition models, with comprehensive annotations that enable detailed performance assessment across multiple classificatory dimensions.

Abstract: The Simon Fraser University Speech Error Database (SFUSED) is a public data
collection developed for linguistic and psycholinguistic research. Here we
demonstrate how its design and annotations can be used to test and evaluate
speech recognition models. The database comprises systematically annotated
speech errors from spontaneous English speech, with each error tagged for
intended and actual error productions. The annotation schema incorporates
multiple classificatory dimensions that are of some value to model assessment,
including linguistic hierarchical level, contextual sensitivity, degraded
words, word corrections, and both word-level and syllable-level error
positioning. To assess the value of these classificatory variables, we
evaluated the transcription accuracy of WhisperX across 5,300 documented word
and phonological errors. This analysis demonstrates the atabase's effectiveness
as a diagnostic tool for ASR system performance.

</details>


### [283] [Reinforced Context Order Recovery for Adaptive Reasoning and Planning](https://arxiv.org/abs/2508.13070)
*Long Ma,Fangwei Zhong,Yizhou Wang*

Main category: cs.CL

TL;DR: ReCOR is a reinforcement learning framework that learns adaptive token generation orders from unannotated text data, improving performance on reasoning and planning tasks by selecting tokens based on prediction difficulty rather than fixed orders.


<details>
  <summary>Details</summary>
Motivation: Current causal and diffusion models use fixed or random token generation orders that may not align with logical reasoning sequences, causing difficulties in tasks requiring adaptive generation strategies.

Method: Reinforcement-learning-based framework that self-supervises by estimating token prediction difficulty and adaptively selects the next token during training and inference without requiring order annotations.

Result: Superior performance on challenging reasoning and planning datasets, sometimes outperforming oracle models that use ground-truth generation orders.

Conclusion: Adaptive token generation orders learned through reinforcement learning can significantly improve model performance on complex reasoning tasks compared to fixed-order generation approaches.

Abstract: Modern causal language models, followed by rapid developments in discrete
diffusion models, can now produce a wide variety of interesting and useful
content. However, these families of models are predominantly trained to output
tokens with a fixed (left-to-right) or random order, which may deviate from the
logical order in which tokens are generated originally. In this paper, we
observe that current causal and diffusion models encounter difficulties in
problems that require adaptive token generation orders to solve tractably,
which we characterize with the $\mathcal{V}$-information framework. Motivated
by this, we propose Reinforced Context Order Recovery (ReCOR), a
reinforcement-learning-based framework to extract adaptive, data-dependent
token generation orders from text data without annotations. Self-supervised by
token prediction statistics, ReCOR estimates the hardness of predicting every
unfilled token and adaptively selects the next token during both training and
inference. Experiments on challenging reasoning and planning datasets
demonstrate the superior performance of ReCOR compared with baselines,
sometimes outperforming oracle models supervised with the ground-truth order.

</details>


### [284] [DocHPLT: A Massively Multilingual Document-Level Translation Dataset](https://arxiv.org/abs/2508.13079)
*Dayyán O'Brien,Bhavitvya Malik,Ona de Gibert,Pinzhen Chen,Barry Haddow,Jörg Tiedemann*

Main category: cs.CL

TL;DR: DocHPLT is the largest publicly available document-level translation dataset with 124M document pairs across 50 languages, created by preserving complete document integrity from web sources rather than reconstructing from sentence-level data.


<details>
  <summary>Details</summary>
Motivation: Existing document-level machine translation resources are limited to high-resourced languages, creating a need for comprehensive datasets to facilitate document-level translation and long-context modeling for global communities.

Method: Modified an existing web extraction pipeline to preserve complete document integrity from source, retaining all content including unaligned portions, rather than using reconstruction-based approaches that piece together documents from sentence-level data.

Result: LLMs fine-tuned on DocHPLT substantially outperform off-the-shelf instruction-tuned baselines, with particularly dramatic improvements for under-resourced languages.

Conclusion: DocHPLT provides essential infrastructure for advancing multilingual document-level translation and is open-sourced under a permissive license to benefit the research community.

Abstract: Existing document-level machine translation resources are only available for
a handful of languages, mostly high-resourced ones. To facilitate the training
and evaluation of document-level translation and, more broadly, long-context
modeling for global communities, we create DocHPLT, the largest publicly
available document-level translation dataset to date. It contains 124 million
aligned document pairs across 50 languages paired with English, comprising 4.26
billion sentences, with further possibility to provide 2500 bonus pairs not
involving English. Unlike previous reconstruction-based approaches that piece
together documents from sentence-level data, we modify an existing web
extraction pipeline to preserve complete document integrity from the source,
retaining all content including unaligned portions. After our preliminary
experiments identify the optimal training context strategy for document-level
translation, we demonstrate that LLMs fine-tuned on DocHPLT substantially
outperform off-the-shelf instruction-tuned baselines, with particularly
dramatic improvements for under-resourced languages. We open-source the dataset
under a permissive license, providing essential infrastructure for advancing
multilingual document-level translation.

</details>


### [285] [All for law and law for all: Adaptive RAG Pipeline for Legal Research](https://arxiv.org/abs/2508.13107)
*Figarri Keisha,Prince Singh,Pallavi,Dion Fernandes,Aravindh Manivannan,Ilham Wicaksono,Faisal Ahmad*

Main category: cs.CL

TL;DR: Enhanced open-source RAG pipeline for legal domain with context-aware query translation, improved retrieval strategies, and comprehensive evaluation framework that outperforms proprietary approaches while being cost-effective.


<details>
  <summary>Details</summary>
Motivation: To mitigate hallucinations in legal domain by grounding LLM outputs in cited sources, addressing the critical need for accurate and verifiable legal research assistance.

Method: End-to-end RAG pipeline with three enhancements: (1) context-aware query translator that handles document references and adapts retrieval parameters, (2) open-source retrieval using SBERT and GTE embeddings, (3) comprehensive evaluation framework combining RAGAS, BERTScore-F1, and ROUGE-Recall.

Result: Substantial performance gains (30-95% improvement in Recall@K and ~2.5x Precision@K for K>4), open-source pipelines rival or outperform proprietary approaches, custom legal-grounded prompts produce more faithful and contextually relevant answers.

Conclusion: Task-aware, component-level tuning enables legally grounded, reproducible, and cost-effective RAG systems for legal research, demonstrating the potential of carefully designed open-source solutions.

Abstract: Retrieval-Augmented Generation (RAG) mitigates hallucinations by grounding
large language model outputs in cited sources, a capability that is especially
critical in the legal domain. We present an end-to-end RAG pipeline that
revisits and extends the LegalBenchRAG baseline with three targeted
enhancements: (i) a context-aware query translator that disentangles document
references from natural-language questions and adapts retrieval depth and
response style based on expertise and specificity, (ii) open-source retrieval
strategies using SBERT and GTE embeddings that achieve substantial performance
gains (improving Recall@K by 30-95\% and Precision@K by $\sim$2.5$\times$ for
$K>4$) while remaining cost-efficient, and (iii) a comprehensive evaluation and
generation framework that combines RAGAS, BERTScore-F1, and ROUGE-Recall to
assess semantic alignment and faithfulness across models and prompt designs.
Our results show that carefully designed open-source pipelines can rival or
outperform proprietary approaches in retrieval quality, while a custom
legal-grounded prompt consistently produces more faithful and contextually
relevant answers than baseline prompting. Taken together, these contributions
demonstrate the potential of task-aware, component-level tuning to deliver
legally grounded, reproducible, and cost-effective RAG systems for legal
research assistance.

</details>


### [286] [AutoBnB-RAG: Enhancing Multi-Agent Incident Response with Retrieval-Augmented Generation](https://arxiv.org/abs/2508.13118)
*Zefang Liu,Arman Anwar*

Main category: cs.CL

TL;DR: AutoBnB-RAG enhances incident response simulations by integrating retrieval-augmented generation (RAG) into multi-agent systems, improving decision quality and success rates through access to external knowledge sources.


<details>
  <summary>Details</summary>
Motivation: Incident response requires fast, coordinated decision-making, but current LLM-based agents lack access to external knowledge, limiting their reasoning capabilities in cybersecurity simulations.

Method: Extends AutoBnB framework with RAG capabilities, using two retrieval settings: technical documentation (RAG-Wiki) and narrative incident reports (RAG-News). Evaluates eight team structures including argumentative configurations to promote critical reasoning, and simulates real-world cyber incidents based on public breach reports.

Result: Retrieval augmentation improves decision quality and success rates across diverse organizational models. The system demonstrates ability to reconstruct complex multi-stage attacks from real-world breach scenarios.

Conclusion: Integration of retrieval mechanisms into LLM-based multi-agent systems provides significant value for cybersecurity decision-making, enhancing autonomous incident response capabilities.

Abstract: Incident response (IR) requires fast, coordinated, and well-informed
decision-making to contain and mitigate cyber threats. While large language
models (LLMs) have shown promise as autonomous agents in simulated IR settings,
their reasoning is often limited by a lack of access to external knowledge. In
this work, we present AutoBnB-RAG, an extension of the AutoBnB framework that
incorporates retrieval-augmented generation (RAG) into multi-agent incident
response simulations. Built on the Backdoors & Breaches (B&B) tabletop game
environment, AutoBnB-RAG enables agents to issue retrieval queries and
incorporate external evidence during collaborative investigations. We introduce
two retrieval settings: one grounded in curated technical documentation
(RAG-Wiki), and another using narrative-style incident reports (RAG-News). We
evaluate performance across eight team structures, including newly introduced
argumentative configurations designed to promote critical reasoning. To
validate practical utility, we also simulate real-world cyber incidents based
on public breach reports, demonstrating AutoBnB-RAG's ability to reconstruct
complex multi-stage attacks. Our results show that retrieval augmentation
improves decision quality and success rates across diverse organizational
models. This work demonstrates the value of integrating retrieval mechanisms
into LLM-based multi-agent systems for cybersecurity decision-making.

</details>


### [287] [Spot the BlindSpots: Systematic Identification and Quantification of Fine-Grained LLM Biases in Contact Center Summaries](https://arxiv.org/abs/2508.13124)
*Kawin Mayilvaghanan,Siddhant Gupta,Ayush Kumar*

Main category: cs.CL

TL;DR: BlindSpot framework identifies and quantifies operational biases in LLM-generated call center summaries, revealing systemic biases across all tested models.


<details>
  <summary>Details</summary>
Motivation: LLMs generate millions of call transcript summaries daily in contact centers, but it's unclear if they systematically under- or over-attend to specific aspects, potentially introducing operational biases that haven't been explored.

Method: Introduced BlindSpot framework with 15 operational bias dimensions, using LLM as zero-shot classifier to derive categorical distributions for transcript-summary pairs, quantified via Fidelity Gap (JS Divergence) and Coverage metrics.

Result: Analysis of 2500 real call transcripts and summaries from 20 LLMs (various scales and families) revealed that biases are systemic and present across all evaluated models, regardless of size or family.

Conclusion: Operational biases in LLM-generated call summaries are pervasive and systematic, affecting all model types and sizes, highlighting the need for bias detection frameworks like BlindSpot in contact center applications.

Abstract: Abstractive summarization is a core application in contact centers, where
Large Language Models (LLMs) generate millions of summaries of call transcripts
daily. Despite their apparent quality, it remains unclear whether LLMs
systematically under- or over-attend to specific aspects of the transcript,
potentially introducing biases in the generated summary. While prior work has
examined social and positional biases, the specific forms of bias pertinent to
contact center operations - which we term Operational Bias - have remained
unexplored. To address this gap, we introduce BlindSpot, a framework built upon
a taxonomy of 15 operational bias dimensions (e.g., disfluency, speaker, topic)
for the identification and quantification of these biases. BlindSpot leverages
an LLM as a zero-shot classifier to derive categorical distributions for each
bias dimension in a pair of transcript and its summary. The bias is then
quantified using two metrics: Fidelity Gap (the JS Divergence between
distributions) and Coverage (the percentage of source labels omitted). Using
BlindSpot, we conducted an empirical study with 2500 real call transcripts and
their summaries generated by 20 LLMs of varying scales and families (e.g., GPT,
Llama, Claude). Our analysis reveals that biases are systemic and present
across all evaluated models, regardless of size or family.

</details>


### [288] [MuDRiC: Multi-Dialect Reasoning for Arabic Commonsense Validation](https://arxiv.org/abs/2508.13130)
*Kareem Elozeiri,Mervat Abassy,Preslav Nakov,Yuxia Wang*

Main category: cs.CL

TL;DR: Introduces MuDRiC - first Arabic multi-dialect commonsense dataset and novel GCN-based method for Arabic commonsense validation, achieving superior performance.


<details>
  <summary>Details</summary>
Motivation: Address the gap in Arabic commonsense validation resources, which have focused primarily on Modern Standard Arabic while neglecting regional dialects despite their prevalence in spoken contexts.

Method: Proposes a novel method adapting Graph Convolutional Networks (GCNs) to Arabic commonsense reasoning, enhancing semantic relationship modeling for improved commonsense validation.

Result: Experimental results demonstrate superior performance in Arabic commonsense validation compared to existing approaches.

Conclusion: The work enhances Arabic natural language understanding by providing both a foundational multi-dialect dataset and a novel method for handling Arabic's complex linguistic variations.

Abstract: Commonsense validation evaluates whether a sentence aligns with everyday
human understanding, a critical capability for developing robust natural
language understanding systems. While substantial progress has been made in
English, the task remains underexplored in Arabic, particularly given its rich
linguistic diversity. Existing Arabic resources have primarily focused on
Modern Standard Arabic (MSA), leaving regional dialects underrepresented
despite their prevalence in spoken contexts. To bridge this gap, we present two
key contributions: (i) we introduce MuDRiC, an extended Arabic commonsense
dataset incorporating multiple dialects, and (ii) a novel method adapting Graph
Convolutional Networks (GCNs) to Arabic commonsense reasoning, which enhances
semantic relationship modeling for improved commonsense validation. Our
experimental results demonstrate that this approach achieves superior
performance in Arabic commonsense validation. Our work enhances Arabic natural
language understanding by providing both a foundational dataset and a novel
method for handling its complex variations. To the best of our knowledge, we
release the first Arabic multi-dialect commonsense reasoning dataset.

</details>


### [289] [Improving Detection of Watermarked Language Models](https://arxiv.org/abs/2508.13131)
*Dara Bahri,John Wieting*

Main category: cs.CL

TL;DR: Hybrid detection combining watermark and non-watermark methods improves LLM generation detection, especially in low-entropy scenarios where watermarking alone struggles.


<details>
  <summary>Details</summary>
Motivation: Watermark detection effectiveness depends heavily on language model entropy, which is often limited in post-trained models (instruction tuning, RLHF), making standalone watermark detection challenging.

Method: Explored various hybrid schemes that combine watermark detectors with non-watermark detectors to improve detection performance.

Result: Observed performance gains over either class of detector (watermark-only or non-watermark-only) under a wide range of experimental conditions.

Conclusion: Combining watermark and non-watermark detection methods provides improved detection capabilities for LLM-generated content, particularly in low-entropy scenarios.

Abstract: Watermarking has recently emerged as an effective strategy for detecting the
generations of large language models (LLMs). The strength of a watermark
typically depends strongly on the entropy afforded by the language model and
the set of input prompts. However, entropy can be quite limited in practice,
especially for models that are post-trained, for example via instruction tuning
or reinforcement learning from human feedback (RLHF), which makes detection
based on watermarking alone challenging. In this work, we investigate whether
detection can be improved by combining watermark detectors with non-watermark
ones. We explore a number of hybrid schemes that combine the two, observing
performance gains over either class of detector under a wide range of
experimental conditions.

</details>


### [290] [OptimalThinkingBench: Evaluating Over and Underthinking in LLMs](https://arxiv.org/abs/2508.13141)
*Pranjal Aggarwal,Seungone Kim,Jack Lanchantin,Sean Welleck,Jason Weston,Ilia Kulikov,Swarnadeep Saha*

Main category: cs.CL

TL;DR: OptimalThinkingBench is a unified benchmark that evaluates both overthinking and underthinking in LLMs, showing current models fail to balance performance and efficiency optimally.


<details>
  <summary>Details</summary>
Motivation: Current LLMs either overthink simple problems (wasting compute) or underthink complex reasoning tasks, requiring users to manually select appropriate models for different queries.

Method: Developed two sub-benchmarks: OverthinkingBench with 72 simple domains and UnderthinkingBench with 11 challenging reasoning tasks, using novel thinking-adjusted accuracy metrics to evaluate 33 different thinking and non-thinking models.

Result: No model achieved optimal thinking - thinking models overthink simple queries without performance gains, while large non-thinking models underthink and perform worse than smaller thinking models on complex tasks.

Conclusion: Current approaches improve one sub-benchmark at the expense of the other, highlighting the need for better unified models that can balance performance and efficiency across different query complexities.

Abstract: Thinking LLMs solve complex tasks at the expense of increased compute and
overthinking on simpler problems, while non-thinking LLMs are faster and
cheaper but underthink on harder reasoning problems. This has led to the
development of separate thinking and non-thinking LLM variants, leaving the
onus of selecting the optimal model for each query on the end user. In this
work, we introduce OptimalThinkingBench, a unified benchmark that jointly
evaluates overthinking and underthinking in LLMs and also encourages the
development of optimally-thinking models that balance performance and
efficiency. Our benchmark comprises two sub-benchmarks: OverthinkingBench,
featuring simple queries in 72 domains, and UnderthinkingBench, containing 11
challenging reasoning tasks. Using novel thinking-adjusted accuracy metrics, we
perform extensive evaluation of 33 different thinking and non-thinking models
and show that no model is able to optimally think on our benchmark. Thinking
models often overthink for hundreds of tokens on the simplest user queries
without improving performance. In contrast, large non-thinking models
underthink, often falling short of much smaller thinking models. We further
explore several methods to encourage optimal thinking, but find that these
approaches often improve on one sub-benchmark at the expense of the other,
highlighting the need for better unified and optimal models in the future.

</details>


### [291] [Signal and Noise: A Framework for Reducing Uncertainty in Language Model Evaluation](https://arxiv.org/abs/2508.13144)
*David Heineman,Valentin Hofmann,Ian Magnusson,Yuling Gu,Noah A. Smith,Hannaneh Hajishirzi,Kyle Lo,Jesse Dodge*

Main category: cs.CL

TL;DR: Analysis of benchmark reliability metrics (signal and noise) showing that better signal-to-noise ratios improve decision-making in LLM development and scaling law predictions, with practical interventions to enhance benchmark quality.


<details>
  <summary>Details</summary>
Motivation: Large language model development is expensive and relies on small-scale experiments with evaluation benchmarks. Current benchmarks vary in reliability, making it difficult to make confident decisions about model improvements.

Method: Introduced two key metrics: signal (ability to separate better from worse models) and noise (sensitivity to random variability). Tested 30 benchmarks with 375 language models (60M-32B parameters), analyzing 900K evaluation results. Proposed three interventions: switching to better metrics (e.g., perplexity over accuracy), filtering noisy subtasks, and averaging intermediate checkpoints.

Result: Benchmarks with better signal-to-noise ratio were more reliable for small-scale decisions and had lower scaling law prediction error. All three interventions consistently improved reliability - better metrics improved both signal and noise, filtering noisy subtasks improved aggregate signal-to-noise, and checkpoint averaging reduced noise.

Conclusion: Benchmark creators and users should prioritize high signal and low noise. Practical recommendations include using perplexity over accuracy metrics, filtering unreliable subtasks, and averaging model checkpoints to improve evaluation reliability.

Abstract: Developing large language models is expensive and involves making decisions
with small experiments, typically by evaluating on large, multi-task evaluation
suites. In this work, we analyze specific properties which make a benchmark
more reliable for such decisions, and interventions to design higher-quality
evaluation benchmarks. We introduce two key metrics that show differences in
current benchmarks: signal, a benchmark's ability to separate better models
from worse models, and noise, a benchmark's sensitivity to random variability
between training steps. We demonstrate that benchmarks with a better
signal-to-noise ratio are more reliable when making decisions at small scale,
and those with less noise have lower scaling law prediction error. These
results suggest that improving signal or noise will lead to more useful
benchmarks, so we introduce three interventions designed to directly affect
signal or noise. For example, we propose that switching to a metric that has
better signal and noise (e.g., perplexity rather than accuracy) leads to better
reliability and improved scaling law error. We also find that filtering noisy
subtasks, to improve an aggregate signal-to-noise ratio, leads to more reliable
multi-task evaluations. We also find that averaging the output of a model's
intermediate checkpoints to reduce noise leads to consistent improvements. We
conclude by recommending that those creating new benchmarks, or selecting which
existing benchmarks to use, aim for high signal and low noise. We use 30
benchmarks for these experiments, and 375 open-weight language models from 60M
to 32B parameters, resulting in a new, publicly available dataset of 900K
evaluation benchmark results, totaling 200M instances.

</details>


### [292] [RepreGuard: Detecting LLM-Generated Text by Revealing Hidden Representation Patterns](https://arxiv.org/abs/2508.13152)
*Xin Chen,Junchao Wu,Shu Yang,Runzhe Zhan,Zeyu Wu,Ziyang Luo,Di Wang,Min Yang,Lidia S. Chao,Derek F. Wong*

Main category: cs.CL

TL;DR: RepreGuard is a new LLM-generated text detection method that uses internal model representations to better distinguish AI-generated from human-written text, achieving 94.92% AUROC across both in-distribution and out-of-distribution scenarios.


<details>
  <summary>Details</summary>
Motivation: Existing detection methods lack robustness in out-of-distribution scenarios. The authors hypothesize that LLMs' internal representations contain more comprehensive features that can better capture statistical pattern differences between AI-generated and human-written texts.

Method: RepreGuard uses a surrogate model to collect representations of both text types, extracts distinct activation features that identify AI-generated content, and classifies texts by calculating projection scores along these feature directions compared to a precomputed threshold.

Result: The method achieves average 94.92% AUROC, outperforming all baselines in both in-distribution and out-of-distribution scenarios, while showing robustness to various text sizes and mainstream attacks.

Conclusion: Internal LLM representations provide superior features for detecting AI-generated content, enabling more robust detection across diverse scenarios compared to existing methods.

Abstract: Detecting content generated by large language models (LLMs) is crucial for
preventing misuse and building trustworthy AI systems. Although existing
detection methods perform well, their robustness in out-of-distribution (OOD)
scenarios is still lacking. In this paper, we hypothesize that, compared to
features used by existing detection methods, the internal representations of
LLMs contain more comprehensive and raw features that can more effectively
capture and distinguish the statistical pattern differences between
LLM-generated texts (LGT) and human-written texts (HWT). We validated this
hypothesis across different LLMs and observed significant differences in neural
activation patterns when processing these two types of texts. Based on this, we
propose RepreGuard, an efficient statistics-based detection method.
Specifically, we first employ a surrogate model to collect representation of
LGT and HWT, and extract the distinct activation feature that can better
identify LGT. We can classify the text by calculating the projection score of
the text representations along this feature direction and comparing with a
precomputed threshold. Experimental results show that RepreGuard outperforms
all baselines with average 94.92% AUROC on both in-distribution (ID) and OOD
scenarios, while also demonstrating robust resilience to various text sizes and
mainstream attacks. Data and code are publicly available at:
https://github.com/NLP2CT/RepreGuard

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [293] [Sparse Attention across Multiple-context KV Cache](https://arxiv.org/abs/2508.11661)
*Ziyi Cao,Qingyi Si,Jingbin Zhang,Bingquan Liu*

Main category: cs.LG

TL;DR: SamKV is a novel method that applies attention sparsification to multiple-context KV Cache in RAG scenarios, enabling 85% sequence length compression without accuracy loss by considering cross-context information during sparsification and local recomputation.


<details>
  <summary>Details</summary>
Motivation: Existing KV Cache sparsification methods only work for single-context scenarios with causal attention dependencies, but fail in RAG where multiple documents have independent KV Caches without cross-attention. Prior approaches require retaining all KV Cache, failing to reduce memory overhead.

Method: SamKV performs attention sparsification for multiple-context KV Cache by considering complementary information from other contexts when sparsifying one context, followed by local recomputation of the sparsified information to maintain accuracy.

Result: The method compresses sequence length to 15% (85% reduction) without accuracy degradation compared to full-recomputation baselines, significantly improving throughput in multi-context RAG scenarios.

Conclusion: SamKV successfully addresses the challenge of efficient inference in RAG scenarios by enabling effective sparsification of multiple-context KV Cache while maintaining accuracy, representing the first exploration of attention sparsification for this problem domain.

Abstract: Large language models face significant cost challenges in long-sequence
inference. To address this, reusing historical Key-Value (KV) Cache for
improved inference efficiency has become a mainstream approach. Recent advances
further enhance throughput by sparse attention mechanisms to select the most
relevant KV Cache, thereby reducing sequence length. However, such techniques
are limited to single-context scenarios, where historical KV Cache is computed
sequentially with causal-attention dependencies. In retrieval-augmented
generation (RAG) scenarios, where retrieved documents as context are unknown
beforehand, each document's KV Cache is computed and stored independently
(termed multiple-context KV Cache), lacking cross-attention between contexts.
This renders existing methods ineffective. Although prior work partially
recomputes multiple-context KV Cache to mitigate accuracy loss from missing
cross-attention, it requires retaining all KV Cache throughout, failing to
reduce memory overhead. This paper presents SamKV, the first exploration of
attention sparsification for multiple-context KV Cache. Specifically, SamKV
takes into account the complementary information of other contexts when
sparsifying one context, and then locally recomputes the sparsified
information. Experiments demonstrate that our method compresses sequence length
to 15% without accuracy degradation compared with full-recompuation baselines,
significantly boosting throughput in multi-context RAG scenarios.

</details>


### [294] [Assessing Representation Stability for Transformer Models](https://arxiv.org/abs/2508.11667)
*Bryan E. Tuck,Rakesh M. Verma*

Main category: cs.LG

TL;DR: RS is a model-agnostic adversarial text detection framework that identifies attacks by measuring embedding sensitivity when masking important words, achieving over 88% accuracy across multiple datasets and attacks without retraining.


<details>
  <summary>Details</summary>
Motivation: Adversarial text attacks threaten transformer models, but existing defenses are attack-specific or require costly model retraining, creating a need for practical, generalizable detection methods.

Method: Ranks words using importance heuristics, measures embedding sensitivity when masking top-k critical words, and processes patterns with a BiLSTM detector. Uses gradient-based ranking for perturbation identification.

Result: Achieves over 88% detection accuracy across 3 datasets, 3 attack types, and 2 victim models. Gradient-based ranking outperforms attention and random selection. Generalizes well to unseen datasets/attacks/models.

Conclusion: RS provides a practical, effective solution for adversarial text detection with competitive performance, lower computational cost, and strong generalization without requiring model retraining.

Abstract: Adversarial text attacks remain a persistent threat to transformer models,
yet existing defenses are typically attack-specific or require costly model
retraining. We introduce Representation Stability (RS), a model-agnostic
detection framework that identifies adversarial examples by measuring how
embedding representations change when important words are masked. RS first
ranks words using importance heuristics, then measures embedding sensitivity to
masking top-k critical words, and processes the resulting patterns with a
BiLSTM detector. Experiments show that adversarially perturbed words exhibit
disproportionately high masking sensitivity compared to naturally important
words. Across three datasets, three attack types, and two victim models, RS
achieves over 88% detection accuracy and demonstrates competitive performance
compared to existing state-of-the-art methods, often at lower computational
cost. Using Normalized Discounted Cumulative Gain (NDCG) to measure
perturbation identification quality, we reveal that gradient-based ranking
outperforms attention and random selection approaches, with identification
quality correlating with detection performance for word-level attacks. RS also
generalizes well to unseen datasets, attacks, and models without retraining,
providing a practical solution for adversarial text detection.

</details>


### [295] [Collaborative Learning-Enhanced Lightweight Models for Predicting Arterial Blood Pressure Waveform in a Large-scale Perioperative Dataset](https://arxiv.org/abs/2508.11669)
*Wentao Li,Yonghu He,Kun Gao,Qing Liu,Yali Zheng*

Main category: cs.LG

TL;DR: Lightweight sInvResUNet model with collaborative learning achieves real-time arterial blood pressure monitoring on embedded devices with minimal computational load (0.89M params, 0.02 GFLOPS, 8.49ms inference).


<details>
  <summary>Details</summary>
Motivation: Existing deep learning models for ABP waveform reconstruction lack optimization for embedded system deployment due to high computational requirements and limited research on performance-computation tradeoffs.

Method: Proposed lightweight sInvResUNet architecture with KDCL (collaborative learning scheme), validated on large perioperative dataset (1.25M segments from 2,154 patients) with subject-independent testing.

Result: Achieved MAE of 10.06 mmHg and Pearson correlation of 0.88 for ABP tracking, outperforming larger models while enabling real-time inference on embedded devices.

Conclusion: Successfully demonstrated real-time ABP monitoring on embedded systems but revealed significant performance variations across demographic groups, highlighting generalization challenges in diverse populations.

Abstract: Noninvasive arterial blood pressure (ABP) monitoring is essential for patient
management in critical care and perioperative settings, providing continuous
assessment of cardiovascular hemodynamics with minimal risks. Numerous deep
learning models have developed to reconstruct ABP waveform from noninvasively
acquired physiological signals such as electrocardiogram and
photoplethysmogram. However, limited research has addressed the issue of model
performance and computational load for deployment on embedded systems. The
study introduces a lightweight sInvResUNet, along with a collaborative learning
scheme named KDCL_sInvResUNet. With only 0.89 million parameters and a
computational load of 0.02 GFLOPS, real-time ABP estimation was successfully
achieved on embedded devices with an inference time of just 8.49 milliseconds
for a 10-second output. We performed subject-independent validation in a
large-scale and heterogeneous perioperative dataset containing 1,257,141 data
segments from 2,154 patients, with a wide BP range (41-257 mmHg for SBP, and
31-234 mmHg for DBP). The proposed KDCL_sInvResUNet achieved lightly better
performance compared to large models, with a mean absolute error of 10.06 mmHg
and mean Pearson correlation of 0.88 in tracking ABP changes. Despite these
promising results, all deep learning models showed significant performance
variations across different demographic and cardiovascular conditions,
highlighting their limited ability to generalize across such a broad and
diverse population. This study lays a foundation work for real-time,
unobtrusive ABP monitoring in real-world perioperative settings, providing
baseline for future advancements in this area.

</details>


### [296] [Contrastive Regularization over LoRA for Multimodal Biomedical Image Incremental Learning](https://arxiv.org/abs/2508.11673)
*Haojie Zhang,Yixiong Liang,Hulin Kuang,Lihui Cen,Zhe Qu,Yigang Cen,Min Zeng,Shichao Kan*

Main category: cs.LG

TL;DR: MSLoRA-CR is a multimodal biomedical image incremental learning method that uses modality-specific LoRA modules with contrastive regularization to enable efficient knowledge sharing across modalities while preventing catastrophic forgetting.


<details>
  <summary>Details</summary>
Motivation: Existing incremental learning methods focus on task expansion within single modalities, but biomedical applications require handling diverse modalities. Training separate models for each modality significantly increases inference costs, creating a need for unified multimodal incremental learning.

Method: The method fine-tunes Modality-Specific LoRA modules while incorporating Contrastive Regularization. It builds upon a large vision-language model, keeping the pretrained model frozen while incrementally adapting new LoRA modules for each modality/task to enhance intra-modality knowledge sharing and promote inter-modality knowledge differentiation.

Result: MSLoRA-CR outperforms both training separate models for each modality and general incremental learning methods. It achieves a 1.88% improvement in overall performance compared to unconstrained incremental learning methods while maintaining computational efficiency.

Conclusion: The proposed MSLoRA-CR method effectively addresses the challenges of multimodal biomedical image incremental learning by preserving previously learned knowledge and leveraging cross-modal knowledge transfer through modality-specific LoRA adaptation with contrastive regularization.

Abstract: Multimodal Biomedical Image Incremental Learning (MBIIL) is essential for
handling diverse tasks and modalities in the biomedical domain, as training
separate models for each modality or task significantly increases inference
costs. Existing incremental learning methods focus on task expansion within a
single modality, whereas MBIIL seeks to train a unified model incrementally
across modalities. The MBIIL faces two challenges: I) How to preserve
previously learned knowledge during incremental updates? II) How to effectively
leverage knowledge acquired from existing modalities to support new modalities?
To address these challenges, we propose MSLoRA-CR, a method that fine-tunes
Modality-Specific LoRA modules while incorporating Contrastive Regularization
to enhance intra-modality knowledge sharing and promote inter-modality
knowledge differentiation. Our approach builds upon a large vision-language
model (LVLM), keeping the pretrained model frozen while incrementally adapting
new LoRA modules for each modality or task. Experiments on the incremental
learning of biomedical images demonstrate that MSLoRA-CR outperforms both the
state-of-the-art (SOTA) approach of training separate models for each modality
and the general incremental learning method (incrementally fine-tuning LoRA).
Specifically, MSLoRA-CR achieves a 1.88% improvement in overall performance
compared to unconstrained incremental learning methods while maintaining
computational efficiency. Our code is publicly available at
https://github.com/VentusAislant/MSLoRA_CR.

</details>


### [297] [Lifelong Learner: Discovering Versatile Neural Solvers for Vehicle Routing Problems](https://arxiv.org/abs/2508.11679)
*Shaodi Feng,Zhuoyi Lin,Jianan Zhou,Cong Zhang,Jingwen Li,Kuan-Wen Chen,Senthilnath Jayavelu,Yew-Soon Ong*

Main category: cs.LG

TL;DR: Lifelong learning framework for neural VRP solvers that handles varying problem contexts through inter-context self-attention and dynamic context scheduling.


<details>
  <summary>Details</summary>
Motivation: Most neural solvers are trained in monotonous contexts (Euclidean distance, single problem size), limiting their off-the-shelf application in different scenarios.

Method: Proposes a lifelong learner (LL) with Transformer backbone and inter-context self-attention to transfer knowledge between VRPs, plus dynamic context scheduler (DCS) with cross-context experience replay.

Result: Outperforms other neural solvers on synthetic and benchmark instances (up to 18k problem sizes), achieving best performance for most VRPs.

Conclusion: The framework enables neural solvers to discover effective policies for generic VRPs across varying contexts, enhancing versatility and practical applicability.

Abstract: Deep learning has been extensively explored to solve vehicle routing problems
(VRPs), which yields a range of data-driven neural solvers with promising
outcomes. However, most neural solvers are trained to tackle VRP instances in a
relatively monotonous context, e.g., simplifying VRPs by using Euclidean
distance between nodes and adhering to a single problem size, which harms their
off-the-shelf application in different scenarios. To enhance their versatility,
this paper presents a novel lifelong learning framework that incrementally
trains a neural solver to manage VRPs in distinct contexts. Specifically, we
propose a lifelong learner (LL), exploiting a Transformer network as the
backbone, to solve a series of VRPs. The inter-context self-attention mechanism
is proposed within LL to transfer the knowledge obtained from solving preceding
VRPs into the succeeding ones. On top of that, we develop a dynamic context
scheduler (DCS), employing the cross-context experience replay to further
facilitate LL looking back on the attained policies of solving preceding VRPs.
Extensive results on synthetic and benchmark instances (problem sizes up to
18k) show that our LL is capable of discovering effective policies for tackling
generic VRPs in varying contexts, which outperforms other neural solvers and
achieves the best performance for most VRPs.

</details>


### [298] [Comparative Analysis of Time Series Foundation Models for Demographic Forecasting: Enhancing Predictive Accuracy in US Population Dynamics](https://arxiv.org/abs/2508.11680)
*Aditya Akella,Jonathan Farah*

Main category: cs.LG

TL;DR: TimesFM foundation model outperforms traditional methods (LSTM, ARIMA, Linear Regression) in demographic forecasting, achieving lowest MSE in 86.67% of cases across diverse US states, especially for minority populations with sparse data.


<details>
  <summary>Details</summary>
Motivation: Accurate demographic forecasting is crucial for policymaking in urban planning, healthcare, and economic policy, but traditional methods struggle with complex demographic shifts influenced by globalization, economic conditions, and environmental factors.

Method: Applied Time Series Foundation Model (TimesFM) to predict US demographic changes using Census Bureau and FRED data, comparing against LSTM, ARIMA, and Linear Regression baselines across six demographically diverse states.

Result: TimesFM achieved the lowest Mean Squared Error in 86.67% of test cases, with particularly strong performance on minority populations that have sparse historical data.

Conclusion: Pre-trained foundation models like TimesFM show significant potential for enhancing demographic analysis and informing proactive policy interventions without requiring extensive task-specific fine-tuning.

Abstract: Demographic shifts, influenced by globalization, economic conditions,
geopolitical events, and environmental factors, pose significant challenges for
policymakers and researchers. Accurate demographic forecasting is essential for
informed decision-making in areas such as urban planning, healthcare, and
economic policy. This study explores the application of time series foundation
models to predict demographic changes in the United States using datasets from
the U.S. Census Bureau and Federal Reserve Economic Data (FRED). We evaluate
the performance of the Time Series Foundation Model (TimesFM) against
traditional baselines including Long Short-Term Memory (LSTM) networks,
Autoregressive Integrated Moving Average (ARIMA), and Linear Regression. Our
experiments across six demographically diverse states demonstrate that TimesFM
achieves the lowest Mean Squared Error (MSE) in 86.67% of test cases, with
particularly strong performance on minority populations with sparse historical
data. These findings highlight the potential of pre-trained foundation models
to enhance demographic analysis and inform proactive policy interventions
without requiring extensive task-specific fine-tuning.

</details>


### [299] [From Heuristics to Data: Quantifying Site Planning Layout Indicators with Deep Learning and Multi-Modal Data](https://arxiv.org/abs/2508.11723)
*Qian Cao,Jielin Chen,Junchao Zhao,Rudi Stouffs*

Main category: cs.LG

TL;DR: Proposes a Site Planning Layout Indicator (SPLI) system that integrates multi-source data and deep learning to quantitatively analyze urban spatial layouts across five dimensions: building function classification, spatial organization, functional diversity, accessibility, and land use intensity.


<details>
  <summary>Details</summary>
Motivation: Traditional urban site planning relies on experiential judgment and single-source data, limiting systematic quantification of multifunctional layouts and hindering data-driven urban spatial analytics.

Method: Developed a data-driven framework integrating OSM, POI, building morphology, land use, and satellite imagery with deep learning (RGNN and GNN) to create structured indicators across five dimensions: hierarchical building function classification, spatial organization patterns, functional diversity metrics, accessibility measures, and land use intensity ratios.

Result: The SPLI system improves functional classification accuracy and provides a standardized basis for automated urban spatial analytics, successfully addressing data gaps and enabling comprehensive quantitative assessment of urban layouts.

Conclusion: The proposed SPLI framework enables systematic, data-driven quantification of urban spatial layouts, overcoming limitations of traditional planning methods and supporting advanced analytics, inference, and retrieval for urban planning applications.

Abstract: The spatial layout of urban sites shapes land-use efficiency and spatial
organization. Traditional site planning often relies on experiential judgment
and single-source data, limiting systematic quantification of multifunctional
layouts. We propose a Site Planning Layout Indicator (SPLI) system, a
data-driven framework integrating empirical knowledge with heterogeneous
multi-source data to produce structured urban spatial information. The SPLI
supports multimodal spatial data systems for analytics, inference, and
retrieval by combining OpenStreetMap (OSM), Points of Interest (POI), building
morphology, land use, and satellite imagery. It extends conventional metrics
through five dimensions: (1) Hierarchical Building Function Classification,
refining empirical systems into clear hierarchies; (2) Spatial Organization,
quantifying seven layout patterns (e.g., symmetrical, concentric,
axial-oriented); (3) Functional Diversity, transforming qualitative assessments
into measurable indicators using Functional Ratio (FR) and Simpson Index (SI);
(4) Accessibility to Essential Services, integrating facility distribution and
transport networks for comprehensive accessibility metrics; and (5) Land Use
Intensity, using Floor Area Ratio (FAR) and Building Coverage Ratio (BCR) to
assess utilization efficiency. Data gaps are addressed through deep learning,
including Relational Graph Neural Networks (RGNN) and Graph Neural Networks
(GNN). Experiments show the SPLI improves functional classification accuracy
and provides a standardized basis for automated, data-driven urban spatial
analytics.

</details>


### [300] [Causal Structure Learning in Hawkes Processes with Complex Latent Confounder Networks](https://arxiv.org/abs/2508.11727)
*Songyao Jin,Biwei Huang*

Main category: cs.LG

TL;DR: The paper presents a method for identifying latent subprocesses and causal influences in multivariate Hawkes processes, addressing the challenge of partially observed real-world systems.


<details>
  <summary>Details</summary>
Motivation: Real-world systems are often only partially observed with latent subprocesses, which existing methods fail to address as they primarily focus on causal structures among observed subprocesses only.

Method: A two-phase iterative algorithm that alternates between inferring causal relationships among discovered subprocesses and uncovering new latent subprocesses, guided by path-based identifiability conditions derived from discrete-time representation of continuous-time event sequences.

Result: Experiments on synthetic and real-world datasets demonstrate that the method effectively recovers causal structures despite the presence of latent subprocesses.

Conclusion: The proposed approach successfully addresses the challenge of latent subprocesses in multivariate Hawkes processes by establishing identifiability conditions and providing an effective algorithmic solution for causal structure recovery.

Abstract: Multivariate Hawkes process provides a powerful framework for modeling
temporal dependencies and event-driven interactions in complex systems. While
existing methods primarily focus on uncovering causal structures among observed
subprocesses, real-world systems are often only partially observed, with latent
subprocesses posing significant challenges. In this paper, we show that
continuous-time event sequences can be represented by a discrete-time model as
the time interval shrinks, and we leverage this insight to establish necessary
and sufficient conditions for identifying latent subprocesses and the causal
influences. Accordingly, we propose a two-phase iterative algorithm that
alternates between inferring causal relationships among discovered subprocesses
and uncovering new latent subprocesses, guided by path-based conditions that
guarantee identifiability. Experiments on both synthetic and real-world
datasets show that our method effectively recovers causal structures despite
the presence of latent subprocesses.

</details>


### [301] [BRIEF: BRain-Inspired network connection search with Extensive temporal feature Fusion enhances disease classification](https://arxiv.org/abs/2508.11732)
*Xiangxiang Cui,Min Zhao,Dongmei Zhi,Shile Qi,Vince D Calhoun,Jing Sui*

Main category: cs.LG

TL;DR: A brain-inspired feature fusion framework (BRIEF) using neural connection search and Transformer fusion for fMRI-based mental disorder classification, achieving 2.2-12.1% improvement over 21 state-of-the-art models.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning models for fMRI classification have limitations in network architecture determination (relying on experience) and feature space fusion (mostly simple concatenation without mutual learning). The authors were inspired by the human brain's mechanism of updating neural connections through learning.

Method: Proposed BRIEF framework with improved neural network connection search (NCS) strategy and Transformer-based multi-feature fusion. Extracted 4 fMRI temporal representations (TCs, FNC, dFNC, MsDE) to construct encoders. Used modified Q-learning to dynamically optimize NCS as Markov Decision Process. Fused features via Transformer with attention module for interpretability.

Result: BRIEF demonstrated 2.2% to 12.1% improvements compared to 21 state-of-the-art algorithms, reaching AUC of 91.5% for schizophrenia and 78.4% for autism spectrum disorder classification.

Conclusion: This is the first attempt to incorporate brain-inspired reinforcement learning strategy for fMRI-based mental disorder classification, showing significant potential for identifying precise neuroimaging biomarkers.

Abstract: Existing deep learning models for functional MRI-based classification have
limitations in network architecture determination (relying on experience) and
feature space fusion (mostly simple concatenation, lacking mutual learning).
Inspired by the human brain's mechanism of updating neural connections through
learning and decision-making, we proposed a novel BRain-Inspired feature Fusion
(BRIEF) framework, which is able to optimize network architecture automatically
by incorporating an improved neural network connection search (NCS) strategy
and a Transformer-based multi-feature fusion module. Specifically, we first
extracted 4 types of fMRI temporal representations, i.e., time series (TCs),
static/dynamic functional connection (FNC/dFNC), and multi-scale dispersion
entropy (MsDE), to construct four encoders. Within each encoder, we employed a
modified Q-learning to dynamically optimize the NCS to extract high-level
feature vectors, where the NCS is formulated as a Markov Decision Process.
Then, all feature vectors were fused via a Transformer, leveraging both
stable/time-varying connections and multi-scale dependencies across different
brain regions to achieve the final classification. Additionally, an attention
module was embedded to improve interpretability. The classification performance
of our proposed BRIEF was compared with 21 state-of-the-art models by
discriminating two mental disorders from healthy controls: schizophrenia (SZ,
n=1100) and autism spectrum disorder (ASD, n=1550). BRIEF demonstrated
significant improvements of 2.2% to 12.1% compared to 21 algorithms, reaching
an AUC of 91.5% - 0.6% for SZ and 78.4% - 0.5% for ASD, respectively. This is
the first attempt to incorporate a brain-inspired, reinforcement learning
strategy to optimize fMRI-based mental disorder classification, showing
significant potential for identifying precise neuroimaging biomarkers.

</details>


### [302] [Scalable Geospatial Data Generation Using AlphaEarth Foundations Model](https://arxiv.org/abs/2508.11739)
*Luc Houriez,Sebastian Pilarski,Behzad Vahedi,Ali Ahmadalipour,Teo Honda Scully,Nicholas Aflitto,David Andre,Caroline Jaffe,Martha Wedner,Rich Mazzola,Josh Jeffery,Ben Messinger,Sage McGinley-Smith,Sarah Russell*

Main category: cs.LG

TL;DR: Using Google DeepMind's AlphaEarth Foundations to extend geospatial labeled datasets beyond their original regions with basic models like random forests, achieving 81% and 73% accuracy on vegetation classification.


<details>
  <summary>Details</summary>
Motivation: High-quality labeled geospatial datasets are often limited to specific geographic regions where data was collected, creating gaps in global coverage.

Method: Leveraging AlphaEarth Foundations (AEF) as input representation and using basic models (random forests, logistic regression) to extend geospatial datasets beyond initial regions.

Result: Achieved 81% classification accuracy on USA validation set and 73% on Canada validation set for EvtPhys vegetation type classification, with qualitative alignment to ground truth.

Conclusion: Even basic models can effectively extend geospatial datasets using AEF representations, demonstrating practical applicability for global geospatial analysis despite limitations.

Abstract: High-quality labeled geospatial datasets are essential for extracting
insights and understanding our planet. Unfortunately, these datasets often do
not span the entire globe and are limited to certain geographic regions where
data was collected. Google DeepMind's recently released AlphaEarth Foundations
(AEF) provides an information-dense global geospatial representation designed
to serve as a useful input across a wide gamut of tasks. In this article we
propose and evaluate a methodology which leverages AEF to extend geospatial
labeled datasets beyond their initial geographic regions. We show that even
basic models like random forests or logistic regression can be used to
accomplish this task. We investigate a case study of extending LANDFIRE's
Existing Vegetation Type (EVT) dataset beyond the USA into Canada at two levels
of granularity: EvtPhys (13 classes) and EvtGp (80 classes). Qualitatively, for
EvtPhys, model predictions align with ground truth. Trained models achieve 81%
and 73% classification accuracy on EvtPhys validation sets in the USA and
Canada, despite discussed limitations.

</details>


### [303] [Fed-Meta-Align: A Similarity-Aware Aggregation and Personalization Pipeline for Federated TinyML on Heterogeneous Data](https://arxiv.org/abs/2508.11794)
*Hemanth Macharla,Mayukha Pal*

Main category: cs.LG

TL;DR: Fed-Meta-Align is a novel four-phase federated learning framework that addresses non-IID data challenges in IoT fault classification through meta-initialization and dual-criterion aggregation, achieving 91.27% average accuracy.


<details>
  <summary>Details</summary>
Motivation: Real-time fault classification in resource-constrained IoT devices is critical for industrial safety, but standard Federated Learning fails with non-IID data, leading to model divergence.

Method: Four-phase framework: 1) foundational model training on public dataset, 2) serial meta-initialization on IoT subset for heterogeneity-aware initialization, 3) parallel FL with dual-criterion aggregation (local performance + cosine similarity), 4) on-device personalization for specialized experts.

Result: Achieves 91.27% average test accuracy across heterogeneous IoT devices, outperforming personalized FedAvg by 3.87% and FedProx by 3.37% on electrical and mechanical fault datasets.

Conclusion: The multi-stage approach of sequenced initialization and adaptive aggregation provides a robust pathway for deploying high-performance intelligence on diverse TinyML networks.

Abstract: Real-time fault classification in resource-constrained Internet of Things
(IoT) devices is critical for industrial safety, yet training robust models in
such heterogeneous environments remains a significant challenge. Standard
Federated Learning (FL) often fails in the presence of non-IID data, leading to
model divergence. This paper introduces Fed-Meta-Align, a novel four-phase
framework designed to overcome these limitations through a sophisticated
initialization and training pipeline. Our process begins by training a
foundational model on a general public dataset to establish a competent
starting point. This model then undergoes a serial meta-initialization phase,
where it sequentially trains on a subset of IOT Device data to learn a
heterogeneity-aware initialization that is already situated in a favorable
region of the loss landscape. This informed model is subsequently refined in a
parallel FL phase, which utilizes a dual-criterion aggregation mechanism that
weights for IOT devices updates based on both local performance and cosine
similarity alignment. Finally, an on-device personalization phase adapts the
converged global model into a specialized expert for each IOT Device.
Comprehensive experiments demonstrate that Fed-Meta-Align achieves an average
test accuracy of 91.27% across heterogeneous IOT devices, outperforming
personalized FedAvg and FedProx by up to 3.87% and 3.37% on electrical and
mechanical fault datasets, respectively. This multi-stage approach of sequenced
initialization and adaptive aggregation provides a robust pathway for deploying
high-performance intelligence on diverse TinyML networks.

</details>


### [304] [Uncalibrated Reasoning: GRPO Induces Overconfidence for Stochastic Outcomes](https://arxiv.org/abs/2508.11800)
*Michael Bereket,Jure Leskovec*

Main category: cs.LG

TL;DR: RL methods for language models in stochastic domains: GRPO causes overconfidence in probability predictions, while PPO and RLOO yield well-calibrated models. Removing group normalization in GRPO fixes the issue.


<details>
  <summary>Details</summary>
Motivation: To examine if current RL methods are effective at optimizing language models in verifiable domains with stochastic outcomes like scientific experiments, beyond deterministic domains like mathematics.

Method: Applied Group Relative Policy Optimization (GRPO), Proximal Policy Optimization (PPO), and REINFORCE Leave-One-Out (RLOO) to synthetic data and real-world biological experiments to evaluate their performance in stochastic domains.

Result: GRPO induced overconfident probability predictions for binary stochastic outcomes, while PPO and RLOO yielded well-calibrated models. Removing group standard normalization in GRPO fixed its miscalibration.

Conclusion: Provides evidence against using standard normalization in GRPO and helps enable RL applications for reasoning language models beyond deterministic domains into stochastic environments like scientific experiments.

Abstract: Reinforcement learning (RL) has proven remarkably effective at improving the
accuracy of language models in verifiable and deterministic domains like
mathematics. Here, we examine if current RL methods are also effective at
optimizing language models in verifiable domains with stochastic outcomes, like
scientific experiments. Through applications to synthetic data and real-world
biological experiments, we demonstrate that Group Relative Policy Optimization
(GRPO) induces overconfident probability predictions for binary stochastic
outcomes, while Proximal Policy Optimization (PPO) and REINFORCE Leave-One-Out
(RLOO) yield well-calibrated models. We show that removing group standard
normalization in GRPO fixes its miscalibration and provide a theoretical
explanation for why normalization causes overconfidence. Our results provide
new evidence against the use of standard normalization in GRPO and help pave
the way for applications of RL for reasoning language models beyond
deterministic domains.

</details>


### [305] [FairTabGen: Unifying Counterfactual and Causal Fairness in Synthetic Tabular Data Generation](https://arxiv.org/abs/2508.11810)
*Nitish Nagesh,Salar Shakibhamedan,Mahdi Bagheri,Ziyu Wang,Nima TaheriNejad,Axel Jantsch,Amir M. Rahmani*

Main category: cs.LG

TL;DR: FairTabGen is an LLM-based framework that generates fair synthetic tabular data with improved counterfactual and causal fairness while maintaining utility, outperforming existing methods using only 20% of original data.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of improving counterfactual and causal fairness in synthetic tabular data generation while preserving utility, especially in privacy-sensitive and data-scarce settings.

Method: Uses in-context learning, prompt refinement, and fairness-aware data curation to integrate multiple fairness definitions into both generation and evaluation pipelines.

Result: Outperforms state-of-the-art GAN-based and LLM-based methods with up to 10% improvements on fairness metrics (demographic parity, path-specific causal effects) while retaining statistical utility, using less than 20% of original data.

Conclusion: Demonstrates a principled and practical approach for generating fair and useful synthetic tabular data that works efficiently in low-data regimes.

Abstract: Generating synthetic data is crucial in privacy-sensitive, data-scarce
settings, especially for tabular datasets widely used in real-world
applications. A key challenge is improving counterfactual and causal fairness,
while preserving high utility. We present FairTabGen, a fairness-aware large
language model-based framework for tabular synthetic data generation. We
integrate multiple fairness definitions including counterfactual and causal
fairness into both its generation and evaluation pipelines. We use in-context
learning, prompt refinement, and fairness-aware data curation to balance
fairness and utility. Across diverse datasets, our method outperforms
state-of-the-art GAN-based and LLM-based methods, achieving up to 10%
improvements on fairness metrics such as demographic parity and path-specific
causal effects while retaining statistical utility. Remarkably, it achieves
these gains using less than 20% of the original data, highlighting its
efficiency in low-data regimes. These results demonstrate a principled and
practical approach for generating fair and useful synthetic tabular data.

</details>


### [306] [Combinations of Fast Activation and Trigonometric Functions in Kolmogorov-Arnold Networks](https://arxiv.org/abs/2508.11876)
*Hoang-Thang Ta,Duy-Quy Thai,Phuong-Linh Tran-Thi*

Main category: cs.LG

TL;DR: Proposes using GPU-friendly functions (ReLU, trigonometric) instead of polynomial functions in Kolmogorov-Arnold Networks to improve computational efficiency while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Existing KANs use polynomial functions like B-splines and RBFs that are not well-supported by GPUs and are less popular, limiting computational efficiency.

Method: Replace polynomial basis functions with fast computational functions (ReLU, sin, cos, arctan) in Kolmogorov-Arnold Network structure to enhance GPU compatibility and efficiency.

Result: Experimental results show the proposed function combinations maintain competitive performance while offering potential improvements in training time and generalization.

Conclusion: GPU-friendly function combinations (ReLU and trigonometric functions) can effectively replace polynomial functions in KANs, providing computational efficiency benefits without sacrificing performance.

Abstract: For years, many neural networks have been developed based on the
Kolmogorov-Arnold Representation Theorem (KART), which was created to address
Hilbert's 13th problem. Recently, relying on KART, Kolmogorov-Arnold Networks
(KANs) have attracted attention from the research community, stimulating the
use of polynomial functions such as B-splines and RBFs. However, these
functions are not fully supported by GPU devices and are still considered less
popular. In this paper, we propose the use of fast computational functions,
such as ReLU and trigonometric functions (e.g., ReLU, sin, cos, arctan), as
basis components in Kolmogorov-Arnold Networks (KANs). By integrating these
function combinations into the network structure, we aim to enhance
computational efficiency. Experimental results show that these combinations
maintain competitive performance while offering potential improvements in
training time and generalization.

</details>


### [307] [PCA- and SVM-Grad-CAM for Convolutional Neural Networks: Closed-form Jacobian Expression](https://arxiv.org/abs/2508.11880)
*Yuto Omae*

Main category: cs.LG

TL;DR: Proposes PCA-Grad-CAM and SVM-Grad-CAM methods to visualize attention regions in PCA and SVM layers within CNNs, enabling white-box interpretation for models with limited training data.


<details>
  <summary>Details</summary>
Motivation: Traditional Grad-CAM cannot visualize attention regions in PCA and SVM layers integrated into CNNs for improved performance with limited training samples, creating a need for specialized visualization methods.

Method: Develops closed-form Jacobian solutions for partial derivatives from the last convolutional layer to PCA and SVM layers, enabling exact computation of attention maps for PCA feature vectors and SVM classifier layers.

Result: Successfully derived exact closed-form Jacobian formulations and demonstrated visualization results on several major datasets, showing effective attention region generation for PCA and SVM layers.

Conclusion: The proposed PCA-Grad-CAM and SVM-Grad-CAM methods enable effective visualization of attention regions in PCA and SVM layers within CNNs, facilitating white-box interpretation for models using these components with limited training data.

Abstract: Convolutional Neural Networks (CNNs) are an effective approach for
classification tasks, particularly when the training dataset is large. Although
CNNs have long been considered a black-box classification method, they can be
used as a white-box method through visualization techniques such as Grad-CAM.
When training samples are limited, incorporating a Principal Component Analysis
(PCA) layer and/or a Support Vector Machine (SVM) classifier into a CNN can
effectively improve classification performance. However, traditional Grad-CAM
cannot be directly applied to PCA and/or SVM layers. It is important to
generate attention regions for PCA and/or SVM layers in CNNs to facilitate the
development of white-box methods. Therefore, we propose ``PCA-Grad-CAM'', a
method for visualizing attention regions in PCA feature vectors, and
``SVM-Grad-CAM'', a method for visualizing attention regions in an SVM
classifier layer. To complete our methods analytically, it is necessary to
solve the closed-form Jacobian consisting of partial derivatives from the last
convolutional layer to the PCA and/or SVM layers. In this paper, we present the
exact closed-form Jacobian and the visualization results of our methods applied
to several major datasets.

</details>


### [308] [ENA: Efficient N-dimensional Attention](https://arxiv.org/abs/2508.11921)
*Yibo Zhong*

Main category: cs.LG

TL;DR: ENA combines linear recurrence with tiled high-order sliding window attention to efficiently model ultra-long high-dimensional data, outperforming Transformers.


<details>
  <summary>Details</summary>
Motivation: Transformer architectures are inefficient for modeling long sequences of high-order data, requiring more efficient alternatives that can handle 1D to ND data effectively.

Method: Investigates scanning strategies and attention-hybrid architectures, focusing on combining linear recurrence (for global compression) with tiled high-order sliding window attention (for local modeling).

Result: Scanning provides limited benefits, while attention-hybrid models show promise. Tiled high-order sliding window attention proves efficient both theoretically and practically.

Conclusion: ENA offers a simple yet effective framework for ultra-long high-order data modeling by combining global information compression through linear recurrence with strict local modeling via sliding window attention.

Abstract: Efficient modeling of long sequences of high-order data requires a more
efficient architecture than Transformer. In this paper, we investigate two key
aspects of extending linear recurrent models, especially those originally
designed for language modeling, to high-order data (1D to ND): scanning
strategies and attention-hybrid architectures. Empirical results suggest that
scanning provides limited benefits, while attention-hybrid models yield
promising results. Focusing on the latter, we further evaluate types of
attention and find that tiled high-order sliding window attention (SWA) is
efficient in both theory and practice. We term the resulting hybrid
architecture of linear recurrence and high-order SWA as Efficient N-dimensional
Attention (ENA). We then conduct several experiments to demonstrate its
effectiveness. The intuition behind ENA is that linear recurrence compresses
global information into a state, while SWA complements it by enforcing strict
local modeling. Together, they form a simple framework that offers a promising
and practical solution for ultra-long high-order data modeling.

</details>


### [309] [Scale-Disentangled spatiotemporal Modeling for Long-term Traffic Emission Forecasting](https://arxiv.org/abs/2508.11923)
*Yan Wu,Lihong Pei,Yukai Han,Yang Cao,Yu Kang,Yanlong Zhao*

Main category: cs.LG

TL;DR: Proposes SDSTM framework for long-term traffic emission forecasting using scale-disentangled spatio-temporal modeling to address cascading error amplification in traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional spatiotemporal graph models suffer from cascading error amplification due to multi-scale entanglement of traffic emissions across time and space during long-term inference.

Method: Uses dual-stream feature decomposition based on Koopman lifting operator to separate scales, gated wavelet decomposition for predictability boundaries, and cross-term loss for independence constraints to refine predictions and suppress interference.

Result: Extensive experiments on Xi'an's Second Ring Road traffic emission dataset demonstrate state-of-the-art performance.

Conclusion: SDSTM framework effectively addresses long-term traffic emission forecasting challenges by disentangling multi-scale spatiotemporal dependencies and maintaining feature independence while enhancing prediction accuracy.

Abstract: Long-term traffic emission forecasting is crucial for the comprehensive
management of urban air pollution. Traditional forecasting methods typically
construct spatiotemporal graph models by mining spatiotemporal dependencies to
predict emissions. However, due to the multi-scale entanglement of traffic
emissions across time and space, these spatiotemporal graph modeling method
tend to suffer from cascading error amplification during long-term inference.
To address this issue, we propose a Scale-Disentangled Spatio-Temporal Modeling
(SDSTM) framework for long-term traffic emission forecasting. It leverages the
predictability differences across multiple scales to decompose and fuse
features at different scales, while constraining them to remain independent yet
complementary. Specifically, the model first introduces a dual-stream feature
decomposition strategy based on the Koopman lifting operator. It lifts the
scale-coupled spatiotemporal dynamical system into an infinite-dimensional
linear space via Koopman operator, and delineates the predictability boundary
using gated wavelet decomposition. Then a novel fusion mechanism is
constructed, incorporating a dual-stream independence constraint based on
cross-term loss to dynamically refine the dual-stream prediction results,
suppress mutual interference, and enhance the accuracy of long-term traffic
emission prediction. Extensive experiments conducted on a road-level traffic
emission dataset within Xi'an's Second Ring Road demonstrate that the proposed
model achieves state-of-the-art performance.

</details>


### [310] [An Improved Algorithm for Adversarial Linear Contextual Bandits via Reduction](https://arxiv.org/abs/2508.11931)
*Tim van Erven,Jack Mayo,Julia Olkhovskaya,Chen-Yu Wei*

Main category: cs.LG

TL;DR: Efficient algorithm for linear contextual bandits with adversarial losses and stochastic action sets, achieving poly(d)√T regret in polynomial time without requiring context distribution knowledge or simulator access.


<details>
  <summary>Details</summary>
Motivation: Addresses the open question by Liu et al. (2023) on whether poly(d)√T regret can be achieved in polynomial time independent of the number of actions, particularly for combinatorial bandits where prior algorithms couldn't achieve o(T) regret in polynomial time.

Method: Reduces the setting to misspecification-robust adversarial linear bandits with fixed action sets. The algorithm runs in poly(d,C,T) time without knowledge of context distribution or access to context simulator.

Result: Achieves Õ(min{d²√T, √(d³T log K)}) regret without simulator, and improved Õ(d√L*) regret when simulator is available, where L* is the cumulative loss of the best policy.

Conclusion: Resolves the open problem by providing the first polynomial-time algorithm achieving poly(d)√T regret for combinatorial bandits with adversarial losses and stochastic action sets, representing a significant advancement in the field.

Abstract: We present an efficient algorithm for linear contextual bandits with
adversarial losses and stochastic action sets. Our approach reduces this
setting to misspecification-robust adversarial linear bandits with fixed action
sets. Without knowledge of the context distribution or access to a context
simulator, the algorithm achieves $\tilde{O}(\min\{d^2\sqrt{T}, \sqrt{d^3T\log
K}\})$ regret and runs in $\text{poly}(d,C,T)$ time, where $d$ is the feature
dimension, $C$ is an upper bound on the number of linear constraints defining
the action set in each round, $K$ is an upper bound on the number of actions in
each round, and $T$ is number of rounds. This resolves the open question by Liu
et al. (2023) on whether one can obtain $\text{poly}(d)\sqrt{T}$ regret in
polynomial time independent of the number of actions. For the important class
of combinatorial bandits with adversarial losses and stochastic action sets
where the action sets can be described by a polynomial number of linear
constraints, our algorithm is the first to achieve $\text{poly}(d)\sqrt{T}$
regret in polynomial time, while no prior algorithm achieves even $o(T)$ regret
in polynomial time to our knowledge. When a simulator is available, the regret
bound can be improved to $\tilde{O}(d\sqrt{L^\star})$, where $L^\star$ is the
cumulative loss of the best policy.

</details>


### [311] [M3OOD: Automatic Selection of Multimodal OOD Detectors](https://arxiv.org/abs/2508.11936)
*Yuehan Qin,Li Li,Defu Cao,Tiankai Yang,Yue Zhao*

Main category: cs.LG

TL;DR: M3OOD is a meta-learning framework that automatically selects optimal out-of-distribution (OOD) detectors for multimodal data by learning from historical model behaviors and using multimodal embeddings with handcrafted meta-features.


<details>
  <summary>Details</summary>
Motivation: Current OOD detection methods are designed for specific distribution shifts, but no single detector works well across all scenarios. Manual selection is difficult due to the unsupervised nature of OOD detection and the impracticality of testing models on new unseen data.

Method: Meta-learning framework that combines multimodal embeddings with handcrafted meta-features capturing distributional and cross-modal characteristics. Learns from historical performance across diverse multimodal benchmarks to recommend suitable detectors for new distribution shifts.

Result: M3OOD consistently outperforms 10 competitive baselines across 12 test scenarios with minimal computational overhead.

Conclusion: The proposed meta-learning approach effectively addresses the challenge of automatic OOD detector selection in multimodal settings, demonstrating superior performance and practical efficiency.

Abstract: Out-of-distribution (OOD) robustness is a critical challenge for modern
machine learning systems, particularly as they increasingly operate in
multimodal settings involving inputs like video, audio, and sensor data.
Currently, many OOD detection methods have been proposed, each with different
designs targeting various distribution shifts. A single OOD detector may not
prevail across all the scenarios; therefore, how can we automatically select an
ideal OOD detection model for different distribution shifts? Due to the
inherent unsupervised nature of the OOD detection task, it is difficult to
predict model performance and find a universally Best model. Also,
systematically comparing models on the new unseen data is costly or even
impractical. To address this challenge, we introduce M3OOD, a
meta-learning-based framework for OOD detector selection in multimodal
settings. Meta learning offers a solution by learning from historical model
behaviors, enabling rapid adaptation to new data distribution shifts with
minimal supervision. Our approach combines multimodal embeddings with
handcrafted meta-features that capture distributional and cross-modal
characteristics to represent datasets. By leveraging historical performance
across diverse multimodal benchmarks, M3OOD can recommend suitable detectors
for a new data distribution shift. Experimental evaluation demonstrates that
M3OOD consistently outperforms 10 competitive baselines across 12 test
scenarios with minimal computational overhead.

</details>


### [312] [Extending Straight-Through Estimation for Robust Neural Networks on Analog CIM Hardware](https://arxiv.org/abs/2508.11940)
*Yuannuo Feng,Wenyong Zhou,Yuexi Lyu,Yixiang Zhang,Zhengwu Liu,Ngai Wong,Wang Kang*

Main category: cs.LG

TL;DR: Extended STE framework enables noise-aware training for analog CIM systems by decoupling forward noise simulation from backward gradients, achieving significant accuracy improvements and efficiency gains.


<details>
  <summary>Details</summary>
Motivation: Analog CIM architectures offer energy efficiency but suffer from complex hardware noise that existing noise-aware training methods fail to capture accurately due to reliance on idealized differentiable noise models.

Method: Decouple forward noise simulation from backward gradient computation using an extended Straight-Through Estimator framework, enabling use of more accurate but computationally intractable noise models while preserving gradient directional information.

Result: Achieves up to 5.3% accuracy improvement on image classification, 0.72 perplexity reduction on text generation, 2.2x training speedup, and 37.9% lower peak memory usage compared to standard methods.

Conclusion: The extended STE framework provides an effective solution for noise-aware training in analog CIM systems, enabling accurate noise modeling while maintaining computational tractability and optimization stability.

Abstract: Analog Compute-In-Memory (CIM) architectures promise significant energy
efficiency gains for neural network inference, but suffer from complex
hardware-induced noise that poses major challenges for deployment. While
noise-aware training methods have been proposed to address this issue, they
typically rely on idealized and differentiable noise models that fail to
capture the full complexity of analog CIM hardware variations. Motivated by the
Straight-Through Estimator (STE) framework in quantization, we decouple forward
noise simulation from backward gradient computation, enabling noise-aware
training with more accurate but computationally intractable noise modeling in
analog CIM systems. We provide theoretical analysis demonstrating that our
approach preserves essential gradient directional information while maintaining
computational tractability and optimization stability. Extensive experiments
show that our extended STE framework achieves up to 5.3% accuracy improvement
on image classification, 0.72 perplexity reduction on text generation,
2.2$\times$ speedup in training time, and 37.9% lower peak memory usage
compared to standard noise-aware training methods.

</details>


### [313] [Learning Marked Temporal Point Process Explanations based on Counterfactual and Factual Reasoning](https://arxiv.org/abs/2508.11943)
*Sishun Liu,Ke Deng,Xiuzhen Zhang,Yan Wang*

Main category: cs.LG

TL;DR: CFF combines counterfactual and factual explanations to identify minimal rational subsets of historical events that maintain MTPP prediction accuracy, outperforming baseline methods in quality and efficiency.


<details>
  <summary>Details</summary>
Motivation: Neural MTPP models are used in high-stakes applications but lack trustworthy explanations. Existing explanation methods (counterfactual or factual alone) can produce irrational explanations for event sequence predictions.

Method: Proposes CFF (Counterfactual and Factual Explainer) that combines both explanation types with deliberately designed techniques to identify minimal event subsets that preserve prediction accuracy comparable to full history.

Result: Experiments show CFF achieves superior explanation quality and processing efficiency compared to baseline methods, demonstrating correctness and effectiveness.

Conclusion: Combining counterfactual and factual explanations provides more rational and minimal explanations for MTPP predictions, enhancing trustworthiness in high-stakes applications.

Abstract: Neural network-based Marked Temporal Point Process (MTPP) models have been
widely adopted to model event sequences in high-stakes applications, raising
concerns about the trustworthiness of outputs from these models. This study
focuses on Explanation for MTPP, aiming to identify the minimal and rational
explanation, that is, the minimum subset of events in history, based on which
the prediction accuracy of MTPP matches that based on full history to a great
extent and better than that based on the complement of the subset. This study
finds that directly defining Explanation for MTPP as counterfactual explanation
or factual explanation can result in irrational explanations. To address this
issue, we define Explanation for MTPP as a combination of counterfactual
explanation and factual explanation. This study proposes Counterfactual and
Factual Explainer for MTPP (CFF) to solve Explanation for MTPP with a series of
deliberately designed techniques. Experiments demonstrate the correctness and
superiority of CFF over baselines regarding explanation quality and processing
efficiency.

</details>


### [314] [Set-Valued Transformer Network for High-Emission Mobile Source Identification](https://arxiv.org/abs/2508.11976)
*Yunning Cao,Lihong Pei,Jian Guo,Yang Cao,Yu Kang,Yanlong Zhao*

Main category: cs.LG

TL;DR: Proposes Set-Valued Transformer Network (SVTN) to identify high-emission vehicles by addressing long-tailed data distribution and nonlinear emission patterns, achieving 9.5% reduction in missed detection rate.


<details>
  <summary>Details</summary>
Motivation: High-emission vehicle identification is crucial for urban pollution regulation, but practical data shows long-tailed distribution with few high-emission samples, making feature extraction difficult. Nonlinear emission states and lack of prior knowledge further complicate model construction.

Method: Uses transformer to measure temporal similarity of micro-trip condition variations, mapping high-dimensional emission data to low-dimensional feature space. Then applies set-valued identification algorithm to probabilistically model feature-label relationships for classification.

Result: Experimental results on 2020 Hefei diesel vehicle data show 9.5% reduction in missed detection rate for high-emission vehicles compared to transformer baseline.

Conclusion: SVTN effectively addresses long-tailed distribution challenges and improves detection accuracy for high-emission mobile pollution sources through comprehensive discriminative feature learning.

Abstract: Identifying high-emission vehicles is a crucial step in regulating urban
pollution levels and formulating traffic emission reduction strategies.
However, in practical monitoring data, the proportion of high-emission state
data is significantly lower compared to normal emission states. This
characteristic long-tailed distribution severely impedes the extraction of
discriminative features for emission state identification during data mining.
Furthermore, the highly nonlinear nature of vehicle emission states and the
lack of relevant prior knowledge also pose significant challenges to the
construction of identification models.To address the aforementioned issues, we
propose a Set-Valued Transformer Network (SVTN) to achieve comprehensive
learning of discriminative features from high-emission samples, thereby
enhancing detection accuracy. Specifically, this model first employs the
transformer to measure the temporal similarity of micro-trip condition
variations, thus constructing a mapping rule that projects the original
high-dimensional emission data into a low-dimensional feature space. Next, a
set-valued identification algorithm is used to probabilistically model the
relationship between the generated feature vectors and their labels, providing
an accurate metric criterion for the classification algorithm. To validate the
effectiveness of our proposed approach, we conducted extensive experiments on
the diesel vehicle monitoring data of Hefei city in 2020. The results
demonstrate that our method achieves a 9.5\% reduction in the missed detection
rate for high-emission vehicles compared to the transformer-based baseline,
highlighting its superior capability in accurately identifying high-emission
mobile pollution sources.

</details>


### [315] [Efficient Modular Learning through Naive LoRA Summation: Leveraging Orthogonality in High-Dimensional Models](https://arxiv.org/abs/2508.11985)
*Zhanhao Cao,Clement Truong,Andrew Lizarraga*

Main category: cs.LG

TL;DR: LoRA adapters trained on disjoint domains can be combined through simple addition with performance comparable to merged-data fine-tuning, requiring no additional training.


<details>
  <summary>Details</summary>
Motivation: To enable efficient combination of independently trained parameter-efficient fine-tuning modules (LoRA) across different domains without additional training.

Method: Train LoRA adapters (rank 4, alpha=64) on GPT-2 Small for three QA domains (math, medicine, finance) and combine them through naive summation of parameter deltas.

Result: Math+Medicine combination improved perplexity by -9.10% relative to merged-data fine-tuning, while Math+Finance and Finance+Medicine showed +4.54% and +27.56% changes respectively. RMS cosine similarity between LoRA deltas correlates linearly with perplexity changes.

Conclusion: LoRA modules can be efficiently combined via addition with performance comparable to merged-data training, while revealing interference patterns in higher-order compositions through cosine similarity analysis.

Abstract: Recent advances in large language models are driven by scale, while
parameter-efficient fine-tuning (PEFT) enables updating only a small fraction
of parameters. Low-Rank Adaptation (LoRA) stores parameter deltas as the
product of two small matrices, which makes them natural building blocks that
can be composed. Motivated by the superposition principle, we hypothesize that
independently trained LoRA modules on disjoint domains are approximately
orthogonal and can be combined by simple addition. Using GPT-2 Small (117M)
with LoRA rank 4 and alpha=64, we train adapters for three QA domains (math,
medicine, finance). In pairwise tests, adding Math+Medicine adapters improves
perplexity by -9.10% relative to merged-data fine-tuning, while Math+Finance
and Finance+Medicine change by +4.54% and +27.56%, respectively. Across
combinations, the RMS cosine similarity between LoRA deltas correlates
positively and approximately linearly with the change in perplexity. Naive
summation requires no additional training, can be applied in seconds, and
achieves performance comparable to models trained on merged data, while
clarifying when interference appears in higher-order compositions.

</details>


### [316] [Universal Learning of Nonlinear Dynamics](https://arxiv.org/abs/2508.11990)
*Evan Dogariu,Anand Brahmbhatt,Elad Hazan*

Main category: cs.LG

TL;DR: A spectral filtering algorithm for learning marginally stable nonlinear dynamical systems with vanishing prediction error and novel learnability rates.


<details>
  <summary>Details</summary>
Motivation: To address the fundamental problem of learning unknown nonlinear dynamical systems that are marginally stable, which is challenging due to the system's stability properties and potential noise.

Method: Develops a spectral filtering algorithm that learns mappings from past observations to future states using spectral representation, incorporating techniques from online convex optimization and extending to handle asymmetric dynamics and noise correction.

Result: Achieves vanishing prediction error for nonlinear dynamical systems with finitely many marginally stable modes, with rates governed by a novel quantitative control-theoretic notion of learnability.

Conclusion: The proposed spectral filtering method significantly generalizes previous approaches to handle more complex dynamical systems and provides theoretical guarantees for learning performance.

Abstract: We study the fundamental problem of learning a marginally stable unknown
nonlinear dynamical system. We describe an algorithm for this problem, based on
the technique of spectral filtering, which learns a mapping from past
observations to the next based on a spectral representation of the system.
Using techniques from online convex optimization, we prove vanishing prediction
error for any nonlinear dynamical system that has finitely many marginally
stable modes, with rates governed by a novel quantitative control-theoretic
notion of learnability. The main technical component of our method is a new
spectral filtering algorithm for linear dynamical systems, which incorporates
past observations and applies to general noisy and marginally stable systems.
This significantly generalizes the original spectral filtering algorithm to
both asymmetric dynamics as well as incorporating noise correction, and is of
independent interest.

</details>


### [317] [FedUHD: Unsupervised Federated Learning using Hyperdimensional Computing](https://arxiv.org/abs/2508.12021)
*You Hak Lee,Xiaofan Yu,Quanling Zhao,Flavio Ponzina,Tajana Rosing*

Main category: cs.LG

TL;DR: FedUHD is the first unsupervised federated learning framework using Hyperdimensional Computing, achieving massive speedup, energy efficiency, and noise robustness while handling non-iid data challenges.


<details>
  <summary>Details</summary>
Motivation: Address challenges in unsupervised federated learning including non-iid data distribution, high computational/communication costs, and vulnerability to noise, while eliminating dependency on deep neural networks.

Method: Proposes FedUHD framework with HDC-based designs: client-side kNN cluster hypervector removal for outlier elimination, and server-side weighted HDC aggregation for balancing non-iid data distribution.

Result: Achieves up to 173.6x speedup, 612.7x better energy efficiency, 271x lower communication cost, 15.50% higher accuracy on average, and superior noise robustness compared to state-of-the-art NN-based approaches.

Conclusion: HDC-based FedUHD provides an efficient, robust, and accurate solution for unsupervised federated learning with significant advantages over traditional neural network approaches.

Abstract: Unsupervised federated learning (UFL) has gained attention as a
privacy-preserving, decentralized machine learning approach that eliminates the
need for labor-intensive data labeling. However, UFL faces several challenges
in practical applications: (1) non-independent and identically distributed
(non-iid) data distribution across devices, (2) expensive computational and
communication costs at the edge, and (3) vulnerability to communication noise.
Previous UFL approaches have relied on deep neural networks (NN), which
introduce substantial overhead in both computation and communication. In this
paper, we propose FedUHD, the first UFL framework based on Hyperdimensional
Computing (HDC). HDC is a brain-inspired computing scheme with lightweight
training and inference operations, much smaller model size, and robustness to
communication noise. FedUHD introduces two novel HDC-based designs to improve
UFL performance. On the client side, a kNN-based cluster hypervector removal
method addresses non-iid data samples by eliminating detrimental outliers. On
the server side, a weighted HDC aggregation technique balances the non-iid data
distribution across clients. Our experiments demonstrate that FedUHD achieves
up to 173.6x and 612.7x better speedup and energy efficiency, respectively, in
training, up to 271x lower communication cost, and 15.50% higher accuracy on
average across diverse settings, along with superior robustness to various
types of noise compared to state-of-the-art NN-based UFL approaches.

</details>


### [318] [Fairness Regularization in Federated Learning](https://arxiv.org/abs/2508.12042)
*Zahra Kharaghani,Ali Dadras,Tommy Löfstedt*

Main category: cs.LG

TL;DR: This paper analyzes fairness methods in Federated Learning, introduces FairGrad variants for performance equity, and shows they improve both fairness and model performance in heterogeneous data settings.


<details>
  <summary>Details</summary>
Motivation: Federated Learning faces fairness issues due to data heterogeneity causing disproportionate client impacts on the global model, but existing fairness methods' effectiveness remains unclear.

Method: Focuses on performance equitable fairness methods that regularize client losses, introduces FairGrad and FairGrad* (gradient variance regularization variants), and provides theoretical analysis of connections between fairness approaches.

Result: FairGrad and FairGrad* improve both fairness (minimizing performance differences across clients) and overall model performance in heterogeneous data settings.

Conclusion: The proposed FairGrad variants effectively address performance equity in FL, demonstrating superior results compared to existing fairness-aware methods in heterogeneous environments.

Abstract: Federated Learning (FL) has emerged as a vital paradigm in modern machine
learning that enables collaborative training across decentralized data sources
without exchanging raw data. This approach not only addresses privacy concerns
but also allows access to overall substantially larger and potentially more
diverse datasets, without the need for centralized storage or hardware
resources. However, heterogeneity in client data may cause certain clients to
have disproportionate impacts on the global model, leading to disparities in
the clients' performances. Fairness, therefore, becomes a crucial concern in FL
and can be addressed in various ways. However, the effectiveness of existing
fairness-aware methods, particularly in heterogeneous data settings, remains
unclear, and the relationships between different approaches are not well
understood. In this work, we focus on performance equitable fairness, which
aims to minimize differences in performance across clients. We restrict our
study to fairness-aware methods that explicitly regularize client losses,
evaluating both existing and newly proposed approaches. We identify and
theoretically explain connections between the investigated fairness methods,
and empirically show that FairGrad (approximate) and FairGrad* (exact) (two
variants of a gradient variance regularization method introduced here for
performance equitable fairness) improve both fairness and overall model
performance in heterogeneous data settings.

</details>


### [319] [VARAN: Variational Inference for Self-Supervised Speech Models Fine-Tuning on Downstream Tasks](https://arxiv.org/abs/2508.12061)
*Daria Diatlova,Nikita Balagansky,Alexander Varlamov,Egor Spirin*

Main category: cs.LG

TL;DR: VARAN is a dynamic layer aggregation framework that adaptively weights features from different layers based on individual inputs, outperforming static aggregation methods in speech tasks.


<details>
  <summary>Details</summary>
Motivation: Conventional layer aggregation methods in fine-tuned self-supervised speech models suffer from information bottlenecks and static feature weighting that applies the same weights to all dataset examples, limiting performance.

Method: VARAN employs layer-specialized probing heads and data-dependent weighting to dynamically tailor layer aggregation to individual inputs, adaptively prioritizing different layers' features based on the specific input characteristics.

Result: Evaluations on automatic speech recognition and speech emotion recognition tasks demonstrate VARAN's superior performance, particularly when using the LoRA fine-tuning technique.

Conclusion: The framework successfully resolves the trade-off between preserving layer-specific information and enabling flexible feature utilization, advancing efficient adaptation of self-supervised speech representations.

Abstract: Conventional methods for aggregating layers in fine-tuned self-supervised
speech models, such as using the final layer or weighted sum, suffer from
information bottlenecks and static feature weighting for all dataset examples.
We propose VARAN, a framework that dynamically tailors layer aggregation to
individual inputs. By employing layer-specialized probing heads and
data-dependent weighting, VARAN adaptively prioritizes layer's features based
on input. Evaluations on automatic speech recognition and speech emotion
recognition tasks demonstrate VARAN's superior performance, particularly when
using the LoRA fine-tuning technique. The framework resolves the trade-off
between preserving layer-specific information and enabling flexible feature
utilization, advancing efficient adaptation of self-supervised speech
representations.

</details>


### [320] [Content Accuracy and Quality Aware Resource Allocation Based on LP-Guided DRL for ISAC-Driven AIGC Networks](https://arxiv.org/abs/2508.12079)
*Ningzhe Shi,Yiqing Zhou,Ling Liu,Jinglin Shi,Yihao Wu,Haiwei Shi,Hanxiao Yu*

Main category: cs.LG

TL;DR: Proposes LPDRL-F algorithm for optimizing 3D resource tradeoff in ISAC-AIGC networks to maximize content accuracy and quality, achieving 50%+ improvement over CGQ-only schemes.


<details>
  <summary>Details</summary>
Motivation: Existing AIGC services assume accurate input data and focus only on content generation quality, but ISAC-based AIGC networks face inaccurate sensed data and generation errors, requiring joint optimization of sensing, computing, and communication resources.

Method: Proposes LP-guided deep reinforcement learning with action filter (LPDRL-F) to transform 3D solution space to 2D, reducing complexity while improving DRL performance for resource allocation optimization.

Result: LPDRL-F converges 60% faster than existing DRL/diffusion methods, improves AvgCAQA by over 14%, and achieves 50%+ AvgCAQA improvement compared to CGQ-only schemes.

Conclusion: The proposed CAQA metric and LPDRL-F algorithm effectively address the 3D resource tradeoff in ISAC-AIGC networks, significantly enhancing service quality through optimized resource allocation.

Abstract: Integrated sensing and communication (ISAC) can enhance artificial
intelligence-generated content (AIGC) networks by providing efficient sensing
and transmission. Existing AIGC services usually assume that the accuracy of
the generated content can be ensured, given accurate input data and prompt,
thus only the content generation quality (CGQ) is concerned. However, it is not
applicable in ISAC-based AIGC networks, where content generation is based on
inaccurate sensed data. Moreover, the AIGC model itself introduces generation
errors, which depend on the number of generating steps (i.e., computing
resources). To assess the quality of experience of ISAC-based AIGC services, we
propose a content accuracy and quality aware service assessment metric (CAQA).
Since allocating more resources to sensing and generating improves content
accuracy but may reduce communication quality, and vice versa, this
sensing-generating (computing)-communication three-dimensional resource
tradeoff must be optimized to maximize the average CAQA (AvgCAQA) across all
users with AIGC (CAQA-AIGC). This problem is NP-hard, with a large solution
space that grows exponentially with users. To solve the CAQA-AIGC problem with
low complexity, a linear programming (LP) guided deep reinforcement learning
(DRL) algorithm with an action filter (LPDRL-F) is proposed. Through the
LP-guided approach and the action filter, LPDRL-F can transform the original
three-dimensional solution space to two dimensions, reducing complexity while
improving the learning performance of DRL. Simulations show that compared to
existing DRL and generative diffusion model algorithms without LP, LPDRL-F
converges faster by over 60% and finds better resource allocation solutions,
improving AvgCAQA by more than 14%. With LPDRL-F, CAQA-AIGC can achieve an
improvement in AvgCAQA of more than 50% compared to existing schemes focusing
solely on CGQ.

</details>


### [321] [Generative Medical Event Models Improve with Scale](https://arxiv.org/abs/2508.12104)
*Shane Waxler,Paul Blazek,Davis White,Daniel Sneider,Kevin Chung,Mani Nagarathnam,Patrick Williams,Hank Voeller,Karen Wong,Matthew Swanhorst,Sheng Zhang,Naoto Usuyama,Cliff Wong,Tristan Naumann,Hoifung Poon,Andrew Loza,Daniella Meeker,Seth Hain,Rahul Shah*

Main category: cs.LG

TL;DR: CoMET is a 1B-parameter transformer foundation model pretrained on 115B medical events from 118M patients that generates future medical events and outperforms task-specific models on 78 healthcare tasks without fine-tuning.


<details>
  <summary>Details</summary>
Motivation: To enable personalized medicine at scale by developing foundation models that can distill insights from longitudinal patient journeys and generalize across diverse healthcare tasks.

Method: Decoder-only transformer models pretrained on Epic Cosmos dataset (16.3B encounters, 300M patients) using autoregressive generation of medical events, with scaling-law optimization for compute, tokens, and model size.

Result: CoMET outperformed or matched task-specific supervised models on 78 real-world tasks including diagnosis prediction, disease prognosis, and healthcare operations, with performance improving with model scale.

Conclusion: Generative medical event foundation models like CoMET can effectively capture clinical dynamics and provide a generalizable framework for clinical decision-making and healthcare operations without task-specific training.

Abstract: Realizing personalized medicine at scale calls for methods that distill
insights from longitudinal patient journeys, which can be viewed as a sequence
of medical events. Foundation models pretrained on large-scale medical event
data represent a promising direction for scaling real-world evidence generation
and generalizing to diverse downstream tasks. Using Epic Cosmos, a dataset with
medical events from de-identified longitudinal health records for 16.3 billion
encounters over 300 million unique patient records from 310 health systems, we
introduce the Cosmos Medical Event Transformer ( CoMET) models, a family of
decoder-only transformer models pretrained on 118 million patients representing
115 billion discrete medical events (151 billion tokens). We present the
largest scaling-law study for medical event data, establishing a methodology
for pretraining and revealing power-law scaling relationships for compute,
tokens, and model size. Based on this, we pretrained a series of
compute-optimal models with up to 1 billion parameters. Conditioned on a
patient's real-world history, CoMET autoregressively generates the next medical
event, simulating patient health timelines. We studied 78 real-world tasks,
including diagnosis prediction, disease prognosis, and healthcare operations.
Remarkably for a foundation model with generic pretraining and simulation-based
inference, CoMET generally outperformed or matched task-specific supervised
models on these tasks, without requiring task-specific fine-tuning or few-shot
examples. CoMET's predictive power consistently improves as the model and
pretraining scale. Our results show that CoMET, a generative medical event
foundation model, can effectively capture complex clinical dynamics, providing
an extensible and generalizable framework to support clinical decision-making,
streamline healthcare operations, and improve patient outcomes.

</details>


### [322] [DynamixSFT: Dynamic Mixture Optimization of Instruction Tuning Collections](https://arxiv.org/abs/2508.12116)
*Haebin Shin,Lei Ji,Xiao Liu,Zhiwei Yu,Qi Chen,Yeyun Gong*

Main category: cs.LG

TL;DR: DynamixSFT is a dynamic automated method that treats instruction-tuning dataset mixture optimization as a multi-armed bandit problem, using Prior-scaled Boltzmann Exploration and 1-Step Look-ahead Reward to achieve up to 2.2% performance improvement.


<details>
  <summary>Details</summary>
Motivation: Address the challenge of dynamically balancing and optimizing instruction-tuning dataset mixtures as numerous datasets emerge during post-training stage.

Method: Formulates as multi-armed bandit setup with Prior-scaled Boltzmann Exploration that anchors sampling distribution to original proportions, using lightweight 1-Step Look-ahead Reward to update sampling probabilities based on dataset contribution to model improvement.

Result: Achieves up to 2.2% performance improvement across 10 benchmarks when applied to Tulu-v2-mixture collection of 16 instruction-tuning datasets.

Conclusion: DynamixSFT provides an effective dynamic and automated approach for optimizing instruction-tuning dataset mixtures while preserving dataset diversity and coverage.

Abstract: As numerous instruction-tuning datasets continue to emerge during the
post-training stage, dynamically balancing and optimizing their mixtures has
become a critical challenge. To address this, we propose DynamixSFT, a dynamic
and automated method for instruction-tuning dataset mixture optimization. We
formulate the problem as a multi-armed bandit setup and introduce a
Prior-scaled Boltzmann Exploration that softly anchors the updated sampling
distribution to the original dataset proportions, thereby preserving the
inherent diversity and coverage of the collection. Sampling probabilities are
updated using a lightweight 1-Step Look-ahead Reward, reflecting how much the
dataset contributes to improving the model's performance at its current state.
When applied to the Tulu-v2-mixture collection comprising 16 instruction-tuning
datasets, DynamixSFT achieves up to a 2.2% performance improvement across 10
benchmarks. Furthermore, we provide a comprehensive analysis and visualizations
to offer deeper insights into the adaptive dynamics of our method.

</details>


### [323] [Time-Scale Coupling Between States and Parameters in Recurrent Neural Networks](https://arxiv.org/abs/2508.12121)
*Lorenzo Livi*

Main category: cs.LG

TL;DR: Gating mechanisms in RNNs implicitly create adaptive learning-rate behavior during training, acting as data-driven preconditioners that reshape gradient propagation and parameter updates.


<details>
  <summary>Details</summary>
Motivation: To understand how gating mechanisms in RNNs influence optimization dynamics and explain why gated architectures achieve robust trainability and stability in practice.

Method: Derived exact Jacobians for leaky-integrator and gated RNNs, performed first-order expansion analysis, and conducted numerical experiments to validate the perturbative analysis.

Result: Gates not only control memory retention but also modulate effective step sizes, introduce anisotropy in parameter updates, and create optimization behaviors analogous to learning-rate schedules, momentum, and adaptive methods like Adam.

Conclusion: Gating mechanisms provide a unified dynamical-systems perspective that couples state evolution with parameter updates, explaining the robust trainability and stability of gated architectures in practice.

Abstract: We study how gating mechanisms in recurrent neural networks (RNNs) implicitly
induce adaptive learning-rate behavior, even when training is carried out with
a fixed, global learning rate. This effect arises from the coupling between
state-space time scales--parametrized by the gates--and parameter-space
dynamics during gradient descent. By deriving exact Jacobians for
leaky-integrator and gated RNNs, we obtain a first-order expansion that makes
explicit how constant, scalar, and multi-dimensional gates reshape gradient
propagation, modulate effective step sizes, and introduce anisotropy in
parameter updates. These findings reveal that gates not only control memory
retention in the hidden states, but also act as data-driven preconditioners
that adapt optimization trajectories in parameter space. We further draw formal
analogies with learning-rate schedules, momentum, and adaptive methods such as
Adam, showing that these optimization behaviors emerge naturally from gating.
Numerical experiments confirm the validity of our perturbative analysis,
supporting the view that gate-induced corrections remain small while exerting
systematic effects on training dynamics. Overall, this work provides a unified
dynamical-systems perspective on how gating couples state evolution with
parameter updates, explaining why gated architectures achieve robust
trainability and stability in practice.

</details>


### [324] [DE-VAE: Revealing Uncertainty in Parametric and Inverse Projections with Variational Autoencoders using Differential Entropy](https://arxiv.org/abs/2508.12145)
*Frederik L. Dennig,Daniel A. Keim*

Main category: cs.LG

TL;DR: DE-VAE is an uncertainty-aware variational autoencoder that uses differential entropy to create parametric and invertible 2D projections while handling out-of-distribution samples better than existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing autoencoder methods perform poorly with out-of-distribution samples in data or embedding space, limiting their practical utility for parametric and invertible projections.

Method: Proposes DE-VAE, a variational autoencoder that uses differential entropy to learn parametric mappings to 2D space and inverse mappings back to original space, trained with UMAP and t-SNE as baseline projections.

Result: DE-VAE achieves comparable accuracy to current AE-based approaches while enabling embedding uncertainty analysis, validated through quantitative and qualitative evaluations on four datasets.

Conclusion: DE-VAE successfully addresses the limitations of existing methods by providing uncertainty-aware parametric and invertible projections that handle out-of-distribution samples effectively.

Abstract: Recently, autoencoders (AEs) have gained interest for creating parametric and
invertible projections of multidimensional data. Parametric projections make it
possible to embed new, unseen samples without recalculating the entire
projection, while invertible projections allow the synthesis of new data
instances. However, existing methods perform poorly when dealing with
out-of-distribution samples in either the data or embedding space. Thus, we
propose DE-VAE, an uncertainty-aware variational AE using differential entropy
(DE) to improve the learned parametric and invertible projections. Given a
fixed projection, we train DE-VAE to learn a mapping into 2D space and an
inverse mapping back to the original space. We conduct quantitative and
qualitative evaluations on four well-known datasets, using UMAP and t-SNE as
baseline projection methods. Our findings show that DE-VAE can create
parametric and inverse projections with comparable accuracy to other current
AE-based approaches while enabling the analysis of embedding uncertainty.

</details>


### [325] [AICRN: Attention-Integrated Convolutional Residual Network for Interpretable Electrocardiogram Analysis](https://arxiv.org/abs/2508.12162)
*J. M. I. H. Jayakody,A. M. H. H. Alahakoon,C. R. M. Perera,R. M. L. C. Srimal,Roshan Ragel,Vajira Thambawita,Isuru Nawinne*

Main category: cs.LG

TL;DR: A novel deep learning architecture called AICRN uses attention mechanisms and convolutional residual networks for precise ECG parameter regression, outperforming existing models and improving interpretability in cardiac analysis.


<details>
  <summary>Details</summary>
Motivation: To improve diagnostic precision and predictive capacity of cardiac diseases through AI/ML, addressing challenges like human errors in traditional ECG analysis and enabling fast detection of cardiac events.

Method: Proposes attention-integrated convolutional residual network (AICRN) with spatial and channel attention mechanisms to regress key ECG parameters (PR interval, QT interval, QRS duration, heart rate, R wave amplitude, T wave amplitude) using convolutional residual networks to prevent gradient problems.

Result: AICRN models outperform existing models in parameter regression with higher precision, demonstrating improved interpretability and accuracy in ECG analysis.

Conclusion: Deep learning can play a crucial role in enhancing interpretability and precision of ECG analysis, opening new clinical applications for cardiac monitoring and management.

Abstract: The paradigm of electrocardiogram (ECG) analysis has evolved into real-time
digital analysis, facilitated by artificial intelligence (AI) and machine
learning (ML), which has improved the diagnostic precision and predictive
capacity of cardiac diseases. This work proposes a novel deep learning (DL)
architecture called the attention-integrated convolutional residual network
(AICRN) to regress key ECG parameters such as the PR interval, the QT interval,
the QRS duration, the heart rate, the peak amplitude of the R wave, and the
amplitude of the T wave for interpretable ECG analysis. Our architecture is
specially designed with spatial and channel attention-related mechanisms to
address the type and spatial location of the ECG features for regression. The
models employ a convolutional residual network to address vanishing and
exploding gradient problems. The designed system addresses traditional analysis
challenges, such as loss of focus due to human errors, and facilitates the fast
and easy detection of cardiac events, thereby reducing the manual efforts
required to solve analysis tasks. AICRN models outperform existing models in
parameter regression with higher precision. This work demonstrates that DL can
play a crucial role in the interpretability and precision of ECG analysis,
opening up new clinical applications for cardiac monitoring and management.

</details>


### [326] [ProtTeX-CC: Activating In-Context Learning in Protein LLM via Two-Stage Instruction Compression](https://arxiv.org/abs/2508.12212)
*Chuanliu Fan,Zicheng Ma,Jun Gao,Nan Yu,Jun Zhang,Ziqiang Cao,Yi Qin Gao,Guohong Fu*

Main category: cs.LG

TL;DR: ProtTeX-CC is a lightweight compression framework that enhances protein language models by reducing input length through joint embedding compression and self-compression modules, achieving significant performance improvements in few-shot protein function prediction.


<details>
  <summary>Details</summary>
Motivation: Address limitations of existing protein language models like ProtTeX, which suffer from doubled protein length due to concatenated sequence/structure tokens and incompatibility with in-context learning due to limited context windows.

Method: Two-stage compression framework: 1) Joint embedding compression fuses sequence and structure representations at residue level, reducing input length by half. 2) Self-compression module aggregates demonstrations into latent space of last few tokens, reducing demonstration length from 751 to <16 tokens.

Result: Achieves 93.68% compression ratio in total prompt length under 16-shot setting. Improves in-domain benchmark performance by 2% and out-of-domain performance by 11% without modifying backbone model.

Conclusion: ProtTeX-CC effectively addresses the limitations of existing protein language models through innovative compression techniques, enabling better in-context learning and generalization capabilities with minimal additional parameters.

Abstract: Recent advances in protein large language models, such as ProtTeX, represent
both side-chain amino acids and backbone structure as discrete token sequences
of residue length. While this design enables unified modeling of multimodal
protein information, it suffers from two major limitations: (1) The
concatenation of sequence and structure tokens approximately doubles the
protein length and breaks the intrinsic residue-level alignment between
modalities. (2) Constrained by the training corpus and limited context window,
ProtTeX is typically trained on single-protein inputs, rendering it
incompatible with in-context learning (ICL) and thus limiting its
generalization capability. To address these issues, we propose ProtTeX-CC, a
lightweight two-stage compression framework designed to enhance ProtTeX under
few-shot settings. We first design a joint embedding compression mechanism that
fuses sequence and structure representations at the residue level, effectively
reducing the protein input length by half without sacrificing performance. Then
we propose a self-compression module that aggregates each full demonstration
into the latent space of the last few linguistic tokens, reducing the average
demonstration length from 751 tokens to less than 16 tokens. Compared to the
original ProtTeX, our self-compression approach achieves a compression ratio of
approximately 93.68% in the total prompt length under the 16-shot setting.
Without modifying the backbone model, ProtTeX-CC introduces only a small number
of additional parameters through PEFT-based tuning in the joint embedding
compression stage and a single trainable projection layer in the
self-compression stage. Extensive experiments on protein function prediction
show that ProtTeX-CC improves performance on the in-domain benchmark by 2%, and
generalizes well to the out-of-domain dataset with a performance gain of 11%.

</details>


### [327] [Unlearning at Scale: Implementing the Right to be Forgotten in Large Language Models](https://arxiv.org/abs/2508.12220)
*Abdullah X*

Main category: cs.LG

TL;DR: A deterministic approach to machine unlearning for large language models that logs minimal training metadata to enable exact replay and removal of specific data points while maintaining model integrity.


<details>
  <summary>Details</summary>
Motivation: To address the right to be forgotten (GDPR Article 17) requirements for large language models by creating a reproducible system for data removal that maintains model performance on retained data.

Method: Treats training as deterministic program with minimal logging (ID hash, RNG seed, learning rate, optimizer step, accumulation boundary). Uses exact replay of training tail while filtering out forget data, with complementary paths including micro-checkpoints, adapter deletion, and curvature-guided anti-updates.

Result: Achieves byte-identical equality of model and optimizer states when preconditions are satisfied, demonstrating successful unlearning while maintaining identical parameters to training on only the retain set.

Conclusion: The approach provides a practical solution for GDPR compliance through deterministic training replay, offering multiple complementary methods to meet latency and availability constraints while ensuring verifiable data removal.

Abstract: We study the right to be forgotten (GDPR Art. 17) for large language models
and frame unlearning as a reproducible systems problem. Our approach treats
training as a deterministic program and logs a minimal per-microbatch record
(ordered ID hash, RNG seed, learning-rate value, optimizer-step counter, and
accumulation boundary). Under a pinned stack and deterministic kernels,
replaying the training tail while filtering only the forget closure yields the
same parameters as training on the retain set (bit-identical in the training
dtype) when preconditions hold. To meet latency and availability constraints,
we add complementary paths: (i) exact reverts of recent steps via
micro-checkpoints or dense per-step deltas, (ii) cohort-scoped adapter deletion
when the base is frozen, and (iii) a curvature-guided anti-update followed by a
short retain-tune, audit-gated with escalation to exact replay. We report
storage/latency budgets and a toy artifact validating mechanics; in a
controlled run that satisfies the preconditions we demonstrate byte-identical
equality of model and optimizer states.

</details>


### [328] [Distribution Matching via Generalized Consistency Models](https://arxiv.org/abs/2508.12222)
*Sagar Shrestha,Rajesh Shrestha,Tri Nguyen,Subash Timilsina*

Main category: cs.LG

TL;DR: Proposes a novel distribution matching approach using consistency models from Continuous Normalizing Flow (CNF) to overcome GAN training challenges while maintaining flexibility.


<details>
  <summary>Details</summary>
Motivation: GANs face training difficulties due to min-max optimization and mode collapse, despite being effective for distribution matching tasks like domain adaptation and latent variable modeling.

Method: Leverages consistency models from CNF with a straightforward norm minimization objective, combining CNF's training advantages with GAN-like constraint adaptability.

Result: Theoretical validation and experimental results on synthetic and real-world datasets demonstrate the proposed method's performance.

Conclusion: The approach successfully addresses GAN limitations while preserving the flexibility needed for various distribution matching applications.

Abstract: Recent advancement in generative models have demonstrated remarkable
performance across various data modalities. Beyond their typical use in data
synthesis, these models play a crucial role in distribution matching tasks such
as latent variable modeling, domain translation, and domain adaptation.
Generative Adversarial Networks (GANs) have emerged as the preferred method of
distribution matching due to their efficacy in handling high-dimensional data
and their flexibility in accommodating various constraints. However, GANs often
encounter challenge in training due to their bi-level min-max optimization
objective and susceptibility to mode collapse. In this work, we propose a novel
approach for distribution matching inspired by the consistency models employed
in Continuous Normalizing Flow (CNF). Our model inherits the advantages of CNF
models, such as having a straight forward norm minimization objective, while
remaining adaptable to different constraints similar to GANs. We provide
theoretical validation of our proposed objective and demonstrate its
performance through experiments on synthetic and real-world datasets.

</details>


### [329] [Communication-Efficient Distributed Asynchronous ADMM](https://arxiv.org/abs/2508.12233)
*Sagar Shrestha*

Main category: cs.LG

TL;DR: Quantized asynchronous ADMM for distributed optimization reduces communication costs through coarse quantization while maintaining convergence.


<details>
  <summary>Details</summary>
Motivation: Communication costs are a major bottleneck in distributed optimization and federated learning, especially with limited communication budgets or large data sizes.

Method: Introducing coarse quantization to the data exchanged in asynchronous ADMM to reduce communication overhead.

Result: Experimental verification shows convergence of the proposed method for various distributed learning tasks, including neural networks.

Conclusion: Quantized asynchronous ADMM effectively reduces communication costs while preserving convergence properties for large-scale distributed optimization applications.

Abstract: In distributed optimization and federated learning, asynchronous alternating
direction method of multipliers (ADMM) serves as an attractive option for
large-scale optimization, data privacy, straggler nodes and variety of
objective functions. However, communication costs can become a major bottleneck
when the nodes have limited communication budgets or when the data to be
communicated is prohibitively large. In this work, we propose introducing
coarse quantization to the data to be exchanged in aynchronous ADMM so as to
reduce communication overhead for large-scale federated learning and
distributed optimization applications. We experimentally verify the convergence
of the proposed method for several distributed learning tasks, including neural
networks.

</details>


### [330] [CC-Time: Cross-Model and Cross-Modality Time Series Forecasting](https://arxiv.org/abs/2508.12235)
*Peng Chen,Yihang Wang,Yang Shu,Yunyao Cheng,Kai Zhao,Zhongwen Rao,Lujia Pan,Bin Yang,Chenjuan Guo*

Main category: cs.LG

TL;DR: CC-Time is a novel approach that combines cross-modality learning and cross-model fusion to leverage pre-trained language models for improved time series forecasting accuracy.


<details>
  <summary>Details</summary>
Motivation: Current PLM-based time series forecasting methods fail to achieve satisfactory prediction accuracy despite the strong sequential modeling capabilities of language models, indicating a gap between potential and performance.

Method: CC-Time uses cross-modality learning to model temporal dependency and channel correlations from both time series sequences and text descriptions, and cross-model fusion to integrate knowledge from PLMs and traditional time series models.

Result: Extensive experiments on nine real-world datasets show CC-Time achieves state-of-the-art prediction accuracy in both full-data training and few-shot learning scenarios.

Conclusion: The proposed CC-Time framework successfully bridges the gap between PLMs' potential and actual performance in time series forecasting by effectively combining cross-modality learning and model fusion techniques.

Abstract: With the success of pre-trained language models (PLMs) in various application
fields beyond natural language processing, language models have raised emerging
attention in the field of time series forecasting (TSF) and have shown great
prospects. However, current PLM-based TSF methods still fail to achieve
satisfactory prediction accuracy matching the strong sequential modeling power
of language models. To address this issue, we propose Cross-Model and
Cross-Modality Learning with PLMs for time series forecasting (CC-Time). We
explore the potential of PLMs for time series forecasting from two aspects: 1)
what time series features could be modeled by PLMs, and 2) whether relying
solely on PLMs is sufficient for building time series models. In the first
aspect, CC-Time incorporates cross-modality learning to model temporal
dependency and channel correlations in the language model from both time series
sequences and their corresponding text descriptions. In the second aspect,
CC-Time further proposes the cross-model fusion block to adaptively integrate
knowledge from the PLMs and time series model to form a more comprehensive
modeling of time series patterns. Extensive experiments on nine real-world
datasets demonstrate that CC-Time achieves state-of-the-art prediction accuracy
in both full-data training and few-shot learning situations.

</details>


### [331] [DHG-Bench: A Comprehensive Benchmark on Deep Hypergraph Learning](https://arxiv.org/abs/2508.12244)
*Fan Li,Xiaoyang Wang,Wenjie Zhang,Ying Zhang,Xuemin Lin*

Main category: cs.LG

TL;DR: DHG-Bench is the first comprehensive benchmark for deep hypergraph learning, addressing limitations in current hypergraph neural network evaluation by providing standardized datasets, algorithms, and experimental protocols across multiple dimensions.


<details>
  <summary>Details</summary>
Motivation: Existing hypergraph neural network research lacks comprehensive benchmarking, with insufficient dataset coverage, narrow performance evaluation, and inconsistent experimental setups that hinder comparability and understanding of progress in deep hypergraph learning.

Method: The authors introduce DHG-Bench, which integrates 20 diverse datasets spanning node-, edge-, and graph-level tasks, along with 16 state-of-the-art HNN algorithms, under consistent data processing and experimental protocols. They systematically evaluate HNNs across four dimensions: effectiveness, efficiency, robustness, and fairness.

Result: Extensive experiments with DHG-BBench reveal both strengths and inherent limitations of existing hypergraph neural network algorithms, providing valuable insights into their performance characteristics across different evaluation dimensions.

Conclusion: DHG-Bench fills a critical gap in deep hypergraph learning research by providing the first comprehensive benchmark that enables standardized evaluation and comparison of hypergraph neural networks, offering valuable directions for future research and development in this field.

Abstract: Although conventional deep graph models have achieved great success in
relational learning, their focus on pairwise relationships limits their
capacity to learn pervasive higher-order interactions in real-world complex
systems, which can be naturally modeled as hypergraphs. To tackle this,
hypergraph neural networks (HNNs), the dominant approach in deep hypergraph
learning (DHGL), has garnered substantial attention in recent years. Despite
the proposal of numerous HNN methods, there is no comprehensive benchmark for
HNNs, which creates a great obstacle to understanding the progress of DHGL in
several aspects: (i) insufficient coverage of datasets, algorithms, and tasks;
(ii) a narrow evaluation of algorithm performance; and (iii) inconsistent
dataset usage, preprocessing, and experimental setups that hinder
comparability. To fill the gap, we introduce DHG-Bench, the first comprehensive
benchmark for DHGL. Specifically, DHG-Bench integrates 20 diverse datasets
spanning node-, edge-, and graph-level tasks, along with 16 state-of-the-art
HNN algorithms, under consistent data processing and experimental protocols.
Our benchmark systematically investigates the characteristics of HNNs in terms
of four dimensions: effectiveness, efficiency, robustness, and fairness.
Further, to facilitate reproducible research, we have developed an easy-to-use
library for training and evaluating different HNN methods. Extensive
experiments conducted with DHG-Bench reveal both the strengths and inherent
limitations of existing algorithms, offering valuable insights and directions
for future research. The code is publicly available at:
https://github.com/Coco-Hut/DHG-Bench.

</details>


### [332] [STM3: Mixture of Multiscale Mamba for Long-Term Spatio-Temporal Time-Series Prediction](https://arxiv.org/abs/2508.12247)
*Haolong Chen,Liang Zhang,Zhengyuan Xin,Guangxu Zhu*

Main category: cs.LG

TL;DR: STM2 and STM3 models for efficient long-term spatio-temporal time-series prediction using multiscale Mamba architecture and adaptive graph networks to handle complex dependencies.


<details>
  <summary>Details</summary>
Motivation: Existing deep learning methods struggle with learning complex long-term spatio-temporal dependencies efficiently, particularly with multiscale temporal information and correlated node data.

Method: STM2 uses multiscale Mamba architecture with hierarchical information aggregation and adaptive graph causal convolution. STM3 enhances this with Mixture-of-Experts architecture, stable routing strategy, and causal contrastive learning.

Result: Extensive experiments on real-world benchmarks demonstrate superior performance, achieving state-of-the-art results in long-term spatio-temporal time-series prediction.

Conclusion: The proposed STM2 and STM3 models effectively address long-term spatio-temporal dependency challenges and outperform existing methods.

Abstract: Recently, spatio-temporal time-series prediction has developed rapidly, yet
existing deep learning methods struggle with learning complex long-term
spatio-temporal dependencies efficiently. The long-term spatio-temporal
dependency learning brings two new challenges: 1) The long-term temporal
sequence includes multiscale information naturally which is hard to extract
efficiently; 2) The multiscale temporal information from different nodes is
highly correlated and hard to model. To address these challenges, we propose an
efficient \textit{\textbf{S}patio-\textbf{T}emporal \textbf{M}ultiscale
\textbf{M}amba} (STM2) that includes a multiscale Mamba architecture to capture
the multiscale information efficiently and simultaneously, and an adaptive
graph causal convolution network to learn the complex multiscale
spatio-temporal dependency. STM2 includes hierarchical information aggregation
for different-scale information that guarantees their distinguishability. To
capture diverse temporal dynamics across all spatial nodes more efficiently, we
further propose an enhanced version termed
\textit{\textbf{S}patio-\textbf{T}emporal \textbf{M}ixture of
\textbf{M}ultiscale \textbf{M}amba} (STM3) that employs a special
Mixture-of-Experts architecture, including a more stable routing strategy and a
causal contrastive learning strategy to enhance the scale distinguishability.
We prove that STM3 has much better routing smoothness and guarantees the
pattern disentanglement for each expert successfully. Extensive experiments on
real-world benchmarks demonstrate STM2/STM3's superior performance, achieving
state-of-the-art results in long-term spatio-temporal time-series prediction.

</details>


### [333] [Interpreting Time Series Forecasts with LIME and SHAP: A Case Study on the Air Passengers Dataset](https://arxiv.org/abs/2508.12253)
*Manish Shukla*

Main category: cs.LG

TL;DR: A unified framework for interpreting time-series forecasts using LIME and SHAP that combines ARIMA's interpretability with XGBoost's accuracy while maintaining chronological integrity.


<details>
  <summary>Details</summary>
Motivation: Time-series forecasting is critical across many industries, but existing models face trade-offs: ARIMA offers interpretability but struggles with nonlinearities, while tree-based models like XGBoost provide high accuracy but are often opaque black boxes.

Method: Convert univariate time series into leakage-free supervised learning problem, train gradient-boosted tree alongside ARIMA baseline, and apply post-hoc explainability using LIME and SHAP while preserving chronological order.

Result: Using the Air Passengers dataset, the study shows that a small set of lagged features (particularly twelve-month lag) and seasonal encodings explain most forecast variance, demonstrating effective interpretation of complex forecasts.

Conclusion: The paper provides a practical methodology for applying explainable AI techniques to time series forecasting, along with theoretical exposition, empirical evaluation, and guidelines for practitioners to interpret forecasts while maintaining model accuracy.

Abstract: Time-series forecasting underpins critical decisions across aviation, energy,
retail and health. Classical autoregressive integrated moving average (ARIMA)
models offer interpretability via coefficients but struggle with
nonlinearities, whereas tree-based machine-learning models such as XGBoost
deliver high accuracy but are often opaque. This paper presents a unified
framework for interpreting time-series forecasts using local interpretable
model-agnostic explanations (LIME) and SHapley additive exPlanations (SHAP). We
convert a univariate series into a leakage-free supervised learning problem,
train a gradient-boosted tree alongside an ARIMA baseline and apply post-hoc
explainability. Using the Air Passengers dataset as a case study, we show that
a small set of lagged features -- particularly the twelve-month lag -- and
seasonal encodings explain most forecast variance. We contribute: (i) a
methodology for applying LIME and SHAP to time series without violating
chronology; (ii) theoretical exposition of the underlying algorithms; (iii)
empirical evaluation with extensive analysis; and (iv) guidelines for
practitioners.

</details>


### [334] [L-SR1: Learned Symmetric-Rank-One Preconditioning](https://arxiv.org/abs/2508.12270)
*Gal Lifshitz,Shahar Zuler,Ori Fouks,Dan Raviv*

Main category: cs.LG

TL;DR: A novel learned second-order optimizer that enhances the classical SR1 algorithm with trainable preconditioning, outperforming existing learned optimization methods on monocular human mesh recovery tasks without requiring annotated data or fine-tuning.


<details>
  <summary>Details</summary>
Motivation: End-to-end deep learning has limitations in data efficiency and generalization, while classical optimization methods are data-efficient but slow. Learned optimizers combine both advantages but most focus on first-order methods, leaving second-order approaches underexplored.

Method: Proposes a learned second-order optimizer with trainable preconditioning unit that generates data-driven vectors to construct positive semi-definite rank-one matrices, aligned with secant constraint via learned projection.

Result: Outperforms existing learned optimization-based approaches on monocular human mesh recovery tasks, demonstrating strong generalization without requiring annotated data or fine-tuning.

Conclusion: The proposed lightweight learned second-order optimizer offers effective fusion of deep learning and classical optimization benefits, suitable for integration into broader optimization-based frameworks.

Abstract: End-to-end deep learning has achieved impressive results but remains limited
by its reliance on large labeled datasets, poor generalization to unseen
scenarios, and growing computational demands. In contrast, classical
optimization methods are data-efficient and lightweight but often suffer from
slow convergence. While learned optimizers offer a promising fusion of both
worlds, most focus on first-order methods, leaving learned second-order
approaches largely unexplored.
  We propose a novel learned second-order optimizer that introduces a trainable
preconditioning unit to enhance the classical Symmetric-Rank-One (SR1)
algorithm. This unit generates data-driven vectors used to construct positive
semi-definite rank-one matrices, aligned with the secant constraint via a
learned projection. Our method is evaluated through analytic experiments and on
the real-world task of Monocular Human Mesh Recovery (HMR), where it
outperforms existing learned optimization-based approaches. Featuring a
lightweight model and requiring no annotated data or fine-tuning, our approach
offers strong generalization and is well-suited for integration into broader
optimization-based frameworks.

</details>


### [335] [CRoC: Context Refactoring Contrast for Graph Anomaly Detection with Limited Supervision](https://arxiv.org/abs/2508.12278)
*Siyue Xie,Da Sun Handason Tam,Wing Cheong Lau*

Main category: cs.LG

TL;DR: CRoC is a novel framework that uses context refactoring and contrastive learning to train GNNs for graph anomaly detection with limited labeled data, achieving significant performance improvements over existing methods.


<details>
  <summary>Details</summary>
Motivation: Training robust GNNs for graph anomaly detection requires abundant labeled data, but anomalies are rare, costly to label, and often camouflage their patterns, creating a critical bottleneck in real-world applications.

Method: CRoC refactors node contexts by recomposing attributes while preserving interaction patterns, encodes heterogeneous relations separately, integrates them into message-passing, and uses contrastive learning to leverage both labeled and unlabeled data.

Result: Extensive experiments on seven real-world datasets show CRoC achieves up to 14% AUC improvement over baseline GNNs and outperforms state-of-the-art GAD methods under limited-label settings.

Conclusion: CRoC effectively addresses the label scarcity problem in graph anomaly detection by combining context refactoring with contrastive learning, enabling robust detection of camouflaged anomalies with limited supervision.

Abstract: Graph Neural Networks (GNNs) are widely used as the engine for various
graph-related tasks, with their effectiveness in analyzing graph-structured
data. However, training robust GNNs often demands abundant labeled data, which
is a critical bottleneck in real-world applications. This limitation severely
impedes progress in Graph Anomaly Detection (GAD), where anomalies are
inherently rare, costly to label, and may actively camouflage their patterns to
evade detection. To address these problems, we propose Context Refactoring
Contrast (CRoC), a simple yet effective framework that trains GNNs for GAD by
jointly leveraging limited labeled and abundant unlabeled data. Different from
previous works, CRoC exploits the class imbalance inherent in GAD to refactor
the context of each node, which builds augmented graphs by recomposing the
attributes of nodes while preserving their interaction patterns. Furthermore,
CRoC encodes heterogeneous relations separately and integrates them into the
message-passing process, enhancing the model's capacity to capture complex
interaction semantics. These operations preserve node semantics while
encouraging robustness to adversarial camouflage, enabling GNNs to uncover
intricate anomalous cases. In the training stage, CRoC is further integrated
with the contrastive learning paradigm. This allows GNNs to effectively harness
unlabeled data during joint training, producing richer, more discriminative
node embeddings. CRoC is evaluated on seven real-world GAD datasets with
varying scales. Extensive experiments demonstrate that CRoC achieves up to 14%
AUC improvement over baseline GNNs and outperforms state-of-the-art GAD methods
under limited-label settings.

</details>


### [336] [Convergence Analysis of the Lion Optimizer in Centralized and Distributed Settings](https://arxiv.org/abs/2508.12327)
*Wei Jiang,Lijun Zhang*

Main category: cs.LG

TL;DR: Convergence analysis of Lion optimizer showing O(d^{1/2}T^{-1/4}) rate, improved to O(d^{1/2}T^{-1/3}) with variance reduction, with distributed variants achieving better rates using multiple nodes and communication-efficient sign compression.


<details>
  <summary>Details</summary>
Motivation: To analyze and improve the convergence properties of the Lion optimizer, particularly in distributed settings where communication efficiency is crucial for practical applications.

Method: Theoretical analysis of standard Lion optimizer convergence, introduction of variance reduction technique, extension to distributed settings with multiple nodes, and development of communication-efficient variant using unbiased sign compression operations.

Result: Established convergence rates: standard Lion O(d^{1/2}T^{-1/4}), variance-reduced Lion O(d^{1/2}T^{-1/3}), distributed variants O(d^{1/2}(nT)^{-1/4}) and O(d^{1/2}(nT)^{-1/3}), and communication-efficient variants with sign compression achieving O(max{d^{1/4}/T^{1/4}, d^{1/10}/(n^{1/5}T^{1/5})}) and O(d^{1/4}/T^{1/4}) rates.

Conclusion: The Lion optimizer and its variants demonstrate strong convergence properties, with variance reduction and distributed implementations providing significant improvements, while communication-efficient sign compression maintains competitive convergence rates while reducing communication overhead.

Abstract: In this paper, we analyze the convergence properties of the Lion optimizer.
First, we establish that the Lion optimizer attains a convergence rate of
$\mathcal{O}(d^{1/2}T^{-1/4})$ under standard assumptions, where $d$ denotes
the problem dimension and $T$ is the iteration number. To further improve this
rate, we introduce the Lion optimizer with variance reduction, resulting in an
enhanced convergence rate of $\mathcal{O}(d^{1/2}T^{-1/3})$. We then analyze in
distributed settings, where the standard and variance reduced version of the
distributed Lion can obtain the convergence rates of
$\mathcal{O}(d^{1/2}(nT)^{-1/4})$ and $\mathcal{O}(d^{1/2}(nT)^{-1/3})$, with
$n$ denoting the number of nodes. Furthermore, we investigate a
communication-efficient variant of the distributed Lion that ensures sign
compression in both communication directions. By employing the unbiased sign
operations, the proposed Lion variant and its variance reduction counterpart,
achieve convergence rates of $\mathcal{O}\left( \max
\left\{\frac{d^{1/4}}{T^{1/4}}, \frac{d^{1/10}}{n^{1/5}T^{1/5}} \right\}
\right)$ and $\mathcal{O}\left( \frac{d^{1/4}}{T^{1/4}} \right)$, respectively.

</details>


### [337] [Navigating the Exploration-Exploitation Tradeoff in Inference-Time Scaling of Diffusion Models](https://arxiv.org/abs/2508.12361)
*Xun Su,Jianming Huang,Yang Yusen,Zhongxi Fang,Hiroyuki Kasai*

Main category: cs.LG

TL;DR: Novel Sequential Monte Carlo methods with Funnel Schedule and Adaptive Temperature for diffusion models, addressing exploration-exploitation trade-off in inference-time scaling without increasing computational cost.


<details>
  <summary>Details</summary>
Motivation: Current SMC methods for diffusion models face a dilemma: early-stage noise samples have high improvement potential but are hard to evaluate accurately, while late-stage samples are reliable but irreversible. This creates an exploration-exploitation trade-off that limits inference-time scaling effectiveness.

Method: Proposed two strategies: 1) Funnel Schedule - progressively reduces number of maintained particles during generation, and 2) Adaptive Temperature - down-weights influence of early-stage rewards. Both methods are tailored to diffusion models' unique generation dynamics and phase-transition behavior.

Result: Experimental results on multiple benchmarks and state-of-the-art text-to-image diffusion models show the approach outperforms previous baselines, significantly enhancing sample quality without increasing Noise Function Evaluations.

Conclusion: The proposed methods effectively address the exploration-exploitation trade-off in diffusion model inference, demonstrating that simple yet targeted strategies can significantly improve sample quality while maintaining computational efficiency.

Abstract: Inference-time scaling has achieved remarkable success in language models,
yet its adaptation to diffusion models remains underexplored. We observe that
the efficacy of recent Sequential Monte Carlo (SMC)-based methods largely stems
from globally fitting the The reward-tilted distribution, which inherently
preserves diversity during multi-modal search. However, current applications of
SMC to diffusion models face a fundamental dilemma: early-stage noise samples
offer high potential for improvement but are difficult to evaluate accurately,
whereas late-stage samples can be reliably assessed but are largely
irreversible. To address this exploration-exploitation trade-off, we approach
the problem from the perspective of the search algorithm and propose two
strategies: Funnel Schedule and Adaptive Temperature. These simple yet
effective methods are tailored to the unique generation dynamics and
phase-transition behavior of diffusion models. By progressively reducing the
number of maintained particles and down-weighting the influence of early-stage
rewards, our methods significantly enhance sample quality without increasing
the total number of Noise Function Evaluations. Experimental results on
multiple benchmarks and state-of-the-art text-to-image diffusion models
demonstrate that our approach outperforms previous baselines.

</details>


### [338] [Bi-Axial Transformers: Addressing the Increasing Complexity of EHR Classification](https://arxiv.org/abs/2508.12418)
*Rachael DeVries,Casper Christensen,Marie Lisandra Zepeda Mendoza,Ole Winther*

Main category: cs.LG

TL;DR: BAT (Bi-Axial Transformer) is a novel transformer architecture that attends to both clinical variable and time point axes in EHR data, achieving state-of-the-art performance on sepsis prediction and competitive results for mortality classification while being robust to data missingness.


<details>
  <summary>Details</summary>
Motivation: EHR data is becoming increasingly complex with larger datasets and multi-modal integrations. Transformers are well-suited for EHR analysis but face limitations due to data representations that reduce performance or fail to capture informative missingness patterns.

Method: The Bi-Axial Transformer (BAT) architecture that processes EHR data by attending to both clinical variable and time point axes simultaneously, addressing data sparsity issues and learning richer data relationships.

Result: BAT achieves state-of-the-art performance on sepsis prediction and is competitive with top methods for mortality classification. It demonstrates increased robustness to data missingness and learns unique sensor embeddings suitable for transfer learning.

Conclusion: BAT effectively addresses key challenges in EHR data analysis by modeling both clinical variable and temporal dimensions, providing a robust framework for medical prediction tasks with improved performance and transfer learning capabilities.

Abstract: Electronic Health Records (EHRs), the digital representation of a patient's
medical history, are a valuable resource for epidemiological and clinical
research. They are also becoming increasingly complex, with recent trends
indicating larger datasets, longer time series, and multi-modal integrations.
Transformers, which have rapidly gained popularity due to their success in
natural language processing and other domains, are well-suited to address these
challenges due to their ability to model long-range dependencies and process
data in parallel. But their application to EHR classification remains limited
by data representations, which can reduce performance or fail to capture
informative missingness. In this paper, we present the Bi-Axial Transformer
(BAT), which attends to both the clinical variable and time point axes of EHR
data to learn richer data relationships and address the difficulties of data
sparsity. BAT achieves state-of-the-art performance on sepsis prediction and is
competitive to top methods for mortality classification. In comparison to other
transformers, BAT demonstrates increased robustness to data missingness, and
learns unique sensor embeddings which can be used in transfer learning.
Baseline models, which were previously located across multiple repositories or
utilized deprecated libraries, were re-implemented with PyTorch and made
available for reproduction and future benchmarking.

</details>


### [339] [Machine Learning-Based Manufacturing Cost Prediction from 2D Engineering Drawings via Geometric Features](https://arxiv.org/abs/2508.12440)
*Ahmet Bilal Arıkan,Şener Özönder,Mustafa Taha Koçyiğit,Hüseyin Oktay Altun,H. Kübra Küçükkartal,Murat Arslanoğlu,Fatih Çağırankaya,Berk Ayvaz*

Main category: cs.LG

TL;DR: Machine learning framework that predicts manufacturing costs from 2D engineering drawings with 10% error using geometric features and gradient-boosted models, providing explainable cost insights for automotive parts.


<details>
  <summary>Details</summary>
Motivation: Traditional manufacturing cost estimation requires labor-intensive process planning and lacks scalability across part families. There's a need for automated, accurate cost prediction from engineering drawings to shorten quotation lead times and provide consistent cost assessments.

Method: Extracted 200 geometric and statistical descriptors from 13,684 DWG drawings of automotive suspension and steering parts. Used gradient-boosted decision tree models (XGBoost, CatBoost, LightGBM) trained on these features, coupled with SHAP for explainability.

Result: Achieved nearly 10% mean absolute percentage error across 24 product groups, demonstrating robust scalability beyond part-specific heuristics. Identified key geometric design drivers including rotated dimension maxima, arc statistics and divergence metrics.

Conclusion: The end-to-end CAD-to-cost pipeline provides deployable real-time cost estimation with explainable insights, enabling cost-aware design and ERP-integrated decision support in Industry 4.0 manufacturing environments.

Abstract: We present an integrated machine learning framework that transforms how
manufacturing cost is estimated from 2D engineering drawings. Unlike
traditional quotation workflows that require labor-intensive process planning,
our approach about 200 geometric and statistical descriptors directly from
13,684 DWG drawings of automotive suspension and steering parts spanning 24
product groups. Gradient-boosted decision tree models (XGBoost, CatBoost,
LightGBM) trained on these features achieve nearly 10% mean absolute percentage
error across groups, demonstrating robust scalability beyond part-specific
heuristics. By coupling cost prediction with explainability tools such as SHAP,
the framework identifies geometric design drivers including rotated dimension
maxima, arc statistics and divergence metrics, offering actionable insights for
cost-aware design. This end-to-end CAD-to-cost pipeline shortens quotation lead
times, ensures consistent and transparent cost assessments across part families
and provides a deployable pathway toward real-time, ERP-integrated decision
support in Industry 4.0 manufacturing environments.

</details>


### [340] [Local Cluster Cardinality Estimation for Adaptive Mean Shift](https://arxiv.org/abs/2508.12450)
*Étienne Pepin*

Main category: cs.LG

TL;DR: Adaptive mean shift algorithm that uses local distance distributions to estimate cluster cardinality and dynamically adjust bandwidth parameters, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Traditional mean shift algorithms struggle with datasets containing varying local scale and cluster cardinality, as they use fixed bandwidth parameters that don't adapt to local cluster characteristics.

Method: Uses local distance distributions to identify local minima in density to estimate cluster cardinality. These estimates then inform adaptive adjustments of bandwidth and kernel radius threshold during mean shift execution, providing insights into entire clusters rather than just localized regions.

Result: The algorithm outperformed a recently proposed adaptive mean shift method on its original dataset and demonstrated competitive performance on a broader clustering benchmark.

Conclusion: The adaptive approach using local distance distributions for cardinality estimation effectively handles varying cluster scales and sizes, providing superior performance compared to existing adaptive mean shift methods.

Abstract: This article presents an adaptive mean shift algorithm designed for datasets
with varying local scale and cluster cardinality. Local distance distributions,
from a point to all others, are used to estimate the cardinality of the local
cluster by identifying a local minimum in the density of the distance
distribution. Based on these cardinality estimates, local cluster parameters
are then computed for the entire cluster in contrast to KDE-based methods,
which provide insight only into localized regions of the cluster. During the
mean shift execution, the cluster cardinality estimate is used to adaptively
adjust the bandwidth and the mean shift kernel radius threshold. Our algorithm
outperformed a recently proposed adaptive mean shift method on its original
dataset and demonstrated competitive performance on a broader clustering
benchmark.

</details>


### [341] [Cold-RL: Learning Cache Eviction with Offline Reinforcement Learning for NGINX](https://arxiv.org/abs/2508.12485)
*Aayush Gupta,Arpit Bhayani*

Main category: cs.LG

TL;DR: Cold-RL is a reinforcement learning-based eviction policy for NGINX web proxies that replaces traditional LRU with a Dueling Deep Q-Network, achieving up to 146% hit ratio improvement while maintaining strict microsecond latency budgets.


<details>
  <summary>Details</summary>
Motivation: Traditional LRU eviction in web proxies is size-agnostic and suffers from thrashing under periodic bursts and mixed object sizes, leading to suboptimal cache performance.

Method: Uses a dueling Deep Q-Network served by an ONNX sidecar that samples K least-recently-used objects and extracts six lightweight features (age, size, hit count, inter-arrival time, remaining TTL, last origin RTT) to select victims for eviction with a 500 microsecond timeout fallback to LRU.

Result: Achieved 146% hit ratio improvement over best classical baseline at 25MB cache (0.1436 to 0.3538), 15% gain at 100MB (0.7530 to 0.8675), and matches classical methods at 400MB (~0.918) with less than 2% CPU overhead and maintained 95th percentile latency within budget.

Conclusion: Cold-RL successfully demonstrates the first RL-based eviction policy integrated into NGINX with strict service level objectives, significantly improving cache performance while maintaining microsecond-level latency constraints.

Abstract: Web proxies such as NGINX commonly rely on least-recently-used (LRU)
eviction, which is size agnostic and can thrash under periodic bursts and mixed
object sizes. We introduce Cold-RL, a learned eviction policy for NGINX that
replaces LRU's forced-expire path with a dueling Deep Q-Network served by an
ONNX sidecar within a strict microsecond budget. On each eviction, Cold-RL
samples the K least-recently-used objects, extracts six lightweight features
(age, size, hit count, inter-arrival time, remaining TTL, and last origin RTT),
and requests a bitmask of victims; a hard timeout of 500 microseconds triggers
immediate fallback to native LRU. Policies are trained offline by replaying
NGINX access logs through a cache simulator with a simple reward: a retained
object earns one point if it is hit again before TTL expiry. We compare against
LRU, LFU, size-based, adaptive LRU, and a hybrid baseline on two adversarial
workloads. With a 25 MB cache, Cold-RL raises hit ratio from 0.1436 to 0.3538,
a 146 percent improvement over the best classical baseline; at 100 MB, from
0.7530 to 0.8675, a 15 percent gain; and at 400 MB it matches classical methods
(about 0.918). Inference adds less than 2 percent CPU overhead and keeps 95th
percentile eviction latency within budget. To our knowledge, this is the first
reinforcement learning eviction policy integrated into NGINX with strict SLOs.

</details>


### [342] [Cost-Aware Contrastive Routing for LLMs](https://arxiv.org/abs/2508.12491)
*Reza Shirkavand,Shangqian Gao,Peiran Yu,Heng Huang*

Main category: cs.LG

TL;DR: CSCR is a lightweight routing framework that maps prompts and models to a shared embedding space for fast, cost-sensitive LLM selection using compact model fingerprints and contrastive learning.


<details>
  <summary>Details</summary>
Motivation: Existing routing approaches overlook prompt-specific context, require expensive model profiling, assume fixed expert sets, or use inefficient trial-and-error strategies for large language model routing.

Method: Uses compact logit footprints for open-source models and perplexity fingerprints for black-box APIs. Trains contrastive encoder to favor cheapest accurate expert within adaptive cost bands. Inference uses single k-NN lookup via FAISS index.

Result: Outperforms baselines across multiple benchmarks, improving accuracy-cost tradeoff by up to 25%, with robust generalization to unseen LLMs and out-of-distribution prompts.

Conclusion: CSCR enables fast, cost-aware routing with microsecond latency, requires no retraining when expert pool changes, and provides significant improvements in cost-accuracy efficiency.

Abstract: We study cost-aware routing for large language models across diverse and
dynamic pools of models. Existing approaches often overlook prompt-specific
context, rely on expensive model profiling, assume a fixed set of experts, or
use inefficient trial-and-error strategies. We introduce Cost-Spectrum
Contrastive Routing (CSCR), a lightweight framework that maps both prompts and
models into a shared embedding space to enable fast, cost-sensitive selection.
CSCR uses compact, fast-to-compute logit footprints for open-source models and
perplexity fingerprints for black-box APIs. A contrastive encoder is trained to
favor the cheapest accurate expert within adaptive cost bands. At inference
time, routing reduces to a single k-NN lookup via a FAISS index, requiring no
retraining when the expert pool changes and enabling microsecond latency.
Across multiple benchmarks, CSCR consistently outperforms baselines, improving
the accuracy-cost tradeoff by up to 25%, while generalizing robustly to unseen
LLMs and out-of-distribution prompts.

</details>


### [343] [Trust Region Constrained Measure Transport in Path Space for Stochastic Optimal Control and Inference](https://arxiv.org/abs/2508.12511)
*Denis Blessing,Julius Berner,Lorenz Richter,Carles Domingo-Enrich,Yuanqi Du,Arash Vahdat,Gerhard Neumann*

Main category: cs.LG

TL;DR: A novel trust region-based iterative method for solving stochastic optimal control problems with quadratic costs, using geometric annealing from prior to target measure with principled time step selection.


<details>
  <summary>Details</summary>
Motivation: Solving stochastic optimal control problems is challenging when the target measure differs substantially from the prior, requiring better optimization approaches.

Method: Iteratively solving constrained problems with trust regions that gradually approach the target measure, implementing geometric annealing with principled time step selection.

Result: The method significantly improves performance in various optimal control applications including diffusion-based sampling, transition path sampling, and diffusion model fine-tuning.

Conclusion: Trust region-based geometric annealing provides a systematic and effective approach for solving challenging stochastic optimal control problems with substantial differences between prior and target measures.

Abstract: Solving stochastic optimal control problems with quadratic control costs can
be viewed as approximating a target path space measure, e.g. via gradient-based
optimization. In practice, however, this optimization is challenging in
particular if the target measure differs substantially from the prior. In this
work, we therefore approach the problem by iteratively solving constrained
problems incorporating trust regions that aim for approaching the target
measure gradually in a systematic way. It turns out that this trust region
based strategy can be understood as a geometric annealing from the prior to the
target measure, where, however, the incorporated trust regions lead to a
principled and educated way of choosing the time steps in the annealing path.
We demonstrate in multiple optimal control applications that our novel method
can improve performance significantly, including tasks in diffusion-based
sampling, transition path sampling, and fine-tuning of diffusion models.

</details>


### [344] [Results of the NeurIPS 2023 Neural MMO Competition on Multi-task Reinforcement Learning](https://arxiv.org/abs/2508.12524)
*Joseph Suárez,Kyoung Whan Choe,David Bloomin,Jianming Gao,Yunkun Li,Yao Feng,Saidinesh Pola,Kun Zhang,Yonghui Zhu,Nikhil Pinnaparaju,Hao Xiang Li,Nishaanth Kanna,Daniel Scott,Ryan Sullivan,Rose S. Shuman,Lucas de Alcântara,Herbie Bradley,Kirsty You,Bo Wu,Yuhao Jiang,Qimai Li,Jiaxin Chen,Louis Castricato,Xiaolong Zhu,Phillip Isola*

Main category: cs.LG

TL;DR: NeurIPS 2023 Neural MMO Competition results with over 200 participants, where top solution achieved 4x baseline performance in 8 hours on single GPU.


<details>
  <summary>Details</summary>
Motivation: To advance research in goal-conditional policies that generalize to unseen tasks, maps, and opponents through competitive benchmarking.

Method: Competition framework where participants trained goal-conditional policies on Neural MMO platform, with evaluation on unseen environments and opponents.

Result: Top solution outperformed baseline by 4x within 8 hours of training on a single 4090 GPU, with over 200 participants submitting solutions.

Conclusion: Successful competition demonstrating rapid progress in generalization capabilities, with full open-sourcing of code, baseline, and top submissions under MIT license.

Abstract: We present the results of the NeurIPS 2023 Neural MMO Competition, which
attracted over 200 participants and submissions. Participants trained
goal-conditional policies that generalize to tasks, maps, and opponents never
seen during training. The top solution achieved a score 4x higher than our
baseline within 8 hours of training on a single 4090 GPU. We open-source
everything relating to Neural MMO and the competition under the MIT license,
including the policy weights and training code for our baseline and for the top
submissions.

</details>


### [345] [Toward Architecture-Agnostic Local Control of Posterior Collapse in VAEs](https://arxiv.org/abs/2508.12530)
*Hyunsoo Song,Seungwhan Kim,Seungkyu Lee*

Main category: cs.LG

TL;DR: Proposes Latent Reconstruction loss to address posterior collapse in VAEs without architectural constraints, using mathematical properties of injective functions to improve sample diversity.


<details>
  <summary>Details</summary>
Motivation: VAEs suffer from posterior collapse that reduces sample diversity. Existing methods require architectural constraints or have unsatisfactory trade-offs between reconstruction and regularization.

Method: Defines local posterior collapse and proposes Latent Reconstruction loss based on mathematical properties of injective and composite functions to control posterior collapse without specific architecture restrictions.

Result: Experimentally evaluated on MNIST, fashionMNIST, Omniglot, CelebA, and FFHQ datasets, successfully controlling posterior collapse across varied datasets.

Conclusion: The proposed LR loss effectively addresses posterior collapse in VAEs without requiring structural network constraints, improving generative diversity across multiple datasets.

Abstract: Variational autoencoders (VAEs), one of the most widely used generative
models, are known to suffer from posterior collapse, a phenomenon that reduces
the diversity of generated samples. To avoid posterior collapse, many prior
works have tried to control the influence of regularization loss. However, the
trade-off between reconstruction and regularization is not satisfactory. For
this reason, several methods have been proposed to guarantee latent
identifiability, which is the key to avoiding posterior collapse. However, they
require structural constraints on the network architecture. For further
clarification, we define local posterior collapse to reflect the importance of
individual sample points in the data space and to relax the network constraint.
Then, we propose Latent Reconstruction(LR) loss, which is inspired by
mathematical properties of injective and composite functions, to control
posterior collapse without restriction to a specific architecture. We
experimentally evaluate our approach, which controls posterior collapse on
varied datasets such as MNIST, fashionMNIST, Omniglot, CelebA, and FFHQ.

</details>


### [346] [Rethinking Safety in LLM Fine-tuning: An Optimization Perspective](https://arxiv.org/abs/2508.12531)
*Minseon Kim,Jin Myung Kwak,Lama Alssum,Bernard Ghanem,Philip Torr,David Krueger,Fazl Barez,Adel Bibi*

Main category: cs.LG

TL;DR: Fine-tuning language models doesn't inherently harm safety - poor optimization choices cause safety issues, not inherent trade-offs. Proper hyperparameter selection and EMA momentum technique can maintain safety while preserving utility.


<details>
  <summary>Details</summary>
Motivation: Challenge the common belief that fine-tuning inevitably harms model safety, showing that optimization choices rather than inherent limitations cause safety degradation.

Method: Systematic testing of hyperparameters (learning rate, batch size, gradient steps) and proposing exponential moving average (EMA) momentum technique to preserve safety properties during fine-tuning.

Result: Reduced unsafe responses from 16% to ~5% while maintaining utility performance across Llama models on multiple datasets (Dolly, Alpaca, ORCA).

Conclusion: Safety problems during fine-tuning can be largely avoided without specialized interventions, outperforming approaches requiring additional safety data while providing practical guidelines.

Abstract: Fine-tuning language models is commonly believed to inevitably harm their
safety, i.e., refusing to respond to harmful user requests, even when using
harmless datasets, thus requiring additional safety measures. We challenge this
belief through systematic testing, showing that poor optimization choices,
rather than inherent trade-offs, often cause safety problems, measured as
harmful responses to adversarial prompts. By properly selecting key training
hyper-parameters, e.g., learning rate, batch size, and gradient steps, we
reduce unsafe model responses from 16\% to approximately 5\%, as measured by
keyword matching, while maintaining utility performance. Based on this
observation, we propose a simple exponential moving average (EMA) momentum
technique in parameter space that preserves safety performance by creating a
stable optimization path and retains the original pre-trained model's safety
properties. Our experiments on the Llama families across multiple datasets
(Dolly, Alpaca, ORCA) demonstrate that safety problems during fine-tuning can
largely be avoided without specialized interventions, outperforming existing
approaches that require additional safety data while offering practical
guidelines for maintaining both model performance and safety during adaptation.

</details>


### [347] [Defining and Benchmarking a Data-Centric Design Space for Brain Graph Construction](https://arxiv.org/abs/2508.12533)
*Qinwen Ge,Roza G. Bayrak,Anwar Said,Catie Chang,Xenofon Koutsoukos,Tyler Derr*

Main category: cs.LG

TL;DR: Systematic benchmarking of data-centric design choices in brain graph construction from fMRI data shows that thoughtful configurations outperform standard pipelines in classification tasks.


<details>
  <summary>Details</summary>
Motivation: Current brain graph construction practices rely on rigid pipelines that overlook critical data-centric choices, limiting the potential of graph machine learning in neuroimaging.

Method: Organized a data-centric design space into three stages: temporal signal processing, topology extraction, and graph featurization. Evaluated combinations of existing techniques including BOLD signal filtering, sparsification strategies, alternative correlation metrics, and multi-view node/edge features.

Result: Experiments on HCP1200 and ABIDE datasets showed that data-centric configurations consistently improve classification accuracy over standard pipelines.

Conclusion: Upstream data decisions play a critical role in graph-based neuroimaging, highlighting the importance of systematically exploring the data-centric design space rather than focusing primarily on model-centric approaches.

Abstract: The construction of brain graphs from functional Magnetic Resonance Imaging
(fMRI) data plays a crucial role in enabling graph machine learning for
neuroimaging. However, current practices often rely on rigid pipelines that
overlook critical data-centric choices in how brain graphs are constructed. In
this work, we adopt a Data-Centric AI perspective and systematically define and
benchmark a data-centric design space for brain graph construction,
constrasting with primarily model-centric prior work. We organize this design
space into three stages: temporal signal processing, topology extraction, and
graph featurization. Our contributions lie less in novel components and more in
evaluating how combinations of existing and modified techniques influence
downstream performance. Specifically, we study high-amplitude BOLD signal
filtering, sparsification and unification strategies for connectivity,
alternative correlation metrics, and multi-view node and edge features, such as
incorporating lagged dynamics. Experiments on the HCP1200 and ABIDE datasets
show that thoughtful data-centric configurations consistently improve
classification accuracy over standard pipelines. These findings highlight the
critical role of upstream data decisions and underscore the importance of
systematically exploring the data-centric design space for graph-based
neuroimaging. Our code is available at
https://github.com/GeQinwen/DataCentricBrainGraphs.

</details>


### [348] [OS-R1: Agentic Operating System Kernel Tuning with Reinforcement Learning](https://arxiv.org/abs/2508.12551)
*Hongyu Lin,Yuchen Li,Haoran Luo,Kaichun Yao,Libo Zhang,Mingjie Xing,Yanjun Wu*

Main category: cs.LG

TL;DR: OS-R1 is a rule-based reinforcement learning framework for Linux kernel tuning that uses LLMs to efficiently explore configuration spaces, achieving up to 5.6% performance improvement over heuristic methods with high data efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing Linux kernel tuning methods face challenges in efficiency, scalability, and generalization, requiring a more effective automated approach.

Method: Uses rule-based reinforcement learning to abstract kernel configuration as RL environment, employs LLMs for exploration, custom reward functions for standardization, and a two-phase training process for faster convergence.

Result: Achieves up to 5.6% performance improvement over heuristic tuning methods while maintaining high data efficiency and adaptability across diverse real-world applications.

Conclusion: OS-R1 demonstrates significant practical potential for automated Linux kernel optimization and is publicly available for deployment in various environments.

Abstract: Linux kernel tuning is essential for optimizing operating system (OS)
performance. However, existing methods often face challenges in terms of
efficiency, scalability, and generalization. This paper introduces OS-R1, an
agentic Linux kernel tuning framework powered by rule-based reinforcement
learning (RL). By abstracting the kernel configuration space as an RL
environment, OS-R1 facilitates efficient exploration by large language models
(LLMs) and ensures accurate configuration modifications. Additionally, custom
reward functions are designed to enhance reasoning standardization,
configuration modification accuracy, and system performance awareness of the
LLMs. Furthermore, we propose a two-phase training process that accelerates
convergence and minimizes retraining across diverse tuning scenarios.
Experimental results show that OS-R1 significantly outperforms existing
baseline methods, achieving up to 5.6% performance improvement over heuristic
tuning and maintaining high data efficiency. Notably, OS-R1 is adaptable across
various real-world applications, demonstrating its potential for practical
deployment in diverse environments. Our dataset and code are publicly available
at https://github.com/LHY-24/OS-R1.

</details>


### [349] [Illuminating LLM Coding Agents: Visual Analytics for Deeper Understanding and Enhancement](https://arxiv.org/abs/2508.12555)
*Junpeng Wang,Yuzhong Chen,Menghai Pan,Chin-Chia Michael Yeh,Mahashweta Das*

Main category: cs.LG

TL;DR: A visual analytics system for examining coding agent behaviors across code-level, process-level, and LLM-level analysis to help ML scientists debug and improve AI coding agents.


<details>
  <summary>Details</summary>
Motivation: Current manual inspection of LLM-powered coding agents is inefficient, making it difficult to track code evolution, compare iterations, and identify improvement opportunities in frameworks like AIDE.

Method: Developed a visual analytics system that supports comparative analysis at three levels: code-level (debugging and refinement), process-level (solution-seeking processes), and LLM-level (behavior variations across different LLMs).

Result: The system enables structured understanding of agent behaviors, facilitating effective debugging and prompt engineering, as demonstrated through case studies using Kaggle competitions.

Conclusion: The visual analytics system provides valuable insights into iterative coding processes, helping ML scientists better review and adjust coding agents' behaviors across multiple analytical dimensions.

Abstract: Coding agents powered by large language models (LLMs) have gained traction
for automating code generation through iterative problem-solving with minimal
human involvement. Despite the emergence of various frameworks, e.g.,
LangChain, AutoML, and AIDE, ML scientists still struggle to effectively review
and adjust the agents' coding process. The current approach of manually
inspecting individual outputs is inefficient, making it difficult to track code
evolution, compare coding iterations, and identify improvement opportunities.
To address this challenge, we introduce a visual analytics system designed to
enhance the examination of coding agent behaviors. Focusing on the AIDE
framework, our system supports comparative analysis across three levels: (1)
Code-Level Analysis, which reveals how the agent debugs and refines its code
over iterations; (2) Process-Level Analysis, which contrasts different
solution-seeking processes explored by the agent; and (3) LLM-Level Analysis,
which highlights variations in coding behavior across different LLMs. By
integrating these perspectives, our system enables ML scientists to gain a
structured understanding of agent behaviors, facilitating more effective
debugging and prompt engineering. Through case studies using coding agents to
tackle popular Kaggle competitions, we demonstrate how our system provides
valuable insights into the iterative coding process.

</details>


### [350] [Deep Learning-Based Financial Time Series Forecasting via Sliding Window and Variational Mode Decomposition](https://arxiv.org/abs/2508.12565)
*Luke Li*

Main category: cs.LG

TL;DR: Proposes VMD-LSTM model combining variational mode decomposition with deep learning for financial time series forecasting, showing improved performance over raw data approaches.


<details>
  <summary>Details</summary>
Motivation: To address the complexity and non-stationarity of financial time series data which makes accurate forecasting challenging.

Method: Uses variational mode decomposition (VMD) to decompose non-stationary financial time series into smoother subcomponents, then feeds the decomposed data into an LSTM deep learning model for prediction.

Result: The VMD-processed LSTM model demonstrates better forecasting performance and stability compared to models using raw time series data.

Conclusion: Combining VMD decomposition with deep learning models effectively improves financial time series forecasting accuracy and robustness.

Abstract: To address the complexity of financial time series, this paper proposes a
forecasting model combining sliding window and variational mode decomposition
(VMD) methods. Historical stock prices and relevant market indicators are used
to construct datasets. VMD decomposes non-stationary financial time series into
smoother subcomponents, improving model adaptability. The decomposed data is
then input into a deep learning model for prediction. The study compares the
forecasting effects of an LSTM model trained on VMD-processed sequences with
those using raw time series, demonstrating better performance and stability.

</details>


### [351] [Data-driven particle dynamics: Structure-preserving coarse-graining for emergent behavior in non-equilibrium systems](https://arxiv.org/abs/2508.12569)
*Quercus Hernandez,Max Win,Thomas C. O'Connor,Paulo E. Arratia,Nathaniel Trask*

Main category: cs.LG

TL;DR: A machine learning framework for coarse-graining multiscale systems using metriplectic bracket formalism that preserves thermodynamic laws and fluctuation-dissipation balance.


<details>
  <summary>Details</summary>
Motivation: Multiscale systems are challenging to simulate due to information loss during coarse-graining, leading to emergent dissipative, history-dependent, and stochastic physics that needs to be properly captured.

Method: Proposes a framework using metriplectic bracket formalism that guarantees thermodynamic consistency, introduces self-supervised learning to identify emergent structural variables, and validates on benchmark systems including star polymers and colloidal suspensions.

Result: The method successfully coarse-grains challenging systems like star polymers while preserving non-equilibrium statistics and captures coupling between local rearrangement events and emergent stochastic dynamics from high-speed video data.

Conclusion: The framework provides thermodynamically consistent coarse-grained models with open-source implementations in PyTorch and LAMMPS, enabling large-scale inference for diverse particle-based systems.

Abstract: Multiscale systems are ubiquitous in science and technology, but are
notoriously challenging to simulate as short spatiotemporal scales must be
appropriately linked to emergent bulk physics. When expensive high-dimensional
dynamical systems are coarse-grained into low-dimensional models, the entropic
loss of information leads to emergent physics which are dissipative,
history-dependent, and stochastic. To machine learn coarse-grained dynamics
from time-series observations of particle trajectories, we propose a framework
using the metriplectic bracket formalism that preserves these properties by
construction; most notably, the framework guarantees discrete notions of the
first and second laws of thermodynamics, conservation of momentum, and a
discrete fluctuation-dissipation balance crucial for capturing non-equilibrium
statistics. We introduce the mathematical framework abstractly before
specializing to a particle discretization. As labels are generally unavailable
for entropic state variables, we introduce a novel self-supervised learning
strategy to identify emergent structural variables. We validate the method on
benchmark systems and demonstrate its utility on two challenging examples: (1)
coarse-graining star polymers at challenging levels of coarse-graining while
preserving non-equilibrium statistics, and (2) learning models from high-speed
video of colloidal suspensions that capture coupling between local
rearrangement events and emergent stochastic dynamics. We provide open-source
implementations in both PyTorch and LAMMPS, enabling large-scale inference and
extensibility to diverse particle-based systems.

</details>


### [352] [Deep Learning Model for Amyloidogenicity Prediction using a Pre-trained Protein LLM](https://arxiv.org/abs/2508.12575)
*Zohra Yagoub,Hafida Bouziane*

Main category: cs.LG

TL;DR: Using protein language models with bidirectional LSTM/GRU to predict amyloidogenic regions achieves 84.5% accuracy, demonstrating LLMs' potential for amyloid prediction.


<details>
  <summary>Details</summary>
Motivation: Current amyloid prediction methods rely heavily on evolutionary motifs and amino acid properties, but sequence-based features show high predictive performance, prompting exploration of protein language models.

Method: Leveraged pretrained protein large language model to extract contextual features, combined with bidirectional LSTM and GRU architectures for amyloidogenic region prediction.

Result: Achieved 84.5% accuracy on 10-fold cross-validation and 83% accuracy on test dataset, showing competitive performance.

Conclusion: Protein large language models significantly enhance amyloid prediction accuracy, demonstrating their potential as powerful tools in bioinformatics for amyloidogenicity prediction.

Abstract: The prediction of amyloidogenicity in peptides and proteins remains a focal
point of ongoing bioinformatics. The crucial step in this field is to apply
advanced computational methodologies. Many recent approaches to predicting
amyloidogenicity within proteins are highly based on evolutionary motifs and
the individual properties of amino acids. It is becoming increasingly evident
that the sequence information-based features show high predictive performance.
Consequently, our study evaluated the contextual features of protein sequences
obtained from a pretrained protein large language model leveraging
bidirectional LSTM and GRU to predict amyloidogenic regions in peptide and
protein sequences. Our method achieved an accuracy of 84.5% on 10-fold
cross-validation and an accuracy of 83% in the test dataset. Our results
demonstrate competitive performance, highlighting the potential of LLMs in
enhancing the accuracy of amyloid prediction.

</details>


### [353] [Widening the Network Mitigates the Impact of Data Heterogeneity on FedAvg](https://arxiv.org/abs/2508.12576)
*Like Jian,Dong Liu*

Main category: cs.LG

TL;DR: Overparameterized FedAvg with gradient descent converges better as neural network width increases, with data heterogeneity impact vanishing at infinite width where FedAvg matches centralized learning performance.


<details>
  <summary>Details</summary>
Motivation: Federated learning faces challenges with non-IID client data distributions, making it difficult to train global models that generalize well across heterogeneous local data.

Method: Theoretical analysis of FedAvg convergence with gradient descent on overparameterized neural networks, proving convergence properties as network width increases to infinity.

Result: Data heterogeneity impact diminishes with increasing network width, vanishing completely at infinite width where FedAvg behaves like linear models and achieves same generalization as centralized learning.

Conclusion: Overparameterization in neural networks effectively mitigates data heterogeneity challenges in federated learning, with infinite-width networks enabling FedAvg to match centralized learning performance.

Abstract: Federated learning (FL) enables decentralized clients to train a model
collaboratively without sharing local data. A key distinction between FL and
centralized learning is that clients' data are non-independent and identically
distributed, which poses significant challenges in training a global model that
generalizes well across heterogeneous local data distributions. In this paper,
we analyze the convergence of overparameterized FedAvg with gradient descent
(GD). We prove that the impact of data heterogeneity diminishes as the width of
neural networks increases, ultimately vanishing when the width approaches
infinity. In the infinite-width regime, we further prove that both the global
and local models in FedAvg behave as linear models, and that FedAvg achieves
the same generalization performance as centralized learning with the same
number of GD iterations. Extensive experiments validate our theoretical
findings across various network architectures, loss functions, and optimization
methods.

</details>


### [354] [Energy-Efficient Wireless LLM Inference via Uncertainty and Importance-Aware Speculative Decoding](https://arxiv.org/abs/2508.12590)
*Jihoon Park,Seungeun Oh,Seong-Lyun Kim*

Main category: cs.LG

TL;DR: Token-level filtering mechanism for hybrid LLMs that reduces energy consumption by 40.7% while maintaining high accuracy through selective upload of informative tokens based on uncertainty and importance.


<details>
  <summary>Details</summary>
Motivation: Address the need for energy-efficient on-device LLM inference in resource-constrained environments by reducing communication costs and energy consumption in hybrid language models, which current approaches overlook.

Method: Proposes a token-level filtering mechanism that leverages both epistemic uncertainty and attention-based importance to opportunistically upload only informative tokens to cloud-based LLMs, reducing LLM usage and communication overhead.

Result: Achieves up to 87.5% BERT Score, 0.37 tokens/sec throughput, and 40.7% energy savings compared to standard HLM. Outperforms previous U-HLM baseline with improved BERTScore (85.8% to 87.0%), energy savings (31.6% to 43.6%), and throughput (0.36 to 0.40).

Conclusion: Enables energy-efficient and accurate deployment of LLMs in bandwidth-constrained edge environments through selective token uploading based on uncertainty and importance metrics.

Abstract: To address the growing demand for on-device LLM inference in
resource-constrained environments, hybrid language models (HLM) have emerged,
combining lightweight local models with powerful cloud-based LLMs. Recent
studies on HLM have primarily focused on improving accuracy and latency, while
often overlooking communication and energy efficiency. We propose a token-level
filtering mechanism for an energy-efficient importance- and uncertainty-aware
HLM inference that leverages both epistemic uncertainty and attention-based
importance. Our method opportunistically uploads only informative tokens,
reducing LLM usage and communication costs. Experiments with TinyLlama-1.1B and
LLaMA-2-7B demonstrate that our method achieves up to 87.5% BERT Score and
token throughput of 0.37 tokens/sec while saving the energy consumption by
40.7% compared to standard HLM. Furthermore, compared to our previous U-HLM
baseline, our method improves BERTScore from 85.8% to 87.0%, energy savings
from 31.6% to 43.6%, and throughput from 0.36 to 0.40. This approach enables an
energy-efficient and accurate deployment of LLMs in bandwidth-constrained edge
environments.

</details>


### [355] [Physics-informed deep operator network for traffic state estimation](https://arxiv.org/abs/2508.12593)
*Zhihao Li,Ting Wang,Guojian Zou,Ruofei Wang,Ye Li*

Main category: cs.LG

TL;DR: PI-DeepONet framework for traffic state estimation that learns neural operators mapping sparse data to full traffic fields while enforcing physical conservation laws, outperforming traditional PINNs and other baselines on NGSIM data.


<details>
  <summary>Details</summary>
Motivation: Traditional Physics-Informed Neural Networks (PINNs) enforce PDE constraints point-wise but struggle with high-dimensional spatiotemporal traffic flow problems. There's a need for a more effective approach that integrates physical laws directly into operator learning for traffic state estimation from limited noisy measurements.

Method: Proposes physics-informed deep operator network (PI-DeepONet) that reformulates TSE as operator learning problem. Trains parameterized neural operator mapping sparse input data to full spatiotemporal traffic state field while integrating traffic flow conservation model and fundamental diagram directly into learning process.

Result: Superior performance over state-of-the-art baselines on NGSIM dataset. Framework effectively captures congestion propagation, spatial correlations, and temporal evolution while ensuring physical consistency. Analysis reveals optimal function generation strategies and branch network complexity requirements.

Conclusion: PI-DeepONet provides a robust and effective framework for traffic state estimation that successfully integrates physical constraints into operator learning, demonstrating significant advantages over point-wise PDE enforcement methods like PINNs.

Abstract: Traffic state estimation (TSE) fundamentally involves solving
high-dimensional spatiotemporal partial differential equations (PDEs) governing
traffic flow dynamics from limited, noisy measurements. While Physics-Informed
Neural Networks (PINNs) enforce PDE constraints point-wise, this paper adopts a
physics-informed deep operator network (PI-DeepONet) framework that
reformulates TSE as an operator learning problem. Our approach trains a
parameterized neural operator that maps sparse input data to the full
spatiotemporal traffic state field, governed by the traffic flow conservation
law. Crucially, unlike PINNs that enforce PDE constraints point-wise,
PI-DeepONet integrates traffic flow conservation model and the fundamental
diagram directly into the operator learning process, ensuring physical
consistency while capturing congestion propagation, spatial correlations, and
temporal evolution. Experiments on the NGSIM dataset demonstrate superior
performance over state-of-the-art baselines. Further analysis reveals insights
into optimal function generation strategies and branch network complexity.
Additionally, the impact of input function generation methods and the number of
functions on model performance is explored, highlighting the robustness and
efficacy of proposed framework.

</details>


### [356] [FLARE: Fast Low-rank Attention Routing Engine](https://arxiv.org/abs/2508.12594)
*Vedant Puri,Aditya Joglekar,Kevin Ferguson,Yu-hsuan Chen,Yongjie Jessica Zhang,Levent Burak Kara*

Main category: cs.LG

TL;DR: FLARE is a linear complexity self-attention mechanism that routes attention through fixed-length latent sequences to enable efficient processing of large unstructured meshes.


<details>
  <summary>Details</summary>
Motivation: The quadratic complexity of standard self-attention limits its scalability on large unstructured meshes, creating a need for more efficient attention mechanisms.

Method: FLARE projects input sequences onto fixed-length latent sequences using learnable query tokens, routing attention through a bottleneck sequence to achieve O(NM) complexity where M << N.

Result: FLARE scales to unprecedented problem sizes while delivering superior accuracy compared to state-of-the-art neural PDE surrogates across diverse benchmarks.

Conclusion: FLARE provides an efficient low-rank attention mechanism that enables scalable processing of large unstructured meshes with improved performance, and the authors release a new additive manufacturing dataset to support further research.

Abstract: The quadratic complexity of self-attention limits its applicability and
scalability on large unstructured meshes. We introduce Fast Low-rank Attention
Routing Engine (FLARE), a linear complexity self-attention mechanism that
routes attention through fixed-length latent sequences. Each attention head
performs global communication among $N$ tokens by projecting the input sequence
onto a fixed length latent sequence of $M \ll N$ tokens using learnable query
tokens. By routing attention through a bottleneck sequence, FLARE learns a
low-rank form of attention that can be applied at $O(NM)$ cost. FLARE not only
scales to unprecedented problem sizes, but also delivers superior accuracy
compared to state-of-the-art neural PDE surrogates across diverse benchmarks.
We also release a new additive manufacturing dataset to spur further research.
Our code is available at https://github.com/vpuri3/FLARE.py.

</details>


### [357] [Constructing Invariant and Equivariant Operations by Symmetric Tensor Network](https://arxiv.org/abs/2508.12596)
*Meng Zhang,Chao Wang,Hao Zhang,Shaojun Dong,Lixin He*

Main category: cs.LG

TL;DR: A systematic method for constructing invariant and equivariant operations in neural networks using symmetric tensor networks, with applications to geometric graph neural networks and material constitutive law learning.


<details>
  <summary>Details</summary>
Motivation: To develop neural networks that incorporate symmetry for geometric deep learning, requiring systematic construction of invariant and equivariant operations that can handle various tensor types.

Method: Presents a systematic construction method using symmetric tensor networks with graphical representation, handling Cartesian tensors of different ranks and spherical tensors of different types.

Result: Developed a method that simplifies proofs and constructions of invariant and equivariant functions, and successfully applied it to design equivariant interaction messages for geometry graph neural networks.

Conclusion: The approach provides an effective framework for building symmetry-incorporated neural networks with practical applications in geometric deep learning and material science.

Abstract: Design of neural networks that incorporate symmetry is crucial for geometric
deep learning. Central to this effort is the development of invariant and
equivariant operations. This works presents a systematic method for
constructing valid invariant and equivariant operations. It can handle inputs
and outputs in the form of Cartesian tensors with different rank, as well as
spherical tensors with different types. In addition, our method features a
graphical representation utilizing the symmetric tensor network, which
simplifies both the proofs and constructions related to invariant and
equivariant functions. We also apply this approach to design the equivariant
interaction message for the geometry graph neural network, and equivariant
machine learning model to learn the constitutive law of materials.

</details>


### [358] [A Hybrid Surrogate for Electric Vehicle Parameter Estimation and Power Consumption via Physics-Informed Neural Operators](https://arxiv.org/abs/2508.12602)
*Hansol Lim,Jongseong Brad Choi,Jee Won Lee,Haeseong Jeoung,Minkyu Han*

Main category: cs.LG

TL;DR: Hybrid surrogate model combining Fourier Neural Operator with differentiable physics for EV parameter estimation from speed/acceleration data, achieving 0.2-0.8kW error on real-world Tesla and Kia vehicles.


<details>
  <summary>Details</summary>
Motivation: To develop an interpretable and accurate electric vehicle parameter estimation model that can extract physically meaningful parameters from minimal sensor data (speed and acceleration alone) for practical applications like eco-routing and diagnostics.

Method: Novel Spectral Parameter Operator architecture built on Fourier Neural Operator backbone for global context, combined with differentiable physics module in forward pass. Outputs time-varying motor/braking efficiencies, drag, rolling resistance, mass, and auxiliary power without separate physics-residual loss.

Result: Achieved mean absolute error of 0.2kW (~1% of average traction power) for Tesla vehicles and 0.8kW for Kia EV9 on real-world logs. Model generalizes well to unseen conditions and sampling rates.

Conclusion: The hybrid surrogate model provides interpretable, physically meaningful parameter estimation with high accuracy, making it practical for EV applications including path optimization, eco-routing, diagnostics, and health management.

Abstract: We present a hybrid surrogate model for electric vehicle parameter estimation
and power consumption. We combine our novel architecture Spectral Parameter
Operator built on a Fourier Neural Operator backbone for global context and a
differentiable physics module in the forward pass. From speed and acceleration
alone, it outputs time-varying motor and regenerative braking efficiencies, as
well as aerodynamic drag, rolling resistance, effective mass, and auxiliary
power. These parameters drive a physics-embedded estimate of battery power,
eliminating any separate physics-residual loss. The modular design lets
representations converge to physically meaningful parameters that reflect the
current state and condition of the vehicle. We evaluate on real-world logs from
a Tesla Model 3, Tesla Model S, and the Kia EV9. The surrogate achieves a mean
absolute error of 0.2kW (about 1% of average traction power at highway speeds)
for Tesla vehicles and about 0.8kW on the Kia EV9. The framework is
interpretable, and it generalizes well to unseen conditions, and sampling
rates, making it practical for path optimization, eco-routing, on-board
diagnostics, and prognostics health management.

</details>


### [359] [SSPO: Self-traced Step-wise Preference Optimization for Process Supervision and Reasoning Compression](https://arxiv.org/abs/2508.12604)
*Yuyang Xu,Yi Cheng,Haochao Ying,Zhuoyun Du,Renjun Hu,Xing Shi,Wei Lin,Jian Wu*

Main category: cs.LG

TL;DR: SSPO is a pluggable RL framework that uses self-generated step-wise preference signals to optimize reasoning steps in LLMs, reducing overthinking and improving efficiency without auxiliary models or manual annotations.


<details>
  <summary>Details</summary>
Motivation: Mainstream post-training methods for LLMs incur substantial computational overhead due to auxiliary models and overthinking, with incorrect answers often stemming from verbose reasoning processes lacking correct self-fix where errors accumulate.

Method: Self-traced Step-wise Preference Optimization (SSPO) - a pluggable RL process supervision framework that leverages step-wise preference signals generated by the model itself to guide optimization for reasoning compression, requiring no auxiliary models or stepwise manual annotations.

Result: Experiments show SSPO generates accurate and succinct reasoning sequences, effectively mitigating overthinking behaviors without compromising model performance across diverse domains and languages.

Conclusion: SSPO provides an efficient solution for optimizing LLM reasoning processes through self-generated step-wise preferences, eliminating the need for external resources while maintaining performance across various applications.

Abstract: Test-time scaling has proven effective in further enhancing the performance
of pretrained Large Language Models (LLMs). However, mainstream post-training
methods (i.e., reinforcement learning (RL) with chain-of-thought (CoT)
reasoning) often incur substantial computational overhead due to auxiliary
models and overthinking. In this paper, we empirically reveal that the
incorrect answers partially stem from verbose reasoning processes lacking
correct self-fix, where errors accumulate across multiple reasoning steps. To
this end, we propose Self-traced Step-wise Preference Optimization (SSPO), a
pluggable RL process supervision framework that enables fine-grained
optimization of each reasoning step. Specifically, SSPO requires neither
auxiliary models nor stepwise manual annotations. Instead, it leverages
step-wise preference signals generated by the model itself to guide the
optimization process for reasoning compression. Experiments demonstrate that
the generated reasoning sequences from SSPO are both accurate and succinct,
effectively mitigating overthinking behaviors without compromising model
performance across diverse domains and languages.

</details>


### [360] [How can we trust opaque systems? Criteria for robust explanations in XAI](https://arxiv.org/abs/2508.12623)
*Florian J. Boge,Annika Schuster*

Main category: cs.LG

TL;DR: The paper argues that current XAI methods lack trustworthiness and proposes two robustness criteria - explanatory robustness (ER) and explanation method robustness (EMR) - as necessary conditions for trustworthy explanations of deep learning algorithms.


<details>
  <summary>Details</summary>
Motivation: Deep learning algorithms are opaque and their inner workings are unknown, making it difficult to trust their predictions. While XAI methods promise to provide explanations, recent reviews show they may not be reliable.

Method: The authors develop and formalize criteria for explanatory robustness (different XAI methods producing the same explanations) and explanation method robustness (individual methods producing consistent explanations across similar contexts).

Result: The paper provides a framework for establishing trust in DL algorithms by requiring both ER and EMR as necessary conditions for trustworthy explanations.

Conclusion: Robustness of individual XAI methods is insufficient for trustworthiness - both explanatory robustness and explanation method robustness are required to ensure explanations truly reflect the underlying processes of DL algorithms.

Abstract: Deep learning (DL) algorithms are becoming ubiquitous in everyday life and in
scientific research. However, the price we pay for their impressively accurate
predictions is significant: their inner workings are notoriously opaque - it is
unknown to laypeople and researchers alike what features of the data a DL
system focuses on and how it ultimately succeeds in predicting correct outputs.
A necessary criterion for trustworthy explanations is that they should reflect
the relevant processes the algorithms' predictions are based on. The field of
eXplainable Artificial Intelligence (XAI) presents promising methods to create
such explanations. But recent reviews about their performance offer reasons for
skepticism. As we will argue, a good criterion for trustworthiness is
explanatory robustness: different XAI methods produce the same explanations in
comparable contexts. However, in some instances, all methods may give the same,
but still wrong, explanation. We therefore argue that in addition to
explanatory robustness (ER), a prior requirement of explanation method
robustness (EMR) has to be fulfilled by every XAI method. Conversely, the
robustness of an individual method is in itself insufficient for
trustworthiness. In what follows, we develop and formalize criteria for ER as
well as EMR, providing a framework for explaining and establishing trust in DL
algorithms. We also highlight interesting application cases and outline
directions for future work.

</details>


### [361] [FlowMol3: Flow Matching for 3D De Novo Small-Molecule Generation](https://arxiv.org/abs/2508.12629)
*Ian Dunn,David R. Koes*

Main category: cs.LG

TL;DR: FlowMol3 is an improved flow matching model for generating valid 3D drug-like molecules with 100% validity using three simple, low-cost techniques: self-conditioning, fake atoms, and train-time geometry distortion.


<details>
  <summary>Details</summary>
Motivation: To accelerate chemical discovery by developing a generative model that can sample realistic molecules with desired properties, particularly focusing on jointly sampling molecular topology and 3D structure.

Method: Uses flow matching with three architecture-agnostic techniques: self-conditioning, fake atoms, and train-time geometry distortion. These are applied without changing the graph neural network architecture or flow matching formulation.

Result: Achieves nearly 100% molecular validity for drug-like molecules, more accurately reproduces functional group composition and geometry of training data, and uses an order of magnitude fewer parameters than comparable methods.

Conclusion: The three simple techniques mitigate distribution drift during inference and provide transferable strategies for improving stability and quality of diffusion- and flow-based molecular generative models.

Abstract: A generative model capable of sampling realistic molecules with desired
properties could accelerate chemical discovery across a wide range of
applications. Toward this goal, significant effort has focused on developing
models that jointly sample molecular topology and 3D structure. We present
FlowMol3, an open-source, multi-modal flow matching model that advances the
state of the art for all-atom, small-molecule generation. Its substantial
performance gains over previous FlowMol versions are achieved without changes
to the graph neural network architecture or the underlying flow matching
formulation. Instead, FlowMol3's improvements arise from three
architecture-agnostic techniques that incur negligible computational cost:
self-conditioning, fake atoms, and train-time geometry distortion. FlowMol3
achieves nearly 100% molecular validity for drug-like molecules with explicit
hydrogens, more accurately reproduces the functional group composition and
geometry of its training data, and does so with an order of magnitude fewer
learnable parameters than comparable methods. We hypothesize that these
techniques mitigate a general pathology affecting transport-based generative
models, enabling detection and correction of distribution drift during
inference. Our results highlight simple, transferable strategies for improving
the stability and quality of diffusion- and flow-based molecular generative
models.

</details>


### [362] [Score-informed Neural Operator for Enhancing Ordering-based Causal Discovery](https://arxiv.org/abs/2508.12650)
*Jiyeon Kang,Songseong Kim,Chanhui Lee,Doyeong Hwang,Joanie Hayoun Chung,Yunkyung Ko,Sumin Lee,Sungwoong Kim,Sungbin Lim*

Main category: cs.LG

TL;DR: SciNO is a neural operator model that improves causal discovery by providing stable Hessian diagonal approximations for score matching, reducing order divergence by 42.7% on synthetic and 31.5% on real data compared to previous methods, while enabling better causal reasoning in LLMs.


<details>
  <summary>Details</summary>
Motivation: Existing causal ordering methods using Stein gradient estimators are computationally expensive and memory-intensive, while DiffAN suffers from numerical instability due to second-order derivatives of score models.

Method: Proposes Score-informed Neural Operator (SciNO), a probabilistic generative model in smooth function spaces that stably approximates Hessian diagonal and preserves structural information during score modeling. Also introduces a probabilistic control algorithm integrating SciNO's probability estimates with autoregressive model priors.

Result: SciNO reduces order divergence by 42.7% on synthetic graphs and 31.5% on real-world datasets compared to DiffAN, while maintaining memory efficiency and scalability. Enhances causal reasoning abilities of LLMs without additional fine-tuning.

Conclusion: SciNO provides a stable and efficient approach for causal discovery that significantly outperforms existing methods and enables improved causal reasoning capabilities in large language models through probabilistic integration with autoregressive priors.

Abstract: Ordering-based approaches to causal discovery identify topological orders of
causal graphs, providing scalable alternatives to combinatorial search methods.
Under the Additive Noise Model (ANM) assumption, recent causal ordering methods
based on score matching require an accurate estimation of the Hessian diagonal
of the log-densities. However, previous approaches mainly use Stein gradient
estimators, which are computationally expensive and memory-intensive. Although
DiffAN addresses these limitations by substituting kernel-based estimates with
diffusion models, it remains numerically unstable due to the second-order
derivatives of score models. To alleviate these problems, we propose
Score-informed Neural Operator (SciNO), a probabilistic generative model in
smooth function spaces designed to stably approximate the Hessian diagonal and
to preserve structural information during the score modeling. Empirical results
show that SciNO reduces order divergence by 42.7% on synthetic graphs and by
31.5% on real-world datasets on average compared to DiffAN, while maintaining
memory efficiency and scalability. Furthermore, we propose a probabilistic
control algorithm for causal reasoning with autoregressive models that
integrates SciNO's probability estimates with autoregressive model priors,
enabling reliable data-driven causal ordering informed by semantic information.
Consequently, the proposed method enhances causal reasoning abilities of LLMs
without additional fine-tuning or prompt engineering.

</details>


### [363] [Robust Federated Learning under Adversarial Attacks via Loss-Based Client Clustering](https://arxiv.org/abs/2508.12672)
*Emmanouil Kritharakis,Dusan Jakovetic,Antonios Makris,Konstantinos Tserpes*

Main category: cs.LG

TL;DR: Robust federated learning algorithm that requires only one honest client and a trusted server with side data to defend against Byzantine attacks, outperforming existing baselines.


<details>
  <summary>Details</summary>
Motivation: Federated learning is vulnerable to adversarial (Byzantine) attacks from malicious clients, and existing robust aggregation methods often require knowing the number of malicious clients or have limited effectiveness.

Method: Proposes a FL approach that uses a trusted server with trustworthy side dataset and requires only one honest client to function effectively, without prior knowledge of malicious client count. Uses theoretical analysis to bound optimality gaps under strong attacks.

Result: Significantly outperforms standard and robust FL baselines (Mean, Trimmed Mean, Median, Krum, Multi-Krum) under various attack strategies including label flipping, sign flipping, and Gaussian noise across MNIST, FMNIST, and CIFAR-10 benchmarks using Flower framework.

Conclusion: The approach provides effective Byzantine robustness in FL with minimal trust assumptions (only server and one honest client needed) and no prior knowledge of attack scale, demonstrating strong theoretical guarantees and empirical performance.

Abstract: Federated Learning (FL) enables collaborative model training across multiple
clients without sharing private data. We consider FL scenarios wherein FL
clients are subject to adversarial (Byzantine) attacks, while the FL server is
trusted (honest) and has a trustworthy side dataset. This may correspond to,
e.g., cases where the server possesses trusted data prior to federation, or to
the presence of a trusted client that temporarily assumes the server role. Our
approach requires only two honest participants, i.e., the server and one
client, to function effectively, without prior knowledge of the number of
malicious clients. Theoretical analysis demonstrates bounded optimality gaps
even under strong Byzantine attacks. Experimental results show that our
algorithm significantly outperforms standard and robust FL baselines such as
Mean, Trimmed Mean, Median, Krum, and Multi-Krum under various attack
strategies including label flipping, sign flipping, and Gaussian noise addition
across MNIST, FMNIST, and CIFAR-10 benchmarks using the Flower framework.

</details>


### [364] [Deploying Models to Non-participating Clients in Federated Learning without Fine-tuning: A Hypernetwork-based Approach](https://arxiv.org/abs/2508.12673)
*Yuhao Zhou,Jindi Lv,Yuxin Tian,Dan Si,Qing Ye,Jiancheng Lv*

Main category: cs.LG

TL;DR: HyperFedZero is a federated learning method that uses hypernetworks to generate specialized models for non-participating clients with data distribution shifts, addressing generalization challenges through distribution-aware embeddings.


<details>
  <summary>Details</summary>
Motivation: Existing federated learning methods fail to generalize to non-participating clients with in-domain distribution shifts and resource constraints, creating a need for more adaptable solutions.

Method: Dynamically generates specialized models via hypernetwork conditioned on distribution-aware embeddings, using NoisyEmbed-enhanced extractor with Balancing Penalty to prevent feature collapse and generate models chunk-by-chunk.

Result: Extensive experiments show remarkable performance surpassing competing methods with minimal computational, storage, and communication overhead. Ablation studies confirm component effectiveness.

Conclusion: HyperFedZero effectively addresses generalization to non-participating clients in federated learning through distribution-aware model specialization, validated by comprehensive experiments.

Abstract: Federated Learning (FL) has emerged as a promising paradigm for
privacy-preserving collaborative learning, yet data heterogeneity remains a
critical challenge. While existing methods achieve progress in addressing data
heterogeneity for participating clients, they fail to generalize to
non-participating clients with in-domain distribution shifts and resource
constraints. To mitigate this issue, we present HyperFedZero, a novel method
that dynamically generates specialized models via a hypernetwork conditioned on
distribution-aware embeddings. Our approach explicitly incorporates
distribution-aware inductive biases into the model's forward pass, extracting
robust distribution embeddings using a NoisyEmbed-enhanced extractor with a
Balancing Penalty, effectively preventing feature collapse. The hypernetwork
then leverages these embeddings to generate specialized models chunk-by-chunk
for non-participating clients, ensuring adaptability to their unique data
distributions. Extensive experiments on multiple datasets and models
demonstrate HyperFedZero's remarkable performance, surpassing competing methods
consistently with minimal computational, storage, and communication overhead.
Moreover, ablation studies and visualizations further validate the necessity of
each component, confirming meaningful adaptations and validating the
effectiveness of HyperFedZero.

</details>


### [365] [BUILDA: A Thermal Building Data Generation Framework for Transfer Learning](https://arxiv.org/abs/2508.12703)
*Thomas Krug,Fabian Raisch,Dominik Aimer,Markus Wirnsberger,Ferdinand Sigg,Benjamin Schäfer,Benjamin Tischler*

Main category: cs.LG

TL;DR: BuilDa is a framework that generates synthetic thermal building data for transfer learning research without requiring expert building simulation knowledge, addressing the current lack of adequate public datasets.


<details>
  <summary>Details</summary>
Motivation: Transfer learning research for building thermal dynamics requires large amounts of high-quality data, but existing datasets and generators are insufficient and typically require expert simulation knowledge.

Method: BuilDa uses a single-zone Modelica model exported as a Functional Mock-up Unit (FMU) and simulated in Python to generate synthetic thermal building data.

Result: The framework successfully generates adequate quality and quantity of data for transfer learning research, demonstrated through pretraining and fine-tuning applications.

Conclusion: BuilDa provides a practical solution for generating the large volumes of thermal building data needed for transfer learning research without requiring deep building simulation expertise.

Abstract: Transfer learning (TL) can improve data-driven modeling of building thermal
dynamics. Therefore, many new TL research areas emerge in the field, such as
selecting the right source model for TL. However, these research directions
require massive amounts of thermal building data which is lacking presently.
Neither public datasets nor existing data generators meet the needs of TL
research in terms of data quality and quantity. Moreover, existing data
generation approaches typically require expert knowledge in building
simulation. We present BuilDa, a thermal building data generation framework for
producing synthetic data of adequate quality and quantity for TL research. The
framework does not require profound building simulation knowledge to generate
large volumes of data. BuilDa uses a single-zone Modelica model that is
exported as a Functional Mock-up Unit (FMU) and simulated in Python. We
demonstrate BuilDa by generating data and utilizing it for pretraining and
fine-tuning TL models.

</details>


### [366] [Argos: A Decentralized Federated System for Detection of Traffic Signs in CAVs](https://arxiv.org/abs/2508.12712)
*Seyed Mahdi Haji Seyed Hossein,Alireza Hosseini,Soheil Hajian Manesh,Amirali Shahriary*

Main category: cs.LG

TL;DR: Federated learning framework for traffic sign detection in vehicular networks that enables collaborative training without sharing raw data, achieving up to 0.83 accuracy while preserving privacy.


<details>
  <summary>Details</summary>
Motivation: Address privacy and communication challenges from vast sensor data generated by connected/automated vehicles, avoiding centralized machine learning approaches that require raw data sharing.

Method: Decentralized federated learning with specialized local training using lightweight object detectors, parameter aggregation via FedProx/FedAdam/FedAVG algorithms, and evaluation of multiple configurations (server rounds, local epochs, client participation, data distributions) in simulated Flower framework environment.

Result: Increasing server rounds (2→20) boosted accuracy from <0.1 to >0.8, moderate local epochs (8-10) achieved ~0.67 accuracy, higher client participation enhanced generalization to 0.83, FedProx outperformed other aggregators in heterogeneous settings, non-IID data reduced performance vs IID, training duration scaled with rounds not aggregation strategy.

Conclusion: Federated learning offers scalable, privacy-preserving solution for real-world vehicular deployments, with potential for future integrations of robust aggregation and communication optimizations to advance intelligent transportation systems.

Abstract: Connected and automated vehicles generate vast amounts of sensor data daily,
raising significant privacy and communication challenges for centralized
machine learning approaches in perception tasks. This study presents a
decentralized, federated learning framework tailored for traffic sign detection
in vehicular networks to enable collaborative model training without sharing
raw data. The framework partitioned traffic sign classes across vehicles for
specialized local training using lightweight object detectors, aggregated model
parameters via algorithms like FedProx, FedAdam and FedAVG in a simulated
environment with the Flower framework, and evaluated multiple configurations
including varying server rounds, local epochs, client participation fractions,
and data distributions. Experiments demonstrated that increasing server rounds
from 2 to 20 boosted accuracy from below 0.1 to over 0.8, moderate local epochs
(8-10) provided optimal efficiency with accuracies around 0.67, higher client
participation fractions enhanced generalization up to 0.83, FedProx
outperformed other aggregators in handling heterogeneity, non-IID data
distributions reduced performance compared to IID, and training duration
primarily scaled with the number of rounds rather than aggregation strategy. We
conclude that this federated approach may offer a scalable, privacy-preserving
solution for real-world vehicular deployments, potentially guiding future
integrations of robust aggregation and communication optimizations to advance
intelligent transportation systems.

</details>


### [367] [FedSODA: Federated Fine-tuning of LLMs via Similarity Group Pruning and Orchestrated Distillation Alignment](https://arxiv.org/abs/2508.12727)
*Manning Zhu,Songtao Guo,Pengzhan Zhou,Yansong Ning,Chang Han,Dewen Qiao*

Main category: cs.LG

TL;DR: FedSODA is a resource-efficient federated fine-tuning framework that reduces computational and memory demands by pruning redundant LLM layers and using orchestrated distillation alignment with QLoRA.


<details>
  <summary>Details</summary>
Motivation: Federated fine-tuning of LLMs faces challenges with high computational and memory requirements on resource-constrained clients, limiting practical deployment.

Method: Uses similarity group pruning to remove redundant layers while preserving critical ones, orchestrated distillation alignment to reduce gradient divergence, and QLoRA for quantized sub-LLMs with lightweight adapter fine-tuning.

Result: Reduces communication overhead by 70.6%, decreases storage usage by 75.6%, and improves task accuracy by 3.1% across various downstream tasks.

Conclusion: FedSODA enables practical federated fine-tuning under resource constraints while maintaining performance and privacy.

Abstract: Federated fine-tuning (FFT) of large language models (LLMs) has recently
emerged as a promising solution to enable domain-specific adaptation while
preserving data privacy. Despite its benefits, FFT on resource-constrained
clients relies on the high computational and memory demands of full-model
fine-tuning, which limits the potential advancement. This paper presents
FedSODA, a resource-efficient FFT framework that enables clients to adapt LLMs
without accessing or storing the full model. Specifically, we first propose a
similarity group pruning (SGP) module, which prunes redundant layers from the
full LLM while retaining the most critical layers to preserve the model
performance. Moreover, we introduce an orchestrated distillation alignment
(ODA) module to reduce gradient divergence between the sub-LLM and the full LLM
during FFT. Through the use of the QLoRA, clients only need to deploy quantized
sub-LLMs and fine-tune lightweight adapters, significantly reducing local
resource requirements. We conduct extensive experiments on three open-source
LLMs across a variety of downstream tasks. The experimental results demonstrate
that FedSODA reduces communication overhead by an average of 70.6%, decreases
storage usage by 75.6%, and improves task accuracy by 3.1%, making it highly
suitable for practical FFT applications under resource constraints.

</details>


### [368] [FedUNet: A Lightweight Additive U-Net Module for Federated Learning with Heterogeneous Models](https://arxiv.org/abs/2508.12740)
*Beomseok Seo,Kichang Lee,JaeYeon Park*

Main category: cs.LG

TL;DR: FedUNet is a lightweight federated learning framework that uses U-Net-inspired additive modules to enable heterogeneous model training across clients with minimal communication overhead.


<details>
  <summary>Details</summary>
Motivation: Existing federated learning methods assume identical model architectures across clients, which limits applicability in real-world heterogeneous environments where clients may have different hardware capabilities and model architectures.

Method: Attaches a U-Net-inspired additive module to each client's backbone, sharing only the compact bottleneck of the U-Net for efficient knowledge transfer without requiring structural alignment between different client models.

Result: Achieves 93.11% accuracy with standard version and 92.68% with lightweight version, while maintaining only 0.89 MB communication overhead, as demonstrated with VGG variants.

Conclusion: FedUNet enables effective federated learning across heterogeneous model architectures with minimal communication cost, making it suitable for real-world deployment where clients have varying computational resources and model structures.

Abstract: Federated learning (FL) enables decentralized model training without sharing
local data. However, most existing methods assume identical model architectures
across clients, limiting their applicability in heterogeneous real-world
environments. To address this, we propose FedUNet, a lightweight and
architecture-agnostic FL framework that attaches a U-Net-inspired additive
module to each client's backbone. By sharing only the compact bottleneck of the
U-Net, FedUNet enables efficient knowledge transfer without structural
alignment. The encoder-decoder design and skip connections in the U-Net help
capture both low-level and high-level features, facilitating the extraction of
clientinvariant representations. This enables cooperative learning between the
backbone and the additive module with minimal communication cost. Experiment
with VGG variants shows that FedUNet achieves 93.11% accuracy and 92.68% in
compact form (i.e., a lightweight version of FedUNet) with only 0.89 MB low
communication overhead.

</details>


### [369] [A Multi-Resolution Benchmark Framework for Spatial Reasoning Assessment in Neural Networks](https://arxiv.org/abs/2508.12741)
*Manuela Imbriani,Gina Belmonte,Mieke Massink,Alessandro Tofani,Vincenzo Ciancia*

Main category: cs.LG

TL;DR: A benchmark framework for evaluating neural network spatial reasoning capabilities using synthetic datasets and automated ML workflow, revealing systematic failures in geometric and topological understanding.


<details>
  <summary>Details</summary>
Motivation: To systematically evaluate spatial reasoning capabilities in neural networks, particularly focusing on morphological properties like connectivity and distance relationships, to identify limitations that could be addressed through hybrid approaches.

Method: Uses spatial model checker VoxLogicA to generate synthetic datasets (maze connectivity problems and spatial distance tasks) across multiple resolutions. Implements automated pipeline with dataset generation, standardized training with cross-validation, inference execution, and evaluation using Dice coefficient and IoU metrics.

Result: Preliminary results demonstrate significant challenges and systematic failures in neural network spatial reasoning capabilities for basic geometric and topological understanding tasks.

Conclusion: The framework provides reproducible experimental protocol to identify specific limitations, suggesting hybrid approaches combining neural networks with symbolic reasoning could improve spatial understanding for clinical applications.

Abstract: This paper presents preliminary results in the definition of a comprehensive
benchmark framework designed to systematically evaluate spatial reasoning
capabilities in neural networks, with a particular focus on morphological
properties such as connectivity and distance relationships. The framework is
currently being used to study the capabilities of nnU-Net, exploiting the
spatial model checker VoxLogicA to generate two distinct categories of
synthetic datasets: maze connectivity problems for topological analysis and
spatial distance computation tasks for geometric understanding. Each category
is evaluated across multiple resolutions to assess scalability and
generalization properties. The automated pipeline encompasses a complete
machine learning workflow including: synthetic dataset generation, standardized
training with cross-validation, inference execution, and comprehensive
evaluation using Dice coefficient and IoU (Intersection over Union) metrics.
Preliminary experimental results demonstrate significant challenges in neural
network spatial reasoning capabilities, revealing systematic failures in basic
geometric and topological understanding tasks. The framework provides a
reproducible experimental protocol, enabling researchers to identify specific
limitations. Such limitations could be addressed through hybrid approaches
combining neural networks with symbolic reasoning methods for improved spatial
understanding in clinical applications, establishing a foundation for ongoing
research into neural network spatial reasoning limitations and potential
solutions.

</details>


### [370] [Constrained Centroid Clustering: A Novel Approach for Compact and Structured Partitioning](https://arxiv.org/abs/2508.12758)
*Sowmini Devi Veeramachaneni,Ramamurthy Garimella*

Main category: cs.LG

TL;DR: CCC extends centroid clustering with maximum distance constraints, using Lagrangian formulation for closed-form solutions that control cluster spread while maintaining interpretability.


<details>
  <summary>Details</summary>
Motivation: Standard centroid-based clustering methods lack explicit control over cluster spread, which is needed for applications requiring structured clustering with bounded cluster sizes.

Method: Constrained Centroid Clustering (CCC) enforces maximum distance constraints between cluster centers and farthest points using Lagrangian formulation to derive closed-form solutions.

Result: Experiments on synthetic circular data show CCC achieves more compact clusters by reducing radial spread while preserving angular structure, outperforming K-means and GMM.

Conclusion: CCC provides effective spread-controlled clustering suitable for sensor networks, collaborative robotics, and interpretable pattern analysis applications.

Abstract: This paper presents Constrained Centroid Clustering (CCC), a method that
extends classical centroid-based clustering by enforcing a constraint on the
maximum distance between the cluster center and the farthest point in the
cluster. Using a Lagrangian formulation, we derive a closed-form solution that
maintains interpretability while controlling cluster spread. To evaluate CCC,
we conduct experiments on synthetic circular data with radial symmetry and
uniform angular distribution. Using ring-wise, sector-wise, and joint entropy
as evaluation metrics, we show that CCC achieves more compact clusters by
reducing radial spread while preserving angular structure, outperforming
standard methods such as K-means and GMM. The proposed approach is suitable for
applications requiring structured clustering with spread control, including
sensor networks, collaborative robotics, and interpretable pattern analysis.

</details>


### [371] [Short-Term Forecasting of Energy Production and Consumption Using Extreme Learning Machine: A Comprehensive MIMO based ELM Approach](https://arxiv.org/abs/2508.12764)
*Cyril Voyant,Milan Despotovic,Luis Garcia-Gutierrez,Mohammed Asloune,Yves-Marie Saint-Drenan,Jean-Laurent Duchaud,hjuvan Antone Faggianelli,Elena Magliaro*

Main category: cs.LG

TL;DR: ELM-based MIMO architecture for short-term energy forecasting outperforms persistence models, achieves high accuracy up to 5 hours ahead with nRMSE as low as 5.1%, and offers computational efficiency for real-time applications.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient short-term energy forecasting method that can handle multiple energy sources, address non-stationarity and seasonal variability, and provide real-time predictions adaptable to local grid constraints.

Method: Extreme Learning Machine (ELM) with Multi-Input Multi-Output (MIMO) architecture, using sliding window techniques and cyclic time encoding to handle non-stationarity and seasonal patterns from 6 years of hourly multi-source energy data.

Result: Significantly outperforms persistence forecasting with nRMSE of 17.9% for solar and 5.1% for thermal energy, R² > 0.98 at 1-hour horizon. Maintains high accuracy up to 5 hours ahead. MIMO provides marginal gains over SISO with lower computational demands than LSTM.

Conclusion: ELM-based MIMO approach provides accurate, computationally efficient short-term energy forecasting suitable for real-time applications and adaptable to various local constraints and energy market structures.

Abstract: A novel methodology for short-term energy forecasting using an Extreme
Learning Machine ($\mathtt{ELM}$) is proposed. Using six years of hourly data
collected in Corsica (France) from multiple energy sources (solar, wind, hydro,
thermal, bioenergy, and imported electricity), our approach predicts both
individual energy outputs and total production (\cyr{including imports, which
closely follow energy demand, modulo losses)} through a Multi-Input
Multi-Output ($\mathtt{MIMO}$) architecture. To address non-stationarity and
seasonal variability, sliding window techniques and cyclic time encoding are
incorporated, enabling dynamic adaptation to fluctuations. The $\mathtt{ELM}$
model significantly outperforms persistence-based forecasting, particularly for
solar and thermal energy, achieving an $\mathtt{nRMSE}$ of $17.9\%$ and
$5.1\%$, respectively, with $\mathtt{R^2} > 0.98$ (1-hour horizon). The model
maintains high accuracy up to five hours ahead, beyond which renewable energy
sources become increasingly volatile. While $\mathtt{MIMO}$ provides marginal
gains over Single-Input Single-Output ($\mathtt{SISO}$) architectures and
offers key advantages over deep learning methods such as $\mathtt{LSTM}$, it
provides a closed-form solution with lower computational demands, making it
well-suited for real-time applications, including online learning. Beyond
predictive accuracy, the proposed methodology is adaptable to various contexts
and datasets, as it can be tuned to local constraints such as resource
availability, grid characteristics, and market structures.

</details>


### [372] [Online Ensemble Transformer for Accurate Cloud Workload Forecasting in Predictive Auto-Scaling](https://arxiv.org/abs/2508.12773)
*Jiadong Chen,Xiao He,Hengyu Ye,Fuxin Jiang,Tieying Zhang,Jianjun Chen,Xiaofeng Gao*

Main category: cs.LG

TL;DR: E3Former is an online ensemble model for workload forecasting in serverless systems that reduces forecast error by 10% and enables 40% resource utilization reduction while supporting over 600,000 CPU cores.


<details>
  <summary>Details</summary>
Motivation: Serverless systems require predictive auto-scaling for optimal resource allocation in volatile environments, but existing forecasting models struggle with rapid adaptation to online workload dynamics and capturing complex periodicity in high-frequency tasks.

Method: Proposes E3Former, a novel online ensemble model that synergizes multiple subnetworks to overcome single-model limitations, achieving superior accuracy and robustness with minimal computational overhead increase.

Result: Reduces forecast error by average 10% in online forecasting tasks, deployed in ByteDance's IHPA platform supporting 30+ applications and 600,000+ CPU cores, achieving over 40% resource utilization reduction while maintaining service quality.

Conclusion: E3Former demonstrates effective online workload forecasting for large-scale predictive auto-scaling, providing both accuracy improvements and significant resource savings in real-world serverless deployments.

Abstract: In the swiftly evolving domain of cloud computing, the advent of serverless
systems underscores the crucial need for predictive auto-scaling systems. This
necessity arises to ensure optimal resource allocation and maintain operational
efficiency in inherently volatile environments. At the core of a predictive
auto-scaling system is the workload forecasting model. Existing forecasting
models struggle to quickly adapt to the dynamics in online workload streams and
have difficulty capturing the complex periodicity brought by fine-grained,
high-frequency forecasting tasks. Addressing this, we propose a novel online
ensemble model, E3Former, for online workload forecasting in large-scale
predictive auto-scaling. Our model synergizes the predictive capabilities of
multiple subnetworks to surmount the limitations of single-model approaches,
thus ensuring superior accuracy and robustness. Remarkably, it accomplishes
this with a minimal increase in computational overhead, adhering to the lean
operational ethos of serverless systems. Through extensive experimentation on
real-world workload datasets, we establish the efficacy of our ensemble model.
In online forecasting tasks, the proposed method reduces forecast error by an
average of 10%, and its effectiveness is further demonstrated through a
predictive auto-scaling test in the real-life online system. Currently, our
method has been deployed within ByteDance's Intelligent Horizontal Pod
Auto-scaling (IHPA) platform, which supports the stable operation of over 30
applications, such as Douyin E-Comerce, TouTiao, and Volcano Engine. The
predictive auto-scaling capacity reaching over 600,000 CPU cores. On the basis
of essentially ensuring service quality, the predictive auto-scaling system can
reduce resource utilization by over 40%.

</details>


### [373] [Randomized PCA Forest for Outlier Detection](https://arxiv.org/abs/2508.12776)
*Muhammad Rajabinasab,Farhad Pakdaman,Moncef Gabbouj,Peter Schneider-Kamp,Arthur Zimek*

Main category: cs.LG

TL;DR: A novel unsupervised outlier detection method using Randomized PCA Forest that outperforms classical and state-of-the-art methods on multiple datasets while maintaining computational efficiency.


<details>
  <summary>Details</summary>
Motivation: Inspired by the success of Randomized PCA Forest in approximate K-Nearest Neighbor search, the authors aim to develop an efficient and effective unsupervised outlier detection method that can generalize well across different datasets.

Method: The method utilizes Randomized Principal Component Analysis (PCA) Forest for outlier detection, building on the RPCA Forest approach that was previously successful in approximate KNN search tasks.

Result: Experimental results demonstrate superiority over classical and state-of-the-art methods on several datasets, with competitive performance on others. The method shows high generalization power and computational efficiency.

Conclusion: The proposed Randomized PCA Forest-based approach is presented as an excellent choice for unsupervised outlier detection due to its strong performance, generalization capabilities, and computational efficiency.

Abstract: We propose a novel unsupervised outlier detection method based on Randomized
Principal Component Analysis (PCA). Inspired by the performance of Randomized
PCA (RPCA) Forest in approximate K-Nearest Neighbor (KNN) search, we develop a
novel unsupervised outlier detection method that utilizes RPCA Forest for
outlier detection. Experimental results showcase the superiority of the
proposed approach compared to the classical and state-of-the-art methods in
performing the outlier detection task on several datasets while performing
competitively on the rest. The extensive analysis of the proposed method
reflects it high generalization power and its computational efficiency,
highlighting it as a good choice for unsupervised outlier detection.

</details>


### [374] [Wavy Transformer](https://arxiv.org/abs/2508.12787)
*Satoshi Noguchi,Yoshinobu Kawahara*

Main category: cs.LG

TL;DR: Wavy Transformer addresses over-smoothing in deep transformers by modeling attention layers as graph neural diffusion and introducing second-order wavy dynamics to prevent token representation convergence.


<details>
  <summary>Details</summary>
Motivation: Deep transformer models suffer from over-smoothing where token representations become similar across layers, limiting model performance. The paper establishes an equivalence between attention layers and graph neural diffusion, revealing over-smoothing as a consequence of dissipative diffusion dynamics.

Method: Proposes Wavy Transformer with novel attention layer based on second-order wavy dynamics. Also introduces specialized feed-forward network and normalization layer to preserve physical state-velocity relationship under chain rule, extending transformer architecture.

Result: Validated on various transformer models for NLP and CV tasks. Consistently improves performance with minimal additional parameters and no extra hyperparameter tuning required.

Conclusion: The physical interpretation of transformers as diffusion processes enables effective solutions to over-smoothing. Wavy Transformer provides a principled approach to enhance transformer performance without significant computational overhead.

Abstract: Transformers have achieved remarkable success across natural language
processing (NLP) and computer vision (CV). However, deep transformer models
often suffer from an over-smoothing issue, in which token representations
converge to similar values as they pass through successive transformer blocks.
In this paper, we establish an equivalence between the hidden-state dynamics
induced by stacked attention layers and graph neural diffusion on a complete
graph. From this perspective, over-smoothing can be interpreted as a
consequence of the dissipative nature of the underlying diffusion dynamics.
Motivated by this physical interpretation, we propose Wavy Transformer, which
consists of a novel attention layer based on second-order wavy dynamics. We
also introduce a feed-forward network and a normalization layer designed to
preserve the physical state-velocity relationship under the chain rule, thereby
extending the transformer architecture. We further validate our proposed
techniques on various transformer models for NLP and CV tasks. The results
consistently demonstrate that Wavy Transformer improves performance with
minimal additional parameters and no extra hyperparameter tuning.

</details>


### [375] [Bridging Human and LLM Judgments: Understanding and Narrowing the Gap](https://arxiv.org/abs/2508.12792)
*Felipe Maia Polo,Xinhe Wang,Mikhail Yurochkin,Gongjun Xu,Moulinath Banerjee,Yuekai Sun*

Main category: cs.LG

TL;DR: Bridge is a statistical framework that bridges human and LLM evaluations by modeling systematic discrepancies through linear transformations of covariates, improving LLM-as-a-judge accuracy.


<details>
  <summary>Details</summary>
Motivation: Large language models are increasingly used as judges to evaluate model outputs, but their assessments often diverge systematically from human judgments, creating a need for better alignment.

Method: Bridge posits latent human preference scores and models LLM deviations as linear transformations of covariates that capture sources of discrepancies, with an efficient fitting algorithm for statistical inference.

Result: Using six LLM judges and two benchmarks (BigGen Bench and Chatbot Arena), Bridge achieves higher agreement with human ratings in accuracy, calibration, and KL divergence, while exposing systematic human-LLM gaps.

Conclusion: Bridge provides a simple and principled framework for refining LLM ratings and characterizing systematic discrepancies between human and LLM evaluations, improving the reliability of LLM-as-a-judge systems.

Abstract: Large language models are increasingly used as judges (LLM-as-a-judge) to
evaluate model outputs at scale, but their assessments often diverge
systematically from human judgments. We present Bridge, a unified statistical
framework that explicitly bridges human and LLM evaluations under both absolute
scoring and pairwise comparison paradigms. Bridge posits a latent human
preference score for each prompt-response pair and models LLM deviations as
linear transformations of covariates that capture sources of discrepancies.
This offers a simple and principled framework for refining LLM ratings and
characterizing systematic discrepancies between humans and LLMs. We provide an
efficient fitting algorithm with asymptotic guarantees for statistical
inference. Using six LLM judges and two benchmarks (BigGen Bench and Chatbot
Arena), Bridge achieves higher agreement with human ratings (accuracy,
calibration, and KL divergence) and exposes systematic human-LLM gaps.

</details>


### [376] [A Shift in Perspective on Causality in Domain Generalization](https://arxiv.org/abs/2508.12798)
*Damian Machlanski,Stephanie Riley,Edward Moroshko,Kurt Butler,Panagiotis Dimitrakopoulos,Thomas Melistas,Akchunya Chanchal,Steven McDonagh,Ricardo Silva,Sotirios A. Tsaftaris*

Main category: cs.LG

TL;DR: The paper reconciles contradictions between causal modeling claims and domain generalization benchmarks, advocating for a more nuanced theory of causality's role in AI generalization.


<details>
  <summary>Details</summary>
Motivation: Recent domain generalization benchmarks have challenged the promise that causal modeling leads to robust AI generalization, creating apparent contradictions that need reconciliation.

Method: The authors revisit claims from both causality and domain generalization literature, analyzing and reconciling apparent contradictions through theoretical examination.

Result: The paper provides a more nuanced understanding of causality's role in generalization and offers an interactive demo to demonstrate their findings.

Conclusion: A more sophisticated theory is needed for understanding how causal modeling contributes to robust AI generalization, moving beyond oversimplified claims.

Abstract: The promise that causal modelling can lead to robust AI generalization has
been challenged in recent work on domain generalization (DG) benchmarks. We
revisit the claims of the causality and DG literature, reconciling apparent
contradictions and advocating for a more nuanced theory of the role of
causality in generalization. We also provide an interactive demo at
https://chai-uk.github.io/ukairs25-causal-predictors/.

</details>


### [377] [Maximum Score Routing For Mixture-of-Experts](https://arxiv.org/abs/2508.12801)
*Bowen Dong,Yilong Fan,Yutao Sun,Zhenyu Li,Tengyu Pan,Xun Zhou,Jianyong Wang*

Main category: cs.LG

TL;DR: MaxScore is a novel MoE routing method that uses minimum-cost maximum-flow optimization with SoftTopk operator to eliminate token dropping and improve hardware efficiency while maintaining load balancing.


<details>
  <summary>Details</summary>
Motivation: Traditional MoE networks suffer from token dropping when expert capacity is saturated and low hardware efficiency due to padding in underutilized experts, while removing capacity constraints compromises load balancing and computational efficiency.

Method: Proposes Maximum Score Routing (MaxScore) that models routing as a minimum-cost maximum-flow problem and integrates a SoftTopk operator to resolve limitations of iterative rerouting and optimal transport formulations.

Result: Achieves lower training losses and higher evaluation scores at equivalent FLOPs compared to both constrained and unconstrained baselines.

Conclusion: MaxScore provides an effective solution to the fundamental limitations of MoE routing, offering improved performance and efficiency without the drawbacks of traditional capacity-constrained approaches.

Abstract: Routing networks in sparsely activated mixture-of-experts (MoE) dynamically
allocate input tokens to top-k experts through differentiable sparse
transformations, enabling scalable model capacity while preserving
computational efficiency. Traditional MoE networks impose an expert capacity
constraint to ensure GPU-friendly computation. However, this leads to token
dropping when capacity is saturated and results in low hardware efficiency due
to padding in underutilized experts. Removing the capacity constraint, in turn,
compromises load balancing and computational efficiency. To address these
issues, we propose Maximum Score Routing ($\mathbf{MaxScore}$), a novel MoE
routing paradigm that models routing as a minimum-cost maximum-flow problem and
integrates a SoftTopk operator. MaxScore resolves the fundamental limitations
of iterative rerouting and optimal transport formulations, achieving lower
training losses and higher evaluation scores at equivalent FLOPs compared to
both constrained and unconstrained baselines. Implementation details and
experimental configurations can be obtained from
$\href{https://github.com/dongbw18/MaxScore.git}{MaxScore}$.

</details>


### [378] [Learning to Steer: Input-dependent Steering for Multimodal LLMs](https://arxiv.org/abs/2508.12815)
*Jayneel Parekh,Pegah Khayatan,Mustafa Shukor,Arnaud Dapogny,Alasdair Newson,Matthieu Cord*

Main category: cs.LG

TL;DR: L2S (Learn-to-Steer) introduces input-specific steering for multimodal LLMs using contrastive prompting and an auxiliary module to predict steering vectors, reducing hallucinations and improving safety.


<details>
  <summary>Details</summary>
Motivation: Existing steering techniques like mean steering use a single static vector that doesn't account for input-dependent behaviors, limiting effectiveness for context-sensitive tasks like safety enforcement.

Method: Proposes fine-grained steering with input-specific linear shifts computed via contrastive prompting, and trains a small auxiliary module to predict these steering vectors at test time.

Result: L2S outperforms static baselines by reducing hallucinations and enforcing safety in multimodal LLMs through context-aware steering.

Conclusion: Input-specific steering via learned auxiliary modules provides more effective and context-aware control over MLLM behavior compared to static steering approaches.

Abstract: Steering has emerged as a practical approach to enable post-hoc guidance of
LLMs towards enforcing a specific behavior. However, it remains largely
underexplored for multimodal LLMs (MLLMs); furthermore, existing steering
techniques, such as mean steering, rely on a single steering vector, applied
independently of the input query. This paradigm faces limitations when the
desired behavior is dependent on the example at hand. For example, a safe
answer may consist in abstaining from answering when asked for an illegal
activity, or may point to external resources or consultation with an expert
when asked about medical advice. In this paper, we investigate a fine-grained
steering that uses an input-specific linear shift. This shift is computed using
contrastive input-specific prompting. However, the input-specific prompts
required for this approach are not known at test time. Therefore, we propose to
train a small auxiliary module to predict the input-specific steering vector.
Our approach, dubbed as L2S (Learn-to-Steer), demonstrates that it reduces
hallucinations and enforces safety in MLLMs, outperforming other static
baselines.

</details>


### [379] [Toward Storage-Aware Learning with Compressed Data An Empirical Exploratory Study on JPEG](https://arxiv.org/abs/2508.12833)
*Kichang Lee,Songkuk Kim,JaeYeon Park,JeongGil Ko*

Main category: cs.LG

TL;DR: Empirical study shows naive compression strategies are suboptimal for on-device ML storage constraints, revealing sample-dependent compression sensitivity enables adaptive strategies.


<details>
  <summary>Details</summary>
Motivation: On-device machine learning faces storage limitations, especially in continuous data collection scenarios, requiring efficient compression strategies.

Method: Empirical study analyzing trade-offs between data quantity and quality through compression, comparing naive strategies vs. sample-wise adaptive approaches.

Result: Demonstrated that uniform data dropping and one-size-fits-all compression are suboptimal, and revealed varying compression sensitivity across data samples.

Conclusion: Findings provide foundation for developing storage-aware learning systems with sample-adaptive compression strategies, advancing understanding of this under-explored challenge.

Abstract: On-device machine learning is often constrained by limited storage,
particularly in continuous data collection scenarios. This paper presents an
empirical study on storage-aware learning, focusing on the trade-off between
data quantity and quality via compression. We demonstrate that naive
strategies, such as uniform data dropping or one-size-fits-all compression, are
suboptimal. Our findings further reveal that data samples exhibit varying
sensitivities to compression, supporting the feasibility of a sample-wise
adaptive compression strategy. These insights provide a foundation for
developing a new class of storage-aware learning systems. The primary
contribution of this work is the systematic characterization of this
under-explored challenge, offering valuable insights that advance the
understanding of storage-aware learning.

</details>


### [380] [Learning In-context $\pmb{n}$-grams with Transformers: Sub-$\pmb{n}$-grams Are Near-stationary Points](https://arxiv.org/abs/2508.12837)
*Aditya Varre,Gizem Yüce,Nicolas Flammarion*

Main category: cs.LG

TL;DR: Transformers learning n-gram language models exhibit stage-wise progression with sub-n-grams as near-stationary points in the loss landscape, explaining observed plateaus and discrete transitions during training.


<details>
  <summary>Details</summary>
Motivation: Empirical observations show prolonged plateaus and stage-wise progression during transformer training, motivating investigation of the loss landscape for in-context next-token prediction tasks.

Method: Analyze learning of in-context n-gram language models under cross-entropy loss, establish sufficient conditions for stationary points, and construct parameter configurations representing k-gram estimators for simplified transformer models.

Result: Sub-n-grams are near-stationary points of population cross-entropy loss in the limit of infinite sequence length and parameter norm, revealing discrete transitions between near-stationary solutions.

Conclusion: Theoretical analysis provides insight into stage-wise learning dynamics and emergent phase transitions, supported by numerical experiments showing characteristic learning dynamics of n-grams.

Abstract: Motivated by empirical observations of prolonged plateaus and stage-wise
progression during training, we investigate the loss landscape of transformer
models trained on in-context next-token prediction tasks. In particular, we
focus on learning in-context $n$-gram language models under cross-entropy loss,
and establish a sufficient condition for parameter configurations to be
stationary points. We then construct a set of parameter configurations for a
simplified transformer model that represent $k$-gram estimators (for $k \leq
n$), and show that the gradient of the population loss at these solutions
vanishes in the limit of infinite sequence length and parameter norm. This
reveals a key property of the loss landscape: {sub-$n$-grams are
near-stationary points of the population cross-entropy loss}, offering
theoretical insight into widely observed phenomena such as stage-wise learning
dynamics and emergent phase transitions. These insights are further supported
by numerical experiments that illustrate the learning dynamics of $n$-grams,
characterized by discrete transitions between near-stationary solutions.

</details>


### [381] [HRS: Hybrid Representation Framework with Scheduling Awareness for Time Series Forecasting in Crowdsourced Cloud-Edge Platforms](https://arxiv.org/abs/2508.12839)
*Tiancheng Zhang,Cheng Zhang,Shuren Liu,Xiaofei Wang,Shaoyuan Huang,Wenyu Wang*

Main category: cs.LG

TL;DR: HRS framework combines numerical and image representations with scheduling-aware loss to improve load forecasting accuracy during traffic surges, reducing SLA violations by 63.1% and profit loss by 32.3%.


<details>
  <summary>Details</summary>
Motivation: Existing load forecasting methods for streaming services either cause underprovisioning (SLA violations) or conservative overprovisioning (increased costs), creating a dilemma for maintaining QoS in Crowdsourced Cloud-Edge Platforms.

Method: Proposed HRS - a hybrid representation framework that integrates numerical and image-based representations to capture extreme load dynamics, plus a Scheduling-Aware Loss (SAL) that accounts for asymmetric impact of prediction errors.

Result: Extensive experiments on four real-world datasets show HRS outperforms ten baselines, achieving state-of-the-art performance with 63.1% reduction in SLA violation rates and 32.3% reduction in total profit loss.

Conclusion: HRS effectively addresses the forecasting dilemma in CCPs by providing more accurate predictions that better support scheduling decisions, significantly improving both QoS and profitability.

Abstract: With the rapid proliferation of streaming services, network load exhibits
highly time-varying and bursty behavior, posing serious challenges for
maintaining Quality of Service (QoS) in Crowdsourced Cloud-Edge Platforms
(CCPs). While CCPs leverage Predict-then-Schedule architecture to improve QoS
and profitability, accurate load forecasting remains challenging under traffic
surges. Existing methods either minimize mean absolute error, resulting in
underprovisioning and potential Service Level Agreement (SLA) violations during
peak periods, or adopt conservative overprovisioning strategies, which mitigate
SLA risks at the expense of increased resource expenditure. To address this
dilemma, we propose HRS, a hybrid representation framework with scheduling
awareness that integrates numerical and image-based representations to better
capture extreme load dynamics. We further introduce a Scheduling-Aware Loss
(SAL) that captures the asymmetric impact of prediction errors, guiding
predictions that better support scheduling decisions. Extensive experiments on
four real-world datasets demonstrate that HRS consistently outperforms ten
baselines and achieves state-of-the-art performance, reducing SLA violation
rates by 63.1% and total profit loss by 32.3%.

</details>


### [382] [One-Class Intrusion Detection with Dynamic Graphs](https://arxiv.org/abs/2508.12885)
*Aleksei Liuliakov,Alexander Schulz,Luca Hermes,Barbara Hammer*

Main category: cs.LG

TL;DR: TGN-SVDD: A novel intrusion detection method combining dynamic graph modeling and deep anomaly detection that outperforms baselines on realistic network data.


<details>
  <summary>Details</summary>
Motivation: Growing digitalization increases network security importance. Machine learning-based intrusion detection faces challenges including detecting novel/unseen network events, handling temporal events, and capturing inherent graph structure of network communications.

Method: Proposes TGN-SVDD method that builds upon modern dynamic graph modelling and deep anomaly detection techniques to address the specific challenges of network intrusion detection.

Result: Demonstrates superiority over several baselines for realistic intrusion detection data.

Conclusion: Suggests a more challenging variant of intrusion detection data and presents TGN-SVDD as an effective solution that outperforms existing approaches.

Abstract: With the growing digitalization all over the globe, the relevance of network
security becomes increasingly important. Machine learning-based intrusion
detection constitutes a promising approach for improving security, but it bears
several challenges. These include the requirement to detect novel and unseen
network events, as well as specific data properties, such as events over time
together with the inherent graph structure of network communication. In this
work, we propose a novel intrusion detection method, TGN-SVDD, which builds
upon modern dynamic graph modelling and deep anomaly detection. We demonstrate
its superiority over several baselines for realistic intrusion detection data
and suggest a more challenging variant of the latter.

</details>


### [383] [TCUQ: Single-Pass Uncertainty Quantification from Temporal Consistency with Streaming Conformal Calibration for TinyML](https://arxiv.org/abs/2508.12905)
*Ismail Lamaakal,Chaymae Yahyati,Khalid El Makkaoui,Ibrahim Ouahbi,Yassine Maleh*

Main category: cs.LG

TL;DR: TCUQ is a lightweight uncertainty monitor for TinyML that uses temporal consistency and streaming conformal calibration to provide risk scores with minimal resource usage, achieving significant footprint and latency reductions compared to alternatives.


<details>
  <summary>Details</summary>
Motivation: To enable reliable uncertainty monitoring on resource-constrained TinyML devices without requiring online labels or extra computational overhead, addressing the limitations of existing methods like early exit and deep ensembles.

Method: Single-pass, label-free approach that converts short-horizon temporal consistency from lightweight signals on posteriors and features into calibrated risk scores using O(W) ring buffer and O(1) per-step updates, with streaming conformal calibration for budgeted accept/abstain decisions.

Result: 50-60% smaller footprint and 30-45% faster than early exit/ensembles; 3-7 AUPRC improvement in accuracy drop detection (up to 0.86 AUPRC); up to 0.92 AUROC for failure detection; fits on kilobyte-scale microcontrollers.

Conclusion: Temporal consistency coupled with streaming conformal calibration provides a practical, resource-efficient foundation for on-device uncertainty monitoring in TinyML applications.

Abstract: We introduce TCUQ, a single pass, label free uncertainty monitor for
streaming TinyML that converts short horizon temporal consistency captured via
lightweight signals on posteriors and features into a calibrated risk score
with an O(W ) ring buffer and O(1) per step updates. A streaming conformal
layer turns this score into a budgeted accept/abstain rule, yielding calibrated
behavior without online labels or extra forward passes. On microcontrollers,
TCUQ fits comfortably on kilobyte scale devices and reduces footprint and
latency versus early exit and deep ensembles (typically about 50 to 60% smaller
and about 30 to 45% faster), while methods of similar accuracy often run out of
memory. Under corrupted in distribution streams, TCUQ improves accuracy drop
detection by 3 to 7 AUPRC points and reaches up to 0.86 AUPRC at high
severities; for failure detection it attains up to 0.92 AUROC. These results
show that temporal consistency, coupled with streaming conformal calibration,
provides a practical and resource efficient foundation for on device monitoring
in TinyML.

</details>


### [384] [SparseMap: A Sparse Tensor Accelerator Framework Based on Evolution Strategy](https://arxiv.org/abs/2508.12906)
*Boran Zhao,Haiming Zhai,Zihang Yuan,Hetian Liu,Tian Xia,Wenzhe Zhao,Pengju Ren*

Main category: cs.LG

TL;DR: SparseMap is an evolution strategy-based framework that jointly optimizes mapping and sparse strategies for tensor accelerators, overcoming combinatorial explosion in design space to find superior solutions.


<details>
  <summary>Details</summary>
Motivation: Existing sparse tensor accelerator designs are limited to specific scenarios and lack comprehensive optimization of both mapping and sparse strategies, leading to suboptimal performance when scenarios change.

Method: Proposes SparseMap framework using enhanced genetic encoding and evolutionary operators to efficiently explore the vast design space (up to O(10^41) that combines both mapping (tiling communication/computation) and sparse strategies (zero-element bypassing).

Result: SparseMap consistently finds superior solutions compared to prior works and classical optimization methods like particle swarm optimization, reinforcement learning, and Monte Carlo tree search.

Conclusion: The unified framework successfully addresses the combinatorial explosion challenge in sparse tensor accelerator design space, providing an efficient automated optimization approach that outperforms conventional methods.

Abstract: The growing demand for sparse tensor algebra (SpTA) in machine learning and
big data has driven the development of various sparse tensor accelerators.
However, most existing manually designed accelerators are limited to specific
scenarios, and it's time-consuming and challenging to adjust a large number of
design factors when scenarios change. Therefore, automating the design of SpTA
accelerators is crucial. Nevertheless, previous works focus solely on either
mapping (i.e., tiling communication and computation in space and time) or
sparse strategy (i.e., bypassing zero elements for efficiency), leading to
suboptimal designs due to the lack of comprehensive consideration of both. A
unified framework that jointly optimizes both is urgently needed. However,
integrating mapping and sparse strategies leads to a combinatorial explosion in
the design space(e.g., as large as $O(10^{41})$ for the workload $P_{32 \times
64} \times Q_{64 \times 48} = Z_{32 \times 48}$). This vast search space
renders most conventional optimization methods (e.g., particle swarm
optimization, reinforcement learning and Monte Carlo tree search) inefficient.
To address this challenge, we propose an evolution strategy-based sparse tensor
accelerator optimization framework, called SparseMap. SparseMap constructing a
more comprehensive design space with the consideration of both mapping and
sparse strategy. We introduce a series of enhancements to genetic encoding and
evolutionary operators, enabling SparseMap to efficiently explore the vast and
diverse design space. We quantitatively compare SparseMap with prior works and
classical optimization methods, demonstrating that SparseMap consistently finds
superior solutions.

</details>


### [385] [SNAP-UQ: Self-supervised Next-Activation Prediction for Single-Pass Uncertainty in TinyML](https://arxiv.org/abs/2508.12907)
*Ismail Lamaakal,Chaymae Yahyati,Khalid El Makkaoui,Ibrahim Ouahbi,Yassine Maleh*

Main category: cs.LG

TL;DR: SNAP-UQ is a single-pass, label-free uncertainty estimation method for TinyML that predicts next layer activations to estimate risk, requiring minimal resources while maintaining strong performance.


<details>
  <summary>Details</summary>
Motivation: Existing uncertainty quantification methods for TinyML are resource-intensive, requiring temporal buffers, auxiliary exits, or repeated forward passes, which are impractical for memory-constrained microcontrollers.

Method: Uses tiny int8 heads to forecast statistics of the next layer from compressed previous layer views, with a lightweight monotone mapper converting surprisal into actionable uncertainty scores without buffers or multiple passes.

Result: Reduces flash and latency by 40-60% and 25-35% respectively compared to early-exit and deep ensembles, improves accuracy-drop detection by several AUPRC points, and maintains AUROC ≈0.9 for failure detection in single pass.

Conclusion: Grounding uncertainty in layer-to-layer dynamics provides a practical, resource-efficient solution for on-device monitoring in TinyML applications with minimal memory footprint.

Abstract: We introduce \textbf{SNAP-UQ}, a single-pass, label-free uncertainty method
for TinyML that estimates risk from \emph{depth-wise next-activation
prediction}: tiny int8 heads forecast the statistics of the next layer from a
compressed view of the previous one, and a lightweight monotone mapper turns
the resulting surprisal into an actionable score. The design requires no
temporal buffers, auxiliary exits, or repeated forward passes, and adds only a
few tens of kilobytes to MCU deployments. Across vision and audio backbones,
SNAP-UQ consistently reduces flash and latency relative to early-exit and deep
ensembles (typically $\sim$40--60\% smaller and $\sim$25--35\% faster), with
competing methods of similar accuracy often exceeding memory limits. In
corrupted streams it improves accuracy-drop detection by several AUPRC points
and maintains strong failure detection (AUROC $\approx$0.9) in a single pass.
Grounding uncertainty in layer-to-layer dynamics yields a practical,
resource-efficient basis for on-device monitoring in TinyML.

</details>


### [386] [Fed-DPRoC:Communication-Efficient Differentially Private and Robust Federated Learning](https://arxiv.org/abs/2508.12978)
*Yue Xia,Tayyebeh Jahani-Nezhad,Rawad Bitar*

Main category: cs.LG

TL;DR: Fed-DPRoC is a federated learning framework that combines differential privacy, Byzantine robustness, and communication efficiency through robust-compatible compression.


<details>
  <summary>Details</summary>
Motivation: Existing federated learning approaches struggle to simultaneously address privacy protection (DP), robustness against Byzantine attacks, and communication efficiency, creating a need for an integrated solution.

Method: Proposes RobAJoL which combines Johnson-Lindenstrauss transform for compression with robust averaging for aggregation, theoretically proving their compatibility for maintaining robustness while compressing DP-protected updates.

Result: Theoretical proofs show RobAJoL preserves robustness guarantees, ensures differential privacy, and reduces communication costs. Experiments on CIFAR-10 and Fashion MNIST demonstrate superior robustness and utility under various Byzantine attacks compared to existing methods.

Conclusion: Fed-DPRoC successfully integrates DP, Byzantine robustness, and communication efficiency through the novel concept of robust-compatible compression, with RobAJoL providing a practical instantiation that outperforms current approaches.

Abstract: We propose Fed-DPRoC, a novel federated learning framework that
simultaneously ensures differential privacy (DP), Byzantine robustness, and
communication efficiency. We introduce the concept of robust-compatible
compression, which enables users to compress DP-protected updates while
maintaining the robustness of the aggregation rule. We instantiate our
framework as RobAJoL, combining the Johnson-Lindenstrauss (JL) transform for
compression with robust averaging for robust aggregation. We theoretically
prove the compatibility of JL transform with robust averaging and show that
RobAJoL preserves robustness guarantees, ensures DP, and reduces communication
cost. Experiments on CIFAR-10 and Fashion MNIST validate our theoretical claims
and demonstrate that RobAJoL outperforms existing methods in terms of
robustness and utility under different Byzantine attacks.

</details>


### [387] [SL-ACC: A Communication-Efficient Split Learning Framework with Adaptive Channel-wise Compression](https://arxiv.org/abs/2508.12984)
*Zehang Lin,Zheng Lin,Miao Yang,Jianhao Huang,Yuxin Zhang,Zihan Fang,Xia Du,Zhe Chen,Shunzhi Zhu,Wei Ni*

Main category: cs.LG

TL;DR: SL-ACC is a communication-efficient split learning framework that reduces transmission bottlenecks by adaptively compressing smashed data using entropy-based channel importance identification and group-wise compression.


<details>
  <summary>Details</summary>
Motivation: The increasing complexity of neural networks creates deployment challenges for distributed ML on resource-constrained devices. Split learning helps but suffers from transmission bottlenecks as smashed data grows with more participating devices.

Method: Proposes SL-ACC framework with two components: Adaptive Channel Importance Identification (ACII) using Shannon entropy to identify channel contributions, and Channel Grouping Compression (CGC) that groups channels by entropy and performs group-wise adaptive compression.

Result: Extensive experiments across various datasets show SL-ACC takes considerably less time to achieve target accuracy compared to state-of-the-art benchmarks.

Conclusion: SL-ACC effectively addresses communication bottlenecks in split learning through adaptive entropy-based compression, enabling faster training without compromising accuracy.

Abstract: The increasing complexity of neural networks poses a significant barrier to
the deployment of distributed machine learning (ML) on resource-constrained
devices, such as federated learning (FL). Split learning (SL) offers a
promising solution by offloading the primary computing load from edge devices
to a server via model partitioning. However, as the number of participating
devices increases, the transmission of excessive smashed data (i.e.,
activations and gradients) becomes a major bottleneck for SL, slowing down the
model training. To tackle this challenge, we propose a communication-efficient
SL framework, named SL-ACC, which comprises two key components: adaptive
channel importance identification (ACII) and channel grouping compression
(CGC). ACII first identifies the contribution of each channel in the smashed
data to model training using Shannon entropy. Following this, CGC groups the
channels based on their entropy and performs group-wise adaptive compression to
shrink the transmission volume without compromising training accuracy.
Extensive experiments across various datasets validate that our proposed SL-ACC
framework takes considerably less time to achieve a target accuracy than
state-of-the-art benchmarks.

</details>


### [388] [Predicting the Performance of Graph Convolutional Networks with Spectral Properties of the Graph Laplacian](https://arxiv.org/abs/2508.12993)
*Shalima Binta Manir,Tim Oates*

Main category: cs.LG

TL;DR: The Fiedler value (algebraic connectivity) predicts GCN performance - graphs with similar Fiedler values have analogous structural properties enabling better transfer learning and hyperparameter sharing.


<details>
  <summary>Details</summary>
Motivation: Stacking GCN layers inconsistently improves performance on tasks like node classification, suggesting the need for a reliable predictor of GCN effectiveness across different graph structures.

Method: Theoretical analysis and empirical experiments on synthetic and real graph datasets (Cora, CiteSeer, Polblogs) using the Fiedler value as a predictor, exploring multiple aggregation methods for connected components.

Result: The Fiedler value serves as a good predictor of GCN performance, with graphs sharing similar algebraic connectivity showing comparable structural properties and performance patterns.

Conclusion: Algebraic connectivity (Fiedler value) provides a reliable metric for predicting GCN performance and facilitating transfer learning between graphs with similar structural properties.

Abstract: A common observation in the Graph Convolutional Network (GCN) literature is
that stacking GCN layers may or may not result in better performance on tasks
like node classification and edge prediction. We have found empirically that a
graph's algebraic connectivity, which is known as the Fiedler value, is a good
predictor of GCN performance. Intuitively, graphs with similar Fiedler values
have analogous structural properties, suggesting that the same filters and
hyperparameters may yield similar results when used with GCNs, and that
transfer learning may be more effective between graphs with similar algebraic
connectivity. We explore this theoretically and empirically with experiments on
synthetic and real graph data, including the Cora, CiteSeer and Polblogs
datasets. We explore multiple ways of aggregating the Fiedler value for
connected components in the graphs to arrive at a value for the entire graph,
and show that it can be used to predict GCN performance. We also present
theoretical arguments as to why the Fiedler value is a good predictor.

</details>


### [389] [Kourkoutas-Beta: A Sunspike-Driven Adam Optimizer with Desert Flair](https://arxiv.org/abs/2508.12996)
*Stavros C. Kassinos*

Main category: cs.LG

TL;DR: Kourkoutas-Beta is a novel Adam-style optimizer that dynamically adjusts the second-moment discount factor beta2 based on gradient spike detection, improving stability and performance for physics-based problems with erratic gradients.


<details>
  <summary>Details</summary>
Motivation: Transformer neural networks for physics problems suffer from erratic losses and spiky gradients in PDE surrogates and PINNs, where stiff composite losses amplify gradient instability issues.

Method: Replaces fixed beta2 with layer-wise dynamic values driven by a bounded 'sunspike' ratio (current pooled gradient norm divided by EMA of past norms). Spikes lower beta2 toward beta2_min, calm phases keep it near beta2_max, with options for leaky-AMSGrad, trust-region clipping, and bias-correction modes.

Result: Improves stability and final loss versus fixed-beta2 Adam across four test settings. On small-enwik8, lowers bits-per-character by 38% vs Adam-0.95 and 58% vs Adam-0.999 with smaller variance. Runtime overhead comparable to Adam.

Conclusion: Kourkoutas-Beta provides drop-in replacement for Adam with improved robustness under spiky gradients while preserving Adam-style convergence guarantees, making it particularly effective for physics-based neural network applications.

Abstract: Transformer neural networks are increasingly used for physics-based problems.
In data-driven PDE surrogates, training samples from varying boundary and
initial conditions can cause erratic losses and spiky gradients; in
physics-informed neural networks (PINNs), stiff composite losses amplify this
effect.
  We introduce Kourkoutas-Beta, an Adam-style optimizer where the fixed
second-moment discount beta2 is replaced by a layer-wise dynamic value driven
by a bounded ``sunspike'' ratio: the current pooled gradient norm divided by an
exponential moving average (EMA) of past norms, squashed to the interval [0,1).
Spikes lower beta2 toward beta2_min; calm phases keep it near beta2_max.
Options include leaky-AMSGrad (decay), trust-region clipping (max_ratio),
adaptive tiny terms, and several bias-correction modes ``none'', ``beta2max'',
``exact'). With all features off and bias_correction=``none'', the method is
exactly Adam.
  We test on four settings: (i) a Transformer PDE surrogate (Heat2D), (ii) a 3D
PINN for heat conduction (Heat3D), (iii) a lightweight MLX synthetic task with
jitter and rare-trigger bursts, and (iv) a character-level Transformer on 30 MB
of enwik8 (small-enwik8). Kourkoutas-Beta improves stability and final loss
versus fixed-beta2 Adam. On small-enwik8 it lowers bits-per-character by about
38% vs Adam-0.95 and about 58% vs Adam-0.999 over 10 seeds, with smaller
variance. The method remains drop-in, with runtime overhead comparable to Adam
in testbeds A-C and within single-digit percent in testbed D. It preserves
Adam-style convergence guarantees while improving robustness under spiky
gradients.

</details>


### [390] [Fairness-Aware Multi-view Evidential Learning with Adaptive Prior](https://arxiv.org/abs/2508.12997)
*Haishun Chen,Cai Xu,Jinlong Yu,Yilin Zhang,Ziyu Guan,Wei Zhao*

Main category: cs.LG

TL;DR: FAML addresses biased evidence learning in multi-view evidential learning by introducing adaptive priors, fairness constraints, and opinion alignment to achieve balanced evidence allocation and improved uncertainty estimation.


<details>
  <summary>Details</summary>
Motivation: Traditional multi-view evidential learning assumes reliable view-specific evidence learning, but empirical analysis reveals samples are assigned more evidence to data-rich classes, leading to unreliable uncertainty estimation.

Method: Proposes FAML with three components: 1) adaptive prior based on training trajectory for regularization, 2) fairness constraint based on class-wise evidence variance, 3) opinion alignment mechanism for multi-view fusion to mitigate view-specific bias.

Result: Extensive experiments on five real-world datasets show FAML achieves more balanced evidence allocation and improves both prediction performance and uncertainty estimation reliability compared to state-of-the-art methods.

Conclusion: FAML effectively addresses the biased evidential multi-view learning problem through adaptive regularization, fairness constraints, and opinion alignment, demonstrating superior performance in balanced evidence allocation and trustworthy uncertainty estimation.

Abstract: Multi-view evidential learning aims to integrate information from multiple
views to improve prediction performance and provide trustworthy uncertainty
esitimation. Most previous methods assume that view-specific evidence learning
is naturally reliable. However, in practice, the evidence learning process
tends to be biased. Through empirical analysis on real-world data, we reveal
that samples tend to be assigned more evidence to support data-rich classes,
thereby leading to unreliable uncertainty estimation in predictions. This
motivates us to delve into a new Biased Evidential Multi-view Learning (BEML)
problem. To this end, we propose Fairness-Aware Multi-view Evidential Learning
(FAML). FAML first introduces an adaptive prior based on training trajectory,
which acts as a regularization strategy to flexibly calibrate the biased
evidence learning process. Furthermore, we explicitly incorporate a fairness
constraint based on class-wise evidence variance to promote balanced evidence
allocation. In the multi-view fusion stage, we propose an opinion alignment
mechanism to mitigate view-specific bias across views, thereby encouraging the
integration of consistent and mutually supportive evidence. Extensive
experiments on five real-world multi-view datasets demonstrate that FAML
achieves more balanced evidence allocation and improves both prediction
performance and the reliability of uncertainty estimation compared to
state-of-the-art methods.

</details>


### [391] [Monte Carlo Functional Regularisation for Continual Learning](https://arxiv.org/abs/2508.13006)
*Pengcheng Hao,Menghao Waiyan William Zhu,Ercan Engin Kuruoglu*

Main category: cs.LG

TL;DR: MCFRCL is a new functional regularization continual learning method that uses Monte Carlo sampling and statistical distributions to improve accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Functional regularization CL methods outperform weight-space approaches but suffer from high computational costs and large linear approximation errors.

Method: Uses Monte Carlo sampling to approximate model prediction distributions, leverages three continuous distributions to capture statistical characteristics via moment-based methods, and employs both Wasserstein and KL distances for regularization.

Result: Evaluated on MNIST and CIFAR datasets, MCFRCL shows effectiveness in both prediction accuracy and training efficiency compared to benchmark methods.

Conclusion: The proposed MCFRCL framework provides an effective solution for continual learning with improved computational efficiency and reduced approximation errors.

Abstract: Continual learning (CL) is crucial for the adaptation of neural network
models to new environments. Although outperforming weight-space regularisation
approaches, the functional regularisation-based CL methods suffer from high
computational costs and large linear approximation errors. In this work, we
present a new functional regularisation CL framework, called MCFRCL, which
approximates model prediction distributions by Monte Carlo (MC) sampling.
Moreover, three continuous distributions are leveraged to capture the
statistical characteristics of the MC samples via moment-based methods.
Additionally, both the Wasserstein distance and the Kullback-Leibler (KL)
distance are employed to construct the regularisation function. The proposed
MCFRCL is evaluated against multiple benchmark methods on the MNIST and CIFAR
datasets, with simulation results highlighting its effectiveness in both
prediction accuracy and training efficiency.

</details>


### [392] [Design and Analysis of Robust Adaptive Filtering with the Hyperbolic Tangent Exponential Kernel M-Estimator Function for Active Noise Control](https://arxiv.org/abs/2508.13018)
*Iam Kim de S. Hermont,Andre R. Flores,Rodrigo C. de Lamare*

Main category: cs.LG

TL;DR: Proposed FXHEKM robust adaptive algorithm for active noise control with impulsive noise, showing superior performance against alpha-stable noises compared to competing methods.


<details>
  <summary>Details</summary>
Motivation: Need for robust noise cancellation in presence of impulsive noise environments where traditional methods fail, particularly addressing alpha-stable noise distributions.

Method: Developed filtered-x hyperbolic tangent exponential generalized Kernel M-estimate function (FXHEKM) algorithm with statistical analysis and computational cost study.

Result: Numerical results demonstrate efficient cancellation of additive spurious signals and alpha-stable noises, outperforming competing algorithms in MSE and ANR metrics.

Conclusion: FXHEKM algorithm provides effective robust adaptive filtering solution for active noise control applications dealing with impulsive noise scenarios.

Abstract: In this work, we propose a robust adaptive filtering approach for active
noise control applications in the presence of impulsive noise. In particular,
we develop the filtered-x hyperbolic tangent exponential generalized Kernel
M-estimate function (FXHEKM) robust adaptive algorithm. A statistical analysis
of the proposed FXHEKM algorithm is carried out along with a study of its
computational cost. {In order to evaluate the proposed FXHEKM algorithm, the
mean-square error (MSE) and the average noise reduction (ANR) performance
metrics have been adopted.} Numerical results show the efficiency of the
proposed FXHEKM algorithm to cancel the presence of the additive spurious
signals, such as \textbf{$\alpha$}-stable noises against competing algorithms.

</details>


### [393] [The Application of Transformer-Based Models for Predicting Consequences of Cyber Attacks](https://arxiv.org/abs/2508.13030)
*Bipin Chhetri,Akbar Siami Namin*

Main category: cs.LG

TL;DR: This paper presents an NLP-based approach using BERT and HANs for multi-label classification of cyberattack consequences from MITRE CWE descriptions, achieving superior performance over traditional deep learning models.


<details>
  <summary>Details</summary>
Motivation: The increasing complexity and cost of cyberattacks require automated methods to analyze attack descriptions and predict consequences, helping cybersecurity professionals allocate resources effectively.

Method: The study uses BERT (Bidirectional Encoder Representations from Transformers) combined with Hierarchical Attention Networks (HANs) for multi-label classification of cyberattack consequences into five categories: Availability, Access Control, Confidentiality, Integrity, and Other.

Result: BERT achieved an overall accuracy of 0.972, significantly outperforming conventional CNN and LSTM-based models. HANs also outperformed baseline CNN and LSTM models on specific cybersecurity labels, but BERT consistently showed better precision and recall.

Conclusion: BERT is more suitable for predicting cyberattack consequences due to its superior performance in precision and recall compared to traditional deep learning models, making it effective for automated threat modeling and consequence analysis.

Abstract: Cyberattacks are increasing, and securing against such threats is costing
industries billions of dollars annually. Threat Modeling, that is,
comprehending the consequences of these attacks, can provide critical support
to cybersecurity professionals, enabling them to take timely action and
allocate resources that could be used elsewhere. Cybersecurity is heavily
dependent on threat modeling, as it assists security experts in assessing and
mitigating risks related to identifying vulnerabilities and threats. Recently,
there has been a pressing need for automated methods to assess attack
descriptions and forecast the future consequences of the increasing complexity
of cyberattacks. This study examines how Natural Language Processing (NLP) and
deep learning can be applied to analyze the potential impact of cyberattacks by
leveraging textual descriptions from the MITRE Common Weakness Enumeration
(CWE) database. We emphasize classifying attack consequences into five
principal categories: Availability, Access Control, Confidentiality, Integrity,
and Other. This paper investigates the use of Bidirectional Encoder
Representations from Transformers (BERT) in combination with Hierarchical
Attention Networks (HANs) for Multi-label classification, evaluating their
performance in comparison with conventional CNN and LSTM-based models.
Experimental findings show that BERT achieves an overall accuracy of $0.972$,
far higher than conventional deep learning models in multi-label
classification. HAN outperforms baseline forms of CNN and LSTM-based models on
specific cybersecurity labels. However, BERT consistently achieves better
precision and recall, making it more suitable for predicting the consequences
of a cyberattack.

</details>


### [394] [Beyond Internal Data: Bounding and Estimating Fairness from Incomplete Data](https://arxiv.org/abs/2508.13040)
*Varsha Ramineni,Hossein A. Rahmani,Emine Yilmaz,David Barber*

Main category: cs.LG

TL;DR: Proposes method to estimate AI fairness metrics using separate incomplete datasets when complete demographic data is unavailable due to privacy/legal constraints.


<details>
  <summary>Details</summary>
Motivation: Addresses the challenge of fairness testing in AI systems where complete demographic data is inaccessible due to legal, privacy, and practical constraints, particularly in high-stakes domains like lending, hiring, and healthcare.

Method: Utilizes available separate datasets (internal with predictive attributes and external with protected attributes) to estimate feasible joint distributions and compute plausible fairness metrics through bounds estimation.

Result: Demonstrates through simulations and real experiments that meaningful bounds on fairness metrics can be derived, providing reliable estimates of true fairness metrics.

Conclusion: The approach serves as a practical and effective solution for fairness testing in real-world settings where access to complete data is restricted, enabling compliance with emerging fairness regulations.

Abstract: Ensuring fairness in AI systems is critical, especially in high-stakes
domains such as lending, hiring, and healthcare. This urgency is reflected in
emerging global regulations that mandate fairness assessments and independent
bias audits. However, procuring the necessary complete data for fairness
testing remains a significant challenge. In industry settings, legal and
privacy concerns restrict the collection of demographic data required to assess
group disparities, and auditors face practical and cultural challenges in
gaining access to data. In practice, data relevant for fairness testing is
often split across separate sources: internal datasets held by institutions
with predictive attributes, and external public datasets such as census data
containing protected attributes, each providing only partial, marginal
information. Our work seeks to leverage such available separate data to
estimate model fairness when complete data is inaccessible. We propose
utilising the available separate data to estimate a set of feasible joint
distributions and then compute the set plausible fairness metrics. Through
simulation and real experiments, we demonstrate that we can derive meaningful
bounds on fairness metrics and obtain reliable estimates of the true metric.
Our results demonstrate that this approach can serve as a practical and
effective solution for fairness testing in real-world settings where access to
complete data is restricted.

</details>


### [395] [Hierarchical Evaluation Function (HEF): A Multi-Metric Approach for Optimizing Demand Forecasting Models](https://arxiv.org/abs/2508.13057)
*Adolfo González,Víctor Parada*

Main category: cs.LG

TL;DR: Comparison of two custom evaluation functions (FMAE and HEF) for demand forecasting shows HEF excels in global metrics and robustness for strategic planning, while FMAE performs better in local metrics and speed for operational efficiency.


<details>
  <summary>Details</summary>
Motivation: Multivariate time series modeling faces challenges from data complexity, uncertainty, and regime shifts, with traditional evaluation metrics introducing biases and limiting generalization in demand forecasting.

Method: Experiments comparing FMAE (focused on minimizing absolute errors) and HEF (weighting global metrics and penalizing large deviations) under different data splits (91:9, 80:20, 70:30) using three optimizers (Grid Search, PSO, Optuna).

Result: HEF consistently outperforms FMAE in global metrics (R2, Relative Accuracy, RMSE, RMSSE) and enhances model robustness, while FMAE offers advantages in local metrics (MAE, MASE) and execution time.

Conclusion: Methodological trade-off exists: HEF is ideal for strategic planning due to better global performance, while FMAE is better suited for operational efficiency with faster execution and local metric advantages.

Abstract: Demand forecasting is essential for strategic planning in competitive
environments, enabling resource optimization and improved responsiveness to
market dynamics. However, multivariate time series modeling faces challenges
due to data complexity, uncertainty, and frequent regime shifts. Traditional
evaluation metrics can introduce biases and limit generalization. This work
compares two custom evaluation functions: FMAE (Focused Mean Absolute Error),
focused on minimizing absolute errors, and HEF (Hierarchical Evaluation
Function), designed to weight global metrics and penalize large deviations.
Experiments were conducted under different data splits (91:9, 80:20, 70:30)
using three optimizers (Grid Search, PSO, Optuna), assessing fit, relative
accuracy, robustness, and computational efficiency. Results show that HEF
consistently outperforms FMAE in global metrics (R2, Relative Accuracy, RMSE,
RMSSE), enhancing model robustness and explanatory power. These findings were
confirmed via visualizations and statistical tests. Conversely, FMAE offers
advantages in local metrics (MAE, MASE) and execution time, making it suitable
for short-term scenarios. The study highlights a methodological trade-off: HEF
is ideal for strategic planning, while FMAE is better suited for operational
efficiency. A replicable framework is proposed for optimizing predictive models
in dynamic environments.

</details>


### [396] [Seeing the Many: Exploring Parameter Distributions Conditioned on Features in Surrogates](https://arxiv.org/abs/2508.13088)
*Xiaohan Wang,Zhimin Li,Joshua A. Levine,Matthew Berger*

Main category: cs.LG

TL;DR: This paper presents a method for modeling and visualizing the distribution of input parameters that produce specific output features in neural surrogate models, addressing approximation error and enabling interactive exploration of parameter spaces.


<details>
  <summary>Details</summary>
Motivation: Existing surrogate-based solutions focus on finding individual matching parameters but overlook the broader distribution of plausible parameters. The authors aim to address this gap by modeling the complete parameter distribution that produces given output features.

Method: The approach uses density estimation to model surrogate approximation error, reporting high density only for parameters close to training data in both input and output spaces. This density estimate forms a prior belief on parameters, which when combined with feature likelihood, enables efficient sampling of plausible parameter configurations for target output features.

Result: The method demonstrates usability through a visualization interface for feature-driven parameter analysis across three simulation datasets, providing an efficient way to explore high-dimensional parameter spaces.

Conclusion: The proposed solution successfully addresses the challenges of surrogate approximation error and interactive parameter distribution formation, enabling comprehensive exploration of plausible parameter configurations that generate target output features in scientific simulations.

Abstract: Recently, neural surrogate models have emerged as a compelling alternative to
traditional simulation workflows. This is accomplished by modeling the
underlying function of scientific simulations, removing the need to run
expensive simulations. Beyond just mapping from input parameter to output,
surrogates have also been shown useful for inverse problems: output to input
parameters. Inverse problems can be understood as search, where we aim to find
parameters whose surrogate outputs contain a specified feature. Yet finding
these parameters can be costly, especially for high-dimensional parameter
spaces. Thus, existing surrogate-based solutions primarily focus on finding a
small set of matching parameters, in the process overlooking the broader
picture of plausible parameters. Our work aims to model and visualize the
distribution of possible input parameters that produce a given output feature.
To achieve this goal, we aim to address two challenges: (1) the approximation
error inherent in the surrogate model and (2) forming the parameter
distribution in an interactive manner. We model error via density estimation,
reporting high density only if a given parameter configuration is close to
training parameters, measured both over the input and output space. Our density
estimate is used to form a prior belief on parameters, and when combined with a
likelihood on features, gives us an efficient way to sample plausible parameter
configurations that generate a target output feature. We demonstrate the
usability of our solution through a visualization interface by performing
feature-driven parameter analysis over the input parameter space of three
simulation datasets. Source code is available at
https://github.com/matthewberger/seeing-the-many

</details>


### [397] [Outlier Detection of Poisson-Distributed Targets Using a Seabed Sensor Network](https://arxiv.org/abs/2508.13099)
*Mingyu Kim,Daniel Stilwell,Jorge Jimenez*

Main category: cs.LG

TL;DR: Framework for detecting spatial outliers in maritime environments using sensor networks and LGCPs, with improved classification accuracy through second-order probability approximation and dynamic sensor placement.


<details>
  <summary>Details</summary>
Motivation: Need to accurately classify and detect spatial commission outliers in maritime environments to improve surveillance and security using seabed acoustic sensor networks.

Method: Model target arrivals as mixture of normal and outlier processes using log Gaussian Cox processes. Propose second-order probability approximation incorporating mean and variance of normal intensity. Use Jensen's inequality for tighter probability bounds. Implement real-time near-optimal sensor placement strategy.

Result: Validated with real ship traffic data near Norfolk, Virginia. Numerical results show improved classification performance and outlier detection effectiveness through optimized sensor deployment.

Conclusion: The proposed framework successfully enhances spatial outlier detection in maritime environments through improved probability estimation and dynamic sensor placement, demonstrating practical effectiveness with real-world data.

Abstract: This paper presents a framework for classifying and detecting spatial
commission outliers in maritime environments using seabed acoustic sensor
networks and log Gaussian Cox processes (LGCPs). By modeling target arrivals as
a mixture of normal and outlier processes, we estimate the probability that a
newly observed event is an outlier. We propose a second-order approximation of
this probability that incorporates both the mean and variance of the normal
intensity function, providing improved classification accuracy compared to
mean-only approaches. We analytically show that our method yields a tighter
bound to the true probability using Jensen's inequality. To enhance detection,
we integrate a real-time, near-optimal sensor placement strategy that
dynamically adjusts sensor locations based on the evolving outlier intensity.
The proposed framework is validated using real ship traffic data near Norfolk,
Virginia, where numerical results demonstrate the effectiveness of our approach
in improving both classification performance and outlier detection through
sensor deployment.

</details>


### [398] [A Perfectly Truthful Calibration Measure](https://arxiv.org/abs/2508.13100)
*Jason Hartline,Lunjia Hu,Yifan Wu*

Main category: cs.LG

TL;DR: ATB is a perfectly truthful calibration measure that prevents predictors from lying to appear more calibrated on finite samples, with efficient computation and improved running time.


<details>
  <summary>Details</summary>
Motivation: Existing calibration measures are not truthful - they incentivize predictors to lie about probabilities to appear more calibrated on finite samples rather than outputting true probabilities.

Method: Designed averaged two-bin calibration error (ATB) as a perfectly truthful measure, and introduced a general recipe for constructing truthful calibration measures including quantile-binned l_2-ECE.

Result: ATB is truthful, sound, complete, continuous, and quadratically related to existing measures (smCal and distCal), with faster estimation algorithms and easier implementation.

Conclusion: ATB provides the first perfectly truthful calibration measure that enables reliable probability interpretation while being computationally efficient and simple to implement.

Abstract: Calibration requires that predictions are conditionally unbiased and,
therefore, reliably interpretable as probabilities. Calibration measures
quantify how far a predictor is from perfect calibration. As introduced by
Haghtalab et al. (2024), a calibration measure is truthful if it is minimized
in expectation when a predictor outputs the ground-truth probabilities.
Although predicting the true probabilities guarantees perfect calibration, in
reality, when calibration is evaluated on a finite sample, predicting the truth
is not guaranteed to minimize any known calibration measure. All known
calibration measures incentivize predictors to lie in order to appear more
calibrated on a finite sample. Such lack of truthfulness motivated Haghtalab et
al. (2024) and Qiao and Zhao (2025) to construct approximately truthful
calibration measures in the sequential prediction setting, but no perfectly
truthful calibration measure was known to exist even in the more basic batch
setting.
  We design a perfectly truthful calibration measure in the batch setting:
averaged two-bin calibration error (ATB). In addition to being truthful, ATB is
sound, complete, continuous, and quadratically related to two existing
calibration measures: the smooth calibration error (smCal) and the (lower)
distance to calibration (distCal). The simplicity in our definition of ATB
makes it efficient and straightforward to compute. ATB allows faster estimation
algorithms with significantly easier implementations than smCal and distCal,
achieving improved running time and simplicity for the calibration testing
problem studied by Hu et al. (2024). We also introduce a general recipe for
constructing truthful measures, which proves the truthfulness of ATB as a
special case and allows us to construct other truthful calibration measures
such as quantile-binned l_2-ECE.

</details>


### [399] [Causally-Guided Pairwise Transformer -- Towards Foundational Digital Twins in Process Industry](https://arxiv.org/abs/2508.13111)
*Michael Mayr,Georgios C. Chasparis*

Main category: cs.LG

TL;DR: CGPT is a novel transformer architecture that integrates causal graphs to resolve the trade-off between channel-dependent and channel-independent models for industrial time-series forecasting.


<details>
  <summary>Details</summary>
Motivation: To address the fundamental trade-off where channel-dependent models capture specific cross-variable dynamics but lack robustness, while channel-independent models offer generality but miss explicit interactions crucial for system-level predictive tasks.

Method: Proposes Causally-Guided Pairwise Transformer (CGPT) that uses known causal graphs as inductive bias, decomposes multidimensional data into pairs, employs channel-agnostic learnable layers, and enforces CD information flow at pair-level with CI-like generalization across pairs.

Result: CGPT significantly outperforms both CI and CD baselines in predictive accuracy on synthetic and real-world industrial datasets, showing competitive performance with end-to-end trained CD models while remaining dimensionality-agnostic.

Conclusion: The pairwise modeling paradigm with causal guidance successfully resolves the CD/CI conflict, creating a flexible architecture that ensures scalability and any-variate adaptability for industrial time-series forecasting.

Abstract: Foundational modelling of multi-dimensional time-series data in industrial
systems presents a central trade-off: channel-dependent (CD) models capture
specific cross-variable dynamics but lack robustness and adaptability as model
layers are commonly bound to the data dimensionality of the tackled use-case,
while channel-independent (CI) models offer generality at the cost of modelling
the explicit interactions crucial for system-level predictive regression tasks.
To resolve this, we propose the Causally-Guided Pairwise Transformer (CGPT), a
novel architecture that integrates a known causal graph as an inductive bias.
The core of CGPT is built around a pairwise modeling paradigm, tackling the
CD/CI conflict by decomposing the multidimensional data into pairs. The model
uses channel-agnostic learnable layers where all parameter dimensions are
independent of the number of variables. CGPT enforces a CD information flow at
the pair-level and CI-like generalization across pairs. This approach
disentangles complex system dynamics and results in a highly flexible
architecture that ensures scalability and any-variate adaptability. We validate
CGPT on a suite of synthetic and real-world industrial datasets on long-term
and one-step forecasting tasks designed to simulate common industrial
complexities. Results demonstrate that CGPT significantly outperforms both CI
and CD baselines in predictive accuracy and shows competitive performance with
end-to-end trained CD models while remaining agnostic to the problem
dimensionality.

</details>


### [400] [Contrastive Representations for Temporal Reasoning](https://arxiv.org/abs/2508.13113)
*Alicja Ziarko,Michal Bortkiewicz,Michal Zawalski,Benjamin Eysenbach,Piotr Milos*

Main category: cs.LG

TL;DR: CRTR is a novel temporal representation learning method that overcomes spurious feature reliance in standard contrastive learning, enabling effective temporal reasoning for complex domains like Sokoban and Rubik's Cube without external search algorithms.


<details>
  <summary>Details</summary>
Motivation: Traditional AI separates perception (state-based representations) from planning (search-based temporal reasoning). The paper explores whether temporal reasoning can emerge from representations that capture both perceptual and temporal structure simultaneously.

Method: Introduces Combinatorial Representations for Temporal Reasoning (CRTR), which uses a negative sampling scheme to provably remove spurious features that plague standard temporal contrastive learning methods.

Result: CRTR achieves strong performance on complex temporal domains: solves Sokoban and Rubik's Cube using only learned representations, generalizes across all initial Cube states, and requires fewer search steps than BestFS (though with longer solution paths).

Conclusion: CRTR demonstrates that temporal reasoning can effectively emerge from properly designed representations, enabling efficient solving of complex temporal problems without relying on external search algorithms - a first for arbitrary Rubik's Cube states.

Abstract: In classical AI, perception relies on learning state-based representations,
while planning, which can be thought of as temporal reasoning over action
sequences, is typically achieved through search. We study whether such
reasoning can instead emerge from representations that capture both perceptual
and temporal structure. We show that standard temporal contrastive learning,
despite its popularity, often fails to capture temporal structure due to its
reliance on spurious features. To address this, we introduce Combinatorial
Representations for Temporal Reasoning (CRTR), a method that uses a negative
sampling scheme to provably remove these spurious features and facilitate
temporal reasoning. CRTR achieves strong results on domains with complex
temporal structure, such as Sokoban and Rubik's Cube. In particular, for the
Rubik's Cube, CRTR learns representations that generalize across all initial
states and allow it to solve the puzzle using fewer search steps than BestFS,
though with longer solutions. To our knowledge, this is the first method that
efficiently solves arbitrary Cube states using only learned representations,
without relying on an external search algorithm.

</details>


### [401] [Training Machine Learning Models on Human Spatio-temporal Mobility Data: An Experimental Study [Experiment Paper]](https://arxiv.org/abs/2508.13135)
*Yueyang Liu,Lance Kennedy,Ruochen Kong,Joon-Seok Kim,Andreas Züfle*

Main category: cs.LG

TL;DR: This paper explores best practices for training ML models to predict complete individual trajectories over days/weeks, focusing on incorporating life patterns and addressing data imbalance issues.


<details>
  <summary>Details</summary>
Motivation: Existing mobility prediction research focuses on short-term trajectories and next-location prediction, neglecting macro-level mobility patterns and life routines. The paper aims to determine optimal training strategies for forecasting complete individual trajectories over extended periods.

Method: Comprehensive experimental analysis of LSTM and Transformer models with various parameter configurations and training strategies. Incorporates semantic information (day-of-week, user-specific historical data) and addresses data skewness through user semantic clustering with stratified sampling. Tests small-batch stochastic gradient optimization.

Result: Explicit inclusion of semantic information improves model understanding of individual life patterns. User sampling without proper stratification exacerbates data skewness and reduces predictive accuracy. Small-batch optimization enhances performance, especially with limited training data.

Conclusion: Incorporating life pattern semantics and addressing data imbalance through stratified sampling are crucial for effective long-term human mobility prediction. Small-batch optimization benefits models when training data is limited.

Abstract: Individual-level human mobility prediction has emerged as a significant topic
of research with applications in infectious disease monitoring, child, and
elderly care. Existing studies predominantly focus on the microscopic aspects
of human trajectories: such as predicting short-term trajectories or the next
location visited, while offering limited attention to macro-level mobility
patterns and the corresponding life routines. In this paper, we focus on an
underexplored problem in human mobility prediction: determining the best
practices to train a machine learning model using historical data to forecast
an individuals complete trajectory over the next days and weeks. In this
experiment paper, we undertake a comprehensive experimental analysis of diverse
models, parameter configurations, and training strategies, accompanied by an
in-depth examination of the statistical distribution inherent in human mobility
patterns. Our empirical evaluations encompass both Long Short-Term Memory and
Transformer-based architectures, and further investigate how incorporating
individual life patterns can enhance the effectiveness of the prediction. We
show that explicitly including semantic information such as day-of-the-week and
user-specific historical information can help the model better understand
individual patterns of life and improve predictions. Moreover, since the
absence of explicit user information is often missing due to user privacy, we
show that the sampling of users may exacerbate data skewness and result in a
substantial loss in predictive accuracy. To mitigate data imbalance and
preserve diversity, we apply user semantic clustering with stratified sampling
to ensure that the sampled dataset remains representative. Our results further
show that small-batch stochastic gradient optimization improves model
performance, especially when human mobility training data is limited.

</details>


### [402] [MDPO: Overcoming the Training-Inference Divide of Masked Diffusion Language Models](https://arxiv.org/abs/2508.13148)
*Haoyu He,Katrin Renz,Yong Cao,Andreas Geiger*

Main category: cs.LG

TL;DR: MDPO addresses training-inference discrepancy in masked diffusion language models using reinforcement learning and progressive refining, achieving 60x faster training and significant performance improvements on MATH500 and Countdown tasks.


<details>
  <summary>Details</summary>
Motivation: Diffusion language models suffer from a key discrepancy between training (random masking) and inference (progressive revealing of structure), leading to suboptimal performance that previous works have overlooked.

Method: Proposed Masked Diffusion Policy Optimization (MDPO) that frames denoising trajectories as sequential decision-making using reinforcement learning, explicitly training under the same progressive refining schedule used at inference. Also improved remasking strategy (RCR) as a plug-in inference replacement.

Result: MDPO matches previous SOTA performance with 60x fewer gradient updates, achieves 9.6% improvement on MATH500 and 54.2% on Countdown over SOTA with same update budget. RCR strategy provides consistent performance improvements and additional gains when combined with MDPO.

Conclusion: The work establishes great potential for investigating the discrepancy between pre-training and inference of masked diffusion language models, with MDPO and RCR providing effective solutions for improved training efficiency and performance.

Abstract: Diffusion language models, as a promising alternative to traditional
autoregressive (AR) models, enable faster generation and richer conditioning on
bidirectional context. However, they suffer from a key discrepancy between
training and inference: during inference, MDLMs progressively reveal the
structure of the generated sequence by producing fewer and fewer masked tokens,
whereas this structure is ignored in training as tokens are masked at random.
Although this discrepancy between training and inference can lead to suboptimal
performance, it has been largely overlooked by previous works, leaving closing
this gap between the two stages an open problem. To address this, we frame the
problem of learning effective denoising trajectories as a sequential
decision-making problem and use the resulting framework to apply reinforcement
learning. We propose a novel Masked Diffusion Policy Optimization (MDPO) to
exploit the Markov property diffusion possesses and explicitly train the model
under the same progressive refining schedule used at inference. MDPO matches
the performance of the previous state-of-the-art (SOTA) method with 60x fewer
gradient updates, while achieving average improvements of 9.6% on MATH500 and
54.2% on Countdown over SOTA when trained within the same number of weight
updates. Additionally, we improve the remasking strategy of MDLMs as a plug-in
inference replacement to overcome the limitation that the model cannot refine
tokens flexibly. This simple yet effective training-free strategy, what we
refer to as RCR, consistently improves performance and yields additional gains
when combined with MDPO. Our findings establish great potential for
investigating the discrepancy between pre-training and inference of MDLMs.
Code: https://github.com/autonomousvision/mdpo. Project Page:
https://cli212.github.io/MDPO/.

</details>


<div id='cond-mat.soft'></div>

# cond-mat.soft [[Back]](#toc)

### [403] [Generalized invariants meet constitutive neural networks: A novel framework for hyperelastic materials](https://arxiv.org/abs/2508.12063)
*Denisa Martonová,Alain Goriely,Ellen Kuhl*

Main category: cond-mat.soft

TL;DR: A new data-driven framework that simultaneously discovers optimal invariants and constitutive models for isotropic incompressible hyperelastic materials using neural networks.


<details>
  <summary>Details</summary>
Motivation: Traditional methods face challenges in choosing appropriate invariants and strain energy functions for hyperelastic materials, requiring manual selection and sequential fitting procedures.

Method: Integrated neural network architecture that explores a continuous family of generalized invariants and learns the strain energy function directly from experimental data in a unified process.

Result: Successfully recovered stretch-dominated formulation for rubber (consistent with classical models) and identified small-stretch sensitive formulation for brain tissue capturing nonlinear shear response. Achieved improved predictive accuracy and interpretability compared to traditional and neural-network-based models.

Conclusion: The framework provides a robust, automated tool for physically meaningful model discovery in hyperelasticity that adapts flexibly to different material behaviors.

Abstract: The major challenge in determining a hyperelastic model for a given material
is the choice of invariants and the selection how the strain energy function
depends functionally on these invariants. Here we introduce a new data-driven
framework that simultaneously discovers appropriate invariants and constitutive
models for isotropic incompressible hyperelastic materials. Our approach
identifies both the most suitable invariants in a class of generalized
invariants and the corresponding strain energy function directly from
experimental observations. Unlike previous methods that rely on fixed invariant
choices or sequential fitting procedures, our method integrates the discovery
process into a single neural network architecture. By looking at a continuous
family of possible invariants, the model can flexibly adapt to different
material behaviors. We demonstrate the effectiveness of this approach using
popular benchmark datasets for rubber and brain tissue. For rubber, the method
recovers a stretch-dominated formulation consistent with classical models. For
brain tissue, it identifies a formulation sensitive to small stretches,
capturing the nonlinear shear response characteristic of soft biological
matter. Compared to traditional and neural-network-based models, our framework
provides improved predictive accuracy and interpretability across a wide range
of deformation states. This unified strategy offers a robust tool for automated
and physically meaningful model discovery in hyperelasticity.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [404] [BConformeR: A Conformer Based on Mutual Sampling for Unified Prediction of Continuous and Discontinuous Antibody Binding Sites](https://arxiv.org/abs/2508.12029)
*Zhangyu You,Jiahao Ma,Hongzong Li,Ye-Fan Hu,Jian-Dong Huang*

Main category: q-bio.BM

TL;DR: A conformer-based model combining CNNs and Transformers outperforms existing methods for predicting conformational antibody-binding sites (epitopes) on antigens.


<details>
  <summary>Details</summary>
Motivation: Accurate prediction of conformational epitopes is crucial for vaccine design, therapeutic antibody development, and understanding immune responses, but existing methods consistently underperform for conformational epitopes compared to linear ones.

Method: A conformer-based model trained on antigen sequences from 1,080 antigen-antibody complexes, using CNNs to extract local features and Transformers to capture long-range dependencies within antigen sequences.

Result: The model outperforms existing baselines in PCC, ROC-AUC, PR-AUC, and F1 scores on conformational epitopes. Ablation studies show CNNs enhance linear epitope prediction while Transformers improve conformational epitope prediction.

Conclusion: The proposed hybrid CNN-Transformer architecture effectively addresses the challenge of conformational epitope prediction, demonstrating superior performance over existing methods.

Abstract: Accurate prediction of antibody-binding sites (epitopes) on antigens is
crucial for vaccine design, immunodiagnostics, therapeutic antibody
development, antibody engineering, research into autoimmune and allergic
diseases, and for advancing our understanding of immune responses. Despite in
silico methods that have been proposed to predict both linear (continuous) and
conformational (discontinuous) epitopes, they consistently underperform in
predicting conformational epitopes. In this work, we propose a conformer-based
model trained on antigen sequences derived from 1,080 antigen-antibody
complexes, leveraging convolutional neural networks (CNNs) to extract local
features and Transformers to capture long-range dependencies within antigen
sequences. Ablation studies demonstrate that CNN enhances the prediction of
linear epitopes, and the Transformer module improves the prediction of
conformational epitopes. Experimental results show that our model outperforms
existing baselines in terms of PCC, ROC-AUC, PR-AUC, and F1 scores on
conformational epitopes.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [405] [Point upsampling networks for single-photon sensing](https://arxiv.org/abs/2508.12986)
*Jinyi Liu,Guoyang Zhao,Lijun Liu,Yiguang Hong,Weiping Zhang,Shuming Cheng*

Main category: physics.optics

TL;DR: Proposes a point upsampling network using state space models to enhance sparse single-photon point clouds by increasing density and reducing spatial distortion.


<details>
  <summary>Details</summary>
Motivation: Single-photon sensing produces sparse and spatially biased point clouds that limit practical utility, requiring methods to improve point density and reduce distortion.

Method: Built on state space model with multi-path scanning mechanism, bidirectional Mamba backbone for global/local feature capture, and adaptive upsample shift module for distortion correction.

Result: Achieves high reconstruction accuracy and strong robustness to distortion noise, generates visually consistent, detail-preserving, and noise-suppressed point clouds on real-world data.

Conclusion: First upsampling framework for single-photon sensing that opens new avenues for practical applications in downstream tasks.

Abstract: Single-photon sensing has generated great interest as a prominent technique
of long-distance and ultra-sensitive imaging, however, it tends to yield sparse
and spatially biased point clouds, thus limiting its practical utility. In this
work, we propose using point upsampling networks to increase point density and
reduce spatial distortion in single-photon point cloud. Particularly, our
network is built on the state space model which integrates a multi-path
scanning mechanism to enrich spatial context, a bidirectional Mamba backbone to
capture global geometry and local details, and an adaptive upsample shift
module to correct offset-induced distortions. Extensive experiments are
implemented on commonly-used datasets to confirm its high reconstruction
accuracy and strong robustness to the distortion noise, and also on real-world
data to demonstrate that our model is able to generate visually consistent,
detail-preserving, and noise suppressed point clouds. Our work is the first to
establish the upsampling framework for single-photon sensing, and hence opens a
new avenue for single-photon sensing and its practical applications in the
downstreaming tasks.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [406] [HPD: Hybrid Projection Decomposition for Robust State Space Models on Analog CIM Hardware](https://arxiv.org/abs/2508.11935)
*Yuannuo Feng,Wenyong Zhou,Yuexi Lyu,Hanjie Liu,Zhengwu Liu,Ngai Wong,Wang Kang*

Main category: cs.AR

TL;DR: HPD method improves State Space Models' robustness to hardware noise by decomposing the output projection layer, achieving up to 99.57% perplexity reduction and 96.67% accuracy gains.


<details>
  <summary>Details</summary>
Motivation: State Space Models are efficient for long sequences and suitable for compute-in-memory architectures, but device non-idealities cause weight perturbations that degrade inference accuracy.

Method: Proposed HPD (Hybrid Projection Decomposition) that replaces the output projection weight matrix with U and Σ from SVD, offloading Vᵀ to digital hardware for precise correction while maintaining hardware compatibility.

Result: Comprehensive tests on Mamba models show up to 99.57% perplexity reduction under various noise conditions and up to 96.67% accuracy improvement on PIQA benchmark compared to baseline models.

Conclusion: The HPD strategy effectively enhances SSM robustness to hardware noise by identifying and protecting vulnerable components, particularly the output projection layer, through hybrid analog-digital decomposition.

Abstract: State Space Models (SSMs) are efficient alternatives to traditional sequence
models, excelling at processing long sequences with lower computational
complexity. Their reliance on matrix multiplications makes them ideal for
compute-in-memory (CIM) architectures, which improve energy efficiency by
computing within memory arrays. However, device non-idealities in CIM introduce
weight perturbations that can degrade inference accuracy. In this paper, we
systematically analyze the robustness of SSMs under noisy conditions,
identifying that the final block and output projection layers are more
susceptible to perturbations compared to other components. Building on these
insights, we propose HPD, a Hybrid Projection Decomposition strategy for the
last output projection layer. We replace the original weight matrix with the
multiplication of U and {\Sigma} in its SVD to ensure compatibility with
existing hardware architectures, while offloading V> to digital hardware for
precise and robust correction. Comprehensive tests on Mamba models show that
our method reduces perplexity by up to 99.57% under various noise conditions
compared to baseline models, with accuracy gains of up to 96.67% on the PIQA
benchmark for commonsense reasoning.

</details>


### [407] [HOMI: Ultra-Fast EdgeAI platform for Event Cameras](https://arxiv.org/abs/2508.12637)
*Shankaranarayanan H,Satyapreet Singh Yadav,Adithya Krishna,Ajay Vikram P,Mahesh Mehendale,Chetan Singh Thakur*

Main category: cs.AR

TL;DR: HOMI is an ultra-low latency edge AI platform combining event camera sensor with FPGA chip and custom AI accelerator, achieving 94% accuracy on gesture recognition and 1000 fps throughput.


<details>
  <summary>Details</summary>
Motivation: Event cameras have advantages for robotics but existing solutions lack complete end-to-end implementations, have high latency, and don't fully exploit event data sparsity.

Method: Hardware-optimized pre-processing pipelines with constant-time and constant-event modes for histogram accumulation, linear and exponential time surfaces on Xilinx Zynq UltraScale+MPSoC FPGA with custom AI accelerator.

Result: 94% accuracy on DVS Gesture dataset for high accuracy mode, 1000 fps throughput for low-latency configuration, using only 33% of FPGA LUT resources with compact memory footprint.

Conclusion: HOMI provides efficient end-to-end event processing with flexibility for both accuracy-driven and low-latency applications, leaving room for further optimization and complex architectures.

Abstract: Event cameras offer significant advantages for edge robotics applications due
to their asynchronous operation and sparse, event-driven output, making them
well-suited for tasks requiring fast and efficient closed-loop control, such as
gesture-based human-robot interaction. Despite this potential, existing event
processing solutions remain limited, often lacking complete end-to-end
implementations, exhibiting high latency, and insufficiently exploiting event
data sparsity. In this paper, we present HOMI, an ultra-low latency, end-to-end
edge AI platform comprising a Prophesee IMX636 event sensor chip with an Xilinx
Zynq UltraScale+MPSoC FPGA chip, deploying an in-house developed AI
accelerator. We have developed hardware-optimized pre-processing pipelines
supporting both constant-time and constant-event modes for histogram
accumulation, linear and exponential time surfaces. Our general-purpose
implementation caters to both accuracy-driven and low-latency applications.
HOMI achieves 94% accuracy on the DVS Gesture dataset as a use case when
configured for high accuracy operation and provides a throughput of 1000 fps
for low-latency configuration. The hardware-optimised pipeline maintains a
compact memory footprint and utilises only 33% of the available LUT resources
on the FPGA, leaving ample headroom for further latency reduction, model
parallelisation, multi-task deployments, or integration of more complex
architectures.

</details>


### [408] [XR-NPE: High-Throughput Mixed-precision SIMD Neural Processing Engine for Extended Reality Perception Workloads](https://arxiv.org/abs/2508.13049)
*Tejas Chaudhari,Akarsh J.,Tanushree Dewangan,Mukul Lokhande,Santosh Kumar Vishvakarma*

Main category: cs.AR

TL;DR: XR-NPE is a high-throughput mixed-precision SIMD neural processing engine designed for XR workloads, supporting novel FP4 and Posit formats with layer-adaptive implementation and quantization-aware training to reduce memory bandwidth while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Extended reality (XR) devices require efficient neural processing for perception workloads like visual inertial odometry and object classification, but face challenges with memory bandwidth and energy consumption in resource-constrained environments.

Method: Proposes XR-NPE with Reconfigurable Mantissa Multiplication and Exponent processing Circuitry (RMMEC), selective power gating, and support for FP4, Posit(4,1), Posit(8,0), and Posit(16,1) formats with quantization-aware training.

Result: Achieves 1.72 GHz operating frequency, 0.016 mm² area, 14 pJ arithmetic intensity at 28nm CMOS, with 42% area reduction and 38% power reduction compared to state-of-the-art MAC approaches. Provides 23% better energy efficiency and 4% better compute density for VIO workloads.

Conclusion: XR-NPE establishes itself as a scalable, precision-adaptive compute engine suitable for future resource-constrained XR devices, with publicly released code for reproducibility and adoption.

Abstract: This work proposes XR-NPE, a high-throughput Mixed-precision SIMD Neural
Processing Engine, designed for extended reality (XR) perception workloads like
visual inertial odometry (VIO), object classification, and eye gaze extraction.
XR-NPE is first to support FP4, Posit (4,1), Posit (8,0), and Posit (16,1)
formats, with layer adaptive hybrid-algorithmic implementation supporting
ultra-low bit precision to significantly reduce memory bandwidth requirements,
and accompanied by quantization-aware training for minimal accuracy loss. The
proposed Reconfigurable Mantissa Multiplication and Exponent processing
Circuitry (RMMEC) reduces dark silicon in the SIMD MAC compute engine, assisted
by selective power gating to reduce energy consumption, providing 2.85x
improved arithmetic intensity. XR-NPE achieves a maximum operating frequency of
1.72 GHz, area 0.016 mm2 , and arithmetic intensity 14 pJ at CMOS 28nm,
reducing 42% area, 38% power compared to the best of state-of-the-art MAC
approaches. The proposed XR-NPE based AXI-enabled Matrix-multiplication
co-processor consumes 1.4x fewer LUTs, 1.77x fewer FFs, and provides 1.2x
better energy efficiency compared to SoTA accelerators on VCU129. The proposed
co-processor provides 23% better energy efficiency and 4% better compute
density for VIO workloads. XR-NPE establishes itself as a scalable,
precision-adaptive compute engine for future resource-constrained XR devices.
The complete set for codes for results reproducibility are released publicly,
enabling designers and researchers to readily adopt and build upon them.
https://github.com/mukullokhande99/XR-NPE.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [409] [DIT: Dimension Reduction View on Optimal NFT Rarity Meters](https://arxiv.org/abs/2508.12671)
*Dmitry Belousov,Yury Yanovich*

Main category: cs.DC

TL;DR: The paper introduces a new NFT rarity assessment framework called ROAR benchmark and proposes a dimension reduction approach with novel performance measures, including DIT, which outperforms existing rarity meters.


<details>
  <summary>Details</summary>
Motivation: NFT rarity quantification is important for valuation but existing rarity meters are hard to compare without direct access to collection data, requiring a standardized evaluation framework.

Method: Developed an optimal rarity meter using non-metric weighted multidimensional scaling and introduced Dissimilarity in Trades (DIT) as a performance measure inspired by dimension reduction techniques.

Result: The proposed non-interpretable rarity meter DIT demonstrates superior performance compared to existing methods when evaluated using the ROAR benchmark.

Conclusion: The ROAR benchmark provides a standardized framework for NFT rarity evaluation, and the dimension reduction approach with DIT performance measure offers improved rarity assessment capabilities.

Abstract: Non-fungible tokens (NFTs) have become a significant digital asset class,
each uniquely representing virtual entities such as artworks. These tokens are
stored in collections within smart contracts and are actively traded across
platforms on Ethereum, Bitcoin, and Solana blockchains. The value of NFTs is
closely tied to their distinctive characteristics that define rarity, leading
to a growing interest in quantifying rarity within both industry and academia.
While there are existing rarity meters for assessing NFT rarity, comparing them
can be challenging without direct access to the underlying collection data. The
Rating over all Rarities (ROAR) benchmark addresses this challenge by providing
a standardized framework for evaluating NFT rarity. This paper explores a
dimension reduction approach to rarity design, introducing new performance
measures and meters, and evaluates them using the ROAR benchmark. Our
contributions to the rarity meter design issue include developing an optimal
rarity meter design using non-metric weighted multidimensional scaling,
introducing Dissimilarity in Trades (DIT) as a performance measure inspired by
dimension reduction techniques, and unveiling the non-interpretable rarity
meter DIT, which demonstrates superior performance compared to existing
methods.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [410] [Code Vulnerability Detection Across Different Programming Languages with AI Models](https://arxiv.org/abs/2508.11710)
*Hael Abdulhakim Ali Humran,Ferdi Sonmez*

Main category: cs.CR

TL;DR: Transformer-based AI models like CodeBERT and CodeLlama show strong potential for detecting security vulnerabilities in code, achieving over 97% accuracy and outperforming traditional rule-based static analysis tools.


<details>
  <summary>Details</summary>
Motivation: Traditional static analysis tools based on rule-based patterns struggle with context-dependent bugs and produce high false positive rates, creating a need for more effective vulnerability detection methods.

Method: The approach involves dataset collection, code normalization, fine-tuning transformer models (CodeBERT/CodeLlama) on vulnerable/safe code fragments, and incorporating ensemble learning with explainable AI techniques.

Result: Experiments show well-trained CodeBERT achieves >97% accuracy, outperforming existing static analyzers. Models demonstrate good generalization across programming languages and vulnerability types, though precision can decrease while recall remains high.

Conclusion: AI-based solutions show promise for enhancing vulnerability detection trustworthiness, usability, and scalability, but require further development in robustness, interpretability, and deployment readiness through hybrid models and validation procedures.

Abstract: Security vulnerabilities present in a code that has been written in diverse
programming languages are among the most critical yet complicated aspects of
source code to detect. Static analysis tools based on rule-based patterns
usually do not work well at detecting the context-dependent bugs and lead to
high false positive rates. Recent developments in artificial intelligence,
specifically the use of transformer-based models like CodeBERT and CodeLlama,
provide light to this problem, as they show potential in finding such flaws
better. This paper presents the implementations of these models on various
datasets of code vulnerability, showing how off-the-shelf models can
successfully produce predictive capacity in models through dynamic fine-tuning
of the models on vulnerable and safe code fragments. The methodology comprises
the gathering of the dataset, normalization of the language, fine-tuning of the
model, and incorporation of ensemble learning and explainable AI. Experiments
show that a well-trained CodeBERT can be as good as or even better than some
existing static analyzers in terms of accuracy greater than 97%. Further study
has indicated that although language models can achieve close-to-perfect
recall, the precision can decrease. A solution to this is given by hybrid
models and validation procedures, which will reduce false positives. According
to the results, the AI-based solutions generalize to different programming
languages and classes of vulnerability. Nevertheless, robustness,
interpretability, and deployment readiness are still being developed. The
results illustrate the probabilities that AI will enhance the trustworthiness
in the usability and scalability of machine-learning-based detectors of
vulnerabilities.

</details>


### [411] [Enhancing GraphQL Security by Detecting Malicious Queries Using Large Language Models, Sentence Transformers, and Convolutional Neural Networks](https://arxiv.org/abs/2508.11711)
*Irash Perera,Hiranya Abeyrathne,Sanjeewa Malalgoda,Arshardh Ifthikar*

Main category: cs.CR

TL;DR: AI-driven real-time detection system for malicious GraphQL queries using machine learning and static analysis to address unique security vulnerabilities in GraphQL APIs.


<details>
  <summary>Details</summary>
Motivation: GraphQL's flexibility introduces security vulnerabilities that traditional API security mechanisms fail to address, including DoS attacks, data exfiltration, and injection exploits that existing solutions like static analysis and WAFs cannot effectively handle.

Method: Combines static analysis with machine learning techniques including LLMs for dynamic schema configuration, Sentence Transformers (SBERT and Doc2Vec) for contextual embedding, and CNNs, Random Forests, and MLPs for classification. Features production-optimized implementation with ONNX Runtime and parallel processing.

Result: High accuracy in detecting various threats including SQL injection, OS command injection, XSS exploits, and effective mitigation of DoS and SSRF attacks. System performs well under load with optimized production deployment.

Conclusion: Presents a robust and adaptable AI-driven solution that significantly enhances GraphQL API security by effectively detecting and mitigating sophisticated, context-aware attacks that traditional methods cannot address.

Abstract: GraphQL's flexibility, while beneficial for efficient data fetching,
introduces unique security vulnerabilities that traditional API security
mechanisms often fail to address. Malicious GraphQL queries can exploit the
language's dynamic nature, leading to denial-of-service attacks, data
exfiltration through injection, and other exploits. Existing solutions, such as
static analysis, rate limiting, and general-purpose Web Application Firewalls,
offer limited protection against sophisticated, context-aware attacks. This
paper presents a novel, AI-driven approach for real-time detection of malicious
GraphQL queries. Our method combines static analysis with machine learning
techniques, including Large Language Models (LLMs) for dynamic schema-based
configuration, Sentence Transformers (SBERT and Doc2Vec) for contextual
embedding of query payloads, and Convolutional Neural Networks (CNNs), Random
Forests, and Multilayer Perceptrons for classification. We detail the system
architecture, implementation strategies optimized for production environments
(including ONNX Runtime optimization and parallel processing), and evaluate the
performance of our detection models and the overall system under load. Results
demonstrate high accuracy in detecting various threats, including SQL
injection, OS command injection, and XSS exploits, alongside effective
mitigation of DoS and SSRF attempts. This research contributes a robust and
adaptable solution for enhancing GraphQL API security.

</details>


### [412] [Privacy-Aware Detection of Fake Identity Documents: Methodology, Benchmark, and Improved Detection Methods (FakeIDet2)](https://arxiv.org/abs/2508.11716)
*Javier Muñoz-Haro,Ruben Tolosana,Ruben Vera-Rodriguez,Aythami Morales,Julian Fierrez*

Main category: cs.CR

TL;DR: Proposes privacy-preserving fake ID detection using patch-based methodology, releases FakeIDet2-db dataset with 900K+ patches, and introduces FakeIDet2 method with benchmark for physical and synthetic ID attacks.


<details>
  <summary>Details</summary>
Motivation: Remote user verification via ID documents is crucial but vulnerable to AI-generated fake IDs. Research is hindered by lack of real ID data due to privacy concerns.

Method: Patch-based methodology to preserve privacy, creation of FakeIDet2-db dataset with 900K+ real/fake ID patches, development of FakeIDet2 detection method, and reproducible benchmark.

Result: Provides comprehensive solution addressing data scarcity through privacy-preserving patches and establishes benchmark for evaluating fake ID detection against various physical attacks.

Conclusion: The proposed approach enables effective fake ID detection research while maintaining privacy, with publicly available dataset and benchmark to advance the field.

Abstract: Remote user verification in Internet-based applications is becoming
increasingly important nowadays. A popular scenario for it consists of
submitting a picture of the user's Identity Document (ID) to a service
platform, authenticating its veracity, and then granting access to the
requested digital service. An ID is well-suited to verify the identity of an
individual, since it is government issued, unique, and nontransferable.
However, with recent advances in Artificial Intelligence (AI), attackers can
surpass security measures in IDs and create very realistic physical and
synthetic fake IDs. Researchers are now trying to develop methods to detect an
ever-growing number of these AI-based fakes that are almost indistinguishable
from authentic (bona fide) IDs. In this counterattack effort, researchers are
faced with an important challenge: the difficulty in using real data to train
fake ID detectors. This real data scarcity for research and development is
originated by the sensitive nature of these documents, which are usually kept
private by the ID owners (the users) and the ID Holders (e.g., government,
police, bank, etc.). The main contributions of our study are: 1) We propose and
discuss a patch-based methodology to preserve privacy in fake ID detection
research. 2) We provide a new public database, FakeIDet2-db, comprising over
900K real/fake ID patches extracted from 2,000 ID images, acquired using
different smartphone sensors, illumination and height conditions, etc. In
addition, three physical attacks are considered: print, screen, and composite.
3) We present a new privacy-aware fake ID detection method, FakeIDet2. 4) We
release a standard reproducible benchmark that considers physical and synthetic
attacks from popular databases in the literature.

</details>


### [413] [Optimizing Token Choice for Code Watermarking: A RL Approach](https://arxiv.org/abs/2508.11925)
*Zhimeng Guo,Huaisheng Zhu,Siyuan Xu,Hangfan Zhang,Teng Xiao,Minhao Cheng*

Main category: cs.CR

TL;DR: CodeTracer is a reinforcement learning-based framework for watermarking LLM-generated code that intelligently biases token choices to embed detectable watermarks while preserving code functionality.


<details>
  <summary>Details</summary>
Motivation: There's a growing need to detect LLM-generated code, requiring watermarking systems that can operate within the highly structured and syntactically constrained environment of programming languages.

Method: Uses a policy-driven reinforcement learning approach with parameterized model to bias token choices during next-token prediction, Gumbel Top-k reparameterization for gradient optimization, and comprehensive reward system integrating execution feedback with watermark signals.

Result: Extensive evaluations show CodeTracer significantly outperforms state-of-the-art baselines in both watermark detectability and preservation of generated code's functionality.

Conclusion: The framework successfully addresses the challenge of watermarking LLM-generated code by maintaining functionality while embedding statistically detectable deviations through adaptive reinforcement learning.

Abstract: The need for detecting LLM-generated code necessitates watermarking systems
capable of operating within its highly structured and syntactically constrained
environment. To address this, we introduce CodeTracer, an innovative adaptive
code watermarking framework underpinned by a novel reinforcement learning
training paradigm. At its core, CodeTracer features a policy-driven approach
that utilizes a parameterized model to intelligently bias token choices during
next-token prediction. This strategy ensures that embedded watermarks maintain
code functionality while exhibiting subtle yet statistically detectable
deviations from typical token distributions. To facilitate policy learning, we
devise a comprehensive reward system that seamlessly integrates execution
feedback with watermark embedding signals, balancing process-level and
outcome-level rewards. Additionally, we employ Gumbel Top-k reparameterization
to enable gradient-based optimization of discrete watermarking decisions.
Extensive comparative evaluations demonstrate CodeTracer's significant
superiority over state-of-the-art baselines in both watermark detectability and
the preservation of generated code's functionality.

</details>


### [414] [Mitigating Jailbreaks with Intent-Aware LLMs](https://arxiv.org/abs/2508.12072)
*Wei Jie Yeo,Ranjan Satapathy,Erik Cambria*

Main category: cs.CR

TL;DR: Intent-FT is a lightweight fine-tuning method that trains LLMs to infer instruction intent before responding, significantly improving jailbreak resistance while preserving general capabilities.


<details>
  <summary>Details</summary>
Motivation: Large language models remain vulnerable to jailbreak attacks despite safety-tuning, reflecting a persistent trade-off between safety and task performance.

Method: Fine-tuning LLMs on adversarial instructions to explicitly infer underlying intent before generating responses, enabling generalization to unseen attacks.

Result: Consistently mitigates all evaluated attack categories with no single attack exceeding 50% success rate, preserves general capabilities, reduces excessive refusals, and enables intent transfer to enhance vanilla models.

Conclusion: Intent-FT provides a simple yet effective approach to significantly improve LLM robustness against jailbreak attacks while maintaining utility and reducing over-refusal issues.

Abstract: Despite extensive safety-tuning, large language models (LLMs) remain
vulnerable to jailbreak attacks via adversarially crafted instructions,
reflecting a persistent trade-off between safety and task performance. In this
work, we propose Intent-FT, a simple and lightweight fine-tuning approach that
explicitly trains LLMs to infer the underlying intent of an instruction before
responding. By fine-tuning on a targeted set of adversarial instructions,
Intent-FT enables LLMs to generalize intent deduction to unseen attacks,
thereby substantially improving their robustness. We comprehensively evaluate
both parametric and non-parametric attacks across open-source and proprietary
models, considering harmfulness from attacks, utility, over-refusal, and impact
against white-box threats. Empirically, Intent-FT consistently mitigates all
evaluated attack categories, with no single attack exceeding a 50\% success
rate -- whereas existing defenses remain only partially effective. Importantly,
our method preserves the model's general capabilities and reduces excessive
refusals on benign instructions containing superficially harmful keywords.
Furthermore, models trained with Intent-FT accurately identify hidden harmful
intent in adversarial attacks, and these learned intentions can be effectively
transferred to enhance vanilla model defenses.

</details>


### [415] [Where to Start Alignment? Diffusion Large Language Model May Demand a Distinct Position](https://arxiv.org/abs/2508.12398)
*Zhixin Xie,Xurui Song,Jun Luo*

Main category: cs.CR

TL;DR: First safety analysis of Diffusion Large Language Models (dLLMs) revealing critical asymmetry between defenders and attackers, with middle tokens being most important for safety. Proposes MOSA method for middle-token alignment using reinforcement learning.


<details>
  <summary>Details</summary>
Motivation: dLLMs have emerged as competitive non-autoregressive models but lack safety studies. There's a need to understand their unique security characteristics and develop tailored safety alignment methods.

Method: Identifies asymmetry in security: defenders benefit from aligning middle tokens, while attackers have limited middle-token manipulation due to dLLMs' sequential generation order. Proposes MOSA (Middle-tOken Safety Alignment) using reinforcement learning to directly align middle generation with safe refusals.

Result: Tested MOSA against eight attack methods on two benchmarks. MOSA-aligned dLLM showed superior security performance while maintaining utility on coding, math, and general reasoning tasks.

Conclusion: MOSA proves highly effective for dLLM safety alignment, leveraging the identified asymmetry between defender and attacker capabilities. Middle tokens are critical for safety in dLLMs, and MOSA successfully addresses this through targeted reinforcement learning alignment.

Abstract: Diffusion Large Language Models (dLLMs) have recently emerged as a
competitive non-autoregressive paradigm due to their unique training and
inference approach. However, there is currently a lack of safety study on this
novel architecture. In this paper, we present the first analysis of dLLMs'
safety performance and propose a novel safety alignment method tailored to
their unique generation characteristics. Specifically, we identify a critical
asymmetry between the defender and attacker in terms of security. For the
defender, we reveal that the middle tokens of the response, rather than the
initial ones, are more critical to the overall safety of dLLM outputs; this
seems to suggest that aligning middle tokens can be more beneficial to the
defender. The attacker, on the contrary, may have limited power to manipulate
middle tokens, as we find dLLMs have a strong tendency towards a sequential
generation order in practice, forcing the attack to meet this distribution and
diverting it from influencing the critical middle tokens. Building on this
asymmetry, we introduce Middle-tOken Safety Alignment (MOSA), a novel method
that directly aligns the model's middle generation with safe refusals
exploiting reinforcement learning. We implement MOSA and compare its security
performance against eight attack methods on two benchmarks. We also test the
utility of MOSA-aligned dLLM on coding, math, and general reasoning. The
results strongly prove the superiority of MOSA.

</details>


### [416] [Deciphering the Interplay between Attack and Protection Complexity in Privacy-Preserving Federated Learning](https://arxiv.org/abs/2508.11907)
*Xiaojin Zhang,Mingcong Xu,Yiming Li,Wei Chen,Qiang Yang*

Main category: cs.CR

TL;DR: A theoretical framework analyzing the trade-offs between attack and protection complexities in privacy-preserving federated learning, with formal definitions and bounds for both attack complexity (resources needed to reconstruct private data) and protection complexity (distortion from privacy mechanisms).


<details>
  <summary>Details</summary>
Motivation: Federated learning's vulnerability to gradient inversion attacks requires robust privacy protection, but there's a need to understand the fundamental trade-offs between attack difficulty, protection effectiveness, and system utility.

Method: Introduces Maximum Bayesian Privacy (MBP) framework to formally define and analyze attack complexity (minimum resources for data reconstruction) and protection complexity (expected distortion from privacy mechanisms), deriving tight theoretical bounds for both.

Result: Established comprehensive bounds showing protection complexity scales with model dimensionality and privacy budget, while attack complexity depends on privacy leakage, gradient distortion, model dimension, and privacy level.

Conclusion: The framework provides quantitative insights into privacy-utility trade-offs and offers critical guidance for designing more secure and efficient federated learning systems.

Abstract: Federated learning (FL) offers a promising paradigm for collaborative model
training while preserving data privacy. However, its susceptibility to gradient
inversion attacks poses a significant challenge, necessitating robust privacy
protection mechanisms. This paper introduces a novel theoretical framework to
decipher the intricate interplay between attack and protection complexities in
privacy-preserving FL. We formally define "Attack Complexity" as the minimum
computational and data resources an adversary requires to reconstruct private
data below a given error threshold, and "Protection Complexity" as the expected
distortion introduced by privacy mechanisms. Leveraging Maximum Bayesian
Privacy (MBP), we derive tight theoretical bounds for protection complexity,
demonstrating its scaling with model dimensionality and privacy budget.
Furthermore, we establish comprehensive bounds for attack complexity, revealing
its dependence on privacy leakage, gradient distortion, model dimension, and
the chosen privacy level. Our findings quantitatively illuminate the
fundamental trade-offs between privacy guarantees, system utility, and the
effort required for both attacking and defending. This framework provides
critical insights for designing more secure and efficient federated learning
systems.

</details>


### [417] [Substituting Proof of Work in Blockchain with Training-Verified Collaborative Model Computation](https://arxiv.org/abs/2508.12138)
*Mohammad Ishzaz Asif Rafid,Morsalin Sakib*

Main category: cs.CR

TL;DR: Hybrid architecture replaces Bitcoin's PoW with cloud-based collaborative ML training, where miners train model segments and get certificates based on contribution quality instead of solving cryptographic puzzles.


<details>
  <summary>Details</summary>
Motivation: Address Bitcoin's excessive energy consumption and hardware inefficiencies in traditional Proof of Work mining by redirecting computational resources toward productive machine learning tasks.

Method: Centralized cloud framework where miners contribute computing resources to train segments of horizontally scaled ML models. Central server evaluates contributions using parameter count and loss reduction metrics, then conducts weighted lottery to select winning miner who receives digitally signed certificate as PoW substitute.

Result: System preserves blockchain integrity through digital signatures and SHA-256 hashing while converting mining energy expenditure into socially valuable computational work through ML model training.

Conclusion: Proposed approach addresses sustainability concerns by aligning security incentives with real-world computational progress, making cryptocurrency mining more energy-efficient and productive.

Abstract: Bitcoin's Proof of Work (PoW) mechanism, while central to achieving
decentralized consensus, has long been criticized for excessive energy use and
hardware inefficiencies \cite{devries2018bitcoin, truby2018decarbonizing}. This
paper introduces a hybrid architecture that replaces Bitcoin's traditional PoW
with a centralized, cloud-based collaborative training framework. In this
model, miners contribute computing resources to train segments of horizontally
scaled machine learning models on preprocessed datasets, ensuring privacy and
generating meaningful outputs \cite{li2017securing}. A central server evaluates
contributions using two metrics: number of parameters trained and reduction in
model loss during each cycle. At the end of every cycle, a weighted lottery
selects the winning miner, who receives a digitally signed certificate. This
certificate serves as a verifiable substitute for PoW and grants the right to
append a block to the blockchain \cite{nakamoto2008bitcoin}. By integrating
digital signatures and SHA-256 hashing \cite{nist2015sha}, the system preserves
blockchain integrity while redirecting energy toward productive computation.
The proposed approach addresses the sustainability concerns of traditional
mining by converting resource expenditure into socially valuable work, aligning
security incentives with real-world computational progress.

</details>


### [418] [Fortifying the Agentic Web: A Unified Zero-Trust Architecture Against Logic-layer Threats](https://arxiv.org/abs/2508.12259)
*Ken Huang,Yasir Mehmood,Hammad Atta,Jerry Huang,Muhammad Zeeshan Baig,Sree Bhargavi Balija*

Main category: cs.CR

TL;DR: A unified security architecture for Agentic Web using Zero-Trust IAM with DIDs, VCs, and innovative Trust Fabric components to provide provable security against LPCI attacks.


<details>
  <summary>Details</summary>
Motivation: To address security vulnerabilities in agent-based web systems, particularly LPCI (Low-Probability Compromise of Integrity) threats, by creating a comprehensive security framework for trustworthy agentic ecosystems.

Method: Developed a Zero-Trust IAM framework using Decentralized Identifiers (DIDs) and Verifiable Credentials (VCs) with Agent Name Service (ANS), Trust-Adaptive Runtime Environments (TARE), Causal Chain Auditing, and Dynamic Identity with Behavioral Attestation.

Result: The architecture provides provable security guarantees against LPCI attacks with bounded probability of success, demonstrating formal security analysis.

Conclusion: The proposed unified security architecture offers a comprehensive blueprint for building secure, resilient, and trustworthy agentic web ecosystems with formal security guarantees.

Abstract: This paper presents a Unified Security Architecture that fortifies the
Agentic Web through a Zero-Trust IAM framework. This architecture is built on a
foundation of rich, verifiable agent identities using Decentralized Identifiers
(DIDs) and Verifiable Credentials (VCs), with discovery managed by a
protocol-agnostic Agent Name Service (ANS). Security is operationalized through
a multi-layered Trust Fabric which introduces significant innovations,
including Trust-Adaptive Runtime Environments (TARE), Causal Chain Auditing,
and Dynamic Identity with Behavioral Attestation. By explicitly linking the
LPCI threat to these enhanced architectural countermeasures within a formal
security model, we propose a comprehensive and forward-looking blueprint for a
secure, resilient, and trustworthy agentic ecosystem. Our formal analysis
demonstrates that the proposed architecture provides provable security
guarantees against LPCI attacks with bounded probability of success.

</details>


### [419] [LumiMAS: A Comprehensive Framework for Real-Time Monitoring and Enhanced Observability in Multi-Agent Systems](https://arxiv.org/abs/2508.12412)
*Ron Solomon,Yarin Yerushalmi Levi,Lior Vaknin,Eran Aizikovich,Amit Baras,Etai Ohana,Amit Giloni,Shamik Bose,Chiara Picardi,Yuval Elovici,Asaf Shabtai*

Main category: cs.CR

TL;DR: LumiMAS is a novel multi-agent system observability framework that detects and explains system-wide failures in real-time through three-layer monitoring, anomaly detection, and root cause analysis.


<details>
  <summary>Details</summary>
Motivation: Existing MAS observability frameworks focus on individual agents and overlook system-wide failures, creating a gap in monitoring complex multi-agent systems with large language models.

Method: Three-layer framework: 1) Monitoring and logging layer tracks agent activities, 2) Anomaly detection layer identifies workflow anomalies in real-time, 3) Anomaly explanation layer performs classification and root cause analysis.

Result: Evaluated on seven MAS applications using two platforms with diverse failures including hallucination and bias scenarios. Demonstrated effectiveness in failure detection, classification, and root cause analysis.

Conclusion: LumiMAS successfully bridges the gap in MAS observability by providing comprehensive system-wide monitoring and failure analysis capabilities for complex multi-agent systems.

Abstract: The incorporation of large language models in multi-agent systems (MASs) has
the potential to significantly improve our ability to autonomously solve
complex problems. However, such systems introduce unique challenges in
monitoring, interpreting, and detecting system failures. Most existing MAS
observability frameworks focus on analyzing each individual agent separately,
overlooking failures associated with the entire MAS. To bridge this gap, we
propose LumiMAS, a novel MAS observability framework that incorporates advanced
analytics and monitoring techniques. The proposed framework consists of three
key components: a monitoring and logging layer, anomaly detection layer, and
anomaly explanation layer. LumiMAS's first layer monitors MAS executions,
creating detailed logs of the agents' activity. These logs serve as input to
the anomaly detection layer, which detects anomalies across the MAS workflow in
real time. Then, the anomaly explanation layer performs classification and root
cause analysis (RCA) of the detected anomalies. LumiMAS was evaluated on seven
different MAS applications, implemented using two popular MAS platforms, and a
diverse set of possible failures. The applications include two novel
failure-tailored applications that illustrate the effects of a hallucination or
bias on the MAS. The evaluation results demonstrate LumiMAS's effectiveness in
failure detection, classification, and RCA.

</details>


### [420] [A Robust Cross-Domain IDS using BiGRU-LSTM-Attention for Medical and Industrial IoT Security](https://arxiv.org/abs/2508.12470)
*Afrah Gueriani,Hamza Kheddar,Ahmed Cherif Mazari,Mohamed Chahine Ghanem*

Main category: cs.CR

TL;DR: Novel transformer-based IDS (BiGAT-ID) combining BiGRU, LSTM, and multi-head attention for IoT security, achieving 99%+ accuracy on medical and industrial IoT datasets with fast inference times.


<details>
  <summary>Details</summary>
Motivation: Address cybersecurity challenges in interconnected medical and industrial IoT environments where sensitive data, patient safety, and industrial operations face advanced cyber threats.

Method: Hybrid model architecture combining bidirectional gated recurrent units (BiGRU), long short-term memory (LSTM) networks, and multi-head attention (MHA) to capture bidirectional temporal dependencies and enhance contextual feature representation.

Result: Achieved 99.13% accuracy on CICIoMT2024 medical IoT dataset and 99.34% on EdgeIIoTset industrial IoT dataset, with inference times as low as 0.0002s (IoMT) and 0.0001s (IIoT) per instance, plus low false positive rates.

Conclusion: BiGAT-ID proves to be a reliable and efficient intrusion detection system suitable for real-world deployment in heterogeneous IoT environments due to its cross-domain robustness and exceptional runtime performance.

Abstract: The increased Internet of Medical Things IoMT and the Industrial Internet of
Things IIoT interconnectivity has introduced complex cybersecurity challenges,
exposing sensitive data, patient safety, and industrial operations to advanced
cyber threats. To mitigate these risks, this paper introduces a novel
transformer-based intrusion detection system IDS, termed BiGAT-ID a hybrid
model that combines bidirectional gated recurrent units BiGRU, long short-term
memory LSTM networks, and multi-head attention MHA. The proposed architecture
is designed to effectively capture bidirectional temporal dependencies, model
sequential patterns, and enhance contextual feature representation. Extensive
experiments on two benchmark datasets, CICIoMT2024 medical IoT and EdgeIIoTset
industrial IoT demonstrate the model's cross-domain robustness, achieving
detection accuracies of 99.13 percent and 99.34 percent, respectively.
Additionally, the model exhibits exceptional runtime efficiency, with inference
times as low as 0.0002 seconds per instance in IoMT and 0.0001 seconds in IIoT
scenarios. Coupled with a low false positive rate, BiGAT-ID proves to be a
reliable and efficient IDS for deployment in real-world heterogeneous IoT
environments

</details>


### [421] [Data-driven Trust Bootstrapping for Mobile Edge Computing-based Industrial IoT Services](https://arxiv.org/abs/2508.12560)
*Prabath Abeysekara,Hai Dong*

Main category: cs.CR

TL;DR: A data-driven, context-aware trust bootstrapping approach for IoT services in MEC-based IIoT systems that addresses limitations of existing methods through knowledge sharing across MEC environments.


<details>
  <summary>Details</summary>
Motivation: Existing trust bootstrapping approaches face limitations in MEC-based IIoT systems including lack of prolonged interaction opportunities, unreliable peer recommendations, and uneven context parameters across different MEC environments causing inconsistent trust evaluation.

Method: Proposes a data-driven and context-aware approach that enables knowledge sharing among different MEC environments within a MEC topology to address data sparsity and uneven trust environments.

Result: Comprehensive evaluation on two real-world datasets (adjusted to exhibit context-dependent trust information) affirmed the effectiveness and suitability of the approach for bootstrapping service trustworthiness in MEC-based IIoT systems.

Conclusion: The proposed approach effectively addresses key limitations of existing trust bootstrapping methods and is suitable for establishing service trustworthiness in MEC-based industrial IoT systems through context-aware data sharing across environments.

Abstract: We propose a data-driven and context-aware approach to bootstrap
trustworthiness of homogeneous Internet of Things (IoT) services in Mobile Edge
Computing (MEC) based industrial IoT (IIoT) systems. The proposed approach
addresses key limitations in adapting existing trust bootstrapping approaches
into MEC-based IIoT systems. These key limitations include, the lack of
opportunity for a service consumer to interact with a lesser-known service over
a prolonged period of time to get a robust measure of its trustworthiness,
inability of service consumers to consistently interact with their peers to
receive reliable recommendations of the trustworthiness of a lesser-known
service as well as the impact of uneven context parameters in different MEC
environments causing uneven trust environments for trust evaluation. In
addition, the proposed approach also tackles the problem of data sparsity via
enabling knowledge sharing among different MEC environments within a given MEC
topology. To verify the effectiveness of the proposed approach, we carried out
a comprehensive evaluation on two real-world datasets suitably adjusted to
exhibit the context-dependent trust information accumulated in MEC environments
within a given MEC topology. The experimental results affirmed the
effectiveness of our approach and its suitability to bootstrap trustworthiness
of services in MEC-based IIoT systems.

</details>


### [422] [Systematic Analysis of MCP Security](https://arxiv.org/abs/2508.12538)
*Yongjian Guo,Puzhuo Liu,Wanlun Ma,Zehang Deng,Xiaogang Zhu,Peng Di,Xi Xiao,Sheng Wen*

Main category: cs.CR

TL;DR: MCPLIB introduces a comprehensive attack library with 31 methods across 4 categories to address security vulnerabilities in Model Context Protocol (MCP) systems, revealing critical weaknesses in AI agent tool interactions.


<details>
  <summary>Details</summary>
Motivation: MCP enables AI agents to connect with external tools but introduces significant security vulnerabilities like Tool Poisoning Attacks, yet current research lacks comprehensive quantitative analysis of real-world threats.

Method: Developed MCP Attack Library (MCPLIB) categorizing 31 attack methods into four classifications: direct tool injection, indirect tool injection, malicious user attacks, and LLM inherent attacks, followed by quantitative efficacy analysis.

Result: Experiments revealed key vulnerabilities: agents' blind reliance on tool descriptions, sensitivity to file-based attacks, chain attacks exploiting shared context, and difficulty distinguishing external data from executable commands.

Conclusion: The work provides a foundational framework for MCP security with comprehensive attack taxonomy, unified attack framework, and empirical vulnerability analysis to support secure evolution of MCP ecosystems.

Abstract: The Model Context Protocol (MCP) has emerged as a universal standard that
enables AI agents to seamlessly connect with external tools, significantly
enhancing their functionality. However, while MCP brings notable benefits, it
also introduces significant vulnerabilities, such as Tool Poisoning Attacks
(TPA), where hidden malicious instructions exploit the sycophancy of large
language models (LLMs) to manipulate agent behavior. Despite these risks,
current academic research on MCP security remains limited, with most studies
focusing on narrow or qualitative analyses that fail to capture the diversity
of real-world threats. To address this gap, we present the MCP Attack Library
(MCPLIB), which categorizes and implements 31 distinct attack methods under
four key classifications: direct tool injection, indirect tool injection,
malicious user attacks, and LLM inherent attack. We further conduct a
quantitative analysis of the efficacy of each attack. Our experiments reveal
key insights into MCP vulnerabilities, including agents' blind reliance on tool
descriptions, sensitivity to file-based attacks, chain attacks exploiting
shared context, and difficulty distinguishing external data from executable
commands. These insights, validated through attack experiments, underscore the
urgency for robust defense strategies and informed MCP design. Our
contributions include 1) constructing a comprehensive MCP attack taxonomy, 2)
introducing a unified attack framework MCPLIB, and 3) conducting empirical
vulnerability analysis to enhance MCP security mechanisms. This work provides a
foundational framework, supporting the secure evolution of MCP ecosystems.

</details>


### [423] [Unlearning Comparator: A Visual Analytics System for Comparative Evaluation of Machine Unlearning Methods](https://arxiv.org/abs/2508.12730)
*Jaeung Lee,Suhyeon Yu,Yurim Jang,Simon S. Woo,Jaemin Jo*

Main category: cs.CR

TL;DR: A visual analytics system called Unlearning Comparator is introduced to systematically evaluate Machine Unlearning methods by enabling model comparison at multiple levels and simulating privacy attacks.


<details>
  <summary>Details</summary>
Motivation: Researchers face challenges in analyzing Machine Unlearning methods due to reliance on aggregate metrics and ad-hoc evaluations, making it difficult to assess trade-offs between accuracy, efficiency, and privacy.

Method: Developed a visual analytics system that supports two main tasks: model comparison (class-, instance-, and layer-level analysis) and membership inference attack simulation to evaluate privacy.

Result: The system helps users understand model behaviors and gain insights for improving Machine Unlearning methods, as demonstrated through case studies analyzing prominent MU methods.

Conclusion: Unlearning Comparator provides a systematic framework for evaluating Machine Unlearning methods, addressing the need for better analysis tools in this emerging field to fulfill data privacy obligations.

Abstract: Machine Unlearning (MU) aims to remove target training data from a trained
model so that the removed data no longer influences the model's behavior,
fulfilling "right to be forgotten" obligations under data privacy laws. Yet, we
observe that researchers in this rapidly emerging field face challenges in
analyzing and understanding the behavior of different MU methods, especially in
terms of three fundamental principles in MU: accuracy, efficiency, and privacy.
Consequently, they often rely on aggregate metrics and ad-hoc evaluations,
making it difficult to accurately assess the trade-offs between methods. To
fill this gap, we introduce a visual analytics system, Unlearning Comparator,
designed to facilitate the systematic evaluation of MU methods. Our system
supports two important tasks in the evaluation process: model comparison and
attack simulation. First, it allows the user to compare the behaviors of two
models, such as a model generated by a certain method and a retrained baseline,
at class-, instance-, and layer-levels to better understand the changes made
after unlearning. Second, our system simulates membership inference attacks
(MIAs) to evaluate the privacy of a method, where an attacker attempts to
determine whether specific data samples were part of the original training set.
We evaluate our system through a case study visually analyzing prominent MU
methods and demonstrate that it helps the user not only understand model
behaviors but also gain insights that can inform the improvement of MU methods.

</details>


### [424] [Efficient and Verifiable Privacy-Preserving Convolutional Computation for CNN Inference with Untrusted Clouds](https://arxiv.org/abs/2508.12832)
*Jinyu Lu,Xinrong Sun,Yunting Tao,Tong Ji,Fanyu Kong,Guoqiang Yang*

Main category: cs.CR

TL;DR: A verifiable privacy-preserving scheme for CNN convolutional layers that achieves 26-87x speedup while maintaining accuracy and providing result verification.


<details>
  <summary>Details</summary>
Motivation: MLaaS systems using CNNs in resource-constrained environments face privacy leakage risks when sending sensitive data to untrusted cloud servers, and existing privacy-preserving schemes suffer from efficiency bottlenecks in convolution operations.

Method: Proposes a novel verifiable privacy-preserving scheme tailored for CNN convolutional layers with efficient encryption/decryption for resource-constrained clients and a verification mechanism to detect result correctness.

Result: Achieves speedups of 26-87x compared to original plaintext models while maintaining accuracy, with verification capable of detecting correctness with success probability of at least 1-1/|Z| across 10 datasets and various CNN models.

Conclusion: The scheme enables secure and efficient offloading of CNN computations to untrusted cloud servers while preserving privacy and providing verifiable results, making it suitable for resource-constrained MLaaS scenarios.

Abstract: The widespread adoption of convolutional neural networks (CNNs) in
resource-constrained scenarios has driven the development of Machine Learning
as a Service (MLaaS) system. However, this approach is susceptible to privacy
leakage, as the data sent from the client to the untrusted cloud server often
contains sensitive information. Existing CNN privacy-preserving schemes, while
effective in ensuring data confidentiality through homomorphic encryption and
secret sharing, face efficiency bottlenecks, particularly in convolution
operations. In this paper, we propose a novel verifiable privacy-preserving
scheme tailored for CNN convolutional layers. Our scheme enables efficient
encryption and decryption, allowing resource-constrained clients to securely
offload computations to the untrusted cloud server. Additionally, we present a
verification mechanism capable of detecting the correctness of the results with
a success probability of at least $1-\frac{1}{\left|Z\right|}$. Extensive
experiments conducted on 10 datasets and various CNN models demonstrate that
our scheme achieves speedups ranging $26 \times$ ~ $\ 87\times$ compared to the
original plaintext model while maintaining accuracy.

</details>


### [425] [SecFSM: Knowledge Graph-Guided Verilog Code Generation for Secure Finite State Machines in Systems-on-Chip](https://arxiv.org/abs/2508.12910)
*Ziteng Hu,Yingjie Xia,Xiyuan Chen,Li Kuang*

Main category: cs.CR

TL;DR: SecFSM is a novel method that uses a security-oriented knowledge graph to guide LLMs in generating secure Verilog code for Finite State Machines, achieving a 84% pass rate on security tests.


<details>
  <summary>Details</summary>
Motivation: Traditional manual Verilog coding for FSMs is tedious and time-consuming, while LLM-generated code often contains security vulnerabilities that are particularly concerning for security-sensitive FSM implementations.

Method: Constructs a FSM Security Knowledge Graph (FSKG) as external aid, analyzes user requirements to identify vulnerabilities, retrieves security knowledge from FSKG, and constructs security prompts for LLM-based Verilog code generation.

Result: Outperforms state-of-the-art baselines, achieving an outstanding 21/25 (84%) pass rate on security test cases evaluated by DeepSeek-R1 benchmark.

Conclusion: SecFSM effectively addresses security vulnerabilities in LLM-generated Verilog code for FSMs by leveraging security knowledge graphs to guide the generation process, making it suitable for security-sensitive SoC implementations.

Abstract: Finite State Machines (FSMs) play a critical role in implementing control
logic for Systems-on-Chip (SoC). Traditionally, FSMs are implemented by
hardware engineers through Verilog coding, which is often tedious and
time-consuming. Recently, with the remarkable progress of Large Language Models
(LLMs) in code generation, LLMs have been increasingly explored for automating
Verilog code generation. However, LLM-generated Verilog code often suffers from
security vulnerabilities, which is particularly concerning for
security-sensitive FSM implementations. To address this issue, we propose
SecFSM, a novel method that leverages a security-oriented knowledge graph to
guide LLMs in generating more secure Verilog code. Specifically, we first
construct a FSM Security Knowledge Graph (FSKG) as an external aid to LLMs.
Subsequently, we analyze users' requirements to identify vulnerabilities and
get a list of vulnerabilities in the requirements. Then, we retrieve knowledge
from FSKG based on the vulnerabilities list. Finally, we construct security
prompts based on the security knowledge for Verilog code generation. To
evaluate SecFSM, we build a dedicated dataset collected from academic datasets,
artificial datasets, papers, and industrial cases. Extensive experiments
demonstrate that SecFSM outperforms state-of-the-art baselines. In particular,
on a benchmark of 25 security test cases evaluated by DeepSeek-R1, SecFSM
achieves an outstanding pass rate of 21/25.

</details>


### [426] [VerilogLAVD: LLM-Aided Rule Generation for Vulnerability Detection in Verilog](https://arxiv.org/abs/2508.13092)
*Xiang Long,Yingjie Xia,Xiyuan Chen,Li Kuang*

Main category: cs.CR

TL;DR: VerilogLAVD is an LLM-aided graph traversal approach that uses a unified Verilog Property Graph representation to detect hardware vulnerabilities in Verilog code, achieving significant improvement over LLM-only baselines.


<details>
  <summary>Details</summary>
Motivation: Timely detection of hardware vulnerabilities during early design stages is critical for reducing remediation costs, but existing techniques require specialized security expertise and LLMs struggle with Verilog code structure.

Method: Proposes Verilog Property Graph (VeriPG) combining syntactic features from AST with semantic information from control flow and data dependency graphs. Uses LLMs to generate detection rules from CWE descriptions that guide traversal of VeriPG.

Result: Achieves F1-score of 0.54 on 77 Verilog designs covering 12 CWE types, improving F1-score by 0.31 and 0.27 compared to LLM-only and LLM with external knowledge baselines respectively.

Conclusion: VerilogLAVD effectively combines LLMs with structured graph representations to overcome limitations of pure LLM approaches for hardware vulnerability detection, providing a more reliable solution for early-stage security analysis.

Abstract: Timely detection of hardware vulnerabilities during the early design stage is
critical for reducing remediation costs. Existing early detection techniques
often require specialized security expertise, limiting their usability. Recent
efforts have explored the use of large language models (LLMs) for Verilog
vulnerability detection. However, LLMs struggle to capture the structure in
Verilog code, resulting in inconsistent detection results. To this end, we
propose VerilogLAVD, the first LLM-aided graph traversal rule generation
approach for Verilog vulnerability detection. Our approach introduces the
Verilog Property Graph (VeriPG), a unified representation of Verilog code. It
combines syntactic features extracted from the abstract syntax tree (AST) with
semantic information derived from control flow and data dependency graphs. We
leverage LLMs to generate VeriPG-based detection rules from Common Weakness
Enumeration (CWE) descriptions. These rules guide the rule executor that
traversal VeriPG for potential vulnerabilities. To evaluate VerilogLAVD, we
build a dataset collected from open-source repositories and synthesized data.
In our empirical evaluation on 77 Verilog designs encompassing 12 CWE types,
VerilogLAVD achieves an F1-score of 0.54. Compared to the LLM-only and LLM with
external knowledge baselines, VerilogLAVD improves F1-score by 0.31 and 0.27,
respectively.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [427] [RRRA: Resampling and Reranking through a Retriever Adapter](https://arxiv.org/abs/2508.11670)
*Bongsu Kim*

Main category: cs.IR

TL;DR: A learnable adapter module that dynamically identifies false negatives in dense retrieval training, improving performance through resampling and reranking.


<details>
  <summary>Details</summary>
Motivation: Current hard negative selection methods use global heuristics that often miss instance-specific false negatives, limiting training effectiveness.

Method: Proposes a learnable adapter that monitors Bi-Encoder representations to estimate query-specific false negative probabilities, used for resampling during training and reranking at inference.

Result: Empirical results show the adapter-enhanced framework consistently outperforms strong Bi-Encoder baselines on standard benchmarks.

Conclusion: Explicit false negative modeling through contextual probability estimation significantly benefits dense retrieval performance.

Abstract: In dense retrieval, effective training hinges on selecting high quality hard
negatives while avoiding false negatives. Recent methods apply heuristics based
on positive document scores to identify hard negatives, improving both
performance and interpretability. However, these global, example agnostic
strategies often miss instance specific false negatives. To address this, we
propose a learnable adapter module that monitors Bi-Encoder representations to
estimate the likelihood that a hard negative is actually a false negative. This
probability is modeled dynamically and contextually, enabling fine-grained,
query specific judgments. The predicted scores are used in two downstream
components: (1) resampling, where negatives are reweighted during training, and
(2) reranking, where top-k retrieved documents are reordered at inference.
Empirical results on standard benchmarks show that our adapter-enhanced
framework consistently outperforms strong Bi-Encoder baselines, underscoring
the benefit of explicit false negative modeling in dense retrieval.

</details>


### [428] [LLM-Based Intelligent Agents for Music Recommendation: A Comparison with Classical Content-Based Filtering](https://arxiv.org/abs/2508.11671)
*Ronald Carvalho Boadana,Ademir Guimarães da Costa Junior,Ricardo Rios,Fábio Santos da Silva*

Main category: cs.IR

TL;DR: LLMs from Gemini and LLaMA families combined with intelligent agents outperform traditional content-based models in music recommendation, achieving 89.32% user satisfaction.


<details>
  <summary>Details</summary>
Motivation: Address information overload on music streaming platforms and enhance user experience through sophisticated recommendation systems.

Method: Multi-agent personalized music recommendation system using Large Language Models (Gemini and LLaMA families) combined with intelligent agents, compared against traditional content-based recommendation model.

Result: LLMs achieved satisfaction rates of up to 89.32%, demonstrating superior performance in user satisfaction, novelty, and computational efficiency compared to traditional content-based models.

Conclusion: LLMs show promising potential for music recommendation systems, offering significant improvements over traditional approaches.

Abstract: The growing availability of music on streaming platforms has led to
information overload for users. To address this issue and enhance the user
experience, increasingly sophisticated recommendation systems have been
proposed. This work investigates the use of Large Language Models (LLMs) from
the Gemini and LLaMA families, combined with intelligent agents, in a
multi-agent personalized music recommendation system. The results are compared
with a traditional content-based recommendation model, considering user
satisfaction, novelty, and computational efficiency. LLMs achieved satisfaction
rates of up to \textit{89{,}32\%}, indicating their promising potential in
music recommendation systems.

</details>


### [429] [TBGRecall: A Generative Retrieval Model for E-commerce Recommendation Scenarios](https://arxiv.org/abs/2508.11977)
*Zida Liang,Changfa Wu,Dunxian Huang,Weiqiang Sun,Ziyang Wang,Yuliang Yan,Jian Wu,Yuning Jiang,Bo Zheng,Ke Chen,Silu Zhou,Yu Zhang*

Main category: cs.IR

TL;DR: TBGRecall framework integrates Next Session Prediction (NSP) to enhance generative retrieval models for e-commerce recommendations by addressing limitations of autoregressive generation in multi-item retrieval tasks.


<details>
  <summary>Details</summary>
Motivation: Current generative models for recommendation systems struggle with efficient retrieval due to sequential dependencies and autoregressive generation mechanisms that are unsuitable for generating multiple items without positional constraints in single sessions.

Method: Proposes TBGRecall framework that partitions input samples into multi-session sequences (session token + item tokens), incorporates multiple optimizations for generative retrieval, and uses limited historical data pre-training with stochastic partial incremental training to emphasize data recency over volume.

Result: Extensive experiments on public benchmarks and TaoBao industrial dataset show TBGRecall outperforms state-of-the-art recommendation methods and exhibits clear scaling law trends.

Conclusion: NSP represents a significant advancement in generative recommendation systems for e-commerce applications, improving retrieval efficiency and effectiveness.

Abstract: Recommendation systems are essential tools in modern e-commerce, facilitating
personalized user experiences by suggesting relevant products. Recent
advancements in generative models have demonstrated potential in enhancing
recommendation systems; however, these models often exhibit limitations in
optimizing retrieval tasks, primarily due to their reliance on autoregressive
generation mechanisms. Conventional approaches introduce sequential
dependencies that impede efficient retrieval, as they are inherently unsuitable
for generating multiple items without positional constraints within a single
request session. To address these limitations, we propose TBGRecall, a
framework integrating Next Session Prediction (NSP), designed to enhance
generative retrieval models for e-commerce applications. Our framework
reformulation involves partitioning input samples into multi-session sequences,
where each sequence comprises a session token followed by a set of item tokens,
and then further incorporate multiple optimizations tailored to the generative
task in retrieval scenarios. In terms of training methodology, our pipeline
integrates limited historical data pre-training with stochastic partial
incremental training, significantly improving training efficiency and
emphasizing the superiority of data recency over sheer data volume. Our
extensive experiments, conducted on public benchmarks alongside a large-scale
industrial dataset from TaoBao, show TBGRecall outperforms the state-of-the-art
recommendation methods, and exhibits a clear scaling law trend. Ultimately, NSP
represents a significant advancement in the effectiveness of generative
recommendation systems for e-commerce applications.

</details>


### [430] [Ontology-Guided Query Expansion for Biomedical Document Retrieval using Large Language Models](https://arxiv.org/abs/2508.11784)
*Zabir Al Nazi,Vagelis Hristidis,Aaron Lawson McLean,Jannat Ara Meem,Md Taukir Azam Chowdhury*

Main category: cs.IR

TL;DR: BMQExpander is an ontology-aware query expansion pipeline that combines UMLS medical knowledge with LLMs to improve biomedical document retrieval, achieving up to 22.1% improvement over sparse baselines and demonstrating robust generalization.


<details>
  <summary>Details</summary>
Motivation: Biomedical QA requires effective document retrieval, which is challenging due to domain-specific vocabulary and semantic ambiguity in user queries.

Method: Proposes BMQExpander - a pipeline combining UMLS Metathesaurus medical knowledge (definitions and relationships) with LLM generative capabilities for query expansion. Implemented various baselines including sparse/dense retrievers and biomedical-specific solutions.

Result: Superior performance on NFCorpus, TREC-COVID, and SciFact benchmarks: up to 22.1% improvement in NDCG@10 over sparse baselines and 6.5% over strongest baseline. Robust generalization under query perturbation (15.7% improvement). Fewer hallucinations compared to other LLM-based methods.

Conclusion: BMQExpander effectively enhances biomedical retrieval by combining structured medical knowledge with LLMs, demonstrating significant performance improvements and better generalization than supervised approaches.

Abstract: Effective Question Answering (QA) on large biomedical document collections
requires effective document retrieval techniques. The latter remains a
challenging task due to the domain-specific vocabulary and semantic ambiguity
in user queries. We propose BMQExpander, a novel ontology-aware query expansion
pipeline that combines medical knowledge - definitions and relationships - from
the UMLS Metathesaurus with the generative capabilities of large language
models (LLMs) to enhance retrieval effectiveness. We implemented several
state-of-the-art baselines, including sparse and dense retrievers, query
expansion methods, and biomedical-specific solutions. We show that BMQExpander
has superior retrieval performance on three popular biomedical Information
Retrieval (IR) benchmarks: NFCorpus, TREC-COVID, and SciFact - with
improvements of up to 22.1% in NDCG@10 over sparse baselines and up to 6.5%
over the strongest baseline. Further, BMQExpander generalizes robustly under
query perturbation settings, in contrast to supervised baselines, achieving up
to 15.7% improvement over the strongest baseline. As a side contribution, we
publish our paraphrased benchmarks. Finally, our qualitative analysis shows
that BMQExpander has fewer hallucinations compared to other LLM-based query
expansion baselines.

</details>


### [431] [Leveraging Geometric Insights in Hyperbolic Triplet Loss for Improved Recommendations](https://arxiv.org/abs/2508.11978)
*Viacheslav Yusupov,Maxim Rakhuba,Evgeny Frolov*

Main category: cs.IR

TL;DR: Novel hyperbolic recommendation model that improves representation learning and computational stability through geometric insights and triplet loss formulation.


<details>
  <summary>Details</summary>
Motivation: To leverage hyperbolic geometry's potential for capturing complex patterns in interaction data and overcome limitations of conventional Euclidean space in recommender systems.

Method: Reformulates hyperbolic distances to increase representation capacity, constructs triplet loss with pairwise interaction terms driven by data geometry to model ternary relations between users and their preferred/nonpreferred choices.

Result: Outperforms existing Euclidean and hyperbolic models while reducing popularity bias, leading to more diverse and personalized recommendations.

Conclusion: Hyperbolic geometry provides superior representation learning capabilities for recommender systems, enabling both better performance and reduced bias compared to traditional approaches.

Abstract: Recent studies have demonstrated the potential of hyperbolic geometry for
capturing complex patterns from interaction data in recommender systems. In
this work, we introduce a novel hyperbolic recommendation model that uses
geometrical insights to improve representation learning and increase
computational stability at the same time. We reformulate the notion of
hyperbolic distances to unlock additional representation capacity over
conventional Euclidean space and learn more expressive user and item
representations. To better capture user-items interactions, we construct a
triplet loss that models ternary relations between users and their
corresponding preferred and nonpreferred choices through a mix of pairwise
interaction terms driven by the geometry of data. Our hyperbolic approach not
only outperforms existing Euclidean and hyperbolic models but also reduces
popularity bias, leading to more diverse and personalized recommendations.

</details>


### [432] [A Large-Scale Web Search Dataset for Federated Online Learning to Rank](https://arxiv.org/abs/2508.12353)
*Marcel Gregoriadis,Jingwei Kang,Johan Pouwelse*

Main category: cs.IR

TL;DR: AOL4FOLTR is a new large-scale web search dataset designed for realistic Federated Online Learning to Rank (FOLTR) benchmarking, addressing privacy concerns and limitations of existing synthetic benchmarks.


<details>
  <summary>Details</summary>
Motivation: Current FOLTR benchmarks use random dataset partitioning, simulated clicks, and assume synchronous client participation, which oversimplifies real-world dynamics and undermines experimental realism.

Method: Created AOL4FOLTR dataset with 2.6 million queries from 10,000 users, including user identifiers, real click data, and query timestamps to enable realistic user partitioning, behavior modeling, and asynchronous federated learning scenarios.

Result: The dataset provides a more realistic foundation for FOLTR research by capturing authentic user behavior patterns and enabling asynchronous learning scenarios that reflect real-world deployment conditions.

Conclusion: AOL4FOLTR addresses key limitations of existing FOLTR benchmarks and provides a more realistic dataset for evaluating privacy-preserving federated learning approaches in search ranking systems.

Abstract: The centralized collection of search interaction logs for training ranking
models raises significant privacy concerns. Federated Online Learning to Rank
(FOLTR) offers a privacy-preserving alternative by enabling collaborative model
training without sharing raw user data. However, benchmarks in FOLTR are
largely based on random partitioning of classical learning-to-rank datasets,
simulated user clicks, and the assumption of synchronous client participation.
This oversimplifies real-world dynamics and undermines the realism of
experimental results. We present AOL4FOLTR, a large-scale web search dataset
with 2.6 million queries from 10,000 users. Our dataset addresses key
limitations of existing benchmarks by including user identifiers, real click
data, and query timestamps, enabling realistic user partitioning, behavior
modeling, and asynchronous federated learning scenarios.

</details>


### [433] [Asymmetric Diffusion Recommendation Model](https://arxiv.org/abs/2508.12706)
*Yongchun Zhu,Guanyu Jiang,Jingwu Chen,Feng Zhang,Xiao Yang,Zuotao Liu*

Main category: cs.IR

TL;DR: AsymDiffRec is an asymmetric diffusion model for recommendation systems that handles discrete data spaces and preserves personalized information through task-oriented optimization, achieving significant improvements in user engagement metrics.


<details>
  <summary>Details</summary>
Motivation: Standard diffusion models use Gaussian noise in continuous spaces, but recommendation data is discrete and Gaussian noise can corrupt personalized information in latent representations.

Method: Asymmetric forward and reverse processes where forward process simulates missing features and reverse process operates in asymmetric latent space with task-oriented optimization to preserve personalized information.

Result: Online A/B tests showed +0.131% improvement in users' active days and +0.166% improvement in app usage duration. Offline experiments also demonstrated improvements.

Conclusion: AsymDiffRec effectively addresses the discrete nature of recommendation data and preserves personalized information, leading to significant performance gains in real-world deployment on Douyin Music App.

Abstract: Recently, motivated by the outstanding achievements of diffusion models, the
diffusion process has been employed to strengthen representation learning in
recommendation systems. Most diffusion-based recommendation models typically
utilize standard Gaussian noise in symmetric forward and reverse processes in
continuous data space. Nevertheless, the samples derived from recommendation
systems inhabit a discrete data space, which is fundamentally different from
the continuous one. Moreover, Gaussian noise has the potential to corrupt
personalized information within latent representations. In this work, we
propose a novel and effective method, named Asymmetric Diffusion Recommendation
Model (AsymDiffRec), which learns forward and reverse processes in an
asymmetric manner. We define a generalized forward process that simulates the
missing features in real-world recommendation samples. The reverse process is
then performed in an asymmetric latent feature space. To preserve personalized
information within the latent representation, a task-oriented optimization
strategy is introduced. In the serving stage, the raw sample with missing
features is regarded as a noisy input to generate a denoising and robust
representation for the final prediction. By equipping base models with
AsymDiffRec, we conduct online A/B tests, achieving improvements of +0.131% and
+0.166% in terms of users' active days and app usage duration respectively.
Additionally, the extended offline experiments also demonstrate improvements.
AsymDiffRec has been implemented in the Douyin Music App.

</details>


### [434] [Is This News Still Interesting to You?: Lifetime-aware Interest Matching for News Recommendation](https://arxiv.org/abs/2508.13064)
*Seongeun Ryu,Yunyong Ko,Sang-Wook Kim*

Main category: cs.IR

TL;DR: LIME is a news recommendation framework that addresses time-related challenges by modeling news lifetime and interest persistence through user-topic aware age representation, candidate-aware attention, and freshness-guided refinement.


<details>
  <summary>Details</summary>
Motivation: Existing news recommendation systems underutilize time-related factors like how long users maintain interest in topics and how news lifespan varies across topics and users, leaving room for improvement in temporal interest matching.

Method: Proposes LIME framework with three strategies: 1) User-Topic lifetime-aware age representation to capture relative news age, 2) Candidate-aware lifetime attention for temporally aligned user representation, and 3) Freshness-guided interest refinement to prioritize valid news.

Result: Extensive experiments on two real-world datasets show LIME consistently outperforms state-of-the-art news recommendation methods and its model-agnostic strategies significantly improve recommendation accuracy.

Conclusion: LIME effectively addresses temporal challenges in news recommendation by modeling interest persistence and news lifetime variations, demonstrating superior performance and general applicability across different recommendation models.

Abstract: Personalized news recommendation aims to deliver news articles aligned with
users' interests, serving as a key solution to alleviate the problem of
information overload on online news platforms. While prior work has improved
interest matching through refined representations of news and users, the
following time-related challenges remain underexplored: (C1) leveraging the age
of clicked news to infer users' interest persistence, and (C2) modeling the
varying lifetime of news across topics and users. To jointly address these
challenges, we propose a novel Lifetime-aware Interest Matching framework for
nEws recommendation, named LIME, which incorporates three key strategies: (1)
User-Topic lifetime-aware age representation to capture the relative age of
news with respect to a user-topic pair, (2) Candidate-aware lifetime attention
for generating temporally aligned user representation, and (3) Freshness-guided
interest refinement for prioritizing valid candidate news at prediction time.
Extensive experiments on two real-world datasets demonstrate that LIME
consistently outperforms a wide range of state-of-the-art news recommendation
methods, and its model agnostic strategies significantly improve recommendation
accuracy.

</details>


<div id='physics.ao-ph'></div>

# physics.ao-ph [[Back]](#toc)

### [435] [Exploring Multimodal AI Reasoning for Meteorological Forecasting from Skew-T Diagrams](https://arxiv.org/abs/2508.12198)
*ChangJae Lee,Heecheol Yang,Jonghak Choi*

Main category: physics.ao-ph

TL;DR: A lightweight AI assistant using small vision-language models achieves operational-level skill in precipitation forecasting from Skew-T diagrams through visual grounding and chain-of-thought reasoning.


<details>
  <summary>Details</summary>
Motivation: To develop an efficient AI system that can interpret meteorological diagrams like human forecasters, as current Vision-Language Models haven't been well explored for meteorological diagram interpretation despite their success in other scientific domains.

Method: Fine-tuned a small language model and vision-language model using curriculum learning: first trained on visual question answering to identify atmospheric features from Skew-T diagrams, then on chain-of-thought reasoning tasks for precipitation probability estimation. Used textual summaries or generated diagrams from NWP forecasts paired with precipitation observations.

Result: The fine-tuned VLM achieved skill comparable to operational NWP models using only static atmospheric profiles. Visual grounding and reasoning supervision were critical for performance, and attention maps showed the model learned to focus on relevant meteorological features.

Conclusion: Compact multimodal models show strong potential for weather forecasting tasks, offering computationally efficient alternatives to large systems. The approach could be extended to more complex applications in the future.

Abstract: Forecasting from atmospheric soundings is a fundamental task in operational
meteorology, often requiring structured visual reasoning over Skew-T log-P
diagrams by human forecasters. While recent advances in Vision-Language Models
(VLMs) have shown promise in other scientific domains, their application to
meteorological diagram interpretation remains largely unexplored. In this
study, we present a lightweight AI assistant that interprets Skew-T diagrams
using a small language model (LM) and a small VLM fine-tuned to emulate human
forecasters. Using a curriculum learning framework, we first train the models
to identify key atmospheric features from diagrams through visual question
answering, followed by chain-of-thought reasoning tasks that estimate
precipitation probability based on the derived visual groundings. Model inputs
include either textual summaries or generated Skew-T diagrams derived from
operational Numerical Weather Prediction (NWP) forecasts, paired with
three-hour precipitation observations from South Korea's Auto Weather Stations
network. Evaluation results demonstrate that the fine-tuned VLM achieves skill
comparable to an operational NWP model, despite relying solely on static
atmospheric profiles. Ablation studies reveal that visual grounding and
reasoning supervision are critical for performance, while attention map
analysis confirms that the model learns to focus on relevant meteorological
features. These findings highlight the potential of compact, interpretable
multimodal models to support weather forecasting tasks. The approach offers a
computationally efficient alternative to large-scale systems, and future work
could extend it to more complex applications.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [436] [Tightening the mixed integer linear formulation for the piecewise linear approximation in general dimensions](https://arxiv.org/abs/2508.09395)
*Quentin Ploussard,Xiang Li,Matija Pavičević*

Main category: math.OC

TL;DR: This paper presents methods to tighten MILP formulations for continuous piecewise linear approximations using difference-of-convex representations and well-behaved interpolations, showing significant improvements in solution times.


<details>
  <summary>Details</summary>
Motivation: To improve the efficiency of solving mixed-integer linear programming problems for continuous piecewise linear approximations by tightening the formulation through better mathematical representations and constraints.

Method: Introduces well-behaved CPWL interpolations concept and six tightening strategies including variable fixing, additional constraints, optimized big-M parameters, and tighter variable bounds leveraging DC representation properties.

Result: Experimental results show that specific combinations of tightening strategies lead to significant improvements in solution times, particularly those considering well-behaved CPWL solutions.

Conclusion: The proposed tightening methods based on well-behaved CPWL interpolations and DC representations effectively improve MILP solution efficiency for piecewise linear approximation problems.

Abstract: This paper addresses the problem of tightening the mixed-integer linear
programming (MILP) formulation for continuous piecewise linear (CPWL)
approximations of data sets in arbitrary dimensions. The MILP formulation
leverages the difference-of-convex (DC) representation of CPWL functions. We
introduce the concept of well-behaved CPWL interpolations and demonstrate that
any CPWL interpolation of a data set has a well-behaved version. This result is
critical to tighten the MILP problem. We present six different strategies to
tighten the problem, which include fixing the values of some variables,
introducing additional constraints, identifying small big-M parameter values
and applying tighter variable bounds. These methods leverage key aspects of the
DC representation and the inherent structure of well-behaved CPWL
interpolations. Experimental results demonstrate that specific combinations of
these tightening strategies lead to significant improvement in solution times,
especially for tightening strategies that consider well-behaved CPWL solutions.

</details>


### [437] [EXOTIC: An Exact, Optimistic, Tree-Based Algorithm for Min-Max Optimization](https://arxiv.org/abs/2508.12479)
*Chinmay Maheshwari,Chinmay Pimpalkhare,Debasish Chatterjee*

Main category: math.OC

TL;DR: EXOTIC algorithm for globally optimal solutions in convex-non-concave and non-convex-concave min-max optimization problems, outperforming gradient-based methods with theoretical guarantees and practical applications in multi-player games.


<details>
  <summary>Details</summary>
Motivation: Gradient-based methods for min-max optimization often fail to find global optima beyond convex-concave settings, leading to arbitrarily suboptimal solutions in convex-non-concave and non-convex-concave problems common in game theory and adversarial ML.

Method: Reformulate convex-non-concave min-max as non-concave-convex max-min problem, then use EXOTIC algorithm combining iterative convex optimization for inner minimization and hierarchical tree search for outer maximization with optimistic region selection.

Result: Established theoretical upper bound on optimality gap, outperformed gradient-based methods on new benchmark problems and existing literature benchmarks, successfully computed security strategies in multi-player (3+ players) games.

Conclusion: EXOTIC provides an effective algorithmic framework with theoretical guarantees for globally optimal solutions in challenging min-max optimization problems where gradient methods fail, with practical utility in complex game scenarios.

Abstract: Min-max optimization arises in many domains such as game theory, adversarial
machine learning, etc., with gradient-based methods as a typical computational
tool. Beyond convex-concave min-max optimization, the solutions found by
gradient-based methods may be arbitrarily far from global optima. In this work,
we present an algorithmic apparatus for computing globally optimal solutions in
convex-non-concave and non-convex-concave min-max optimization. For former, we
employ a reformulation that transforms it into a non-concave-convex max-min
optimization problem with suitably defined feasible sets and objective
function. The new form can be viewed as a generalization of Sion's minimax
theorem. Next, we introduce EXOTIC-an Exact, Optimistic, Tree-based algorithm
for solving the reformulated max-min problem. EXOTIC employs an iterative
convex optimization solver to (approximately) solve the inner minimization and
a hierarchical tree search for the outer maximization to optimistically select
promising regions to search based on the approximate solution returned by
convex optimization solver. We establish an upper bound on its optimality gap
as a function of the number of calls to the inner solver, the solver's
convergence rate, and additional problem-dependent parameters. Both our
algorithmic apparatus along with its accompanying theoretical analysis can also
be applied for non-convex-concave min-max optimization. In addition, we propose
a class of benchmark convex-non-concave min-max problems along with their
analytical global solutions, providing a testbed for evaluating algorithms for
min-max optimization. Empirically, EXOTIC outperforms gradient-based methods on
this benchmark as well as on existing numerical benchmark problems from the
literature. Finally, we demonstrate the utility of EXOTIC by computing security
strategies in multi-player games with three or more players.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [438] [Revealing Neurocognitive and Behavioral Patterns by Unsupervised Manifold Learning from Dynamic Brain Data](https://arxiv.org/abs/2508.11672)
*Zixia Zhou,Junyan Liu,Wei Emma Wu,Ruogu Fang,Sheng Liu,Qingyue Wei,Rui Yan,Yi Guo,Qian Tao,Yuanyuan Wang,Md Tauhidul Islam,Lei Xing*

Main category: q-bio.NC

TL;DR: BCNE is an unsupervised deep manifold learning method that captures brain-state trajectories by analyzing temporospatial correlations in dynamic brain data, revealing interpretable neurocognitive and behavioral patterns.


<details>
  <summary>Details</summary>
Motivation: Dynamic brain data contains valuable biological and functional insights but is challenging to analyze due to its vast size and complexity. Existing methods extract patterns directly from input data, which may not effectively capture the underlying brain-state trajectories.

Method: Brain-dynamic Convolutional-Network-based Embedding (BCNE) deciphers temporospatial correlations within brain data and applies manifold learning to this correlative representation to capture brain-state trajectories.

Result: BCNE successfully identifies scene transitions, reveals brain region involvement in memory and narrative processing, distinguishes dynamic learning stages, and differentiates active vs. passive behaviors through both visual and quantitative analysis.

Conclusion: BCNE provides an effective unsupervised tool for exploring general neuroscience questions and individual-specific patterns in dynamic brain data by capturing meaningful brain-state trajectories through correlation-based manifold learning.

Abstract: Dynamic brain data, teeming with biological and functional insights, are
becoming increasingly accessible through advanced measurements, providing a
gateway to understanding the inner workings of the brain in living subjects.
However, the vast size and intricate complexity of the data also pose a
daunting challenge in reliably extracting meaningful information across various
data sources. This paper introduces a generalizable unsupervised deep manifold
learning for exploration of neurocognitive and behavioral patterns. Unlike
existing methods that extract patterns directly from the input data as in the
existing methods, the proposed Brain-dynamic Convolutional-Network-based
Embedding (BCNE) seeks to capture the brain-state trajectories by deciphering
the temporospatial correlations within the data and subsequently applying
manifold learning to this correlative representation. The performance of BCNE
is showcased through the analysis of several important dynamic brain datasets.
The results, both visual and quantitative, reveal a diverse array of intriguing
and interpretable patterns. BCNE effectively delineates scene transitions,
underscores the involvement of different brain regions in memory and narrative
processing, distinguishes various stages of dynamic learning processes, and
identifies differences between active and passive behaviors. BCNE provides an
effective tool for exploring general neuroscience inquiries or
individual-specific patterns.

</details>


### [439] [HetSyn: Versatile Timescale Integration in Spiking Neural Networks via Heterogeneous Synapses](https://arxiv.org/abs/2508.11644)
*Zhichao Deng,Zhikun Liu,Junxue Wang,Shengqian Chen,Xiang Wei,Qiang Yu*

Main category: q-bio.NC

TL;DR: HetSyn introduces synaptic heterogeneity with synapse-specific time constants to Spiking Neural Networks, improving performance across various tasks while maintaining biological plausibility.


<details>
  <summary>Details</summary>
Motivation: Existing SNN studies overlook synaptic heterogeneity, a fundamental biological property crucial for temporal processing and cognitive capabilities.

Method: Developed HetSyn framework with synapse-specific time constants, implemented as HetSynLIF - an extended LIF model that shifts temporal integration to synaptic current.

Result: HetSynLIF improves SNN performance across pattern generation, delayed match-to-sample, speech/visual recognition tasks, shows strong noise robustness, enhanced working memory, efficiency with limited neurons, and generalization across timescales.

Conclusion: Synaptic heterogeneity is significant for efficient neural computation, with learned time constants matching biological observations, offering new insights for brain-inspired temporal modeling.

Abstract: Spiking Neural Networks (SNNs) offer a biologically plausible and
energy-efficient framework for temporal information processing. However,
existing studies overlook a fundamental property widely observed in biological
neurons-synaptic heterogeneity, which plays a crucial role in temporal
processing and cognitive capabilities. To bridge this gap, we introduce HetSyn,
a generalized framework that models synaptic heterogeneity with
synapse-specific time constants. This design shifts temporal integration from
the membrane potential to the synaptic current, enabling versatile timescale
integration and allowing the model to capture diverse synaptic dynamics. We
implement HetSyn as HetSynLIF, an extended form of the leaky integrate-and-fire
(LIF) model equipped with synapse-specific decay dynamics. By adjusting the
parameter configuration, HetSynLIF can be specialized into vanilla LIF neurons,
neurons with threshold adaptation, and neuron-level heterogeneous models. We
demonstrate that HetSynLIF not only improves the performance of SNNs across a
variety of tasks-including pattern generation, delayed match-to-sample, speech
recognition, and visual recognition-but also exhibits strong robustness to
noise, enhanced working memory performance, efficiency under limited neuron
resources, and generalization across timescales. In addition, analysis of the
learned synaptic time constants reveals trends consistent with empirical
observations in biological synapses. These findings underscore the significance
of synaptic heterogeneity in enabling efficient neural computation, offering
new insights into brain-inspired temporal modeling.

</details>


### [440] [A Unified Cortical Circuit Model with Divisive Normalization and Self-Excitation for Robust Representation and Memory Maintenance](https://arxiv.org/abs/2508.12702)
*Jie Su,Weiwei Wang,Zhaotian Gu,Dahui Wang,Tianyi Qian*

Main category: q-bio.NC

TL;DR: A unified recurrent neural circuit combining divisive normalization and self-excitation achieves both robust encoding and stable memory retention, bridging noise suppression, working memory, and Bayesian inference.


<details>
  <summary>Details</summary>
Motivation: Existing models use separate neural mechanisms for noise-resistant processing and information maintenance, lacking a unified framework for understanding cortical computation.

Method: A recurrent neural circuit that combines divisive normalization with self-excitation, mathematically analyzed to form a continuous attractor with input-proportional stabilization and self-sustained memory states.

Result: The model demonstrates versatility in noise-robust encoding (random-dot kinematogram) and approximate Bayesian belief updating (probabilistic Wisconsin Card Sorting Test).

Conclusion: This work establishes a unified mathematical framework that bridges noise suppression, working memory, and approximate Bayesian inference within a single cortical microcircuit, offering insights into brain computation and guiding biologically plausible AI design.

Abstract: Robust information representation and its persistent maintenance are
fundamental for higher cognitive functions. Existing models employ distinct
neural mechanisms to separately address noise-resistant processing or
information maintenance, yet a unified framework integrating both operations
remains elusive -- a critical gap in understanding cortical computation. Here,
we introduce a recurrent neural circuit that combines divisive normalization
with self-excitation to achieve both robust encoding and stable retention of
normalized inputs. Mathematical analysis shows that, for suitable parameter
regimes, the system forms a continuous attractor with two key properties: (1)
input-proportional stabilization during stimulus presentation; and (2)
self-sustained memory states persisting after stimulus offset. We demonstrate
the model's versatility in two canonical tasks: (a) noise-robust encoding in a
random-dot kinematogram (RDK) paradigm; and (b) approximate Bayesian belief
updating in a probabilistic Wisconsin Card Sorting Test (pWCST). This work
establishes a unified mathematical framework that bridges noise suppression,
working memory, and approximate Bayesian inference within a single cortical
microcircuit, offering fresh insights into the brain's canonical computation
and guiding the design of biologically plausible artificial neural
architectures.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [441] [Discovering Expert-Level Nash Equilibrium Algorithms with Large Language Models](https://arxiv.org/abs/2508.11874)
*Hanyu Li,Dongchen Li,Xiaotie Deng*

Main category: cs.GT

TL;DR: LegoNE is a framework that automates algorithm design and formal analysis by translating algorithms into constrained optimization problems to derive and prove approximation bounds, enabling AI to rediscover state-of-the-art algorithms and discover novel ones.


<details>
  <summary>Details</summary>
Motivation: Traditional algorithm design requires extensive human effort for proving performance guarantees, and while AI excels at solving specific instances, automating the discovery of general algorithms with provable guarantees has remained challenging due to the difficulty of integrating creative design with rigorous analysis.

Method: LegoNE framework translates algorithms written in a simple Python-like language into constrained optimization problems, which when solved, automatically derive and prove the algorithm's approximation bound for computing approximate Nash equilibria.

Result: Using LegoNE, a large language model rediscovered the state-of-the-art algorithm for two-player games within hours (which took humans 15 years) and discovered a novel algorithm for three-player games that surpasses all existing human-designed ones.

Conclusion: This work demonstrates a new human-machine collaborative paradigm where humans reason at abstract levels using symbols to compress search spaces, and AI explores within them, achieving what neither could accomplish alone in theoretical science.

Abstract: Algorithm design and analysis is a cornerstone of computer science, but it
confronts a major challenge. Proving an algorithm's performance guarantee
across all inputs has traditionally required extensive and often error-prone
human effort. While AI has shown great success in finding solutions to specific
problem instances, automating the discovery of general algorithms with such
provable guarantees has remained a significant barrier. This challenge stems
from the difficulty of integrating the creative process of algorithm design
with the rigorous process of formal analysis. To address this gap, we propose
LegoNE, a framework that tightly fuses these two processes for the fundamental
and notoriously difficult problem of computing approximate Nash equilibria.
LegoNE automatically translates any algorithm written by a simple Python-like
language into a constrained optimization problem. Solving this problem derives
and proves the algorithm's approximation bound. Using LegoNE, a
state-of-the-art large language model rediscovered the state-of-the-art
algorithm for two-player games within hours, a feat that had taken human
researchers 15 years to achieve. For three-player games, the model discovered a
novel algorithm surpassing all existing human-designed ones. This work
demonstrates a new human-machine collaborative paradigm for theoretical
science: humans reason at a higher-abstract level, using symbols to compress
the search space, and AI explores within it, achieving what neither could
alone.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [442] [Categorical Construction of Logically Verifiable Neural Architectures](https://arxiv.org/abs/2508.11647)
*Logan Nye*

Main category: cs.LO

TL;DR: A categorical framework that constructs neural networks with provable logical guarantees by embedding logical principles directly into network architecture using Lawvere theories and categorical algebra.


<details>
  <summary>Details</summary>
Motivation: Neural networks struggle with reliable logical reasoning and often violate basic logical principles during inference, limiting their trustworthiness in safety-critical applications.

Method: Treats logical theories as Lawvere theories and transforms them into neural networks using categorical algebra in the 2-category of parametric maps, embedding logical constraints directly into architectural structure.

Result: Demonstrated differentiable neural architectures for propositional logic that preserve boolean reasoning while remaining trainable via gradient descent, with a bijective correspondence between logical theories and neural architectures.

Conclusion: The framework extends Categorical Deep Learning to semantic constraints, enabling automatic derivation of verified architectures from logical specifications for trustworthy AI systems in theorem proving and safety-critical reasoning.

Abstract: Neural networks excel at pattern recognition but struggle with reliable
logical reasoning, often violating basic logical principles during inference.
We address this limitation by developing a categorical framework that
systematically constructs neural architectures with provable logical
guarantees. Our approach treats logical theories as algebraic structures called
Lawvere theories, which we transform into neural networks using categorical
algebra in the 2-category of parametric maps. Unlike existing methods that
impose logical constraints during training, our categorical construction embeds
logical principles directly into the network's architectural structure, making
logical violations mathematically impossible. We demonstrate this framework by
constructing differentiable neural architectures for propositional logic that
preserve boolean reasoning while remaining trainable via gradient descent. Our
main theoretical result establishes a bijective correspondence between finitary
logical theories and neural architectures, proving that every logically
constrained network arises uniquely from our construction. This extends
Categorical Deep Learning beyond geometric symmetries to semantic constraints,
enabling automatic derivation of verified architectures from logical
specifications. The framework provides mathematical foundations for trustworthy
AI systems, with applications to theorem proving, formal verification, and
safety-critical reasoning tasks requiring verifiable logical behavior.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [443] [Using Natural Language for Human-Robot Collaboration in the Real World](https://arxiv.org/abs/2508.11759)
*Peter Lindes,Kaoutar Skiker*

Main category: cs.RO

TL;DR: Exploring integration of large language models (LLMs) with physical robots to enable natural language collaboration between humans and robotic assistants, addressing challenges in real-world language understanding.


<details>
  <summary>Details</summary>
Motivation: To enable autonomous robots to collaborate with humans using natural language, overcoming limitations of traditional Interactive Task Learning systems by leveraging LLMs' advanced language capabilities.

Method: Proposes a cognitive agent architecture that controls physical robots, interacts with humans and LLMs, and accumulates situational knowledge. Presents proof-of-concept experiments using ChatGPT to address three specific natural language understanding challenges.

Result: Demonstrates feasibility through simple experiments showing that LLMs can help robots understand natural language commands, though operational integration requires further development.

Conclusion: LLM-assisted language understanding shows promise for creating integrated robotic assistants that can collaborate with humans using natural language, but requires turning proof-of-concept experiments into fully operational systems.

Abstract: We have a vision of a day when autonomous robots can collaborate with humans
as assistants in performing complex tasks in the physical world. This vision
includes that the robots will have the ability to communicate with their human
collaborators using language that is natural to the humans. Traditional
Interactive Task Learning (ITL) systems have some of this ability, but the
language they can understand is very limited. The advent of large language
models (LLMs) provides an opportunity to greatly improve the language
understanding of robots, yet integrating the language abilities of LLMs with
robots that operate in the real physical world is a challenging problem.
  In this chapter we first review briefly a few commercial robot products that
work closely with humans, and discuss how they could be much better
collaborators with robust language abilities. We then explore how an AI system
with a cognitive agent that controls a physical robot at its core, interacts
with both a human and an LLM, and accumulates situational knowledge through its
experiences, can be a possible approach to reach that vision. We focus on three
specific challenges of having the robot understand natural language, and
present a simple proof-of-concept experiment using ChatGPT for each. Finally,
we discuss what it will take to turn these simple experiments into an
operational system where LLM-assisted language understanding is a part of an
integrated robotic assistant that uses language to collaborate with humans.

</details>


### [444] [Data Shift of Object Detection in Autonomous Driving](https://arxiv.org/abs/2508.11868)
*Lida Xu*

Main category: cs.RO

TL;DR: This paper addresses data shift problems in autonomous driving object detection by analyzing data shift complexity, using detection methods for dataset categorization, and integrating CycleGAN data augmentation with YOLOv5 to achieve superior performance on BDD100K dataset.


<details>
  <summary>Details</summary>
Motivation: Machine learning models in autonomous driving are vulnerable to data distribution shifts caused by seasonal and weather changes, which violate the independent and identically distributed assumption critical for model performance.

Method: Systematic analysis of data shift complexity, comprehensive review of detection methods, dataset categorization and balancing, and integration of CycleGAN-based data augmentation with YOLOv5 framework.

Result: Experimental results show superior performance compared to baseline models on the BDD100K dataset.

Conclusion: The proposed approach effectively addresses data shift problems in autonomous driving object detection through systematic analysis and optimized data augmentation techniques.

Abstract: With the widespread adoption of machine learning technologies in autonomous
driving systems, their role in addressing complex environmental perception
challenges has become increasingly crucial. However, existing machine learning
models exhibit significant vulnerability, as their performance critically
depends on the fundamental assumption that training and testing data satisfy
the independent and identically distributed condition, which is difficult to
guarantee in real-world applications. Dynamic variations in data distribution
caused by seasonal changes, weather fluctuations lead to data shift problems in
autonomous driving systems. This study investigates the data shift problem in
autonomous driving object detection tasks, systematically analyzing its
complexity and diverse manifestations. We conduct a comprehensive review of
data shift detection methods and employ shift detection analysis techniques to
perform dataset categorization and balancing. Building upon this foundation, we
construct an object detection model. To validate our approach, we optimize the
model by integrating CycleGAN-based data augmentation techniques with the
YOLOv5 framework. Experimental results demonstrate that our method achieves
superior performance compared to baseline models on the BDD100K dataset.

</details>


### [445] [Integrating Symbolic RL Planning into a BDI-based Autonomous UAV Framework: System Integration and SIL Validation](https://arxiv.org/abs/2508.11890)
*Sangwoo Jeon,Juchul Shin,YeonJe Cho,Gyeong-Tae Kim,Seongwoo Kim*

Main category: cs.RO

TL;DR: AMAD-SRL framework integrates symbolic planning with reinforcement learning for drone autonomy, improving mission efficiency by 75% in target acquisition scenarios through dynamic path planning and threat avoidance.


<details>
  <summary>Details</summary>
Motivation: Modern drone missions require frameworks that combine structured symbolic planning with adaptive reinforcement learning to handle dynamically complex environments where traditional rule-based systems fall short.

Method: Proposed AMAD-SRL framework extends AMAD cognitive multi-agent architecture with symbolic RL using PDDL, validated in Software-in-the-Loop environment with target acquisition scenario involving surveillance path planning and dynamic reentry path.

Result: Stable module integration, successful transitions between planning phases, and 75% mission efficiency improvement over coverage-based baseline measured by travel distance reduction in SIL evaluation.

Conclusion: The framework establishes a robust foundation for complex UAV missions and provides directions for further enhancement and validation, demonstrating reliable integration of symbolic planning with reinforcement learning.

Abstract: Modern autonomous drone missions increasingly require software frameworks
capable of seamlessly integrating structured symbolic planning with adaptive
reinforcement learning (RL). Although traditional rule-based architectures
offer robust structured reasoning for drone autonomy, their capabilities fall
short in dynamically complex operational environments that require adaptive
symbolic planning. Symbolic RL (SRL), using the Planning Domain Definition
Language (PDDL), explicitly integrates domain-specific knowledge and
operational constraints, significantly improving the reliability and safety of
unmanned aerial vehicle (UAV) decision making. In this study, we propose the
AMAD-SRL framework, an extended and refined version of the Autonomous Mission
Agents for Drones (AMAD) cognitive multi-agent architecture, enhanced with
symbolic reinforcement learning for dynamic mission planning and execution. We
validated our framework in a Software-in-the-Loop (SIL) environment structured
identically to an intended Hardware-In-the-Loop Simulation (HILS) platform,
ensuring seamless transition to real hardware. Experimental results demonstrate
stable integration and interoperability of modules, successful transitions
between BDI-driven and symbolic RL-driven planning phases, and consistent
mission performance. Specifically, we evaluate a target acquisition scenario in
which the UAV plans a surveillance path followed by a dynamic reentry path to
secure the target while avoiding threat zones. In this SIL evaluation, mission
efficiency improved by approximately 75% over a coverage-based baseline,
measured by travel distance reduction. This study establishes a robust
foundation for handling complex UAV missions and discusses directions for
further enhancement and validation.

</details>


### [446] [No More Blind Spots: Learning Vision-Based Omnidirectional Bipedal Locomotion for Challenging Terrain](https://arxiv.org/abs/2508.11929)
*Mohitvishnu S. Gadde,Pranay Dugar,Ashish Malik,Alan Fern*

Main category: cs.RO

TL;DR: A learning framework for vision-based omnidirectional bipedal locomotion that avoids expensive omnidirectional depth rendering by combining a blind controller with teacher-student policy training and noise-augmented terrain data.


<details>
  <summary>Details</summary>
Motivation: Effective bipedal locomotion in dynamic environments requires agile movement in all directions with omnidirectional terrain sensing, but traditional sim-to-real RL is impractical due to high computational costs of rendering omnidirectional depth images.

Method: Combines robust blind controller with teacher policy supervising vision-based student policy, trained on noise-augmented terrain data to avoid rendering costs during RL. Introduces data augmentation technique for supervised student training.

Result: Framework validated through simulation and real-world tests, demonstrating effective omnidirectional locomotion with minimal reliance on expensive rendering. Training accelerated by up to 10x compared to conventional methods.

Conclusion: First demonstration of vision-based omnidirectional bipedal locomotion, showcasing adaptability to diverse terrains while overcoming computational challenges of omnidirectional depth rendering.

Abstract: Effective bipedal locomotion in dynamic environments, such as cluttered
indoor spaces or uneven terrain, requires agile and adaptive movement in all
directions. This necessitates omnidirectional terrain sensing and a controller
capable of processing such input. We present a learning framework for
vision-based omnidirectional bipedal locomotion, enabling seamless movement
using depth images. A key challenge is the high computational cost of rendering
omnidirectional depth images in simulation, making traditional sim-to-real
reinforcement learning (RL) impractical. Our method combines a robust blind
controller with a teacher policy that supervises a vision-based student policy,
trained on noise-augmented terrain data to avoid rendering costs during RL and
ensure robustness. We also introduce a data augmentation technique for
supervised student training, accelerating training by up to 10 times compared
to conventional methods. Our framework is validated through simulation and
real-world tests, demonstrating effective omnidirectional locomotion with
minimal reliance on expensive rendering. This is, to the best of our knowledge,
the first demonstration of vision-based omnidirectional bipedal locomotion,
showcasing its adaptability to diverse terrains.

</details>


### [447] [Self-Guided Action Diffusion](https://arxiv.org/abs/2508.12189)
*Rhea Malhotra,Yuejiang Liu,Chelsea Finn*

Main category: cs.RO

TL;DR: Self-guided action diffusion improves efficiency of bidirectional decoding for diffusion policies by using prior decisions to guide proposal distributions, achieving near-optimal performance with negligible inference cost and 70% higher success rates on dynamic tasks.


<details>
  <summary>Details</summary>
Motivation: Existing inference-time search methods for generative robot policies using bidirectional decoding are computationally expensive as action sample diversity grows, limiting practical deployment.

Method: Self-guided action diffusion that guides the proposal distribution at each diffusion step based on prior decisions, creating a more efficient variant of bidirectional decoding tailored for diffusion-based policies.

Result: Achieves near-optimal performance at negligible inference cost, with up to 70% higher success rates than existing methods on challenging dynamic tasks under tight sampling budgets.

Conclusion: Self-guidance mechanism significantly improves computational efficiency while maintaining high performance, making diffusion policies more practical for real-time robotic applications.

Abstract: Recent works have shown the promise of inference-time search over action
samples for improving generative robot policies. In particular, optimizing
cross-chunk coherence via bidirectional decoding has proven effective in
boosting the consistency and reactivity of diffusion policies. However, this
approach remains computationally expensive as the diversity of sampled actions
grows. In this paper, we introduce self-guided action diffusion, a more
efficient variant of bidirectional decoding tailored for diffusion-based
policies. At the core of our method is to guide the proposal distribution at
each diffusion step based on the prior decision. Experiments in simulation
tasks show that the proposed self-guidance enables near-optimal performance at
negligible inference cost. Notably, under a tight sampling budget, our method
achieves up to 70% higher success rates than existing counterparts on
challenging dynamic tasks. See project website at
https://rhea-mal.github.io/selfgad.github.io.

</details>


### [448] [Improving Pre-Trained Vision-Language-Action Policies with Model-Based Search](https://arxiv.org/abs/2508.12211)
*Cyrus Neary,Omar G. Younis,Artur Kuramshin,Ozgur Aslan,Glen Berseth*

Main category: cs.RO

TL;DR: VLAPS integrates model-based search with pre-trained VLA policies using Monte Carlo Tree Search and environment models to improve robotic task performance in out-of-distribution scenarios.


<details>
  <summary>Details</summary>
Motivation: Pre-trained VLA models often produce brittle behaviors and unsafe failures when deployed zero-shot in out-of-distribution scenarios, requiring a more robust framework.

Method: Embeds model-based search into VLA policy inference using modified Monte Carlo Tree Search with VLA-derived action priors and environment models.

Result: Significantly outperforms VLA-only baselines, increasing success rates by up to 67 percentage points on language-specified tasks that are intractable for uninformed search.

Conclusion: VLAPS provides a principled framework to control test-time compute, leverage environmental knowledge, and integrate planning techniques into VLA inference for more robust robotic behaviors.

Abstract: Pre-trained vision-language-action (VLA) models offer a promising foundation
for generalist robot policies, but often produce brittle behaviours or unsafe
failures when deployed zero-shot in out-of-distribution scenarios. We present
Vision-Language-Action Planning & Search (VLAPS) -- a novel framework and
accompanying algorithms that embed model-based search into the inference
procedure of pre-trained VLA policies to improve their performance on robotic
tasks. Specifically, our method biases a modified Monte Carlo Tree Search
(MCTS) algorithm -- run using a model of the target environment -- using action
priors defined by the VLA policy. By using VLA-derived abstractions and priors
in model-based search, VLAPS efficiently explores language-conditioned robotics
tasks whose search spaces would otherwise be intractably large. Conversely, by
integrating model-based search with the VLA policy's inference procedure, VLAPS
yields behaviours that are more performant than those obtained by directly
following the VLA policy's action predictions. VLAPS offers a principled
framework to: i) control test-time compute in VLA models, ii) leverage a priori
knowledge of the robotic environment, and iii) integrate established planning
and reinforcement learning techniques into the VLA inference process. Across
all experiments, VLAPS significantly outperforms VLA-only baselines on
language-specified tasks that would otherwise be intractable for uninformed
search algorithms, increasing success rates by as much as 67 percentage points.

</details>


### [449] [Belief-Conditioned One-Step Diffusion: Real-Time Trajectory Planning with Just-Enough Sensing](https://arxiv.org/abs/2508.12166)
*Gokul Puthumanaillam,Aditya Penumarti,Manav Vora,Paulo Padrao,Jose Fuentes,Leonardo Bobadilla,Jane Shin,Melkior Ornik*

Main category: cs.RO

TL;DR: B-COD is a belief-conditioned diffusion planner that enables real-time sensor selection for robots, reducing energy consumption while maintaining localization accuracy through a single forward pass.


<details>
  <summary>Details</summary>
Motivation: Traditional belief-space planners are brittle and computationally expensive for sensor switching, while data-driven approaches assume always-on state estimation. There's a need for minimal sensor activation that maintains just enough localization accuracy for task completion.

Method: Belief-Conditioned One-Step Diffusion (B-COD) conditions diffusion models on pose-belief raster and sensor mask, using trajectory spread as a differentiable proxy for localization error. This enables soft-actor-critic to choose sensors online.

Result: B-COD reduces sensing energy consumption while matching goal-reach performance of always-on baselines. It operates in 10ms forward passes without external covariance rollouts.

Conclusion: The approach successfully addresses minimal sensor subset selection for task completion, providing real-time planning with energy optimization while bounding pose uncertainty growth in real-world marine deployments.

Abstract: Robots equipped with rich sensor suites can localize reliably in
partially-observable environments, but powering every sensor continuously is
wasteful and often infeasible. Belief-space planners address this by
propagating pose-belief covariance through analytic models and switching
sensors heuristically--a brittle, runtime-expensive approach. Data-driven
approaches--including diffusion models--learn multi-modal trajectories from
demonstrations, but presuppose an accurate, always-on state estimate. We
address the largely open problem: for a given task in a mapped environment,
which \textit{minimal sensor subset} must be active at each location to
maintain state uncertainty \textit{just low enough} to complete the task? Our
key insight is that when a diffusion planner is explicitly conditioned on a
pose-belief raster and a sensor mask, the spread of its denoising trajectories
yields a calibrated, differentiable proxy for the expected localisation error.
Building on this insight, we present Belief-Conditioned One-Step Diffusion
(B-COD), the first planner that, in a 10 ms forward pass, returns a
short-horizon trajectory, per-waypoint aleatoric variances, and a proxy for
localisation error--eliminating external covariance rollouts. We show that this
single proxy suffices for a soft-actor-critic to choose sensors online,
optimising energy while bounding pose-covariance growth. We deploy B-COD in
real-time marine trials on an unmanned surface vehicle and show that it reduces
sensing energy consumption while matching the goal-reach performance of an
always-on baseline.

</details>


### [450] [Mechanical Automation with Vision: A Design for Rubik's Cube Solver](https://arxiv.org/abs/2508.12469)
*Abhinav Chalise,Nimesh Gopal Pradhan,Nishan Khanal,Prashant Raj Bista,Dinesh Baniya Kshatri*

Main category: cs.RO

TL;DR: A robotic system that solves Rubik's cubes using YOLOv8 for real-time detection, Kociemba's algorithm for solution finding, and stepper motors for physical manipulation with ~2.2 minute average solving time.


<details>
  <summary>Details</summary>
Motivation: To develop an automated system that can physically detect and solve Rubik's cubes using computer vision and robotics, providing both hardware manipulation and user-friendly visualization.

Method: Uses three stepper motors for physical manipulation, microcontroller for control, YOLOv8 model for real-time cube state detection (Precision: 0.98443, Recall: 0.98419), Unity GUI for visualization, and Kociemba's algorithm for solution finding with single degree of freedom manipulation.

Result: Achieved high detection accuracy with YOLOv8 (low box/class losses: 0.42051 and 0.2611 respectively) and average solving time of approximately 2.2 minutes.

Conclusion: The system successfully integrates computer vision, algorithmic solving, and robotic manipulation to create an efficient automated Rubik's cube solver with real-time state detection and user-friendly interface.

Abstract: The core mechanical system is built around three stepper motors for physical
manipulation, a microcontroller for hardware control, a camera and YOLO
detection model for real-time cube state detection. A significant software
component is the development of a user-friendly graphical user interface (GUI)
designed in Unity. The initial state after detection from real-time YOLOv8
model (Precision 0.98443, Recall 0.98419, Box Loss 0.42051, Class Loss 0.2611)
is virtualized on GUI. To get the solution, the system employs the Kociemba's
algorithm while physical manipulation with a single degree of freedom is done
by combination of stepper motors' interaction with the cube achieving the
average solving time of ~2.2 minutes.

</details>


### [451] [PROD: Palpative Reconstruction of Deformable Objects through Elastostatic Signed Distance Functions](https://arxiv.org/abs/2508.12554)
*Hamza El-Kebir*

Main category: cs.RO

TL;DR: PROD is a novel method that reconstructs deformable object shapes and mechanical properties using palpative interaction and elastostatic SDFs, enabling robust estimation from sparse force and pose measurements.


<details>
  <summary>Details</summary>
Motivation: Traditional approaches rely on purely geometric or visual data, which lack the ability to capture mechanical properties and dynamic responses of soft materials needed for applications like robotic manipulation and medical imaging.

Method: Models deformation as an elastostatic process using a Poisson equation for SDF estimation from force-controlled surface probing. Incorporates steady-state elastodynamic assumptions to recover undeformed SDF from deformed observations with provable convergence.

Result: Demonstrates robustness in handling pose errors, non-normal force application, and curvature errors in simulated soft body interactions. Enables estimation of material stiffness from displacement responses to varying force inputs.

Conclusion: PROD provides a powerful tool for reconstructing deformable objects with applications in robotic manipulation, medical imaging, and haptic feedback systems by integrating palpative interaction with elastostatic modeling.

Abstract: We introduce PROD (Palpative Reconstruction of Deformables), a novel method
for reconstructing the shape and mechanical properties of deformable objects
using elastostatic signed distance functions (SDFs). Unlike traditional
approaches that rely on purely geometric or visual data, PROD integrates
palpative interaction -- measured through force-controlled surface probing --
to estimate both the static and dynamic response of soft materials. We model
the deformation of an object as an elastostatic process and derive a governing
Poisson equation for estimating its SDF from a sparse set of pose and force
measurements. By incorporating steady-state elastodynamic assumptions, we show
that the undeformed SDF can be recovered from deformed observations with
provable convergence. Our approach also enables the estimation of material
stiffness by analyzing displacement responses to varying force inputs. We
demonstrate the robustness of PROD in handling pose errors, non-normal force
application, and curvature errors in simulated soft body interactions. These
capabilities make PROD a powerful tool for reconstructing deformable objects in
applications ranging from robotic manipulation to medical imaging and haptic
feedback systems.

</details>


### [452] [Temporal and Rotational Calibration for Event-Centric Multi-Sensor Systems](https://arxiv.org/abs/2508.12564)
*Jiayao Mai,Xiuyuan Lu,Kuan Dai,Shaojie Shen,Yi Zhou*

Main category: cs.RO

TL;DR: Motion-based calibration framework for event camera systems that estimates temporal offset and rotational extrinsics without calibration targets, using angular velocity from event data and joint optimization.


<details>
  <summary>Details</summary>
Motivation: Event cameras offer microsecond latency but extrinsic calibration for multi-sensor systems with event cameras is understudied. Target-based calibration is inconvenient, so a target-free method is needed.

Method: Two-step approach: 1) Initialize temporal offset and rotational extrinsics using kinematic correlations (CCA-inspired), 2) Refine parameters through joint non-linear optimization with continuous-time SO(3) parametrization. Uses angular velocity from normal flow observations derived from event data.

Result: Achieves calibration accuracy comparable to target-based methods, with superior stability over purely CCA-based methods. Demonstrates precision, robustness and flexibility on public and self-collected datasets.

Conclusion: Proposed framework provides accurate target-free calibration for event-centric multi-sensor systems, with open-source implementation to facilitate future research.

Abstract: Event cameras generate asynchronous signals in response to pixel-level
brightness changes, offering a sensing paradigm with theoretically
microsecond-scale latency that can significantly enhance the performance of
multi-sensor systems. Extrinsic calibration is a critical prerequisite for
effective sensor fusion; however, the configuration that involves event cameras
remains an understudied topic. In this paper, we propose a motion-based
temporal and rotational calibration framework tailored for event-centric
multi-sensor systems, eliminating the need for dedicated calibration targets.
Our method uses as input the rotational motion estimates obtained from event
cameras and other heterogeneous sensors, respectively. Different from
conventional approaches that rely on event-to-frame conversion, our method
efficiently estimates angular velocity from normal flow observations, which are
derived from the spatio-temporal profile of event data. The overall calibration
pipeline adopts a two-step approach: it first initializes the temporal offset
and rotational extrinsics by exploiting kinematic correlations in the spirit of
Canonical Correlation Analysis (CCA), and then refines both temporal and
rotational parameters through a joint non-linear optimization using a
continuous-time parametrization in SO(3). Extensive evaluations on both
publicly available and self-collected datasets validate that the proposed
method achieves calibration accuracy comparable to target-based methods, while
exhibiting superior stability over purely CCA-based methods, and highlighting
its precision, robustness and flexibility. To facilitate future research, our
implementation will be made open-source. Code:
https://github.com/NAIL-HNU/EvMultiCalib.

</details>


### [453] [Tactile Gesture Recognition with Built-in Joint Sensors for Industrial Robots](https://arxiv.org/abs/2508.12435)
*Deqing Song,Weimin Yang,Maryam Rezayati,Hans Wernher van de Venn*

Main category: cs.RO

TL;DR: Deep learning methods using only robot joint sensors achieve over 95% accuracy for gesture recognition, eliminating need for external sensors.


<details>
  <summary>Details</summary>
Motivation: To explore gesture recognition in Human-Robot Collaboration without requiring external sensors like vision systems or robot skins, using only built-in joint sensors for cost-effective and scalable solutions.

Method: Evaluated various CNN architectures on two datasets, comparing data representations and model architectures. Tested spectrogram-based representations and implemented methods (STFT2DCNN and STT3DCNN) on Franka Emika Research robot.

Result: Spectrogram-based representations significantly improved accuracy while model architecture played smaller role. Methods achieved over 95% accuracy in contact detection and gesture classification, with better generalization to new robot poses.

Conclusion: Demonstrates feasibility of external-sensor-free tactile recognition, enabling cost-effective and scalable Human-Robot Collaboration solutions without additional hardware.

Abstract: While gesture recognition using vision or robot skins is an active research
area in Human-Robot Collaboration (HRC), this paper explores deep learning
methods relying solely on a robot's built-in joint sensors, eliminating the
need for external sensors. We evaluated various convolutional neural network
(CNN) architectures and collected two datasets to study the impact of data
representation and model architecture on the recognition accuracy. Our results
show that spectrogram-based representations significantly improve accuracy,
while model architecture plays a smaller role. We also tested generalization to
new robot poses, where spectrogram-based models performed better. Implemented
on a Franka Emika Research robot, two of our methods, STFT2DCNN and STT3DCNN,
achieved over 95% accuracy in contact detection and gesture classification.
These findings demonstrate the feasibility of external-sensor-free tactile
recognition and promote further research toward cost-effective, scalable
solutions for HRC.

</details>


### [454] [Grounding Actions in Camera Space: Observation-Centric Vision-Language-Action Policy](https://arxiv.org/abs/2508.13103)
*Tianyi Zhang,Haonan Duan,Haoran Hao,Yu Qiao,Jifeng Dai,Zhi Hou*

Main category: cs.RO

TL;DR: OC-VLA framework transforms action predictions from robot base coordinates to camera observation space using extrinsic calibration, improving VLA model generalization across diverse camera viewpoints.


<details>
  <summary>Details</summary>
Motivation: VLA models struggle with generalization due to spatial inconsistencies between observation and action spaces - models predict end-effector poses in robot base coordinates while observations come from diverse camera perspectives.

Method: Proposes Observation-Centric VLA (OC-VLA) framework that grounds action predictions directly in camera observation space using camera's extrinsic calibration matrix to transform poses from robot base to camera coordinate system.

Result: OC-VLA accelerates convergence, enhances task success rates, and improves cross-view generalization in both simulated and real-world robotic manipulation tasks.

Conclusion: Lightweight plug-and-play strategy that ensures robust perception-action alignment and improves model resilience to camera viewpoint variations without requiring substantial architecture modifications.

Abstract: Vision-Language-Action (VLA) models frequently encounter challenges in
generalizing to real-world environments due to inherent discrepancies between
observation and action spaces. Although training data are collected from
diverse camera perspectives, the models typically predict end-effector poses
within the robot base coordinate frame, resulting in spatial inconsistencies.
To mitigate this limitation, we introduce the Observation-Centric VLA (OC-VLA)
framework, which grounds action predictions directly in the camera observation
space. Leveraging the camera's extrinsic calibration matrix, OC-VLA transforms
end-effector poses from the robot base coordinate system into the camera
coordinate system, thereby unifying prediction targets across heterogeneous
viewpoints. This lightweight, plug-and-play strategy ensures robust alignment
between perception and action, substantially improving model resilience to
camera viewpoint variations. The proposed approach is readily compatible with
existing VLA architectures, requiring no substantial modifications.
Comprehensive evaluations on both simulated and real-world robotic manipulation
tasks demonstrate that OC-VLA accelerates convergence, enhances task success
rates, and improves cross-view generalization. The code will be publicly
available.

</details>


### [455] [Adaptive Model-Predictive Control of a Soft Continuum Robot Using a Physics-Informed Neural Network Based on Cosserat Rod Theory](https://arxiv.org/abs/2508.12681)
*Johann Licher,Max Bartholdt,Henrik Krauss,Tim-Lukas Habich,Thomas Seel,Moritz Schappler*

Main category: cs.RO

TL;DR: Real-time nonlinear MPC for soft continuum robots using physics-informed neural network surrogate model with 44000x speedup, achieving 3mm tracking accuracy at 70Hz.


<details>
  <summary>Details</summary>
Motivation: Dynamic control of soft continuum robots is challenging due to computational demands of accurate models, and existing data-driven methods lack adaptability and full shape capture capabilities.

Method: Domain-decoupled physics-informed neural network (DD-PINN) with adaptable bending stiffness as surrogate for dynamic Cosserat rod model, combined with unscented Kalman filter for state estimation and nonlinear MPC running at 70Hz on GPU.

Result: 44000x speedup over traditional model, 3mm end-effector position errors (2.3% of actuator length) in simulation, similar accuracy and accelerations up to 3.55 m/s² in real-world experiments.

Conclusion: The framework enables real-time capable nonlinear MPC for soft continuum robots with high accuracy and computational efficiency, demonstrating practical applicability in both simulation and real-world scenarios.

Abstract: Dynamic control of soft continuum robots (SCRs) holds great potential for
expanding their applications, but remains a challenging problem due to the high
computational demands of accurate dynamic models. While data-driven approaches
like Koopman-operator-based methods have been proposed, they typically lack
adaptability and cannot capture the full robot shape, limiting their
applicability. This work introduces a real-time-capable nonlinear
model-predictive control (MPC) framework for SCRs based on a domain-decoupled
physics-informed neural network (DD-PINN) with adaptable bending stiffness. The
DD-PINN serves as a surrogate for the dynamic Cosserat rod model with a
speed-up factor of 44000. It is also used within an unscented Kalman filter for
estimating the model states and bending compliance from end-effector position
measurements. We implement a nonlinear evolutionary MPC running at 70 Hz on the
GPU. In simulation, it demonstrates accurate tracking of dynamic trajectories
and setpoint control with end-effector position errors below 3 mm (2.3% of the
actuator's length). In real-world experiments, the controller achieves similar
accuracy and accelerations up to 3.55 m/s2.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [456] [Generative AI in Training and Coaching: Redefining the Design Process of Learning Materials](https://arxiv.org/abs/2508.11662)
*Alexander Komar,Marc-André Heidelmann,Kristina Schaaff*

Main category: cs.CY

TL;DR: GenAI transforms education by shifting trainers from creators to facilitators, improving efficiency but requiring new skills, with successful implementation needing individual, organizational, and strategic approaches.


<details>
  <summary>Details</summary>
Motivation: To explore how generative AI integrates into learning material design and assess its impact on efficiency, pedagogical quality, and the evolving role of human trainers and coaches.

Method: Qualitative interviews with professionals in education and corporate training to identify key integration patterns and impacts.

Result: Trainers increasingly act as facilitators and content moderators rather than creators; efficiency gains enable strategic focus but require new skills; anthropomorphism of AI affects user trust and expectations.

Conclusion: Successful GenAI implementation for trainers requires consideration at individual, organizational, systemic, and strategic levels based on the identified insights.

Abstract: Generative artificial intelligence (GenAI) is transforming education,
redefining the role of trainers and coaches in learning environments. In our
study, we explore how AI integrates into the design process of learning
materials, assessing its impact on efficiency, pedagogical quality, and the
evolving role of human trainers and coaches. Through qualitative interviews
with professionals in education and corporate training, we identify the
following key topics: trainers and coaches increasingly act as facilitators and
content moderators rather than primary creators, efficiency gains allow for a
stronger strategic focus but at the same time the new tools require new skills.
Additionally, we analyze how the anthropomorphism of AI shapes user trust and
expectations. From these insights, we derive how tools based on GenAI can
successfully be implemented for trainers and coaches on an individual,
organizational, systemic, and strategic level.

</details>


### [457] [Future progress in artificial intelligence: A survey of expert opinion](https://arxiv.org/abs/2508.11681)
*Vincent C. Müller,Nick Bostrom*

Main category: cs.CY

TL;DR: Expert survey shows 50% chance of high-level machine intelligence by 2040-2050, 90% by 2075, with superintelligence following within 30 years and 1/3 chance of negative outcomes for humanity.


<details>
  <summary>Details</summary>
Motivation: To clarify expert opinions on the timeline and risks of high-level machine intelligence and superintelligent AI development, addressing concerns about potential risks to humanity.

Method: Designed and distributed a brief questionnaire to four groups of AI experts in 2012/2013 to gather their estimates on development timelines and risk assessments.

Result: Median expert estimates: 50% chance of high-level machine intelligence by 2040-2050, 90% chance by 2075; superintelligence expected within 30 years after; 33% chance of bad/extremely bad outcomes for humanity.

Conclusion: AI experts predict rapid development of advanced AI systems with significant probability of negative consequences, highlighting the need for careful consideration of AI safety and governance.

Abstract: There is, in some quarters, concern about high-level machine intelligence and
superintelligent AI coming up in a few decades, bringing with it significant
risks for humanity. In other quarters, these issues are ignored or considered
science fiction. We wanted to clarify what the distribution of opinions
actually is, what probability the best experts currently assign to high-level
machine intelligence coming up within a particular time-frame, which risks they
see with that development, and how fast they see these developing. We thus
designed a brief questionnaire and distributed it to four groups of experts in
2012/2013. The median estimate of respondents was for a one in two chance that
high-level machine intelligence will be developed around 2040-2050, rising to a
nine in ten chance by 2075. Experts expect that systems will move on to
superintelligence in less than 30 years thereafter. They estimate the chance is
about one in three that this development turns out to be 'bad' or 'extremely
bad' for humanity.

</details>


### [458] [Real Time Child Abduction And Detection System](https://arxiv.org/abs/2508.11690)
*Tadisetty Sai Yashwanth,Yangalasetty Sruthi Royal,Vankayala Rajeshwari Shreya,Mayank Kashyap,Divyaprabha K N*

Main category: cs.CY

TL;DR: Edge-based child abduction detection system using multi-agent VLMs on Raspberry Pi with real-time SMS/WhatsApp alerts


<details>
  <summary>Details</summary>
Motivation: Child safety is a global concern, with abduction posing significant threats requiring proactive monitoring solutions

Method: Multi-agent framework with Vision-Language Models deployed on Raspberry Pi edge devices, processing video feeds and using Twilio API for immediate SMS/WhatsApp notifications

Result: High accuracy in detecting potential abduction scenarios with near real-time performance, outperforming traditional single-model approaches

Conclusion: The system provides scalable, cost-effective edge deployment for widespread child safety monitoring with rapid alerting capabilities

Abstract: Child safety continues to be a paramount concern worldwide, with child
abduction posing significant threats to communities. This paper presents the
development of an edge-based child abduction detection and alert system
utilizing a multi-agent framework where each agent incorporates Vision-Language
Models (VLMs) deployed on a Raspberry Pi. Leveraging the advanced capabilities
of VLMs within individual agents of a multi-agent team, our system is trained
to accurately detect and interpret complex interactions involving children in
various environments in real-time. The multi-agent system is deployed on a
Raspberry Pi connected to a webcam, forming an edge device capable of
processing video feeds, thereby reducing latency and enhancing privacy. An
integrated alert system utilizes the Twilio API to send immediate SMS and
WhatsApp notifications, including calls and messages, when a potential child
abduction event is detected. Experimental results demonstrate that the system
achieves high accuracy in detecting potential abduction scenarios, with near
real-time performance suitable for practical deployment. The multi-agent
architecture enhances the system's ability to process complex situational data,
improving detection capabilities over traditional single-model approaches. The
edge deployment ensures scalability and cost-effectiveness, making it
accessible for widespread use. The proposed system offers a proactive solution
to enhance child safety through continuous monitoring and rapid alerting,
contributing a valuable tool in efforts to prevent child abductions.

</details>


### [459] [Next-Gen Education: Enhancing AI for Microlearning](https://arxiv.org/abs/2508.11704)
*Suman Saha,Fatemeh Rahbari,Farhan Sadique,Sri Krishna Chaitanya Velamakanni,Mahfuza Farooque,William J. Rothwell*

Main category: cs.CY

TL;DR: AI-enhanced microlearning using ChatGPT automates creation of interactive educational materials to improve engagement and outcomes in computer science education.


<details>
  <summary>Details</summary>
Motivation: Address declining class attendance and engagement in US universities post-COVID by adapting to students' preference for remote learning and shorter attention spans.

Method: Integrate microlearning strategies with AI tools (ChatGPT) to automate creation of interactive materials (videos, quizzes, flashcards, scenario-based exercises) for complex computer science topics.

Result: Potential to transform educational practices by reducing educator workload while keeping course content current with latest research and technology.

Conclusion: AI-enhanced microlearning offers a practical model combining advanced technology with established teaching methods to improve computer science education outcomes.

Abstract: This paper explores integrating microlearning strategies into university
curricula, particularly in computer science education, to counteract the
decline in class attendance and engagement in US universities after COVID. As
students increasingly opt for remote learning and recorded lectures,
traditional educational approaches struggle to maintain engagement and
effectiveness. Microlearning, which breaks complex subjects into manageable
units, is proposed to address shorter attention spans and enhance educational
outcomes. It uses interactive formats such as videos, quizzes, flashcards, and
scenario-based exercises, which are especially beneficial for topics like
algorithms and programming logic requiring deep understanding and ongoing
practice. Adoption of microlearning is often limited by the effort needed to
create such materials. This paper proposes leveraging AI tools, specifically
ChatGPT, to reduce the workload for educators by automating the creation of
supplementary materials. While AI can automate certain tasks, educators remain
essential in guiding and shaping the learning process. This AI-enhanced
approach ensures course content is kept current with the latest research and
technology, with educators providing context and insights. By examining AI
capabilities in microlearning, this study shows the potential to transform
educational practices and outcomes in computer science, offering a practical
model for combining advanced technology with established teaching methods.

</details>


### [460] [Listening with Language Models: Using LLMs to Collect and Interpret Classroom Feedback](https://arxiv.org/abs/2508.11707)
*Sai Siddartha Maram,Ulia Zaman,Magy Seif El-Nasr*

Main category: cs.CY

TL;DR: LLM-powered chatbots provide more timely, detailed, and actionable classroom feedback compared to traditional end-of-quarter surveys through conversational dialogues with students.


<details>
  <summary>Details</summary>
Motivation: Traditional end-of-quarter surveys fail to provide instructors with timely, detailed, and actionable feedback about their teaching, limiting opportunities for mid-course improvements.

Method: Designed and deployed a three-part system (PromptDesigner, FeedbackCollector, FeedbackAnalyzer) using LLM-powered chatbots to engage students in reflective dialogues. Conducted a pilot study across two graduate courses at UC Santa Cruz.

Result: LLM-based feedback systems offered richer insights, greater contextual relevance, and higher engagement compared to standard surveys. Instructors valued adaptability and specificity, while students appreciated conversational format and elaboration opportunities.

Conclusion: AI-powered conversational feedback systems can facilitate more meaningful and responsive feedback in higher education, supporting mid-course adjustments and improving teaching effectiveness.

Abstract: Traditional end-of-quarter surveys often fail to provide instructors with
timely, detailed, and actionable feedback about their teaching. In this paper,
we explore how Large Language Model (LLM)-powered chatbots can reimagine the
classroom feedback process by engaging students in reflective, conversational
dialogues. Through the design and deployment of a three-part
system-PromptDesigner, FeedbackCollector, and FeedbackAnalyzer-we conducted a
pilot study across two graduate courses at UC Santa Cruz. Our findings suggest
that LLM-based feedback systems offer richer insights, greater contextual
relevance, and higher engagement compared to standard survey tools. Instructors
valued the system's adaptability, specificity, and ability to support
mid-course adjustments, while students appreciated the conversational format
and opportunity for elaboration. We conclude by discussing the design
implications of using AI to facilitate more meaningful and responsive feedback
in higher education.

</details>


### [461] [Street Review: A Participatory AI-Based Framework for Assessing Streetscape Inclusivity](https://arxiv.org/abs/2508.11708)
*Rashid Mushkani,Shin Koseki*

Main category: cs.CY

TL;DR: Street Review is a mixed-methods framework combining participatory research with AI analysis to assess streetscape inclusivity using resident feedback and 45,000 street-view images in Montreal.


<details>
  <summary>Details</summary>
Motivation: Urban centers experience social and demographic changes that require systematic evaluation of public spaces to understand how streetscapes serve diverse communities.

Method: Combines participatory research (28 resident interviews and image evaluations) with AI-based analysis of 45,000 Mapillary street-view images, producing visual analytics like heatmaps to correlate subjective ratings with physical attributes.

Result: Revealed variations in perceptions of inclusivity and accessibility across demographic groups, showing that diverse user feedback enhances machine learning models through careful data-labeling and co-production strategies.

Conclusion: The Street Review framework provides urban planners and policy analysts with a systematic method to inform planning, policy development, and management of public streets based on inclusive assessment.

Abstract: Urban centers undergo social, demographic, and cultural changes that shape
public street use and require systematic evaluation of public spaces. This
study presents Street Review, a mixed-methods approach that combines
participatory research with AI-based analysis to assess streetscape
inclusivity. In Montr\'eal, Canada, 28 residents participated in semi-directed
interviews and image evaluations, supported by the analysis of approximately
45,000 street-view images from Mapillary. The approach produced visual
analytics, such as heatmaps, to correlate subjective user ratings with physical
attributes like sidewalk, maintenance, greenery, and seating. Findings reveal
variations in perceptions of inclusivity and accessibility across demographic
groups, demonstrating that incorporating diverse user feedback can enhance
machine learning models through careful data-labeling and co-production
strategies. The Street Review framework offers a systematic method for urban
planners and policy analysts to inform planning, policy development, and
management of public streets.

</details>


### [462] [Navigating the New Landscape: A Conceptual Model for Project-Based Assessment (PBA) in the Age of GenAI](https://arxiv.org/abs/2508.11709)
*Rajan Kadel,Samar Shailendra,Urvashi Rahul Saxena*

Main category: cs.CY

TL;DR: Proposes a process-oriented assessment model for Project-Based Learning that addresses GenAI challenges by focusing on learning process evaluation, multimodal assessment, and ethical AI engagement rather than final products.


<details>
  <summary>Details</summary>
Motivation: The integration of Generative AI in higher education threatens traditional Project-Based Assessment methods by enabling AI-generated final products, raising concerns about authenticity, academic integrity, and learning validation.

Method: Develops a reimagined assessment model emphasizing process-oriented evaluation, multimodal multifaceted assessment design, ethical GenAI engagement, and GenAI-assisted personalized feedback throughout the project lifecycle.

Result: Presents a use case scenario demonstrating the model's application in capstone projects, showing how it maintains assessment robustness while accommodating GenAI tools.

Conclusion: Provides recommendations for educators to ensure assessments remain learner-centric and integrity-driven in the GenAI era, shifting focus from product to process evaluation.

Abstract: The rapid integration of Generative Artificial Intelligence (GenAI) into
higher education presents both opportunities and challenges for assessment
design, particularly within Project-Based Assessment (PBA) contexts.
Traditional assessment methods often emphasise the final product in the PBA,
which can now be significantly influenced or created by GenAI tools, raising
concerns regarding product authenticity, academic integrity, and learning
validation. This paper advocates for a reimagined assessment model for
Project-Based Learning (PBL) or a capstone project that prioritises
process-oriented evaluation, multi-modal and multifaceted assessment design,
and ethical engagement with GenAI to enable higher-order thinking. The model
also emphasises the use of (GenAI-assisted) personalised feedback by a
supervisor as an observance of the learning process during the project
lifecycle. A use case scenario is provided to illustrate the application of the
model in a capstone project setting. The paper concludes with recommendations
for educators and curriculum designers to ensure that assessment practices
remain robust, learner-centric, and integrity-driven in the evolving landscape
of GenAI.

</details>


### [463] [Are AI Machines Making Humans Obsolete?](https://arxiv.org/abs/2508.11719)
*Matthias Scheutz*

Main category: cs.CY

TL;DR: Overview of generative AI's evolution, impacts, opportunities, challenges, and control measures


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive understanding of generative AI's development, current impacts, and future implications including both opportunities and risks

Method: Analytical review and discussion of generative AI's trajectory, examining its technological evolution, societal impacts, and potential control mechanisms

Result: Identifies both promising opportunities and significant dangers of generative AI, including dystopian risks from uncontrolled machine learning systems

Conclusion: Proposes specific suggestions for controlling generative AI and addressing its potential dangers to prevent negative outcomes

Abstract: This chapter starts with a sketch of how we got to "generative AI" (GenAI)
and a brief summary of the various impacts it had so far. It then discusses
some of the opportunities of GenAI, followed by the challenges and dangers,
including dystopian outcomes resulting from using uncontrolled machine learning
and our failures to understand the results. It concludes with some suggestions
for how to control GenAI and address its dangers.

</details>


### [464] [The Stories We Govern By: AI, Risk, and the Power of Imaginaries](https://arxiv.org/abs/2508.11729)
*Ninell Oldenburg,Gleb Papyshev*

Main category: cs.CY

TL;DR: Analysis of three competing AI risk narratives and their influence on governance, advocating for pragmatic regulatory approaches over speculative determinism.


<details>
  <summary>Details</summary>
Motivation: To understand how different sociotechnical imaginaries of AI risk shape governance decisions and regulatory constraints, examining how these narratives influence policy-making processes.

Method: Analysis of representative manifesto-style texts from three dominant narrative groups: existential risk proponents, accelerationists, and critical AI scholars, comparing them across four dimensions: normative visions, present diagnoses, views on technology, and perceived human agency.

Result: Reveals how these narratives embed distinct assumptions about risk and narrow the space for alternative governance approaches, potentially progressing into policy-making processes.

Conclusion: Argues against speculative dogmatism and for moving beyond deterministic imaginaries toward regulatory strategies grounded in pragmatism rather than speculative future scenarios.

Abstract: This paper examines how competing sociotechnical imaginaries of artificial
intelligence (AI) risk shape governance decisions and regulatory constraints.
Drawing on concepts from science and technology studies, we analyse three
dominant narrative groups: existential risk proponents, who emphasise
catastrophic AGI scenarios; accelerationists, who portray AI as a
transformative force to be unleashed; and critical AI scholars, who foreground
present-day harms rooted in systemic inequality. Through an analysis of
representative manifesto-style texts, we explore how these imaginaries differ
across four dimensions: normative visions of the future, diagnoses of the
present social order, views on science and technology, and perceived human
agency in managing AI risks. Our findings reveal how these narratives embed
distinct assumptions about risk and have the potential to progress into
policy-making processes by narrowing the space for alternative governance
approaches. We argue against speculative dogmatism and for moving beyond
deterministic imaginaries toward regulatory strategies that are grounded in
pragmatism.

</details>


### [465] [Artificial Intelligence in Rural Healthcare Delivery: Bridging Gaps and Enhancing Equity through Innovation](https://arxiv.org/abs/2508.11738)
*Kiruthika Balakrishnan,Durgadevi Velusamy,Hana E. Hinkle,Zhi Li,Karthikeyan Ramasamy,Hikmat Khan,Srini Ramaswamy,Pir Masoom Shah*

Main category: cs.CY

TL;DR: AI shows transformative potential for rural healthcare through predictive analytics, telemedicine, and diagnostic tools, but faces infrastructure, data quality, and ethical barriers that require interdisciplinary solutions.


<details>
  <summary>Details</summary>
Motivation: Rural healthcare faces persistent challenges including inadequate infrastructure, workforce shortages, and socioeconomic disparities that limit access to essential services, creating a need for innovative solutions.

Method: Systematic review of 109 studies (2019-2024) from multiple databases using PRISMA guidelines and Covidence software, followed by thematic analysis to identify AI implementation patterns in rural healthcare.

Result: AI applications (predictive analytics, telemedicine, diagnostic tools) significantly improve healthcare accessibility, quality, and efficiency. MFMs and LLMs show particular promise for comprehensive decision-making and clinical support, though infrastructure limitations and ethical concerns remain barriers.

Conclusion: AI can revolutionize rural healthcare by augmenting human capacity and democratizing expertise, but requires interdisciplinary collaboration, infrastructure investment, and regulatory frameworks for equitable and sustainable integration.

Abstract: Rural healthcare faces persistent challenges, including inadequate
infrastructure, workforce shortages, and socioeconomic disparities that hinder
access to essential services. This study investigates the transformative
potential of artificial intelligence (AI) in addressing these issues in
underserved rural areas. We systematically reviewed 109 studies published
between 2019 and 2024 from PubMed, Embase, Web of Science, IEEE Xplore, and
Scopus. Articles were screened using PRISMA guidelines and Covidence software.
A thematic analysis was conducted to identify key patterns and insights
regarding AI implementation in rural healthcare delivery. The findings reveal
significant promise for AI applications, such as predictive analytics,
telemedicine platforms, and automated diagnostic tools, in improving healthcare
accessibility, quality, and efficiency. Among these, advanced AI systems,
including Multimodal Foundation Models (MFMs) and Large Language Models (LLMs),
offer particularly transformative potential. MFMs integrate diverse data
sources, such as imaging, clinical records, and bio signals, to support
comprehensive decision-making, while LLMs facilitate clinical documentation,
patient triage, translation, and virtual assistance. Together, these
technologies can revolutionize rural healthcare by augmenting human capacity,
reducing diagnostic delays, and democratizing access to expertise. However,
barriers remain, including infrastructural limitations, data quality concerns,
and ethical considerations. Addressing these challenges requires
interdisciplinary collaboration, investment in digital infrastructure, and the
development of regulatory frameworks. This review offers actionable
recommendations and highlights areas for future research to ensure equitable
and sustainable integration of AI in rural healthcare systems.

</details>


### [466] [Singing Syllabi with Virtual Avatars: Enhancing Student Engagement Through AI-Generated Music and Digital Embodiment](https://arxiv.org/abs/2508.11872)
*Xinxing Wu*

Main category: cs.CY

TL;DR: AI-generated singing avatars transform text-based course syllabi into engaging audiovisual presentations to improve student engagement and information retention.


<details>
  <summary>Details</summary>
Motivation: Traditional text-based syllabi are often overlooked by students, leading to missed critical course information like policies and learning outcomes.

Method: Leveraged open-source tool HeyGem to convert textual syllabi into audiovisual presentations where digital avatars sing the syllabus content.

Result: Student feedback showed AI-sung syllabi significantly improved awareness and recall of key course information.

Conclusion: AI-generated singing avatars present an effective approach to make course syllabi more engaging and memorable for students.

Abstract: In practical teaching, we observe that few students thoroughly read or fully
comprehend the information provided in traditional, text-based course syllabi.
As a result, essential details, such as course policies and learning outcomes,
are frequently overlooked. To address this challenge, in this paper, we propose
a novel approach leveraging AI-generated singing and virtual avatars to present
syllabi in a format that is more visually appealing, engaging, and memorable.
Especially, we leveraged the open-source tool, HeyGem, to transform textual
syllabi into audiovisual presentations, in which digital avatars perform the
syllabus content as songs. The proposed approach aims to stimulate students'
curiosity, foster emotional connection, and enhance retention of critical
course information. Student feedback indicated that AI-sung syllabi
significantly improved awareness and recall of key course information.

</details>


### [467] [SimInterview: Transforming Business Education through Large Language Model-Based Simulated Multilingual Interview Training System](https://arxiv.org/abs/2508.11873)
*Truong Thanh Hung Nguyen,Tran Diem Quynh Nguyen,Hoang Loc Cao,Thi Cam Thanh Tran,Thi Cam Mai Truong,Hung Cao*

Main category: cs.CY

TL;DR: SimInterview is an LLM-based multilingual interview training system that uses AI technologies to create realistic virtual recruiters, adapting interviews to individual resumes and job requirements across languages, showing improved interview readiness and cultural adaptation.


<details>
  <summary>Details</summary>
Motivation: Business interview preparation requires both theoretical knowledge and soft skills, but traditional classroom methods fail to provide individualized, culturally aware practice that modern employers expect in the AI-transformed labor market.

Method: Leverages LLM agents and synthetic AI technologies with retrieval-augmented generation (RAG) to create virtual recruiters. Built on multiple LLMs (OpenAI o3, Llama 4 Maverick, Gemma 3) integrated with Whisper speech recognition, GPT-SoVITS voice synthesis, Ditto talking head generation, and ChromaDB vector databases for multilingual interview simulations.

Result: System consistently aligns assessments with job requirements, preserves resume content accurately, and earns high satisfaction ratings. Gemma 3 produced most engaging conversations. Japanese standardized resumes improved document retrieval while diverse English formats introduced variability. Cultural norms influenced follow-up questioning strategies.

Conclusion: The system significantly improves interview readiness across English and Japanese markets. The paper also outlines a contestable AI design that can explain decisions, detect bias, and preserve human oversight to meet emerging regulatory requirements.

Abstract: Business interview preparation demands both solid theoretical grounding and
refined soft skills, yet conventional classroom methods rarely deliver the
individualized, culturally aware practice employers currently expect. This
paper introduces SimInterview, a large language model (LLM)-based simulated
multilingual interview training system designed for business professionals
entering the AI-transformed labor market. Our system leverages an LLM agent and
synthetic AI technologies to create realistic virtual recruiters capable of
conducting personalized, real-time conversational interviews. The framework
dynamically adapts interview scenarios using retrieval-augmented generation
(RAG) to match individual resumes with specific job requirements across
multiple languages. Built on LLMs (OpenAI o3, Llama 4 Maverick, Gemma 3),
integrated with Whisper speech recognition, GPT-SoVITS voice synthesis, Ditto
diffusion-based talking head generation model, and ChromaDB vector databases,
our system significantly improves interview readiness across English and
Japanese markets. Experiments with university-level candidates show that the
system consistently aligns its assessments with job requirements, faithfully
preserves resume content, and earns high satisfaction ratings, with the
lightweight Gemma 3 model producing the most engaging conversations.
Qualitative findings revealed that the standardized Japanese resume format
improved document retrieval while diverse English resumes introduced additional
variability, and they highlighted how cultural norms shape follow-up
questioning strategies. Finally, we also outlined a contestable AI design that
can explain, detect bias, and preserve human-in-the-loop to meet emerging
regulatory expectations.

</details>


### [468] [Predicting ChatGPT Use in Assignments: Implications for AI-Aware Assessment Design](https://arxiv.org/abs/2508.12013)
*Surajit Das,Aleksei Eliseev*

Main category: cs.CY

TL;DR: Study analyzes ChatGPT usage patterns among university students using XGBoost algorithm, finding 80.1% accuracy in predicting usage based on learning habits and attitudes, with concerns about potential overreliance affecting critical thinking.


<details>
  <summary>Details</summary>
Motivation: Address the lack of quantitative analysis on how generative AI tools like ChatGPT influence real-world student behavior and academic practices, particularly assignment preparation.

Method: Surveyed 388 university students (primarily from Russia with international participants) and used XGBoost algorithm to model predictors of ChatGPT usage in academic assignments, analyzing learning habits, subject preferences, and attitudes toward AI.

Result: Binary classifier achieved 80.1% test accuracy with 80.2% sensitivity and 79.9% specificity. Multiclass classifier achieved 64.5% test accuracy. Found frequent ChatGPT use for learning correlates with potential overreliance, raising concerns about academic independence.

Conclusion: Generative AI can enhance knowledge access but unchecked reliance may erode critical thinking. Proposes discipline-specific guidelines and reimagined assessment strategies to balance innovation with academic rigor for ethical AI integration.

Abstract: The rise of generative AI tools like ChatGPT has significantly reshaped
education, sparking debates about their impact on learning outcomes and
academic integrity. While prior research highlights opportunities and risks,
there remains a lack of quantitative analysis of student behavior when
completing assignments. Understanding how these tools influence real-world
academic practices, particularly assignment preparation, is a pressing and
timely research priority.
  This study addresses this gap by analyzing survey responses from 388
university students, primarily from Russia, including a subset of international
participants. Using the XGBoost algorithm, we modeled predictors of ChatGPT
usage in academic assignments. Key predictive factors included learning habits,
subject preferences, and student attitudes toward AI. Our binary classifier
demonstrated strong predictive performance, achieving 80.1\% test accuracy,
with 80.2\% sensitivity and 79.9\% specificity. The multiclass classifier
achieved 64.5\% test accuracy, 64.6\% weighted precision, and 64.5\% recall,
with similar training scores, indicating potential data scarcity challenges.
  The study reveals that frequent use of ChatGPT for learning new concepts
correlates with potential overreliance, raising concerns about long-term
academic independence. These findings suggest that while generative AI can
enhance access to knowledge, unchecked reliance may erode critical thinking and
originality. We propose discipline-specific guidelines and reimagined
assessment strategies to balance innovation with academic rigor. These insights
can guide educators and policymakers in ethically and effectively integrating
AI into education.

</details>


### [469] [Large Language Models Enable Personalized Nudges to Promote Carbon Offsetting Among Air Travellers](https://arxiv.org/abs/2508.12045)
*Vladimir Maksimenko,Qingyao Xin,Prateek Gupta,Bin Zhang,Prateek Bansal*

Main category: cs.CY

TL;DR: LLMs can design personalized decoy nudges that increase CO2 offset rates by 3-7% for air travelers, primarily by engaging skeptical travelers with low trust in offset programs.


<details>
  <summary>Details</summary>
Motivation: Nudge strategies are effective for sustainable behavior but depend on individual preferences. LLMs offer a cost-effective way to personalize nudges without extensive behavioral datasets, particularly for encouraging voluntary CO2 emission offsets in aviation.

Method: Used LLMs to design personalized decoy-based nudge strategies for air travelers, validated through 3,495 surveys across China, Germany, India, Singapore, and the United States.

Result: LLM-informed personalized nudges increased offsetting rates by 3-7% compared to uniform settings, translating to an additional 2.3 million tonnes of CO2 mitigated annually in aviation. The improvement was driven by increased participation among skeptical travelers with low trust in offset programs.

Conclusion: LLM-driven personalized nudging strategies show significant potential for boosting offsetting behaviors and accelerating aviation decarbonization through targeted engagement of skeptical travelers.

Abstract: Nudge strategies are effective tools for promoting sustainable behaviour, but
their impact depends on individual preferences. By emulating human
decision-making, large language models (LLMs) offer a cost-effective route for
tailoring nudges without extensive behavioural datasets, yet this potential
remains unexplored. Focusing on aviation, we use LLMs to design personalized
decoy-based nudge strategies that encourage air travellers to voluntarily
offset CO$_2$ emissions from flights, and validate their efficacy through 3495
surveys from China, Germany, India, Singapore, and the United States. Results
show that LLM-informed personalized nudges are more effective than uniform
settings, raising offsetting rates by 3-7$\%$ and yielding an additional 2.3
million tonnes of CO$_2$ mitigated annually in aviation. This improvement is
driven primarily by increased participation among sceptical travellers with low
trust in offset programmes. Our study highlights the potential of LLM-driven
personalized nudging strategies for boosting offsetting behaviours to
accelerate aviation decarbonization.

</details>


### [470] [Mutually Assured Deregulation](https://arxiv.org/abs/2508.12300)
*Gilad Abiri*

Main category: cs.CY

TL;DR: The paper argues that dismantling AI safety regulations for competitive advantage is counterproductive and dangerous, as it creates shared risks without delivering lasting benefits.


<details>
  <summary>Details</summary>
Motivation: To expose the flawed logic of the 'Regulation Sacrifice' - the belief that eliminating AI safety oversight will provide security through technological dominance, when in reality it creates collective vulnerability.

Method: The essay presents a critical analysis of three false promises made by deregulation advocates: durable technological leads, accelerated innovation through deregulation, and enhanced national security. It uses empirical evidence like the rapid narrowing of AI performance gaps between US and Chinese systems.

Result: The analysis demonstrates that regulatory frameworks actually streamline development, attract investment, and provide necessary safeguards. Deregulation instead enables information warfare, democratizes dangerous capabilities, and guarantees deployment of uncontrollable AGI systems.

Conclusion: The race to deregulate AI serves powerful interests rather than security, creating 'mutually assured deregulation' where each nation's pursuit of advantage guarantees collective vulnerability. The only safe path is stronger regulatory frameworks, not weaker ones.

Abstract: We have convinced ourselves that the way to make AI safe is to make it
unsafe. Since 2022, policymakers worldwide have embraced the Regulation
Sacrifice - the belief that dismantling safety oversight will deliver security
through AI dominance. Fearing China or USA will gain advantage, nations rush to
eliminate safeguards that might slow progress. This Essay reveals the fatal
flaw: though AI poses national security challenges, the solution demands
stronger regulatory frameworks, not weaker ones. A race without guardrails
breeds shared danger, not competitive strength. The Regulation Sacrifice makes
three false promises. First, it promises durable technological leads. But AI
capabilities spread rapidly - performance gaps between U.S. and Chinese systems
collapsed from 9 percent to 2 percent in thirteen months. When advantages
evaporate in months, sacrificing permanent safety for temporary speed makes no
sense. Second, it promises deregulation accelerates innovation. The opposite
often proves true. Companies report well-designed governance streamlines
development. Investment flows toward regulated markets. Clear rules reduce
uncertainty; uncertain liability creates paralysis. Environmental standards did
not kill the auto industry; they created Tesla and BYD. Third, enhanced
national security through deregulation actually undermines security across all
timeframes. Near term: it hands adversaries information warfare tools. Medium
term: it democratizes bioweapon capabilities. Long term: it guarantees
deployment of uncontrollable AGI systems. The Regulation Sacrifice persists
because it serves powerful interests, not security. Tech companies prefer
freedom to accountability. Politicians prefer simple stories to complex truths.
This creates mutually assured deregulation, where each nation's sprint for
advantage guarantees collective vulnerability. The only way to win is not to
play.

</details>


### [471] [Vitamin N: Benefits of Different Forms of Public Greenery for Urban Health](https://arxiv.org/abs/2508.12998)
*Sanja Šćepanović,Sagar Joglekar,Stephen Law,Daniele Quercia,Ke Zhou,Alice Battiston,Rossano Schifanella*

Main category: cs.CY

TL;DR: On-road greenery visible during daily activities shows stronger health benefits than traditional official greenery metrics, potentially saving millions in prescription costs.


<details>
  <summary>Details</summary>
Motivation: Traditional greenery metrics measure quantity or proximity but ignore actual visibility and daily exposure, leading to inconsistent health benefit findings.

Method: Combined aerial imagery, OpenStreetMap data, Google Street View analysis of 100k+ images, and accessibility estimates across 160k road segments in London, linked to 7.45 billion NHS prescriptions.

Result: On-road greenery reduced hypertension prescriptions by 3.68% in above-median areas, with potential annual savings of £3.15 million if all below-median areas reached citywide median levels.

Conclusion: Daily visible greenery is more relevant for health benefits than secluded public greenery, and official metrics have significant limitations in capturing actual exposure.

Abstract: Urban greenery is often linked to better health, yet findings from past
research have been inconsistent. One reason is that official greenery metrics
measure the amount or nearness of greenery but ignore how often people actually
may potentially see or use it in daily life. To address this gap, we introduced
a new classification that separates on-road greenery, which people see while
walking through streets, from off-road greenery, which requires planned visits.
We did so by combining aerial imagery of Greater London and greenery data from
OpenStreetMap with quantified greenery from over 100,000 Google Street View
images and accessibility estimates based on 160,000 road segments. We linked
these measures to 7.45 billion medical prescriptions issued by the National
Health Service and processed through our methodology. These prescriptions cover
five conditions: diabetes, hypertension, asthma, depression, and anxiety, as
well as opioid use. As hypothesized, we found that green on-road was more
strongly linked to better health than four widely used official measures. For
example, hypertension prescriptions dropped by 3.68% in wards with on-road
greenery above the median citywide level compared to those below it. If all
below-median wards reached the citywide median in on-road greenery,
prescription costs could fall by up to {\pounds}3.15 million each year. These
results suggest that greenery seen in daily life may be more relevant than
public yet secluded greenery, and that official metrics commonly used in the
literature have important limitations.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [472] [Adversarial Robustness in Distributed Quantum Machine Learning](https://arxiv.org/abs/2508.11848)
*Pouya Kananian,Hans-Arno Jacobsen*

Main category: quant-ph

TL;DR: This paper reviews how different distribution methods (federated learning and quantum-specific techniques like circuit cutting/teleportation) affect the adversarial robustness of distributed quantum machine learning models, highlighting vulnerabilities and open research questions.


<details>
  <summary>Details</summary>
Motivation: Understanding adversarial robustness is crucial for building trustworthy quantum ML systems, and distribution across multiple quantum processors introduces new attack surfaces that could compromise model security.

Method: The paper conducts a comprehensive review and analysis of existing approaches, comparing federated learning with quantum-specific distribution methods (circuit cutting and teleportation-based techniques) and their impact on adversarial robustness.

Result: Distribution of QML models can introduce new vulnerabilities and attack vectors, making the systems potentially more susceptible to adversarial attacks compared to centralized implementations.

Conclusion: There are significant open questions regarding the security implications of distributed QML paradigms, and further research is needed to understand and mitigate adversarial threats in these distributed quantum computing architectures.

Abstract: Studying adversarial robustness of quantum machine learning (QML) models is
essential in order to understand their potential advantages over classical
models and build trustworthy systems. Distributing QML models allows leveraging
multiple quantum processors to overcome the limitations of individual devices
and build scalable systems. However, this distribution can affect their
adversarial robustness, potentially making them more vulnerable to new attacks.
Key paradigms in distributed QML include federated learning, which, similar to
classical models, involves training a shared model on local data and sending
only the model updates, as well as circuit distribution methods inherent to
quantum computing, such as circuit cutting and teleportation-based techniques.
These quantum-specific methods enable the distributed execution of quantum
circuits across multiple devices. This work reviews the differences between
these distribution methods, summarizes existing approaches on the adversarial
robustness of QML models when distributed using each paradigm, and discusses
open questions in this area.

</details>


### [473] [Quantum Flow Matching](https://arxiv.org/abs/2508.12413)
*Zidong Cui,Pan Zhang,Ying Tang*

Main category: quant-ph

TL;DR: Quantum Flow Matching (QFM) extends classical flow matching to quantum systems, enabling efficient interpolation between density matrices using quantum circuits without costly redesigns.


<details>
  <summary>Details</summary>
Motivation: To bring the efficiency and effectiveness of classical flow matching generative modeling to quantum systems for systematic density matrix preparation and observable estimation.

Method: Quantum Flow Matching (QFM) - a fully quantum-circuit realization that interpolates between two density matrices, implemented on quantum computers without circuit redesign requirements.

Result: Validated on multiple applications: generating target states with specific magnetization/entanglement, estimating nonequilibrium free-energy differences for quantum Jarzynski equality testing, and accelerating superdiffusion breakdown studies.

Conclusion: QFM emerges as a unifying and promising framework for generative modeling across quantum systems, offering versatile applications in quantum state preparation and observable estimation.

Abstract: Flow matching has rapidly become a dominant paradigm in classical generative
modeling, offering an efficient way to interpolate between two complex
distributions. We extend this idea to the quantum realm and introduce Quantum
Flow Matching (QFM)-a fully quantum-circuit realization that offers efficient
interpolation between two density matrices. QFM offers systematic preparation
of density matrices and generation of samples for accurately estimating
observables, and can be realized on a quantum computer without the need for
costly circuit redesigns. We validate its versatility on a set of applications:
(i) generating target states with prescribed magnetization and entanglement
entropy, (ii) estimating nonequilibrium free-energy differences to test the
quantum Jarzynski equality, and (iii) expediting the study on superdiffusion
breakdown. These results position QFM as a unifying and promising framework for
generative modeling across quantum systems.

</details>


### [474] [SimQFL: A Quantum Federated Learning Simulator with Real-Time Visualization](https://arxiv.org/abs/2508.12477)
*Ratun Rahman,Atit Pokharel,Md Raihan Uddin,Dinh C. Nguyen*

Main category: quant-ph

TL;DR: SimQFL is a specialized quantum federated learning simulator that provides real-time visualization, customization, and interactive debugging for quantum neural networks in distributed quantum networks.


<details>
  <summary>Details</summary>
Motivation: Existing quantum simulators lack integrated ML support, real-time updates, and user-specific data integration, making QFL algorithm development difficult and resource-intensive.

Method: Developed SimQFL - a customized simulator with real-time epoch-wise output visualization, intuitive interface, and customizable parameters (epochs, learning rates, clients, qubits, quantum layers).

Result: Provides immediate feedback after each epoch, shows intermediate outcomes, dynamically illustrates learning curves, and enables prototyping and analysis of quantum neural networks.

Conclusion: SimQFL is a practical interactive platform that simplifies and accelerates QFL experiments with greater transparency and control in distributed quantum network applications.

Abstract: Quantum federated learning (QFL) is an emerging field that has the potential
to revolutionize computation by taking advantage of quantum physics concepts in
a distributed machine learning (ML) environment. However, the majority of
available quantum simulators are primarily built for general quantum circuit
simulation and do not include integrated support for machine learning tasks
such as training, evaluation, and iterative optimization. Furthermore,
designing and assessing quantum learning algorithms is still a difficult and
resource-intensive task. Real-time updates are essential for observing model
convergence, debugging quantum circuits, and making conscious choices during
training with the use of limited resources. Furthermore, most current
simulators fail to support the integration of user-specific data for training
purposes, undermining the main purpose of using a simulator. In this study, we
introduce SimQFL, a customized simulator that simplifies and accelerates QFL
experiments in quantum network applications. SimQFL supports real-time,
epoch-wise output development and visualization, allowing researchers to
monitor the process of learning across each training round. Furthermore, SimQFL
offers an intuitive and visually appealing interface that facilitates ease of
use and seamless execution. Users can customize key variables such as the
number of epochs, learning rates, number of clients, and quantum
hyperparameters such as qubits and quantum layers, making the simulator
suitable for various QFL applications. The system gives immediate feedback
following each epoch by showing intermediate outcomes and dynamically
illustrating learning curves. SimQFL is a practical and interactive platform
enabling academics and developers to prototype, analyze, and tune quantum
neural networks with greater transparency and control in distributed quantum
networks.

</details>


<div id='hep-ph'></div>

# hep-ph [[Back]](#toc)

### [475] [Transfer Learning for Neutrino Scattering: Domain Adaptation with GANs](https://arxiv.org/abs/2508.12987)
*Jose L. Bonilla,Krzysztof M. Graczyk,Artur M. Ankowski,Rwik Dharmapal Banerjee,Beata E. Kowal,Hemant Prasad,Jan T. Sobczyk*

Main category: hep-ph

TL;DR: Transfer learning enables effective adaptation of GAN models from neutrino-carbon to neutrino-argon and antineutrino-carbon scattering, outperforming training from scratch especially with limited data.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of sparse experimental data in neutrino scattering physics by leveraging transfer learning to adapt existing generative models to new interaction types and nuclei.

Method: Used transfer learning to adapt a GAN model trained on synthetic charged-current neutrino-carbon scattering data to generate events for neutrino-argon and antineutrino-carbon interactions. Evaluated performance with 10,000 and 100,000 event training sets.

Result: Transfer learning significantly outperformed training generative models from scratch. Models performed well even with smaller training data (10,000 events).

Conclusion: Transfer learning provides a promising approach for building neutrino scattering event generators when experimental data is limited, enabling effective model adaptation across different interaction types and nuclear targets.

Abstract: We utilize transfer learning to extrapolate the physics knowledge encoded in
a Generative Adversarial Network (GAN) model trained on synthetic
charged-current (CC) neutrino-carbon inclusive scattering data. This base model
is adapted to generate CC inclusive scattering events (lepton kinematics only)
for neutrino-argon and antineutrino-carbon interactions. Furthermore, we assess
the effectiveness of transfer learning in re-optimizing a custom model when new
data comes from a different neutrino-nucleus interaction model. Our results
demonstrate that transfer learning significantly outperforms training
generative models from scratch. To study this, we consider two training data
sets: one with 10,000 and another with 100,000 events. The models obtained via
transfer learning perform well even with smaller training data. The proposed
method provides a promising approach for constructing neutrino scattering event
generators in scenarios where experimental data is sparse.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [476] [BaMANI: Bayesian Multi-Algorithm causal Network Inference](https://arxiv.org/abs/2508.11741)
*Habibolla Latifizadeh,Anika C. Pirkey,Alanna Gould,David J. Klinke II*

Main category: stat.ML

TL;DR: An ensemble learning approach called BaMANI that combines multiple Bayesian network inference algorithms to reduce algorithm-specific biases and improve causal network prediction reliability.


<details>
  <summary>Details</summary>
Motivation: Bayesian causal network inference algorithms produce results that reflect both the underlying generative process and the specific computational algorithm used, creating algorithm-dependent biases that affect reliability.

Method: Developed BaMANI (Bayesian Multi-Algorithm causal Network Inference) - an ensemble learning framework that marginalizes the impact of individual algorithms by combining predictions from multiple Bayesian network inference algorithms.

Result: The approach provides a software tool implementation and demonstrates its application in human breast cancer studies, showing improved reliability in causal network inference.

Conclusion: Ensemble learning through BaMANI effectively reduces algorithm-specific biases in Bayesian causal network inference, providing more robust and reliable predictions for scientific applications like cancer research.

Abstract: Improved computational power has enabled different disciplines to predict
causal relationships among modeled variables using Bayesian network inference.
While many alternative algorithms have been proposed to improve the efficiency
and reliability of network prediction, the predicted causal networks reflect
the generative process but also bear an opaque imprint of the specific
computational algorithm used. Following a ``wisdom of the crowds" strategy, we
developed an ensemble learning approach to marginalize the impact of a single
algorithm on Bayesian causal network inference. To introduce the approach, we
first present the theoretical foundation of this framework. Next, we present a
comprehensive implementation of the framework in terms of a new software tool
called BaMANI (Bayesian Multi-Algorithm causal Network Inference). Finally, we
describe a BaMANI use-case from biology, particularly within human breast
cancer studies.

</details>


### [477] [Dropping Just a Handful of Preferences Can Change Top Large Language Model Rankings](https://arxiv.org/abs/2508.11847)
*Jenny Y. Huang,Yunyi Shen,Dennis Wei,Tamara Broderick*

Main category: stat.ML

TL;DR: The paper proposes a method to evaluate the robustness of Bradley-Terry ranking systems for LLMs, finding that top model rankings are highly sensitive to removing even 0.02% of evaluation data.


<details>
  <summary>Details</summary>
Motivation: To assess how robust LLM ranking systems are to worst-case data removal, particularly for popular human-preference evaluation platforms like Chatbot Arena and MT-Bench.

Method: A computationally fast framework that identifies influential evaluations and tests sensitivity by removing small fractions of worst-case data from Bradley-Terry ranking systems.

Result: Top model rankings are remarkably sensitive - removing just 0.02% of evaluations can change the top-ranked model. MT-Bench rankings are more robust than Chatbot Arena due to expert annotators and better prompts.

Conclusion: Current LLM ranking systems are highly fragile, and careful evaluation design (like expert annotations) improves robustness, but both human-evaluated and LLM-judge systems show similar sensitivity issues.

Abstract: We propose a method for evaluating the robustness of a widely used LLM
ranking system -- the Bradley--Terry ranking system -- to dropping a worst-case
very small fraction of evaluation data. Our approach is computationally fast
and easy to adopt. When we apply our method to matchups from two popular
human-preference platforms, Chatbot Arena and MT-Bench, we find that the
Bradley--Terry rankings of top-performing models are remarkably sensitive to
the removal of a small fraction of evaluations. Our framework also identifies
the specific evaluations most responsible for such ranking flips, allowing for
inspections of these influential preferences. We observe that the rankings
derived from MT-Bench preferences are notably more robust than those from
Chatbot Arena, likely due to MT-bench's use of expert annotators and carefully
constructed prompts. Finally, we find that rankings based on crowdsourced
human-evaluated systems are just as sensitive as those based on LLM-as-a-judge
evaluations, where in both, dropping as little as 0.02% of the total
evaluations in the dataset can change the top-ranked model.

</details>


### [478] [Robust Data Fusion via Subsampling](https://arxiv.org/abs/2508.12048)
*Jing Wang,HaiYing Wang,Kun Chen*

Main category: stat.ML

TL;DR: Robust transfer learning methods with subsampling strategies for handling contaminated external data, improving estimation efficiency for target populations with limited data.


<details>
  <summary>Details</summary>
Motivation: Address the gap in transfer learning where external data is large but contaminated with outliers, while target data is limited, requiring proper subsampling strategies to handle data heterogeneity and contamination.

Method: Proposed two subsampling strategies: one for bias reduction and one for variance minimization, with approaches to combine them. Studied transfer learning methods with subsamples of contaminated external data, accounting for outliers with arbitrary mean shifts.

Result: Provided non-asymptotic error bounds showing the roles of sample sizes, signal strength, sampling rates, outlier magnitude, and error distribution tails. Extensive simulations demonstrated superior performance. Applied to A380 airplane hard landing risk analysis using data from other airplane types.

Conclusion: Robust transfer learning with proper subsampling can significantly improve estimation efficiency for target populations with limited data by effectively leveraging contaminated external data sources.

Abstract: Data fusion and transfer learning are rapidly growing fields that enhance
model performance for a target population by leveraging other related data
sources or tasks. The challenges lie in the various potential heterogeneities
between the target and external data, as well as various practical concerns
that prevent a na\"ive data integration. We consider a realistic scenario where
the target data is limited in size while the external data is large but
contaminated with outliers; such data contamination, along with other
computational and operational constraints, necessitates proper selection or
subsampling of the external data for transfer learning. To our
knowledge,transfer learning and subsampling under data contamination have not
been thoroughly investigated. We address this gap by studying various transfer
learning methods with subsamples of the external data, accounting for outliers
deviating from the underlying true model due to arbitrary mean shifts. Two
subsampling strategies are investigated: one aimed at reducing biases and the
other at minimizing variances. Approaches to combine these strategies are also
introduced to enhance the performance of the estimators. We provide
non-asymptotic error bounds for the transfer learning estimators, clarifying
the roles of sample sizes, signal strength, sampling rates, magnitude of
outliers, and tail behaviors of model error distributions, among other factors.
Extensive simulations show the superior performance of the proposed methods.
Additionally, we apply our methods to analyze the risk of hard landings in A380
airplanes by utilizing data from other airplane types,demonstrating that robust
transfer learning can improve estimation efficiency for relatively rare
airplane types with the help of data from other types of airplanes.

</details>


### [479] [An Introduction to Sliced Optimal Transport](https://arxiv.org/abs/2508.12519)
*Khai Nguyen*

Main category: stat.ML

TL;DR: Comprehensive review of Sliced Optimal Transport (SOT) - a fast computational approach using 1D OT projections through Radon transform, covering mathematical foundations, methodological advances, computational methods, and diverse applications.


<details>
  <summary>Details</summary>
Motivation: To provide efficient and scalable alternatives to classical optimal transport by leveraging the tractability of one-dimensional OT problems while preserving geometric structure.

Method: Combines tools from optimal transport, integral geometry (Radon transform for projecting measures), and computational statistics. Uses one-dimensional OT projections, Monte Carlo approximations, statistical estimation techniques, and various slicing methods.

Result: Enables fast computation of distances, barycenters, and kernels for probability measures. Supports extensions to unbalanced, partial, multi-marginal, and Gromov-Wasserstein settings with applications across multiple domains.

Conclusion: SOT serves as a versatile and practical computational tool that provides efficient alternatives to classical OT, making it valuable for researchers and practitioners in machine learning, data sciences, and computational disciplines.

Abstract: Sliced Optimal Transport (SOT) is a rapidly developing branch of optimal
transport (OT) that exploits the tractability of one-dimensional OT problems.
By combining tools from OT, integral geometry, and computational statistics,
SOT enables fast and scalable computation of distances, barycenters, and
kernels for probability measures, while retaining rich geometric structure.
This paper provides a comprehensive review of SOT, covering its mathematical
foundations, methodological advances, computational methods, and applications.
We discuss key concepts of OT and one-dimensional OT, the role of tools from
integral geometry such as Radon transform in projecting measures, and
statistical techniques for estimating sliced distances. The paper further
explores recent methodological advances, including non-linear projections,
improved Monte Carlo approximations, statistical estimation techniques for
one-dimensional optimal transport, weighted slicing techniques, and
transportation plan estimation methods. Variational problems, such as minimum
sliced Wasserstein estimation, barycenters, gradient flows, kernel
constructions, and embeddings are examined alongside extensions to unbalanced,
partial, multi-marginal, and Gromov-Wasserstein settings. Applications span
machine learning, statistics, computer graphics and computer visions,
highlighting SOT's versatility as a practical computational tool. This work
will be of interest to researchers and practitioners in machine learning, data
sciences, and computational disciplines seeking efficient alternatives to
classical OT.

</details>


### [480] [Unfolded Laplacian Spectral Embedding: A Theoretically Grounded Approach to Dynamic Network Representation](https://arxiv.org/abs/2508.12674)
*Haruka Ezoe,Hiroki Matsumoto,Ryohei Hisano*

Main category: stat.ML

TL;DR: Unfolded Laplacian Spectral Embedding method extends spectral embedding to dynamic graphs using normalized Laplacians, providing formal stability guarantees and Cheeger-style inequality connections to graph conductance.


<details>
  <summary>Details</summary>
Motivation: Dynamic relational structures are central to AI tasks but their evolving nature challenges consistent and interpretable representation. Existing time-varying node embedding methods need to satisfy key stability properties for effectiveness.

Method: Proposes Unfolded Laplacian Spectral Embedding, which extends the Unfolded Adjacency Spectral Embedding framework to normalized Laplacians while preserving cross-sectional and longitudinal stability.

Result: Formal proof that the method satisfies stability conditions, establishes a new Cheeger-style inequality connecting embeddings to graph conductance, and empirical evaluations on synthetic and real-world datasets demonstrate strong performance.

Conclusion: Establishes a principled and stable framework for dynamic network representation grounded in spectral graph theory with both theoretical guarantees and empirical validation.

Abstract: Dynamic relational structures play a central role in many AI tasks, but their
evolving nature presents challenges for consistent and interpretable
representation. A common approach is to learn time-varying node embeddings,
whose effectiveness depends on satisfying key stability properties. In this
paper, we propose Unfolded Laplacian Spectral Embedding, a new method that
extends the Unfolded Adjacency Spectral Embedding framework to normalized
Laplacians while preserving both cross-sectional and longitudinal stability. We
provide formal proof that our method satisfies these stability conditions. In
addition, as a bonus of using the Laplacian matrix, we establish a new
Cheeger-style inequality that connects the embeddings to the conductance of the
underlying dynamic graphs. Empirical evaluations on synthetic and real-world
datasets support our theoretical findings and demonstrate the strong
performance of our method. These results establish a principled and stable
framework for dynamic network representation grounded in spectral graph theory.

</details>


### [481] [Optimal Condition for Initialization Variance in Deep Neural Networks: An SGD Dynamics Perspective](https://arxiv.org/abs/2508.12834)
*Hiroshi Horii,Sothea Has*

Main category: stat.ML

TL;DR: The paper establishes a mathematical framework connecting SGD dynamics to initialization variance through Fokker-Planck equations and KL divergence, deriving an optimal initialization condition that outperforms conventional methods.


<details>
  <summary>Details</summary>
Motivation: To provide a mathematically rigorous criterion for weight initialization in deep neural networks, moving beyond heuristic approaches by connecting SGD dynamics to initialization parameters through theoretical analysis.

Method: Recast SGD as a Fokker-Planck equation for Langevin dynamics, analyze the relationship between quasi-stationary distribution and initial distribution using KL divergence, derive bounds for expected loss function in terms of initialization parameters, and validate experimentally on MNIST and Fashion-MNIST datasets.

Result: Derived an optimal condition for initialization variance in Gaussian case that consistently achieves lower final training loss and higher test accuracy compared to conventional He-normal initialization when satisfied.

Conclusion: Provides a mathematically grounded indicator for initialization variance selection that clarifies the physical meaning of parameter dynamics in DNNs, offering concrete theoretical foundation rather than heuristic approaches.

Abstract: Stochastic gradient descent (SGD), one of the most fundamental optimization
algorithms in machine learning (ML), can be recast through a continuous-time
approximation as a Fokker-Planck equation for Langevin dynamics, a viewpoint
that has motivated many theoretical studies. Within this framework, we study
the relationship between the quasi-stationary distribution derived from this
equation and the initial distribution through the Kullback-Leibler (KL)
divergence. As the quasi-steady-state distribution depends on the expected cost
function, the KL divergence eventually reveals the connection between the
expected cost function and the initialization distribution. By applying this to
deep neural network models (DNNs), we can express the bounds of the expected
loss function explicitly in terms of the initialization parameters. Then, by
minimizing this bound, we obtain an optimal condition of the initialization
variance in the Gaussian case. This result provides a concrete mathematical
criterion, rather than a heuristic approach, to select the scale of weight
initialization in DNNs. In addition, we experimentally confirm our theoretical
results by using the classical SGD to train fully connected neural networks on
the MNIST and Fashion-MNIST datasets. The result shows that if the variance of
the initialization distribution satisfies our theoretical optimal condition,
then the corresponding DNN model always achieves lower final training loss and
higher test accuracy than the conventional He-normal initialization. Our work
thus supplies a mathematically grounded indicator that guides the choice of
initialization variance and clarifies its physical meaning of the dynamics of
parameters in DNNs.

</details>


### [482] [The path to a goal: Understanding soccer possessions via path signatures](https://arxiv.org/abs/2508.12930)
*David Hirnschall,Robert Bajons*

Main category: stat.ML

TL;DR: Novel framework using path signatures to predict soccer actions, outperforms transformers with lower computational cost, introduces new possession evaluation metric validated on Premier League data.


<details>
  <summary>Details</summary>
Motivation: Existing approaches rely on fixed historical windows and handcrafted features, which may include irrelevant information. Need for mathematically grounded encoding that captures complex spatio-temporal structure without manual feature engineering.

Method: Leverages path signatures to encode entire recent possession, capturing order and interaction of events. Handles variable-length time series with irregular sampling frequencies. Uses predicted action type probabilities and locations for possession evaluation.

Result: Outperforms transformer-based benchmark across various loss metrics. Reduces computational cost considerably. New possession metric shows greater reliability than existing metrics in domain-specific comparisons.

Conclusion: Path signatures provide effective encoding for soccer action prediction. Framework validated on 2017/18 Premier League season data. Shows promise for further applications and extensions in soccer analytics.

Abstract: We present a novel framework for predicting next actions in soccer
possessions by leveraging path signatures to encode their complex
spatio-temporal structure. Unlike existing approaches, we do not rely on fixed
historical windows and handcrafted features, but rather encode the entire
recent possession, thereby avoiding the inclusion of potentially irrelevant or
misleading historical information. Path signatures naturally capture the order
and interaction of events, providing a mathematically grounded feature encoding
for variable-length time series of irregular sampling frequencies without the
necessity for manual feature engineering. Our proposed approach outperforms a
transformer-based benchmark across various loss metrics and considerably
reduces computational cost. Building on these results, we introduce a new
possession evaluation metric based on well-established frameworks in soccer
analytics, incorporating both predicted action type probabilities and action
location. Our metric shows greater reliability than existing metrics in
domain-specific comparisons. Finally, we validate our approach through a
detailed analysis of the 2017/18 Premier League season and discuss further
applications and future extensions.

</details>


### [483] [Simulation-Based Inference: A Practical Guide](https://arxiv.org/abs/2508.12939)
*Michael Deistler,Jan Boelts,Peter Steinbach,Guy Moss,Thomas Moreau,Manuel Gloeckler,Pedro L. C. Rodrigues,Julia Linhart,Janne K. Lappalainen,Benjamin Kurt Miller,Pedro J. Gonçalves,Jan-Matthis Lueckmann,Cornelius Schröder,Jakob H. Macke*

Main category: stat.ML

TL;DR: Practical guide for Simulation-based Inference (SBI) methods that enable Bayesian parameter inference without likelihood evaluations, using neural networks trained on simulator data for amortized inference.


<details>
  <summary>Details</summary>
Motivation: Bayesian inference is computationally prohibitive for stochastic simulator models, requiring methods that can perform inference without direct likelihood evaluations.

Method: Train neural networks on data generated by simulators to learn the mapping from observations to posterior distributions, enabling amortized inference that doesn't require additional simulations for new data.

Result: Provides a structured SBI workflow with practical guidelines and diagnostic tools for simulator setup, prior specification, network training, inference, and validation across multiple scientific domains.

Conclusion: SBI methods empower researchers to efficiently perform parameter inference for scientific discovery, with applications demonstrated in astrophysics, psychophysics, and neuroscience.

Abstract: A central challenge in many areas of science and engineering is to identify
model parameters that are consistent with prior knowledge and empirical data.
Bayesian inference offers a principled framework for this task, but can be
computationally prohibitive when models are defined by stochastic simulators.
Simulation-based Inference (SBI) is a suite of methods developed to overcome
this limitation, which has enabled scientific discoveries in fields such as
particle physics, astrophysics, and neuroscience. The core idea of SBI is to
train neural networks on data generated by a simulator, without requiring
access to likelihood evaluations. Once trained, inference is amortized: The
neural network can rapidly perform Bayesian inference on empirical observations
without requiring additional training or simulations. In this tutorial, we
provide a practical guide for practitioners aiming to apply SBI methods. We
outline a structured SBI workflow and offer practical guidelines and diagnostic
tools for every stage of the process -- from setting up the simulator and
prior, choosing and training inference networks, to performing inference and
validating the results. We illustrate these steps through examples from
astrophysics, psychophysics, and neuroscience. This tutorial empowers
researchers to apply state-of-the-art SBI methods, facilitating efficient
parameter inference for scientific discovery.

</details>


### [484] [Shapley Values: Paired-Sampling Approximations](https://arxiv.org/abs/2508.12947)
*Michael Mayer,Mario V. Wüthrich*

Main category: stat.ML

TL;DR: Asymptotic normality results for KernelSHAP and PermutationSHAP sampling approximations, showing paired-sampling provides exact results for second-order interactions and PermutationSHAP has additive recovery property.


<details>
  <summary>Details</summary>
Motivation: Shapley values are widely used to explain ML predictions but face computational limitations. The paper aims to provide theoretical foundations for sampling approximations to improve their reliability and understanding.

Method: Theoretical analysis of sampling KernelSHAP and PermutationSHAP approximations, proving asymptotic normality results and investigating properties of paired-sampling approaches.

Result: Established asymptotic normality for both sampling methods, proved paired-sampling gives exact results for second-order interactions, and showed PermutationSHAP has additive recovery property while KernelSHAP does not.

Conclusion: The theoretical results provide important foundations for reliable use of Shapley value approximations in ML explainability, with PermutationSHAP offering better theoretical properties through additive recovery.

Abstract: Originally introduced in cooperative game theory, Shapley values have become
a very popular tool to explain machine learning predictions. Based on Shapley's
fairness axioms, every input (feature component) gets a credit how it
contributes to an output (prediction). These credits are then used to explain
the prediction. The only limitation in computing the Shapley values (credits)
for many different predictions is of computational nature. There are two
popular sampling approximations, sampling KernelSHAP and sampling
PermutationSHAP. Our first novel contributions are asymptotic normality results
for these sampling approximations. Next, we show that the paired-sampling
approaches provide exact results in case of interactions being of maximal order
two. Furthermore, the paired-sampling PermutationSHAP possesses the additive
recovery property, whereas its kernel counterpart does not.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [485] [Statistical analysis of multivariate planar curves and applications to X-ray classification](https://arxiv.org/abs/2508.11780)
*Moindjié Issam-Ali,Descary Marie-Hélène,Beaulac Cédric*

Main category: stat.ME

TL;DR: A new method for medical image classification using segmented image contours as shape-based predictors, addressing alignment issues in statistical shape analysis and demonstrating effectiveness in cardiomegaly detection.


<details>
  <summary>Details</summary>
Motivation: Leverage segmented medical images (particularly radiography) for improved diagnosis by using object contours as predictors in supervised classification, addressing the need for shape-aware analysis in medical imaging.

Method: Developed a formalism for multivariate planar curves to handle multiple curves jointly, solved alignment issues in statistical shape analysis, and used functional classification with tangent projections on the obtained shape variables.

Result: The method demonstrated appeal and robustness in detecting cardiomegaly from segmented X-rays and performed well in numerical experiments on synthetic data.

Conclusion: The proposed approach provides an effective framework for shape-based image analysis that can enhance medical diagnosis through better utilization of segmented image contours as predictive features.

Abstract: Recent developments in computer vision have enabled the availability of
segmented images across various domains, such as medicine, where segmented
radiography images play an important role in diagnosis-making. As prediction
problems are common in medical image analysis, this work explores the use of
segmented images (through the associated contours they highlight) as predictors
in a supervised classification context. Consequently, we develop a new approach
for image analysis that takes into account the shape of objects within images.
For this aim, we introduce a new formalism that extends the study of single
random planar curves to the joint analysis of multiple planar curves-referred
to here as multivariate planar curves. In this framework, we propose a solution
to the alignment issue in statistical shape analysis. The obtained multivariate
shape variables are then used in functional classification methods through
tangent projections. Detection of cardiomegaly in segmented X-rays and
numerical experiments on synthetic data demonstrate the appeal and robustness
of the proposed method.

</details>


### [486] [A Generalized Genetic Random Field Method for the Genetic Association Analysis of Sequencing Data](https://arxiv.org/abs/2508.12617)
*Ming Li,Zihuai He,Min Zhang,Xiaowei Zhan,Changshuai Wei,Robert C Elston,Qing Lu*

Main category: stat.ME

TL;DR: Proposes GGRF method for sequencing data association analysis that handles rare variants without thresholds, accommodates various phenotypes, and shows improved power over SKAT especially when rare variants are important.


<details>
  <summary>Details</summary>
Motivation: High-throughput sequencing enables investigation of genetic variants for complex diseases, but statistical analysis of high-dimensional sequencing data remains challenging, requiring advanced methods.

Method: Generalized genetic random field (GGRF) method built on generalized estimating equation framework, similarity-based approach that avoids threshold specification for rare variants and allows testing multiple variants with different effect directions.

Result: Simulations show GGRF attains improved or comparable power over SKAT under various disease scenarios, especially when rare variants play significant roles. Applied to Dallas Heart Study data, detected association of ANGPTL3 and ANGPTL4 genes with serum triglyceride.

Conclusion: GGRF provides an effective statistical method for association analysis of sequencing data that handles rare variants well, accommodates diverse phenotypes, and demonstrates superior performance particularly in scenarios where rare variants contribute to disease etiology.

Abstract: With the advance of high-throughput sequencing technologies, it has become
feasible to investigate the influence of the entire spectrum of sequencing
variations on complex human diseases. Although association studies utilizing
the new sequencing technologies hold great promise to unravel novel genetic
variants, especially rare genetic variants that contribute to human diseases,
the statistical analysis of high-dimensional sequencing data remains a
challenge. Advanced analytical methods are in great need to facilitate
high-dimensional sequencing data analyses. In this article, we propose a
generalized genetic random field (GGRF) method for association analyses of
sequencing data. Like other similarity-based methods (e.g., SIMreg and SKAT),
the new method has the advantages of avoiding the need to specify thresholds
for rare variants and allowing for testing multiple variants acting in
different directions and magnitude of effects. The method is built on the
generalized estimating equation framework and thus accommodates a variety of
disease phenotypes (e.g., quantitative and binary phenotypes). Moreover, it has
a nice asymptotic property, and can be applied to small-scale sequencing data
without need for small-sample adjustment. Through simulations, we demonstrate
that the proposed GGRF attains an improved or comparable power over a commonly
used method, SKAT, under various disease scenarios, especially when rare
variants play a significant role in disease etiology. We further illustrate
GGRF with an application to a real dataset from the Dallas Heart Study. By
using GGRF, we were able to detect the association of two candidate genes,
ANGPTL3 and ANGPTL4, with serum triglyceride.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [487] [iTrace: Click-Based Gaze Visualization on the Apple Vision Pro](https://arxiv.org/abs/2508.12268)
*Esra Mehmedova,Santiago Berrezueta-Guzman,Stefan Wagner*

Main category: cs.HC

TL;DR: iTrace enables gaze tracking on Apple Vision Pro despite privacy restrictions by using click-based methods, achieving 91% precision and revealing distinct attention patterns through heatmap visualizations.


<details>
  <summary>Details</summary>
Motivation: Apple Vision Pro has accurate eye-tracking but privacy restrictions prevent direct access to continuous gaze data, limiting research and application development.

Method: Developed iTrace with client-server architecture using click-based gaze extraction (pinch gestures, dwell control, gaming controller) to capture gaze coordinates and generate dynamic heatmaps for video and spatial tracking.

Result: 8BitDo controller achieved 14.22 clicks/s vs 0.45 clicks/s with dwell control, enabling denser heatmaps. System showed 91% gaze precision and revealed distinct attention patterns (concentrated focus in lectures, broader scanning in problem-solving).

Conclusion: iTrace demonstrates strong potential for educational content analysis, environmental design, marketing, and clinical assessment despite current restrictions, but should be used only in research settings.

Abstract: The Apple Vision Pro is equipped with accurate eye-tracking capabilities, yet
the privacy restrictions on the device prevent direct access to continuous user
gaze data. This study introduces iTrace, a novel application that overcomes
these limitations through click-based gaze extraction techniques, including
manual methods like a pinch gesture, and automatic approaches utilizing dwell
control or a gaming controller. We developed a system with a client-server
architecture that captures the gaze coordinates and transforms them into
dynamic heatmaps for video and spatial eye tracking. The system can generate
individual and averaged heatmaps, enabling analysis of personal and collective
attention patterns.
  To demonstrate its effectiveness and evaluate the usability and performance,
a study was conducted with two groups of 10 participants, each testing
different clicking methods. The 8BitDo controller achieved higher average data
collection rates at 14.22 clicks/s compared to 0.45 clicks/s with dwell
control, enabling significantly denser heatmap visualizations. The resulting
heatmaps reveal distinct attention patterns, including concentrated focus in
lecture videos and broader scanning during problem-solving tasks. By allowing
dynamic attention visualization while maintaining a high gaze precision of 91
%, iTrace demonstrates strong potential for a wide range of applications in
educational content engagement, environmental design evaluation, marketing
analysis, and clinical cognitive assessment. Despite the current gaze data
restrictions on the Apple Vision Pro, we encourage developers to use iTrace
only in research settings.

</details>


### [488] [fCrit: A Visual Explanation System for Furniture Design Creative Support](https://arxiv.org/abs/2508.12416)
*Vuong Nguyen,Gabriel Vigliensoni*

Main category: cs.HC

TL;DR: fCrit is a dialogue-based AI system that critiques furniture design with explainable AI, using multi-agent architecture and design knowledge to provide tailored explanations that match users' design language and cognitive framing.


<details>
  <summary>Details</summary>
Motivation: To advance explainable AI in creative domains by making AI reasoning transparent while adapting to how users think and talk about their designs, particularly in furniture design critique.

Method: Uses a multi-agent architecture grounded in reflective learning and formal analysis, informed by a structured design knowledge base to provide dialogic and visually grounded critiques.

Result: Developed fCrit system that supports Human-Centered Explainable AI by tailoring explanations to users' design language and cognitive framing in furniture design critique.

Conclusion: This work contributes to domain-specific HCXAI methods for creative practice, enabling situated, dialogic, and visually grounded AI support that aligns with how designers naturally communicate.

Abstract: We introduce fCrit, a dialogue-based AI system designed to critique furniture
design with a focus on explainability. Grounded in reflective learning and
formal analysis, fCrit employs a multi-agent architecture informed by a
structured design knowledge base. We argue that explainability in the arts
should not only make AI reasoning transparent but also adapt to the ways users
think and talk about their designs. We demonstrate how fCrit supports this
process by tailoring explanations to users' design language and cognitive
framing. This work contributes to Human-Centered Explainable AI (HCXAI) in
creative practice, advancing domain-specific methods for situated, dialogic,
and visually grounded AI support.

</details>


### [489] [Using AI for User Representation: An Analysis of 83 Persona Prompts](https://arxiv.org/abs/2508.13047)
*Joni Salminen,Danial Amin,Bernard Jansen*

Main category: cs.HC

TL;DR: Analysis of 83 persona prompts from 27 studies shows LLMs predominantly generate single, concise personas with demographic attributes, often in structured formats like JSON, deviating from traditional rich persona profiles.


<details>
  <summary>Details</summary>
Motivation: To understand how researchers are using LLM prompts to generate user personas and examine the characteristics and limitations of current computational persona generation practices.

Method: Analyzed 83 persona prompts from 27 research articles that used large language models to generate user personas, examining prompt characteristics, output formats, and research practices.

Result: Prompts mainly generate single personas with short descriptions, include demographic attributes in text/number formats, use structured outputs (JSON), and rarely test multiple LLMs. Most research uses few prompts despite some studies using up to 12.

Conclusion: Current LLM persona generation practices favor concise, structured outputs over traditional rich profiles, raising concerns about the quality and completeness of computational user representations.

Abstract: We analyzed 83 persona prompts from 27 research articles that used large
language models (LLMs) to generate user personas. Findings show that the
prompts predominantly generate single personas. Several prompts express a
desire for short or concise persona descriptions, which deviates from the
tradition of creating rich, informative, and rounded persona profiles. Text is
the most common format for generated persona attributes, followed by numbers.
Text and numbers are often generated together, and demographic attributes are
included in nearly all generated personas. Researchers use up to 12 prompts in
a single study, though most research uses a small number of prompts. Comparison
and testing multiple LLMs is rare. More than half of the prompts require the
persona output in a structured format, such as JSON, and 74% of the prompts
insert data or dynamic variables. We discuss the implications of increased use
of computational personas for user representation.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [490] [Centralized Permutation Equivariant Policy for Cooperative Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2508.11706)
*Zhuofan Xu,Benedikt Bollig,Matthias Függer,Thomas Nowak,Vincent Le Dréau*

Main category: cs.MA

TL;DR: CPE learning is a centralized training and execution framework that uses permutation equivariant networks to overcome limitations of decentralized policies while maintaining scalability.


<details>
  <summary>Details</summary>
Motivation: Decentralized policies under CTDE suffer from partial observability and suboptimal performance, while fully centralized approaches face scalability issues as agent numbers increase.

Method: Proposes Centralized Permutation Equivariant (CPE) learning with Global-Local Permutation Equivariant (GLPE) networks - a lightweight, scalable permutation equivariant architecture for fully centralized policy execution.

Result: CPE integrates seamlessly with value decomposition and actor-critic methods, substantially improves performance of standard CTDE algorithms across MPE, SMAC, and RWARE benchmarks, and matches state-of-the-art RWARE performance.

Conclusion: The CPE framework with GLPE networks provides an effective solution for centralized execution that overcomes scalability challenges while maintaining or improving performance over decentralized approaches.

Abstract: The Centralized Training with Decentralized Execution (CTDE) paradigm has
gained significant attention in multi-agent reinforcement learning (MARL) and
is the foundation of many recent algorithms. However, decentralized policies
operate under partial observability and often yield suboptimal performance
compared to centralized policies, while fully centralized approaches typically
face scalability challenges as the number of agents increases.
  We propose Centralized Permutation Equivariant (CPE) learning, a centralized
training and execution framework that employs a fully centralized policy to
overcome these limitations. Our approach leverages a novel permutation
equivariant architecture, Global-Local Permutation Equivariant (GLPE) networks,
that is lightweight, scalable, and easy to implement. Experiments show that CPE
integrates seamlessly with both value decomposition and actor-critic methods,
substantially improving the performance of standard CTDE algorithms across
cooperative benchmarks including MPE, SMAC, and RWARE, and matching the
performance of state-of-the-art RWARE implementations.

</details>


### [491] [SafeSieve: From Heuristics to Experience in Progressive Pruning for LLM-based Multi-Agent Communication](https://arxiv.org/abs/2508.11733)
*Ruijia Zhang,Xinyan Zhao,Ruixiang Wang,Sigen Chen,Guibin Zhang,An Zhang,Kun Wang,Qingsong Wen*

Main category: cs.MA

TL;DR: SafeSieve is a progressive multi-agent pruning algorithm that reduces token usage by 12.4%-27.8% while maintaining 94.01% accuracy, using dual-mechanism communication refinement and 0-extension clustering.


<details>
  <summary>Details</summary>
Motivation: LLM-based multi-agent systems suffer from redundant communication and excessive token overhead, with existing methods isolating pre- and post-task optimization without unified strategy.

Method: Progressive adaptive pruning algorithm with dual-mechanism: initial LLM-based semantic evaluation + accumulated performance feedback, using 0-extension clustering to preserve coherent agent groups while eliminating ineffective links.

Result: Achieves 94.01% average accuracy, reduces token usage by 12.4%-27.8%, shows robustness under prompt injection attacks (only 1.23% accuracy drop), and reduces deployment costs by 13.3% in heterogeneous settings.

Conclusion: SafeSieve establishes a robust, efficient, and scalable framework for practical multi-agent systems, outperforming existing greedy Top-k pruning methods with better structural coherence preservation.

Abstract: LLM-based multi-agent systems exhibit strong collaborative capabilities but
often suffer from redundant communication and excessive token overhead.
Existing methods typically enhance efficiency through pretrained GNNs or greedy
algorithms, but often isolate pre- and post-task optimization, lacking a
unified strategy. To this end, we present SafeSieve, a progressive and adaptive
multi-agent pruning algorithm that dynamically refines the inter-agent
communication through a novel dual-mechanism. SafeSieve integrates initial
LLM-based semantic evaluation with accumulated performance feedback, enabling a
smooth transition from heuristic initialization to experience-driven
refinement. Unlike existing greedy Top-k pruning methods, SafeSieve employs
0-extension clustering to preserve structurally coherent agent groups while
eliminating ineffective links. Experiments across benchmarks (SVAMP, HumanEval,
etc.) showcase that SafeSieve achieves 94.01% average accuracy while reducing
token usage by 12.4%-27.8%. Results further demonstrate robustness under prompt
injection attacks (1.23% average accuracy drop). In heterogeneous settings,
SafeSieve reduces deployment costs by 13.3% while maintaining performance.
These results establish SafeSieve as a robust, efficient, and scalable
framework for practical multi-agent systems. Our code can be found in
https://anonymous.4open.science/r/SafeSieve-D8F2FFUN.

</details>


### [492] [A Comprehensive Review of AI Agents: Transforming Possibilities in Technology and Beyond](https://arxiv.org/abs/2508.11957)
*Xiaodong Qu,Andrews Damoah,Joshua Sherwood,Peiyan Liu,Christian Shun Jin,Lulu Chen,Minjie Shen,Nawwaf Aleisa,Zeyuan Hou,Chenyu Zhang,Lifu Gao,Yanshu Li,Qikai Yang,Qun Wang,Cristabelle De Souza*

Main category: cs.MA

TL;DR: A comprehensive review of AI agent evolution from rule-based systems to modern autonomous agents, covering architectural principles, foundational components, and addressing ethical concerns for future development.


<details>
  <summary>Details</summary>
Motivation: To address the grand challenge of designing unified AI agents that seamlessly integrate cognition, planning, and interaction, and to guide the next generation of robust and trustworthy autonomous systems.

Method: Systematic examination of architectural principles and foundational components, synthesizing insights from cognitive science-inspired models, hierarchical reinforcement learning frameworks, and large language model-based reasoning.

Result: Provides a comprehensive landscape analysis of contemporary AI agents, highlighting major breakthroughs and synthesizing knowledge across multiple AI paradigms.

Conclusion: The review identifies persistent challenges and promising research directions to guide development toward more robust, adaptable, and trustworthy autonomous intelligence systems, while addressing ethical and safety concerns.

Abstract: Artificial Intelligence (AI) agents have rapidly evolved from specialized,
rule-based programs to versatile, learning-driven autonomous systems capable of
perception, reasoning, and action in complex environments. The explosion of
data, advances in deep learning, reinforcement learning, and multi-agent
coordination have accelerated this transformation. Yet, designing and deploying
unified AI agents that seamlessly integrate cognition, planning, and
interaction remains a grand challenge. In this review, we systematically
examine the architectural principles, foundational components, and emergent
paradigms that define the landscape of contemporary AI agents. We synthesize
insights from cognitive science-inspired models, hierarchical reinforcement
learning frameworks, and large language model-based reasoning. Moreover, we
discuss the pressing ethical, safety, and interpretability concerns associated
with deploying these agents in real-world scenarios. By highlighting major
breakthroughs, persistent challenges, and promising research directions, this
review aims to guide the next generation of AI agent systems toward more
robust, adaptable, and trustworthy autonomous intelligence.

</details>


### [493] [Synchronization Dynamics of Heterogeneous, Collaborative Multi-Agent AI Systems](https://arxiv.org/abs/2508.12314)
*Chiranjit Mitra*

Main category: cs.MA

TL;DR: A physics-inspired framework that applies the Kuramoto oscillator model to multi-agent AI systems, enabling mathematical analysis of agent coordination, synchronization, and emergent collective intelligence.


<details>
  <summary>Details</summary>
Motivation: To establish a rigorous mathematical foundation for understanding and designing multi-agent AI systems by bridging synchronization theory from physics with AI agent dynamics, addressing the need for principled orchestration of heterogeneous AI agents in collaborative scenarios.

Method: Adapted the Kuramoto model to represent AI agents as coupled oscillators with phase and amplitude dynamics, introduced an order parameter to quantify coordination, and conducted extensive simulations on all-to-all and deterministic scale-free networks to study coupling strength, agent diversity, and network topology effects.

Result: Demonstrated that increased coupling promotes robust synchronization despite heterogeneous agent capabilities, successfully formalized the correspondence between Chain-of-Thought prompting and synchronization phenomena, and showed that the model captures essential aspects of agent specialization, influence, and communication.

Conclusion: The framework provides a physics-informed mathematical foundation for designing, analyzing, and optimizing scalable, adaptive multi-agent AI systems, opening pathways for principled agent orchestration and laying groundwork for future incorporation of learning dynamics and adaptive network architectures.

Abstract: We present a novel interdisciplinary framework that bridges synchronization
theory and multi-agent AI systems by adapting the Kuramoto model to describe
the collective dynamics of heterogeneous AI agents engaged in complex task
execution. By representing AI agents as coupled oscillators with both phase and
amplitude dynamics, our model captures essential aspects of agent
specialization, influence, and communication within networked systems. We
introduce an order parameter to quantify the degree of coordination and
synchronization, providing insights into how coupling strength, agent
diversity, and network topology impact emergent collective behavior.
Furthermore, we formalize a detailed correspondence between Chain-of-Thought
prompting in AI reasoning and synchronization phenomena, unifying human-like
iterative problem solving with emergent group intelligence. Through extensive
simulations on all-to-all and deterministic scale-free networks, we demonstrate
that increased coupling promotes robust synchronization despite heterogeneous
agent capabilities, reflecting realistic collaborative AI scenarios. Our
physics-informed approach establishes a rigorous mathematical foundation for
designing, analyzing, and optimizing scalable, adaptive, and interpretable
multi-agent AI systems. This work opens pathways for principled orchestration
of agentic AI and lays the groundwork for future incorporation of learning
dynamics and adaptive network architectures to further enhance system
resilience and efficiency.

</details>


### [494] [A Taxonomy of Hierarchical Multi-Agent Systems: Design Patterns, Coordination Mechanisms, and Industrial Applications](https://arxiv.org/abs/2508.12683)
*David J. Moore*

Main category: cs.MA

TL;DR: A multi-dimensional taxonomy for hierarchical multi-agent systems (HMAS) across five axes: control hierarchy, information flow, role/task delegation, temporal layering, and communication structure, providing a framework to compare different approaches rather than prescribing a single best design.


<details>
  <summary>Details</summary>
Motivation: Hierarchical multi-agent systems help manage complexity and scale but introduce trade-offs that are not always obvious. The paper aims to provide a comprehensive framework for comparing different HMAS approaches across multiple dimensions.

Method: Proposes a five-axis taxonomy connected to concrete coordination mechanisms (contract-net protocol, hierarchical reinforcement learning) and illustrated with industrial case studies from power grids and oilfield operations.

Result: The taxonomy unifies structural, temporal, and communication dimensions into a single design framework, showing that hierarchical structures can achieve global efficiency while preserving local autonomy, though the balance is delicate.

Conclusion: Identifies open challenges including making hierarchical decisions explainable, scaling to large agent populations, and safely integrating learning-based agents like LLMs. Presents the first unified taxonomy bridging classical coordination with modern reinforcement learning and LLM agents.

Abstract: Hierarchical multi-agent systems (HMAS) organize collections of agents into
layered structures that help manage complexity and scale. These hierarchies can
simplify coordination, but they also can introduce trade-offs that are not
always obvious. This paper proposes a multi-dimensional taxonomy for HMAS along
five axes: control hierarchy, information flow, role and task delegation,
temporal layering, and communication structure. The intent is not to prescribe
a single "best" design but to provide a lens for comparing different
approaches.
  Rather than treating these dimensions in isolation, the taxonomy is connected
to concrete coordination mechanisms - from the long-standing contract-net
protocol for task allocation to more recent work in hierarchical reinforcement
learning. Industrial contexts illustrate the framework, including power grids
and oilfield operations, where agents at production, maintenance, and supply
levels coordinate to diagnose well issues or balance energy demand. These cases
suggest that hierarchical structures may achieve global efficiency while
preserving local autonomy, though the balance is delicate.
  The paper closes by identifying open challenges: making hierarchical
decisions explainable to human operators, scaling to very large agent
populations, and assessing whether learning-based agents such as large language
models can be safely integrated into layered frameworks. This paper presents
what appears to be the first taxonomy that unifies structural, temporal, and
communication dimensions of hierarchical MAS into a single design framework,
bridging classical coordination mechanisms with modern reinforcement learning
and large language model agents.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [495] [RefAdGen: High-Fidelity Advertising Image Generation](https://arxiv.org/abs/2508.11695)
*Yiyun Chen,Weikai Yang*

Main category: cs.GR

TL;DR: RefAdGen is a novel framework for high-fidelity advertising image generation that addresses fidelity-efficiency tradeoffs in existing AIGC methods through a decoupled design with product mask injection and attention fusion.


<details>
  <summary>Details</summary>
Motivation: Existing AIGC techniques for advertising image generation either require extensive fine-tuning per product or struggle to maintain fidelity across diverse products, making them impractical for e-commerce and marketing applications.

Method: Constructed AdProd-100K dataset with dual data augmentation strategy, then proposed RefAdGen framework with decoupled design: product mask injection for spatial control and Attention Fusion Module (AFM) for product feature integration.

Result: State-of-the-art performance with robust generalization, maintaining high fidelity for both unseen products and challenging real-world images, offering scalable and cost-effective alternative to traditional workflows.

Conclusion: RefAdGen effectively resolves the fidelity-efficiency dilemma in AIGC-based advertising generation, providing a practical solution for e-commerce and marketing industries with publicly available code and datasets.

Abstract: The rapid advancement of Artificial Intelligence Generated Content (AIGC)
techniques has unlocked opportunities in generating diverse and compelling
advertising images based on referenced product images and textual scene
descriptions. This capability substantially reduces human labor and production
costs in traditional marketing workflows. However, existing AIGC techniques
either demand extensive fine-tuning for each referenced image to achieve high
fidelity, or they struggle to maintain fidelity across diverse products, making
them impractical for e-commerce and marketing industries. To tackle this
limitation, we first construct AdProd-100K, a large-scale advertising image
generation dataset. A key innovation in its construction is our dual data
augmentation strategy, which fosters robust, 3D-aware representations crucial
for realistic and high-fidelity image synthesis. Leveraging this dataset, we
propose RefAdGen, a generation framework that achieves high fidelity through a
decoupled design. The framework enforces precise spatial control by injecting a
product mask at the U-Net input, and employs an efficient Attention Fusion
Module (AFM) to integrate product features. This design effectively resolves
the fidelity-efficiency dilemma present in existing methods. Extensive
experiments demonstrate that RefAdGen achieves state-of-the-art performance,
showcasing robust generalization by maintaining high fidelity and remarkable
visual results for both unseen products and challenging real-world, in-the-wild
images. This offers a scalable and cost-effective alternative to traditional
workflows. Code and datasets are publicly available at
https://github.com/Anonymous-Name-139/RefAdgen.

</details>


### [496] [Express4D: Expressive, Friendly, and Extensible 4D Facial Motion Generation Benchmark](https://arxiv.org/abs/2508.12438)
*Yaron Aloni,Rotem Shalev-Arkushin,Yonatan Shafir,Guy Tevet,Ohad Fried,Amit Haim Bermano*

Main category: cs.GR

TL;DR: Express4D dataset enables fine-grained text-to-facial-expression generation using commodity equipment and LLM-generated annotations, providing riggable motion data for improved facial animation control.


<details>
  <summary>Details</summary>
Motivation: Current facial expression datasets are limited to speech-driven or coarse emotion labels, lacking nuanced expressive descriptions and requiring expensive capture equipment, which hinders fine-grained control in animation and virtual avatars.

Method: Created a new facial motion dataset using commodity equipment with ARKit blendshape format, featuring nuanced performances and semantic annotations generated by LLMs. Trained two baseline models for text-to-expression motion generation.

Result: The Express4D dataset provides rich expressive performances with detailed labels. Trained models successfully learn meaningful text-to-expression generation and capture the many-to-many mapping between text and facial expressions.

Conclusion: Express4D enables accessible, fine-grained facial expression generation from natural language, overcoming limitations of existing datasets and expensive equipment requirements, with applications in animation, virtual avatars, and human-computer interaction.

Abstract: Dynamic facial expression generation from natural language is a crucial task
in Computer Graphics, with applications in Animation, Virtual Avatars, and
Human-Computer Interaction. However, current generative models suffer from
datasets that are either speech-driven or limited to coarse emotion labels,
lacking the nuanced, expressive descriptions needed for fine-grained control,
and were captured using elaborate and expensive equipment. We hence present a
new dataset of facial motion sequences featuring nuanced performances and
semantic annotation. The data is easily collected using commodity equipment and
LLM-generated natural language instructions, in the popular ARKit blendshape
format. This provides riggable motion, rich with expressive performances and
labels. We accordingly train two baseline models, and evaluate their
performance for future benchmarking. Using our Express4D dataset, the trained
models can learn meaningful text-to-expression motion generation and capture
the many-to-many mapping of the two modalities. The dataset, code, and video
examples are available on our webpage: https://jaron1990.github.io/Express4D/

</details>


### [497] [MixCache: Mixture-of-Cache for Video Diffusion Transformer Acceleration](https://arxiv.org/abs/2508.12691)
*Yuanxin Wei,Lansong Diao,Bujiao Chen,Shenggan Cheng,Zhengping Qian,Wenyuan Yu,Nong Xiao,Wei Lin,Jiangsu Du*

Main category: cs.GR

TL;DR: MixCache is a training-free caching framework that accelerates video diffusion transformer inference by combining multiple caching granularities with adaptive triggering strategies, achieving near 2x speedup while maintaining quality.


<details>
  <summary>Details</summary>
Motivation: Video DiT models suffer from high computational costs and latency due to multi-step iterative denoising. Existing caching methods are limited to single-granularity strategies and struggle to balance quality and speed effectively.

Method: Proposes MixCache with context-aware cache triggering to determine when to enable caching, and adaptive hybrid cache decision strategy to dynamically select optimal caching granularity (step, cfg, block levels).

Result: Achieves 1.94x speedup on Wan 14B and 1.97x speedup on HunyuanVideo models while delivering superior generation quality and inference efficiency compared to baseline methods.

Conclusion: MixCache effectively addresses the limitations of single-granularity caching by providing a flexible framework that significantly accelerates video generation without compromising quality, demonstrating strong performance across diverse models.

Abstract: Leveraging the Transformer architecture and the diffusion process, video DiT
models have emerged as a dominant approach for high-quality video generation.
However, their multi-step iterative denoising process incurs high computational
cost and inference latency. Caching, a widely adopted optimization method in
DiT models, leverages the redundancy in the diffusion process to skip
computations in different granularities (e.g., step, cfg, block). Nevertheless,
existing caching methods are limited to single-granularity strategies,
struggling to balance generation quality and inference speed in a flexible
manner. In this work, we propose MixCache, a training-free caching-based
framework for efficient video DiT inference. It first distinguishes the
interference and boundary between different caching strategies, and then
introduces a context-aware cache triggering strategy to determine when caching
should be enabled, along with an adaptive hybrid cache decision strategy for
dynamically selecting the optimal caching granularity. Extensive experiments on
diverse models demonstrate that, MixCache can significantly accelerate video
generation (e.g., 1.94$\times$ speedup on Wan 14B, 1.97$\times$ speedup on
HunyuanVideo) while delivering both superior generation quality and inference
efficiency compared to baseline methods.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [498] [Reduced-order modeling of Hamiltonian dynamics based on symplectic neural networks](https://arxiv.org/abs/2508.11911)
*Yongsheng Chen,Wei Guo,Qi Tang,Xinghui Zhong*

Main category: math.NA

TL;DR: A novel symplectic reduced-order modeling framework using Henon neural networks that preserves Hamiltonian structure while learning latent dynamics end-to-end.


<details>
  <summary>Details</summary>
Motivation: To develop a unified neural architecture that simultaneously discovers latent spaces and learns dynamics while exactly preserving the symplectic structure of Hamiltonian systems for improved fidelity and long-term stability.

Method: End-to-end neural architecture with HenonNets encoder-decoder and symplectic flow map, augmented with linear SGS-reflector layers to create exact symplectic maps between full and latent phase spaces.

Result: Accurate trajectory reconstruction, robust predictive performance beyond training horizon, and exact Hamiltonian preservation demonstrated through comprehensive numerical experiments on canonical Hamiltonian systems.

Conclusion: The framework shows effectiveness and broad applicability potential for complex dynamical systems across scientific and engineering disciplines due to its structure-preserving properties and neural network integration.

Abstract: We introduce a novel data-driven symplectic induced-order modeling (ROM)
framework for high-dimensional Hamiltonian systems that unifies latent-space
discovery and dynamics learning within a single, end-to-end neural
architecture. The encoder-decoder is built from Henon neural networks
(HenonNets) and may be augmented with linear SGS-reflector layers. This yields
an exact symplectic map between full and latent phase spaces. Latent dynamics
are advanced by a symplectic flow map implemented as a HenonNet. This unified
neural architecture ensures exact preservation of the underlying symplectic
structure at the reduced-order level, significantly enhancing the fidelity and
long-term stability of the resulting ROM. We validate our method through
comprehensive numerical experiments on canonical Hamiltonian systems. The
results demonstrate the method's capability for accurate trajectory
reconstruction, robust predictive performance beyond the training horizon, and
accurate Hamiltonian preservation. These promising outcomes underscore the
effectiveness and potential applicability of our symplectic ROM framework for
complex dynamical systems across a broad range of scientific and engineering
disciplines.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [499] [Arabic ASR on the SADA Large-Scale Arabic Speech Corpus with Transformer-Based Models](https://arxiv.org/abs/2508.12968)
*Branislav Gerazov,Marcello Politi,Sébastien Bratières*

Main category: eess.AS

TL;DR: Evaluation of state-of-the-art ASR models on Saudi Arabic dataset shows MMS 1B model with fine-tuning and 4-gram LM achieves best performance (40.9% WER, 17.6% CER).


<details>
  <summary>Details</summary>
Motivation: To assess performance of modern ASR systems on challenging Arabic speech data with multiple dialects and noisy environments from Saudi television content.

Method: Evaluated several ASR models on SADA dataset (668 hours of Saudi TV audio), testing fine-tuning, language models, and noise/denoising impact on performance.

Result: MMS 1B model fine-tuned on SADA with 4-gram language model achieved best results: 40.9% WER and 17.6% CER on clean test set.

Conclusion: Fine-tuning with appropriate language modeling significantly improves ASR performance on challenging Arabic dialect datasets with noise variations.

Abstract: We explore the performance of several state-of-the-art automatic speech
recognition (ASR) models on a large-scale Arabic speech dataset, the SADA
(Saudi Audio Dataset for Arabic), which contains 668 hours of high-quality
audio from Saudi television shows. The dataset includes multiple dialects and
environments, specifically a noisy subset that makes it particularly
challenging for ASR. We evaluate the performance of the models on the SADA test
set, and we explore the impact of fine-tuning, language models, as well as
noise and denoising on their performance. We find that the best performing
model is the MMS 1B model finetuned on SADA with a 4-gram language model that
achieves a WER of 40.9\% and a CER of 17.6\% on the SADA test clean set.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [500] [Deep Semantic Inference over the Air: An Efficient Task-Oriented Communication System](https://arxiv.org/abs/2508.12748)
*Chenyang Wang,Roger Olsson,Stefan Forsström,Qing He*

Main category: cs.IT

TL;DR: Deep learning-based semantic communication framework for wireless systems that optimizes classification accuracy, computational latency, and communication cost through model partitioning and semantic feature compression.


<details>
  <summary>Details</summary>
Motivation: To shift from transmitting raw data to conveying task-relevant meaning in wireless communication, enabling more efficient and intelligent systems by jointly considering performance metrics and resource constraints.

Method: Used ResNets-based models on CIFAR-10 and CIFAR-100 datasets, partitioned models at various points to simulate split inference over wireless channels, and varied split locations and semantic feature vector sizes to analyze trade-offs.

Result: The system achieved over 85% of baseline accuracy while significantly reducing computational load and communication overhead through appropriate model partitioning and semantic feature compression.

Conclusion: Semantic communication with deep learning enables efficient wireless systems by balancing task performance with resource efficiency through strategic model partitioning and feature compression techniques.

Abstract: Empowered by deep learning, semantic communication marks a paradigm shift
from transmitting raw data to conveying task-relevant meaning, enabling more
efficient and intelligent wireless systems. In this study, we explore a deep
learning-based task-oriented communication framework that jointly considers
classification performance, computational latency, and communication cost. We
adopt ResNets-based models and evaluate them on the CIFAR-10 and CIFAR-100
datasets to simulate real-world classification tasks in wireless environments.
We partition the model at various points to simulate split inference across a
wireless channel. By varying the split location and the size of the transmitted
semantic feature vector, we systematically analyze the trade-offs between task
accuracy and resource efficiency. Experimental results show that, with
appropriate model partitioning and semantic feature compression, the system can
retain over 85\% of baseline accuracy while significantly reducing both
computational load and communication overhead.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [501] [What Matters for Bioacoustic Encoding](https://arxiv.org/abs/2508.11845)
*Marius Miron,David Robinson,Milad Alizadeh,Ellen Gilsenan-McMahon,Gagan Narula,Olivier Pietquin,Matthieu Geist,Emmanuel Chemla,Maddie Cusimano,Felix Effenberger,Masato Hagiwara,Benjamin Hoffman,Sara Keen,Diane Kim,Jane Lawton,Jen-Yu Liu,Aza Raskin*

Main category: cs.SD

TL;DR: Large-scale study on bioacoustic encoders showing that self-supervised pre-training followed by supervised training on diverse audio data yields state-of-the-art performance across 26 bioacoustic tasks.


<details>
  <summary>Details</summary>
Motivation: Bioacoustic tasks suffer from limited annotated data, and existing encoders are limited in scope (focusing mainly on birds), architecture diversity, and evaluation breadth. There's a need for general-purpose bioacoustic encoders that can handle diverse species and tasks.

Method: Conducted large-scale empirical study covering training data diversity/scale, model architectures, and training recipes. Used self-supervised pre-training followed by supervised post-training on mixed bioacoustics + general-audio corpus across 26 datasets.

Result: Achieved state-of-the-art performance on existing and proposed benchmarks. Identified that data diversity in both pre-training and post-training stages is crucial for strong in-distribution and out-of-distribution performance.

Conclusion: The proposed training approach with diverse data yields the best bioacoustic encoders. The study provides a foundation for future extensions with more data or better architectures, and model checkpoints will be released to support ongoing research.

Abstract: Bioacoustics, the study of sounds produced by living organisms, plays a vital
role in conservation, biodiversity monitoring, and behavioral studies. Many
tasks in this field, such as species, individual, and behavior classification
and detection, are well-suited to machine learning. However, they often suffer
from limited annotated data, highlighting the need for a general-purpose
bioacoustic encoder capable of extracting useful representations for diverse
downstream tasks. Such encoders have been proposed before, but are often
limited in scope due to a focus on a narrow range of species (typically birds),
and a reliance on a single model architecture or training paradigm. Moreover,
they are usually evaluated on a small set of tasks and datasets. In this work,
we present a large-scale empirical study that covers aspects of bioacoustics
that are relevant to research but have previously been scarcely considered:
training data diversity and scale, model architectures and training recipes,
and the breadth of evaluation tasks and datasets. We obtain encoders that are
state-of-the-art on the existing and proposed benchmarks. We also identify what
matters for training these encoders, such that this work can be extended when
more data are available or better architectures are proposed. Specifically,
across 26 datasets with tasks including species classification, detection,
individual ID, and vocal repertoire discovery, we find self-supervised
pre-training followed by supervised post-training on a mixed bioacoustics +
general-audio corpus yields the strongest in- and out-of-distribution
performance. We show the importance of data diversity in both stages. To
support ongoing research and application, we will release the model
checkpoints.

</details>


### [502] [Audio Flamingo Sound-CoT Technical Report: Improving Chain-of-Thought Reasoning in Sound Understanding](https://arxiv.org/abs/2508.11818)
*Zhifeng Kong,Arushi Goel,Joao Felipe Santos,Sreyan Ghosh,Rafael Valle,Wei Ping,Bryan Catanzaro*

Main category: cs.SD

TL;DR: This paper explores chain-of-thought reasoning for audio language models, proposing a new benchmark (AF-Reasoning-Eval) and training dataset (AF-CoT-Train) that significantly improves audio reasoning capabilities.


<details>
  <summary>Details</summary>
Motivation: Chain-of-thought reasoning has shown great success in language and vision models, but its potential for audio language models remains unexplored. The authors aim to bridge this gap by developing specialized benchmarks and training data for sound reasoning.

Method: Proposed AF-Reasoning-Eval benchmark for sound reasoning assessment, created automatic pipelines to transform existing audio QA/classification data into explicit reasoning chains (AF-CoT-Train with 1.24M samples), and finetuned Audio Flamingo models on this data.

Result: Considerable improvements observed on several reasoning benchmarks after finetuning Audio Flamingo series on AF-CoT-Train, validating the effectiveness of chain-of-thought finetuning for advanced sound understanding.

Conclusion: Chain-of-thought finetuning is effective for enhancing audio language models' reasoning capabilities, and the proposed benchmarks and training data successfully address the previously unexplored area of sound reasoning in multimodal models.

Abstract: Chain-of-thought reasoning has demonstrated significant improvements in large
language models and vision language models, yet its potential for audio
language models remains largely unexplored. In this technical report, we take a
preliminary step towards closing this gap. For better assessment of sound
reasoning, we propose AF-Reasoning-Eval, a benchmark targeting common-sense
reasoning and the ability to discriminate among closely related choices. To
prepare training corpus for sound reasoning abilities, we propose automatic
pipelines that transform existing audio question answering and classification
data into explicit reasoning chains, yielding AF-CoT-Train with 1.24M samples.
We study the effect of finetuning Audio Flamingo series on AF-CoT-Train and
observe considerable improvements on several reasoning benchmarks, validating
the effectiveness of chain-of-thought finetuning on advanced sound
understanding.

</details>


### [503] [Optimizing Neural Architectures for Hindi Speech Separation and Enhancement in Noisy Environments](https://arxiv.org/abs/2508.12009)
*Arnav Ramamoorthy*

Main category: cs.SD

TL;DR: Improved Hindi speech separation using DEMUCS model with U-Net/LSTM layers, achieving better clarity in noisy conditions while optimizing for edge device deployment through quantization.


<details>
  <summary>Details</summary>
Motivation: Address challenges of Hindi speech separation and enhancement for edge devices, overcoming limitations of traditional methods in Indian acoustic environments.

Method: Refined DEMUCS model with U-Net and LSTM layers, trained on 400,000 Hindi speech clips augmented with ESC-50 and MS-SNSD datasets for diverse noise conditions.

Result: Superior performance in PESQ and STOI metrics, particularly under extreme noise conditions, with successful quantization for resource-constrained edge devices.

Conclusion: Customized AI algorithms are effective for Hindi speech processing, with future directions focusing on optimizing edge-based architectures for real-world deployment.

Abstract: This paper addresses the challenges of Hindi speech separation and
enhancement using advanced neural network architectures, with a focus on edge
devices. We propose a refined approach leveraging the DEMUCS model to overcome
limitations of traditional methods, achieving substantial improvements in
speech clarity and intelligibility. The model is fine-tuned with U-Net and LSTM
layers, trained on a dataset of 400,000 Hindi speech clips augmented with
ESC-50 and MS-SNSD for diverse acoustic environments. Evaluation using PESQ and
STOI metrics shows superior performance, particularly under extreme noise
conditions. To ensure deployment on resource-constrained devices like TWS
earbuds, we explore quantization techniques to reduce computational
requirements. This research highlights the effectiveness of customized AI
algorithms for speech processing in Indian contexts and suggests future
directions for optimizing edge-based architectures.

</details>


### [504] [HuBERT-VIC: Improving Noise-Robust Automatic Speech Recognition of Speech Foundation Model via Variance-Invariance-Covariance Regularization](https://arxiv.org/abs/2508.12292)
*Hyebin Ahn,Kangwook Jang,Hoirin Kim*

Main category: cs.SD

TL;DR: HuBERT-VIC improves noise robustness in speech foundation models using variance, invariance, and covariance regularization to handle noisy speech better than baseline models.


<details>
  <summary>Details</summary>
Motivation: Speech foundation models degrade in noisy environments since they're primarily trained on clean data, creating a need for better noise robustness.

Method: Proposes HuBERT-VIC with VICReg objectives that adjust noisy speech representation statistics to capture diverse acoustic characteristics and improve generalization across noise types.

Result: Achieves 23.3% relative improvement on LibriSpeech test-clean and 13.2% on test-other compared to baseline models pre-trained on noisy speech.

Conclusion: VICReg regularization effectively enhances noise robustness in speech foundation models, demonstrating significant performance improvements in noisy conditions.

Abstract: Noise robustness in speech foundation models (SFMs) has been a critical
challenge, as most models are primarily trained on clean data and experience
performance degradation when the models are exposed to noisy speech. To address
this issue, we propose HuBERT-VIC, a noise-robust SFM with variance,
in-variance, and covariance regularization (VICReg) objectives. These
objectives adjust the statistics of noisy speech representations, enabling the
model to capture diverse acoustic characteristics and improving the
generalization ability across different types of noise. When applied to HuBERT,
our model shows relative performance improvements of 23.3% on LibriSpeech
test-clean and 13.2% on test-other, compared to the baseline model pre-trained
on noisy speech.

</details>


### [505] [MATPAC++: Enhanced Masked Latent Prediction for Self-Supervised Audio Representation Learning](https://arxiv.org/abs/2508.12709)
*Aurian Quelennec,Pierre Chouteau,Geoffroy Peeters,Slim Essid*

Main category: cs.SD

TL;DR: MATPAC++ enhances masked audio SSL with Multiple Choice Learning to handle prediction ambiguity, achieving SOTA performance on AudioSet and downstream tasks with improved efficiency.


<details>
  <summary>Details</summary>
Motivation: The predictor module in masked latent prediction SSL systems is crucial but often overlooked, especially for handling ambiguity in audio content with multiple sound sources.

Method: Integrates Multiple Choice Learning (MCL) into MATPAC system to explicitly model prediction ambiguity, improving both prediction and unsupervised classification pretext tasks.

Result: Achieves state-of-the-art performance when fine-tuned on AudioSet and overall SOTA scores on downstream tasks. Music-only training also achieves SOTA with significantly improved efficiency.

Conclusion: MCL effectively addresses prediction ambiguity in audio SSL, leading to superior representation quality and performance across various audio domains.

Abstract: Masked latent prediction has emerged as a leading paradigm in self-supervised
learning (SSL), especially for general audio and music representation learning.
While recent methods have demonstrated strong performance, the role of the
predictor module used at the output of such SSL systems remains mainly
overlooked, despite being crucial for solving the pretext task at hand. In
particular, this module should be able to deal with the ambiguity inherent in
audio content, especially when it is composed of multiple sound sources. This
work proposes a novel enhancement: integrating Multiple Choice Learning (MCL)
to explicitly model prediction ambiguity and improve representation quality. We
build on top of the recently proposed MATPAC system, improving its prediction
and unsupervised classification pretext tasks with MCL. We extensively evaluate
our method, MATPAC++, through both linear probing across multiple downstream
tasks and fine-tuning on AudioSet, employing a unified protocol that enables
rigorous and fair comparisons with state-of-the-art SSL approaches. Results
show that our proposal achieves state-of-the-art when fine-tuned on AudioSet
and overall state-of-the-art scores on downstream tasks. Additionally, we
examine domain specialisation by training exclusively on music data, where our
model achieves state-of-the-art performance with significantly improved
efficiency.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [506] [A Hierarchical Surrogate Model for Efficient Multi-Task Parameter Learning in Closed-Loop Contro](https://arxiv.org/abs/2508.12738)
*Sebastian Hirt,Lukas Theiner,Maik Pfefferkorn,Rolf Findeisen*

Main category: eess.SY

TL;DR: Hierarchical Bayesian optimization framework for efficient controller parameter learning across distinct closed-loop tasks, leveraging structural knowledge of dynamical systems rather than treating costs as black-box functions.


<details>
  <summary>Details</summary>
Motivation: Many control problems require repeated tuning and adaptation of controllers across distinct tasks where data efficiency and adaptability are critical, but standard black-box approaches lack structural knowledge exploitation.

Method: Construct hierarchical Gaussian process surrogate models that capture closed-loop state evolution under different parameterizations, while computing task-specific cost weighting and accumulation exactly via known closed-form expressions.

Result: The framework achieves substantial benefits in both sample efficiency and adaptability compared to purely black-box BO approaches, while retaining sublinear regret guarantees.

Conclusion: The proposed hierarchical BO framework enables effective knowledge transfer and enhanced data efficiency between different closed-loop tasks while maintaining theoretical performance guarantees.

Abstract: Many control problems require repeated tuning and adaptation of controllers
across distinct closed-loop tasks, where data efficiency and adaptability are
critical. We propose a hierarchical Bayesian optimization (BO) framework that
is tailored to efficient controller parameter learning in sequential
decision-making and control scenarios for distinct tasks. Instead of treating
the closed-loop cost as a black-box, our method exploits structural knowledge
of the underlying problem, consisting of a dynamical system, a control law, and
an associated closed-loop cost function. We construct a hierarchical surrogate
model using Gaussian processes that capture the closed-loop state evolution
under different parameterizations, while the task-specific weighting and
accumulation into the closed-loop cost are computed exactly via known
closed-form expressions. This allows knowledge transfer and enhanced data
efficiency between different closed-loop tasks. The proposed framework retains
sublinear regret guarantees on par with standard black-box BO, while enabling
multi-task or transfer learning. Simulation experiments with model predictive
control demonstrate substantial benefits in both sample efficiency and
adaptability when compared to purely black-box BO approaches.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [507] [Toward Practical Equilibrium Propagation: Brain-inspired Recurrent Neural Network with Feedback Regulation and Residual Connections](https://arxiv.org/abs/2508.11659)
*Zhuo Liu,Tao Chen*

Main category: cs.NE

TL;DR: A biologically plausible Feedback-regulated REsidual RNN (FRE-RNN) that dramatically improves Equilibrium Propagation by reducing computational costs and training time while achieving backpropagation-level performance.


<details>
  <summary>Details</summary>
Motivation: Existing Equilibrium Propagation implementations suffer from instability and prohibitively high computational costs, limiting their practical application in brain-inspired computing hardware.

Method: Proposed FRE-RNN with feedback regulation to reduce spectral radius for rapid convergence, and residual connections with brain-inspired topologies to alleviate vanishing gradient problems in deep RNNs.

Result: Achieves orders of magnitude reduction in computational cost and training time while delivering performance on par with backpropagation in benchmark tasks.

Conclusion: Substantially enhances applicability and practicality of EP in large-scale networks and provides guidance for implementing in-situ learning in physical neural networks.

Abstract: Brain-like intelligent systems need brain-like learning methods. Equilibrium
Propagation (EP) is a biologically plausible learning framework with strong
potential for brain-inspired computing hardware. However, existing
im-plementations of EP suffer from instability and prohibi-tively high
computational costs. Inspired by the structure and dynamics of the brain, we
propose a biologically plau-sible Feedback-regulated REsidual recurrent neural
network (FRE-RNN) and study its learning performance in EP framework. Feedback
regulation enables rapid convergence by reducing the spectral radius. The
improvement in con-vergence property reduces the computational cost and
train-ing time of EP by orders of magnitude, delivering perfor-mance on par
with backpropagation (BP) in benchmark tasks. Meanwhile, residual connections
with brain-inspired topologies help alleviate the vanishing gradient problem
that arises when feedback pathways are weak in deep RNNs. Our approach
substantially enhances the applicabil-ity and practicality of EP in large-scale
networks that un-derpin artificial intelligence. The techniques developed here
also offer guidance to implementing in-situ learning in physical neural
networks.

</details>


### [508] [Learning Internal Biological Neuron Parameters and Complexity-Based Encoding for Improved Spiking Neural Networks Performance](https://arxiv.org/abs/2508.11674)
*Zofia Rudnicka,Janusz Szczepanski,Agnieszka Pregowska*

Main category: cs.NE

TL;DR: Novel probabilistic meta neuron model improves SNN classification accuracy by up to 11% when combined with Lempel-Ziv complexity for spatiotemporal neural data analysis.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of traditional perceptron neuron models and improve classification accuracy of spiking neural networks by incorporating biologically inspired probabilistic neurons and entropy-based complexity measures.

Method: Developed probabilistic meta neuron model with jointly learned internal parameters, implemented two SNN architectures (LIF neurons vs probabilistic meta neurons), integrated Lempel-Ziv complexity with SNNs, used backpropagation/STDP/Tempotron learning rules, and modeled spike trains with Poisson processes.

Result: Classification efficiency improved by up to 11.00% depending on training method, demonstrating advantage of learning additional neuron parameters beyond traditional weighted inputs.

Conclusion: The probabilistic meta neuron approach combined with Lempel-Ziv complexity enables more efficient and interpretable classification of spatiotemporal neural data, addressing a gap in existing literature.

Abstract: This study introduces a novel approach by replacing the traditional
perceptron neuron model with a biologically inspired probabilistic meta neuron,
where the internal neuron parameters are jointly learned, leading to improved
classification accuracy of spiking neural networks (SNNs). To validate this
innovation, we implement and compare two SNN architectures: one based on
standard leaky integrate-and-fire (LIF) neurons and another utilizing the
proposed probabilistic meta neuron model. As a second key contribution, we
present a new biologically inspired classification framework that uniquely
integrates SNNs with Lempel-Ziv complexity (LZC) a measure closely related to
entropy rate. By combining the temporal precision and biological plausibility
of SNNs with the capacity of LZC to capture structural regularity, the proposed
approach enables efficient and interpretable classification of spatiotemporal
neural data, an aspect not addressed in existing works. We consider learning
algorithms such as backpropagation, spike-timing-dependent plasticity (STDP),
and the Tempotron learning rule. To explore neural dynamics, we use Poisson
processes to model neuronal spike trains, a well-established method for
simulating the stochastic firing behavior of biological neurons. Our results
reveal that depending on the training method, the classifier's efficiency can
improve by up to 11.00%, highlighting the advantage of learning additional
neuron parameters beyond the traditional focus on weighted inputs alone.

</details>


### [509] [Adaptive Spiking with Plasticity for Energy Aware Neuromorphic Systems](https://arxiv.org/abs/2508.11689)
*Eduardo Calle-Ortiz,Hui Guan,Deepak Ganesan,Phuc Nguyen*

Main category: cs.NE

TL;DR: ASPEN is an energy-aware neuromorphic technique that uses stochastic threshold perturbations during training to reduce spike counts and energy consumption in wearables while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: Neuromorphic systems for wearables need ultra-low-power operation. Since energy consumption is tied to spiking activity, minimizing spikes is crucial for energy-constrained always-on applications.

Method: ASPEN applies stochastic perturbations to neuronal thresholds during training to enhance robustness, improve generalization, and reduce spiking activity. It adaptively adjusts intrinsic neuronal parameters for dynamic energy control without model reconfiguration.

Result: Evaluation on neuromorphic emulator and hardware shows ASPEN significantly reduces spike counts and energy consumption while maintaining accuracy comparable to state-of-the-art methods.

Conclusion: ASPEN provides a lightweight, scalable technique for energy control in neuromorphic wearables without complex retraining or pruning, enabling efficient always-on operation.

Abstract: This paper presents ASPEN, a novel energy-aware technique for neuromorphic
systems that could unleash the future of intelligent, always-on,
ultra-low-power, and low-burden wearables. Our main research objectives are to
explore the feasibility of neuromorphic computing for wearables, identify open
research directions, and demonstrate the feasibility of developing an adaptive
spiking technique for energy-aware computation, which can be game-changing for
resource-constrained devices in always-on applications. As neuromorphic
computing systems operate based on spike events, their energy consumption is
closely related to spiking activity, i.e., each spike incurs computational and
power costs; consequently, minimizing the number of spikes is a critical
strategy for operating under constrained energy budgets. To support this goal,
ASPEN utilizes stochastic perturbations to the neuronal threshold during
training to not only enhance the network's robustness across varying
thresholds, which can be controlled at inference time, but also act as a
regularizer that improves generalization, reduces spiking activity, and enables
energy control without the need for complex retraining or pruning. More
specifically, ASPEN adaptively adjusts intrinsic neuronal parameters as a
lightweight and scalable technique for dynamic energy control without
reconfiguring the entire model. Our evaluation on neuromorphic emulator and
hardware shows that ASPEN significantly reduces spike counts and energy
consumption while maintaining accuracy comparable to state-of-the-art methods.

</details>


### [510] [Data-Driven Discovery of Interpretable Kalman Filter Variants through Large Language Models and Genetic Programming](https://arxiv.org/abs/2508.11703)
*Vasileios Saketos,Sebastian Kaltenbach,Sergey Litvinov,Petros Koumoutsakos*

Main category: cs.NE

TL;DR: Automated discovery of Kalman Filter using evolutionary programming (CGP) and LLMs, showing convergence to optimal solutions when assumptions hold and superior alternatives when assumptions are violated.


<details>
  <summary>Details</summary>
Motivation: To investigate whether scientific computing algorithms like Kalman Filter can be discovered through automated, data-driven evolutionary processes rather than traditional human-driven experimentation.

Method: Combined Cartesian Genetic Programming (CGP) with Large Language Models (LLM) in an evolutionary framework to discover and optimize the Kalman Filter algorithm.

Result: Framework converges to near-optimal solutions when Kalman assumptions hold, and evolves interpretable alternatives that outperform Kalman Filter when assumptions are violated.

Conclusion: Combining evolutionary algorithms and generative models enables effective data-driven synthesis of computational modules for algorithmic discovery in scientific computing.

Abstract: Algorithmic discovery has traditionally relied on human ingenuity and
extensive experimentation. Here we investigate whether a prominent scientific
computing algorithm, the Kalman Filter, can be discovered through an automated,
data-driven, evolutionary process that relies on Cartesian Genetic Programming
(CGP) and Large Language Models (LLM). We evaluate the contributions of both
modalities (CGP and LLM) in discovering the Kalman filter under varying
conditions. Our results demonstrate that our framework of CGP and LLM-assisted
evolution converges to near-optimal solutions when Kalman optimality
assumptions hold. When these assumptions are violated, our framework evolves
interpretable alternatives that outperform the Kalman filter. These results
demonstrate that combining evolutionary algorithms and generative models for
interpretable, data-driven synthesis of simple computational modules is a
potent approach for algorithmic discovery in scientific computing.

</details>


### [511] [A Self-Ensemble Inspired Approach for Effective Training of Binary-Weight Spiking Neural Networks](https://arxiv.org/abs/2508.12609)
*Qingyan Meng,Mingqing Xiao,Zhengyu Ma,Huihui Zhou,Yonghong Tian,Zhouchen Lin*

Main category: cs.NE

TL;DR: This paper presents a novel perspective connecting Spiking Neural Networks (SNNs) and Binary Neural Networks (BNNs), introducing SEI-BWSNN training method that treats SNN training as training a self-ensemble of binary-activation networks with noise injection, achieving high performance with low latency even for 1-bit weights.


<details>
  <summary>Details</summary>
Motivation: SNNs offer energy efficiency for neuromorphic hardware but face training challenges due to non-differentiable spike functions. Similarly, BNNs struggle with non-differentiability. The deep relationship between these fields and how their training techniques can benefit each other hasn't been systematically researched, especially for binary-weight SNNs which are even more difficult to train.

Method: The authors analyze SNN dynamics through backpropagation and demonstrate that training feedforward SNNs can be viewed as training a self-ensemble of binary-activation neural networks with noise injection. They introduce SEI-BWSNN method leveraging multiple shortcuts and knowledge distillation-based training to improve binary-weight SNN training.

Result: The approach achieves 82.52% accuracy on ImageNet with only 2 time steps by binarizing FFN layers in a Transformer architecture, demonstrating high performance with low latency even for 1-bit weights.

Conclusion: The work provides a novel understanding of SNN dynamics and their connection to BNNs, showing the effectiveness of the proposed methodology and the potential of binary-weight SNNs for efficient neuromorphic computing applications.

Abstract: Spiking Neural Networks (SNNs) are a promising approach to low-power
applications on neuromorphic hardware due to their energy efficiency. However,
training SNNs is challenging because of the non-differentiable spike generation
function. To address this issue, the commonly used approach is to adopt the
backpropagation through time framework, while assigning the gradient of the
non-differentiable function with some surrogates. Similarly, Binary Neural
Networks (BNNs) also face the non-differentiability problem and rely on
approximating gradients. However, the deep relationship between these two
fields and how their training techniques can benefit each other has not been
systematically researched. Furthermore, training binary-weight SNNs is even
more difficult. In this work, we present a novel perspective on the dynamics of
SNNs and their close connection to BNNs through an analysis of the
backpropagation process. We demonstrate that training a feedforward SNN can be
viewed as training a self-ensemble of a binary-activation neural network with
noise injection. Drawing from this new understanding of SNN dynamics, we
introduce the Self-Ensemble Inspired training method for (Binary-Weight) SNNs
(SEI-BWSNN), which achieves high-performance results with low latency even for
the case of the 1-bit weights. Specifically, we leverage a structure of
multiple shortcuts and a knowledge distillation-based training technique to
improve the training of (binary-weight) SNNs. Notably, by binarizing FFN layers
in a Transformer architecture, our approach achieves 82.52% accuracy on
ImageNet with only 2 time steps, indicating the effectiveness of our
methodology and the potential of binary-weight SNNs.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [512] [Benchmark Dataset Generation and Evaluation for Excel Formula Repair with LLMs](https://arxiv.org/abs/2508.11715)
*Ananya Singha,Harshita Sahijwani,Walt Williams,Emmanuel Aboah Boateng,Nick Hausman,Miguel Di Luca,Keegan Choudhury,Chaya Binet,Vu Le,Tianwei Chen,Oryan Rokeah Chen,Sulaiman Vesal,Sadid Hasan*

Main category: cs.SE

TL;DR: This paper introduces a novel benchmark dataset and generation pipeline for Excel formula repair, addressing the lack of high-quality datasets for training and evaluating models that fix semantic runtime errors in Excel formulas.


<details>
  <summary>Details</summary>
Motivation: Excel is complex for novice users who often make logical errors in formulas, but there's a severe lack of comprehensive datasets to train and evaluate models for automated Excel formula repair.

Method: Proposed a data generation pipeline using few-shot prompting with LLMs and LLM-as-a-Judge validation framework with execution-based checks. Also developed a context-aware baseline technique that uses LLMs to repair formulas using both faulty formulas and spreadsheet context.

Result: Created a benchmark dataset of 618 high-quality samples covering common runtime errors. Evaluated various LLMs (GPT-4o, GPT-4.1, Phi-3, Mistral) using execution-based metrics, with manual annotation confirming dataset quality.

Conclusion: The generation methodology is scalable and adaptable for creating evaluation benchmarks for similar code repair tasks in other low-resource programming languages.

Abstract: Excel is a pervasive yet often complex tool, particularly for novice users,
where runtime errors arising from logical mistakes or misinterpretations of
functions pose a significant challenge. While large language models (LLMs)
offer promising assistance by explaining formula errors, the automated
correction of these semantic runtime errors remains an open problem. A primary
challenge to advancing models for such scenarios is the severe lack of
high-quality, comprehensive datasets for training and rigorous evaluation. This
paper addresses this gap by introducing a novel approach for constructing a
benchmark dataset specifically designed for Excel formula repair. We propose a
data generation pipeline, which leverages a small set of curated seed samples
from online forums to synthetically expand the dataset. Our pipeline integrates
few-shot prompting with LLMs and employs a robust \textit{LLM-as-a-Judge}
validation framework, combined with execution-based checks to ensure the
correctness and semantic fidelity of the generated data. This process produced
a benchmark dataset of 618 high-quality samples, covering common runtime
errors. Furthermore, we propose a context-aware baseline technique for Excel
formula repair that utilizes LLMs to leverage both the faulty formula, and
relevant spreadsheet context. We evaluate the performance of various LLMs
(GPT-4o, GPT-4.1, Phi-3, Mistral) on our newly generated benchmark using
execution-based metrics. Our analysis demonstrates the dataset's quality
through manual annotation and provides insights into error and function
distributions. The proposed generation methodology is highly scalable and can
be readily adapted to create evaluation benchmarks for similar code repair
tasks in other low-resource programming languages.

</details>


### [513] [Rethinking Autonomy: Preventing Failures in AI-Driven Software Engineering](https://arxiv.org/abs/2508.11824)
*Satyam Kumar Navneet,Joydeep Chandra*

Main category: cs.SE

TL;DR: The paper proposes the SAFE-AI Framework to address security and safety risks in LLM-assisted code generation, focusing on safety, auditability, feedback, and explainability to enable responsible AI integration in software engineering.


<details>
  <summary>Details</summary>
Motivation: The integration of LLMs into software engineering introduces significant risks including insecure code generation, hallucinations, irreversible actions, and lack of transparency, as demonstrated by incidents like the Replit database deletion.

Method: The authors propose the SAFE-AI Framework that integrates guardrails, sandboxing, runtime verification, risk-aware logging, human-in-the-loop systems, and explainable AI techniques. They also introduce a novel taxonomy of AI behaviors categorizing suggestive, generative, autonomous, and destructive actions.

Result: The paper provides a comprehensive analysis of LLM-assisted code generation challenges and proposes a holistic framework with detailed comparisons of autonomy control, prompt engineering, explainability, and governance frameworks.

Conclusion: The SAFE-AI Framework provides a roadmap for responsible AI integration in software engineering, aligning with emerging regulations to ensure safe, transparent, and accountable AI-driven development, while identifying open problems and future research directions.

Abstract: The integration of Large Language Models (LLMs) into software engineering has
revolutionized code generation, enabling unprecedented productivity through
promptware and autonomous AI agents. However, this transformation introduces
significant risks, including insecure code generation, hallucinated outputs,
irreversible actions, and a lack of transparency and accountability. Incidents
like the Replit database deletion underscore the urgent need for robust safety
and governance mechanisms. This paper comprehensively analyzes the inherent
challenges of LLM-assisted code generation, such as vulnerability inheritance,
overtrust, misinterpretation, and the absence of standardized validation and
rollback protocols. To address these, we propose the SAFE-AI Framework, a
holistic approach emphasizing Safety, Auditability, Feedback, and
Explainability. The framework integrates guardrails, sandboxing, runtime
verification, risk-aware logging, human-in-the-loop systems, and explainable AI
techniques to mitigate risks while fostering trust and compliance. We introduce
a novel taxonomy of AI behaviors categorizing suggestive, generative,
autonomous, and destructive actions to guide risk assessment and oversight.
Additionally, we identify open problems, including the lack of standardized
benchmarks for code specific hallucinations and autonomy levels, and propose
future research directions for hybrid verification, semantic guardrails, and
proactive governance tools. Through detailed comparisons of autonomy control,
prompt engineering, explainability, and governance frameworks, this paper
provides a roadmap for responsible AI integration in software engineering,
aligning with emerging regulations like the EU AI Act and Canada's AIDA to
ensure safe, transparent, and accountable AI-driven development.

</details>


### [514] [AI-Augmented CI/CD Pipelines: From Code Commit to Production with Autonomous Decisions](https://arxiv.org/abs/2508.11867)
*Mohammad Baqar,Saba Naqvi,Rajat Khanda*

Main category: cs.SE

TL;DR: AI-Augmented CI/CD Pipelines use LLMs and autonomous agents as policy-bounded co-pilots to automate human decision points in software delivery, reducing latency and operational toil.


<details>
  <summary>Details</summary>
Motivation: Human decision points in CI/CD pipelines (interpreting flaky tests, rollback strategies, feature flag tuning, canary promotion) remain major sources of latency and operational toil despite mature tooling.

Method: Proposes a reference architecture for embedding agentic decision points, decision taxonomy with policy-as-code guardrails, trust-tier framework for staged autonomy, and evaluation using DORA metrics and AI-specific indicators.

Result: Includes a detailed industrial-style case study migrating a React 19 microservice to an AI-augmented pipeline, demonstrating practical implementation.

Conclusion: The paper charts a roadmap for verifiable autonomy in production delivery systems while addressing ethics, verification, auditability, and validity threats.

Abstract: Modern software delivery has accelerated from quarterly releases to multiple
deployments per day. While CI/CD tooling has matured, human decision points
interpreting flaky tests, choosing rollback strategies, tuning feature flags,
and deciding when to promote a canary remain major sources of latency and
operational toil. We propose AI-Augmented CI/CD Pipelines, where large language
models (LLMs) and autonomous agents act as policy-bounded co-pilots and
progressively as decision makers. We contribute: (1) a reference architecture
for embedding agentic decision points into CI/CD, (2) a decision taxonomy and
policy-as-code guardrail pattern, (3) a trust-tier framework for staged
autonomy, (4) an evaluation methodology using DevOps Research and Assessment (
DORA) metrics and AI-specific indicators, and (5) a detailed industrial-style
case study migrating a React 19 microservice to an AI-augmented pipeline. We
discuss ethics, verification, auditability, and threats to validity, and chart
a roadmap for verifiable autonomy in production delivery systems.

</details>


### [515] [LinkAnchor: An Autonomous LLM-Based Agent for Issue-to-Commit Link Recovery](https://arxiv.org/abs/2508.12232)
*Arshia Akhavan,Alireza Hosseinpour,Abbas Heydarnoori,Mehdi Keshani*

Main category: cs.SE

TL;DR: LinkAnchor is an autonomous LLM-based agent that significantly improves issue-to-commit link recovery by using lazy-access architecture to handle large software contexts and automatically pinpoint target commits without exhaustive search.


<details>
  <summary>Details</summary>
Motivation: Only 42.2% of GitHub issues are correctly linked to commits, creating a need for better traceability tools. Existing LLM approaches suffer from limited context windows and impractical pairwise evaluation of issue-commit pairs in large repositories.

Method: LinkAnchor uses lazy-access architecture that dynamically retrieves only the most relevant contextual data (commits, issue comments, code files) without exceeding token limits. It autonomously pinpoints target commits rather than scoring every possible candidate.

Result: LinkAnchor outperforms state-of-the-art approaches by 60-262% in Hit@1 score across all case study projects. It's publicly released as a ready-to-use tool compatible with GitHub and Jira.

Conclusion: LinkAnchor successfully addresses key limitations of previous methods by enabling efficient context access and autonomous commit identification, making issue-to-commit link recovery practical for real-world software repositories.

Abstract: Issue-to-commit link recovery plays an important role in software
traceability and improves project management. However, it remains a challenging
task. A study on GitHub shows that only 42.2% of the issues are correctly
linked to their commits. This highlights the potential for further development
and research in this area. Existing studies have employed various AI/ML-based
approaches, and with the recent development of large language models,
researchers have leveraged LLMs to tackle this problem. These approaches suffer
from two main issues. First, LLMs are constrained by limited context windows
and cannot ingest all of the available data sources, such as long commit
histories, extensive issue comments, and large code repositories. Second, most
methods operate on individual issue-commit pairs; that is, given a single
issue-commit pair, they determine whether the commit resolves the issue. This
quickly becomes impractical in real-world repositories containing tens of
thousands of commits. To address these limitations, we present LinkAnchor, the
first autonomous LLM-based agent designed for issue-to-commit link recovery.
The lazy-access architecture of LinkAnchor enables the underlying LLM to access
the rich context of software, spanning commits, issue comments, and code files,
without exceeding the token limit by dynamically retrieving only the most
relevant contextual data. Additionally, LinkAnchor is able to automatically
pinpoint the target commit rather than exhaustively scoring every possible
candidate. Our evaluations show that LinkAnchor outperforms state-of-the-art
issue-to-commit link recovery approaches by 60-262% in Hit@1 score across all
our case study projects. We also publicly release LinkAnchor as a ready-to-use
tool, along with our replication package. LinkAnchor is designed and tested for
GitHub and Jira, and is easily extendable to other platforms.

</details>


### [516] ["My productivity is boosted, but ..." Demystifying Users' Perception on AI Coding Assistants](https://arxiv.org/abs/2508.12285)
*Yunbo Lyu,Zhou Yang,Jieke Shi,Jianming Chang,Yue Liu,David Lo*

Main category: cs.SE

TL;DR: Analysis of 1,085 AI coding assistants from VS Code Marketplace reveals developers value context-aware, customizable, and efficient AI tools, with over 90% released in past two years. Manual review analysis of 32 popular assistants provides taxonomy of user concerns and satisfaction levels.


<details>
  <summary>Details</summary>
Motivation: To understand what developers truly value and criticize in AI coding assistants in real-world usage contexts, moving beyond controlled studies to analyze authentic user experiences from daily work.

Method: Identified 1,085 AI coding assistants from VS Code Marketplace, manually analyzed user reviews from 32 popular assistants with sufficient installations, and annotated each review's attitude toward specific features and concerns.

Result: Developers demand not just intelligent suggestions but also context-aware, customizable, and resource-efficient interactions. Taxonomy created of user concerns and satisfaction levels with specific features.

Conclusion: Five practical implications and suggestions proposed to guide enhancement of AI coding assistants to better satisfy user needs based on authentic developer feedback from real-world usage.

Abstract: This paper aims to explore fundamental questions in the era when AI coding
assistants like GitHub Copilot are widely adopted: what do developers truly
value and criticize in AI coding assistants, and what does this reveal about
their needs and expectations in real-world software development? Unlike
previous studies that conduct observational research in controlled and
simulated environments, we analyze extensive, first-hand user reviews of AI
coding assistants, which capture developers' authentic perspectives and
experiences drawn directly from their actual day-to-day work contexts. We
identify 1,085 AI coding assistants from the Visual Studio Code Marketplace.
Although they only account for 1.64% of all extensions, we observe a surge in
these assistants: over 90% of them are released within the past two years. We
then manually analyze the user reviews sampled from 32 AI coding assistants
that have sufficient installations and reviews to construct a comprehensive
taxonomy of user concerns and feedback about these assistants. We manually
annotate each review's attitude when mentioning certain aspects of coding
assistants, yielding nuanced insights into user satisfaction and
dissatisfaction regarding specific features, concerns, and overall tool
performance. Built on top of the findings-including how users demand not just
intelligent suggestions but also context-aware, customizable, and
resource-efficient interactions-we propose five practical implications and
suggestions to guide the enhancement of AI coding assistants that satisfy user
needs.

</details>


### [517] [Uncovering Systematic Failures of LLMs in Verifying Code Against Natural Language Specifications](https://arxiv.org/abs/2508.12358)
*Haolin Jin,Huaming Chen*

Main category: cs.SE

TL;DR: LLMs systematically fail at evaluating whether code correctly implements natural language requirements, often misclassifying correct code as defective, with more complex prompting making the problem worse.


<details>
  <summary>Details</summary>
Motivation: LLMs are widely used for code review and requirements validation, but it's unclear if they can reliably determine whether code properly implements natural language specifications.

Method: Used unified prompts on widely used benchmarks to judge code correctness, tested various prompting strategies including explanations and corrections, and analyzed root causes of misjudgments.

Result: LLMs frequently misclassify correct code implementations as either not satisfying requirements or containing defects, with more complex prompting leading to higher misjudgment rates.

Conclusion: LLMs have unrecognized limitations in matching code with requirements, highlighting critical reliability issues for using them as code review assistants, with proposed mitigation strategies.

Abstract: Large language models (LLMs) have become essential tools in software
development, widely used for requirements engineering, code generation and
review tasks. Software engineers often rely on LLMs to assess whether system
code implementation satisfy task requirements, thereby enhancing code
robustness and accuracy. However, it remains unclear whether LLMs can reliably
determine whether the code complies fully with the given task descriptions,
which is usually natural language specifications. In this paper, we uncover a
systematic failure of LLMs in evaluating whether code aligns with natural
language requirements. Specifically, with widely used benchmarks, we employ
unified prompts to judge code correctness. Our results reveal that LLMs
frequently misclassify correct code implementations as either ``not satisfying
requirements'' or containing potential defects. Surprisingly, more complex
prompting, especially when leveraging prompt engineering techniques involving
explanations and proposed corrections, leads to higher misjudgment rate, which
highlights the critical reliability issues in using LLMs as code review
assistants. We further analyze the root causes of these misjudgments, and
propose two improved prompting strategies for mitigation. For the first time,
our findings reveals unrecognized limitations in LLMs to match code with
requirements. We also offer novel insights and practical guidance for effective
use of LLMs in automated code review and task-oriented agent scenarios.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [518] [Insight Rumors: A Novel Textual Rumor Locating and Marking Model Leveraging Att_BiMamba2 Network](https://arxiv.org/abs/2508.12574)
*Bin Ma,Yifei Zhang,Yongjin Xian,Qi Li,Linna Zhou,Gongxun Miao*

Main category: cs.SI

TL;DR: A novel rumor detection model called Insight Rumors that not only classifies but also locates and marks specific rumor content within text using bidirectional Mamba2 networks with attention mechanisms and CRF constraints.


<details>
  <summary>Details</summary>
Motivation: Existing rumor detection models only classify contexts as rumors or not, lacking the capability to locate and mark specific rumor content within textual data.

Method: Proposes Att_BiMamba2 network with bidirectional Mamba2 model and dot-product attention to enhance rumor feature representation, plus a Rumor Locating and Marking module with skip-connection network and Conditional Random Fields for accurate content location.

Result: The model accurately detects rumors and precisely locates/marks them in context, outperforming state-of-the-art schemes that only discriminate rumors roughly.

Conclusion: The proposed Insight Rumors model effectively addresses the limitation of traditional rumor detection by providing both classification and precise content location/marking capabilities.

Abstract: With the development of social media networks, rumor detection models have
attracted more and more attention. Whereas, these models primarily focus on
classifying contexts as rumors or not, lacking the capability to locate and
mark specific rumor content. To address this limitation, this paper proposes a
novel rumor detection model named Insight Rumors to locate and mark rumor
content within textual data. Specifically, we propose the Bidirectional Mamba2
Network with Dot-Product Attention (Att_BiMamba2), a network that constructs a
bidirectional Mamba2 model and applies dot-product attention to weight and
combine the outputs from both directions, thereby enhancing the representation
of high-dimensional rumor features. Simultaneously, a Rumor Locating and
Marking module is designed to locate and mark rumors. The module constructs a
skip-connection network to project high-dimensional rumor features onto
low-dimensional label features. Moreover, Conditional Random Fields (CRF) is
employed to impose strong constraints on the output label features, ensuring
accurate rumor content location. Additionally, a labeled dataset for rumor
locating and marking is constructed, with the effectiveness of the proposed
model is evaluated through comprehensive experiments. Extensive experiments
indicate that the proposed scheme not only detects rumors accurately but also
locates and marks them in context precisely, outperforming state-of-the-art
schemes that can only discriminate rumors roughly.

</details>


### [519] [On Balancing Sparsity with Reliable Connectivity in Distributed Network Design with Random K-out Graphs](https://arxiv.org/abs/2508.11863)
*Mansi Sood,Eray Can Elumar,Osman Yagan*

Main category: cs.SI

TL;DR: The paper provides theoretical analysis of random K-out graphs for balancing network sparsity and connectivity in distributed systems, with applications to privacy-preserving data aggregation and resilient consensus.


<details>
  <summary>Details</summary>
Motivation: Random K-out graphs are widely used as heuristics for sparse yet connected networks in distributed systems, but lack rigorous parameter selection guidelines for finite nodes and adversarial environments common in privacy-preserving applications.

Method: Derived upper/lower bounds for connectivity probability in finite random K-out graphs, analyzed r-robustness property for resilient consensus, and modeled adversarial node impact on connectivity and giant component size.

Result: Theoretical guarantees for network parameter selection that ensure reliable connectivity in finite and unreliable node regimes, enabling resilient consensus and maintaining privacy guarantees despite adversarial nodes.

Conclusion: The results provide end-to-end performance guarantees for reliable inference algorithms on networks, addressing the gap between heuristic network design and rigorous operational requirements in distributed systems.

Abstract: In several applications in distributed systems, an important design criterion
is ensuring that the network is sparse, i.e., does not contain too many edges,
while achieving reliable connectivity. Sparsity ensures communication overhead
remains low, while reliable connectivity is tied to reliable communication and
inference on decentralized data reservoirs and computational resources. A class
of network models called random K-out graphs appear widely as a heuristic to
balance connectivity and sparsity, especially in settings with limited trust,
e.g., privacy-preserving aggregation of networked data in which networks are
deployed. However, several questions remain regarding how to choose network
parameters in response to different operational requirements, including the
need to go beyond asymptotic results and the ability to model the stochastic
and adversarial environments. To address this gap, we present theorems to
inform the choice of network parameters that guarantee reliable connectivity in
regimes where nodes can be finite or unreliable. We first derive upper and
lower bounds for probability of connectivity in random K-out graphs when the
number of nodes is finite. Next, we analyze the property of r-robustness, a
stronger notion than connectivity that enables resilient consensus in the
presence of malicious nodes. Finally, motivated by aggregation mechanisms based
on pairwise masking, we model and analyze the impact of a subset of adversarial
nodes, modeled as deletions, on connectivity and giant component size - metrics
that are closely tied to privacy guarantees. Together, our results pave the way
for end-to-end performance guarantees for a suite of algorithms for reliable
inference on networks.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [520] [Vibe2Spike: Batteryless Wireless Tags for Vibration Sensing with Event Cameras and Spiking Networks](https://arxiv.org/abs/2508.11640)
*Danny Scott,William LaForest,Hritom Das,Ioannis Polykretis,Catherine D. Schuman,Charles Rizzo,James Plank,Sai Swaminathan*

Main category: eess.SP

TL;DR: Vibe2Spike is a battery-free wireless sensing framework that uses vibration energy harvesting, visible light communication, and spiking neural networks for activity recognition without batteries or RF radios.


<details>
  <summary>Details</summary>
Motivation: Existing sensing solutions face energy, scalability, and reliability trade-offs due to battery maintenance, wireless transmission overhead, and data processing complexity. There's a need for dense, low-cost sensors for ubiquitous smart environments.

Method: The system uses ultra-low-cost tags with piezoelectric disc, Zener diode, and LED to harvest vibration energy and emit visible light spikes. Optical spikes are captured by event cameras and classified using spiking neural networks optimized via the EONS framework.

Result: Achieved 94.9% average classification fitness across five device classes, with analysis of latency-accuracy trade-offs for different temporal binning strategies.

Conclusion: Vibe2Spike demonstrates a scalable and energy-efficient approach for enabling intelligent environments in a batteryless manner, overcoming traditional sensing limitations.

Abstract: The deployment of dense, low-cost sensors is critical for realizing
ubiquitous smart environments. However, existing sensing solutions struggle
with the energy, scalability, and reliability trade-offs imposed by battery
maintenance, wireless transmission overhead, and data processing complexity. In
this work, we present Vibe2Spike, a novel battery-free, wireless sensing
framework that enables vibration-based activity recognition using visible light
communication (VLC) and spiking neural networks (SNNs). Our system uses
ultra-low-cost tags composed only of a piezoelectric disc, a Zener diode, and
an LED, which harvest vibration energy and emit sparse visible light spikes
without requiring batteries or RF radios. These optical spikes are captured by
event cameras and classified using optimized SNN models evolved via the EONS
framework. We evaluate Vibe2Spike across five device classes, achieving 94.9\%
average classification fitness while analyzing the latency-accuracy trade-offs
of different temporal binning strategies. Vibe2Spike demonstrates a scalable,
and energy-efficient approach for enabling intelligent environments in a
batteryless manner.

</details>


### [521] [Age-Normalized HRV Features for Non-Invasive Glucose Prediction: A Pilot Sleep-Aware Machine Learning Study](https://arxiv.org/abs/2508.11682)
*Md Basit Azam,Sarangthem Ibotombi Singh*

Main category: eess.SP

TL;DR: Age-normalized HRV features during sleep improve glucose prediction by 25.6% compared to traditional methods, addressing age-related confounding factors in autonomic function assessment.


<details>
  <summary>Details</summary>
Motivation: Non-invasive glucose monitoring is critical for diabetes management, but traditional HRV analyses are confounded by age-related autonomic changes, limiting their predictive accuracy.

Method: Analyzed 43 subjects with multi-modal data including sleep-stage specific ECG, HRV features, and clinical measurements. Applied novel age-normalization to HRV features by dividing raw values by age-scaled factors. Used BayesianRidge regression with 5-fold cross-validation for log-glucose prediction.

Result: Age-normalized HRV features achieved R2 = 0.161 (MAE = 0.182), representing 25.6% improvement over non-normalized features (R2 = 0.132). Top predictive features were REM sleep HRV mean RR, deep sleep HRV mean RR, and diastolic blood pressure.

Conclusion: Age-normalized HRV features significantly enhance glucose prediction accuracy and suggest preliminary feasibility for non-invasive glucose monitoring, though validation in larger cohorts is needed before clinical application.

Abstract: Non-invasive glucose monitoring remains a critical challenge in the
management of diabetes. HRV during sleep shows promise for glucose prediction
however, age-related autonomic changes significantly confound traditional HRV
analyses. We analyzed 43 subjects with multi-modal data including sleep-stage
specific ECG, HRV features, and clinical measurements. A novel
age-normalization technique was applied to the HRV features by, dividing the
raw values by age-scaled factors. BayesianRidge regression with 5-fold
cross-validation was employed for log-glucose prediction. Age-normalized HRV
features achieved R2 = 0.161 (MAE = 0.182) for log-glucose prediction,
representing a 25.6% improvement over non-normalized features (R2 = 0.132). The
top predictive features were hrv rem mean rr age normalized (r = 0.443, p =
0.004), hrv ds mean rr age normalized (r = 0.438, p = 0.005), and diastolic
blood pressure (r = 0.437, p = 0.005). Systematic ablation studies confirmed
age-normalization as the critical component, with sleep-stage specific features
providing additional predictive value. Age-normalized HRV features
significantly enhance glucose prediction accuracy compared with traditional
approaches. This sleep-aware methodology addresses fundamental limitations in
autonomic function assessment and suggests a preliminary feasibility for
non-invasive glucose monitoring applications. However, these results require
validation in larger cohorts before clinical consideration.

</details>


### [522] [Towards Generalizable Learning Models for EEG-Based Identification of Pain Perception](https://arxiv.org/abs/2508.11691)
*Mathis Rezzouk,Fabrice Gagnon,Alyson Champagne,Mathieu Roy,Philippe Albouy,Michel-Pierre Coll,Cem Subakan*

Main category: eess.SP

TL;DR: EEG-based pain perception analysis using machine learning faces cross-participant generalization challenges due to high variability in EEG signals. This study evaluates various models on a novel 108-participant dataset, finding deep learning models more resilient than traditional classifiers for subject-invariant EEG decoding.


<details>
  <summary>Details</summary>
Motivation: The high cross-participant variability in EEG signals and limited research focus on direct pain perception identification create challenges for generalizing machine learning models across individuals in pain perception analysis.

Method: Systematic evaluation of traditional classifiers and deep neural classifiers for identifying thermal pain and aversive auditory stimulation from EEG recordings, using a novel dataset of 108 participants under both within- and cross-participant evaluation settings.

Result: Traditional models suffered the largest performance drop from within- to cross-participant evaluation, while deep learning models proved more resilient. Graph-based models showed strong potential for capturing subject-invariant structure in EEG signals, though performance variability remained high.

Conclusion: Deep learning models demonstrate better generalization capabilities for subject-invariant EEG decoding of pain perception. The study also provides a standardized preprocessed dataset for future algorithm evaluation under the same generalization constraints.

Abstract: EEG-based analysis of pain perception, enhanced by machine learning, reveals
how the brain encodes pain by identifying neural patterns evoked by noxious
stimulation. However, a major challenge that remains is the generalization of
machine learning models across individuals, given the high cross-participant
variability inherent to EEG signals and the limited focus on direct pain
perception identification in current research. In this study, we systematically
evaluate the performance of cross-participant generalization of a wide range of
models, including traditional classifiers and deep neural classifiers for
identifying the sensory modality of thermal pain and aversive auditory
stimulation from EEG recordings. Using a novel dataset of EEG recordings from
108 participants, we benchmark model performance under both within- and
cross-participant evaluation settings. Our findings show that traditional
models suffered the largest drop from within- to cross-participant performance,
while deep learning models proved more resilient, underscoring their potential
for subject-invariant EEG decoding. Even though performance variability
remained high, the strong results of the graph-based model highlight its
potential to capture subject-invariant structure in EEG signals. On the other
hand, we also share the preprocessed dataset used in this study, providing a
standardized benchmark for evaluating future algorithms under the same
generalization constraints.

</details>


### [523] [Scalable, Technology-Agnostic Diagnosis and Predictive Maintenance for Point Machine using Deep Learning](https://arxiv.org/abs/2508.11692)
*Eduardo Di Santi,Ruixiang Ci,Clément Lefebvre,Nenad Mijatovic,Michele Pugnaloni,Jonathan Brown,Victor Martín,Kenza Saiah*

Main category: eess.SP

TL;DR: Deep learning method for railway point machine failure detection using only power signal patterns, achieving >99.99% precision and ISO-17359 compliance through conformal prediction.


<details>
  <summary>Details</summary>
Motivation: Point machine failures cause railway service disruptions. Current methods require multiple inputs and custom feature engineering, making them technology-specific and not scalable. Pre-emptive maintenance through anomaly detection can prevent operational interruptions.

Method: Uses deep learning model applied to power signal patterns to classify point machines as nominal or associated with failure types (obstacles, friction, power issues, misalignment). Requires only one input (power signal) and uses conformal prediction for confidence estimation.

Result: Achieves >99.99% precision, <0.01% false positives, and negligible false negatives. Proven scalable on multiple electromechanical point machine types in both real-world and test bench environments.

Conclusion: Methodology is generic, technology-agnostic, and provides maintainers with clear certainty indications through conformal prediction, making it compliant with ISO-17359 standard for condition monitoring and diagnostics.

Abstract: The Point Machine (PM) is a critical piece of railway equipment that switches
train routes by diverting tracks through a switchblade. As with any critical
safety equipment, a failure will halt operations leading to service
disruptions; therefore, pre-emptive maintenance may avoid unnecessary
interruptions by detecting anomalies before they become failures. Previous work
relies on several inputs and crafting custom features by segmenting the signal.
This not only adds additional requirements for data collection and processing,
but it is also specific to the PM technology, the installed locations and
operational conditions limiting scalability. Based on the available maintenance
records, the main failure causes for PM are obstacles, friction, power source
issues and misalignment. Those failures affect the energy consumption pattern
of PMs, altering the usual (or healthy) shape of the power signal during the PM
movement. In contrast to the current state-of-the-art, our method requires only
one input. We apply a deep learning model to the power signal pattern to
classify if the PM is nominal or associated with any failure type, achieving
>99.99\% precision, <0.01\% false positives and negligible false negatives. Our
methodology is generic and technology-agnostic, proven to be scalable on
several electromechanical PM types deployed in both real-world and test bench
environments. Finally, by using conformal prediction the maintainer gets a
clear indication of the certainty of the system outputs, adding a confidence
layer to operations and making the method compliant with the ISO-17359
standard.

</details>


### [524] [Track Component Failure Detection Using Data Analytics over existing STDS Track Circuit data](https://arxiv.org/abs/2508.11693)
*Francisco López,Eduardo Di Santi,Clément Lefebvre,Nenad Mijatovic,Michele Pugnaloni,Victor Martín,Kenza Saiah*

Main category: eess.SP

TL;DR: Using SVM classifier on STDS track circuit current data to automatically identify 15 specific failures across 3 categories, achieving 100% accuracy on field data from 10 track circuits.


<details>
  <summary>Details</summary>
Motivation: Track circuits are critical signaling devices for train detection, but current maintenance relies on manual troubleshooting. The research aims to automate failure identification to improve maintenance efficiency and reduce downtime.

Method: Applied SVM (support vector machine) classifier to current data from Smart Train Detection System (STDS) track circuits, which use both high and low-frequency bands. Model trained to classify 15 different failures across 3 general categories.

Result: The method was tested with field data from 10 different track circuits and validated by STDS experts and maintainers. All use cases were correctly classified, achieving 100% accuracy in failure identification.

Conclusion: The SVM-based approach successfully automates track circuit failure identification, providing an effective solution for maintenance optimization in railway signaling systems.

Abstract: Track Circuits (TC) are the main signalling devices used to detect the
presence of a train on a rail track. It has been used since the 19th century
and nowadays there are many types depending on the technology. As a general
classification, Track Circuits can be divided into 2 main groups, DC (Direct
Current) and AC (Alternating Current) circuits. This work is focused on a
particular AC track circuit, called "Smart Train Detection System" (STDS),
designed with both high and low-frequency bands. This approach uses STDS
current data applied to an SVM (support vector machine) classifier as a type of
failure identifier. The main purpose of this work consists on determine
automatically which is the component of the track that is failing to improve
the maintenance action. Model was trained to classify 15 different failures
that belong to 3 more general categories. The method was tested with field data
from 10 different track circuits and validated by the STDS track circuit expert
and maintainers. All use cases were correctly classified by the method.

</details>


### [525] [Inductive transfer learning from regression to classification in ECG analysis](https://arxiv.org/abs/2508.11656)
*Ridma Jayasundara,Ishan Fernando,Adeepa Fernando,Roshan Ragel,Vajira Thambawita,Isuru Nawinne*

Main category: eess.SP

TL;DR: Transfer learning from regression to classification improves ECG classification performance using synthetic data, enabling better utilization of diverse datasets while preserving patient privacy.


<details>
  <summary>Details</summary>
Motivation: Privacy concerns with patient ECG data drive need for synthetic alternatives. This study explores using synthetic ECG data for training deep learning models and evaluates transfer learning from regression to classification tasks to enhance performance on real ECG data.

Method: Used popular deep learning models to predict four cardiac parameters (HR, PR interval, QT interval, QRS complex) with regression models, then leveraged these for transfer learning to perform 5-class ECG signal classification. Systematically investigated transfer learning viability from regression to classification.

Result: Transfer learning from regression to classification improves classification performance, demonstrating its viability for better utilization of diverse open-access and synthetic ECG datasets.

Conclusion: Transfer learning approach maximizes utility of available data and advances deep learning applications in ECG analysis, particularly valuable for privacy-preserving research using synthetic data.

Abstract: Cardiovascular diseases (CVDs) are the leading cause of mortality worldwide,
accounting for over 30% of global deaths according to the World Health
Organization (WHO). Importantly, one-third of these deaths are preventable with
timely and accurate diagnosis. The electrocardiogram (ECG), a non-invasive
method for recording the electrical activity of the heart, is crucial for
diagnosing CVDs. However, privacy concerns surrounding the use of patient ECG
data in research have spurred interest in synthetic data, which preserves the
statistical properties of real data without compromising patient
confidentiality. This study explores the potential of synthetic ECG data for
training deep learning models from regression to classification tasks and
evaluates the feasibility of transfer learning to enhance classification
performance on real ECG data. We experimented with popular deep learning models
to predict four key cardiac parameters, namely, Heart Rate (HR), PR interval,
QT interval, and QRS complex-using separate regression models. Subsequently, we
leveraged these regression models for transfer learning to perform 5-class ECG
signal classification. Our experiments systematically investigate whether
transfer learning from regression to classification is viable, enabling better
utilization of diverse open-access and synthetic ECG datasets. Our findings
demonstrate that transfer learning from regression to classification improves
classification performance, highlighting its potential to maximize the utility
of available data and advance deep learning applications in this domain.

</details>


### [526] [Robust Sparse Bayesian Learning Based on Minimum Error Entropy for Noisy High-Dimensional Brain Activity Decoding](https://arxiv.org/abs/2508.11657)
*Yuanhao Li,Badong Chen,Wenjun Bai,Yasuharu Koike,Okito Yamashita*

Main category: eess.SP

TL;DR: Robust sparse Bayesian learning framework using minimum error entropy criterion for noisy high-dimensional brain signal decoding


<details>
  <summary>Details</summary>
Motivation: Traditional Gaussian/binomial distribution assumptions are inadequate for noisy brain signals, requiring a more robust approach to handle complex data distributions in brain activity decoding

Method: Proposed MEE-based likelihood function to enhance sparse Bayesian learning inference for analyzing noisy brain datasets

Result: Superior decoding metrics and physiological patterns compared to conventional and state-of-the-art methods in both regression and classification brain decoding tasks

Conclusion: MEE-based likelihood model enables sparse Bayesian learning to simultaneously address noise and high dimensionality challenges in brain decoding

Abstract: Objective: Sparse Bayesian learning provides an effective scheme to solve the
high-dimensional problem in brain signal decoding. However, traditional
assumptions regarding data distributions such as Gaussian and binomial are
potentially inadequate to characterize the noisy signals of brain activity.
Hence, this study aims to propose a robust sparse Bayesian learning framework
to address noisy highdimensional brain activity decoding. Methods: Motivated by
the commendable robustness of the minimum error entropy (MEE) criterion for
handling complex data distributions, we proposed an MEE-based likelihood
function to facilitate the accurate inference of sparse Bayesian learning in
analyzing noisy brain datasets. Results: Our proposed approach was evaluated
using two high-dimensional brain decoding tasks in regression and
classification contexts, respectively. The experimental results showed that,
our approach can realize superior decoding metrics and physiological patterns
than the conventional and state-of-the-art methods. Conclusion: Utilizing the
proposed MEE-based likelihood model, sparse Bayesian learning is empowered to
simultaneously address the challenges of noise and high dimensionality in the
brain decoding task. Significance: This work provides a powerful tool to
realize robust brain decoding, advancing biomedical engineering applications
such as brain-computer interface.

</details>


### [527] [Unsupervised Pairwise Learning Optimization Framework for Cross-Corpus EEG-Based Emotion Recognition Based on Prototype Representation](https://arxiv.org/abs/2508.11663)
*Guangli Li,Canbiao Wu,Zhen Liang*

Main category: eess.SP

TL;DR: A novel domain adversarial transfer learning framework (McdPL) for cross-corpus emotion recognition that uses dual adversarial classifiers and pairwise learning to align controversial samples near decision boundaries and improve accuracy.


<details>
  <summary>Details</summary>
Motivation: Cross-corpus emotion recognition faces challenges due to physiological differences between subjects, variations in experimental environments/equipment, and difficulties with samples near decision boundaries in affective computing.

Method: Maximum classifier discrepancy with Pairwise Learning (McdPL) framework featuring dual adversarial classifiers (Ada and RMS), three-stage adversarial training to maximize classification discrepancy and minimize feature distribution, and pairwise learning to transform classification into similarity problems.

Result: Superior performance compared to baseline models on SEED, SEED-IV and SEED-V databases with average accuracy improvements of 4.76% and 3.97% respectively in cross-corpus emotion recognition tasks.

Conclusion: The McdPL model provides an effective solution for cross-corpus emotion recognition by enabling precise feature alignment and mitigating label noise influence, advancing affective computing in brain-computer interface applications.

Abstract: Affective computing is a rapidly developing interdisciplinary research
direction in the field of brain-computer interface. In recent years, the
introduction of deep learning technology has greatly promoted the development
of the field of emotion recognition. However, due to physiological differences
between subjects, as well as the variations in experimental environments and
equipment, cross-corpus emotion recognition faces serious challenges,
especially for samples near the decision boundary. To solve the above problems,
we propose an optimization method based on domain adversarial transfer learning
to fine-grained alignment of affective features, named Maximum classifier
discrepancy with Pairwise Learning (McdPL) framework. In McdPL, we design a
dual adversarial classifier (Ada classifier and RMS classifier), and apply a
three-stage adversarial training to maximize classification discrepancy and
minimize feature distribution to align controversy samples near the decision
boundary. In the process of domain adversarial training, the two classifiers
also maintain an adversarial relationship, ultimately enabling precise
cross-corpus feature alignment. In addition, the introduction of pairwise
learning transforms the classification problem of samples into a similarity
problem between samples, alleviating the influence of label noise. We conducted
systematic experimental evaluation of the model using publicly available SEED,
SEED-IV and SEED-V databases. The results show that the McdPL model is superior
to other baseline models in the cross-corpus emotion recognition task, and the
average accuracy improvements of 4.76\% and 3.97\%, respectively. Our work
provides a promising solution for emotion recognition cross-corpus. The source
code is available at https://github.com/WuCB-BCI/Mcd_PL.

</details>


### [528] [Energy-Efficient Real-Time 4-Stage Sleep Classification at 10-Second Resolution: A Comprehensive Study](https://arxiv.org/abs/2508.11664)
*Zahra Mohammadi,Parnian Fazel,Siamak Mohammadi*

Main category: eess.SP

TL;DR: Energy-efficient sleep stage classification from single-lead ECG using lightweight deep learning models and custom SleepLiteCNN architecture, achieving 89-92% accuracy with ultra-low power consumption for wearable applications.


<details>
  <summary>Details</summary>
Motivation: Conventional polysomnography is costly and impractical for long-term home sleep monitoring, creating need for efficient ECG-based solutions that can enable continuous wearable sleep stage detection.

Method: Two windowing strategies: 5-minute windows for ML models with handcrafted features, and 30-second windows for deep learning models. Developed custom SleepLiteCNN architecture optimized for energy efficiency, with 8-bit quantization and FPGA deployment validation.

Result: MobileNet-v1 achieved 92% accuracy/91% F1-score but high energy. SleepLiteCNN reached 89% accuracy/89% F1-score with only 5.48 microjoules per inference at 45nm. Quantization preserved accuracy while further reducing power, with low FPGA resource usage.

Conclusion: The proposed system provides a practical, energy-efficient solution for continuous wearable ECG-based sleep monitoring with near-real-time 10-second resolution, making long-term home sleep assessment feasible.

Abstract: Sleep stage classification is crucial for diagnosing and managing disorders
such as sleep apnea and insomnia. Conventional clinical methods like
polysomnography are costly and impractical for long-term home use. We present
an energy-efficient pipeline that detects four sleep stages (wake, REM, light,
and deep) from a single-lead ECG. Two windowing strategies are introduced: (1)
a 5-minute window with 30-second steps for machine-learning models that use
handcrafted features, and (2) a 30-second window with 10-second steps for
deep-learning models, enabling near-real-time 10-second resolution. Lightweight
networks such as MobileNet-v1 reach 92 percent accuracy and 91 percent F1-score
but still draw significant energy. We therefore design SleepLiteCNN, a custom
model that achieves 89 percent accuracy and 89 percent F1-score while lowering
energy use to 5.48 microjoules per inference at 45 nm. Applying eight-bit
quantization preserves accuracy and further reduces power, and FPGA deployment
confirms low resource usage. The proposed system offers a practical solution
for continuous, wearable ECG-based sleep monitoring.

</details>


### [529] [Explainable Deep Neural Network for Multimodal ECG Signals: Intermediate vs Late Fusion](https://arxiv.org/abs/2508.11666)
*Timothy Oladunni,Ehimen Aneni*

Main category: eess.SP

TL;DR: Intermediate fusion outperforms late fusion in ECG-based cardiovascular disease classification, achieving 97% accuracy with better interpretability through saliency maps and mutual information analysis.


<details>
  <summary>Details</summary>
Motivation: Unimodal deep learning models have limitations in overfitting and generalizability, while optimal fusion strategies (intermediate vs late fusion) remain insufficiently examined in high-stakes clinical contexts like ECG-based CVD classification.

Method: Comparative study of intermediate and late fusion strategies using ECG signals across time, frequency, and time-frequency domains. Experiments conducted to identify best fusion architecture, with interpretability analyses using saliency maps and mutual information.

Result: Intermediate fusion consistently outperformed late fusion with peak accuracy of 97%, Cohen's d > 0.8 relative to standalone models and d = 0.40 compared to late fusion. Saliency maps showed alignment with discretized ECG signals, confirmed by mutual information analysis.

Conclusion: The proposed ECG domain-based multimodal model offers superior predictive capability and enhanced explainability, making it crucial for medical AI applications and surpassing state-of-the-art models.

Abstract: The limitations of unimodal deep learning models, particularly their tendency
to overfit and limited generalizability, have renewed interest in multimodal
fusion strategies. Multimodal deep neural networks (MDNN) have the capability
of integrating diverse data domains and offer a promising solution for robust
and accurate predictions. However, the optimal fusion strategy, intermediate
fusion (feature-level) versus late fusion (decision-level) remains
insufficiently examined, especially in high-stakes clinical contexts such as
ECG-based cardiovascular disease (CVD) classification. This study investigates
the comparative effectiveness of intermediate and late fusion strategies using
ECG signals across three domains: time, frequency, and time-frequency. A series
of experiments were conducted to identify the highest-performing fusion
architecture. Results demonstrate that intermediate fusion consistently
outperformed late fusion, achieving a peak accuracy of 97 percent, with Cohen's
d > 0.8 relative to standalone models and d = 0.40 compared to late fusion.
Interpretability analyses using saliency maps reveal that both models align
with the discretized ECG signals. Statistical dependency between the
discretized ECG signals and corresponding saliency maps for each class was
confirmed using Mutual Information (MI). The proposed ECG domain-based
multimodal model offers superior predictive capability and enhanced
explainability, crucial attributes in medical AI applications, surpassing
state-of-the-art models.

</details>


### [530] [A Graph Neural Network based on a Functional Topology Model: Unveiling the Dynamic Mechanisms of Non-Suicidal Self-Injury in Single-Channel EEG](https://arxiv.org/abs/2508.11684)
*BG Tong*

Main category: eess.SP

TL;DR: A novel Functional-Energetic Topology Model using Graph Neural Networks successfully decodes NSSI brain patterns from single-channel EEG, revealing dysfunctional feedback loops in somatic regulation during self-injury states.


<details>
  <summary>Details</summary>
Motivation: To uncover neurodynamic mechanisms of Non-Suicidal Self-Injury (NSSI) and develop objective biomarkers using real-world EEG data, addressing the need for computational models that can decode complex mental states from sparse neuroimaging data.

Method: Collected EEG data from three adolescents with NSSI using smartphone app and portable Fp1 EEG headband over ~1 month. Built theory-driven GNN with seven functional nodes, evaluated via intra-subject (80/20 split) and leave-one-subject-out cross-validation, with GNNExplainer for interpretability.

Result: High intra-subject accuracy (>85%) and significantly above-chance cross-subject performance (~73.7%). Key finding: during NSSI states, critical feedback loop regulating somatic sensation shows dysfunction and directional reversal, with brain losing self-correction ability via negative bodily feedback.

Conclusion: Demonstrates feasibility of theory-guided GNNs for sparse, single-channel EEG decoding. Identified 'feedback loop reversal' provides novel dynamic model of NSSI mechanisms, enabling objective biomarkers and next-generation Digital Therapeutics.

Abstract: Objective: This study proposes and preliminarily validates a novel
"Functional-Energetic Topology Model" to uncover neurodynamic mechanisms of
Non-Suicidal Self-Injury (NSSI), using Graph Neural Networks (GNNs) to decode
brain network patterns from single-channel EEG in real-world settings.Methods:
EEG data were collected over ~1 month from three adolescents with NSSI using a
smartphone app and a portable Fp1 EEG headband during impulsive and
non-impulsive states. A theory-driven GNN with seven functional nodes was
built. Performance was evaluated via intra-subject (80/20 split) and
leave-one-subject-out cross-validation (LOSOCV). GNNExplainer was used for
interpretability.Results: The model achieved high intra-subject accuracy (>85%)
and significantly above-chance cross-subject performance (approximately73.7%).
Explainability analysis revealed a key finding: during NSSI states, a critical
feedback loop regulating somatic sensation exhibits dysfunction and directional
reversal. Specifically, the brain loses its ability to self-correct via
negative bodily feedback, and the regulatory mechanism enters an "ineffective
idling" state.Conclusion: This work demonstrates the feasibility of applying
theory-guided GNNs to sparse, single-channel EEG for decoding complex mental
states. The identified "feedback loop reversal" offers a novel, dynamic, and
computable model of NSSI mechanisms, paving the way for objective biomarkers
and next-generation Digital Therapeutics (DTx).

</details>


### [531] [Enhancing Corrosion Resistance of Aluminum Alloys Through AI and ML Modeling](https://arxiv.org/abs/2508.11685)
*Farnaz Kaboudvand,Maham Khalid,Nydia Assaf,Vardaan Sahgal,Jon P. Ruffley,Brian J. McDermott*

Main category: eess.SP

TL;DR: Machine learning algorithms, particularly Gaussian Process Regression with hybrid kernels, effectively predict aluminum alloy corrosion rates and optimize material compositions for marine environments.


<details>
  <summary>Details</summary>
Motivation: Corrosion significantly impacts aluminum alloy performance in marine environments, requiring better prediction and optimization methods to improve material durability and selection.

Method: Used comprehensive open-source dataset with corrosion rate data and environmental conditions. Employed two approaches: direct (composition + environment → corrosion rate) and inverse (corrosion rate → optimal composition). Compared Random Forest regression, feed-forward neural network, and Gaussian Process Regression with various kernel functions.

Result: Gaussian Process Regression demonstrated superior performance, especially with hybrid kernel functions. Log-transformed GPR further refined predictions. ML models successfully predicted corrosion rates and identified suitable material compositions.

Conclusion: Machine learning, particularly Gaussian Process Regression, is highly effective for predicting aluminum alloy corrosion rates and optimizing material properties for marine applications.

Abstract: Corrosion poses a significant challenge to the performance of aluminum
alloys, particularly in marine environments. This study investigates the
application of machine learning (ML) algorithms to predict and optimize
corrosion resistance, utilizing a comprehensive open-source dataset compiled
from various sources. The dataset encompasses corrosion rate data and
environmental conditions, preprocessed to standardize units and formats. We
explored two different approaches, a direct approach, where the material's
composition and environmental conditions were used as inputs to predict
corrosion rates; and an inverse approach, where corrosion rate served as the
input to identify suitable material compositions as output. We employed and
compared three distinct ML methodologies for forward predictions: Random Forest
regression, optimized via grid search; a feed-forward neural network, utilizing
ReLU activation and Adam optimization; and Gaussian Process Regression (GPR),
implemented with GPyTorch and employing various kernel functions. The Random
Forest and neural network models provided predictive capabilities based on
elemental compositions and environmental conditions. Notably, Gaussian Process
Regression demonstrated superior performance, particularly with hybrid kernel
functions. Log-transformed GPR further refined predictions. This study
highlights the efficacy of ML, particularly GPR, in predicting corrosion rates
and material properties.

</details>


### [532] [Towards Generalizable Human Activity Recognition: A Survey](https://arxiv.org/abs/2508.12213)
*Yize Cai,Baoshen Guo,Flora Salim,Zhiqing Hong*

Main category: eess.SP

TL;DR: Survey paper on IMU-based Human Activity Recognition focusing on generalization challenges, reviewing 229 papers and 25 datasets, categorizing model-centric and data-centric approaches, and discussing future directions including LLMs and foundation models.


<details>
  <summary>Details</summary>
Motivation: IMU-based HAR has improved performance but struggles with generalization across different users, sensor positions, and environments due to domain shifts, limiting real-world adoption.

Method: Comprehensive literature review of 229 research papers and 25 public datasets, categorizing approaches into model-centric (pre-training, end-to-end, LLM-based) and data-centric (multi-modal learning, data augmentation) methods.

Result: Provides a broad overview of IMU-based generalizable HAR, summarizing methodologies, datasets, tools, benchmarks, and identifying persistent challenges like data scarcity and efficient training.

Conclusion: Outlines future directions including adoption of foundation/large language models, physics-informed reasoning, generative modeling, and resource-efficient training to address generalization challenges in HAR.

Abstract: As a critical component of Wearable AI, IMU-based Human Activity Recognition
(HAR) has attracted increasing attention from both academia and industry in
recent years. Although HAR performance has improved considerably in specific
scenarios, its generalization capability remains a key barrier to widespread
real-world adoption. For example, domain shifts caused by variations in users,
sensor positions, or environments can significantly decrease the performance in
practice. As a result, in this survey, we explore the rapidly evolving field of
IMU-based generalizable HAR, reviewing 229 research papers alongside 25
publicly available datasets to provide a broad and insightful overview. We
first present the background and overall framework of IMU-based HAR tasks, as
well as the generalization-oriented training settings. Then, we categorize
representative methodologies from two perspectives: (i) model-centric
approaches, including pre-training method, end-to-end method, and large
language model (LLM)-based learning method; and (ii) data-centric approaches,
including multi-modal learning and data augmentation techniques. In addition,
we summarize widely used datasets in this field, as well as relevant tools and
benchmarks. Building on these methodological advances, the broad applicability
of IMU-based HAR is also reviewed and discussed. Finally, we discuss persistent
challenges (e.g., data scarcity, efficient training, and reliable evaluation)
and also outline future directions for HAR, including the adoption of
foundation and large language models, physics-informed and context-aware
reasoning, generative modeling, and resource-efficient training and inference.
The complete list of this survey is available at
https://github.com/rh20624/Awesome-IMU-Sensing, which will be updated
continuously.

</details>


### [533] [Data-driven RF Tomography via Cross-modal Sensing and Continual Learning](https://arxiv.org/abs/2508.11654)
*Yang Zhao,Tao Wang,Said Elhadi*

Main category: eess.SP

TL;DR: DRIFT framework uses cross-modal RF and visual sensing with continual learning to improve underground target detection accuracy in dynamic environments, achieving 23.2% improvement over state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Data-driven RF tomography shows promise for underground detection but struggles with accuracy and robustness in dynamic environments where RF signals change significantly.

Method: Proposes a cross-modal sensing system with RF and visual sensors, trains an RF tomography DNN using cross-modal learning, and applies continual learning to automatically update the model when environmental changes are detected.

Result: Achieves an average equivalent diameter error of 2.29 cm, representing a 23.2% improvement over the state-of-the-art approach.

Conclusion: The DRIFT framework successfully addresses the challenge of dynamic environments in underground target detection through cross-modal learning and continual model updates, with code and dataset made publicly available.

Abstract: Data-driven radio frequency (RF) tomography has demonstrated significant
potential for underground target detection, due to the penetrative nature of RF
signals through soil. However, it is still challenging to achieve accurate and
robust performance in dynamic environments. In this work, we propose a
data-driven radio frequency tomography (DRIFT) framework with the following key
components to reconstruct cross section images of underground root tubers, even
with significant changes in RF signals. First, we design a cross-modal sensing
system with RF and visual sensors, and propose to train an RF tomography deep
neural network (DNN) model following the cross-modal learning approach. Then we
propose to apply continual learning to automatically update the DNN model, once
environment changes are detected in a dynamic environment. Experimental results
show that our approach achieves an average equivalent diameter error of 2.29
cm, 23.2% improvement upon the state-of-the-art approach. Our DRIFT code and
dataset are publicly available on https://github.com/Data-driven-RTI/DRIFT.

</details>


### [534] [ATLAS: AI-Native Receiver Test-and-Measurement by Leveraging AI-Guided Search](https://arxiv.org/abs/2508.12204)
*Mauro Belgiovine,Suyash Pradhan,Johannes Lange,Michael Löhning,Kaushik Chowdhury*

Main category: eess.SP

TL;DR: ATLAS is an AI-guided testing framework that efficiently generates targeted tests for AI-native wireless receivers to identify failure conditions, reducing test requirements by 19% compared to grid search.


<details>
  <summary>Details</summary>
Motivation: Slow industry adoption of AI-native wireless receivers due to lack of explainability and risks from potential failures, with exhaustive testing being infeasible and training data often unavailable.

Method: Uses gradient-based optimization to generate tests online that probe specific configurations with highest failure risk, implemented in NVIDIA's Sionna environment with DeepRx AI-native receiver and classical receiver models.

Result: Uncovered specific combinations of mobility, channel delay spread, and noise where AI-native DeepRx performs suboptimally compared to classical receivers, with 19% fewer tests required per failure found.

Conclusion: ATLAS provides an efficient alternative to exhaustive grid search for testing AI-native wireless receivers, demonstrating practical viability for identifying failure conditions in high-dimensional parameter spaces.

Abstract: Industry adoption of Artificial Intelligence (AI)-native wireless receivers,
or even modular, Machine Learning (ML)-aided wireless signal processing blocks,
has been slow. The main concern is the lack of explainability of these trained
ML models and the significant risks posed to network functionalities in case of
failures, especially since (i) testing on every exhaustive case is infeasible
and (ii) the data used for model training may not be available. This paper
proposes ATLAS, an AI-guided approach that generates a battery of tests for
pre-trained AI-native receiver models and benchmarks the performance against a
classical receiver architecture. Using gradient-based optimization, it avoids
spanning the exhaustive set of all environment and channel conditions; instead,
it generates the next test in an online manner to further probe specific
configurations that offer the highest risk of failure. We implement and
validate our approach by adopting the well-known DeepRx AI-native receiver
model as well as a classical receiver using differentiable tensors in NVIDIA's
Sionna environment. ATLAS uncovers specific combinations of mobility, channel
delay spread, and noise, where fully and partially trained variants of
AI-native DeepRx perform suboptimally compared to the classical receivers. Our
proposed method reduces the number of tests required per failure found by 19%
compared to grid search for a 3-parameters input optimization problem,
demonstrating greater efficiency. In contrast, the computational cost of the
grid-based approach scales exponentially with the number of variables, making
it increasingly impractical for high-dimensional problems.

</details>


### [535] [Towards SISO Bistatic Sensing for ISAC](https://arxiv.org/abs/2508.12614)
*Zhongqin Wang,J. Andrew Zhang,Kai Wu,Min Xu,Y. Jay Guo*

Main category: eess.SP

TL;DR: WiDFS 3.0 enables accurate delay and Doppler estimation in low-cost single-antenna ISAC systems by suppressing Doppler mirroring ambiguity and removing random phase offsets from distorted CSI.


<details>
  <summary>Details</summary>
Motivation: Real-world ISAC deployment is often limited to low-cost, single-antenna transceivers where clock asynchrony introduces random phase offsets in CSI that cannot be mitigated with conventional multi-antenna methods.

Method: Proposes a self-referencing cross-correlation (SRCC) method for SISO random phase removal and employs delay-domain beamforming to resolve Doppler ambiguity, enabling robust sensing with compact neural networks.

Result: WiDFS 3.0 achieves accurate parameter estimation comparable to or surpassing prior multi-antenna methods, especially in delay estimation, and consistently outperforms conventional features with strong sensing accuracy and generalization.

Conclusion: The framework enables high-performance bistatic SISO sensing suitable for low-complexity deployments, demonstrating that accurate ISAC is achievable with minimal hardware requirements.

Abstract: Integrated Sensing and Communication (ISAC) is a key enabler for
next-generation wireless systems. However, real-world deployment is often
limited to low-cost, single-antenna transceivers. In such bistatic Single-Input
Single-Output (SISO) setup, clock asynchrony introduces random phase offsets in
Channel State Information (CSI), which cannot be mitigated using conventional
multi-antenna methods. This work proposes WiDFS 3.0, a lightweight bistatic
SISO sensing framework that enables accurate delay and Doppler estimation from
distorted CSI by effectively suppressing Doppler mirroring ambiguity. It
operates with only a single antenna at both the transmitter and receiver,
making it suitable for low-complexity deployments. We propose a
self-referencing cross-correlation (SRCC) method for SISO random phase removal
and employ delay-domain beamforming to resolve Doppler ambiguity. The resulting
unambiguous delay-Doppler-time features enable robust sensing with compact
neural networks. Extensive experiments show that WiDFS 3.0 achieves accurate
parameter estimation, with performance comparable to or even surpassing that of
prior multi-antenna methods, especially in delay estimation. Validated under
single- and multi-target scenarios, the extracted ambiguity-resolved features
show strong sensing accuracy and generalization. For example, when deployed on
the embedded-friendly MobileViT-XXS with only 1.3M parameters, WiDFS 3.0
consistently outperforms conventional features such as CSI amplitude, mirrored
Doppler, and multi-receiver aggregated Doppler.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [536] [BeeNet: Reconstructing Flower Shapes from Electric Fields using Deep Learning](https://arxiv.org/abs/2508.11724)
*Jake Turley,Ryan A. Palmer,Isaac V. Chenchiah,Daniel Robert*

Main category: q-bio.QM

TL;DR: Deep learning model reconstructs flower shapes from electric fields generated by bee-flower interactions, showing electroreception provides rich spatial information for arthropods.


<details>
  <summary>Details</summary>
Motivation: To understand how arthropods like pollinators use environmental electrical fields to perceive and decode spatial information about their surroundings, particularly flower shapes.

Method: Developed an algorithm using simulated electric field data from bee-flower interactions, trained a deep learning UNet model to reconstruct petal shapes from electric field patterns.

Result: Model accurately reconstructed diverse flower shapes, including complex shapes not in training data, with peak performance at optimal bee-flower distance showing distance-dependent encoding.

Conclusion: Electroreception provides arthropods with rich spatial detail about environmental features, offering new insights into their environmental perception capabilities.

Abstract: Arthropods, including pollinators, respond to environmental electrical
fields. Here, we show that electric field information can be decoded to
reconstruct environmental features. We develop an algorithm capable of
inferring the shapes of polarisable flowers from the electric field generated
by a nearby charged bee. We simulated electric fields arising from bee flower
interactions for flowers with varying petal geometries. These simulated data
were used to train a deep learning UNet model to recreate petal shapes. The
model accurately reconstructed diverse flower shapes including more complex
flower shapes not included in training. Reconstruction performance peaked at an
optimal bee flower distance, indicating distance-dependent encoding of shape
information. These findings show that electroreception can impart rich spatial
detail, offering insights into arthropod environmental perception.

</details>


### [537] [On the Importance of Behavioral Nuances: Amplifying Non-Obvious Motor Noise Under True Empirical Considerations May Lead to Briefer Assays and Faster Classification Processes](https://arxiv.org/abs/2508.12742)
*Theodoros Bermperidis,Joe Vero,Elizabeth B Torres*

Main category: q-bio.QM

TL;DR: A new affective computing platform that uses brief 5-second face videos to capture facial micropeaks and micro expressions, enabling personalized statistical power without requiring large datasets.


<details>
  <summary>Details</summary>
Motivation: To overcome the tradeoff between statistical power (requiring large datasets) and scalability (using brief data samples), and to avoid information loss from traditional grand-averaging techniques that assume normal distributions and linear processes.

Method: Combines a new data type derived from micropeaks in brief face videos with AI-driven face-grid estimation methods, using geometric and nonlinear dynamical systems approaches to analyze kinematics and speed data.

Result: The method captures all facial micropeaks including nuances of different affective micro expressions, and enables differentiation of dynamical and geometric patterns between autistic and neurotypical individuals.

Conclusion: The developed platform provides a scalable solution for affective computing that maintains statistical power with brief data samples, offering new diagnostic capabilities for autism spectrum disorders.

Abstract: There is a tradeoff between attaining statistical power with large, difficult
to gather data sets, and producing highly scalable assays that register brief
data samples. Often, as grand-averaging techniques a priori assume
normally-distributed parameters and linear, stationary processes in
biorhythmic, time series data, important information is lost, averaged out as
gross data. We developed an affective computing platform that enables taking
brief data samples while maintaining personalized statistical power. This is
achieved by combining a new data type derived from the micropeaks present in
time series data registered from brief (5-second-long) face videos with recent
advances in AI-driven face-grid estimation methods. By adopting geometric and
nonlinear dynamical systems approaches to analyze the kinematics, especially
the speed data, the new methods capture all facial micropeaks. These include as
well the nuances of different affective micro expressions. We offer new ways to
differentiate dynamical and geometric patterns present in autistic individuals
from those found more commonly in neurotypical development.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [538] [Denoising diffusion models for inverse design of inflatable structures with programmable deformations](https://arxiv.org/abs/2508.13097)
*Sara Karimi,Nikolaos N. Vlassis*

Main category: cs.CE

TL;DR: A generative design framework using diffusion models for inverse design of inflatable structures that achieve target deformed shapes when pressurized.


<details>
  <summary>Details</summary>
Motivation: Programmable structures need to achieve specific deformed configurations under loading. Inflatable structures are widely used but designing their undeformed geometries to produce desired deformations is challenging.

Method: Uses denoising diffusion probabilistic models (DDPMs) for conditional generation, taking geometric descriptors of target deformed states as input and outputting image-based representations of undeformed configurations with a preprocessing pipeline.

Result: The framework can quickly produce diverse undeformed configurations that achieve desired deformations when inflated, enabling parallel exploration of viable design candidates with complex constraints.

Conclusion: The diffusion-based approach provides an effective method for inverse design of elastic structures undergoing large nonlinear deformations under pressure-driven actuation.

Abstract: Programmable structures are systems whose undeformed geometries and material
property distributions are deliberately designed to achieve prescribed deformed
configurations under specific loading conditions. Inflatable structures are a
prominent example, using internal pressurization to realize large, nonlinear
deformations in applications ranging from soft robotics and deployable
aerospace systems to biomedical devices and adaptive architecture. We present a
generative design framework based on denoising diffusion probabilistic models
(DDPMs) for the inverse design of elastic structures undergoing large,
nonlinear deformations under pressure-driven actuation. The method formulates
the inverse design as a conditional generation task, using geometric
descriptors of target deformed states as inputs and outputting image-based
representations of the undeformed configuration. Representing these
configurations as simple images is achieved by establishing a pre- and
postprocessing pipeline that involves a fixed image processing, simulation
setup, and descriptor extraction methods. Numerical experiments with scalar and
higher-dimensional descriptors show that the framework can quickly produce
diverse undeformed configurations that achieve the desired deformations when
inflated, enabling parallel exploration of viable design candidates while
accommodating complex constraints.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [539] [DermINO: Hybrid Pretraining for a Versatile Dermatology Foundation Model](https://arxiv.org/abs/2508.12190)
*Jingkai Xu,De Cheng,Xiangqian Zhao,Jungang Yang,Zilong Wang,Xinyang Jiang,Xufang Luo,Lili Chen,Xiaoli Ning,Chengxu Li,Xinzhu Zhou,Xuejiao Song,Ang Li,Qingyue Xia,Zhou Zhuang,Hongfei Ouyang,Ke Xue,Yujun Sheng,Rusong Meng,Feng Xu,Xi Yang,Weimin Ma,Yusheng Lee,Dongsheng Li,Xinbo Gao,Jianming Liang,Lili Qiu,Nannan Wang,Xianbo Zuo,Cui Yong*

Main category: eess.IV

TL;DR: DermNIO is a versatile dermatology foundation model that outperforms state-of-the-art models across 20 datasets, achieving 95.79% diagnostic accuracy vs clinicians' 73.66%, and improving clinician performance by 17.21% with AI assistance.


<details>
  <summary>Details</summary>
Motivation: Skin diseases affect up to 70% of the population, with complex diagnostics and dermatologist shortages in resource-limited areas. Current AI models rely on large labeled datasets and are task-specific, limiting real-world effectiveness.

Method: Trained on 432,776 images from public repositories, web-sourced images, and proprietary collections. Uses novel hybrid pretraining framework combining self-supervised learning with semi-supervised learning and knowledge-guided prototype initialization.

Result: Outperforms state-of-the-art models across 20 datasets. Excels in malignancy classification, disease severity grading, multi-category diagnosis, image captioning, and lesion segmentation. Shows strong robustness in federated learning and across diverse skin types/sexes.

Conclusion: DermNIO demonstrates superior performance and generalization across diverse dermatological tasks, significantly outperforming clinicians and enhancing their diagnostic accuracy when used as an AI assistant.

Abstract: Skin diseases impose a substantial burden on global healthcare systems,
driven by their high prevalence (affecting up to 70% of the population),
complex diagnostic processes, and a critical shortage of dermatologists in
resource-limited areas. While artificial intelligence(AI) tools have
demonstrated promise in dermatological image analysis, current models face
limitations-they often rely on large, manually labeled datasets and are built
for narrow, specific tasks, making them less effective in real-world settings.
To tackle these limitations, we present DermNIO, a versatile foundation model
for dermatology. Trained on a curated dataset of 432,776 images from three
sources (public repositories, web-sourced images, and proprietary collections),
DermNIO incorporates a novel hybrid pretraining framework that augments the
self-supervised learning paradigm through semi-supervised learning and
knowledge-guided prototype initialization. This integrated method not only
deepens the understanding of complex dermatological conditions, but also
substantially enhances the generalization capability across various clinical
tasks. Evaluated across 20 datasets, DermNIO consistently outperforms
state-of-the-art models across a wide range of tasks. It excels in high-level
clinical applications including malignancy classification, disease severity
grading, multi-category diagnosis, and dermatological image caption, while also
achieving state-of-the-art performance in low-level tasks such as skin lesion
segmentation. Furthermore, DermNIO demonstrates strong robustness in
privacy-preserving federated learning scenarios and across diverse skin types
and sexes. In a blinded reader study with 23 dermatologists, DermNIO achieved
95.79% diagnostic accuracy (versus clinicians' 73.66%), and AI assistance
improved clinician performance by 17.21%.

</details>


### [540] [FractMorph: A Fractional Fourier-Based Multi-Domain Transformer for Deformable Image Registration](https://arxiv.org/abs/2508.12445)
*Shayan Kebriti,Shahabedin Nabavi,Ali Gooya*

Main category: eess.IV

TL;DR: FractMorph is a novel 3D dual-parallel transformer architecture for deformable image registration that uses multi-domain fractional Fourier transform branches to simultaneously capture local, semi-global, and global deformations in medical images.


<details>
  <summary>Details</summary>
Motivation: Existing deformable image registration approaches struggle to capture both fine-grained local deformations and large-scale global deformations simultaneously within a unified framework, limiting their effectiveness in medical imaging applications.

Method: A 3D dual-parallel transformer architecture with Fractional Cross-Attention blocks that apply parallel FrFTs at 0°, 45°, and 90° angles plus a log-magnitude branch to extract multi-scale features. Features are fused via cross-attention and a lightweight U-Net predicts dense deformation fields.

Result: State-of-the-art performance on ACDC cardiac MRI dataset: 86.45% overall DSC, 75.15% average per-structure DSC, and 1.54mm HD95. Also developed FractMorph-Light variant with 29.6M parameters maintaining similar accuracy with half the memory.

Conclusion: Multi-domain spectral-spatial attention in transformers can robustly and efficiently model complex non-rigid deformations using a single end-to-end network without scenario-specific tuning or hierarchical multi-scale networks.

Abstract: Deformable image registration (DIR) is a crucial and challenging technique
for aligning anatomical structures in medical images and is widely applied in
diverse clinical applications. However, existing approaches often struggle to
capture fine-grained local deformations and large-scale global deformations
simultaneously within a unified framework. We present FractMorph, a novel 3D
dual-parallel transformer-based architecture that enhances cross-image feature
matching through multi-domain fractional Fourier transform (FrFT) branches.
Each Fractional Cross-Attention (FCA) block applies parallel FrFTs at
fractional angles of 0{\deg}, 45{\deg}, 90{\deg}, along with a log-magnitude
branch, to effectively extract local, semi-global, and global features at the
same time. These features are fused via cross-attention between the fixed and
moving image streams. A lightweight U-Net style network then predicts a dense
deformation field from the transformer-enriched features. On the ACDC cardiac
MRI dataset, FractMorph achieves state-of-the-art performance with an overall
Dice Similarity Coefficient (DSC) of 86.45%, an average per-structure DSC of
75.15%, and a 95th-percentile Hausdorff distance (HD95) of 1.54 mm on our data
split. We also introduce FractMorph-Light, a lightweight variant of our model
with only 29.6M parameters, which maintains the superior accuracy of the main
model while using approximately half the memory. Our results demonstrate that
multi-domain spectral-spatial attention in transformers can robustly and
efficiently model complex non-rigid deformations in medical images using a
single end-to-end network, without the need for scenario-specific tuning or
hierarchical multi-scale networks. The source code of our implementation is
available at https://github.com/shayankebriti/FractMorph.

</details>


### [541] [Segmenting Thalamic Nuclei: T1 Maps Provide a Reliable and Efficient Solution](https://arxiv.org/abs/2508.12508)
*Anqi Feng,Zhangxing Bian,Samuel W. Remedios,Savannah P. Hays,Blake E. Dewey,Jiachen Zhuo,Dan Benjamini,Jerry L. Prince*

Main category: eess.IV

TL;DR: T1 maps alone provide the best thalamic nuclei segmentation performance among various MRI contrasts, while PD maps offer no benefit.


<details>
  <summary>Details</summary>
Motivation: Accurate thalamic nuclei segmentation is crucial for neurological disease understanding and clinical interventions, but the optimal MRI inputs remain unclear.

Method: Systematic evaluation of multiple MRI contrasts (MPRAGE, FGATIR, PD maps, T1 maps, multi-TI images) using 3D U-Net. For multi-TI, gradient-based saliency analysis with Monte Carlo dropout and Overall Importance Score to select most contributory images.

Result: T1 maps alone achieve strong quantitative performance and superior qualitative outcomes. PD maps offer no added value for segmentation.

Conclusion: T1 maps are the most reliable and efficient input among evaluated options, providing guidance for optimizing imaging protocols when thalamic structures are of interest.

Abstract: Accurate thalamic nuclei segmentation is crucial for understanding
neurological diseases, brain functions, and guiding clinical interventions.
However, the optimal inputs for segmentation remain unclear. This study
systematically evaluates multiple MRI contrasts, including MPRAGE and FGATIR
sequences, quantitative PD and T1 maps, and multiple T1-weighted images at
different inversion times (multi-TI), to determine the most effective inputs.
For multi-TI images, we employ a gradient-based saliency analysis with Monte
Carlo dropout and propose an Overall Importance Score to select the images
contributing most to segmentation. A 3D U-Net is trained on each of these
configurations. Results show that T1 maps alone achieve strong quantitative
performance and superior qualitative outcomes, while PD maps offer no added
value. These findings underscore the value of T1 maps as a reliable and
efficient input among the evaluated options, providing valuable guidance for
optimizing imaging protocols when thalamic structures are of clinical or
research interest.

</details>


### [542] [Anatomic Feature Fusion Model for Diagnosing Calcified Pulmonary Nodules on Chest X-Ray](https://arxiv.org/abs/2508.12562)
*Hyeonjin Choi,Yang-gon Kim,Dong-yeon Yoo,Ju-sung Sun,Jung-won Lee*

Main category: eess.IV

TL;DR: A calcification classification model using fused features from raw and structure-suppressed chest X-ray images achieves 86.52% accuracy and 0.8889 AUC in identifying benign pulmonary nodules.


<details>
  <summary>Details</summary>
Motivation: Accurate identification of pulmonary nodule calcification on chest X-rays is crucial for early treatment decisions but suffers from physician interpretation variability and anatomical interference from ribs/spine.

Method: Developed a calcification classification model using fused features from both raw images and their structure-suppressed variants to reduce structural interference, trained on 2,517 lesion-free and 656 nodule images (151 calcified, 550 non-calcified).

Result: The model achieved 86.52% accuracy and 0.8889 AUC in calcification diagnosis, outperforming models trained only on raw images by 3.54% and 0.0385 respectively.

Conclusion: The proposed fusion approach with structure-suppressed variants significantly improves calcification classification performance, providing a more reliable tool for differentiating benign pulmonary nodules on chest X-rays.

Abstract: Accurate and timely identification of pulmonary nodules on chest X-rays can
differentiate between life-saving early treatment and avoidable invasive
procedures. Calcification is a definitive indicator of benign nodules and is
the primary foundation for diagnosis. In actual practice, diagnosing pulmonary
nodule calcification on chest X-rays predominantly depends on the physician's
visual assessment, resulting in significant diversity in interpretation.
Furthermore, overlapping anatomical elements, such as ribs and spine,
complicate the precise identification of calcification patterns. This study
presents a calcification classification model that attains strong diagnostic
performance by utilizing fused features derived from raw images and their
structure-suppressed variants to reduce structural interference. We used 2,517
lesion-free images and 656 nodule images (151 calcified nodules and 550
non-calcified nodules), all obtained from Ajou University Hospital. The
suggested model attained an accuracy of 86.52% and an AUC of 0.8889 in
calcification diagnosis, surpassing the model trained on raw images by 3.54%
and 0.0385, respectively.

</details>


### [543] [Learning local and global prototypes with optimal transport for unsupervised anomaly detection and localization](https://arxiv.org/abs/2508.12927)
*Robin Trombetta,Carole Lartizien*

Main category: eess.IV

TL;DR: Novel unsupervised anomaly detection method using prototype learning with optimal transport to balance feature and spatial costs, achieving competitive performance on industrial benchmarks.


<details>
  <summary>Details</summary>
Motivation: Addresses the need for unsupervised anomaly detection in applications like industrial inspection and medical imaging where labeled data is costly or could introduce bias in anomaly types.

Method: Leverages prototype learning with a novel metric combining feature-based and spatial-based costs, uses optimal transport to learn local and global prototypes from pre-trained image encoder embeddings.

Result: Achieves performance on par with strong baselines on two reference benchmarks for industrial image anomaly detection.

Conclusion: The approach effectively enforces structural constraints to capture normal sample organization, improving detection of image incoherencies without requiring labeled anomaly data.

Abstract: Unsupervised anomaly detection aims to detect defective parts of a sample by
having access, during training, to a set of normal, i.e. defect-free, data. It
has many applications in fields, such as industrial inspection or medical
imaging, where acquiring labels is costly or when we want to avoid introducing
biases in the type of anomalies that can be spotted. In this work, we propose a
novel UAD method based on prototype learning and introduce a metric to compare
a structured set of embeddings that balances a feature-based cost and a
spatial-based cost. We leverage this metric to learn local and global
prototypes with optimal transport from latent representations extracted with a
pre-trained image encoder. We demonstrate that our approach can enforce a
structural constraint when learning the prototypes, allowing to capture the
underlying organization of the normal samples, thus improving the detection of
incoherencies in images. Our model achieves performance that is on par with
strong baselines on two reference benchmarks for anomaly detection on
industrial images. The code is available at
https://github.com/robintrmbtt/pradot.

</details>


### [544] [From Transthoracic to Transesophageal: Cross-Modality Generation using LoRA Diffusion](https://arxiv.org/abs/2508.13077)
*Emmanuel Oladokun,Yuxuan Ou,Anna Novikova,Daria Kulikova,Sarina Thomas,Jurica Šprem,Vicente Grau*

Main category: eess.IV

TL;DR: Adapting TTE-trained diffusion models to TEE with minimal data and parameters using Low-Rank Adaptation and MaskR² for effective synthetic TEE image generation and improved segmentation performance.


<details>
  <summary>Details</summary>
Motivation: Address the data scarcity problem in transesophageal echocardiography (TEE) where deep diffusion models require large training sets, limiting deep learning applications in this high-impact medical modality.

Method: Adapt TTE-trained mask-conditioned diffusion backbone to TEE using limited new cases and small adapters (10^5 parameters). Combine Low-Rank Adaptation with MaskR² - a lightweight remapping layer that aligns novel mask formats with pretrained model's conditioning channels. Focus adaptation strategy on MLP layers only.

Result: Successfully generated semantically controlled TEE images with low overhead. MaskR² effectively transforms unseen mask formats without damaging downstream performance. Mixing less than 200 real TEE frames with synthetic echoes improved dice score on multiclass segmentation, particularly boosting performance on underrepresented right-heart structures.

Conclusion: The method enables effective adaptation of diffusion models to new medical imaging domains with minimal data and computational overhead, demonstrating practical utility for improving downstream segmentation tasks in data-scarce medical imaging scenarios.

Abstract: Deep diffusion models excel at realistic image synthesis but demand large
training sets-an obstacle in data-scarce domains like transesophageal
echocardiography (TEE). While synthetic augmentation has boosted performance in
transthoracic echo (TTE), TEE remains critically underrepresented, limiting the
reach of deep learning in this high-impact modality.
  We address this gap by adapting a TTE-trained, mask-conditioned diffusion
backbone to TEE with only a limited number of new cases and adapters as small
as $10^5$ parameters. Our pipeline combines Low-Rank Adaptation with MaskR$^2$,
a lightweight remapping layer that aligns novel mask formats with the
pretrained model's conditioning channels. This design lets users adapt models
to new datasets with a different set of anatomical structures to the base
model's original set.
  Through a targeted adaptation strategy, we find that adapting only MLP layers
suffices for high-fidelity TEE synthesis. Finally, mixing less than 200 real
TEE frames with our synthetic echoes improves the dice score on a multiclass
segmentation task, particularly boosting performance on underrepresented
right-heart structures. Our results demonstrate that (1) semantically
controlled TEE images can be generated with low overhead, (2) MaskR$^2$
effectively transforms unseen mask formats into compatible formats without
damaging downstream task performance, and (3) our method generates images that
are effective for improving performance on a downstream task of multiclass
segmentation.

</details>
