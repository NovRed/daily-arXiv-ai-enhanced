<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 82]
- [cs.AI](#cs.AI) [Total: 20]
- [cs.CL](#cs.CL) [Total: 44]
- [cs.LG](#cs.LG) [Total: 53]
- [quant-ph](#quant-ph) [Total: 1]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.AR](#cs.AR) [Total: 2]
- [cs.CR](#cs.CR) [Total: 6]
- [cs.IR](#cs.IR) [Total: 5]
- [cs.HC](#cs.HC) [Total: 2]
- [cs.IT](#cs.IT) [Total: 1]
- [physics.data-an](#physics.data-an) [Total: 1]
- [cs.GR](#cs.GR) [Total: 2]
- [cs.DC](#cs.DC) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 2]
- [cs.RO](#cs.RO) [Total: 8]
- [cs.DB](#cs.DB) [Total: 2]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.DS](#cs.DS) [Total: 1]
- [cs.SI](#cs.SI) [Total: 3]
- [cs.CY](#cs.CY) [Total: 2]
- [math.NA](#math.NA) [Total: 1]
- [math-ph](#math-ph) [Total: 1]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.CE](#cs.CE) [Total: 1]
- [eess.IV](#eess.IV) [Total: 12]
- [cs.MM](#cs.MM) [Total: 1]
- [stat.ML](#stat.ML) [Total: 5]
- [cs.SE](#cs.SE) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Open-Vocabulary Object Detection in UAV Imagery: A Review and Future Perspectives](https://arxiv.org/abs/2507.13359)
*Yang Zhou,Junjie Li,CongYang Ou,Dawei Yan,Haokui Zhang,Xizhe Xue*

Main category: cs.CV

TL;DR: The paper surveys open-vocabulary object detection (OVOD) in UAV aerial scenes, aligning OVOD principles with UAV vision, categorizing methods, reviewing datasets, and outlining future directions.


<details>
  <summary>Details</summary>
Motivation: Traditional UAV object detection is limited to predefined categories; OVOD, enabled by cross-modal alignment (e.g., CLIP), allows detecting unseen objects via natural language, enhancing UAV autonomy.

Method: The paper constructs a taxonomy of OVOD methods for aerial imagery and reviews relevant datasets, analyzing challenges and open problems.

Result: A systematic review of OVOD in UAV scenes is provided, highlighting key challenges and future research directions.

Conclusion: The survey serves as a roadmap for researchers, fostering innovation in UAV aerial scene understanding through OVOD.

Abstract: Due to its extensive applications, aerial image object detection has long
been a hot topic in computer vision. In recent years, advancements in Unmanned
Aerial Vehicles (UAV) technology have further propelled this field to new
heights, giving rise to a broader range of application requirements. However,
traditional UAV aerial object detection methods primarily focus on detecting
predefined categories, which significantly limits their applicability. The
advent of cross-modal text-image alignment (e.g., CLIP) has overcome this
limitation, enabling open-vocabulary object detection (OVOD), which can
identify previously unseen objects through natural language descriptions. This
breakthrough significantly enhances the intelligence and autonomy of UAVs in
aerial scene understanding. This paper presents a comprehensive survey of OVOD
in the context of UAV aerial scenes. We begin by aligning the core principles
of OVOD with the unique characteristics of UAV vision, setting the stage for a
specialized discussion. Building on this foundation, we construct a systematic
taxonomy that categorizes existing OVOD methods for aerial imagery and provides
a comprehensive overview of the relevant datasets. This structured review
enables us to critically dissect the key challenges and open problems at the
intersection of these fields. Finally, based on this analysis, we outline
promising future research directions and application prospects. This survey
aims to provide a clear road map and a valuable reference for both newcomers
and seasoned researchers, fostering innovation in this rapidly evolving domain.
We keep tracing related works at
https://github.com/zhouyang2002/OVOD-in-UVA-imagery

</details>


### [2] [Low-Light Enhancement via Encoder-Decoder Network with Illumination Guidance](https://arxiv.org/abs/2507.13360)
*Le-Anh Tran,Chung Nguyen Tran,Ngoc-Luu Nguyen,Nhan Cach Dang,Jordi Carrabina,David Castells-Rufas,Minh Son Nguyen*

Main category: cs.CV

TL;DR: EDNIG is a deep learning framework for low-light image enhancement, using illumination guidance and SPP for multi-scale feature extraction, optimized with GAN and composite loss. It outperforms state-of-the-art methods with lower complexity.


<details>
  <summary>Details</summary>
Motivation: Enhancing low-light images is challenging due to underexposed regions and diverse lighting conditions. Existing methods lack focus on illumination guidance and multi-scale feature handling.

Method: EDNIG integrates an illumination map from BCP into a U-Net, uses SPP for multi-scale features, Swish activation, and is optimized with GAN and composite loss (adversarial, MSE, perceptual).

Result: EDNIG achieves competitive performance in metrics and visual quality, with lower complexity than state-of-the-art methods.

Conclusion: EDNIG is effective for low-light enhancement, balancing performance and complexity, making it suitable for real-world applications.

Abstract: This paper introduces a novel deep learning framework for low-light image
enhancement, named the Encoder-Decoder Network with Illumination Guidance
(EDNIG). Building upon the U-Net architecture, EDNIG integrates an illumination
map, derived from Bright Channel Prior (BCP), as a guidance input. This
illumination guidance helps the network focus on underexposed regions,
effectively steering the enhancement process. To further improve the model's
representational power, a Spatial Pyramid Pooling (SPP) module is incorporated
to extract multi-scale contextual features, enabling better handling of diverse
lighting conditions. Additionally, the Swish activation function is employed to
ensure smoother gradient propagation during training. EDNIG is optimized within
a Generative Adversarial Network (GAN) framework using a composite loss
function that combines adversarial loss, pixel-wise mean squared error (MSE),
and perceptual loss. Experimental results show that EDNIG achieves competitive
performance compared to state-of-the-art methods in quantitative metrics and
visual quality, while maintaining lower model complexity, demonstrating its
suitability for real-world applications. The source code for this work is
available at https://github.com/tranleanh/ednig.

</details>


### [3] [VLMs have Tunnel Vision: Evaluating Nonlocal Visual Reasoning in Leading VLMs](https://arxiv.org/abs/2507.13361)
*Shmuel Berman,Jia Deng*

Main category: cs.CV

TL;DR: VLMs struggle with nonlocal visual reasoning tasks like comparative perception, saccadic search, and smooth visual search, despite excelling in complex tasks. Top models fail these tests, highlighting a gap in core visual reasoning.


<details>
  <summary>Details</summary>
Motivation: To evaluate VLMs' capacity for nonlocal visual reasoning, isolating specific challenges where current models underperform compared to humans.

Method: Developed a structured evaluation suite testing three forms of nonlocal vision: comparative perception, saccadic search, and smooth visual search.

Result: Flagship VLMs (e.g., Gemini 2.5 Pro, Claude Vision 3.7, GPT-o4-mini) perform poorly, barely exceeding random accuracy on tasks trivial for humans.

Conclusion: Current VLMs lack core visual reasoning capabilities despite advances in visual acuity, revealing a significant limitation in their design.

Abstract: Visual Language Models (VLMs) excel at complex visual tasks such as VQA and
chart understanding, yet recent work suggests they struggle with simple
perceptual tests. We present an evaluation that tests vision-language models'
capacity for nonlocal visual reasoning -- reasoning that requires chaining
evidence collected from multiple, possibly distant, regions of an image. We
isolate three distinct forms of non-local vision: comparative perception, which
demands holding two images in working memory and comparing them; saccadic
search, which requires making discrete, evidence-driven jumps to locate
successive targets; and smooth visual search, which involves searching smoothly
along a continuous contour. Flagship models (e.g., Gemini 2.5 Pro, Claude
Vision 3.7, GPT-o4-mini), even those that perform well on prior
primitive-vision benchmarks, fail these tests and barely exceed random accuracy
on two variants of our tasks that are trivial for humans. Our structured
evaluation suite allows us to test if VLMs can perform similar visual
algorithms to humans. Our findings show that despite gains in raw visual
acuity, current models lack core visual reasoning capabilities.

</details>


### [4] [Enhancing Spatial Reasoning in Vision-Language Models via Chain-of-Thought Prompting and Reinforcement Learning](https://arxiv.org/abs/2507.13362)
*Binbin Ji,Siddharth Agrawal,Qiance Tang,Yvonne Wu*

Main category: cs.CV

TL;DR: The study explores spatial reasoning in vision-language models (VLMs) using Chain-of-Thought (CoT) prompting and reinforcement learning. Simple CoT harms performance, while structured SceneGraph CoT improves accuracy. Group Relative Policy Optimization (GRPO) outperforms supervised fine-tuning (SFT) in robustness and generalization.


<details>
  <summary>Details</summary>
Motivation: To understand and enhance the spatial reasoning capabilities of VLMs, addressing limitations of existing methods like CoT prompting and SFT.

Method: Evaluated CoT prompting strategies, introduced SceneGraph CoT, and fine-tuned models using GRPO on the SAT dataset, testing on CVBench.

Result: SceneGraph CoT boosts spatial reasoning accuracy. GRPO outperforms SFT in accuracy and robustness, especially under OOD conditions.

Conclusion: Reinforcement learning (GRPO) and structured prompting (SceneGraph CoT) significantly improve VLM spatial reasoning and generalization.

Abstract: This study investigates the spatial reasoning capabilities of vision-language
models (VLMs) through Chain-of-Thought (CoT) prompting and reinforcement
learning. We begin by evaluating the impact of different prompting strategies
and find that simple CoT formats, where the model generates a reasoning step
before the answer, not only fail to help, but can even harm the model's
original performance. In contrast, structured multi-stage prompting based on
scene graphs (SceneGraph CoT) significantly improves spatial reasoning
accuracy. Furthermore, to improve spatial reasoning ability, we fine-tune
models using Group Relative Policy Optimization (GRPO) on the SAT dataset and
evaluate their performance on CVBench. Compared to supervised fine-tuning
(SFT), GRPO achieves higher accuracy on Pass@1 evaluations and demonstrates
superior robustness under out-of-distribution (OOD) conditions. In particular,
we find that SFT overfits to surface-level linguistic patterns and may degrade
performance when test-time phrasing changes (e.g., from "closer to" to "farther
from"). GRPO, on the other hand, generalizes more reliably and maintains stable
performance under such shifts. Our findings provide insights into how
reinforcement learning and structured prompting improve the spatial reasoning
capabilities and generalization behavior of modern VLMs. All code is open
source at: https://github.com/Yvonne511/spatial-vlm-investigator

</details>


### [5] [Just Add Geometry: Gradient-Free Open-Vocabulary 3D Detection Without Human-in-the-Loop](https://arxiv.org/abs/2507.13363)
*Atharv Goel,Mehar Khurana*

Main category: cs.CV

TL;DR: A method for open-vocabulary 3D object detection using 2D vision-language models without human-annotated 3D labels, achieving competitive performance in various settings.


<details>
  <summary>Details</summary>
Motivation: Overcome limitations of narrow class taxonomies and costly manual annotations in 3D object detection by leveraging 2D foundation models for scalable, open-world perception.

Method: Uses a 2D vision-language detector for text-conditioned proposals, segments with SAM, back-projects into 3D using camera geometry and pseudo-depth, and employs geometric inflation for 3D bounding boxes.

Result: Achieves competitive localization performance in LiDAR-based and RGB-D settings, remaining training-free and open-vocabulary.

Conclusion: Demonstrates the potential of 2D foundation models for scalable 3D perception, with open-sourced code and resources.

Abstract: Modern 3D object detection datasets are constrained by narrow class
taxonomies and costly manual annotations, limiting their ability to scale to
open-world settings. In contrast, 2D vision-language models trained on
web-scale image-text pairs exhibit rich semantic understanding and support
open-vocabulary detection via natural language prompts. In this work, we
leverage the maturity and category diversity of 2D foundation models to perform
open-vocabulary 3D object detection without any human-annotated 3D labels.
  Our pipeline uses a 2D vision-language detector to generate text-conditioned
proposals, which are segmented with SAM and back-projected into 3D using camera
geometry and either LiDAR or monocular pseudo-depth. We introduce a geometric
inflation strategy based on DBSCAN clustering and Rotating Calipers to infer 3D
bounding boxes without training. To simulate adverse real-world conditions, we
construct Pseudo-nuScenes, a fog-augmented, RGB-only variant of the nuScenes
dataset.
  Experiments demonstrate that our method achieves competitive localization
performance across multiple settings, including LiDAR-based and purely RGB-D
inputs, all while remaining training-free and open-vocabulary. Our results
highlight the untapped potential of 2D foundation models for scalable 3D
perception. We open-source our code and resources at
https://github.com/atharv0goel/open-world-3D-det.

</details>


### [6] [OmniVec2 -- A Novel Transformer based Network for Large Scale Multimodal and Multitask Learning](https://arxiv.org/abs/2507.13364)
*Siddharth Srivastava,Gaurav Sharma*

Main category: cs.CV

TL;DR: A multimodal multitask network with a shared transformer architecture and cross-attention mechanisms achieves state-of-the-art performance across 12 modalities and 25 datasets.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of processing and integrating diverse data modalities (e.g., image, video, audio) in a unified framework for multitask learning.

Method: Uses modality-specific tokenizers, a shared transformer, and cross-attention for embedding. Introduces iterative modality switching for pretraining and a training algorithm balancing joint and pairwise modality training.

Result: Achieves state-of-the-art performance across 25 datasets from 12 modalities.

Conclusion: The proposed architecture, pretraining strategy, and training algorithm effectively handle multimodal multitask scenarios.

Abstract: We present a novel multimodal multitask network and associated training
algorithm. The method is capable of ingesting data from approximately 12
different modalities namely image, video, audio, text, depth, point cloud, time
series, tabular, graph, X-ray, infrared, IMU, and hyperspectral. The proposed
approach utilizes modality specialized tokenizers, a shared transformer
architecture, and cross-attention mechanisms to project the data from different
modalities into a unified embedding space. It addresses multimodal and
multitask scenarios by incorporating modality-specific task heads for different
tasks in respective modalities. We propose a novel pretraining strategy with
iterative modality switching to initialize the network, and a training
algorithm which trades off fully joint training over all modalities, with
training on pairs of modalities at a time. We provide comprehensive evaluation
across 25 datasets from 12 modalities and show state of the art performances,
demonstrating the effectiveness of the proposed architecture, pretraining
strategy and adapted multitask training.

</details>


### [7] [Transformer-Based Framework for Motion Capture Denoising and Anomaly Detection in Medical Rehabilitation](https://arxiv.org/abs/2507.13371)
*Yeming Cai,Yang Wang,Zhenglin Li*

Main category: cs.CV

TL;DR: An end-to-end deep learning framework using a Transformer-based model enhances medical rehabilitation by denoising motion capture data and detecting anomalies in real time.


<details>
  <summary>Details</summary>
Motivation: Address data noise, missing data due to occlusion, and environmental factors in medical rehabilitation, while ensuring patient safety through real-time anomaly detection.

Method: Integrates optical motion capture with a Transformer-based model for temporal sequence modeling to denoise and complete data.

Result: Superior performance in data reconstruction and anomaly detection on stroke and orthopedic rehabilitation datasets.

Conclusion: The framework offers a scalable, cost-effective solution for remote rehabilitation with reduced on-site supervision.

Abstract: This paper proposes an end-to-end deep learning framework integrating optical
motion capture with a Transformer-based model to enhance medical
rehabilitation. It tackles data noise and missing data caused by occlusion and
environmental factors, while detecting abnormal movements in real time to
ensure patient safety. Utilizing temporal sequence modeling, our framework
denoises and completes motion capture data, improving robustness. Evaluations
on stroke and orthopedic rehabilitation datasets show superior performance in
data reconstruction and anomaly detection, providing a scalable, cost-effective
solution for remote rehabilitation with reduced on-site supervision.

</details>


### [8] [Enhancing Breast Cancer Detection with Vision Transformers and Graph Neural Networks](https://arxiv.org/abs/2507.13372)
*Yeming Cai,Zhenglin Li,Yang Wang*

Main category: cs.CV

TL;DR: The paper presents a hybrid framework combining Vision Transformers (ViT) and Graph Neural Networks (GNN) for improved breast cancer detection, achieving 84.2% accuracy on the CBIS-DDSM dataset.


<details>
  <summary>Details</summary>
Motivation: Early detection of breast cancer is crucial for survival, and advanced methods are needed to improve accuracy and interpretability.

Method: The framework integrates ViT for global image feature extraction and GNN for structural relationship modeling.

Result: The model achieves 84.2% accuracy, surpassing traditional methods, and provides interpretable attention heatmaps.

Conclusion: The hybrid ViT-GNN framework enhances breast cancer detection accuracy and offers interpretability, aiding clinical decision-making.

Abstract: Breast cancer is a leading cause of death among women globally, and early
detection is critical for improving survival rates. This paper introduces an
innovative framework that integrates Vision Transformers (ViT) and Graph Neural
Networks (GNN) to enhance breast cancer detection using the CBIS-DDSM dataset.
Our framework leverages ViT's ability to capture global image features and
GNN's strength in modeling structural relationships, achieving an accuracy of
84.2%, outperforming traditional methods. Additionally, interpretable attention
heatmaps provide insights into the model's decision-making process, aiding
radiologists in clinical settings.

</details>


### [9] [Butter: Frequency Consistency and Hierarchical Fusion for Autonomous Driving Object Detection](https://arxiv.org/abs/2507.13373)
*Xiaojian Lin,Wenxin Zhang,Yuchu Jiang,Wangyu Wu,Yiran Guo,Kangxu Wang,Zongzheng Zhang,Guijin Wang,Lei Jin,Hao Zhao*

Main category: cs.CV

TL;DR: Butter is a new object detection framework for autonomous driving, improving hierarchical feature representation with FAFCE and PHFFNet, enhancing accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing models like YOLO and DETR struggle with feature consistency and computational efficiency in multi-scale object detection for autonomous driving.

Method: Butter introduces FAFCE for adaptive frequency filtering and PHFFNet for progressive feature fusion to refine hierarchical features.

Result: Experiments on BDD100K, KITTI, and Cityscapes show Butter improves detection accuracy and reduces complexity.

Conclusion: Butter balances accuracy and efficiency, offering a robust solution for real-time autonomous driving.

Abstract: Hierarchical feature representations play a pivotal role in computer vision,
particularly in object detection for autonomous driving. Multi-level semantic
understanding is crucial for accurately identifying pedestrians, vehicles, and
traffic signs in dynamic environments. However, existing architectures, such as
YOLO and DETR, struggle to maintain feature consistency across different scales
while balancing detection precision and computational efficiency. To address
these challenges, we propose Butter, a novel object detection framework
designed to enhance hierarchical feature representations for improving
detection robustness. Specifically, Butter introduces two key innovations:
Frequency-Adaptive Feature Consistency Enhancement (FAFCE) Component, which
refines multi-scale feature consistency by leveraging adaptive frequency
filtering to enhance structural and boundary precision, and Progressive
Hierarchical Feature Fusion Network (PHFFNet) Module, which progressively
integrates multi-level features to mitigate semantic gaps and strengthen
hierarchical feature learning. Through extensive experiments on BDD100K, KITTI,
and Cityscapes, Butter demonstrates superior feature representation
capabilities, leading to notable improvements in detection accuracy while
reducing model complexity. By focusing on hierarchical feature refinement and
integration, Butter provides an advanced approach to object detection that
achieves a balance between accuracy, deployability, and computational
efficiency in real-time autonomous driving scenarios. Our model and
implementation are publicly available at https://github.com/Aveiro-Lin/Butter,
facilitating further research and validation within the autonomous driving
community.

</details>


### [10] [Smart Routing for Multimodal Video Retrieval: When to Search What](https://arxiv.org/abs/2507.13374)
*Kevin Dela Rosa*

Main category: cs.CV

TL;DR: ModaRoute is an LLM-based system for multimodal video retrieval, optimizing modality selection to reduce computational costs by 41% while maintaining competitive performance (60.9% Recall@5).


<details>
  <summary>Details</summary>
Motivation: Existing dense text captions are costly and miss visual information (e.g., scene text not captured by ASR), necessitating a smarter routing solution.

Method: Uses GPT-4.1 to analyze query intent and route queries across ASR, OCR, and visual indices, averaging 1.78 modalities per query.

Result: Achieves 60.9% Recall@5, reduces computational overhead by 41%, and averages 1.78 modalities per query (vs. 3.0 in exhaustive search).

Conclusion: ModaRoute offers a scalable, cost-effective solution for multimodal retrieval, balancing performance and efficiency for real-world deployment.

Abstract: We introduce ModaRoute, an LLM-based intelligent routing system that
dynamically selects optimal modalities for multimodal video retrieval. While
dense text captions can achieve 75.9% Recall@5, they require expensive offline
processing and miss critical visual information present in 34% of clips with
scene text not captured by ASR. By analyzing query intent and predicting
information needs, ModaRoute reduces computational overhead by 41% while
achieving 60.9% Recall@5. Our approach uses GPT-4.1 to route queries across ASR
(speech), OCR (text), and visual indices, averaging 1.78 modalities per query
versus exhaustive 3.0 modality search. Evaluation on 1.8M video clips
demonstrates that intelligent routing provides a practical solution for scaling
multimodal retrieval systems, reducing infrastructure costs while maintaining
competitive effectiveness for real-world deployment.

</details>


### [11] [A Comprehensive Survey for Real-World Industrial Defect Detection: Challenges, Approaches, and Prospects](https://arxiv.org/abs/2507.13378)
*Yuqi Cheng,Yunkang Cao,Haiming Yao,Wei Luo,Cheng Jiang,Hui Zhang,Weiming Shen*

Main category: cs.CV

TL;DR: This survey explores industrial defect detection, comparing closed-set and open-set methods in 2D/3D modalities, highlighting advancements and challenges.


<details>
  <summary>Details</summary>
Motivation: Traditional inspection methods fall short in meeting modern manufacturing demands, prompting a shift to more scalable and automated defect detection using computer vision and deep learning.

Method: The paper reviews closed-set and open-set defect detection strategies, analyzing their evolution and practical applications in 2D and 3D modalities.

Result: Open-set techniques are gaining prominence due to reduced dependency on extensive annotations and better handling of novel anomalies.

Conclusion: The survey provides a comprehensive overview of current trends and challenges in industrial defect detection, emphasizing the growing importance of open-set frameworks.

Abstract: Industrial defect detection is vital for upholding product quality across
contemporary manufacturing systems. As the expectations for precision,
automation, and scalability intensify, conventional inspection approaches are
increasingly found wanting in addressing real-world demands. Notable progress
in computer vision and deep learning has substantially bolstered defect
detection capabilities across both 2D and 3D modalities. A significant
development has been the pivot from closed-set to open-set defect detection
frameworks, which diminishes the necessity for extensive defect annotations and
facilitates the recognition of novel anomalies. Despite such strides, a
cohesive and contemporary understanding of industrial defect detection remains
elusive. Consequently, this survey delivers an in-depth analysis of both
closed-set and open-set defect detection strategies within 2D and 3D
modalities, charting their evolution in recent years and underscoring the
rising prominence of open-set techniques. We distill critical challenges
inherent in practical detection environments and illuminate emerging trends,
thereby providing a current and comprehensive vista of this swiftly progressing
field.

</details>


### [12] [Using Multiple Input Modalities Can Improve Data-Efficiency and O.O.D. Generalization for ML with Satellite Imagery](https://arxiv.org/abs/2507.13385)
*Arjun Rao,Esther Rolf*

Main category: cs.CV

TL;DR: The paper explores the benefits of integrating additional geospatial data layers with optical satellite imagery in supervised learning tasks, showing improved model performance, especially in data-limited and out-of-sample scenarios.


<details>
  <summary>Details</summary>
Motivation: To understand the value of combining non-optical geospatial data with optical satellite imagery in machine learning models.

Method: Augmented benchmark tasks by appending extra geographic data layers to datasets for classification, regression, and segmentation.

Result: Fusing additional geographic inputs with optical imagery significantly improves model performance, particularly in data-limited and out-of-sample settings. Hard-coded fusion strategies outperformed learned ones.

Conclusion: Multi-modal inputs enhance data-efficiency and out-of-sample performance in SatML models, with hard-coded fusion being unexpectedly effective.

Abstract: A large variety of geospatial data layers is available around the world
ranging from remotely-sensed raster data like satellite imagery, digital
elevation models, predicted land cover maps, and human-annotated data, to data
derived from environmental sensors such as air temperature or wind speed data.
A large majority of machine learning models trained on satellite imagery
(SatML), however, are designed primarily for optical input modalities such as
multi-spectral satellite imagery. To better understand the value of using other
input modalities alongside optical imagery in supervised learning settings, we
generate augmented versions of SatML benchmark tasks by appending additional
geographic data layers to datasets spanning classification, regression, and
segmentation. Using these augmented datasets, we find that fusing additional
geographic inputs with optical imagery can significantly improve SatML model
performance. Benefits are largest in settings where labeled data are limited
and in geographic out-of-sample settings, suggesting that multi-modal inputs
may be especially valuable for data-efficiency and out-of-sample performance of
SatML models. Surprisingly, we find that hard-coded fusion strategies
outperform learned variants, with interesting implications for future work.

</details>


### [13] [Minimalist Concept Erasure in Generative Models](https://arxiv.org/abs/2507.13386)
*Yang Zhang,Er Jin,Yanfei Dong,Yixuan Wu,Philip Torr,Ashkan Khakzar,Johannes Stegmaier,Kenji Kawaguchi*

Main category: cs.CV

TL;DR: A novel minimalist concept erasure method is proposed to address safety and copyright concerns in generative models, preserving utility while removing unwanted concepts.


<details>
  <summary>Details</summary>
Motivation: To mitigate safety and copyright issues in generative models without excessive modifications that degrade model performance.

Method: Formulates a minimalist erasure objective based on distributional distance of outputs, uses a differentiable loss optimized end-to-end, and incorporates neuron masking for robustness.

Result: Empirical evaluations show robust concept erasure without degrading model performance in state-of-the-art flow-matching models.

Conclusion: The method enables safer and more responsible generative models by effectively erasing unwanted concepts while maintaining utility.

Abstract: Recent advances in generative models have demonstrated remarkable
capabilities in producing high-quality images, but their reliance on
large-scale unlabeled data has raised significant safety and copyright
concerns. Efforts to address these issues by erasing unwanted concepts have
shown promise. However, many existing erasure methods involve excessive
modifications that compromise the overall utility of the model. In this work,
we address these issues by formulating a novel minimalist concept erasure
objective based \emph{only} on the distributional distance of final generation
outputs. Building on our formulation, we derive a tractable loss for
differentiable optimization that leverages backpropagation through all
generation steps in an end-to-end manner. We also conduct extensive analysis to
show theoretical connections with other models and methods. To improve the
robustness of the erasure, we incorporate neuron masking as an alternative to
model fine-tuning. Empirical evaluations on state-of-the-art flow-matching
models demonstrate that our method robustly erases concepts without degrading
overall model performance, paving the way for safer and more responsible
generative models.

</details>


### [14] [From Binary to Semantic: Utilizing Large-Scale Binary Occupancy Data for 3D Semantic Occupancy Prediction](https://arxiv.org/abs/2507.13387)
*Chihiro Noguchi,Takaki Yamamoto*

Main category: cs.CV

TL;DR: The paper proposes a framework leveraging binary occupancy data to enhance 3D semantic occupancy prediction, outperforming existing methods in pre-training and auto-labeling tasks.


<details>
  <summary>Details</summary>
Motivation: Accurate 3D perception is crucial for autonomous driving, but annotated LiDAR data for semantic occupancy prediction is costly. Binary occupancy data is cheaper but underutilized.

Method: A binary occupancy-based framework decomposes prediction into binary and semantic modules, utilizing binary data for pre-training and auto-labeling.

Result: The framework outperforms existing methods in both pre-training and auto-labeling, improving 3D semantic occupancy prediction.

Conclusion: The study demonstrates the effectiveness of binary occupancy data in enhancing 3D semantic occupancy prediction, offering a cost-efficient solution.

Abstract: Accurate perception of the surrounding environment is essential for safe
autonomous driving. 3D occupancy prediction, which estimates detailed 3D
structures of roads, buildings, and other objects, is particularly important
for vision-centric autonomous driving systems that do not rely on LiDAR
sensors. However, in 3D semantic occupancy prediction -- where each voxel is
assigned a semantic label -- annotated LiDAR point clouds are required, making
data acquisition costly. In contrast, large-scale binary occupancy data, which
only indicate occupied or free space without semantic labels, can be collected
at a lower cost. Despite their availability, the potential of leveraging such
data remains unexplored. In this study, we investigate the utilization of
large-scale binary occupancy data from two perspectives: (1) pre-training and
(2) learning-based auto-labeling. We propose a novel binary occupancy-based
framework that decomposes the prediction process into binary and semantic
occupancy modules, enabling effective use of binary occupancy data. Our
experimental results demonstrate that the proposed framework outperforms
existing methods in both pre-training and auto-labeling tasks, highlighting its
effectiveness in enhancing 3D semantic occupancy prediction. The code is
available at https://github.com/ToyotaInfoTech/b2s-occupancy

</details>


### [15] [InSyn: Modeling Complex Interactions for Pedestrian Trajectory Prediction](https://arxiv.org/abs/2507.13397)
*Kaiyuan Zhai,Juan Chen,Chao Wang,Zeyi Xu*

Main category: cs.CV

TL;DR: InSyn, a Transformer-based model, improves pedestrian trajectory prediction by capturing diverse interaction patterns and using SSOS training to reduce initial-step errors.


<details>
  <summary>Details</summary>
Motivation: Existing methods overlook specific pedestrian interaction patterns, limiting accuracy in crowded scenarios.

Method: Proposed InSyn model uses Transformer architecture to capture interaction patterns and SSOS training to address initial-step divergence.

Result: Outperforms baselines on ETH and UCY datasets, with SSOS reducing initial-step error by ~6.58%.

Conclusion: InSyn and SSOS enhance trajectory prediction, especially in high-density scenarios.

Abstract: Accurate pedestrian trajectory prediction is crucial for intelligent
applications, yet it remains highly challenging due to the complexity of
interactions among pedestrians. Previous methods have primarily relied on
relative positions to model pedestrian interactions; however, they tend to
overlook specific interaction patterns such as paired walking or conflicting
behaviors, limiting the prediction accuracy in crowded scenarios. To address
this issue, we propose InSyn (Interaction-Synchronization Network), a novel
Transformer-based model that explicitly captures diverse interaction patterns
(e.g., walking in sync or conflicting) while effectively modeling
direction-sensitive social behaviors. Additionally, we introduce a training
strategy termed Seq-Start of Seq (SSOS), designed to alleviate the common issue
of initial-step divergence in numerical time-series prediction. Experiments on
the ETH and UCY datasets demonstrate that our model outperforms recent
baselines significantly, especially in high-density scenarios. Furthermore, the
SSOS strategy proves effective in improving sequential prediction performance,
reducing the initial-step prediction error by approximately 6.58%.

</details>


### [16] [MADI: Masking-Augmented Diffusion with Inference-Time Scaling for Visual Editing](https://arxiv.org/abs/2507.13401)
*Shreya Kadambi,Risheek Garrepalli,Shubhankar Borse,Munawar Hyatt,Fatih Porikli*

Main category: cs.CV

TL;DR: MADI improves diffusion models for structured, controllable generation and editing via Masking-Augmented gaussian Diffusion (MAgD) and inference-time scaling with Pause Tokens.


<details>
  <summary>Details</summary>
Motivation: Enhancing diffusion models' editability, compositionality, and controllability for grounded visual editing and compositional control.

Method: Introduces MAgD (dual corruption process combining denoising and masked reconstruction) and inference-time scaling with Pause Tokens.

Result: Improved editability, compositionality, and controllability of diffusion models.

Conclusion: MADI advances diffusion models for general-purpose, in-context generative architectures.

Abstract: Despite the remarkable success of diffusion models in text-to-image
generation, their effectiveness in grounded visual editing and compositional
control remains challenging. Motivated by advances in self-supervised learning
and in-context generative modeling, we propose a series of simple yet powerful
design choices that significantly enhance diffusion model capacity for
structured, controllable generation and editing. We introduce Masking-Augmented
Diffusion with Inference-Time Scaling (MADI), a framework that improves the
editability, compositionality and controllability of diffusion models through
two core innovations. First, we introduce Masking-Augmented gaussian Diffusion
(MAgD), a novel training strategy with dual corruption process which combines
standard denoising score matching and masked reconstruction by masking noisy
input from forward process. MAgD encourages the model to learn discriminative
and compositional visual representations, thus enabling localized and
structure-aware editing. Second, we introduce an inference-time capacity
scaling mechanism based on Pause Tokens, which act as special placeholders
inserted into the prompt for increasing computational capacity at inference
time. Our findings show that adopting expressive and dense prompts during
training further enhances performance, particularly for MAgD. Together, these
contributions in MADI substantially enhance the editability of diffusion
models, paving the way toward their integration into more general-purpose,
in-context generative diffusion architectures.

</details>


### [17] [UL-DD: A Multimodal Drowsiness Dataset Using Video, Biometric Signals, and Behavioral Data](https://arxiv.org/abs/2507.13403)
*Morteza Bodaghi,Majid Hosseini,Raju Gottumukkala,Ravi Teja Bhupatiraju,Iftikhar Ahmad,Moncef Gabbouj*

Main category: cs.CV

TL;DR: A multimodal dataset for driver drowsiness detection, including facial, behavioral, and biometric signals, collected from 19 subjects in alert and drowsy states over 40-minute sessions.


<details>
  <summary>Details</summary>
Motivation: To create a comprehensive dataset capturing gradual changes in driver drowsiness using diverse signals, addressing limitations of existing datasets with discrete labels.

Method: Data collection involved 3D facial video, IR footage, posterior videos, biometric signals (heart rate, EDA, SpO2, skin temperature, accelerometer), grip sensor data, and telemetry from a driving simulator. Drowsiness was self-reported every 4 minutes using KSS.

Result: A dataset with 1,400 minutes of continuous, multimodal data from 19 subjects, recording gradual state changes rather than binary labels.

Conclusion: The dataset provides a rich resource for drowsiness detection research, enabling analysis of diverse signals and gradual state transitions.

Abstract: In this study, we present a comprehensive public dataset for driver
drowsiness detection, integrating multimodal signals of facial, behavioral, and
biometric indicators. Our dataset includes 3D facial video using a depth
camera, IR camera footage, posterior videos, and biometric signals such as
heart rate, electrodermal activity, blood oxygen saturation, skin temperature,
and accelerometer data. This data set provides grip sensor data from the
steering wheel and telemetry data from the American truck simulator game to
provide more information about drivers' behavior while they are alert and
drowsy. Drowsiness levels were self-reported every four minutes using the
Karolinska Sleepiness Scale (KSS). The simulation environment consists of three
monitor setups, and the driving condition is completely like a car. Data were
collected from 19 subjects (15 M, 4 F) in two conditions: when they were fully
alert and when they exhibited signs of sleepiness. Unlike other datasets, our
multimodal dataset has a continuous duration of 40 minutes for each data
collection session per subject, contributing to a total length of 1,400
minutes, and we recorded gradual changes in the driver state rather than
discrete alert/drowsy labels. This study aims to create a comprehensive
multimodal dataset of driver drowsiness that captures a wider range of
physiological, behavioral, and driving-related signals. The dataset will be
available upon request to the corresponding author.

</details>


### [18] [AortaDiff: Volume-Guided Conditional Diffusion Models for Multi-Branch Aortic Surface Generation](https://arxiv.org/abs/2507.13404)
*Delin An,Pan Du,Jian-Xun Wang,Chaoli Wang*

Main category: cs.CV

TL;DR: AortaDiff is a diffusion-based framework for generating smooth, CFD-compatible 3D aortic surfaces from CT/MRI volumes, reducing reliance on large annotated datasets and manual intervention.


<details>
  <summary>Details</summary>
Motivation: Accurate 3D aortic construction is essential for clinical and CFD applications, but existing methods require extensive manual effort and large datasets, often producing inconsistent results.

Method: AortaDiff uses a volume-guided conditional diffusion model to iteratively generate aortic centerlines from medical images, then extracts contours and fits them into smooth 3D surfaces.

Result: AortaDiff effectively constructs CFD-compatible meshes, even with limited training data, handling both normal and pathological cases like aneurysms.

Conclusion: AortaDiff provides a practical, end-to-end solution for high-fidelity aortic mesh generation, advancing cardiovascular research and clinical applications.

Abstract: Accurate 3D aortic construction is crucial for clinical diagnosis,
preoperative planning, and computational fluid dynamics (CFD) simulations, as
it enables the estimation of critical hemodynamic parameters such as blood flow
velocity, pressure distribution, and wall shear stress. Existing construction
methods often rely on large annotated training datasets and extensive manual
intervention. While the resulting meshes can serve for visualization purposes,
they struggle to produce geometrically consistent, well-constructed surfaces
suitable for downstream CFD analysis. To address these challenges, we introduce
AortaDiff, a diffusion-based framework that generates smooth aortic surfaces
directly from CT/MRI volumes. AortaDiff first employs a volume-guided
conditional diffusion model (CDM) to iteratively generate aortic centerlines
conditioned on volumetric medical images. Each centerline point is then
automatically used as a prompt to extract the corresponding vessel contour,
ensuring accurate boundary delineation. Finally, the extracted contours are
fitted into a smooth 3D surface, yielding a continuous, CFD-compatible mesh
representation. AortaDiff offers distinct advantages over existing methods,
including an end-to-end workflow, minimal dependency on large labeled datasets,
and the ability to generate CFD-compatible aorta meshes with high geometric
fidelity. Experimental results demonstrate that AortaDiff performs effectively
even with limited training data, successfully constructing both normal and
pathologically altered aorta meshes, including cases with aneurysms or
coarctation. This capability enables the generation of high-quality
visualizations and positions AortaDiff as a practical solution for
cardiovascular research.

</details>


### [19] [COREVQA: A Crowd Observation and Reasoning Entailment Visual Question Answering Benchmark](https://arxiv.org/abs/2507.13405)
*Ishant Chintapatla,Kazuma Choji,Naaisha Agarwal,Andrew Lin,Hannah You,Charles Duong,Kevin Zhu,Sean O'Brien,Vasu Sharma*

Main category: cs.CV

TL;DR: The paper introduces COREVQA, a benchmark for evaluating VLMs on visual entailment tasks using crowded images, revealing significant performance gaps.


<details>
  <summary>Details</summary>
Motivation: Existing benchmarks focus on VQA but lack evaluation of visual entailment, especially in crowded scenes.

Method: Proposed COREVQA, a benchmark with 5608 image and synthetic true/false statement pairs derived from CrowdHuman dataset.

Result: Top VLMs scored below 80% accuracy, with others ranging 39.98%-69.95%, highlighting limitations in crowded scene reasoning.

Conclusion: COREVQA exposes VLMs' weaknesses in visual entailment for crowded images, urging further model improvements.

Abstract: Recently, many benchmarks and datasets have been developed to evaluate
Vision-Language Models (VLMs) using visual question answering (VQA) pairs, and
models have shown significant accuracy improvements. However, these benchmarks
rarely test the model's ability to accurately complete visual entailment, for
instance, accepting or refuting a hypothesis based on the image. To address
this, we propose COREVQA (Crowd Observations and Reasoning Entailment), a
benchmark of 5608 image and synthetically generated true/false statement pairs,
with images derived from the CrowdHuman dataset, to provoke visual entailment
reasoning on challenging crowded images. Our results show that even the
top-performing VLMs achieve accuracy below 80%, with other models performing
substantially worse (39.98%-69.95%). This significant performance gap reveals
key limitations in VLMs' ability to reason over certain types of image-question
pairs in crowded scenes.

</details>


### [20] [IConMark: Robust Interpretable Concept-Based Watermark For AI Images](https://arxiv.org/abs/2507.13407)
*Vinu Sankar Sadasivan,Mehrdad Saberi,Soheil Feizi*

Main category: cs.CV

TL;DR: IConMark is a novel semantic watermarking method for AI-generated images, embedding interpretable concepts to combat adversarial attacks and misinformation. It outperforms traditional methods in robustness and human readability.


<details>
  <summary>Details</summary>
Motivation: The rise of generative AI and synthetic media necessitates robust methods to distinguish AI-generated images from real ones, as traditional watermarking is vulnerable to attacks.

Method: IConMark embeds meaningful semantic attributes into AI-generated images, making watermarks interpretable and resilient. It is combined with StegaStamp and TrustMark for hybrid approaches (IConMark+SS and IConMark+TM).

Result: IConMark and its variants achieve 10.8%, 14.5%, and 15.9% higher AUROC scores for watermark detection compared to baselines, demonstrating superior robustness and image quality.

Conclusion: IConMark offers a robust, interpretable solution for watermarking AI-generated images, with hybrid variants further enhancing resilience against manipulations.

Abstract: With the rapid rise of generative AI and synthetic media, distinguishing
AI-generated images from real ones has become crucial in safeguarding against
misinformation and ensuring digital authenticity. Traditional watermarking
techniques have shown vulnerabilities to adversarial attacks, undermining their
effectiveness in the presence of attackers. We propose IConMark, a novel
in-generation robust semantic watermarking method that embeds interpretable
concepts into AI-generated images, as a first step toward interpretable
watermarking. Unlike traditional methods, which rely on adding noise or
perturbations to AI-generated images, IConMark incorporates meaningful semantic
attributes, making it interpretable to humans and hence, resilient to
adversarial manipulation. This method is not only robust against various image
augmentations but also human-readable, enabling manual verification of
watermarks. We demonstrate a detailed evaluation of IConMark's effectiveness,
demonstrating its superiority in terms of detection accuracy and maintaining
image quality. Moreover, IConMark can be combined with existing watermarking
techniques to further enhance and complement its robustness. We introduce
IConMark+SS and IConMark+TM, hybrid approaches combining IConMark with
StegaStamp and TrustMark, respectively, to further bolster robustness against
multiple types of image manipulations. Our base watermarking technique
(IConMark) and its variants (+TM and +SS) achieve 10.8%, 14.5%, and 15.9%
higher mean area under the receiver operating characteristic curve (AUROC)
scores for watermark detection, respectively, compared to the best baseline on
various datasets.

</details>


### [21] [A Deep Learning-Based Ensemble System for Automated Shoulder Fracture Detection in Clinical Radiographs](https://arxiv.org/abs/2507.13408)
*Hemanth Kumar M,Karthika M,Saianiruth M,Vasanthakumar Venugopal,Anandakumar D,Revathi Ezhumalai,Charulatha K,Kishore Kumar J,Dayana G,Kalyan Sivasailam,Bargava Subramanian*

Main category: cs.CV

TL;DR: AI-driven ensemble model achieves high accuracy (95.5%) in detecting shoulder fractures from X-rays, aiding early diagnosis in clinical settings.


<details>
  <summary>Details</summary>
Motivation: Address underdiagnosis of shoulder fractures in emergency settings by leveraging AI for scalable early detection.

Method: Developed a multi-model deep learning system using 10,000 annotated X-rays, employing architectures like Faster R-CNN, EfficientDet, and RF-DETR, with ensemble techniques (Soft-NMS, WBF, NMW fusion).

Result: NMW ensemble achieved 95.5% accuracy and F1-score of 0.9610, outperforming individual models in recall and localization precision.

Conclusion: Ensemble-based AI reliably detects shoulder fractures with high clinical relevance, suitable for real-time diagnostic workflows, though limited to binary detection for rapid screening.

Abstract: Background: Shoulder fractures are often underdiagnosed, especially in
emergency and high-volume clinical settings. Studies report up to 10% of such
fractures may be missed by radiologists. AI-driven tools offer a scalable way
to assist early detection and reduce diagnostic delays. We address this gap
through a dedicated AI system for shoulder radiographs. Methods: We developed a
multi-model deep learning system using 10,000 annotated shoulder X-rays.
Architectures include Faster R-CNN (ResNet50-FPN, ResNeXt), EfficientDet, and
RF-DETR. To enhance detection, we applied bounding box and classification-level
ensemble techniques such as Soft-NMS, WBF, and NMW fusion. Results: The NMW
ensemble achieved 95.5% accuracy and an F1-score of 0.9610, outperforming
individual models across all key metrics. It demonstrated strong recall and
localization precision, confirming its effectiveness for clinical fracture
detection in shoulder X-rays. Conclusion: The results show ensemble-based AI
can reliably detect shoulder fractures in radiographs with high clinical
relevance. The model's accuracy and deployment readiness position it well for
integration into real-time diagnostic workflows. The current model is limited
to binary fracture detection, reflecting its design for rapid screening and
triage support rather than detailed orthopedic classification.

</details>


### [22] [AI-ming backwards: Vanishing archaeological landscapes in Mesopotamia and automatic detection of sites on CORONA imagery](https://arxiv.org/abs/2507.13420)
*Alessandro Pistola,Valentina Orru',Nicolo' Marchetti,Marco Roccetti*

Main category: cs.CV

TL;DR: Upgrading a deep learning model with CORONA satellite imagery improved archaeological site detection, achieving high accuracy and identifying new sites.


<details>
  <summary>Details</summary>
Motivation: To enhance AI-based identification of archaeological sites in transformed landscapes where many sites are destroyed.

Method: Retrained a Bing-based convolutional network model using CORONA satellite imagery for Abu Ghraib, Iraq.

Result: Increased detection precision (IoU >85%, accuracy 90%) and identified four new archaeological sites.

Conclusion: AI and CORONA imagery are effective for discovering vanished archaeological sites, offering breakthroughs in landscape studies.

Abstract: By upgrading an existing deep learning model with the knowledge provided by
one of the oldest sets of grayscale satellite imagery, known as CORONA, we
improved the AI model attitude towards the automatic identification of
archaeological sites in an environment which has been completely transformed in
the last five decades, including the complete destruction of many of those same
sites. The initial Bing based convolutional network model was retrained using
CORONA satellite imagery for the district of Abu Ghraib, west of Baghdad,
central Mesopotamian floodplain. The results were twofold and surprising.
First, the detection precision obtained on the area of interest increased
sensibly: in particular, the Intersection over Union (IoU) values, at the image
segmentation level, surpassed 85 percent, while the general accuracy in
detecting archeological sites reached 90 percent. Second, our retrained model
allowed the identification of four new sites of archaeological interest
(confirmed through field verification), previously not identified by
archaeologists with traditional techniques. This has confirmed the efficacy of
using AI techniques and the CORONA imagery from the 1960 to discover
archaeological sites currently no longer visible, a concrete breakthrough with
significant consequences for the study of landscapes with vanishing
archaeological evidence induced by anthropization

</details>


### [23] [CaSTFormer: Causal Spatio-Temporal Transformer for Driving Intention Prediction](https://arxiv.org/abs/2507.13425)
*Sirui Wang,Zhou Guan,Bingxi Zhao,Tongjia Gu*

Main category: cs.CV

TL;DR: CaSTFormer, a Causal Spatio-Temporal Transformer, improves driving intention prediction by modeling causal interactions between driver behavior and environmental context, achieving state-of-the-art results.


<details>
  <summary>Details</summary>
Motivation: Current methods fail to accurately model complex spatio-temporal dependencies and variability in human driving behavior, limiting the safety and efficiency of human-machine co-driving systems.

Method: CaSTFormer uses Reciprocal Shift Fusion (RSF) for temporal alignment, Causal Pattern Extraction (CPE) to remove spurious correlations, and a Feature Synthesis Network (FSN) to synthesize purified representations.

Result: Evaluated on the Brain4Cars dataset, CaSTFormer achieves state-of-the-art performance by capturing authentic causal dependencies.

Conclusion: CaSTFormer enhances the accuracy and transparency of driving intention prediction, advancing autonomous driving systems.

Abstract: Accurate prediction of driving intention is key to enhancing the safety and
interactive efficiency of human-machine co-driving systems. It serves as a
cornerstone for achieving high-level autonomous driving. However, current
approaches remain inadequate for accurately modeling the complex
spatio-temporal interdependencies and the unpredictable variability of human
driving behavior. To address these challenges, we propose CaSTFormer, a Causal
Spatio-Temporal Transformer to explicitly model causal interactions between
driver behavior and environmental context for robust intention prediction.
Specifically, CaSTFormer introduces a novel Reciprocal Shift Fusion (RSF)
mechanism for precise temporal alignment of internal and external feature
streams, a Causal Pattern Extraction (CPE) module that systematically
eliminates spurious correlations to reveal authentic causal dependencies, and
an innovative Feature Synthesis Network (FSN) that adaptively synthesizes these
purified representations into coherent spatio-temporal inferences. We evaluate
the proposed CaSTFormer on the public Brain4Cars dataset, and it achieves
state-of-the-art performance. It effectively captures complex causal
spatio-temporal dependencies and enhances both the accuracy and transparency of
driving intention prediction.

</details>


### [24] ["PhyWorldBench": A Comprehensive Evaluation of Physical Realism in Text-to-Video Models](https://arxiv.org/abs/2507.13428)
*Jing Gu,Xian Liu,Yu Zeng,Ashwin Nagarajan,Fangrui Zhu,Daniel Hong,Yue Fan,Qianqi Yan,Kaiwen Zhou,Ming-Yu Liu,Xin Eric Wang*

Main category: cs.CV

TL;DR: PhyWorldBench is introduced to evaluate video generation models' adherence to physics laws, identifying challenges and providing prompt-crafting recommendations.


<details>
  <summary>Details</summary>
Motivation: Current video generation models struggle with accurately simulating physical phenomena, necessitating a benchmark to assess and improve their physics realism.

Method: PhyWorldBench evaluates models using curated prompts across fundamental, composite, and anti-physics scenarios, combining human and MLLM-based zero-shot evaluation.

Result: Testing 12 models reveals challenges in physics adherence, with detailed comparisons and analysis of their performance.

Conclusion: The study highlights key limitations in video generation models and offers targeted recommendations for improving physics realism in outputs.

Abstract: Video generation models have achieved remarkable progress in creating
high-quality, photorealistic content. However, their ability to accurately
simulate physical phenomena remains a critical and unresolved challenge. This
paper presents PhyWorldBench, a comprehensive benchmark designed to evaluate
video generation models based on their adherence to the laws of physics. The
benchmark covers multiple levels of physical phenomena, ranging from
fundamental principles like object motion and energy conservation to more
complex scenarios involving rigid body interactions and human or animal motion.
Additionally, we introduce a novel ""Anti-Physics"" category, where prompts
intentionally violate real-world physics, enabling the assessment of whether
models can follow such instructions while maintaining logical consistency.
Besides large-scale human evaluation, we also design a simple yet effective
method that could utilize current MLLM to evaluate the physics realism in a
zero-shot fashion. We evaluate 12 state-of-the-art text-to-video generation
models, including five open-source and five proprietary models, with a detailed
comparison and analysis. we identify pivotal challenges models face in adhering
to real-world physics. Through systematic testing of their outputs across 1,050
curated prompts-spanning fundamental, composite, and anti-physics scenarios-we
identify pivotal challenges these models face in adhering to real-world
physics. We then rigorously examine their performance on diverse physical
phenomena with varying prompt types, deriving targeted recommendations for
crafting prompts that enhance fidelity to physical principles.

</details>


### [25] [Uncertainty Quantification Framework for Aerial and UAV Photogrammetry through Error Propagation](https://arxiv.org/abs/2507.13486)
*Debao Huang,Rongjun Qin*

Main category: cs.CV

TL;DR: The paper proposes a framework for quantifying uncertainty in photogrammetric point clouds, addressing gaps in Multi-view Stereo (MVS) uncertainty estimation by using self-calibrating methods and reliable 3D points.


<details>
  <summary>Details</summary>
Motivation: Photogrammetric point clouds lack standardized uncertainty quantification in the MVS stage due to its non-differentiable and multi-modal nature, unlike the well-studied SfM stage.

Method: The framework uses a two-step process: SfM with BA and MVS. For MVS uncertainty, it regresses disparity uncertainty using reliable n-view points and relevant cues like matching cost values.

Result: The method outperforms existing approaches, achieving high bounding rates without overestimating uncertainty, as validated on airborne and UAV datasets.

Conclusion: The proposed framework provides robust, certifiable uncertainty quantification for photogrammetric point clouds, closing a critical gap in the field.

Abstract: Uncertainty quantification of the photogrammetry process is essential for
providing per-point accuracy credentials of the point clouds. Unlike airborne
LiDAR, which typically delivers consistent accuracy across various scenes, the
accuracy of photogrammetric point clouds is highly scene-dependent, since it
relies on algorithm-generated measurements (i.e., stereo or multi-view stereo).
Generally, errors of the photogrammetric point clouds propagate through a
two-step process: Structure-from-Motion (SfM) with Bundle adjustment (BA),
followed by Multi-view Stereo (MVS). While uncertainty estimation in the SfM
stage has been well studied using the first-order statistics of the
reprojection error function, that in the MVS stage remains largely unsolved and
non-standardized, primarily due to its non-differentiable and multi-modal
nature (i.e., from pixel values to geometry). In this paper, we present an
uncertainty quantification framework closing this gap by associating an error
covariance matrix per point accounting for this two-step photogrammetry
process. Specifically, to estimate the uncertainty in the MVS stage, we propose
a novel, self-calibrating method by taking reliable n-view points (n>=6)
per-view to regress the disparity uncertainty using highly relevant cues (such
as matching cost values) from the MVS stage. Compared to existing approaches,
our method uses self-contained, reliable 3D points extracted directly from the
MVS process, with the benefit of being self-supervised and naturally adhering
to error propagation path of the photogrammetry process, thereby providing a
robust and certifiable uncertainty quantification across diverse scenes. We
evaluate the framework using a variety of publicly available airborne and UAV
imagery datasets. Results demonstrate that our method outperforms existing
approaches by achieving high bounding rates without overestimating uncertainty.

</details>


### [26] [Sugar-Beet Stress Detection using Satellite Image Time Series](https://arxiv.org/abs/2507.13514)
*Bhumika Laxman Sadbhave,Philipp Vaeth,Denise Dejon,Gunther Schorcht,Magda Gregorová*

Main category: cs.CV

TL;DR: A 3D convolutional autoencoder model is proposed for unsupervised stress detection in sugar-beet fields using Sentinel-2 SITS data, achieving practical applicability across years.


<details>
  <summary>Details</summary>
Motivation: To address stress detection in sugar-beet fields using an unsupervised approach, leveraging the rich spectral and temporal nature of SITS data.

Method: A 3D convolutional autoencoder extracts features from Sentinel-2 sequences, enhanced with temporal encodings. Downstream clustering separates stressed from healthy fields.

Result: The system effectively detects stress and is applicable to data from different years.

Conclusion: The proposed method offers a practical, unsupervised tool for stress detection in sugar-beets.

Abstract: Satellite Image Time Series (SITS) data has proven effective for agricultural
tasks due to its rich spectral and temporal nature. In this study, we tackle
the task of stress detection in sugar-beet fields using a fully unsupervised
approach. We propose a 3D convolutional autoencoder model to extract meaningful
features from Sentinel-2 image sequences, combined with
acquisition-date-specific temporal encodings to better capture the growth
dynamics of sugar-beets. The learned representations are used in a downstream
clustering task to separate stressed from healthy fields. The resulting stress
detection system can be directly applied to data from different years, offering
a practical and accessible tool for stress detection in sugar-beets.

</details>


### [27] [SparseC-AFM: a deep learning method for fast and accurate characterization of MoS$_2$ with C-AFM](https://arxiv.org/abs/2507.13527)
*Levi Harris,Md Jayed Hossain,Mufan Qiu,Ruichen Zhang,Pingchuan Ma,Tianlong Chen,Jiaqi Gu,Seth Ariel Tongay,Umberto Celano*

Main category: cs.CV

TL;DR: SparseC-AFM, a deep learning model, accelerates conductivity mapping of 2D materials like MoS$_2$ from sparse C-AFM scans, reducing acquisition time by 11x while maintaining accuracy.


<details>
  <summary>Details</summary>
Motivation: The slow data acquisition of traditional C-AFM techniques hinders large-scale production of 2D materials, necessitating faster and robust metrology methods.

Method: SparseC-AFM uses deep learning to reconstruct conductivity maps from sparse C-AFM scans, optimizing for speed and accuracy across various conditions.

Result: The method reduces acquisition time to under 5 minutes (vs. 15 minutes for full scans) and accurately extracts material parameters like defect density and film coverage.

Conclusion: SparseC-AFM bridges AI-assisted 2D material characterization from lab research to industrial use, with open-source code available.

Abstract: The increasing use of two-dimensional (2D) materials in nanoelectronics
demands robust metrology techniques for electrical characterization, especially
for large-scale production. While atomic force microscopy (AFM) techniques like
conductive AFM (C-AFM) offer high accuracy, they suffer from slow data
acquisition speeds due to the raster scanning process. To address this, we
introduce SparseC-AFM, a deep learning model that rapidly and accurately
reconstructs conductivity maps of 2D materials like MoS$_2$ from sparse C-AFM
scans. Our approach is robust across various scanning modes, substrates, and
experimental conditions. We report a comparison between (a) classic flow
implementation, where a high pixel density C-AFM image (e.g., 15 minutes to
collect) is manually parsed to extract relevant material parameters, and (b)
our SparseC-AFM method, which achieves the same operation using data that
requires substantially less acquisition time (e.g., under 5 minutes).
SparseC-AFM enables efficient extraction of critical material parameters in
MoS$_2$, including film coverage, defect density, and identification of
crystalline island boundaries, edges, and cracks. We achieve over 11x reduction
in acquisition time compared to manual extraction from a full-resolution C-AFM
image. Moreover, we demonstrate that our model-predicted samples exhibit
remarkably similar electrical properties to full-resolution data gathered using
classic-flow scanning. This work represents a significant step toward
translating AI-assisted 2D material characterization from laboratory research
to industrial fabrication. Code and model weights are available at
github.com/UNITES-Lab/sparse-cafm.

</details>


### [28] [Total Generalized Variation of the Normal Vector Field and Applications to Mesh Denoising](https://arxiv.org/abs/2507.13530)
*Lukas Baumgärtner,Ronny Bergmann,Roland Herzog,Stephan Schmidt,Manuel Weiß*

Main category: cs.CV

TL;DR: A novel second-order TGV formulation for normal vectors on 3D meshes, using a tangential Raviart-Thomas space, is proposed and tested in mesh denoising.


<details>
  <summary>Details</summary>
Motivation: Extend discrete TGV models to manifold-valued functions (normal vectors on meshes) for improved regularization.

Method: Construct a tangential Raviart-Thomas finite element space for the manifold setting and apply it to TGV.

Result: The new regularizer is evaluated in mesh denoising experiments, comparing it to existing methods.

Conclusion: The proposed formulation effectively extends TGV to manifold-valued functions, showing promise in mesh denoising.

Abstract: We propose a novel formulation for the second-order total generalized
variation (TGV) of the normal vector on an oriented, triangular mesh embedded
in $\mathbb{R}^3$. The normal vector is considered as a manifold-valued
function, taking values on the unit sphere. Our formulation extends previous
discrete TGV models for piecewise constant scalar data that utilize a
Raviart-Thomas function space. To exctend this formulation to the manifold
setting, a tailor-made tangential Raviart-Thomas type finite element space is
constructed in this work. The new regularizer is compared to existing methods
in mesh denoising experiments.

</details>


### [29] [$\nabla$NABLA: Neighborhood Adaptive Block-Level Attention](https://arxiv.org/abs/2507.13546)
*Dmitrii Mikhailov,Aleksey Letunovskiy,Maria Kovaleva,Vladimir Arkhipkin,Vladimir Korviakov,Vladimir Polovnikov,Viacheslav Vasilev,Evelina Sidorova,Denis Dimitrov*

Main category: cs.CV

TL;DR: NABLA introduces a Neighborhood Adaptive Block-Level Attention mechanism to reduce computational overhead in video diffusion transformers while maintaining generative quality.


<details>
  <summary>Details</summary>
Motivation: The quadratic complexity of full attention in transformers is a bottleneck for high-resolution and long-duration video generation.

Method: Proposes NABLA, a block-wise attention mechanism with adaptive sparsity-driven thresholds, compatible with PyTorch's Flex Attention.

Result: Achieves up to 2.7x faster training/inference with minimal quality loss (CLIP, VBench, human evaluation scores).

Conclusion: NABLA efficiently addresses computational bottlenecks in video generation without sacrificing performance.

Abstract: Recent progress in transformer-based architectures has demonstrated
remarkable success in video generation tasks. However, the quadratic complexity
of full attention mechanisms remains a critical bottleneck, particularly for
high-resolution and long-duration video sequences. In this paper, we propose
NABLA, a novel Neighborhood Adaptive Block-Level Attention mechanism that
dynamically adapts to sparsity patterns in video diffusion transformers (DiTs).
By leveraging block-wise attention with adaptive sparsity-driven threshold,
NABLA reduces computational overhead while preserving generative quality. Our
method does not require custom low-level operator design and can be seamlessly
integrated with PyTorch's Flex Attention operator. Experiments demonstrate that
NABLA achieves up to 2.7x faster training and inference compared to baseline
almost without compromising quantitative metrics (CLIP score, VBench score,
human evaluation score) and visual quality drop. The code and model weights are
available here: https://github.com/gen-ai-team/Wan2.1-NABLA

</details>


### [30] [LoRA-Loop: Closing the Synthetic Replay Cycle for Continual VLM Learning](https://arxiv.org/abs/2507.13568)
*Kaihong Wang,Donghyun Kim,Margrit Betke*

Main category: cs.CV

TL;DR: A LoRA-enhanced synthetic-replay framework improves continual learning in vision-language models by adapting Stable Diffusion with task-specific low-rank adapters and confidence-based sample selection.


<details>
  <summary>Details</summary>
Motivation: Existing synthetic-replay methods generate misaligned samples due to domain-specific nuances, undermining knowledge retention.

Method: Proposes a two-stage, confidence-based sample selection with LoRA-enhanced Stable Diffusion for task-specific adaptation.

Result: Outperforms previous synthetic-replay techniques on the MTIL benchmark, balancing plasticity, stability, and zero-shot capability.

Conclusion: LoRA adaptation effectively enhances synthetic-replay fidelity for robust continual learning in VLMs.

Abstract: Continual learning for vision-language models has achieved remarkable
performance through synthetic replay, where samples are generated using Stable
Diffusion to regularize during finetuning and retain knowledge. However,
real-world downstream applications often exhibit domain-specific nuances and
fine-grained semantics not captured by generators, causing synthetic-replay
methods to produce misaligned samples that misguide finetuning and undermine
retention of prior knowledge. In this work, we propose a LoRA-enhanced
synthetic-replay framework that injects task-specific low-rank adapters into a
frozen Stable Diffusion model, efficiently capturing each new task's unique
visual and semantic patterns. Specifically, we introduce a two-stage,
confidence-based sample selection: we first rank real task data by
post-finetuning VLM confidence to focus LoRA finetuning on the most
representative examples, then generate synthetic samples and again select them
by confidence for distillation. Our approach integrates seamlessly with
existing replay pipelines-simply swap in the adapted generator to boost replay
fidelity. Extensive experiments on the Multi-domain Task Incremental Learning
(MTIL) benchmark show that our method outperforms previous synthetic-replay
techniques, achieving an optimal balance among plasticity, stability, and
zero-shot capability. These results demonstrate the effectiveness of generator
adaptation via LoRA for robust continual learning in VLMs.

</details>


### [31] [NoiseSDF2NoiseSDF: Learning Clean Neural Fields from Noisy Supervision](https://arxiv.org/abs/2507.13595)
*Tengkai Wang,Weihao Li,Ruikai Cui,Shi Qiu,Nick Barnes*

Main category: cs.CV

TL;DR: NoiseSDF2NoiseSDF extends the Noise2Noise paradigm to 3D neural fields, enabling clean neural SDF learning from noisy point clouds.


<details>
  <summary>Details</summary>
Motivation: Low-quality scanning devices produce noisy point clouds, leading to inaccurate surface reconstructions.

Method: The method minimizes MSE loss between noisy SDF representations to implicitly denoise and refine surfaces.

Result: Significant improvement in surface reconstruction quality on benchmarks like ShapeNet, ABC, Famous, and Real datasets.

Conclusion: NoiseSDF2NoiseSDF effectively addresses noise in 3D point clouds, enhancing surface reconstruction accuracy.

Abstract: Reconstructing accurate implicit surface representations from point clouds
remains a challenging task, particularly when data is captured using
low-quality scanning devices. These point clouds often contain substantial
noise, leading to inaccurate surface reconstructions. Inspired by the
Noise2Noise paradigm for 2D images, we introduce NoiseSDF2NoiseSDF, a novel
method designed to extend this concept to 3D neural fields. Our approach
enables learning clean neural SDFs directly from noisy point clouds through
noisy supervision by minimizing the MSE loss between noisy SDF representations,
allowing the network to implicitly denoise and refine surface estimations. We
evaluate the effectiveness of NoiseSDF2NoiseSDF on benchmarks, including the
ShapeNet, ABC, Famous, and Real datasets. Experimental results demonstrate that
our framework significantly improves surface reconstruction quality from noisy
inputs.

</details>


### [32] [Learning Deblurring Texture Prior from Unpaired Data with Diffusion Model](https://arxiv.org/abs/2507.13599)
*Chengxu Liu,Lu Qi,Jinshan Pan,Xueming Qian,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: A novel diffusion model (DM)-based framework, dubbed \ours, is proposed for unsupervised image deblurring using unpaired data, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Acquiring paired blurry-sharp images is costly and impractical, necessitating unsupervised solutions that handle real-world blur complexity.

Method: The framework uses a Texture Prior Encoder (TPE) with a memory mechanism and a Texture Transfer Transformer (TTformer) with Filter-Modulated Multi-head Self-Attention (FM-MSA) for adaptive blur removal. A wavelet-based adversarial loss preserves texture details.

Result: Extensive evaluations show \ours outperforms existing methods in benchmarks.

Conclusion: \ours offers a promising unsupervised deblurring solution by leveraging diffusion models and texture priors.

Abstract: Since acquiring large amounts of realistic blurry-sharp image pairs is
difficult and expensive, learning blind image deblurring from unpaired data is
a more practical and promising solution. Unfortunately, dominant approaches
rely heavily on adversarial learning to bridge the gap from blurry domains to
sharp domains, ignoring the complex and unpredictable nature of real-world blur
patterns. In this paper, we propose a novel diffusion model (DM)-based
framework, dubbed \ours, for image deblurring by learning spatially varying
texture prior from unpaired data. In particular, \ours performs DM to generate
the prior knowledge that aids in recovering the textures of blurry images. To
implement this, we propose a Texture Prior Encoder (TPE) that introduces a
memory mechanism to represent the image textures and provides supervision for
DM training. To fully exploit the generated texture priors, we present the
Texture Transfer Transformer layer (TTformer), in which a novel
Filter-Modulated Multi-head Self-Attention (FM-MSA) efficiently removes
spatially varying blurring through adaptive filtering. Furthermore, we
implement a wavelet-based adversarial loss to preserve high-frequency texture
details. Extensive evaluations show that \ours provides a promising
unsupervised deblurring solution and outperforms SOTA methods in widely-used
benchmarks.

</details>


### [33] [Efficient Burst Super-Resolution with One-step Diffusion](https://arxiv.org/abs/2507.13607)
*Kento Kawai,Takeru Oba,Kyotaro Tokoro,Kazutoshi Akita,Norimichi Ukita*

Main category: cs.CV

TL;DR: A diffusion model is proposed for burst SR to produce sharp, high-fidelity images, improving efficiency with a stochastic sampler and one-step diffusion, reducing runtime significantly while maintaining quality.


<details>
  <summary>Details</summary>
Motivation: Prior burst SR methods produce blurry images; the goal is to achieve sharp, high-fidelity SR images using a diffusion model.

Method: Uses a diffusion model with a stochastic sampler (high-order ODE) and one-step diffusion via knowledge distillation.

Result: Runtime reduced to 1.6% of baseline while preserving SR quality in distortion and perceptual metrics.

Conclusion: The method efficiently enhances burst SR images, balancing speed and quality.

Abstract: While burst Low-Resolution (LR) images are useful for improving their Super
Resolution (SR) image compared to a single LR image, prior burst SR methods are
trained in a deterministic manner, which produces a blurry SR image. Since such
blurry images are perceptually degraded, we aim to reconstruct sharp and
high-fidelity SR images by a diffusion model. Our method improves the
efficiency of the diffusion model with a stochastic sampler with a high-order
ODE as well as one-step diffusion using knowledge distillation. Our
experimental results demonstrate that our method can reduce the runtime to 1.6
% of its baseline while maintaining the SR quality measured based on image
distortion and perceptual quality.

</details>


### [34] [CoTasks: Chain-of-Thought based Video Instruction Tuning Tasks](https://arxiv.org/abs/2507.13609)
*Yanan Wang,Julio Vizcarra,Zhi Li,Hao Niu,Mori Kurokawa*

Main category: cs.CV

TL;DR: CoTasks introduces a framework for chain-of-thought reasoning in video understanding, improving model performance on tasks like frame localization and entity tracking.


<details>
  <summary>Details</summary>
Motivation: Existing video LLMs lack fine-grained, object-level reasoning abilities due to high-level training data. CoTasks addresses this by decomposing complex video questions into foundational tasks.

Method: CoTasks breaks down video questions into four entity-level tasks (frame localization, entity tracking, spatial/temporal relation extraction) and embeds intermediate reasoning steps.

Result: Models like LLaVA-video-7B and Qwen2.5-VL-3B show significant performance gains (e.g., +17.4 points for Qwen) on the NeXT-QA benchmark.

Conclusion: CoTasks effectively enhances compositional video reasoning through structured, step-by-step supervision.

Abstract: Despite recent progress in video large language models (VideoLLMs), a key
open challenge remains: how to equip models with chain-of-thought (CoT)
reasoning abilities grounded in fine-grained object-level video understanding.
Existing instruction-tuned models, such as the Qwen and LLaVA series, are
trained on high-level video-text pairs, often lacking structured annotations
necessary for compositional, step-by-step reasoning. We propose CoTasks:
Chain-of-Thought based Video Instruction Tuning Tasks, a new framework that
decomposes complex video questions of existing datasets (e.g., NeXT-QA, STAR)
into four entity-level foundational tasks: frame localization, entity tracking,
spatial and temporal relation extraction. By embedding these intermediate
CoT-style reasoning steps into the input, CoTasks enables models to explicitly
perform object-centric spatiotemporal reasoning. Experiments on the NeXT-QA
benchmark show that CoTasks significantly enhance inference performance:
LLaVA-video-7B improves by +3.3 points in average GPT-4 evaluation score, and
Qwen2.5-VL-3B gains +17.4, with large boosts in causal (+14.6), temporal
(+10.9), and descriptive (+48.1) subcategories. These results demonstrate the
effectiveness of CoTasks as a structured CoT-style supervision framework for
improving compositional video reasoning.

</details>


### [35] [Moving Object Detection from Moving Camera Using Focus of Expansion Likelihood and Segmentation](https://arxiv.org/abs/2507.13628)
*Masahiro Ogawa,Qi An,Atsushi Yamashita*

Main category: cs.CV

TL;DR: FoELS integrates optical flow and texture to separate moving and static objects in complex scenes with camera motion, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing optical flow-based methods struggle in complex, structured scenes with camera motion, necessitating a more robust solution.

Method: FoELS combines optical flow (focus of expansion) and texture information, fusing motion likelihood with segmentation-based priors for accurate moving object detection.

Result: FoELS achieves state-of-the-art performance on DAVIS 2016 and real-world traffic videos, handling complex scenes and camera motion effectively.

Conclusion: FoELS provides a robust solution for moving object detection in challenging scenarios, advancing scene understanding and robotics applications.

Abstract: Separating moving and static objects from a moving camera viewpoint is
essential for 3D reconstruction, autonomous navigation, and scene understanding
in robotics. Existing approaches often rely primarily on optical flow, which
struggles to detect moving objects in complex, structured scenes involving
camera motion. To address this limitation, we propose Focus of Expansion
Likelihood and Segmentation (FoELS), a method based on the core idea of
integrating both optical flow and texture information. FoELS computes the focus
of expansion (FoE) from optical flow and derives an initial motion likelihood
from the outliers of the FoE computation. This likelihood is then fused with a
segmentation-based prior to estimate the final moving probability. The method
effectively handles challenges including complex structured scenes, rotational
camera motion, and parallel motion. Comprehensive evaluations on the DAVIS 2016
dataset and real-world traffic videos demonstrate its effectiveness and
state-of-the-art performance.

</details>


### [36] [EPSilon: Efficient Point Sampling for Lightening of Hybrid-based 3D Avatar Generation](https://arxiv.org/abs/2507.13648)
*Seungjun Moon,Sangjoon Yu,Gyeong-Moon Park*

Main category: cs.CV

TL;DR: EPSilon introduces efficient point sampling strategies (ERO and EIO) to speed up hybrid NeRF-SMPL avatar generation by omitting empty points, achieving 20x faster inference and 4x faster training without quality loss.


<details>
  <summary>Details</summary>
Motivation: Hybrid NeRF-SMPL models for animatable avatars are slow due to costly deformation computations on empty points.

Method: Proposes EPSilon with two sampling strategies: Empty Ray Omission (ERO) and Empty Interval Omission (EIO) to skip empty spaces.

Result: Reduces sampled points to 3.9%, achieves 20x faster inference, and 4x faster training while maintaining quality.

Conclusion: EPSilon efficiently improves hybrid avatar generation by focusing computations on relevant regions, enabling faster and scalable training and inference.

Abstract: The rapid advancement of neural radiance fields (NeRF) has paved the way to
generate animatable human avatars from a monocular video. However, the sole
usage of NeRF suffers from a lack of details, which results in the emergence of
hybrid representation that utilizes SMPL-based mesh together with NeRF
representation. While hybrid-based models show photo-realistic human avatar
generation qualities, they suffer from extremely slow inference due to their
deformation scheme: to be aligned with the mesh, hybrid-based models use the
deformation based on SMPL skinning weights, which needs high computational
costs on each sampled point. We observe that since most of the sampled points
are located in empty space, they do not affect the generation quality but
result in inference latency with deformation. In light of this observation, we
propose EPSilon, a hybrid-based 3D avatar generation scheme with novel
efficient point sampling strategies that boost both training and inference. In
EPSilon, we propose two methods to omit empty points at rendering; empty ray
omission (ERO) and empty interval omission (EIO). In ERO, we wipe out rays that
progress through the empty space. Then, EIO narrows down the sampling interval
on the ray, which wipes out the region not occupied by either clothes or mesh.
The delicate sampling scheme of EPSilon enables not only great computational
cost reduction during deformation but also the designation of the important
regions to be sampled, which enables a single-stage NeRF structure without
hierarchical sampling. Compared to existing methods, EPSilon maintains the
generation quality while using only 3.9% of sampled points and achieves around
20 times faster inference, together with 4 times faster training convergence.
We provide video results on https://github.com/seungjun-moon/epsilon.

</details>


### [37] [When Person Re-Identification Meets Event Camera: A Benchmark Dataset and An Attribute-guided Re-Identification Framework](https://arxiv.org/abs/2507.13659)
*Xiao Wang,Qian Zhu,Shujuan Wu,Bo Jiang,Shiliang Zhang,Yaowei Wang,Yonghong Tian,Bin Luo*

Main category: cs.CV

TL;DR: The paper introduces EvReID, a large-scale RGB-event person ReID dataset, and proposes TriPro-ReID, a contrastive learning framework leveraging pedestrian attributes for improved feature learning.


<details>
  <summary>Details</summary>
Motivation: Addressing data scarcity in event camera-based person ReID by providing a large-scale dataset and improving feature learning methods.

Method: Creation of the EvReID dataset and development of the TriPro-ReID framework, which uses pedestrian attributes for contrastive learning.

Result: The EvReID dataset contains 118,988 image pairs across 1200 identities. TriPro-ReID shows effectiveness on EvReID and MARS datasets.

Conclusion: The work provides a benchmark dataset and a robust framework for future RGB-event person ReID research.

Abstract: Recent researchers have proposed using event cameras for person
re-identification (ReID) due to their promising performance and better balance
in terms of privacy protection, event camera-based person ReID has attracted
significant attention. Currently, mainstream event-based person ReID algorithms
primarily focus on fusing visible light and event stream, as well as preserving
privacy. Although significant progress has been made, these methods are
typically trained and evaluated on small-scale or simulated event camera
datasets, making it difficult to assess their real identification performance
and generalization ability. To address the issue of data scarcity, this paper
introduces a large-scale RGB-event based person ReID dataset, called EvReID.
The dataset contains 118,988 image pairs and covers 1200 pedestrian identities,
with data collected across multiple seasons, scenes, and lighting conditions.
We also evaluate 15 state-of-the-art person ReID algorithms, laying a solid
foundation for future research in terms of both data and benchmarking. Based on
our newly constructed dataset, this paper further proposes a pedestrian
attribute-guided contrastive learning framework to enhance feature learning for
person re-identification, termed TriPro-ReID. This framework not only
effectively explores the visual features from both RGB frames and event
streams, but also fully utilizes pedestrian attributes as mid-level semantic
features. Extensive experiments on the EvReID dataset and MARS datasets fully
validated the effectiveness of our proposed RGB-Event person ReID framework.
The benchmark dataset and source code will be released on
https://github.com/Event-AHU/Neuromorphic_ReID

</details>


### [38] [Global Modeling Matters: A Fast, Lightweight and Effective Baseline for Efficient Image Restoration](https://arxiv.org/abs/2507.13663)
*Xingyu Jiang,Ning Gao,Hongkun Dou,Xiuhui Zhang,Xiaoqing Zhong,Yue Deng,Hongjue Li*

Main category: cs.CV

TL;DR: PW-FNet, a novel image restoration method, uses pyramid Wavelet-Fourier processing to improve efficiency and quality, outperforming state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: Adverse weather degrades image quality, hindering downstream tasks. Existing transformer-based methods are complex and inefficient for real-time use.

Method: PW-FNet integrates pyramid wavelet-based multi-scale decomposition and Fourier transforms to replace self-attention, reducing complexity.

Result: PW-FNet excels in tasks like deraining, dehazing, and super-resolution, offering better quality and efficiency than current methods.

Conclusion: PW-FNet is a highly efficient and effective baseline for image restoration, balancing performance and computational cost.

Abstract: Natural image quality is often degraded by adverse weather conditions,
significantly impairing the performance of downstream tasks. Image restoration
has emerged as a core solution to this challenge and has been widely discussed
in the literature. Although recent transformer-based approaches have made
remarkable progress in image restoration, their increasing system complexity
poses significant challenges for real-time processing, particularly in
real-world deployment scenarios. To this end, most existing methods attempt to
simplify the self-attention mechanism, such as by channel self-attention or
state space model. However, these methods primarily focus on network
architecture while neglecting the inherent characteristics of image restoration
itself. In this context, we explore a pyramid Wavelet-Fourier iterative
pipeline to demonstrate the potential of Wavelet-Fourier processing for image
restoration. Inspired by the above findings, we propose a novel and efficient
restoration baseline, named Pyramid Wavelet-Fourier Network (PW-FNet).
Specifically, PW-FNet features two key design principles: 1) at the inter-block
level, integrates a pyramid wavelet-based multi-input multi-output structure to
achieve multi-scale and multi-frequency bands decomposition; and 2) at the
intra-block level, incorporates Fourier transforms as an efficient alternative
to self-attention mechanisms, effectively reducing computational complexity
while preserving global modeling capability. Extensive experiments on tasks
such as image deraining, raindrop removal, image super-resolution, motion
deblurring, image dehazing, image desnowing and underwater/low-light
enhancement demonstrate that PW-FNet not only surpasses state-of-the-art
methods in restoration quality but also achieves superior efficiency, with
significantly reduced parameter size, computational cost and inference time.

</details>


### [39] [MaskHOI: Robust 3D Hand-Object Interaction Estimation via Masked Pre-training](https://arxiv.org/abs/2507.13673)
*Yuechen Xie,Haobo Jiang,Jian Yang,Yigong Zhang,Jin Xie*

Main category: cs.CV

TL;DR: MaskHOI is a novel MAE-driven pretraining framework for 3D hand-object interaction pose estimation, using region-specific masking and SDF-driven learning to improve accuracy.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in 3D HOI pose estimation due to geometric ambiguity and occlusions in monocular RGB input.

Method: Proposes MaskHOI with region-specific masking ratios and skeleton-driven hand masking, plus masked SDF-driven multimodal learning for geometric awareness.

Result: Outperforms state-of-the-art methods in experiments.

Conclusion: MaskHOI effectively enhances pose estimation by addressing occlusion and geometric complexity.

Abstract: In 3D hand-object interaction (HOI) tasks, estimating precise joint poses of
hands and objects from monocular RGB input remains highly challenging due to
the inherent geometric ambiguity of RGB images and the severe mutual occlusions
that occur during interaction.To address these challenges, we propose MaskHOI,
a novel Masked Autoencoder (MAE)-driven pretraining framework for enhanced HOI
pose estimation. Our core idea is to leverage the masking-then-reconstruction
strategy of MAE to encourage the feature encoder to infer missing spatial and
structural information, thereby facilitating geometric-aware and
occlusion-robust representation learning. Specifically, based on our
observation that human hands exhibit far greater geometric complexity than
rigid objects, conventional uniform masking fails to effectively guide the
reconstruction of fine-grained hand structures. To overcome this limitation, we
introduce a Region-specific Mask Ratio Allocation, primarily comprising the
region-specific masking assignment and the skeleton-driven hand masking
guidance. The former adaptively assigns lower masking ratios to hand regions
than to rigid objects, balancing their feature learning difficulty, while the
latter prioritizes masking critical hand parts (e.g., fingertips or entire
fingers) to realistically simulate occlusion patterns in real-world
interactions. Furthermore, to enhance the geometric awareness of the pretrained
encoder, we introduce a novel Masked Signed Distance Field (SDF)-driven
multimodal learning mechanism. Through the self-masking 3D SDF prediction, the
learned encoder is able to perceive the global geometric structure of hands and
objects beyond the 2D image plane, overcoming the inherent limitations of
monocular input and alleviating self-occlusion issues. Extensive experiments
demonstrate that our method significantly outperforms existing state-of-the-art
approaches.

</details>


### [40] [HeCoFuse: Cross-Modal Complementary V2X Cooperative Perception with Heterogeneous Sensors](https://arxiv.org/abs/2507.13677)
*Chuheng Wei,Ziye Qin,Walter Zimmer,Guoyuan Wu,Matthew J. Barth*

Main category: cs.CV

TL;DR: HeCoFuse is a unified framework for V2X cooperative perception, addressing heterogeneous sensor setups via hierarchical fusion and adaptive learning, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Heterogeneous sensor configurations in V2X systems challenge feature fusion and perception reliability, necessitating a robust solution.

Method: HeCoFuse uses hierarchical fusion with channel-wise/spatial attention and adaptive resolution adjustment, plus dynamic cooperative learning.

Result: Achieves 43.22% 3D mAP (LC+LC) and up to 43.38% (L+LC), outperforming baselines and maintaining robustness across nine configurations.

Conclusion: HeCoFuse sets a new benchmark for V2X cooperative perception, validated by top performance in the CVPR 2025 DriveX challenge.

Abstract: Real-world Vehicle-to-Everything (V2X) cooperative perception systems often
operate under heterogeneous sensor configurations due to cost constraints and
deployment variability across vehicles and infrastructure. This heterogeneity
poses significant challenges for feature fusion and perception reliability. To
address these issues, we propose HeCoFuse, a unified framework designed for
cooperative perception across mixed sensor setups where nodes may carry Cameras
(C), LiDARs (L), or both. By introducing a hierarchical fusion mechanism that
adaptively weights features through a combination of channel-wise and spatial
attention, HeCoFuse can tackle critical challenges such as cross-modality
feature misalignment and imbalanced representation quality. In addition, an
adaptive spatial resolution adjustment module is employed to balance
computational cost and fusion effectiveness. To enhance robustness across
different configurations, we further implement a cooperative learning strategy
that dynamically adjusts fusion type based on available modalities. Experiments
on the real-world TUMTraf-V2X dataset demonstrate that HeCoFuse achieves 43.22%
3D mAP under the full sensor configuration (LC+LC), outperforming the CoopDet3D
baseline by 1.17%, and reaches an even higher 43.38% 3D mAP in the L+LC
scenario, while maintaining 3D mAP in the range of 21.74% to 43.38% across nine
heterogeneous sensor configurations. These results, validated by our
first-place finish in the CVPR 2025 DriveX challenge, establish HeCoFuse as the
current state-of-the-art on TUM-Traf V2X dataset while demonstrating robust
performance across diverse sensor deployments.

</details>


### [41] [Gaussian kernel-based motion measurement](https://arxiv.org/abs/2507.13693)
*Hongyi Liu,Haifeng Wang*

Main category: cs.CV

TL;DR: A novel Gaussian kernel-based method for high-precision, sub-pixel motion measurement in structural health monitoring, eliminating manual parameter tuning.


<details>
  <summary>Details</summary>
Motivation: Address the lack of accuracy and need for extensive manual tuning in current vision-based motion measurement methods for structural health monitoring.

Method: Developed a Gaussian kernel-based method with motion consistency and super-resolution constraints to enhance accuracy and robustness.

Result: Achieves high accuracy consistently without requiring customized parameter setups for different samples.

Conclusion: The proposed method offers a reliable, efficient solution for sub-pixel motion measurement in structural health monitoring.

Abstract: The growing demand for structural health monitoring has driven increasing
interest in high-precision motion measurement, as structural information
derived from extracted motions can effectively reflect the current condition of
the structure. Among various motion measurement techniques, vision-based
methods stand out due to their low cost, easy installation, and large-scale
measurement. However, when it comes to sub-pixel-level motion measurement,
current vision-based methods either lack sufficient accuracy or require
extensive manual parameter tuning (e.g., pyramid layers, target pixels, and
filter parameters) to reach good precision. To address this issue, we developed
a novel Gaussian kernel-based motion measurement method, which can extract the
motion between different frames via tracking the location of Gaussian kernels.
The motion consistency, which fits practical structural conditions, and a
super-resolution constraint, are introduced to increase accuracy and robustness
of our method. Numerical and experimental validations show that it can
consistently reach high accuracy without customized parameter setup for
different test samples.

</details>


### [42] [GOSPA and T-GOSPA quasi-metrics for evaluation of multi-object tracking algorithms](https://arxiv.org/abs/2507.13706)
*Ángel F. García-Fernández,Jinhao Gu,Lennart Svensson,Yuxuan Xia,Jan Krejčí,Oliver Kost,Ondřej Straka*

Main category: cs.CV

TL;DR: The paper introduces two quasi-metrics for evaluating multi-object tracking (MOT) algorithms, extending GOSPA and T-GOSPA metrics with flexible cost penalties.


<details>
  <summary>Details</summary>
Motivation: To provide more flexible and application-specific performance assessment tools for MOT algorithms.

Method: Extends GOSPA and T-GOSPA metrics into quasi-metrics with adjustable costs for missed/false objects and asymmetric localization errors.

Result: The proposed quasi-metrics effectively evaluate MOT algorithms, demonstrated via simulations.

Conclusion: The quasi-metrics offer adaptable and practical tools for MOT performance assessment in diverse applications.

Abstract: This paper introduces two quasi-metrics for performance assessment of
multi-object tracking (MOT) algorithms. In particular, one quasi-metric is an
extension of the generalised optimal subpattern assignment (GOSPA) metric and
measures the discrepancy between sets of objects. The other quasi-metric is an
extension of the trajectory GOSPA (T-GOSPA) metric and measures the discrepancy
between sets of trajectories. Similar to the GOSPA-based metrics, these
quasi-metrics include costs for localisation error for properly detected
objects, the number of false objects and the number of missed objects. The
T-GOSPA quasi-metric also includes a track switching cost. Differently from the
GOSPA and T-GOSPA metrics, the proposed quasi-metrics have the flexibility of
penalising missed and false objects with different costs, and the localisation
costs are not required to be symmetric. These properties can be useful in MOT
evaluation in certain applications. The performance of several Bayesian MOT
algorithms is assessed with the T-GOSPA quasi-metric via simulations.

</details>


### [43] [PoemTale Diffusion: Minimising Information Loss in Poem to Image Generation with Multi-Stage Prompt Refinement](https://arxiv.org/abs/2507.13708)
*Sofia Jamil,Bollampalli Areen Reddy,Raghvendra Kumar,Sriparna Saha,Koustava Goswami,K. J. Joseph*

Main category: cs.CV

TL;DR: The paper introduces PoemTale Diffusion, a training-free method to improve image generation from poetic texts by refining prompts and modifying self-attention mechanisms in diffusion models.


<details>
  <summary>Details</summary>
Motivation: Text-to-image models struggle with creative expressions like poetic verse due to their abstract and layered meanings.

Method: Proposes a multi-stage prompt refinement loop and modifies self-attention in diffusion models to generate consistent images for poems.

Result: Validated by human and quantitative evaluations, the method enhances poem-to-image generation.

Conclusion: PoemTale Diffusion offers a novel approach to better capture poetic meanings in generated images.

Abstract: Recent advancements in text-to-image diffusion models have achieved
remarkable success in generating realistic and diverse visual content. A
critical factor in this process is the model's ability to accurately interpret
textual prompts. However, these models often struggle with creative
expressions, particularly those involving complex, abstract, or highly
descriptive language. In this work, we introduce a novel training-free approach
tailored to improve image generation for a unique form of creative language:
poetic verse, which frequently features layered, abstract, and dual meanings.
Our proposed PoemTale Diffusion approach aims to minimise the information that
is lost during poetic text-to-image conversion by integrating a multi stage
prompt refinement loop into Language Models to enhance the interpretability of
poetic texts. To support this, we adapt existing state-of-the-art diffusion
models by modifying their self-attention mechanisms with a consistent
self-attention technique to generate multiple consistent images, which are then
collectively used to convey the poem's meaning. Moreover, to encourage research
in the field of poetry, we introduce the P4I (PoemForImage) dataset, consisting
of 1111 poems sourced from multiple online and offline resources. We engaged a
panel of poetry experts for qualitative assessments. The results from both
human and quantitative evaluations validate the efficacy of our method and
contribute a novel perspective to poem-to-image generation with enhanced
information capture in the generated images.

</details>


### [44] [Augmented Reality in Cultural Heritage: A Dual-Model Pipeline for 3D Artwork Reconstruction](https://arxiv.org/abs/2507.13719)
*Daniele Pannone,Alessia Castronovo,Maurizio Mancini,Gian Luca Foresti,Claudio Piciarelli,Rossana Gabrieli,Muhammad Yasir Bilal,Danilo Avola*

Main category: cs.CV

TL;DR: An AR pipeline for museums uses dual depth estimation models (GLPN and Depth-Anything) to create accurate 3D models from single images, enhancing visitor engagement.


<details>
  <summary>Details</summary>
Motivation: To improve artwork recognition and 3D reconstruction in museums for immersive AR experiences.

Method: Integrates GLPN for global structure and Depth-Anything for local details, converting depth maps into point clouds and meshes.

Result: Achieves higher reconstruction accuracy and visual realism, proving robust for interactive museum content.

Conclusion: The system effectively enhances visitor engagement through advanced AR technology.

Abstract: This paper presents an innovative augmented reality pipeline tailored for
museum environments, aimed at recognizing artworks and generating accurate 3D
models from single images. By integrating two complementary pre-trained depth
estimation models, i.e., GLPN for capturing global scene structure and
Depth-Anything for detailed local reconstruction, the proposed approach
produces optimized depth maps that effectively represent complex artistic
features. These maps are then converted into high-quality point clouds and
meshes, enabling the creation of immersive AR experiences. The methodology
leverages state-of-the-art neural network architectures and advanced computer
vision techniques to overcome challenges posed by irregular contours and
variable textures in artworks. Experimental results demonstrate significant
improvements in reconstruction accuracy and visual realism, making the system a
highly robust tool for museums seeking to enhance visitor engagement through
interactive digital content.

</details>


### [45] [Tackling fake images in cybersecurity -- Interpretation of a StyleGAN and lifting its black-box](https://arxiv.org/abs/2507.13722)
*Julia Laubmann,Johannes Reschke*

Main category: cs.CV

TL;DR: The paper analyzes StyleGAN's generator, revealing its efficiency through weight pruning and the latent vector's role in fine-tuning facial features, while highlighting ethical concerns about misuse.


<details>
  <summary>Details</summary>
Motivation: To understand StyleGAN's inner workings and its potential for misuse in generating realistic synthetic faces.

Method: Analyzes StyleGAN's architecture, trains a model in PyTorch, and examines weight pruning and latent vector manipulation.

Result: Many weights can be pruned without major output changes, and latent vectors allow precise facial feature control.

Conclusion: StyleGAN's efficiency and control pose ethical risks, emphasizing the need for safeguards against misuse.

Abstract: In today's digital age, concerns about the dangers of AI-generated images are
increasingly common. One powerful tool in this domain is StyleGAN (style-based
generative adversarial networks), a generative adversarial network capable of
producing highly realistic synthetic faces. To gain a deeper understanding of
how such a model operates, this work focuses on analyzing the inner workings of
StyleGAN's generator component. Key architectural elements and techniques, such
as the Equalized Learning Rate, are explored in detail to shed light on the
model's behavior. A StyleGAN model is trained using the PyTorch framework,
enabling direct inspection of its learned weights. Through pruning, it is
revealed that a significant number of these weights can be removed without
drastically affecting the output, leading to reduced computational
requirements. Moreover, the role of the latent vector -- which heavily
influences the appearance of the generated faces -- is closely examined. Global
alterations to this vector primarily affect aspects like color tones, while
targeted changes to individual dimensions allow for precise manipulation of
specific facial features. This ability to finetune visual traits is not only of
academic interest but also highlights a serious ethical concern: the potential
misuse of such technology. Malicious actors could exploit this capability to
fabricate convincing fake identities, posing significant risks in the context
of digital deception and cybercrime.

</details>


### [46] [Can Synthetic Images Conquer Forgetting? Beyond Unexplored Doubts in Few-Shot Class-Incremental Learning](https://arxiv.org/abs/2507.13739)
*Junsu Kim,Yunhoe Ku,Seungryul Baek*

Main category: cs.CV

TL;DR: Diffusion-FSCIL uses a frozen text-to-image diffusion model for few-shot class-incremental learning, leveraging its generative and representational strengths to outperform existing methods.


<details>
  <summary>Details</summary>
Motivation: Address the challenges of few-shot class-incremental learning (FSCIL), such as limited training data and catastrophic forgetting, by utilizing a pre-trained generative model's capabilities.

Method: Employ a frozen text-to-image diffusion model as a backbone, extract multiple complementary diffusion features for latent replay, and use minimal trainable components for efficiency.

Result: Outperforms state-of-the-art methods on CUB-200, miniImageNet, and CIFAR-100, maintaining performance on old classes and adapting well to new ones.

Conclusion: Diffusion-FSCIL effectively tackles FSCIL challenges by leveraging generative models, offering a scalable and efficient solution.

Abstract: Few-shot class-incremental learning (FSCIL) is challenging due to extremely
limited training data; while aiming to reduce catastrophic forgetting and learn
new information. We propose Diffusion-FSCIL, a novel approach that employs a
text-to-image diffusion model as a frozen backbone. Our conjecture is that
FSCIL can be tackled using a large generative model's capabilities benefiting
from 1) generation ability via large-scale pre-training; 2) multi-scale
representation; 3) representational flexibility through the text encoder. To
maximize the representation capability, we propose to extract multiple
complementary diffusion features to play roles as latent replay with slight
support from feature distillation for preventing generative biases. Our
framework realizes efficiency through 1) using a frozen backbone; 2) minimal
trainable components; 3) batch processing of multiple feature extractions.
Extensive experiments on CUB-200, \emph{mini}ImageNet, and CIFAR-100 show that
Diffusion-FSCIL surpasses state-of-the-art methods, preserving performance on
previously learned classes and adapting effectively to new ones.

</details>


### [47] [Encapsulated Composition of Text-to-Image and Text-to-Video Models for High-Quality Video Synthesis](https://arxiv.org/abs/2507.13753)
*Tongtong Su,Chengyu Wang,Bingyan Liu,Jun Huang,Dongming Lu*

Main category: cs.CV

TL;DR: EVS combines T2I and T2V models to improve video quality and motion smoothness without training, achieving faster inference and better results.


<details>
  <summary>Details</summary>
Motivation: Existing T2V models struggle with visual fidelity and motion consistency, often causing flickering and artifacts.

Method: EVS uses a diffusion-based T2I model to refine frames and T2V backbones for motion consistency, integrating both without training.

Result: EVS enhances video quality and motion smoothness, with a 1.6x-4.5x speedup in inference time.

Conclusion: EVS effectively leverages T2I and T2V models, offering a training-free solution for high-quality video synthesis.

Abstract: In recent years, large text-to-video (T2V) synthesis models have garnered
considerable attention for their abilities to generate videos from textual
descriptions. However, achieving both high imaging quality and effective motion
representation remains a significant challenge for these T2V models. Existing
approaches often adapt pre-trained text-to-image (T2I) models to refine video
frames, leading to issues such as flickering and artifacts due to
inconsistencies across frames. In this paper, we introduce EVS, a training-free
Encapsulated Video Synthesizer that composes T2I and T2V models to enhance both
visual fidelity and motion smoothness of generated videos. Our approach
utilizes a well-trained diffusion-based T2I model to refine low-quality video
frames by treating them as out-of-distribution samples, effectively optimizing
them with noising and denoising steps. Meanwhile, we employ T2V backbones to
ensure consistent motion dynamics. By encapsulating the T2V temporal-only prior
into the T2I generation process, EVS successfully leverages the strengths of
both types of models, resulting in videos of improved imaging and motion
quality. Experimental results validate the effectiveness of our approach
compared to previous approaches. Our composition process also leads to a
significant improvement of 1.6x-4.5x speedup in inference time. Source codes:
https://github.com/Tonniia/EVS.

</details>


### [48] [Learning Spectral Diffusion Prior for Hyperspectral Image Reconstruction](https://arxiv.org/abs/2507.13769)
*Mingyang Yu,Zhijian Wu,Dingjiang Huang*

Main category: cs.CV

TL;DR: A Spectral Diffusion Prior (SDP) and Spectral Prior Injector Module (SPIM) are proposed to improve HSI reconstruction by capturing high-frequency details, outperforming existing methods by 0.5 dB.


<details>
  <summary>Details</summary>
Motivation: Deep learning-based HSI reconstruction struggles with high-frequency detail recovery, prompting the need for a better prior and dynamic guidance.

Method: Uses a diffusion model to learn SDP and introduces SPIM for dynamic detail recovery in HSI models.

Result: Outperforms MST and BISRNet by ~0.5 dB, enhancing HSI reconstruction.

Conclusion: SDP and SPIM effectively improve HSI reconstruction by leveraging diffusion models and dynamic guidance.

Abstract: Hyperspectral image (HSI) reconstruction aims to recover 3D HSI from its
degraded 2D measurements. Recently great progress has been made in deep
learning-based methods, however, these methods often struggle to accurately
capture high-frequency details of the HSI. To address this issue, this paper
proposes a Spectral Diffusion Prior (SDP) that is implicitly learned from
hyperspectral images using a diffusion model. Leveraging the powerful ability
of the diffusion model to reconstruct details, this learned prior can
significantly improve the performance when injected into the HSI model. To
further improve the effectiveness of the learned prior, we also propose the
Spectral Prior Injector Module (SPIM) to dynamically guide the model to recover
the HSI details. We evaluate our method on two representative HSI methods: MST
and BISRNet. Experimental results show that our method outperforms existing
networks by about 0.5 dB, effectively improving the performance of HSI
reconstruction.

</details>


### [49] [Feature Engineering is Not Dead: Reviving Classical Machine Learning with Entropy, HOG, and LBP Feature Fusion for Image Classification](https://arxiv.org/abs/2507.13772)
*Abhijit Sen,Giridas Maiti,Bikram K. Parida,Bhanu P. Mishra,Mahima Arya,Denys I. Bondar*

Main category: cs.CV

TL;DR: The paper proposes a novel image classification method using Permutation Entropy (PE) combined with HOG and LBP features, achieving competitive results without deep learning.


<details>
  <summary>Details</summary>
Motivation: To provide an interpretable and computationally efficient alternative to deep learning models for image classification.

Method: Extends PE to 2D images, integrates HOG and LBP for feature extraction, and trains SVM classifiers.

Result: Competitive performance on benchmark datasets (Fashion-MNIST, KMNIST, EMNIST, CIFAR-10).

Conclusion: PE with HOG and LBP offers a lightweight, interpretable, and effective solution for image classification.

Abstract: Feature engineering continues to play a critical role in image
classification, particularly when interpretability and computational efficiency
are prioritized over deep learning models with millions of parameters. In this
study, we revisit classical machine learning based image classification through
a novel approach centered on Permutation Entropy (PE), a robust and
computationally lightweight measure traditionally used in time series analysis
but rarely applied to image data. We extend PE to two-dimensional images and
propose a multiscale, multi-orientation entropy-based feature extraction
approach that characterizes spatial order and complexity along rows, columns,
diagonals, anti-diagonals, and local patches of the image. To enhance the
discriminatory power of the entropy features, we integrate two classic image
descriptors: the Histogram of Oriented Gradients (HOG) to capture shape and
edge structure, and Local Binary Patterns (LBP) to encode micro-texture of an
image. The resulting hand-crafted feature set, comprising of 780 dimensions, is
used to train Support Vector Machine (SVM) classifiers optimized through grid
search. The proposed approach is evaluated on multiple benchmark datasets,
including Fashion-MNIST, KMNIST, EMNIST, and CIFAR-10, where it delivers
competitive classification performance without relying on deep architectures.
Our results demonstrate that the fusion of PE with HOG and LBP provides a
compact, interpretable, and effective alternative to computationally expensive
and limited interpretable deep learning models. This shows a potential of
entropy-based descriptors in image classification and contributes a lightweight
and generalizable solution to interpretable machine learning in image
classification and computer vision.

</details>


### [50] [Teaching Vision-Language Models to Ask: Resolving Ambiguity in Visual Questions](https://arxiv.org/abs/2507.13773)
*Pu Jian,Donglei Yu,Wen Yang,Shuo Ren,Jiajun Zhang*

Main category: cs.CV

TL;DR: The paper introduces ClearVQA, a benchmark for evaluating visual language models' ability to resolve ambiguities in visual question answering through interactive clarification.


<details>
  <summary>Details</summary>
Motivation: Existing VQA research focuses on rephrasing ambiguous questions but ignores the interactive nature of user feedback for clarification.

Method: The authors propose ClearVQA, a benchmark targeting three common ambiguity categories in VQA, designed to assess VLMs' interactive clarification capabilities.

Result: ClearVQA addresses the lack of benchmarks and VLMs' tendency to answer rather than seek clarification.

Conclusion: ClearVQA provides a framework for improving VLMs' interactive ambiguity resolution in VQA contexts.

Abstract: In visual question answering (VQA) context, users often pose ambiguous
questions to visual language models (VLMs) due to varying expression habits.
Existing research addresses such ambiguities primarily by rephrasing questions.
These approaches neglect the inherently interactive nature of user interactions
with VLMs, where ambiguities can be clarified through user feedback. However,
research on interactive clarification faces two major challenges: (1)
Benchmarks are absent to assess VLMs' capacity for resolving ambiguities
through interaction; (2) VLMs are trained to prefer answering rather than
asking, preventing them from seeking clarification. To overcome these
challenges, we introduce \textbf{ClearVQA} benchmark, which targets three
common categories of ambiguity in VQA context, and encompasses various VQA
scenarios.

</details>


### [51] [SuperCM: Improving Semi-Supervised Learning and Domain Adaptation through differentiable clustering](https://arxiv.org/abs/2507.13779)
*Durgesh Singh,Ahcène Boubekki,Robert Jenssen,Michael Kampffmeyer*

Main category: cs.CV

TL;DR: The paper introduces a differentiable clustering module for SSL and UDA, leveraging supervised data to compute centroids, improving performance in low supervision settings.


<details>
  <summary>Details</summary>
Motivation: To enhance SSL and UDA by explicitly enforcing the clustering assumption, which is typically implicit in existing methods.

Method: Proposes an end-to-end training strategy with a differentiable clustering module, using supervised data to compute centroids.

Result: Demonstrates effectiveness in SSL and UDA, particularly in low supervision regimes, as a standalone model and regularizer.

Conclusion: The explicit clustering approach improves performance and flexibility in SSL and UDA tasks.

Abstract: Semi-Supervised Learning (SSL) and Unsupervised Domain Adaptation (UDA)
enhance the model performance by exploiting information from labeled and
unlabeled data. The clustering assumption has proven advantageous for learning
with limited supervision and states that data points belonging to the same
cluster in a high-dimensional space should be assigned to the same category.
Recent works have utilized different training mechanisms to implicitly enforce
this assumption for the SSL and UDA. In this work, we take a different approach
by explicitly involving a differentiable clustering module which is extended to
leverage the supervised data to compute its centroids. We demonstrate the
effectiveness of our straightforward end-to-end training strategy for SSL and
UDA over extensive experiments and highlight its benefits, especially in low
supervision regimes, both as a standalone model and as a regularizer for
existing approaches.

</details>


### [52] [Localized FNO for Spatiotemporal Hemodynamic Upsampling in Aneurysm MRI](https://arxiv.org/abs/2507.13789)
*Kyriakos Flouris,Moritz Halter,Yolanne Y. R. Lee,Samuel Castonguay,Luuk Jacobs,Pietro Dirix,Jonathan Nestmann,Sebastian Kozerke,Ender Konukoglu*

Main category: cs.CV

TL;DR: LoFNO enhances hemodynamic analysis by improving spatiotemporal resolution and predicting WSS directly from clinical data, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Current magnetic resonance flow imaging has low resolution and noise, limiting its diagnostic utility for aneurysm rupture prediction.

Method: Proposes LoFNO, a 3D architecture using Laplacian eigenvectors for geometric priors and EDSR for upsampling to enhance resolution and predict WSS.

Result: LoFNO outperforms interpolation and other deep learning methods in velocity and WSS predictions.

Conclusion: LoFNO enables more precise cerebrovascular diagnostics by improving data quality and predictive accuracy.

Abstract: Hemodynamic analysis is essential for predicting aneurysm rupture and guiding
treatment. While magnetic resonance flow imaging enables time-resolved
volumetric blood velocity measurements, its low spatiotemporal resolution and
signal-to-noise ratio limit its diagnostic utility. To address this, we propose
the Localized Fourier Neural Operator (LoFNO), a novel 3D architecture that
enhances both spatial and temporal resolution with the ability to predict wall
shear stress (WSS) directly from clinical imaging data. LoFNO integrates
Laplacian eigenvectors as geometric priors for improved structural awareness on
irregular, unseen geometries and employs an Enhanced Deep Super-Resolution
Network (EDSR) layer for robust upsampling. By combining geometric priors with
neural operator frameworks, LoFNO de-noises and spatiotemporally upsamples flow
data, achieving superior velocity and WSS predictions compared to interpolation
and alternative deep learning methods, enabling more precise cerebrovascular
diagnostics.

</details>


### [53] [DynFaceRestore: Balancing Fidelity and Quality in Diffusion-Guided Blind Face Restoration with Dynamic Blur-Level Mapping and Guidance](https://arxiv.org/abs/2507.13797)
*Huu-Phu Do,Yu-Wei Chen,Yi-Cheng Liao,Chi-Wei Hsiao,Han-Yang Wang,Wei-Chen Chiu,Ching-Chun Huang*

Main category: cs.CV

TL;DR: DynFaceRestore dynamically adjusts diffusion sampling and guidance for blind face restoration, balancing fidelity and quality.


<details>
  <summary>Details</summary>
Motivation: Existing methods use fixed diffusion sampling and global guidance, leading to under- or over-diffusion. DynFaceRestore addresses this imbalance.

Method: Learns to map degraded inputs to Gaussian blurry images, dynamically selects timesteps, and uses closed-form guidance with dynamic scaling.

Result: Achieves state-of-the-art performance in quantitative and qualitative evaluations.

Conclusion: DynFaceRestore effectively balances fidelity and quality in blind face restoration.

Abstract: Blind Face Restoration aims to recover high-fidelity, detail-rich facial
images from unknown degraded inputs, presenting significant challenges in
preserving both identity and detail. Pre-trained diffusion models have been
increasingly used as image priors to generate fine details. Still, existing
methods often use fixed diffusion sampling timesteps and a global guidance
scale, assuming uniform degradation. This limitation and potentially imperfect
degradation kernel estimation frequently lead to under- or over-diffusion,
resulting in an imbalance between fidelity and quality. We propose
DynFaceRestore, a novel blind face restoration approach that learns to map any
blindly degraded input to Gaussian blurry images. By leveraging these blurry
images and their respective Gaussian kernels, we dynamically select the
starting timesteps for each blurry image and apply closed-form guidance during
the diffusion sampling process to maintain fidelity. Additionally, we introduce
a dynamic guidance scaling adjuster that modulates the guidance strength across
local regions, enhancing detail generation in complex areas while preserving
structural fidelity in contours. This strategy effectively balances the
trade-off between fidelity and quality. DynFaceRestore achieves
state-of-the-art performance in both quantitative and qualitative evaluations,
demonstrating robustness and effectiveness in blind face restoration.

</details>


### [54] [One Step Closer: Creating the Future to Boost Monocular Semantic Scene Completion](https://arxiv.org/abs/2507.13801)
*Haoang Lu,Yuanqi Su,Xiaoning Zhang,Hao Hu*

Main category: cs.CV

TL;DR: CF-SSC is a temporal SSC framework using pseudo-future frame prediction to enhance 3D scene completion by integrating past, present, and predicted future frames.


<details>
  <summary>Details</summary>
Motivation: Existing monocular SSC methods struggle with occlusions and limited field of view in real-world traffic scenarios.

Method: Combines poses and depths for accurate 3D correspondences, enabling geometrically-consistent fusion of frames. Uses a 3D-aware architecture to model spatial-temporal relationships.

Result: Achieves state-of-the-art performance on SemanticKITTI and SSCBench-KITTI-360 benchmarks, improving occlusion reasoning and 3D completion accuracy.

Conclusion: CF-SSC effectively addresses limitations of monocular SSC methods by leveraging temporal data and 3D-aware modeling.

Abstract: In recent years, visual 3D Semantic Scene Completion (SSC) has emerged as a
critical perception task for autonomous driving due to its ability to infer
complete 3D scene layouts and semantics from single 2D images. However, in
real-world traffic scenarios, a significant portion of the scene remains
occluded or outside the camera's field of view -- a fundamental challenge that
existing monocular SSC methods fail to address adequately. To overcome these
limitations, we propose Creating the Future SSC (CF-SSC), a novel temporal SSC
framework that leverages pseudo-future frame prediction to expand the model's
effective perceptual range. Our approach combines poses and depths to establish
accurate 3D correspondences, enabling geometrically-consistent fusion of past,
present, and predicted future frames in 3D space. Unlike conventional methods
that rely on simple feature stacking, our 3D-aware architecture achieves more
robust scene completion by explicitly modeling spatial-temporal relationships.
Comprehensive experiments on SemanticKITTI and SSCBench-KITTI-360 benchmarks
demonstrate state-of-the-art performance, validating the effectiveness of our
approach, highlighting our method's ability to improve occlusion reasoning and
3D scene completion accuracy.

</details>


### [55] [GRAM-MAMBA: Holistic Feature Alignment for Wireless Perception with Adaptive Low-Rank Compensation](https://arxiv.org/abs/2507.13803)
*Weiqi Yang,Xu Zhou,Jingfu Guan,Hao Du,Tianyu Bai*

Main category: cs.CV

TL;DR: GRAM-MAMBA is a novel framework for efficient and robust multi-modal fusion in IoT, addressing challenges like high complexity, unidirectional alignment, and missing data.


<details>
  <summary>Details</summary>
Motivation: Existing IoT multi-modal fusion systems struggle with high complexity, poor inter-modal alignment, and robustness to missing sensor data, limiting real-world deployment.

Method: GRAM-MAMBA combines a linear-complexity Mamba model for time-series processing with a GRAM matrix for pairwise modal alignment and an adaptive low-rank layer strategy for missing data.

Result: Outperforms baselines on indoor positioning (24.5% boost with missing data) and human activity recognition (93.55% F1, 93.81% OA).

Conclusion: GRAM-MAMBA enables efficient, robust multi-modal perception in resource-constrained IoT environments.

Abstract: Multi-modal fusion is crucial for Internet of Things (IoT) perception, widely
deployed in smart homes, intelligent transport, industrial automation, and
healthcare. However, existing systems often face challenges: high model
complexity hinders deployment in resource-constrained environments,
unidirectional modal alignment neglects inter-modal relationships, and
robustness suffers when sensor data is missing. These issues impede efficient
and robust multimodal perception in real-world IoT settings. To overcome these
limitations, we propose GRAM-MAMBA. This framework utilizes the
linear-complexity Mamba model for efficient sensor time-series processing,
combined with an optimized GRAM matrix strategy for pairwise alignment among
modalities, addressing the shortcomings of traditional single-modality
alignment. Inspired by Low-Rank Adaptation (LoRA), we introduce an adaptive
low-rank layer compensation strategy to handle missing modalities
post-training. This strategy freezes the pre-trained model core and irrelevant
adaptive layers, fine-tuning only those related to available modalities and the
fusion process. Extensive experiments validate GRAM-MAMBA's effectiveness. On
the SPAWC2021 indoor positioning dataset, the pre-trained model shows lower
error than baselines; adapting to missing modalities yields a 24.5% performance
boost by training less than 0.2% of parameters. On the USC-HAD human activity
recognition dataset, it achieves 93.55% F1 and 93.81% Overall Accuracy (OA),
outperforming prior work; the update strategy increases F1 by 23% while
training less than 0.3% of parameters. These results highlight GRAM-MAMBA's
potential for achieving efficient and robust multimodal perception in
resource-constrained environments.

</details>


### [56] [SkySense V2: A Unified Foundation Model for Multi-modal Remote Sensing](https://arxiv.org/abs/2507.13812)
*Yingying Zhang,Lixiang Ru,Kang Wu,Lei Yu,Lei Liang,Yansheng Li,Jingdong Chen*

Main category: cs.CV

TL;DR: SkySense V2 is a unified multi-modal remote sensing foundation model using a single transformer backbone, tailored SSL for RS data, adaptive patch merging, and MoE, outperforming SkySense by 1.8 points.


<details>
  <summary>Details</summary>
Motivation: Existing MM-RSFM approaches require separate backbones per modality and use generic SSL, ignoring RS image traits like complex semantics.

Method: Uses a single transformer backbone, novel SSL for RS data, adaptive patch merging, learnable modality prompts, and MoE.

Result: Outperforms SkySense by 1.8 points on 16 datasets across 7 tasks.

Conclusion: SkySense V2 offers efficient, unified multi-modal handling with improved performance for RS tasks.

Abstract: The multi-modal remote sensing foundation model (MM-RSFM) has significantly
advanced various Earth observation tasks, such as urban planning, environmental
monitoring, and natural disaster management. However, most existing approaches
generally require the training of separate backbone networks for each data
modality, leading to redundancy and inefficient parameter utilization.
Moreover, prevalent pre-training methods typically apply self-supervised
learning (SSL) techniques from natural images without adequately accommodating
the characteristics of remote sensing (RS) images, such as the complicated
semantic distribution within a single RS image. In this work, we present
SkySense V2, a unified MM-RSFM that employs a single transformer backbone to
handle multiple modalities. This backbone is pre-trained with a novel SSL
strategy tailored to the distinct traits of RS data. In particular, SkySense V2
incorporates an innovative adaptive patch merging module and learnable modality
prompt tokens to address challenges related to varying resolutions and limited
feature diversity across modalities. In additional, we incorporate the mixture
of experts (MoE) module to further enhance the performance of the foundation
model. SkySense V2 demonstrates impressive generalization abilities through an
extensive evaluation involving 16 datasets over 7 tasks, outperforming SkySense
by an average of 1.8 points.

</details>


### [57] [Team of One: Cracking Complex Video QA with Model Synergy](https://arxiv.org/abs/2507.13820)
*Jun Xie,Zhaoran Zhao,Xiongjun Guan,Yingjian Zhu,Hongzhu Yi,Xinming Wang,Feng Chen,Zhepeng Wang*

Main category: cs.CV

TL;DR: A novel framework for open-ended video QA improves reasoning depth and robustness by integrating multiple VLMs via structured chains of thought, outperforming baselines on the CVRR-ES dataset.


<details>
  <summary>Details</summary>
Motivation: Address limitations of existing Video-LMMs, such as weak contextual understanding, poor temporal modeling, and generalization issues with ambiguous queries.

Method: Introduces a prompting-and-response integration mechanism coordinating multiple VLMs with structured reasoning pathways, evaluated and fused by an external LLM.

Result: Significantly outperforms baselines in all metrics, showing superior generalization and robustness.

Conclusion: Provides a lightweight, extensible strategy for advancing multimodal reasoning without retraining, setting a foundation for future Video-LMM development.

Abstract: We propose a novel framework for open-ended video question answering that
enhances reasoning depth and robustness in complex real-world scenarios, as
benchmarked on the CVRR-ES dataset. Existing Video-Large Multimodal Models
(Video-LMMs) often exhibit limited contextual understanding, weak temporal
modeling, and poor generalization to ambiguous or compositional queries. To
address these challenges, we introduce a prompting-and-response integration
mechanism that coordinates multiple heterogeneous Video-Language Models (VLMs)
via structured chains of thought, each tailored to distinct reasoning pathways.
An external Large Language Model (LLM) serves as an evaluator and integrator,
selecting and fusing the most reliable responses. Extensive experiments
demonstrate that our method significantly outperforms existing baselines across
all evaluation metrics, showcasing superior generalization and robustness. Our
approach offers a lightweight, extensible strategy for advancing multimodal
reasoning without requiring model retraining, setting a strong foundation for
future Video-LMM development.

</details>


### [58] [A Quantum-assisted Attention U-Net for Building Segmentation over Tunis using Sentinel-1 Data](https://arxiv.org/abs/2507.13852)
*Luigi Russo,Francesco Mauro,Babak Memar,Alessandro Sebastianelli,Silvia Liberata Ullo,Paolo Gamba*

Main category: cs.CV

TL;DR: The study explores Quanvolutional pre-processing with Attention U-Net for building segmentation in urban areas, using SAR imagery of Tunis. It achieves comparable accuracy to standard methods while reducing computational costs.


<details>
  <summary>Details</summary>
Motivation: Accurate building segmentation in dense urban areas is challenging due to large, high-resolution satellite images. The study aims to enhance segmentation using quantum-assisted methods.

Method: Quanvolutional pre-processing is applied to extract informative features from SAR imagery, integrated with Attention U-Net for segmentation.

Result: The method matches standard Attention U-Net accuracy but with fewer parameters, improving computational efficiency.

Conclusion: Quantum-assisted Deep Learning shows promise for efficient, large-scale urban building segmentation.

Abstract: Building segmentation in urban areas is essential in fields such as urban
planning, disaster response, and population mapping. Yet accurately segmenting
buildings in dense urban regions presents challenges due to the large size and
high resolution of satellite images. This study investigates the use of a
Quanvolutional pre-processing to enhance the capability of the Attention U-Net
model in the building segmentation. Specifically, this paper focuses on the
urban landscape of Tunis, utilizing Sentinel-1 Synthetic Aperture Radar (SAR)
imagery. In this work, Quanvolution was used to extract more informative
feature maps that capture essential structural details in radar imagery,
proving beneficial for accurate building segmentation. Preliminary results
indicate that proposed methodology achieves comparable test accuracy to the
standard Attention U-Net model while significantly reducing network parameters.
This result aligns with findings from previous works, confirming that
Quanvolution not only maintains model accuracy but also increases computational
efficiency. These promising outcomes highlight the potential of
quantum-assisted Deep Learning frameworks for large-scale building segmentation
in urban environments.

</details>


### [59] [Depth3DLane: Fusing Monocular 3D Lane Detection with Self-Supervised Monocular Depth Estimation](https://arxiv.org/abs/2507.13857)
*Max van den Hoven,Kishaan Jeeveswaran,Pieter Piscaer,Thijs Wensveen,Elahe Arani,Bahram Zonooz*

Main category: cs.CV

TL;DR: Depth3DLane is a dual-pathway framework for monocular 3D lane detection, integrating self-supervised depth estimation to avoid reliance on expensive sensors or ground-truth depth data. It also predicts camera parameters per-frame, enabling use in scenarios without calibration.


<details>
  <summary>Details</summary>
Motivation: Existing methods rely on costly sensors or ground-truth depth data, and assume known camera parameters, limiting scalability and applicability in uncalibrated scenarios.

Method: Depth3DLane uses a dual-pathway approach: a bird's-eye view for spatial info and a front view for semantics, leveraging self-supervised depth estimation. It also predicts camera parameters per-frame and uses 3D lane anchors for feature sampling.

Result: Depth3DLane achieves competitive performance on OpenLane benchmark and works in scenarios without camera calibration, unlike prior methods.

Conclusion: The framework addresses key limitations in monocular 3D lane detection, offering a practical solution without reliance on expensive sensors or calibration data.

Abstract: Monocular 3D lane detection is essential for autonomous driving, but
challenging due to the inherent lack of explicit spatial information.
Multi-modal approaches rely on expensive depth sensors, while methods
incorporating fully-supervised depth networks rely on ground-truth depth data
that is impractical to collect at scale. Additionally, existing methods assume
that camera parameters are available, limiting their applicability in scenarios
like crowdsourced high-definition (HD) lane mapping. To address these
limitations, we propose Depth3DLane, a novel dual-pathway framework that
integrates self-supervised monocular depth estimation to provide explicit
structural information, without the need for expensive sensors or additional
ground-truth depth data. Leveraging a self-supervised depth network to obtain a
point cloud representation of the scene, our bird's-eye view pathway extracts
explicit spatial information, while our front view pathway simultaneously
extracts rich semantic information. Depth3DLane then uses 3D lane anchors to
sample features from both pathways and infer accurate 3D lane geometry.
Furthermore, we extend the framework to predict camera parameters on a
per-frame basis and introduce a theoretically motivated fitting procedure to
enhance stability on a per-segment basis. Extensive experiments demonstrate
that Depth3DLane achieves competitive performance on the OpenLane benchmark
dataset. Furthermore, experimental results show that using learned parameters
instead of ground-truth parameters allows Depth3DLane to be applied in
scenarios where camera calibration is infeasible, unlike previous methods.

</details>


### [60] [PositionIC: Unified Position and Identity Consistency for Image Customization](https://arxiv.org/abs/2507.13861)
*Junjie Hu,Tianyang Han,Kai Ma,Jialin Gao,Hao Dou,Song Yang,Xianhua He,Jianhui Zhang,Junfeng Luo,Xiaoming Wei,Wenqiang Zhang*

Main category: cs.CV

TL;DR: PositionIC is a framework for multi-subject image customization with precise spatial control, addressing the lack of scalable datasets for identity and positional cues.


<details>
  <summary>Details</summary>
Motivation: Current image customization lacks fine-grained spatial control due to missing datasets linking identity with precise positions.

Method: PositionIC uses a bidirectional generation pipeline to maintain semantic coherence and a positional modulation layer for independent subject placement.

Result: The approach achieves precise spatial control and high consistency in image customization.

Conclusion: PositionIC enables controllable, high-fidelity customization in open-world scenarios and will be released for further research.

Abstract: Recent subject-driven image customization has achieved significant
advancements in fidelity, yet fine-grained entity-level spatial control remains
elusive, hindering the broader real-world application. This limitation is
mainly attributed to scalable datasets that bind identity with precise
positional cues are absent. To this end, we introduce PositionIC, a unified
framework that enforces position and identity consistency for multi-subject
customization. We construct a scalable synthesis pipeline that employs a
bidirectional generation paradigm to eliminate subject drift and maintain
semantic coherence. On top of these data, we design a lightweight positional
modulation layer that decouples spatial embeddings among subjects, enabling
independent, accurate placement while preserving visual fidelity. Extensive
experiments demonstrate that our approach can achieve precise spatial control
while maintaining high consistency in image customization task. PositionIC
paves the way for controllable, high-fidelity image customization in
open-world, multi-entity scenarios and will be released to foster further
research.

</details>


### [61] [NoHumansRequired: Autonomous High-Quality Image Editing Triplet Mining](https://arxiv.org/abs/2507.14119)
*Maksim Kuprashevich,Grigorii Alekseenko,Irina Tolstykh,Georgii Fedorov,Bulat Suleimanov,Vladimir Dokholyan,Aleksandr Gordeev*

Main category: cs.CV

TL;DR: An automated pipeline mines high-fidelity image-editing triplets (original image, instruction, edited image) for training generative models, eliminating human labeling. It includes a validator for quality scoring and releases a dataset (NHR-Edit) and model (Bagel-NHR-Edit) with superior performance.


<details>
  <summary>Details</summary>
Motivation: The challenge of acquiring high-quality, pixel-accurate image-editing triplets for supervised training of generative models, due to the lack of robust automated metrics and the need for manual effort.

Method: An automated, modular pipeline using public generative models and a task-tuned Gemini validator to score edits for instruction adherence and aesthetics. Techniques like inversion and compositional bootstrapping expand the dataset.

Result: Creation of NHR-Edit (358k high-quality triplets) and Bagel-NHR-Edit model, which outperforms public alternatives in evaluations.

Conclusion: The approach enables large-scale, high-fidelity training data without human labeling, democratizing research in image editing by releasing open datasets and models.

Abstract: Recent advances in generative modeling enable image editing assistants that
follow natural language instructions without additional user input. Their
supervised training requires millions of triplets: original image, instruction,
edited image. Yet mining pixel-accurate examples is hard. Each edit must affect
only prompt-specified regions, preserve stylistic coherence, respect physical
plausibility, and retain visual appeal. The lack of robust automated
edit-quality metrics hinders reliable automation at scale. We present an
automated, modular pipeline that mines high-fidelity triplets across domains,
resolutions, instruction complexities, and styles. Built on public generative
models and running without human intervention, our system uses a task-tuned
Gemini validator to score instruction adherence and aesthetics directly,
removing any need for segmentation or grounding models. Inversion and
compositional bootstrapping enlarge the mined set by approximately 2.2x,
enabling large-scale high-fidelity training data. By automating the most
repetitive annotation steps, the approach allows a new scale of training
without human labeling effort. To democratize research in this
resource-intensive area, we release NHR-Edit: an open dataset of 358k
high-quality triplets. In the largest cross-dataset evaluation, it surpasses
all public alternatives. We also release Bagel-NHR-Edit, an open-source
fine-tuned Bagel model, which achieves state-of-the-art metrics in our
experiments.

</details>


### [62] [When Seeing Overrides Knowing: Disentangling Knowledge Conflicts in Vision-Language Models](https://arxiv.org/abs/2507.13868)
*Francesco Ortu,Zhijing Jin,Diego Doimo,Alberto Cazzaniga*

Main category: cs.CV

TL;DR: The paper investigates how vision-language models (VLMs) resolve conflicts between internal knowledge and external visual inputs, identifying specific model heads that control these interactions.


<details>
  <summary>Details</summary>
Motivation: Understanding and mitigating knowledge conflicts in VLMs to prevent hallucinations and unreliable responses.

Method: Introduces a dataset of multimodal counterfactual queries to analyze conflict resolution, uses logit inspection to localize controlling heads, and modifies these heads to steer model behavior.

Result: Identifies specific heads that manage conflicts and shows that attention from these heads outperforms gradient-based attribution in pinpointing image regions driving visual overrides.

Conclusion: The study provides insights into VLM conflict resolution mechanisms, offering a method to control model behavior and improve reliability.

Abstract: Vision-language models (VLMs) increasingly leverage diverse knowledge sources
to address complex tasks, often encountering conflicts between their internal
parametric knowledge and external information. Knowledge conflicts can result
in hallucinations and unreliable responses, but the mechanisms governing such
interactions remain unknown. To address this gap, we analyze the mechanisms
that VLMs use to resolve cross-modal conflicts by introducing a dataset of
multimodal counterfactual queries that deliberately contradict internal
commonsense knowledge. We localize with logit inspection a small set of heads
that control the conflict. Moreover, by modifying these heads, we can steer the
model towards its internal knowledge or the visual inputs. Finally, we show
that attention from such heads pinpoints localized image regions driving visual
overrides, outperforming gradient-based attribution in precision.

</details>


### [63] [Real-Time Fusion of Visual and Chart Data for Enhanced Maritime Vision](https://arxiv.org/abs/2507.13880)
*Marten Kreis,Benjamin Kiefer*

Main category: cs.CV

TL;DR: A novel method fuses real-time marine visuals with nautical charts using a transformer-based neural network for accurate buoy detection and matching, outperforming baseline approaches.


<details>
  <summary>Details</summary>
Motivation: Enhancing marine vision by integrating live video feeds with chart data to improve navigation safety and accuracy.

Method: Transformer-based end-to-end neural network predicts buoy bounding boxes and confidence scores, enabling direct matching with chart markers. Compared to ray-casting and YOLOv7-based baselines.

Result: Significant improvement in object localization and association accuracy in dynamic maritime environments.

Conclusion: The proposed method effectively enhances marine vision by robustly aligning real-time visuals with chart data.

Abstract: This paper presents a novel approach to enhancing marine vision by fusing
real-time visual data with chart information. Our system overlays nautical
chart data onto live video feeds by accurately matching detected navigational
aids, such as buoys, with their corresponding representations in chart data. To
achieve robust association, we introduce a transformer-based end-to-end neural
network that predicts bounding boxes and confidence scores for buoy queries,
enabling the direct matching of image-domain detections with world-space chart
markers. The proposed method is compared against baseline approaches, including
a ray-casting model that estimates buoy positions via camera projection and a
YOLOv7-based network extended with a distance estimation module. Experimental
results on a dataset of real-world maritime scenes demonstrate that our
approach significantly improves object localization and association accuracy in
dynamic and challenging environments.

</details>


### [64] [PCR-GS: COLMAP-Free 3D Gaussian Splatting via Pose Co-Regularizations](https://arxiv.org/abs/2507.13891)
*Yu Wei,Jiahui Zhang,Xiaoqin Zhang,Ling Shao,Shijian Lu*

Main category: cs.CV

TL;DR: PCR-GS improves 3D-GS by co-regularizing camera poses for better performance with complex trajectories.


<details>
  <summary>Details</summary>
Motivation: 3D-GS struggles with complex camera trajectories, leading to degraded pose estimation and optimization issues.

Method: PCR-GS uses feature reprojection and wavelet-based frequency regularization to optimize camera poses.

Result: PCR-GS outperforms in pose-free 3D-GS modeling, especially with dramatic camera trajectory changes.

Conclusion: PCR-GS effectively addresses challenges in 3D-GS for complex scenes, enhancing pose estimation and scene modeling.

Abstract: COLMAP-free 3D Gaussian Splatting (3D-GS) has recently attracted increasing
attention due to its remarkable performance in reconstructing high-quality 3D
scenes from unposed images or videos. However, it often struggles to handle
scenes with complex camera trajectories as featured by drastic rotation and
translation across adjacent camera views, leading to degraded estimation of
camera poses and further local minima in joint optimization of camera poses and
3D-GS. We propose PCR-GS, an innovative COLMAP-free 3DGS technique that
achieves superior 3D scene modeling and camera pose estimation via camera pose
co-regularization. PCR-GS achieves regularization from two perspectives. The
first is feature reprojection regularization which extracts view-robust DINO
features from adjacent camera views and aligns their semantic information for
camera pose regularization. The second is wavelet-based frequency
regularization which exploits discrepancy in high-frequency details to further
optimize the rotation matrix in camera poses. Extensive experiments over
multiple real-world scenes show that the proposed PCR-GS achieves superior
pose-free 3D-GS scene modeling under dramatic changes of camera trajectories.

</details>


### [65] [Enhancing LiDAR Point Features with Foundation Model Priors for 3D Object Detection](https://arxiv.org/abs/2507.13899)
*Yujian Mo,Yan Wu,Junqiao Zhao,Jijun Wang,Yinghao Hu,Jun Yan*

Main category: cs.CV

TL;DR: The paper enhances LiDAR-based 3D object detection by integrating depth priors from DepthAnything, improving point feature representation and detection accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the limited expressiveness of raw LiDAR point features, especially reflectance, by leveraging dense depth priors from monocular RGB images.

Method: Fuses DepthAnything's depth priors with LiDAR data, uses a point-wise feature extraction module, and employs a Dual-Path RoI framework with a bidirectional gated fusion module.

Result: Improved detection accuracy on the KITTI benchmark, validating the use of visual foundation model priors in LiDAR-based detection.

Conclusion: Incorporating depth priors from visual foundation models significantly enhances LiDAR-based 3D object detection.

Abstract: Recent advances in foundation models have opened up new possibilities for
enhancing 3D perception. In particular, DepthAnything offers dense and reliable
geometric priors from monocular RGB images, which can complement sparse LiDAR
data in autonomous driving scenarios. However, such priors remain underutilized
in LiDAR-based 3D object detection. In this paper, we address the limited
expressiveness of raw LiDAR point features, especially the weak discriminative
capability of the reflectance attribute, by introducing depth priors predicted
by DepthAnything. These priors are fused with the original LiDAR attributes to
enrich each point's representation. To leverage the enhanced point features, we
propose a point-wise feature extraction module. Then, a Dual-Path RoI feature
extraction framework is employed, comprising a voxel-based branch for global
semantic context and a point-based branch for fine-grained structural details.
To effectively integrate the complementary RoI features, we introduce a
bidirectional gated RoI feature fusion module that balances global and local
cues. Extensive experiments on the KITTI benchmark show that our method
consistently improves detection accuracy, demonstrating the value of
incorporating visual foundation model priors into LiDAR-based 3D object
detection.

</details>


### [66] [TimeNeRF: Building Generalizable Neural Radiance Fields across Time from Few-Shot Input Views](https://arxiv.org/abs/2507.13929)
*Hsiang-Hui Hung,Huu-Phu Do,Yung-Hui Li,Ching-Chun Huang*

Main category: cs.CV

TL;DR: TimeNeRF is a neural rendering method for novel views at arbitrary times and viewpoints, even with few input views, without per-scene optimization.


<details>
  <summary>Details</summary>
Motivation: Addressing the inefficiency of collecting multiple views and re-optimizing for unseen scenes, and the need for immersive 3D environments that transition between day and night.

Method: Combines multi-view stereo, neural radiance fields (NeRF), and disentanglement strategies to enable generalizability in few-shot settings and temporal scene modeling.

Result: TimeNeRF renders novel views without per-scene optimization and smoothly transitions between times, capturing natural scene changes.

Conclusion: TimeNeRF advances temporal 3D scene modeling, offering efficient and realistic novel view synthesis for immersive experiences.

Abstract: We present TimeNeRF, a generalizable neural rendering approach for rendering
novel views at arbitrary viewpoints and at arbitrary times, even with few input
views. For real-world applications, it is expensive to collect multiple views
and inefficient to re-optimize for unseen scenes. Moreover, as the digital
realm, particularly the metaverse, strives for increasingly immersive
experiences, the ability to model 3D environments that naturally transition
between day and night becomes paramount. While current techniques based on
Neural Radiance Fields (NeRF) have shown remarkable proficiency in synthesizing
novel views, the exploration of NeRF's potential for temporal 3D scene modeling
remains limited, with no dedicated datasets available for this purpose. To this
end, our approach harnesses the strengths of multi-view stereo, neural radiance
fields, and disentanglement strategies across diverse datasets. This equips our
model with the capability for generalizability in a few-shot setting, allows us
to construct an implicit content radiance field for scene representation, and
further enables the building of neural radiance fields at any arbitrary time.
Finally, we synthesize novel views of that time via volume rendering.
Experiments show that TimeNeRF can render novel views in a few-shot setting
without per-scene optimization. Most notably, it excels in creating realistic
novel views that transition smoothly across different times, adeptly capturing
intricate natural scene changes from dawn to dusk.

</details>


### [67] [DiViD: Disentangled Video Diffusion for Static-Dynamic Factorization](https://arxiv.org/abs/2507.13934)
*Marzieh Gheisari,Auguste Genovesio*

Main category: cs.CV

TL;DR: DiViD is a novel video diffusion framework for disentangling static appearance and dynamic motion in videos, outperforming existing methods by reducing leakage and improving fidelity.


<details>
  <summary>Details</summary>
Motivation: Existing VAE- and GAN-based approaches struggle with information leakage and blurry reconstructions in disentangling static and dynamic content in videos.

Method: DiViD uses a sequence encoder to extract static and dynamic tokens, a conditional DDPM decoder with shared-noise schedules, time-varying KL-based bottlenecks, and cross-attention, plus an orthogonality regularizer.

Result: DiViD achieves the highest swap-based joint accuracy, preserves static fidelity, improves dynamic transfer, and reduces cross-leakage compared to state-of-the-art methods.

Conclusion: DiViD effectively disentangles static and dynamic content in videos, setting a new benchmark for sequential disentanglement.

Abstract: Unsupervised disentanglement of static appearance and dynamic motion in video
remains a fundamental challenge, often hindered by information leakage and
blurry reconstructions in existing VAE- and GAN-based approaches. We introduce
DiViD, the first end-to-end video diffusion framework for explicit
static-dynamic factorization. DiViD's sequence encoder extracts a global static
token from the first frame and per-frame dynamic tokens, explicitly removing
static content from the motion code. Its conditional DDPM decoder incorporates
three key inductive biases: a shared-noise schedule for temporal consistency, a
time-varying KL-based bottleneck that tightens at early timesteps (compressing
static information) and relaxes later (enriching dynamics), and cross-attention
that routes the global static token to all frames while keeping dynamic tokens
frame-specific. An orthogonality regularizer further prevents residual
static-dynamic leakage. We evaluate DiViD on real-world benchmarks using
swap-based accuracy and cross-leakage metrics. DiViD outperforms
state-of-the-art sequential disentanglement methods: it achieves the highest
swap-based joint accuracy, preserves static fidelity while improving dynamic
transfer, and reduces average cross-leakage.

</details>


### [68] [Generalist Forecasting with Frozen Video Models via Latent Diffusion](https://arxiv.org/abs/2507.13942)
*Jacob C Walker,Pedro Vélez,Luisa Polania Cabrera,Guangyao Zhou,Rishabh Kabra,Carl Doersch,Maks Ovsjanikov,João Carreira,Shiry Ginosar*

Main category: cs.CV

TL;DR: The paper explores the link between a vision model's perceptual ability and its forecasting performance, introducing a generalist framework using latent diffusion models for future feature prediction.


<details>
  <summary>Details</summary>
Motivation: Understanding the correlation between perceptual ability and forecasting performance to improve general-purpose systems' planning and action capabilities.

Method: A novel generalist forecasting framework using latent diffusion models on frozen vision backbones, with task-specific readouts for decoding.

Result: Strong correlation found between perceptual ability and forecasting performance across diverse pretrained models and abstraction levels.

Conclusion: Bridging representation learning and generative modeling enhances temporally grounded video understanding.

Abstract: Forecasting what will happen next is a critical skill for general-purpose
systems that plan or act in the world at different levels of abstraction. In
this paper, we identify a strong correlation between a vision model's
perceptual ability and its generalist forecasting performance over short time
horizons. This trend holds across a diverse set of pretrained models-including
those trained generatively-and across multiple levels of abstraction, from raw
pixels to depth, point tracks, and object motion. The result is made possible
by a novel generalist forecasting framework that operates on any frozen vision
backbone: we train latent diffusion models to forecast future features in the
frozen representation space, which are then decoded via lightweight,
task-specific readouts. To enable consistent evaluation across tasks, we
introduce distributional metrics that compare distributional properties
directly in the space of downstream tasks and apply this framework to nine
models and four tasks. Our results highlight the value of bridging
representation learning and generative modeling for temporally grounded video
understanding.

</details>


### [69] [Evaluation of Human Visual Privacy Protection: A Three-Dimensional Framework and Benchmark Dataset](https://arxiv.org/abs/2507.13981)
*Sara Abdulaziz,Giacomo D'Amicantonio,Egor Bondarev*

Main category: cs.CV

TL;DR: A framework for evaluating visual privacy-protection methods is introduced, along with HR-VISPR dataset, to assess privacy, utility, and practicality.


<details>
  <summary>Details</summary>
Motivation: Address concerns over AI-powered surveillance by developing objective techniques for evaluating privacy protection.

Method: Proposes a three-dimensional framework (privacy, utility, practicality) and introduces HR-VISPR dataset for training an interpretable privacy metric. Evaluates 11 privacy protection methods.

Result: Differentiates privacy levels aligned with human perception and highlights trade-offs between privacy, utility, and practicality.

Conclusion: The framework and dataset provide a structured evaluation tool applicable in diverse contexts.

Abstract: Recent advances in AI-powered surveillance have intensified concerns over the
collection and processing of sensitive personal data. In response, research has
increasingly focused on privacy-by-design solutions, raising the need for
objective techniques to evaluate privacy protection. This paper presents a
comprehensive framework for evaluating visual privacy-protection methods across
three dimensions: privacy, utility, and practicality. In addition, it
introduces HR-VISPR, a publicly available human-centric dataset with biometric,
soft-biometric, and non-biometric labels to train an interpretable privacy
metric. We evaluate 11 privacy protection methods, ranging from conventional
techniques to advanced deep-learning methods, through the proposed framework.
The framework differentiates privacy levels in alignment with human visual
perception, while highlighting trade-offs between privacy, utility, and
practicality. This study, along with the HR-VISPR dataset, serves as an
insightful tool and offers a structured evaluation framework applicable across
diverse contexts.

</details>


### [70] [CSD-VAR: Content-Style Decomposition in Visual Autoregressive Models](https://arxiv.org/abs/2507.13984)
*Quang-Binh Nguyen,Minh Luu,Quang Nguyen,Anh Tran,Khoi Nguyen*

Main category: cs.CV

TL;DR: The paper introduces CSD-VAR, a method for content-style decomposition using Visual Autoregressive Modeling (VAR), outperforming prior approaches with innovations like scale-aware optimization, SVD-based rectification, and Augmented Key-Value memory.


<details>
  <summary>Details</summary>
Motivation: To leverage VAR's scale-wise generation for improved disentanglement of content and style in images, addressing limitations of existing methods tailored for diffusion models.

Method: Proposes CSD-VAR with three innovations: scale-aware alternating optimization, SVD-based rectification, and Augmented Key-Value memory. Introduces CSD-100 dataset for benchmarking.

Result: CSD-VAR achieves superior content preservation and stylization fidelity compared to prior methods.

Conclusion: VAR is a viable framework for CSD, with CSD-VAR's innovations enhancing disentanglement and performance.

Abstract: Disentangling content and style from a single image, known as content-style
decomposition (CSD), enables recontextualization of extracted content and
stylization of extracted styles, offering greater creative flexibility in
visual synthesis. While recent personalization methods have explored the
decomposition of explicit content style, they remain tailored for diffusion
models. Meanwhile, Visual Autoregressive Modeling (VAR) has emerged as a
promising alternative with a next-scale prediction paradigm, achieving
performance comparable to that of diffusion models. In this paper, we explore
VAR as a generative framework for CSD, leveraging its scale-wise generation
process for improved disentanglement. To this end, we propose CSD-VAR, a novel
method that introduces three key innovations: (1) a scale-aware alternating
optimization strategy that aligns content and style representation with their
respective scales to enhance separation, (2) an SVD-based rectification method
to mitigate content leakage into style representations, and (3) an Augmented
Key-Value (K-V) memory enhancing content identity preservation. To benchmark
this task, we introduce CSD-100, a dataset specifically designed for
content-style decomposition, featuring diverse subjects rendered in various
artistic styles. Experiments demonstrate that CSD-VAR outperforms prior
approaches, achieving superior content preservation and stylization fidelity.

</details>


### [71] [DreamScene: 3D Gaussian-based End-to-end Text-to-3D Scene Generation](https://arxiv.org/abs/2507.13985)
*Haoran Li,Yuli Tian,Kun Lan,Yong Liao,Lin Wang,Pan Hui,Peng Yuan Zhou*

Main category: cs.CV

TL;DR: DreamScene is an end-to-end framework for generating high-quality, editable 3D scenes from text or dialogue, addressing automation, consistency, and control challenges.


<details>
  <summary>Details</summary>
Motivation: Existing methods for 3D scene generation lack automation, 3D consistency, and fine-grained control, limiting practical applications in gaming, film, and design.

Method: DreamScene uses a GPT-4 agent for scene planning, graph-based placement for collision-free layouts, Formation Pattern Sampling (FPS) for geometry, and progressive camera sampling for consistency. It also supports fine-grained editing.

Result: DreamScene outperforms prior methods in quality, consistency, and flexibility, enabling open-domain 3D content creation.

Conclusion: DreamScene provides a practical solution for generating and editing 3D scenes from natural language, advancing the field of 3D content creation.

Abstract: Generating 3D scenes from natural language holds great promise for
applications in gaming, film, and design. However, existing methods struggle
with automation, 3D consistency, and fine-grained control. We present
DreamScene, an end-to-end framework for high-quality and editable 3D scene
generation from text or dialogue. DreamScene begins with a scene planning
module, where a GPT-4 agent infers object semantics and spatial constraints to
construct a hybrid graph. A graph-based placement algorithm then produces a
structured, collision-free layout. Based on this layout, Formation Pattern
Sampling (FPS) generates object geometry using multi-timestep sampling and
reconstructive optimization, enabling fast and realistic synthesis. To ensure
global consistent, DreamScene employs a progressive camera sampling strategy
tailored to both indoor and outdoor settings. Finally, the system supports
fine-grained scene editing, including object movement, appearance changes, and
4D dynamic motion. Experiments demonstrate that DreamScene surpasses prior
methods in quality, consistency, and flexibility, offering a practical solution
for open-domain 3D content creation. Code and demos are available at
https://dreamscene-project.github.io.

</details>


### [72] [Automatic Classification and Segmentation of Tunnel Cracks Based on Deep Learning and Visual Explanations](https://arxiv.org/abs/2507.14010)
*Yong Feng,Xiaolei Zhang,Shijin Feng,Yong Zhao,Yihan Chen*

Main category: cs.CV

TL;DR: A two-step deep learning method for classifying and segmenting tunnel cracks, using DenseNet-169 for classification and DeepLabV3+ for segmentation, achieves high accuracy and efficiency, validated by superior experimental results.


<details>
  <summary>Details</summary>
Motivation: To improve the accuracy and efficiency of tunnel crack detection for assessing tunnel safety.

Method: A two-step approach: 1) DenseNet-169 for crack image classification, 2) DeepLabV3+ for crack segmentation, with visual explanations for model transparency.

Result: Classification accuracy: 92.23%, FPS: 39.80; Segmentation IoU: 57.01%, F1 score: 67.44%, outperforming other models.

Conclusion: The method enables fast, accurate tunnel health assessment and enhances understanding of deep learning models.

Abstract: Tunnel lining crack is a crucial indicator of tunnels' safety status. Aiming
to classify and segment tunnel cracks with enhanced accuracy and efficiency,
this study proposes a two-step deep learning-based method. An automatic tunnel
image classification model is developed using the DenseNet-169 in the first
step. The proposed crack segmentation model in the second step is based on the
DeepLabV3+, whose internal logic is evaluated via a score-weighted visual
explanation technique. Proposed method combines tunnel image classification and
segmentation together, so that the selected images containing cracks from the
first step are segmented in the second step to improve the detection accuracy
and efficiency. The superior performances of the two-step method are validated
by experiments. The results show that the accuracy and frames per second (FPS)
of the tunnel crack classification model are 92.23% and 39.80, respectively,
which are higher than other convolutional neural networks (CNN) based and
Transformer based models. Also, the intersection over union (IoU) and F1 score
of the tunnel crack segmentation model are 57.01% and 67.44%, respectively,
outperforming other state-of-the-art models. Moreover, the provided visual
explanations in this study are conducive to understanding the "black box" of
deep learning-based models. The developed two-stage deep learning-based method
integrating visual explanations provides a basis for fast and accurate
quantitative assessment of tunnel health status.

</details>


### [73] [Analysis of Plant Nutrient Deficiencies Using Multi-Spectral Imaging and Optimized Segmentation Model](https://arxiv.org/abs/2507.14013)
*Ji-Yan Wu,Zheng Yong Poh,Anoop C. Patil,Bongsoo Park,Giovanni Volpe,Daisuke Urano*

Main category: cs.CV

TL;DR: A deep learning framework using multispectral imaging and an enhanced YOLOv5 model with transformer-based attention improves nutrient deficiency detection in plant leaves, outperforming baseline YOLOv5 by 12%.


<details>
  <summary>Details</summary>
Motivation: Accurate detection of nutrient deficiency in plant leaves is crucial for precision agriculture, aiding in early intervention for fertilization and stress management.

Method: The study employs a deep learning framework with multispectral imaging and an enhanced YOLOv5 model featuring a transformer-based attention head for better symptom capture.

Result: The proposed model outperforms baseline YOLOv5 by 12% in Dice score and IoU, excelling in detecting symptoms like chlorosis and pigment accumulation.

Conclusion: Combining multispectral imaging with spectral-spatial feature learning shows promise for advancing plant phenotyping and precision agriculture.

Abstract: Accurate detection of nutrient deficiency in plant leaves is essential for
precision agriculture, enabling early intervention in fertilization, disease,
and stress management. This study presents a deep learning framework for leaf
anomaly segmentation using multispectral imaging and an enhanced YOLOv5 model
with a transformer-based attention head. The model is tailored for processing
nine-channel multispectral input and uses self-attention mechanisms to better
capture subtle, spatially-distributed symptoms. The plants in the experiments
were grown under controlled nutrient stress conditions for evaluation. We carry
out extensive experiments to benchmark the proposed model against the baseline
YOLOv5. Extensive experiments show that the proposed model significantly
outperforms the baseline YOLOv5, with an average Dice score and IoU
(Intersection over Union) improvement of about 12%. In particular, this model
is effective in detecting challenging symptoms like chlorosis and pigment
accumulation. These results highlight the promise of combining multi-spectral
imaging with spectral-spatial feature learning for advancing plant phenotyping
and precision agriculture.

</details>


### [74] [Moodifier: MLLM-Enhanced Emotion-Driven Image Editing](https://arxiv.org/abs/2507.14024)
*Jiarong Ye,Sharon X. Huang*

Main category: cs.CV

TL;DR: The paper introduces an integrated system for emotion-driven image editing, combining a large annotated dataset (MoodArchive), a fine-tuned vision-language model (MoodifyCLIP), and a training-free editing model (Moodifier) to achieve precise emotional transformations while preserving content integrity.


<details>
  <summary>Details</summary>
Motivation: Emotion-driven image editing is valuable for creative industries but challenging due to the abstract nature of emotions and their varied visual manifestations.

Method: The approach includes: 1) MoodArchive dataset with emotional annotations, 2) MoodifyCLIP model for translating emotions to visual attributes, and 3) Moodifier for precise editing.

Result: Moodifier outperforms existing methods in emotional accuracy and content preservation, enabling contextually appropriate edits across diverse domains.

Conclusion: The system bridges emotions and visual content, unlocking new possibilities for emotional content creation in real-world applications.

Abstract: Bridging emotions and visual content for emotion-driven image editing holds
great potential in creative industries, yet precise manipulation remains
challenging due to the abstract nature of emotions and their varied
manifestations across different contexts. We tackle this challenge with an
integrated approach consisting of three complementary components. First, we
introduce MoodArchive, an 8M+ image dataset with detailed hierarchical
emotional annotations generated by LLaVA and partially validated by human
evaluators. Second, we develop MoodifyCLIP, a vision-language model fine-tuned
on MoodArchive to translate abstract emotions into specific visual attributes.
Third, we propose Moodifier, a training-free editing model leveraging
MoodifyCLIP and multimodal large language models (MLLMs) to enable precise
emotional transformations while preserving content integrity. Our system works
across diverse domains such as character expressions, fashion design, jewelry,
and home d\'ecor, enabling creators to quickly visualize emotional variations
while preserving identity and structure. Extensive experimental evaluations
show that Moodifier outperforms existing methods in both emotional accuracy and
content preservation, providing contextually appropriate edits. By linking
abstract emotions to concrete visual changes, our solution unlocks new
possibilities for emotional content creation in real-world applications. We
will release the MoodArchive dataset, MoodifyCLIP model, and make the Moodifier
code and demo publicly available upon acceptance.

</details>


### [75] [QuantEIT: Ultra-Lightweight Quantum-Assisted Inference for Chest Electrical Impedance Tomography](https://arxiv.org/abs/2507.14031)
*Hao Fang,Sihao Teng,Hao Yu,Siyi Yuan,Huaiwu He,Zhe Liu,Yunjie Yang*

Main category: cs.CV

TL;DR: QuantEIT is an ultra-lightweight quantum-assisted framework for EIT image reconstruction, reducing model complexity and parameters while outperforming conventional methods.


<details>
  <summary>Details</summary>
Motivation: EIT's ill-posed inverse problem challenges accurate image reconstruction; existing DL methods are complex and inefficient.

Method: QuantEIT uses a QA-Net with parallel 2-qubit quantum circuits for latent representations and a linear layer for reconstruction, operating unsupervised.

Result: QuantEIT achieves superior accuracy with 0.2% of parameters and enhanced noise robustness.

Conclusion: QuantEIT is a scalable, efficient, and innovative solution for EIT image reconstruction.

Abstract: Electrical Impedance Tomography (EIT) is a non-invasive, low-cost bedside
imaging modality with high temporal resolution, making it suitable for bedside
monitoring. However, its inherently ill-posed inverse problem poses significant
challenges for accurate image reconstruction. Deep learning (DL)-based
approaches have shown promise but often rely on complex network architectures
with a large number of parameters, limiting efficiency and scalability. Here,
we propose an Ultra-Lightweight Quantum-Assisted Inference (QuantEIT) framework
for EIT image reconstruction. QuantEIT leverages a Quantum-Assisted Network
(QA-Net), combining parallel 2-qubit quantum circuits to generate expressive
latent representations that serve as implicit nonlinear priors, followed by a
single linear layer for conductivity reconstruction. This design drastically
reduces model complexity and parameter number. Uniquely, QuantEIT operates in
an unsupervised, training-data-free manner and represents the first integration
of quantum circuits into EIT image reconstruction. Extensive experiments on
simulated and real-world 2D and 3D EIT lung imaging data demonstrate that
QuantEIT outperforms conventional methods, achieving comparable or superior
reconstruction accuracy using only 0.2% of the parameters, with enhanced
robustness to noise.

</details>


### [76] [Training-free Token Reduction for Vision Mamba](https://arxiv.org/abs/2507.14042)
*Qiankun Ma,Ziyao Zhang,Chi Su,Jie Chen,Zhen Song,Hairong Zheng,Wen Gao*

Main category: cs.CV

TL;DR: Vision Mamba competes with Vision Transformers (ViTs) but lacks efficient token reduction techniques. Existing ViT methods degrade Mamba performance due to reliance on attention mechanisms. The paper proposes MTR, a training-free token reduction framework for Mamba, reducing FLOPs by ~40% with minimal performance loss.


<details>
  <summary>Details</summary>
Motivation: To explore efficient token reduction for Vision Mamba, as existing ViT methods are incompatible due to Mamba's lack of attention mechanisms.

Method: Proposes MTR, a training-free framework using a Mamba structure-aware importance score for token reduction, integrating seamlessly into Mamba models.

Result: MTR reduces FLOPs by ~40% on Vim-B backbone with only a 1.6% drop in ImageNet performance, without retraining.

Conclusion: MTR is an effective, plug-and-play solution for token reduction in Vision Mamba, enhancing efficiency without significant performance loss.

Abstract: Vision Mamba has emerged as a strong competitor to Vision Transformers (ViTs)
due to its ability to efficiently capture long-range dependencies with linear
computational complexity. While token reduction, an effective compression
technique in ViTs, has rarely been explored in Vision Mamba. Exploring Vision
Mamba's efficiency is essential for enabling broader applications. However, we
find that directly applying existing token reduction techniques for ViTs to
Vision Mamba leads to significant performance degradation. This is primarily
because Mamba is a sequence model without attention mechanisms, whereas most
token reduction techniques for ViTs rely on attention mechanisms for importance
measurement and overlook the order of compressed tokens. In this paper, we
investigate a Mamba structure-aware importance score to evaluate token
importance in a simple and effective manner. Building on this score, we further
propose MTR, a training-free \textbf{M}amba \textbf{T}oken \textbf{R}eduction
framework. Without the need for training or additional tuning parameters, our
method can be seamlessly integrated as a plug-and-play component across various
Mamba models. Extensive experiments demonstrate that our approach significantly
reduces computational workload while minimizing performance impact across
various tasks and multiple backbones. Notably, MTR reduces FLOPs by
approximately 40\% on the Vim-B backbone, with only a 1.6\% drop in ImageNet
performance without retraining.

</details>


### [77] [Foundation Models as Class-Incremental Learners for Dermatological Image Classification](https://arxiv.org/abs/2507.14050)
*Mohamed Elkhayat,Mohamed Mahmoud,Jamil Fayyad,Nourhan Bayasi*

Main category: cs.CV

TL;DR: The paper evaluates frozen foundation models (FMs) for Class-Incremental Learning (CIL) in dermatology, proposing a lightweight MLP approach that outperforms existing methods. It also explores zero-training scenarios with prototype-based classifiers, achieving competitive results.


<details>
  <summary>Details</summary>
Motivation: To leverage pretrained foundation models for continual learning in dermatology, addressing the gap in their application for incremental disease classification.

Method: Uses frozen FMs with a lightweight MLP trained incrementally for each task and explores zero-training scenarios with prototype-based classifiers.

Result: Achieves state-of-the-art performance without forgetting, outperforming other methods, and shows competitive results in zero-training scenarios.

Conclusion: Frozen FMs are highly effective for continual learning in dermatology, supporting their broader use in medical applications.

Abstract: Class-Incremental Learning (CIL) aims to learn new classes over time without
forgetting previously acquired knowledge. The emergence of foundation models
(FM) pretrained on large datasets presents new opportunities for CIL by
offering rich, transferable representations. However, their potential for
enabling incremental learning in dermatology remains largely unexplored. In
this paper, we systematically evaluate frozen FMs pretrained on large-scale
skin lesion datasets for CIL in dermatological disease classification. We
propose a simple yet effective approach where the backbone remains frozen, and
a lightweight MLP is trained incrementally for each task. This setup achieves
state-of-the-art performance without forgetting, outperforming regularization,
replay, and architecture based methods. To further explore the capabilities of
frozen FMs, we examine zero training scenarios using nearest mean classifiers
with prototypes derived from their embeddings. Through extensive ablation
studies, we demonstrate that this prototype based variant can also achieve
competitive results. Our findings highlight the strength of frozen FMs for
continual learning in dermatology and support their broader adoption in real
world medical applications. Our code and datasets are available here.

</details>


### [78] [VLA-Mark: A cross modal watermark for large vision-language alignment model](https://arxiv.org/abs/2507.14067)
*Shuliang Liu,Qi Zheng,Jesse Jiaxi Xu,Yibo Yan,He Geng,Aiwei Liu,Peijie Jiang,Jia Liu,Yik-Cheung Tam,Xuming Hu*

Main category: cs.CV

TL;DR: VLA-Mark is a vision-aligned watermarking framework that preserves multimodal coherence while protecting intellectual property, outperforming existing methods in semantic fidelity and detection accuracy.


<details>
  <summary>Details</summary>
Motivation: Existing text watermarking methods disrupt visual-textual alignment and compromise semantic-critical concepts, necessitating a solution that balances watermarking with cross-modal coordination.

Method: VLA-Mark integrates multiscale visual-textual alignment metrics (patch affinity, global coherence, contextual attention) and an entropy-sensitive mechanism to dynamically balance watermark strength and semantic preservation.

Result: Achieves 7.4% lower PPL, 26.6% higher BLEU, 98.8% AUC detection, and 96.1% attack resilience while maintaining text-visual consistency.

Conclusion: VLA-Mark sets new standards for quality-preserving multimodal watermarking by ensuring semantic fidelity and robust detection.

Abstract: Vision-language models demand watermarking solutions that protect
intellectual property without compromising multimodal coherence. Existing text
watermarking methods disrupt visual-textual alignment through biased token
selection and static strategies, leaving semantic-critical concepts vulnerable.
We propose VLA-Mark, a vision-aligned framework that embeds detectable
watermarks while preserving semantic fidelity through cross-modal coordination.
Our approach integrates multiscale visual-textual alignment metrics, combining
localized patch affinity, global semantic coherence, and contextual attention
patterns, to guide watermark injection without model retraining. An
entropy-sensitive mechanism dynamically balances watermark strength and
semantic preservation, prioritizing visual grounding during low-uncertainty
generation phases. Experiments show 7.4% lower PPL and 26.6% higher BLEU than
conventional methods, with near-perfect detection (98.8% AUC). The framework
demonstrates 96.1\% attack resilience against attacks such as paraphrasing and
synonym substitution, while maintaining text-visual consistency, establishing
new standards for quality-preserving multimodal watermarking

</details>


### [79] [Unmasking Performance Gaps: A Comparative Study of Human Anonymization and Its Effects on Video Anomaly Detection](https://arxiv.org/abs/2507.14083)
*Sara Abdulaziz,Egor Bondarev*

Main category: cs.CV

TL;DR: The paper evaluates how four human anonymization techniques (blurring, masking, encryption, avatar replacement) affect anomaly detection performance on the UCF-Crime dataset, revealing algorithm-specific sensitivities and trade-offs between privacy and utility.


<details>
  <summary>Details</summary>
Motivation: Address privacy concerns in anomaly detection due to sensitive human data collection while maintaining detection performance.

Method: Analyze four anomaly detection methods (MGFN, UR-DMU, BN-WVAD, PEL4VAD) on anonymized UCF-Crime data using four obfuscation techniques.

Result: Anomaly detection remains viable under anonymization, with some methods performing better under specific techniques (e.g., encryption, masking).

Conclusion: The study highlights algorithm-specific sensitivities to anonymization and the trade-off between privacy and utility, providing benchmarks for future work.

Abstract: Advancements in deep learning have improved anomaly detection in surveillance
videos, yet they raise urgent privacy concerns due to the collection of
sensitive human data. In this paper, we present a comprehensive analysis of
anomaly detection performance under four human anonymization techniques,
including blurring, masking, encryption, and avatar replacement, applied to the
UCF-Crime dataset. We evaluate four anomaly detection methods, MGFN, UR-DMU,
BN-WVAD, and PEL4VAD, on the anonymized UCF-Crime to reveal how each method
responds to different obfuscation techniques. Experimental results demonstrate
that anomaly detection remains viable under anonymized data and is dependent on
the algorithmic design and the learning strategy. For instance, under certain
anonymization patterns, such as encryption and masking, some models
inadvertently achieve higher AUC performance compared to raw data, due to the
strong responsiveness of their algorithmic components to these noise patterns.
These results highlight the algorithm-specific sensitivities to anonymization
and emphasize the trade-off between preserving privacy and maintaining
detection utility. Furthermore, we compare these conventional anonymization
techniques with the emerging privacy-by-design solutions, highlighting an often
overlooked trade-off between robust privacy protection and utility flexibility.
Through comprehensive experiments and analyses, this study provides a
compelling benchmark and insights into balancing human privacy with the demands
of anomaly detection.

</details>


### [80] [Multi-Centre Validation of a Deep Learning Model for Scoliosis Assessment](https://arxiv.org/abs/2507.14093)
*Šimon Kubov,Simon Klíčník,Jakub Dandár,Zdeněk Straka,Karolína Kvaková,Daniel Kvak*

Main category: cs.CV

TL;DR: An AI tool (Carebot AI Bones) was evaluated for automated Cobb angle measurement in scoliosis, showing strong agreement with radiologists and potential for clinical use.


<details>
  <summary>Details</summary>
Motivation: Manual Cobb angle measurement for scoliosis is time-consuming and varies between observers, necessitating an automated solution.

Method: A retrospective, multi-center study tested the AI software on 103 spine radiographs, comparing its results with two radiologists using Bland-Altman analysis, MAE, RMSE, Pearson correlation, and Cohen kappa.

Result: The AI achieved MAEs of ~3.9 degrees, strong correlations (r=0.88-0.91), and moderate kappa scores (0.51-0.64) for severity grading, closely matching radiologists.

Conclusion: The AI software reliably replicates expert-level measurements, offering a practical tool for improving scoliosis assessment in clinical workflows.

Abstract: Scoliosis affects roughly 2 to 4 percent of adolescents, and treatment
decisions depend on precise Cobb angle measurement. Manual assessment is time
consuming and subject to inter observer variation. We conducted a
retrospective, multi centre evaluation of a fully automated deep learning
software (Carebot AI Bones, Spine Measurement functionality; Carebot s.r.o.) on
103 standing anteroposterior whole spine radiographs collected from ten
hospitals. Two musculoskeletal radiologists independently measured each study
and served as reference readers. Agreement between the AI and each radiologist
was assessed with Bland Altman analysis, mean absolute error (MAE), root mean
squared error (RMSE), Pearson correlation coefficient, and Cohen kappa for four
grade severity classification. Against Radiologist 1 the AI achieved an MAE of
3.89 degrees (RMSE 4.77 degrees) with a bias of 0.70 degrees and limits of
agreement from minus 8.59 to plus 9.99 degrees. Against Radiologist 2 the AI
achieved an MAE of 3.90 degrees (RMSE 5.68 degrees) with a bias of 2.14 degrees
and limits from minus 8.23 to plus 12.50 degrees. Pearson correlations were r
equals 0.906 and r equals 0.880 (inter reader r equals 0.928), while Cohen
kappa for severity grading reached 0.51 and 0.64 (inter reader kappa 0.59).
These results demonstrate that the proposed software reproduces expert level
Cobb angle measurements and categorical grading across multiple centres,
suggesting its utility for streamlining scoliosis reporting and triage in
clinical workflows.

</details>


### [81] [C-DOG: Training-Free Multi-View Multi-Object Association in Dense Scenes Without Visual Feature via Connected δ-Overlap Graphs](https://arxiv.org/abs/2507.14095)
*Yung-Hong Sun,Ting-Hung Lin,Jiangang Chen,Hongrui Jiang,Yu Hen Hu*

Main category: cs.CV

TL;DR: C-DOG is a training-free framework for multi-view multi-object association, combining graph modeling and epipolar geometry to robustly group object instances across views without relying on visual features.


<details>
  <summary>Details</summary>
Motivation: Existing methods fail with visually indistinguishable objects or noisy observations, necessitating a robust, feature-independent solution.

Method: C-DOG uses connected delta-overlap graph modeling and epipolar consistency, with delta-neighbor-overlap clustering, IQR filtering, and 3D back-projection error for robustness.

Result: C-DOG outperforms geometry-based baselines in synthetic benchmarks, handling high object density and limited camera overlap.

Conclusion: C-DOG is scalable and robust, suitable for real-world 3D reconstruction without visual features.

Abstract: Multi-view multi-object association is a fundamental step in 3D
reconstruction pipelines, enabling consistent grouping of object instances
across multiple camera views. Existing methods often rely on appearance
features or geometric constraints such as epipolar consistency. However, these
approaches can fail when objects are visually indistinguishable or observations
are corrupted by noise. We propose C-DOG, a training-free framework that serves
as an intermediate module bridging object detection (or pose estimation) and 3D
reconstruction, without relying on visual features. It combines connected
delta-overlap graph modeling with epipolar geometry to robustly associate
detections across views. Each 2D observation is represented as a graph node,
with edges weighted by epipolar consistency. A delta-neighbor-overlap
clustering step identifies strongly consistent groups while tolerating noise
and partial connectivity. To further improve robustness, we incorporate
Interquartile Range (IQR)-based filtering and a 3D back-projection error
criterion to eliminate inconsistent observations. Extensive experiments on
synthetic benchmarks demonstrate that C-DOG outperforms geometry-based
baselines and remains robust under challenging conditions, including high
object density, without visual features, and limited camera overlap, making it
well-suited for scalable 3D reconstruction in real-world scenarios.

</details>


### [82] [Franca: Nested Matryoshka Clustering for Scalable Visual Representation Learning](https://arxiv.org/abs/2507.14137)
*Shashanka Venkataramanan,Valentinos Pariza,Mohammadreza Salehi,Lukas Knobel,Spyros Gidaris,Elias Ramzi,Andrei Bursuc,Yuki M. Asano*

Main category: cs.CV

TL;DR: Franca is the first fully open-source vision foundation model that outperforms proprietary models like DINOv2 and CLIP. It uses transparent training with public data and introduces innovations in SSL clustering and positional disentanglement.


<details>
  <summary>Details</summary>
Motivation: To create a transparent, high-performance vision model that addresses limitations in SSL clustering methods and positional biases in dense representations.

Method: Uses a transparent training pipeline with public datasets (ImageNet-21K, ReLAION-2B). Introduces a multi-head clustering projector with nested Matryoshka representations and a positional disentanglement strategy.

Result: Matches or surpasses proprietary models, with consistent gains on downstream benchmarks due to cleaner feature spaces.

Conclusion: Franca sets a new standard for open-source vision models, promoting reproducibility and generalizability in AI.

Abstract: We present Franca (pronounced Fran-ka): free one; the first fully open-source
(data, code, weights) vision foundation model that matches and in many cases
surpasses the performance of state-of-the-art proprietary models, e.g., DINOv2,
CLIP, SigLIPv2, etc. Our approach is grounded in a transparent training
pipeline inspired by Web-SSL and uses publicly available data: ImageNet-21K and
a subset of ReLAION-2B. Beyond model release, we tackle critical limitations in
SSL clustering methods. While modern models rely on assigning image features to
large codebooks via clustering algorithms like Sinkhorn-Knopp, they fail to
account for the inherent ambiguity in clustering semantics. To address this, we
introduce a parameter-efficient, multi-head clustering projector based on
nested Matryoshka representations. This design progressively refines features
into increasingly fine-grained clusters without increasing the model size,
enabling both performance and memory efficiency. Additionally, we propose a
novel positional disentanglement strategy that explicitly removes positional
biases from dense representations, thereby improving the encoding of semantic
content. This leads to consistent gains on several downstream benchmarks,
demonstrating the utility of cleaner feature spaces. Our contributions
establish a new standard for transparent, high-performance vision models and
open a path toward more reproducible and generalizable foundation models for
the broader AI community. The code and model checkpoints are available at
https://github.com/valeoai/Franca.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [83] [GraphTrafficGPT: Enhancing Traffic Management Through Graph-Based AI Agent Coordination](https://arxiv.org/abs/2507.13511)
*Nabil Abdelaziz Ferhat Taleb,Abdolazim Rezaei,Raj Atulkumar Patel,Mehdi Sookhak*

Main category: cs.AI

TL;DR: GraphTrafficGPT improves traffic management by replacing sequential task execution with a graph-based architecture, reducing token usage and latency while enabling parallel processing.


<details>
  <summary>Details</summary>
Motivation: Current LLM-based traffic systems like TrafficGPT suffer from inefficiencies due to sequential execution, high token consumption, and poor scalability in complex scenarios.

Method: Proposes GraphTrafficGPT, a graph-based architecture with a Brain Agent for task decomposition, dependency graph construction, and coordination of specialized agents. Includes context-aware token management and concurrent multi-query support.

Result: Reduces token consumption by 50.2%, response latency by 19.0%, and improves multi-query efficiency by 23.0% compared to TrafficGPT.

Conclusion: GraphTrafficGPT offers a scalable and efficient solution for LLM-driven traffic management, outperforming existing sequential systems.

Abstract: Large Language Models (LLMs) offer significant promise for intelligent
traffic management; however, current chain-based systems like TrafficGPT are
hindered by sequential task execution, high token usage, and poor scalability,
making them inefficient for complex, real-world scenarios. To address these
limitations, we propose GraphTrafficGPT, a novel graph-based architecture,
which fundamentally redesigns the task coordination process for LLM-driven
traffic applications. GraphTrafficGPT represents tasks and their dependencies
as nodes and edges in a directed graph, enabling efficient parallel execution
and dynamic resource allocation. The main idea behind the proposed model is a
Brain Agent that decomposes user queries, constructs optimized dependency
graphs, and coordinates a network of specialized agents for data retrieval,
analysis, visualization, and simulation. By introducing advanced context-aware
token management and supporting concurrent multi-query processing, the proposed
architecture handles interdependent tasks typical of modern urban mobility
environments. Experimental results demonstrate that GraphTrafficGPT reduces
token consumption by 50.2% and average response latency by 19.0% compared to
TrafficGPT, while supporting simultaneous multi-query execution with up to
23.0% improvement in efficiency.

</details>


### [84] [PrefPalette: Personalized Preference Modeling with Latent Attributes](https://arxiv.org/abs/2507.13541)
*Shuyue Stella Li,Melanie Sclar,Hunter Lang,Ansong Ni,Jacqueline He,Puxin Xu,Andrew Cohen,Chan Young Park,Yulia Tsvetkov,Asli Celikyilmaz*

Main category: cs.AI

TL;DR: PrefPalette decomposes preferences into interpretable attributes, outperforming GPT-4o by 46.6% in accuracy while revealing community-specific value profiles.


<details>
  <summary>Details</summary>
Motivation: Current preference models treat human judgment as a black box, lacking insight into underlying reasons. PrefPalette aims to personalize AI by understanding attribute-driven preferences.

Method: Uses multi-attribute decision making: (1) scalable counterfactual attribute synthesis for training data, (2) attention-based modeling to learn community-specific attribute weighting.

Result: Outperforms GPT-4o by 46.6% in prediction accuracy, revealing distinct community profiles (e.g., scholarly communities value verbosity, conflict-oriented ones prefer sarcasm).

Conclusion: PrefPalette advances preference modeling with transparency and interpretability, paving the way for trustworthy, value-aware AI personalization.

Abstract: Personalizing AI systems requires understanding not just what users prefer,
but the reasons that underlie those preferences - yet current preference models
typically treat human judgment as a black box. We introduce PrefPalette, a
framework that decomposes preferences into attribute dimensions and tailors its
preference prediction to distinct social community values in a
human-interpretable manner. PrefPalette operationalizes a cognitive science
principle known as multi-attribute decision making in two ways: (1) a scalable
counterfactual attribute synthesis step that involves generating synthetic
training data to isolate for individual attribute effects (e.g., formality,
humor, cultural values), and (2) attention-based preference modeling that
learns how different social communities dynamically weight these attributes.
This approach moves beyond aggregate preference modeling to capture the diverse
evaluation frameworks that drive human judgment. When evaluated on 45 social
communities from the online platform Reddit, PrefPalette outperforms GPT-4o by
46.6% in average prediction accuracy. Beyond raw predictive improvements,
PrefPalette also shed light on intuitive, community-specific profiles:
scholarly communities prioritize verbosity and stimulation, conflict-oriented
communities value sarcasm and directness, and support-based communities
emphasize empathy. By modeling the attribute-mediated structure of human
judgment, PrefPalette delivers both superior preference modeling and
transparent, interpretable insights, and serves as a first step toward more
trustworthy, value-aware personalized applications.

</details>


### [85] [GOFAI meets Generative AI: Development of Expert Systems by means of Large Language Models](https://arxiv.org/abs/2507.13550)
*Eduardo C. Garrido-Merchán,Cristina Puente*

Main category: cs.AI

TL;DR: The paper introduces a controlled and transparent method for using LLMs in expert systems, combining their recall with symbolic precision to reduce hallucinations and ensure reliability.


<details>
  <summary>Details</summary>
Motivation: Address the issues of hallucinations and unverifiable facts in LLMs by developing a dependable hybrid approach for expert systems.

Method: Uses domain limitation and structured prompt-based extraction to generate symbolic Prolog knowledge bases, validated by human experts.

Result: Demonstrates strong fact adherence and semantic coherence in knowledge bases using Claude Sonnet 3.7 and GPT-4.1.

Conclusion: The hybrid approach ensures interpretability, scalability, and reliability, making it suitable for sensitive AI applications.

Abstract: The development of large language models (LLMs) has successfully transformed
knowledge-based systems such as open domain question nswering, which can
automatically produce vast amounts of seemingly coherent information. Yet,
those models have several disadvantages like hallucinations or confident
generation of incorrect or unverifiable facts. In this paper, we introduce a
new approach to the development of expert systems using LLMs in a controlled
and transparent way. By limiting the domain and employing a well-structured
prompt-based extraction approach, we produce a symbolic representation of
knowledge in Prolog, which can be validated and corrected by human experts.
This approach also guarantees interpretability, scalability and reliability of
the developed expert systems. Via quantitative and qualitative experiments with
Claude Sonnet 3.7 and GPT-4.1, we show strong adherence to facts and semantic
coherence on our generated knowledge bases. We present a transparent hybrid
solution that combines the recall capacity of LLMs with the precision of
symbolic systems, thereby laying the foundation for dependable AI applications
in sensitive domains.

</details>


### [86] [Why Isn't Relational Learning Taking Over the World?](https://arxiv.org/abs/2507.13558)
*David Poole*

Main category: cs.AI

TL;DR: The paper argues for modeling entities and relations instead of pixels/words, highlights the gap in relational learning's prominence, and suggests improvements.


<details>
  <summary>Details</summary>
Motivation: Current AI focuses on pixels/words, but real-world data (e.g., spreadsheets) involves entities and relations, which are understudied.

Method: Analyzes why relational learning (e.g., statistical relational AI) isn't dominant despite its relevance to valuable data.

Result: Relational learning remains niche due to restricted applications and lack of focus.

Conclusion: Relational learning needs advancements to achieve its potential in AI.

Abstract: AI seems to be taking over the world with systems that model pixels, words,
and phonemes. The world is arguably made up, not of pixels, words, and phonemes
but of entities (objects, things, including events) with properties and
relations among them. Surely we should model these, not the perception or
description of them. You might suspect that concentrating on modeling words and
pixels is because all of the (valuable) data in the world is in terms of text
and images. If you look into almost any company you will find their most
valuable data is in spreadsheets, databases and other relational formats. These
are not the form that are studied in introductory machine learning, but are
full of product numbers, student numbers, transaction numbers and other
identifiers that can't be interpreted naively as numbers. The field that
studies this sort of data has various names including relational learning,
statistical relational AI, and many others. This paper explains why relational
learning is not taking over the world -- except in a few cases with restricted
relations -- and what needs to be done to bring it to it's rightful prominence.

</details>


### [87] [BifrostRAG: Bridging Dual Knowledge Graphs for Multi-Hop Question Answering in Construction Safety](https://arxiv.org/abs/2507.13625)
*Yuxin Zhang,Xi Wang,Mo Hu,Zhenyu Zhang*

Main category: cs.AI

TL;DR: BifrostRAG, a dual-graph RAG system, improves multi-hop question answering in compliance checking by combining linguistic and structural modeling, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Traditional RAG systems struggle with multi-hop queries in complex regulatory texts due to linguistic and structural challenges.

Method: BifrostRAG uses an Entity Network Graph for linguistic relationships and a Document Navigator Graph for structure, enabling hybrid retrieval via graph traversal and semantic search.

Result: Achieves 92.8% precision, 85.5% recall, and 87.3% F1 score, outperforming vector-only and graph-only baselines.

Conclusion: BifrostRAG is a robust solution for compliance checking, offering a transferable approach for complex technical documents.

Abstract: Information retrieval and question answering from safety regulations are
essential for automated construction compliance checking but are hindered by
the linguistic and structural complexity of regulatory text. Many
compliance-related queries are multi-hop, requiring synthesis of information
across interlinked clauses. This poses a challenge for traditional
retrieval-augmented generation (RAG) systems. To overcome this, we introduce
BifrostRAG: a dual-graph RAG-integrated system that explicitly models both
linguistic relationships (via an Entity Network Graph) and document structure
(via a Document Navigator Graph). This architecture powers a hybrid retrieval
mechanism that combines graph traversal with vector-based semantic search,
enabling large language models to reason over both the meaning and the
structure of the text. Evaluation on a multi-hop question dataset shows that
BifrostRAG achieves 92.8 percent precision, 85.5 percent recall, and an F1
score of 87.3 percent. These results significantly outperform vector-only and
graph-only RAG baselines that represent current leading approaches. Error
analysis further highlights the comparative advantages of our hybrid method
over single-modality RAGs. These findings establish BifrostRAG as a robust
knowledge engine for LLM-driven compliance checking. Its dual-graph, hybrid
retrieval mechanism offers a transferable blueprint for navigating complex
technical documents across knowledge-intensive engineering domains.

</details>


### [88] [Buggy rule diagnosis for combined steps through final answer evaluation in stepwise tasks](https://arxiv.org/abs/2507.13651)
*Gerben van der Hoek,Johan Jeuring,Rogier Bos*

Main category: cs.AI

TL;DR: The paper explores automated error diagnosis in intelligent tutoring systems using final answers to mitigate combinatorial explosion in stepwise tasks, showing promising results.


<details>
  <summary>Details</summary>
Motivation: Combinatorial explosion in diagnosing student errors in stepwise tasks makes error diagnosis challenging. Using final answers can simplify this process.

Method: The study designs a service for buggy rule diagnosis based on final answers and validates it using a dataset of student steps in solving quadratic equations.

Result: Final answer evaluation diagnosed 29.4% of previously undiagnosed steps, with 97% alignment to teacher diagnoses in a subset.

Conclusion: The approach shows potential for further exploration in automating error diagnosis in tutoring systems.

Abstract: Many intelligent tutoring systems can support a student in solving a stepwise
task. When a student combines several steps in one step, the number of possible
paths connecting consecutive inputs may be very large. This combinatorial
explosion makes error diagnosis hard. Using a final answer to diagnose a
combination of steps can mitigate the combinatorial explosion, because there
are generally fewer possible (erroneous) final answers than (erroneous)
solution paths. An intermediate input for a task can be diagnosed by
automatically completing it according to the task solution strategy and
diagnosing this solution. This study explores the potential of automated error
diagnosis based on a final answer. We investigate the design of a service that
provides a buggy rule diagnosis when a student combines several steps. To
validate the approach, we apply the service to an existing dataset (n=1939) of
unique student steps when solving quadratic equations, which could not be
diagnosed by a buggy rule service that tries to connect consecutive inputs with
a single rule. Results show that final answer evaluation can diagnose 29,4% of
these steps. Moreover, a comparison of the generated diagnoses with teacher
diagnoses on a subset (n=115) shows that the diagnoses align in 97% of the
cases. These results can be considered a basis for further exploration of the
approach.

</details>


### [89] [Combining model tracing and constraint-based modeling for multistep strategy diagnoses](https://arxiv.org/abs/2507.13652)
*Gerben van der Hoek,Johan Jeuring,Rogier Bos*

Main category: cs.AI

TL;DR: A hybrid approach combining model tracing and constraint-based modeling for diagnosing student input in stepwise tasks, validated with teacher-aligned results.


<details>
  <summary>Details</summary>
Motivation: To diagnose student input accurately even when steps are combined, leveraging strengths of both model tracing and constraint-based modeling.

Method: Defining constraints as shared properties between student input and strategy steps, enabling diagnosis of deviations. Evaluated using a dataset of quadratic equation solutions (n=2136) and teacher-coded samples (n=140).

Result: System diagnoses matched teacher coding in all 140 student steps, demonstrating high alignment.

Conclusion: The hybrid approach effectively diagnoses multistep strategies, validated by teacher agreement.

Abstract: Model tracing and constraint-based modeling are two approaches to diagnose
student input in stepwise tasks. Model tracing supports identifying consecutive
problem-solving steps taken by a student, whereas constraint-based modeling
supports student input diagnosis even when several steps are combined into one
step. We propose an approach that merges both paradigms. By defining
constraints as properties that a student input has in common with a step of a
strategy, it is possible to provide a diagnosis when a student deviates from a
strategy even when the student combines several steps. In this study we explore
the design of a system for multistep strategy diagnoses, and evaluate these
diagnoses. As a proof of concept, we generate diagnoses for an existing dataset
containing steps students take when solving quadratic equations (n=2136). To
compare with human diagnoses, two teachers coded a random sample of deviations
(n=70) and applications of the strategy (n=70). Results show that that the
system diagnosis aligned with the teacher coding in all of the 140 student
steps.

</details>


### [90] [DailyLLM: Context-Aware Activity Log Generation Using Multi-Modal Sensors and LLMs](https://arxiv.org/abs/2507.13737)
*Ye Tian,Xiaoyuan Ren,Zihao Wang,Onat Gungor,Xiaofan Yu,Tajana Rosing*

Main category: cs.AI

TL;DR: DailyLLM is a novel system for generating and summarizing activity logs using a lightweight LLM framework, outperforming SOTA methods in accuracy and efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing activity log generation methods lack accuracy, efficiency, and semantic richness, prompting the need for a better solution.

Method: DailyLLM integrates contextual activity information (location, motion, environment, physiology) using smartphone/smartwatch sensors and employs a lightweight LLM with structured prompting and efficient feature extraction.

Result: DailyLLM achieves a 17% BERTScore precision improvement over a 70B-parameter SOTA baseline and offers 10x faster inference speed with a 1.5B-parameter model.

Conclusion: DailyLLM effectively addresses limitations in log generation, demonstrating superior performance and deployability on resource-constrained devices.

Abstract: Rich and context-aware activity logs facilitate user behavior analysis and
health monitoring, making them a key research focus in ubiquitous computing.
The remarkable semantic understanding and generation capabilities of Large
Language Models (LLMs) have recently created new opportunities for activity log
generation. However, existing methods continue to exhibit notable limitations
in terms of accuracy, efficiency, and semantic richness. To address these
challenges, we propose DailyLLM. To the best of our knowledge, this is the
first log generation and summarization system that comprehensively integrates
contextual activity information across four dimensions: location, motion,
environment, and physiology, using only sensors commonly available on
smartphones and smartwatches. To achieve this, DailyLLM introduces a
lightweight LLM-based framework that integrates structured prompting with
efficient feature extraction to enable high-level activity understanding.
Extensive experiments demonstrate that DailyLLM outperforms state-of-the-art
(SOTA) log generation methods and can be efficiently deployed on personal
computers and Raspberry Pi. Utilizing only a 1.5B-parameter LLM model, DailyLLM
achieves a 17% improvement in log generation BERTScore precision compared to
the 70B-parameter SOTA baseline, while delivering nearly 10x faster inference
speed.

</details>


### [91] [OntView: What you See is What you Meant](https://arxiv.org/abs/2507.13759)
*Carlos Bobed,Carlota Quintana,Eduardo Mena,Jorge Bobed,Fernando Bobillo*

Main category: cs.AI

TL;DR: OntView is an ontology viewer offering intuitive visualization of ontology concepts and inferred knowledge, addressing the challenge of overwhelming representations in existing tools.


<details>
  <summary>Details</summary>
Motivation: Existing ontology visualization tools often fail to clearly represent complex structures, hindering user comprehension. OntView aims to solve this by providing meaningful and simplified views.

Method: OntView uses a DL reasoner for inferred knowledge visualization, supports General Concept Inclusions (GCI), and offers features like ontology summaries, focused TBox visualization, and dynamic branch hiding.

Result: OntView provides a user-friendly interface for visualizing ontologies without semantic loss, addressing information overload with customizable views.

Conclusion: OntView successfully improves ontology visualization by combining intuitive design with advanced features, released as open-source for community use.

Abstract: In the field of knowledge management and computer science, ontologies provide
a structured framework for modeling domain-specific knowledge by defining
concepts and their relationships. However, the lack of tools that provide
effective visualization is still a significant challenge. While numerous
ontology editors and viewers exist, most of them fail to graphically represent
ontology structures in a meaningful and non-overwhelming way, limiting users'
ability to comprehend dependencies and properties within large ontological
frameworks.
  In this paper, we present OntView, an ontology viewer that is designed to
provide users with an intuitive visual representation of ontology concepts and
their formal definitions through a user-friendly interface. Building on the use
of a DL reasoner, OntView follows a "What you see is what you meant" paradigm,
showing the actual inferred knowledge. One key aspect for this is its ability
to visualize General Concept Inclusions (GCI), a feature absent in existing
visualization tools. Moreover, to avoid a possible information overload,
OntView also offers different ways to show a simplified view of the ontology
by: 1) creating ontology summaries by assessing the importance of the concepts
(according to different available algorithms), 2) focusing the visualization on
the existing TBox elements between two given classes and 3) allowing to
hide/show different branches in a dynamic way without losing the semantics.
OntView has been released with an open-source license for the whole community.

</details>


### [92] [From Extraction to Synthesis: Entangled Heuristics for Agent-Augmented Strategic Reasoning](https://arxiv.org/abs/2507.13768)
*Renato Ghisellini,Remo Pareschi,Marco Pedroni,Giovanni Battista Raggi*

Main category: cs.AI

TL;DR: A hybrid architecture for agent-augmented strategic reasoning combines heuristic extraction, semantic activation, and compositional synthesis, fusing conflicting heuristics into context-sensitive narratives.


<details>
  <summary>Details</summary>
Motivation: To improve strategic reasoning by integrating diverse heuristics and resolving conflicts through semantic interdependence, inspired by quantum cognition.

Method: Combines heuristic extraction, semantic activation, and compositional synthesis, guided by semantic interaction modeling and rhetorical framing.

Result: Demonstrated via a Meta vs. FTC case study with preliminary validation using semantic metrics.

Conclusion: The framework shows promise but has limitations; extensions like dynamic interference tuning are suggested.

Abstract: We present a hybrid architecture for agent-augmented strategic reasoning,
combining heuristic extraction, semantic activation, and compositional
synthesis. Drawing on sources ranging from classical military theory to
contemporary corporate strategy, our model activates and composes multiple
heuristics through a process of semantic interdependence inspired by research
in quantum cognition. Unlike traditional decision engines that select the best
rule, our system fuses conflicting heuristics into coherent and
context-sensitive narratives, guided by semantic interaction modeling and
rhetorical framing. We demonstrate the framework via a Meta vs. FTC case study,
with preliminary validation through semantic metrics. Limitations and
extensions (e.g., dynamic interference tuning) are discussed.

</details>


### [93] [When Speed meets Accuracy: an Efficient and Effective Graph Model for Temporal Link Prediction](https://arxiv.org/abs/2507.13825)
*Haoyang Li,Yuming Xu,Yiming Li,Hanmo Liu,Darian Li,Chen Jason Zhang,Lei Chen,Qing Li*

Main category: cs.AI

TL;DR: EAGLE is a lightweight framework for temporal link prediction in dynamic graphs, combining short-term recency and long-term structural patterns for efficiency and effectiveness.


<details>
  <summary>Details</summary>
Motivation: Existing Temporal Graph Neural Networks (T-GNNs) face scalability and efficiency issues due to high computational overhead.

Method: EAGLE integrates a time-aware module for recent neighbor aggregation and a structure-aware module using temporal personalized PageRank, with adaptive weighting.

Result: EAGLE outperforms state-of-the-art T-GNNs, achieving a 50x speedup over transformer-based models.

Conclusion: EAGLE offers a scalable and efficient solution for temporal link prediction without complex architectures.

Abstract: Temporal link prediction in dynamic graphs is a critical task with
applications in diverse domains such as social networks, recommendation
systems, and e-commerce platforms. While existing Temporal Graph Neural
Networks (T-GNNs) have achieved notable success by leveraging complex
architectures to model temporal and structural dependencies, they often suffer
from scalability and efficiency challenges due to high computational overhead.
In this paper, we propose EAGLE, a lightweight framework that integrates
short-term temporal recency and long-term global structural patterns. EAGLE
consists of a time-aware module that aggregates information from a node's most
recent neighbors to reflect its immediate preferences, and a structure-aware
module that leverages temporal personalized PageRank to capture the influence
of globally important nodes. To balance these attributes, EAGLE employs an
adaptive weighting mechanism to dynamically adjust their contributions based on
data characteristics. Also, EAGLE eliminates the need for complex multi-hop
message passing or memory-intensive mechanisms, enabling significant
improvements in efficiency. Extensive experiments on seven real-world temporal
graphs demonstrate that EAGLE consistently achieves superior performance
against state-of-the-art T-GNNs in both effectiveness and efficiency,
delivering more than a 50x speedup over effective transformer-based T-GNNs.

</details>


### [94] [Causal Knowledge Transfer for Multi-Agent Reinforcement Learning in Dynamic Environments](https://arxiv.org/abs/2507.13846)
*Kathrin Korte,Christian Medeiros Adriano,Sona Ghahremani,Holger Giese*

Main category: cs.AI

TL;DR: A causal knowledge transfer framework for MARL enables agents to share compact causal representations, improving adaptation in non-stationary environments without retraining.


<details>
  <summary>Details</summary>
Motivation: Traditional MARL knowledge transfer methods struggle to generalize in non-stationary environments, requiring costly retraining. This paper aims to address this gap.

Method: Introduces a causal knowledge transfer framework where agents learn and share causal representations of paths. Collisions trigger recovery actions (macros) transferred online between agents.

Result: Agents bridged ~50% of the gap between random exploration and fully retrained policies. Performance depends on environment complexity and goal heterogeneity.

Conclusion: Causal knowledge transfer improves MARL adaptability in dynamic environments, reducing the need for retraining while maintaining performance.

Abstract: [Context] Multi-agent reinforcement learning (MARL) has achieved notable
success in environments where agents must learn coordinated behaviors. However,
transferring knowledge across agents remains challenging in non-stationary
environments with changing goals. [Problem] Traditional knowledge transfer
methods in MARL struggle to generalize, and agents often require costly
retraining to adapt. [Approach] This paper introduces a causal knowledge
transfer framework that enables RL agents to learn and share compact causal
representations of paths within a non-stationary environment. As the
environment changes (new obstacles), agents' collisions require adaptive
recovery strategies. We model each collision as a causal intervention
instantiated as a sequence of recovery actions (a macro) whose effect
corresponds to a causal knowledge of how to circumvent the obstacle while
increasing the chances of achieving the agent's goal (maximizing cumulative
reward). This recovery action macro is transferred online from a second agent
and is applied in a zero-shot fashion, i.e., without retraining, just by
querying a lookup model with local context information (collisions). [Results]
Our findings reveal two key insights: (1) agents with heterogeneous goals were
able to bridge about half of the gap between random exploration and a fully
retrained policy when adapting to new environments, and (2) the impact of
causal knowledge transfer depends on the interplay between environment
complexity and agents' heterogeneous goals.

</details>


### [95] [Large Language Models as Innovators: A Framework to Leverage Latent Space Exploration for Novelty Discovery](https://arxiv.org/abs/2507.13874)
*Mateusz Bystroński,Mikołaj Hołysz,Grzegorz Piotrowski,Nitesh V. Chawla,Tomasz Kajdanowicz*

Main category: cs.AI

TL;DR: A model-agnostic latent-space ideation framework is proposed to enhance AI creativity by navigating embedding spaces, avoiding reliance on brittle heuristics or extensive prompt engineering.


<details>
  <summary>Details</summary>
Motivation: Large language models (LLMs) often fail to produce novel and relevant outputs due to pattern replication from training, requiring better methods for controlled creativity.

Method: The paper introduces a latent-space ideation framework that navigates continuous embedding spaces of ideas, eliminating the need for domain-specific rules or structured prompts.

Result: Preliminary results show the framework's potential as a scalable, adaptable solution for creative tasks across domains.

Conclusion: The proposed framework offers a promising approach for human-AI collaboration in idea generation, with broader applicability than prior methods.

Abstract: Innovative idea generation remains a core challenge in AI, as large language
models (LLMs) often struggle to produce outputs that are both novel and
relevant. Despite their fluency, LLMs tend to replicate patterns seen during
training, limiting their ability to diverge creatively without extensive prompt
engineering. Prior work has addressed this through domain-specific heuristics
and structured prompting pipelines, but such solutions are brittle and
difficult to generalize. In this paper, we propose a model-agnostic
latent-space ideation framework that enables controlled, scalable creativity by
navigating the continuous embedding space of ideas. Unlike prior methods, our
framework requires no handcrafted rules and adapts easily to different domains,
input formats, and creative tasks. This paper introduces an early-stage
prototype of our method, outlining the conceptual framework and preliminary
results highlighting its potential as a general-purpose co-ideator for human-AI
collaboration.

</details>


### [96] [Cross-modal Causal Intervention for Alzheimer's Disease Prediction](https://arxiv.org/abs/2507.13956)
*Yutao Jin,Haowen Xiao,Jielei Chu,Fengmao Lv,Yuxiao Li,Tianrui Li*

Main category: cs.AI

TL;DR: A novel visual-language causal intervention framework (ADPC) is proposed to improve Alzheimer's Disease (AD) diagnosis by addressing confounders in multimodal data, achieving state-of-the-art performance.


<details>
  <summary>Details</summary>
Motivation: Early identification of Mild Cognitive Impairment (MCI) as a precursor to AD is challenging due to data biases and complex variable relationships.

Method: ADPC uses LLM to structure clinical data and integrates MRI/fMRI images with textual data for classification (CN/MCI/AD), employing causal intervention to eliminate confounders.

Result: ADPC outperforms existing methods, achieving SOTA metrics in distinguishing CN/MCI/AD cases.

Conclusion: The study highlights the potential of causal reasoning in multimodal learning for neurological disease diagnosis.

Abstract: Mild Cognitive Impairment (MCI) serves as a prodromal stage of Alzheimer's
Disease (AD), where early identification and intervention can effectively slow
the progression to dementia. However, diagnosing AD remains a significant
challenge in neurology due to the confounders caused mainly by the selection
bias of multimodal data and the complex relationships between variables. To
address these issues, we propose a novel visual-language causal intervention
framework named Alzheimer's Disease Prediction with Cross-modal Causal
Intervention (ADPC) for diagnostic assistance. Our ADPC employs large language
model (LLM) to summarize clinical data under strict templates, maintaining
structured text outputs even with incomplete or unevenly distributed datasets.
The ADPC model utilizes Magnetic Resonance Imaging (MRI), functional MRI (fMRI)
images and textual data generated by LLM to classify participants into
Cognitively Normal (CN), MCI, and AD categories. Because of the presence of
confounders, such as neuroimaging artifacts and age-related biomarkers,
non-causal models are likely to capture spurious input-output correlations,
generating less reliable results. Our framework implicitly eliminates
confounders through causal intervention. Experimental results demonstrate the
outstanding performance of our method in distinguishing CN/MCI/AD cases,
achieving state-of-the-art (SOTA) metrics across most evaluation metrics. The
study showcases the potential of integrating causal reasoning with multi-modal
learning for neurological disease diagnosis.

</details>


### [97] [Towards Constraint Temporal Answer Set Programming](https://arxiv.org/abs/2507.13958)
*Pedro Cabalar,Martín Diéguez,François Olivier,Torsten Schaub,Igor Stéphan*

Main category: cs.AI

TL;DR: A novel temporal and constraint-based extension of the logic of Here-and-There is introduced for nonmonotonic temporal reasoning in Answer Set Programming (ASP).


<details>
  <summary>Details</summary>
Motivation: Challenges in reasoning about dynamic systems with fine-grained temporal and numeric resolution in logic-based approaches like ASP.

Method: Synergistic combination of linear-time logic of Here-and-There (for nonmonotonic temporal reasoning) and logic of Here-and-There with constraints (for numeric constraints).

Result: First approach to nonmonotonic temporal reasoning with constraints tailored for ASP, enabling high-resolution dynamic system modeling.

Conclusion: Establishes a foundational logical framework for complex dynamic systems within ASP.

Abstract: Reasoning about dynamic systems with a fine-grained temporal and numeric
resolution presents significant challenges for logic-based approaches like
Answer Set Programming (ASP). To address this, we introduce and elaborate upon
a novel temporal and constraint-based extension of the logic of Here-and-There
and its nonmonotonic equilibrium extension, representing, to the best of our
knowledge, the first approach to nonmonotonic temporal reasoning with
constraints specifically tailored for ASP. This expressive system is achieved
by a synergistic combination of two foundational ASP extensions: the
linear-time logic of Here-and-There, providing robust nonmonotonic temporal
reasoning capabilities, and the logic of Here-and-There with constraints,
enabling the direct integration and manipulation of numeric constraints, among
others. This work establishes the foundational logical framework for tackling
complex dynamic systems with high resolution within the ASP paradigm.

</details>


### [98] [KROMA: Ontology Matching with Knowledge Retrieval and Large Language Models](https://arxiv.org/abs/2507.14032)
*Lam Nguyen,Erika Barcelos,Roger French,Yinghui Wu*

Main category: cs.AI

TL;DR: KROMA is a novel OM framework using LLMs and RAG to enhance semantic context, outperforming existing systems with optimized efficiency.


<details>
  <summary>Details</summary>
Motivation: Existing OM systems rely on rigid rules or specialized models, lacking adaptability. KROMA aims to improve flexibility and performance.

Method: KROMA combines bisimilarity-based concept matching, lightweight ontology refinement, and LLMs in a RAG pipeline for dynamic knowledge enrichment.

Result: KROMA outperforms classic and LLM-based OM systems on benchmarks, maintaining low communication overhead.

Conclusion: The study demonstrates the feasibility of KROMA's optimizations (knowledge retrieval, prompt enrichment, refinement) for scalable OM.

Abstract: Ontology Matching (OM) is a cornerstone task of semantic interoperability,
yet existing systems often rely on handcrafted rules or specialized models with
limited adaptability. We present KROMA, a novel OM framework that harnesses
Large Language Models (LLMs) within a Retrieval-Augmented Generation (RAG)
pipeline to dynamically enrich the semantic context of OM tasks with
structural, lexical, and definitional knowledge. To optimize both performance
and efficiency, KROMA integrates a bisimilarity-based concept matching and a
lightweight ontology refinement step, which prune candidate concepts and
substantially reduce the communication overhead from invoking LLMs. Through
experiments on multiple benchmark datasets, we show that integrating knowledge
retrieval with context-augmented LLMs significantly enhances ontology matching,
outperforming both classic OM systems and cutting-edge LLM-based approaches
while keeping communication overhead comparable. Our study highlights the
feasibility and benefit of the proposed optimization techniques (targeted
knowledge retrieval, prompt enrichment, and ontology refinement) for ontology
matching at scale.

</details>


### [99] [Glucose-ML: A collection of longitudinal diabetes datasets for development of robust AI solutions](https://arxiv.org/abs/2507.14077)
*Temiloluwa Prioleau,Baiying Lu,Yanjun Cui*

Main category: cs.AI

TL;DR: The paper introduces Glucose-ML, a collection of 10 publicly available diabetes datasets, to address barriers in AI development for diabetes management. It includes over 300,000 days of CGM data and provides a benchmark for blood glucose prediction, highlighting dataset variability's impact on AI performance.


<details>
  <summary>Details</summary>
Motivation: Large, high-quality datasets are crucial for robust AI solutions in diabetes management, but access barriers hinder development. This work aims to accelerate transparent and reproducible AI by providing a curated dataset collection.

Method: The authors compile 10 diabetes datasets (Glucose-ML) with 300,000+ days of CGM data. They conduct a comparative analysis and a case study on blood glucose prediction to benchmark performance across datasets.

Result: The study reveals significant variability in AI prediction results across datasets, emphasizing the need for careful data selection. Benchmarks and recommendations for robust AI development are provided.

Conclusion: Glucose-ML facilitates AI research in diabetes management by offering diverse datasets and insights. The work underscores the importance of dataset selection and reproducibility in health AI solutions.

Abstract: Artificial intelligence (AI) algorithms are a critical part of
state-of-the-art digital health technology for diabetes management. Yet, access
to large high-quality datasets is creating barriers that impede development of
robust AI solutions. To accelerate development of transparent, reproducible,
and robust AI solutions, we present Glucose-ML, a collection of 10 publicly
available diabetes datasets, released within the last 7 years (i.e., 2018 -
2025). The Glucose-ML collection comprises over 300,000 days of continuous
glucose monitor (CGM) data with a total of 38 million glucose samples collected
from 2500+ people across 4 countries. Participants include persons living with
type 1 diabetes, type 2 diabetes, prediabetes, and no diabetes. To support
researchers and innovators with using this rich collection of diabetes
datasets, we present a comparative analysis to guide algorithm developers with
data selection. Additionally, we conduct a case study for the task of blood
glucose prediction - one of the most common AI tasks within the field. Through
this case study, we provide a benchmark for short-term blood glucose prediction
across all 10 publicly available diabetes datasets within the Glucose-ML
collection. We show that the same algorithm can have significantly different
prediction results when developed/evaluated with different datasets. Findings
from this study are then used to inform recommendations for developing robust
AI solutions within the diabetes or broader health domain. We provide direct
links to each longitudinal diabetes dataset in the Glucose-ML collection and
openly provide our code.

</details>


### [100] [Generative AI-Driven High-Fidelity Human Motion Simulation](https://arxiv.org/abs/2507.14097)
*Hari Iyer,Neel Macwan,Atharva Jitendra Hude,Heejin Jeong,Shenghan Guo*

Main category: cs.AI

TL;DR: G-AI-HMS uses Generative AI to improve human motion simulation by integrating text-to-text and text-to-motion models, outperforming human-created descriptions in accuracy and alignment.


<details>
  <summary>Details</summary>
Motivation: Existing human motion simulation methods lack fidelity. G-AI-HMS aims to enhance simulation quality for industrial tasks by leveraging AI.

Method: Combines Large Language Models and MotionGPT for task-to-motion translation, validated against real human movements using computer vision and posture estimation.

Result: AI-enhanced motions showed lower error in most tasks, with significant improvements in joint error and temporal alignment (p < 0.0001).

Conclusion: G-AI-HMS effectively improves motion simulation fidelity, demonstrating the potential of AI in industrial task evaluation.

Abstract: Human motion simulation (HMS) supports cost-effective evaluation of worker
behavior, safety, and productivity in industrial tasks. However, existing
methods often suffer from low motion fidelity. This study introduces
Generative-AI-Enabled HMS (G-AI-HMS), which integrates text-to-text and
text-to-motion models to enhance simulation quality for physical tasks.
G-AI-HMS tackles two key challenges: (1) translating task descriptions into
motion-aware language using Large Language Models aligned with MotionGPT's
training vocabulary, and (2) validating AI-enhanced motions against real human
movements using computer vision. Posture estimation algorithms are applied to
real-time videos to extract joint landmarks, and motion similarity metrics are
used to compare them with AI-enhanced sequences. In a case study involving
eight tasks, the AI-enhanced motions showed lower error than human created
descriptions in most scenarios, performing better in six tasks based on spatial
accuracy, four tasks based on alignment after pose normalization, and seven
tasks based on overall temporal similarity. Statistical analysis showed that
AI-enhanced prompts significantly (p $<$ 0.0001) reduced joint error and
temporal misalignment while retaining comparable posture accuracy.

</details>


### [101] [Automated Interpretation of Non-Destructive Evaluation Contour Maps Using Large Language Models for Bridge Condition Assessment](https://arxiv.org/abs/2507.14107)
*Viraj Nishesh Darji,Callie C. Liao,Duoduo Liao*

Main category: cs.AI

TL;DR: The study explores using Large Language Models (LLMs) to automate the interpretation of Non-Destructive Evaluation (NDE) contour maps for bridge inspections, improving efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: Bridge maintenance requires efficient and accurate NDE data interpretation, which is time-consuming and expertise-dependent. LLMs offer a potential solution.

Method: Several LLMs were tested with tailored prompts to interpret NDE contour maps, evaluating their ability to describe images, identify defects, and provide recommendations.

Result: Four of nine LLMs excelled in image descriptions, with ChatGPT-4 and Claude 3.5 Sonnet producing the most effective summaries.

Conclusion: LLMs can enhance bridge inspection workflows by automating analysis, improving efficiency, and maintaining accuracy, as demonstrated in this pilot study.

Abstract: Bridge maintenance and safety are essential for transportation authorities,
and Non-Destructive Evaluation (NDE) techniques are critical to assessing
structural integrity. However, interpreting NDE data can be time-consuming and
requires expertise, potentially delaying decision-making. Recent advancements
in Large Language Models (LLMs) offer new ways to automate and improve this
analysis. This pilot study introduces a holistic assessment of LLM capabilities
for interpreting NDE contour maps and demonstrates the effectiveness of LLMs in
providing detailed bridge condition analyses. It establishes a framework for
integrating LLMs into bridge inspection workflows, indicating that LLM-assisted
analysis can enhance efficiency without compromising accuracy. In this study,
several LLMs are explored with prompts specifically designed to enhance the
quality of image descriptions, which are applied to interpret five different
NDE contour maps obtained through technologies for assessing bridge conditions.
Each LLM model is evaluated based on its ability to produce detailed
descriptions, identify defects, provide actionable recommendations, and
demonstrate overall accuracy. The research indicates that four of the nine
models provide better image descriptions, effectively covering a wide range of
topics related to the bridge's condition. The outputs from these four models
are summarized using five different LLMs to form a comprehensive overview of
the bridge. Notably, LLMs ChatGPT-4 and Claude 3.5 Sonnet generate more
effective summaries. The findings suggest that LLMs have the potential to
significantly improve efficiency and accuracy. This pilot study presents an
innovative approach that leverages LLMs for image captioning in parallel and
summarization, enabling faster decision-making in bridge maintenance and
enhancing infrastructure management and safety assessments.

</details>


### [102] [CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement Learning](https://arxiv.org/abs/2507.14111)
*Xiaoya Li,Xiaofei Sun,Albert Wang,Jiwei Li,Chris Shum*

Main category: cs.AI

TL;DR: CUDA-L1 is an automated reinforcement learning framework for CUDA optimization, achieving significant speedups across various GPU architectures and uncovering novel optimization techniques.


<details>
  <summary>Details</summary>
Motivation: The rapid growth in demand for GPU computing, driven by Large Language Models, necessitates automated CUDA optimization strategies due to the low success rates of current models.

Method: CUDA-L1 uses reinforcement learning to optimize CUDA kernels, trained on NVIDIA A100, and evaluates performance across multiple GPU architectures.

Result: Achieves average speedups of up to x17.7 on A100, with peak speedups of x449, and demonstrates portability across GPUs (e.g., x19.0 on RTX 3090). It also discovers optimization principles and identifies performance bottlenecks.

Conclusion: Reinforcement learning can transform LLMs into effective CUDA optimizers without human expertise, extending reasoning to new kernels and promising improved GPU efficiency.

Abstract: The exponential growth in demand for GPU computing resources, driven by the
rapid advancement of Large Language Models, has created an urgent need for
automated CUDA optimization strategies. While recent advances in LLMs show
promise for code generation, current SOTA models (e.g. R1, o1) achieve low
success rates in improving CUDA speed. In this paper, we introduce CUDA-L1, an
automated reinforcement learning framework for CUDA optimization.
  CUDA-L1 achieves performance improvements on the CUDA optimization task:
trained on NVIDIA A100, it delivers an average speedup of x17.7 across all 250
CUDA kernels of KernelBench, with peak speedups reaching x449. Furthermore, the
model also demonstrates excellent portability across GPU architectures,
achieving average speedups of x17.8 on H100, x19.0 on RTX 3090, x16.5 on L40,
x14.7 on H800, and x13.9 on H20 despite being optimized specifically for A100.
Beyond these benchmark results, CUDA-L1 demonstrates several remarkable
properties: 1) Discovers a variety of CUDA optimization techniques and learns
to combine them strategically to achieve optimal performance; 2) Uncovers
fundamental principles of CUDA optimization; 3) Identifies non-obvious
performance bottlenecks and rejects seemingly beneficial optimizations that
harm performance.
  The capabilities of CUDA-L1 demonstrate that reinforcement learning can
transform an initially poor-performing LLM into an effective CUDA optimizer
through speedup-based reward signals alone, without human expertise or domain
knowledge. More importantly, the trained RL model extend the acquired reasoning
abilities to new kernels. This paradigm opens possibilities for automated
optimization of CUDA operations, and holds promise to substantially promote GPU
efficiency and alleviate the rising pressure on GPU computing resources.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [103] [Adaptive Linguistic Prompting (ALP) Enhances Phishing Webpage Detection in Multimodal Large Language Models](https://arxiv.org/abs/2507.13357)
*Atharva Bhargude,Ishan Gonehal,Chandler Haney,Dave Yoon,Kevin Zhu,Aaron Sandoval,Sean O'Brien,Kaustubh Vinnakota*

Main category: cs.CL

TL;DR: The paper introduces Adaptive Linguistic Prompting (ALP) for phishing detection using multimodal LLMs like GPT-4o and Gemini 1.5 Pro, achieving high accuracy (F1-score: 0.93).


<details>
  <summary>Details</summary>
Motivation: Phishing attacks are a major cybersecurity threat, requiring adaptive detection methods.

Method: ALP uses structured semantic reasoning to guide LLMs in analyzing linguistic patterns, urgency cues, and manipulative diction in phishing content, combining textual, visual, and URL-based analysis.

Result: ALP enhances detection accuracy, outperforming traditional methods with an F1-score of 0.93.

Conclusion: ALP-integrated multimodal LLMs offer a robust, interpretable, and adaptive solution for phishing detection.

Abstract: Phishing attacks represent a significant cybersecurity threat, necessitating
adaptive detection techniques. This study explores few-shot Adaptive Linguistic
Prompting (ALP) in detecting phishing webpages through the multimodal
capabilities of state-of-the-art large language models (LLMs) such as GPT-4o
and Gemini 1.5 Pro. ALP is a structured semantic reasoning method that guides
LLMs to analyze textual deception by breaking down linguistic patterns,
detecting urgency cues, and identifying manipulative diction commonly found in
phishing content. By integrating textual, visual, and URL-based analysis, we
propose a unified model capable of identifying sophisticated phishing attempts.
Our experiments demonstrate that ALP significantly enhances phishing detection
accuracy by guiding LLMs through structured reasoning and contextual analysis.
The findings highlight the potential of ALP-integrated multimodal LLMs to
advance phishing detection frameworks, achieving an F1-score of 0.93,
surpassing traditional approaches. These results establish a foundation for
more robust, interpretable, and adaptive linguistic-based phishing detection
systems using LLMs.

</details>


### [104] [Persona-Based Synthetic Data Generation Using Multi-Stage Conditioning with Large Language Models for Emotion Recognition](https://arxiv.org/abs/2507.13380)
*Keito Inoshita,Rushia Harada*

Main category: cs.CL

TL;DR: PersonaGen is a framework using LLMs to generate diverse emotional text by creating layered virtual personas, addressing the challenge of scarce high-quality emotion datasets.


<details>
  <summary>Details</summary>
Motivation: Emotion recognition lacks diverse datasets due to subjective and context-dependent emotional expressions, making data collection difficult.

Method: PersonaGen uses multi-stage persona-based conditioning with LLMs, combining demographics, socio-cultural backgrounds, and situational contexts to generate emotion-rich text.

Result: PersonaGen outperforms baselines in diversity, coherence, and discriminative power, proving effective for synthetic data generation.

Conclusion: PersonaGen offers a viable solution for augmenting or replacing real-world emotion datasets, enhancing emotion recognition models.

Abstract: In the field of emotion recognition, the development of high-performance
models remains a challenge due to the scarcity of high-quality, diverse
emotional datasets. Emotional expressions are inherently subjective, shaped by
individual personality traits, socio-cultural backgrounds, and contextual
factors, making large-scale, generalizable data collection both ethically and
practically difficult. To address this issue, we introduce PersonaGen, a novel
framework for generating emotionally rich text using a Large Language Model
(LLM) through multi-stage persona-based conditioning. PersonaGen constructs
layered virtual personas by combining demographic attributes, socio-cultural
backgrounds, and detailed situational contexts, which are then used to guide
emotion expression generation. We conduct comprehensive evaluations of the
generated synthetic data, assessing semantic diversity through clustering and
distributional metrics, human-likeness via LLM-based quality scoring, realism
through comparison with real-world emotion corpora, and practical utility in
downstream emotion classification tasks. Experimental results show that
PersonaGen significantly outperforms baseline methods in generating diverse,
coherent, and discriminative emotion expressions, demonstrating its potential
as a robust alternative for augmenting or replacing real-world emotional
datasets.

</details>


### [105] [SAFT: Structure-Aware Fine-Tuning of LLMs for AMR-to-Text Generation](https://arxiv.org/abs/2507.13381)
*Rafiq Kamel,Filippo Guerranti,Simon Geisler,Stephan Günnemann*

Main category: cs.CL

TL;DR: SAFT introduces a structure-aware fine-tuning method for LLMs to improve text generation from graph-structured inputs like AMRs, achieving a 3.5 BLEU improvement.


<details>
  <summary>Details</summary>
Motivation: Current methods for using LLMs with structured inputs like AMRs discard structural cues or require incompatible architectures.

Method: SAFT injects graph topology into LLMs using direction-sensitive positional encodings from the magnetic Laplacian of transformed AMRs, without altering the LLM architecture.

Result: SAFT achieves a 3.5 BLEU improvement on AMR 3.0, with gains scaling with graph complexity.

Conclusion: SAFT provides a general and effective way to enhance LLM performance with structured data, particularly for AMR-to-text generation.

Abstract: Large Language Models (LLMs) are increasingly applied to tasks involving
structured inputs such as graphs. Abstract Meaning Representations (AMRs),
which encode rich semantics as directed graphs, offer a rigorous testbed for
evaluating LLMs on text generation from such structures. Yet, current methods
often arbitrarily linearize AMRs, discarding key structural cues, or rely on
architectures incompatible with standard LLMs. We introduce SAFT, a
structure-aware fine-tuning approach that injects graph topology into
pretrained LLMs without architectural changes. We compute direction-sensitive
positional encodings from the magnetic Laplacian of transformed AMRs and
project them into the embedding space of the LLM. While possibly applicable to
any graph-structured inputs, we focus on AMR-to-text generation as a
representative and challenging benchmark. SAFT sets a new state-of-the-art on
AMR 3.0 with a 3.5 BLEU improvement over baselines. Gains scale with graph
complexity, highlighting the value of structure-aware representations in
enhancing LLM performance. SAFT offers a general and effective pathway for
bridging structured data and language models.

</details>


### [106] [Context-Based Fake News Detection using Graph Based Approach: ACOVID-19 Use-case](https://arxiv.org/abs/2507.13382)
*Chandrashekar Muniyappa,Sirisha Velampalli*

Main category: cs.CL

TL;DR: A graph-based approach using NLP and MDL-based GBAD algorithm is proposed to detect fake news by identifying anomalous patterns in contextual graph structures.


<details>
  <summary>Details</summary>
Motivation: The rapid spread of fake news in the digital world necessitates effective detection methods.

Method: News articles are transformed into contextual graphs using NLP, and anomalies are detected using the MDL-based GBAD algorithm.

Result: The approach identifies normative and anomalous patterns, enhancing fake news detection.

Conclusion: The proposed graph-based method effectively detects fake news by leveraging contextual data and anomaly detection.

Abstract: In today\'s digital world, fake news is spreading with immense speed. Its a
significant concern to address. In this work, we addressed that challenge using
novel graph based approach. We took dataset from Kaggle that contains real and
fake news articles. To test our approach we incorporated recent covid-19
related news articles that contains both genuine and fake news that are
relevant to this problem. This further enhances the dataset as well instead of
relying completely on the original dataset. We propose a contextual graph-based
approach to detect fake news articles. We need to convert news articles into
appropriate schema, so we leverage Natural Language Processing (NLP) techniques
to transform news articles into contextual graph structures. We then apply the
Minimum Description Length (MDL)-based Graph-Based Anomaly Detection (GBAD)
algorithm for graph mining. Graph-based methods are particularly effective for
handling rich contextual data, as they enable the discovery of complex patterns
that traditional query-based or statistical techniques might overlook. Our
proposed approach identifies normative patterns within the dataset and
subsequently uncovers anomalous patterns that deviate from these established
norms.

</details>


### [107] [PARAM-1 BharatGen 2.9B Model](https://arxiv.org/abs/2507.13390)
*Kundeshwar Pundalik,Piyush Sawarkar,Nihar Sahoo,Abhishek Shinde,Prateek Chanda,Vedant Goswami,Ajay Nagpal,Atul Singh,Viraj Thakur,Vijay Dewane,Aamod Thakur,Bhargav Patel,Smita Gautam,Bhagwan Panditi,Shyam Pawar,Madhav Kotcha,Suraj Racha,Saral Sureka,Pankaj Singh,Rishi Bal,Rohit Saluja,Ganesh Ramakrishnan*

Main category: cs.CL

TL;DR: PARAM-1 is a 2.9B parameter LLM designed for Indian linguistic diversity, trained on Hindi and English with equitable representation, fair tokenization, and culturally aligned evaluation.


<details>
  <summary>Details</summary>
Motivation: Address the under-representation of linguistically diverse regions like India in LLMs, which are dominated by English-centric designs.

Method: Trained on a bilingual Hindi-English dataset with 25% Indic language allocation, adapted tokenizer for Indian morphology, and culturally aligned benchmarks.

Result: PARAM-1 serves as a competent general-purpose model and robust baseline for India-centric applications.

Conclusion: PARAM-1 provides a design-first blueprint for equitable foundation modeling by embedding diversity at the pretraining level.

Abstract: Large Language Models (LLMs) have emerged as powerful general-purpose
reasoning systems, yet their development remains dominated by English-centric
data, architectures, and optimization paradigms. This exclusionary design
results in structural under-representation of linguistically diverse regions
such as India, where over 20 official languages and 100+ dialects coexist
alongside phenomena like code-switching and diglossia. We introduce PARAM-1, a
2.9B parameter decoder-only, text-only language model trained from scratch with
an explicit architectural and linguistic focus on Indian diversity. PARAM-1 is
trained on a bilingual dataset consisting of only Hindi and English,
constructed with a strong focus on fact-rich, high-quality content. It is
guided by three core principles: equitable representation of Indic languages
through a 25% corpus allocation; tokenization fairness via a SentencePiece
tokenizer adapted to Indian morphological structures; and culturally aligned
evaluation benchmarks across IndicQA, code-mixed reasoning, and
socio-linguistic robustness tasks. By embedding diversity at the pretraining
level-rather than deferring it to post-hoc alignment-PARAM-1 offers a
design-first blueprint for equitable foundation modeling. Our results
demonstrate that it serves as both a competent general-purpose model and a
robust baseline for India-centric applications.

</details>


### [108] [TopicImpact: Improving Customer Feedback Analysis with Opinion Units for Topic Modeling and Star-Rating Prediction](https://arxiv.org/abs/2507.13392)
*Emil Häglund,Johanna Björklund*

Main category: cs.CL

TL;DR: The paper improves topic modeling by using opinion units (text excerpts with sentiment scores) extracted via large language models, enhancing topic coherence and interpretability while linking topics and sentiments to business metrics like star ratings.


<details>
  <summary>Details</summary>
Motivation: To enhance insight extraction from customer reviews by restructuring the topic modeling pipeline to focus on opinion units, improving topic coherence and sentiment integration.

Method: Restructures topic modeling to operate on opinion units (text excerpts with sentiment scores) extracted using large language models, then correlates topics and sentiments with business metrics.

Result: Improved topic modeling performance, producing coherent and interpretable topics with associated sentiments, and enabling insights into business outcomes.

Conclusion: The system effectively integrates topic and sentiment analysis, offering advantages over traditional methods and demonstrating practical use cases for business insights.

Abstract: We improve the extraction of insights from customer reviews by restructuring
the topic modelling pipeline to operate on opinion units - distinct statements
that include relevant text excerpts and associated sentiment scores. Prior work
has demonstrated that such units can be reliably extracted using large language
models. The result is a heightened performance of the subsequent topic
modeling, leading to coherent and interpretable topics while also capturing the
sentiment associated with each topic. By correlating the topics and sentiments
with business metrics, such as star ratings, we can gain insights on how
specific customer concerns impact business outcomes. We present our system's
implementation, use cases, and advantages over other topic modeling and
classification solutions. We also evaluate its effectiveness in creating
coherent topics and assess methods for integrating topic and sentiment
modalities for accurate star-rating prediction.

</details>


### [109] [Mitigating Stylistic Biases of Machine Translation Systems via Monolingual Corpora Only](https://arxiv.org/abs/2507.13395)
*Xuanqi Gao,Weipeng Jiang,Juan Zhai,Shiqing Ma,Siyi Xie,Xinyang Yin,Chao Shen*

Main category: cs.CL

TL;DR: Babel is a framework for enhancing stylistic fidelity in NMT using monolingual corpora, achieving high precision in style detection and significant improvements in style preservation.


<details>
  <summary>Details</summary>
Motivation: Preserving stylistic nuances in NMT is challenging, especially without parallel corpora. Babel addresses this by leveraging monolingual data.

Method: Babel uses a style detector and a diffusion-based style applicator to refine translations, integrating as a post-processing module with existing NMT systems.

Result: Babel achieves 88.21% precision in style detection, improves style preservation by 150%, and maintains a semantic similarity score of 0.92.

Conclusion: Babel effectively enhances stylistic fidelity in NMT without requiring parallel data or system modifications, validated by human evaluation.

Abstract: The advent of neural machine translation (NMT) has revolutionized
cross-lingual communication, yet preserving stylistic nuances remains a
significant challenge. While existing approaches often require parallel corpora
for style preservation, we introduce Babel, a novel framework that enhances
stylistic fidelity in NMT using only monolingual corpora. Babel employs two key
components: (1) a style detector based on contextual embeddings that identifies
stylistic disparities between source and target texts, and (2) a
diffusion-based style applicator that rectifies stylistic inconsistencies while
maintaining semantic integrity. Our framework integrates with existing NMT
systems as a post-processing module, enabling style-aware translation without
requiring architectural modifications or parallel stylistic data. Extensive
experiments on five diverse domains (law, literature, scientific writing,
medicine, and educational content) demonstrate Babel's effectiveness: it
identifies stylistic inconsistencies with 88.21% precision and improves
stylistic preservation by 150% while maintaining a high semantic similarity
score of 0.92. Human evaluation confirms that translations refined by Babel
better preserve source text style while maintaining fluency and adequacy.

</details>


### [110] [Causal Language Control in Multilingual Transformers via Sparse Feature Steering](https://arxiv.org/abs/2507.13410)
*Cheng-Ting Chou,George Liu,Jessica Sun,Cole Blondin,Kevin Zhu,Vasu Sharma,Sean O'Brien*

Main category: cs.CL

TL;DR: Sparse autoencoder (SAE) features are used to control the target language of multilingual LLMs in zero-shot settings, achieving up to 90% success by modifying a single feature.


<details>
  <summary>Details</summary>
Motivation: Controlling the target language of multilingual LLMs without explicit prompts or fine-tuning is challenging.

Method: Identify SAE features with differing activations between languages and modify them to steer language output.

Result: Up to 90% success in controlled language shifts while preserving semantic fidelity.

Conclusion: Sparse feature steering is a lightweight, interpretable method for multilingual generation control.

Abstract: Deterministically controlling the target generation language of large
multilingual language models (LLMs) remains a fundamental challenge,
particularly in zero-shot settings where neither explicit language prompts nor
fine-tuning are available. In this work, we investigate whether sparse
autoencoder (SAE) features, previously shown to correlate with interpretable
model behaviors, can be leveraged to steer the generated language of LLMs
during inference. Leveraging pretrained SAEs on the residual streams of
Gemma-2B and Gemma-9B, we identify features whose activations differ most
significantly between English and four target languages: Chinese, Japanese,
Spanish, and French. By modifying just a single SAE feature at one transformer
layer, we achieve controlled language shifts with up to 90\% success, as
measured by FastText language classification, while preserving semantic
fidelity according to LaBSE (Language-Agnostic BERT Sentence Embedding)
similarity. Our analysis reveals that language steering is most effective in
mid-to-late transformer layers and is amplified by specific attention heads
disproportionately associated with language-sensitive SAE features. These
results demonstrate the promise of sparse feature steering as a lightweight and
interpretable mechanism for controllable multilingual generation.

</details>


### [111] [Aligning Knowledge Graphs and Language Models for Factual Accuracy](https://arxiv.org/abs/2507.13411)
*Nur A Zarin Nishat,Andrea Coletta,Luigi Bellomarini,Kossi Amouzouvi,Jens Lehmann,Sahar Vahdati*

Main category: cs.CL

TL;DR: ALIGNed-LLM integrates Knowledge Graphs (KGs) into language models to reduce hallucination by aligning entity and text embeddings, improving factuality in tasks like question answering.


<details>
  <summary>Details</summary>
Motivation: Large language models (LLMs) like GPT-4 struggle with hallucination. Integrating structured, reliable KGs can enhance their factual grounding.

Method: ALIGNed-LLM aligns KG embeddings (e.g., TransE) with text embeddings using a trainable projection layer, inspired by LLaVA's multimodal approach.

Result: Tested on QA benchmarks and a financial use case, ALIGNed-LLM significantly improved LLM factuality and reduced hallucination.

Conclusion: ALIGNed-LLM effectively enhances LLM factuality by leveraging KGs, demonstrating practical utility in high-stakes domains like finance.

Abstract: Large language models like GPT-4, Gemini, and Claude have transformed natural
language processing (NLP) tasks such as question answering, dialogue
generation, summarization, and so forth; yet their susceptibility to
hallucination stands as one of the major challenges. Among numerous approaches
to overcome this challenge, integration of Knowledge Graphs (KGs) into language
models has emerged as a promising solution as it provides structured, reliable,
domain-specific, and up-to-date external information to the language models. In
this paper, we introduce ALIGNed-LLM, a simple yet effective approach to
improve language models' factuality via a lean strategy to infuse KGs into the
latent space of language models inspired by LLaVA where visual and textual
information is infused. We use embeddings from a pre-trained Knowledge Graph
Embedding (KGE) model, such as TransE, and a trainable projection layer to
align entity and text embeddings. This alignment enables the language model to
distinguish between similar entities improving factual grounding and reducing
hallucination. We tested our approach on three popular questions-answering
benchmark datasets alongside language models of varying sizes, showing
significant improvement. Furthermore, we applied our approach to a real-world
financial use case from a large central bank in Europe, which demands high
accuracy and precision, demonstrating a substantial improvement of the LLM
answers.

</details>


### [112] [Paper Summary Attack: Jailbreaking LLMs through LLM Safety Papers](https://arxiv.org/abs/2507.13474)
*Liang Lin,Zhihao Xu,Xuehai Tang,Shi Liu,Biyu Zhou,Fuqing Zhu,Jizhong Han,Songlin Hu*

Main category: cs.CL

TL;DR: The paper introduces a novel jailbreaking method, Paper Summary Attack (PSA), exploiting LLMs' trust in authoritative sources to achieve high attack success rates on aligned models.


<details>
  <summary>Details</summary>
Motivation: To investigate vulnerabilities in LLMs due to their propensity to trust authoritative sources like academic papers.

Method: PSA synthesizes content from safety papers to create adversarial prompts, infilling harmful queries into predefined subsections.

Result: PSA achieves 97-98% attack success rates on models like Claude3.5-Sonnet and Deepseek-R1, revealing model-specific vulnerability biases.

Conclusion: The findings highlight significant vulnerabilities in LLMs and suggest future research directions for adversarial and safety alignment methods.

Abstract: The safety of large language models (LLMs) has garnered significant research
attention. In this paper, we argue that previous empirical studies demonstrate
LLMs exhibit a propensity to trust information from authoritative sources, such
as academic papers, implying new possible vulnerabilities. To verify this
possibility, a preliminary analysis is designed to illustrate our two findings.
Based on this insight, a novel jailbreaking method, Paper Summary Attack
(\llmname{PSA}), is proposed. It systematically synthesizes content from either
attack-focused or defense-focused LLM safety paper to construct an adversarial
prompt template, while strategically infilling harmful query as adversarial
payloads within predefined subsections. Extensive experiments show significant
vulnerabilities not only in base LLMs, but also in state-of-the-art reasoning
model like Deepseek-R1. PSA achieves a 97\% attack success rate (ASR) on
well-aligned models like Claude3.5-Sonnet and an even higher 98\% ASR on
Deepseek-R1. More intriguingly, our work has further revealed diametrically
opposed vulnerability bias across different base models, and even between
different versions of the same model, when exposed to either attack-focused or
defense-focused papers. This phenomenon potentially indicates future research
clues for both adversarial methodologies and safety alignment.Code is available
at https://github.com/233liang/Paper-Summary-Attack

</details>


### [113] [Revisiting LLM Value Probing Strategies: Are They Robust and Expressive?](https://arxiv.org/abs/2507.13490)
*Siqi Shen,Mehar Singh,Lajanugen Logeswaran,Moontae Lee,Honglak Lee,Rada Mihalcea*

Main category: cs.CL

TL;DR: The paper evaluates the robustness and expressiveness of value representations in LLMs, highlighting inconsistencies under input perturbations and weak alignment with real-world actions.


<details>
  <summary>Details</summary>
Motivation: To address gaps in systematic comparison of probing methods for LLM values and their real-world applicability.

Method: Evaluates three probing strategies using prompt and option variations, and introduces tasks to test demographic context responsiveness and value-action alignment.

Result: Probing methods show large variances under perturbations; demographic context has little effect, and values weakly correlate with real-world actions.

Conclusion: Highlights the need for more careful examination of LLM value probing and awareness of its limitations.

Abstract: There has been extensive research on assessing the value orientation of Large
Language Models (LLMs) as it can shape user experiences across demographic
groups. However, several challenges remain. First, while the Multiple Choice
Question (MCQ) setting has been shown to be vulnerable to perturbations, there
is no systematic comparison of probing methods for value probing. Second, it is
unclear to what extent the probed values capture in-context information and
reflect models' preferences for real-world actions. In this paper, we evaluate
the robustness and expressiveness of value representations across three widely
used probing strategies. We use variations in prompts and options, showing that
all methods exhibit large variances under input perturbations. We also
introduce two tasks studying whether the values are responsive to demographic
context, and how well they align with the models' behaviors in value-related
scenarios. We show that the demographic context has little effect on the
free-text generation, and the models' values only weakly correlate with their
preference for value-based actions. Our work highlights the need for a more
careful examination of LLM value probing and awareness of its limitations.

</details>


### [114] [Encoding syntactic objects and Merge operations in function spaces](https://arxiv.org/abs/2507.13501)
*Matilde Marcolli,Robert C. Berwick*

Main category: cs.CL

TL;DR: The paper demonstrates a mathematical framework for representing syntactic objects in a function space, compatible with neurocomputational models, using wavelet-like functions and operad algebra.


<details>
  <summary>Details</summary>
Motivation: To theoretically justify the neurocomputational feasibility of syntactic structures by embedding them in a function space with algebraic properties.

Method: Constructs a faithful representation of syntactic objects using wavelet functions, operad algebra, and a semiring structure based on Renyi entropy. Merge operations are modeled via Hopf algebra Markov chains.

Result: Shows the theoretical possibility of neurocomputational realization of syntax, with Merge operations expressed as cross-frequency phase synchronization.

Conclusion: The framework provides a constructive argument for neurocomputational syntax and clarifies Merge's similarity to arithmetic successor functions.

Abstract: We provide a mathematical argument showing that, given a representation of
lexical items as functions (wavelets, for instance) in some function space, it
is possible to construct a faithful representation of arbitrary syntactic
objects in the same function space. This space can be endowed with a
commutative non-associative semiring structure built using the second Renyi
entropy. The resulting representation of syntactic objects is compatible with
the magma structure. The resulting set of functions is an algebra over an
operad, where the operations in the operad model circuits that transform the
input wave forms into a combined output that encodes the syntactic structure.
The action of Merge on workspaces is faithfully implemented as action on these
circuits, through a coproduct and a Hopf algebra Markov chain. The results
obtained here provide a constructive argument showing the theoretical
possibility of a neurocomputational realization of the core computational
structure of syntax. We also present a particular case of this general
construction where this type of realization of Merge is implemented as a cross
frequency phase synchronization on sinusoidal waves. This also shows that Merge
can be expressed in terms of the successor function of a semiring, thus
clarifying the well known observation of its similarities with the successor
function of arithmetic.

</details>


### [115] [A Computational Approach to Modeling Conversational Systems: Analyzing Large-Scale Quasi-Patterned Dialogue Flows](https://arxiv.org/abs/2507.13544)
*Mohamed Achref Ben Ammar,Mohamed Taha Bennani*

Main category: cs.CL

TL;DR: A novel framework for analyzing conversational dynamics using graph simplification (Filter & Reconnect) improves semantic metrics and structural clarity in dialogue modeling.


<details>
  <summary>Details</summary>
Motivation: The rise of large language model-based systems necessitates better tools for analyzing loosely organized dialogues (quasi-patterned conversations).

Method: Proposes the Filter & Reconnect method to simplify conversational graphs, preserving semantic coherence and structural integrity.

Result: Semantic metric S increased by 2.06x, with tree-like structure (0 δ-hyperbolicity) for clearer conversation modeling.

Conclusion: The framework aids in analyzing large-scale dialogue datasets, benefiting chatbots, dialogue tools, and user behavior analytics.

Abstract: The analysis of conversational dynamics has gained increasing importance with
the rise of large language model-based systems, which interact with users
across diverse contexts. In this work, we propose a novel computational
framework for constructing conversational graphs that capture the flow and
structure of loosely organized dialogues, referred to as quasi-patterned
conversations. We introduce the Filter & Reconnect method, a novel graph
simplification technique that minimizes noise while preserving semantic
coherence and structural integrity of conversational graphs. Through
comparative analysis, we demonstrate that the use of large language models
combined with our graph simplification technique has resulted in semantic
metric S increasing by a factor of 2.06 compared to previous approaches while
simultaneously enforcing a tree-like structure with 0 {\delta}-hyperbolicity,
ensuring optimal clarity in conversation modeling. This work provides a
computational method for analyzing large-scale dialogue datasets, with
practical applications related to monitoring automated systems such as
chatbots, dialogue management tools, and user behavior analytics.

</details>


### [116] [Reading Between the Lines: Combining Pause Dynamics and Semantic Coherence for Automated Assessment of Thought Disorder](https://arxiv.org/abs/2507.13551)
*Feng Chen,Weizhe Xu,Changye Li,Serguei Pakhomov,Alex Cohen,Simran Bhola,Sandy Yin,Sunny X Tang,Michael Mackinley,Lena Palaniyappan,Dror Ben-Zeev,Trevor Cohen*

Main category: cs.CL

TL;DR: Automated speech analysis using pause features and semantic coherence metrics improves prediction of Formal Thought Disorder (FTD) severity in schizophrenia, outperforming semantic-only models.


<details>
  <summary>Details</summary>
Motivation: Traditional clinical rating scales for FTD are resource-intensive and lack scalability. Automated speech analysis offers a scalable, objective alternative.

Method: Pause features from ASR and semantic coherence metrics were integrated across three datasets (AVH, TOPSY, PsyCL). Support vector regression (SVR) predicted FTD scores.

Result: Pause features alone predicted FTD severity well. Combining pause and semantic features enhanced performance (correlations up to 0.649, AUC 83.71%).

Conclusion: Integrating temporal (pause) and semantic analyses improves FTD assessment, advancing automated speech analysis in psychosis.

Abstract: Formal thought disorder (FTD), a hallmark of schizophrenia spectrum
disorders, manifests as incoherent speech and poses challenges for clinical
assessment. Traditional clinical rating scales, though validated, are
resource-intensive and lack scalability. Automated speech analysis with
automatic speech recognition (ASR) allows for objective quantification of
linguistic and temporal features of speech, offering scalable alternatives. The
use of utterance timestamps in ASR captures pause dynamics, which are thought
to reflect the cognitive processes underlying speech production. However, the
utility of integrating these ASR-derived features for assessing FTD severity
requires further evaluation. This study integrates pause features with semantic
coherence metrics across three datasets: naturalistic self-recorded diaries
(AVH, n = 140), structured picture descriptions (TOPSY, n = 72), and dream
narratives (PsyCL, n = 43). We evaluated pause related features alongside
established coherence measures, using support vector regression (SVR) to
predict clinical FTD scores. Key findings demonstrate that pause features alone
robustly predict the severity of FTD. Integrating pause features with semantic
coherence metrics enhanced predictive performance compared to semantic-only
models, with integration of independent models achieving correlations up to
\r{ho} = 0.649 and AUC = 83.71% for severe cases detection (TOPSY, with best
\r{ho} = 0.584 and AUC = 79.23% for semantic-only models). The performance
gains from semantic and pause features integration held consistently across all
contexts, though the nature of pause patterns was dataset-dependent. These
findings suggest that frameworks combining temporal and semantic analyses
provide a roadmap for refining the assessment of disorganized speech and
advance automated speech analysis in psychosis.

</details>


### [117] [A Data-Centric Framework for Addressing Phonetic and Prosodic Challenges in Russian Speech Generative Models](https://arxiv.org/abs/2507.13563)
*Kirill Borodin,Nikita Vasiliev,Vasiliy Kudryavtsev,Maxim Maslov,Mikhail Gorodnichev,Oleg Rogov,Grach Mkrtchian*

Main category: cs.CL

TL;DR: Balalaika, a new 2,000-hour Russian speech dataset with detailed annotations, improves speech synthesis and enhancement models.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in Russian speech synthesis like vowel reduction and unnatural intonation.

Method: Created a high-quality dataset with comprehensive annotations (punctuation, stress markings) and detailed its construction pipeline.

Result: Models trained on Balalaika outperformed those using existing datasets.

Conclusion: Balalaika is a valuable resource for improving Russian speech synthesis and enhancement.

Abstract: Russian speech synthesis presents distinctive challenges, including vowel
reduction, consonant devoicing, variable stress patterns, homograph ambiguity,
and unnatural intonation. This paper introduces Balalaika, a novel dataset
comprising more than 2,000 hours of studio-quality Russian speech with
comprehensive textual annotations, including punctuation and stress markings.
Experimental results show that models trained on Balalaika significantly
outperform those trained on existing datasets in both speech synthesis and
enhancement tasks. We detail the dataset construction pipeline, annotation
methodology, and results of comparative evaluations.

</details>


### [118] [Linguistic and Embedding-Based Profiling of Texts generated by Humans and Large Language Models](https://arxiv.org/abs/2507.13614)
*Sergio E. Zanotto,Segun Aroyehun*

Main category: cs.CL

TL;DR: The study analyzes linguistic features (morphology, syntax, semantics) to distinguish human-written and machine-generated texts, revealing simpler syntax and diverse semantics in human texts, and homogenization in newer LLMs.


<details>
  <summary>Details</summary>
Motivation: To characterize human-written and machine-generated texts using linguistic features across multiple levels, beyond binary classification.

Method: Analyzed a dataset of texts from 8 domains and 11 LLMs, calculating features like dependency length and emotionality. Statistical analysis and style embeddings were applied.

Result: Human texts show simpler syntax and more semantic diversity. Newer LLMs produce homogenized outputs. Both exhibit stylistic diversity across domains, with humans showing greater variation.

Conclusion: Linguistic features effectively characterize text origins, with newer LLMs converging in style, reducing variability.

Abstract: The rapid advancements in large language models (LLMs) have significantly
improved their ability to generate natural language, making texts generated by
LLMs increasingly indistinguishable from human-written texts. While recent
research has primarily focused on using LLMs to classify text as either
human-written and machine-generated texts, our study focus on characterizing
these texts using a set of linguistic features across different linguistic
levels such as morphology, syntax, and semantics. We select a dataset of
human-written and machine-generated texts spanning 8 domains and produced by 11
different LLMs. We calculate different linguistic features such as dependency
length and emotionality and we use them for characterizing human-written and
machine-generated texts along with different sampling strategies, repetition
controls and model release date. Our statistical analysis reveals that
human-written texts tend to exhibit simpler syntactic structures and more
diverse semantic content. Furthermore, we calculate the variability of our set
of features across models and domains. Both human and machine texts show
stylistic diversity across domains, with humans displaying greater variation in
our features. Finally, we apply style embeddings to further test variability
among human-written and machine-generated texts. Notably, newer models output
text that is similarly variable, pointing to an homogenization of
machine-generated texts.

</details>


### [119] [Seed-X: Building Strong Multilingual Translation LLM with 7B Parameters](https://arxiv.org/abs/2507.13618)
*Shanbo Cheng,Yu Bao,Qian Cao,Luyang Huang,Liyan Kang,Zhicheng Liu,Yu Lu,Wenhao Zhu,Zhichao Huang,Tao Li,Sitong Liu,Ningxin Peng,Shuaijie She,Lu Xu,Nuo Xu,Sen Yang,Runsheng Yu,Yiming Yu,Liehao Zou,Hang Li,Lu Lu,Yuxuan Wang,Yonghui Wu*

Main category: cs.CL

TL;DR: Seed-X, a family of open-source LLMs, achieves competitive multilingual translation performance with 7B parameters, outperforming larger models and matching closed-source leaders like GPT-4o.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of intricate language patterns and stilted translations in automated multilingual translation.

Method: Pre-training on diverse, high-quality multilingual data, finetuning with Chain-of-Thought reasoning, and enhancing via reinforcement learning.

Result: Competes with Gemini-2.5 and GPT-4o, outperforms larger open-source models in metrics and human evaluations.

Conclusion: Seed-X advances multilingual translation research and applications, with publicly shared parameters and best practices.

Abstract: Multilingual translation stands as a challenging task for large language
models (LLMs) to handle intricate language patterns and stilted translations
that arise in automated translations. In this paper, we introduce Seed-X, a
family of open-source LLMs comprising instruct and reasoning models, pushing
the limits of translation capability with 7B parameter size. The base model is
pre-trained on a diverse, high-quality dataset encompassing both monolingual
and bilingual content across 28 languages, harnessing the full potential of
multilingual data. The instruct model is then finetuned to translate by
Chain-of-Thought (CoT) reasoning and further enhanced through reinforcement
learning (RL) to achieve better generalization across diverse language pairs.
Seed-X achieves performance comparable to leading closed-source models,
including Gemini-2.5 and GPT-4o, across 28 languages, and significantly
outperforms larger open-source models in both automatic metrics and human
evaluations. We share the best practices through our optimization process, and
make the parameter public available for advancing translation research and
applications.

</details>


### [120] [CU-ICU: Customizing Unsupervised Instruction-Finetuned Language Models for ICU Datasets via Text-to-Text Transfer Transformer](https://arxiv.org/abs/2507.13655)
*Teerapong Panboonyuen*

Main category: cs.CL

TL;DR: CU-ICU is a method for adapting unsupervised language models to ICU datasets using T5 architecture, improving accuracy and interpretability with minimal supervision.


<details>
  <summary>Details</summary>
Motivation: Challenges in integrating large language models into healthcare, such as domain adaptation and limited labeled data, drive the need for efficient customization methods.

Method: CU-ICU uses sparse fine-tuning with few-shot prompting and selective parameter updates on the T5 architecture.

Result: CU-ICU achieves up to 15% higher sepsis detection accuracy and 20% better clinical note interpretability while updating <1% of parameters.

Conclusion: CU-ICU is a scalable, efficient solution for accurate and interpretable clinical decision support in ICUs.

Abstract: Integrating large language models into specialized domains like healthcare
presents unique challenges, including domain adaptation and limited labeled
data. We introduce CU-ICU, a method for customizing unsupervised
instruction-finetuned language models for ICU datasets by leveraging the
Text-to-Text Transfer Transformer (T5) architecture. CU-ICU employs a sparse
fine-tuning approach that combines few-shot prompting with selective parameter
updates, enabling efficient adaptation with minimal supervision. Our evaluation
across critical ICU tasks--early sepsis detection, mortality prediction, and
clinical note generation--demonstrates that CU-ICU consistently improves
predictive accuracy and interpretability over standard fine-tuning methods.
Notably, CU-ICU achieves up to a 15% increase in sepsis detection accuracy and
a 20% enhancement in generating clinically relevant explanations while updating
fewer than 1% of model parameters in its most efficient configuration. These
results establish CU-ICU as a scalable, low-overhead solution for delivering
accurate and interpretable clinical decision support in real-world ICU
environments.

</details>


### [121] [KiC: Keyword-inspired Cascade for Cost-Efficient Text Generation with LLMs](https://arxiv.org/abs/2507.13666)
*Woo-Chan Kim,Ji-Hoon Park,Seong-Whan Lee*

Main category: cs.CL

TL;DR: KiC is a cost-efficient cascade framework for free-form text generation, using semantic alignment to decide when to escalate to stronger models, achieving near-GPT-4 accuracy with reduced costs.


<details>
  <summary>Details</summary>
Motivation: High-performing LLMs are costly via APIs; existing cascade methods fail to reliably assess free-form outputs due to reliance on exact text matching.

Method: KiC identifies a representative answer from a weaker model and evaluates semantic alignment of other responses to decide escalation.

Result: KiC achieves 97.53% of GPT-4's accuracy, reduces costs by 28.81%, and outperforms GPT-4 in one benchmark.

Conclusion: KiC offers a cost-effective solution for free-form text generation by leveraging semantic alignment in cascades.

Abstract: Large language models (LLMs) have demonstrated state-of-the-art performance
across a wide range of natural language processing tasks. However,
high-performing models are typically accessible only via APIs, incurring
substantial inference costs. Cascade methods address this by initially
employing a cheaper model and escalating to a stronger one only when necessary.
Nevertheless, existing cascade approaches struggle to select a reliable
representative response and assess the overall reliability of free-form
outputs, as they rely on exact text matching. To overcome these limitations, we
propose Keyword-inspired Cascade (KiC), a novel framework for cost-efficient
free-form text generation. KiC identifies the most representative answer among
multiple outputs from a weaker model and evaluates the semantic alignment of
other responses with it. Based on the degree of alignment, KiC determines
whether to accept the weaker model's output or escalate to a stronger model.
Experiments on three free-form text generation benchmarks show that KiC
achieves 97.53 percent of GPT-4's accuracy while reducing API costs by 28.81
percent on average, and even outperforms GPT-4 in a specific benchmark.

</details>


### [122] [LoopServe: An Adaptive Dual-phase LLM Inference Acceleration System for Multi-Turn Dialogues](https://arxiv.org/abs/2507.13681)
*Haoyang Li,Zhanchao Xu,Yiming Li,Xuejia Chen,Darian Li,Anxin Tian,Qingfa Xiao,Cheng Deng,Jun Wang,Qing Li,Lei Chen,Mingxuan Yuan*

Main category: cs.CL

TL;DR: LoopServe is an adaptive dual-phase framework for accelerating large language models in multi-turn dialogues, using dynamic sparsification and progressive key value compression.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with computational and memory challenges in long multi-turn dialogues due to fixed heuristics.

Method: LoopServe dynamically sparsifies attention matrices during prefilling and compresses key values adaptively during decoding.

Result: LoopServe outperforms baselines, accelerating inference effectively in long-context dialogue tasks.

Conclusion: LoopServe offers a robust solution for efficient and responsive multi-turn dialogue interactions.

Abstract: Multi-turn dialogues are essential in many real-world applications of large
language models, such as chatbots and virtual assistants. As conversation
histories become longer, existing large language models face increasing
computational and memory challenges, which hinder their ability to provide
efficient and responsive interactions. Most current acceleration methods either
compress the context or optimize key value caching, but they often rely on
fixed or position-based heuristics that do not adapt well to the dynamic and
unpredictable patterns found in actual multi-turn conversations. In this paper,
we present LoopServe, an adaptive dual-phase inference acceleration framework
for large language models in multi-turn dialogues. LoopServe introduces two
main innovations. First, it performs online sparsification during the
prefilling phase by dynamically selecting the most important parts of the
attention matrix for each new input. Second, it uses progressive key value
compression during decoding by adaptively maintaining a relevant and efficient
cache based on the most recently generated output tokens. We also propose a
\href{https://huggingface.co/datasets/TreeAILab/Multi-turn_Long-context_Benchmark_for_LLMs}{new
benchmark} with eleven multi-turn datasets that reflect realistic query
positions and conversational dependencies. Extensive experiments demonstrate
that LoopServe consistently achieves superior effectiveness compared to
existing baselines and significantly accelerates LLM inference across a wide
range of long-context dialogue tasks.

</details>


### [123] [Consistent Explainers or Unreliable Narrators? Understanding LLM-generated Group Recommendations](https://arxiv.org/abs/2507.13705)
*Cedric Waterschoot,Nava Tintarev,Francesco Barile*

Main category: cs.CL

TL;DR: LLMs in Group Recommender Systems (GRS) produce recommendations similar to Additive Utilitarian (ADD) aggregation but with inconsistent explanations, raising transparency concerns.


<details>
  <summary>Details</summary>
Motivation: Evaluate LLMs as joint decision-makers and explanation generators in GRS, comparing them to social choice-based aggregation strategies.

Method: Compare LLM-generated recommendations and explanations to ADD aggregation and analyze group structure impact.

Result: LLM recommendations resemble ADD, but explanations are inconsistent and introduce extra criteria. Group structure doesn't affect recommendations.

Conclusion: LLMs in GRS may undermine transparency due to inconsistent explanations, suggesting inefficiency in standard aggregation for larger datasets.

Abstract: Large Language Models (LLMs) are increasingly being implemented as joint
decision-makers and explanation generators for Group Recommender Systems (GRS).
In this paper, we evaluate these recommendations and explanations by comparing
them to social choice-based aggregation strategies. Our results indicate that
LLM-generated recommendations often resembled those produced by Additive
Utilitarian (ADD) aggregation. However, the explanations typically referred to
averaging ratings (resembling but not identical to ADD aggregation). Group
structure, uniform or divergent, did not impact the recommendations.
Furthermore, LLMs regularly claimed additional criteria such as user or item
similarity, diversity, or used undefined popularity metrics or thresholds. Our
findings have important implications for LLMs in the GRS pipeline as well as
standard aggregation strategies. Additional criteria in explanations were
dependent on the number of ratings in the group scenario, indicating potential
inefficiency of standard aggregation methods at larger item set sizes.
Additionally, inconsistent and ambiguous explanations undermine transparency
and explainability, which are key motivations behind the use of LLMs for GRS.

</details>


### [124] [The Judge Variable: Challenging Judge-Agnostic Legal Judgment Prediction](https://arxiv.org/abs/2507.13732)
*Guillaume Zambrano*

Main category: cs.CL

TL;DR: The study uses ML to predict child custody outcomes in French courts, showing judges' individual patterns influence decisions, supporting legal realism. Specialist models outperform generalist ones.


<details>
  <summary>Details</summary>
Motivation: To challenge the assumption of judicial neutrality by examining if individual judges' patterns affect custody outcomes.

Method: Analyzed 18,937 rulings using hybrid ML (LLMs for feature extraction, RF/XGB/SVC for prediction), comparing specialist (judge-specific) and generalist models.

Result: Specialist models (F1 up to 92.85%) outperformed generalist models (82.63%), showing judges' unique decision patterns.

Conclusion: Judicial identity impacts outcomes, supporting legal realism. Data and code will be shared.

Abstract: This study examines the role of human judges in legal decision-making by
using machine learning to predict child physical custody outcomes in French
appellate courts. Building on the legal realism-formalism debate, we test
whether individual judges' decision-making patterns significantly influence
case outcomes, challenging the assumption that judges are neutral variables
that apply the law uniformly. To ensure compliance with French privacy laws, we
implement a strict pseudonymization process. Our analysis uses 18,937 living
arrangements rulings extracted from 10,306 cases. We compare models trained on
individual judges' past rulings (specialist models) with a judge-agnostic model
trained on aggregated data (generalist models). The prediction pipeline is a
hybrid approach combining large language models (LLMs) for structured feature
extraction and ML models for outcome prediction (RF, XGB and SVC). Our results
show that specialist models consistently achieve higher predictive accuracy
than the general model, with top-performing models reaching F1 scores as high
as 92.85%, compared to the generalist model's 82.63% trained on 20x to 100x
more samples. Specialist models capture stable individual patterns that are not
transferable to other judges. In-Domain and Cross-Domain validity tests provide
empirical support for legal realism, demonstrating that judicial identity plays
a measurable role in legal outcomes. All data and code used will be made
available.

</details>


### [125] [PRIDE -- Parameter-Efficient Reduction of Identity Discrimination for Equality in LLMs](https://arxiv.org/abs/2507.13743)
*Maluna Menke,Thilo Hagendorff*

Main category: cs.CL

TL;DR: The paper evaluates LoRA and soft-prompt tuning to reduce LGBTQIA+ biases in LLMs, finding LoRA effective with minimal computational cost.


<details>
  <summary>Details</summary>
Motivation: LLMs often reproduce biases against LGBTQIA+ identities, necessitating lightweight solutions for bias mitigation.

Method: Two PEFT techniques (LoRA and soft-prompt tuning) are tested on three LLMs using the WinoQueer benchmark and a QueerNews corpus.

Result: LoRA reduces bias scores by up to 50 points and increases neutrality to 36%, while soft-prompt tuning shows marginal improvements.

Conclusion: LoRA is a viable solution for bias reduction; the paper calls for community-informed PEFT, larger queer-authored corpora, and richer evaluations.

Abstract: Large Language Models (LLMs) frequently reproduce the gender- and
sexual-identity prejudices embedded in their training corpora, leading to
outputs that marginalize LGBTQIA+ users. Hence, reducing such biases is of
great importance. To achieve this, we evaluate two parameter-efficient
fine-tuning (PEFT) techniques - Low-Rank Adaptation (LoRA) and soft-prompt
tuning - as lightweight alternatives to full-model fine-tuning for mitigating
such biases. Using the WinoQueer benchmark, we quantify bias in three
open-source LLMs and observe baseline bias scores reaching up to 98 (out of
100) across a range of queer identities defined by gender and/or sexual
orientation, where 50 would indicate neutrality. Fine-tuning with LoRA (< 0.1%
additional parameters) on a curated QueerNews corpus reduces those scores by up
to 50 points and raises neutrality from virtually 0% to as much as 36%.
Soft-prompt tuning (10 virtual tokens) delivers only marginal improvements.
These findings show that LoRA can deliver meaningful fairness gains with
minimal computation. We advocate broader adoption of community-informed PEFT,
the creation of larger queer-authored corpora, and richer evaluation suites
beyond WinoQueer, coupled with ongoing audits to keep LLMs inclusive.

</details>


### [126] [Innocence in the Crossfire: Roles of Skip Connections in Jailbreaking Visual Language Models](https://arxiv.org/abs/2507.13761)
*Palash Nandi,Maithili Joshi,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: The paper explores how prompt design in Visual Language Models (VLMs) can be exploited to generate inappropriate content, identifying three key factors and proposing a framework to increase jailbreak success.


<details>
  <summary>Details</summary>
Motivation: To understand and exploit the sensitivity of VLMs to prompt formulations, particularly in generating harmful content, by analyzing discrete components of prompt design.

Method: Analyzes three factors (detailed visual info, adversarial examples, positively framed phrases) and proposes a skip-connection framework to test jailbreak success in VLMs.

Result: VLMs struggle with multimodal inputs; each factor independently triggers jailbreaks, and memes are as effective as toxic visuals in eliciting harmful outputs.

Conclusion: VLMs have subtle vulnerabilities; the proposed framework increases jailbreak success, highlighting risks even with benign inputs.

Abstract: Language models are highly sensitive to prompt formulations - small changes
in input can drastically alter their output. This raises a critical question:
To what extent can prompt sensitivity be exploited to generate inapt content?
In this paper, we investigate how discrete components of prompt design
influence the generation of inappropriate content in Visual Language Models
(VLMs). Specifically, we analyze the impact of three key factors on successful
jailbreaks: (a) the inclusion of detailed visual information, (b) the presence
of adversarial examples, and (c) the use of positively framed beginning
phrases. Our findings reveal that while a VLM can reliably distinguish between
benign and harmful inputs in unimodal settings (text-only or image-only), this
ability significantly degrades in multimodal contexts. Each of the three
factors is independently capable of triggering a jailbreak, and we show that
even a small number of in-context examples (as few as three) can push the model
toward generating inappropriate outputs. Furthermore, we propose a framework
that utilizes a skip-connection between two internal layers of the VLM, which
substantially increases jailbreak success rates, even when using benign images.
Finally, we demonstrate that memes, often perceived as humorous or harmless,
can be as effective as toxic visuals in eliciting harmful content, underscoring
the subtle and complex vulnerabilities of VLMs.

</details>


### [127] [An Enhanced Model-based Approach for Short Text Clustering](https://arxiv.org/abs/2507.13793)
*Enhao Cheng,Shoujia Zhang,Jianhua Yin,Xuemeng Song,Tian Gan,Liqiang Nie*

Main category: cs.CL

TL;DR: The paper proposes GSDMM and its improved version GSDMM+ for short text clustering, addressing sparsity and high dimensionality while optimizing performance through noise reduction and adaptive word weighting.


<details>
  <summary>Details</summary>
Motivation: Short text clustering is challenging due to data sparsity, high dimensionality, and computational intensity. Existing methods (topic models and deep learning) have limitations.

Method: Proposes GSDMM (collapsed Gibbs Sampling for Dirichlet Multinomial Mixture) and GSDMM+ (improved with noise reduction, adaptive word weighting, and cluster merging).

Result: GSDMM+ outperforms classical and state-of-the-art methods, achieving efficient and effective clustering.

Conclusion: GSDMM+ offers a refined solution for short text clustering, with publicly available source code.

Abstract: Short text clustering has become increasingly important with the popularity
of social media like Twitter, Google+, and Facebook. Existing methods can be
broadly categorized into two paradigms: topic model-based approaches and deep
representation learning-based approaches. This task is inherently challenging
due to the sparse, large-scale, and high-dimensional characteristics of the
short text data. Furthermore, the computational intensity required by
representation learning significantly increases the running time. To address
these issues, we propose a collapsed Gibbs Sampling algorithm for the Dirichlet
Multinomial Mixture model (GSDMM), which effectively handles the sparsity and
high dimensionality of short texts while identifying representative words for
each cluster. Based on several aspects of GSDMM that warrant further
refinement, we propose an improved approach, GSDMM+, designed to further
optimize its performance. GSDMM+ reduces initialization noise and adaptively
adjusts word weights based on entropy, achieving fine-grained clustering that
reveals more topic-related information. Additionally, strategic cluster merging
is employed to refine clustering granularity, better aligning the predicted
distribution with the true category distribution. We conduct extensive
experiments, comparing our methods with both classical and state-of-the-art
approaches. The experimental results demonstrate the efficiency and
effectiveness of our methods. The source code for our model is publicly
available at https://github.com/chehaoa/VEMC.

</details>


### [128] [Question-Answer Extraction from Scientific Articles Using Knowledge Graphs and Large Language Models](https://arxiv.org/abs/2507.13827)
*Hosein Azarbonyad,Zi Long Zhu,Georgios Cheirmpos,Zubair Afzal,Vikrant Yadav,Georgios Tsatsaronis*

Main category: cs.CL

TL;DR: The paper proposes two methods for generating QA pairs from scientific articles: one using LLMs for direct content extraction and another leveraging a KG built from fine-tuned ER extraction. The KG-based approach better captures main ideas.


<details>
  <summary>Details</summary>
Motivation: Scholars need quick identification of key ideas in articles. The paper addresses this by extracting QA pairs to summarize contributions.

Method: Two approaches: 1) LLM-based QA generation from salient paragraphs, 2) KG-based QA generation using fine-tuned ER extraction and triplet saliency metrics.

Result: The KG-based approach effectively captures main ideas, and fine-tuning the ER model is crucial for high-quality triplet extraction.

Conclusion: The KG method outperforms the LLM approach in summarizing scientific articles, with fine-tuned ER extraction being key to success.

Abstract: When deciding to read an article or incorporate it into their research,
scholars often seek to quickly identify and understand its main ideas. In this
paper, we aim to extract these key concepts and contributions from scientific
articles in the form of Question and Answer (QA) pairs. We propose two distinct
approaches for generating QAs. The first approach involves selecting salient
paragraphs, using a Large Language Model (LLM) to generate questions, ranking
these questions by the likelihood of obtaining meaningful answers, and
subsequently generating answers. This method relies exclusively on the content
of the articles. However, assessing an article's novelty typically requires
comparison with the existing literature. Therefore, our second approach
leverages a Knowledge Graph (KG) for QA generation. We construct a KG by
fine-tuning an Entity Relationship (ER) extraction model on scientific articles
and using it to build the graph. We then employ a salient triplet extraction
method to select the most pertinent ERs per article, utilizing metrics such as
the centrality of entities based on a triplet TF-IDF-like measure. This measure
assesses the saliency of a triplet based on its importance within the article
compared to its prevalence in the literature. For evaluation, we generate QAs
using both approaches and have them assessed by Subject Matter Experts (SMEs)
through a set of predefined metrics to evaluate the quality of both questions
and answers. Our evaluations demonstrate that the KG-based approach effectively
captures the main ideas discussed in the articles. Furthermore, our findings
indicate that fine-tuning the ER extraction model on our scientific corpus is
crucial for extracting high-quality triplets from such documents.

</details>


### [129] [The Expressions of Depression and Anxiety in Chinese Psycho-counseling: Usage of First-person Singular Pronoun and Negative Emotional Words](https://arxiv.org/abs/2507.13839)
*Lizhi Ma,Tong Zhao,Shuai Zhang,Nirui Song,Hongliang He,Anqi Li,Ran Feng,Huachuan Qiu,Jingsong Ma,Zhenzhong Lan*

Main category: cs.CL

TL;DR: The study examines how linguistic expressions (first-person pronouns and negative words) relate to depression and anxiety in Chinese counseling. Negative words correlate with severity, but first-person pronouns don't, differing from Western findings.


<details>
  <summary>Details</summary>
Motivation: To understand how language reflects psychological states in Chinese counseling, considering cultural and conversational contexts.

Method: Analyzed 735 online counseling sessions using LIWC and a mixed-effect model to quantify linguistic patterns.

Result: Negative words linked to severity of depression/anxiety; first-person pronouns showed no significant correlation, unlike in Western studies.

Conclusion: Cultural and conversational contexts shape language use in mental health, highlighting unique psycholinguistic markers for Chinese populations.

Abstract: This study explores the relationship between linguistic expressions and
psychological states of depression and anxiety within Chinese psycho-counseling
interactions, focusing specifically on the usage of first-person singular
pronouns and negative emotional words. Utilizing a corpus derived from 735
online counseling sessions, the analysis employed a general linear mixed-effect
model to assess linguistic patterns quantified by the Linguistic Inquiry and
Word Count (LIWC) software. Results indicate a significant positive correlation
between the frequency of negative emotional words and the severity of both
depressive and anxious states among clients. However, contrary to prior
findings predominantly derived from English-language contexts, the usage
frequency of first-person singular pronouns did not vary significantly with the
clients' psychological conditions. These outcomes are discussed within the
framework of cultural distinctions between collectivist Chinese contexts and
individualistic Western settings, as well as the interactive dynamics unique to
psycho-counseling conversations. The findings highlight the nuanced influence
of cultural and conversational contexts on language use in mental health
communications, providing insights into psycholinguistic markers relevant to
therapeutic practices in Chinese-speaking populations.

</details>


### [130] [Modeling Fair Play in Detective Stories with Language Models](https://arxiv.org/abs/2507.13841)
*Eitan Wagner,Renana Keydar,Omri Abend*

Main category: cs.CL

TL;DR: A probabilistic framework for detective fiction defines fair play, balancing coherence and surprise, and evaluates LLM-generated stories, finding them lacking in this balance.


<details>
  <summary>Details</summary>
Motivation: To formalize the concept of fair play in detective fiction and assess its balance with surprise in storytelling, particularly in LLM-generated stories.

Method: Develop a probabilistic framework to define fair play and design metrics, then apply it to LLM-generated detective stories.

Result: LLM-generated stories are unpredictable but fail to balance surprise and fair play, leading to poor quality.

Conclusion: The framework successfully identifies the trade-off between coherence and surprise, highlighting shortcomings in LLM-generated detective fiction.

Abstract: Effective storytelling relies on a delicate balance between meeting the
reader's prior expectations and introducing unexpected developments. In the
domain of detective fiction, this tension is known as fair play, which includes
the implicit agreement between the writer and the reader as to the range of
possible resolutions the mystery story may have. In this work, we present a
probabilistic framework for detective fiction that allows us to define desired
qualities. Using this framework, we formally define fair play and design
appropriate metrics for it. Stemming from these definitions is an inherent
tension between the coherence of the story, which measures how much it ``makes
sense'', and the surprise it induces. We validate the framework by applying it
to LLM-generated detective stories. This domain is appealing since we have an
abundance of data, we can sample from the distribution generating the story,
and the story-writing capabilities of LLMs are interesting in their own right.
Results show that while LLM-generated stories may be unpredictable, they
generally fail to balance the trade-off between surprise and fair play, which
greatly contributes to their poor quality.

</details>


### [131] [InTraVisTo: Inside Transformer Visualisation Tool](https://arxiv.org/abs/2507.13858)
*Nicolò Brunello,Davide Rigamonti,Andrea Sassella,Vincenzo Scotti,Mark James Carman*

Main category: cs.CL

TL;DR: InTraVisTo is a visualization tool for Transformer-based LLMs, helping researchers trace token generation and understand internal computations.


<details>
  <summary>Details</summary>
Motivation: LLMs are complex and unpredictable, making their behavior hard to control or interpret in production.

Method: InTraVisTo visualizes internal states (token embeddings per layer) and information flow (via Sankey diagrams) in Transformer models.

Result: The tool aids in understanding LLM computations, revealing internal patterns and reasoning processes.

Conclusion: InTraVisTo enhances interpretability of LLMs, supporting better research and practical applications.

Abstract: The reasoning capabilities of Large Language Models (LLMs) have increased
greatly over the last few years, as have their size and complexity.
Nonetheless, the use of LLMs in production remains challenging due to their
unpredictable nature and discrepancies that can exist between their desired
behavior and their actual model output. In this paper, we introduce a new tool,
InTraVisTo (Inside Transformer Visualisation Tool), designed to enable
researchers to investigate and trace the computational process that generates
each token in a Transformer-based LLM. InTraVisTo provides a visualization of
both the internal state of the Transformer model (by decoding token embeddings
at each layer of the model) and the information flow between the various
components across the different layers of the model (using a Sankey diagram).
With InTraVisTo, we aim to help researchers and practitioners better understand
the computations being performed within the Transformer model and thus to shed
some light on internal patterns and reasoning processes employed by LLMs.

</details>


### [132] [Label Unification for Cross-Dataset Generalization in Cybersecurity NER](https://arxiv.org/abs/2507.13870)
*Maciej Jalocha,Johan Hausted Schmidt,William Michelseen*

Main category: cs.CL

TL;DR: The paper addresses the lack of standardized labels in cybersecurity NER by unifying labels across four datasets, evaluating with BiLSTM models, and proposing alternative architectures like multihead and graph-based transfer models. Results show poor generalization and marginal improvements.


<details>
  <summary>Details</summary>
Motivation: The lack of standardized labels in cybersecurity NER makes combining datasets challenging, reducing data usability.

Method: Coarse-grained label unification, pairwise cross-dataset evaluations with BiLSTM models, and proposing multihead and graph-based transfer models.

Result: Models trained on unified datasets generalize poorly; multihead and graph-based transfer models show marginal or no significant improvements.

Conclusion: Label unification in cybersecurity NER remains challenging, with proposed architectures offering limited benefits.

Abstract: The field of cybersecurity NER lacks standardized labels, making it
challenging to combine datasets. We investigate label unification across four
cybersecurity datasets to increase data resource usability. We perform a
coarse-grained label unification and conduct pairwise cross-dataset evaluations
using BiLSTM models. Qualitative analysis of predictions reveals errors,
limitations, and dataset differences. To address unification limitations, we
propose alternative architectures including a multihead model and a graph-based
transfer model. Results show that models trained on unified datasets generalize
poorly across datasets. The multihead model with weight sharing provides only
marginal improvements over unified training, while our graph-based transfer
model built on BERT-base-NER shows no significant performance gains compared
BERT-base-NER.

</details>


### [133] [Optimizing ASR for Catalan-Spanish Code-Switching: A Comparative Analysis of Methodologies](https://arxiv.org/abs/2507.13875)
*Carlos Mena,Pol Serra,Jacobo Romero,Abir Messaoudi,Jose Giraldo,Carme Armentano-Oller,Rodolfo Zevallos,Ivan Meza,Javier Hernando*

Main category: cs.CL

TL;DR: Improving ASR for Catalan-Spanish code-switching by using synthetic data, monolingual audio concatenation, and real CS data with language tokens, achieving best results with synthetic data and dominant language tokens.


<details>
  <summary>Details</summary>
Motivation: Code-switching (CS) challenges ASR due to scarce training data and linguistic similarities, especially in multilingual societies like Catalan-Spanish contexts.

Method: Three strategies: (1) synthetic CS data generation, (2) monolingual audio concatenation, (3) real CS data with language tokens. Fine-tuned OpenAI's Whisper models.

Result: Combining synthetic CS data with dominant language tokens yields the best transcription performance.

Conclusion: The approach enhances ASR for CS, particularly in Catalan-Spanish, and the models are made available on Hugging Face.

Abstract: Code-switching (CS), the alternating use of two or more languages, challenges
automatic speech recognition (ASR) due to scarce training data and linguistic
similarities. The lack of dedicated CS datasets limits ASR performance, as most
models rely on monolingual or mixed-language corpora that fail to reflect
real-world CS patterns. This issue is critical in multilingual societies where
CS occurs in informal and formal settings. A key example is Catalan-Spanish CS,
widely used in media and parliamentary speeches. In this work, we improve ASR
for Catalan-Spanish CS by exploring three strategies: (1) generating synthetic
CS data, (2) concatenating monolingual audio, and (3) leveraging real CS data
with language tokens. We extract CS data from Catalan speech corpora and
fine-tune OpenAI's Whisper models, making them available on Hugging Face.
Results show that combining a modest amount of synthetic CS data with the
dominant language token yields the best transcription performance.

</details>


### [134] [Using LLMs to identify features of personal and professional skills in an open-response situational judgment test](https://arxiv.org/abs/2507.13881)
*Cole Walsh,Rodica Ivan,Muhammad Zafar Iqbal,Colleen Robb*

Main category: cs.CL

TL;DR: The paper explores using large language models (LLMs) to automate scoring of open-response Situational Judgment Tests (SJTs) for personal and professional skills, addressing scalability and validity issues.


<details>
  <summary>Details</summary>
Motivation: The need for scalable systems to measure and develop personal and professional skills alongside technical expertise in academic programs.

Method: A novel approach using LLMs to extract construct-relevant features from SJT responses, demonstrated with the Casper SJT.

Result: The study shows the efficacy of LLMs in automating SJT scoring, addressing past issues with construct validity.

Conclusion: This work lays the foundation for future advancements in automated scoring for personal and professional skills.

Abstract: Academic programs are increasingly recognizing the importance of personal and
professional skills and their critical role alongside technical expertise in
preparing students for future success in diverse career paths. With this
growing demand comes the need for scalable systems to measure, evaluate, and
develop these skills. Situational Judgment Tests (SJTs) offer one potential
avenue for measuring these skills in a standardized and reliable way, but
open-response SJTs have traditionally relied on trained human raters for
evaluation, presenting operational challenges to delivering SJTs at scale. Past
attempts at developing NLP-based scoring systems for SJTs have fallen short due
to issues with construct validity of these systems. In this article, we explore
a novel approach to extracting construct-relevant features from SJT responses
using large language models (LLMs). We use the Casper SJT to demonstrate the
efficacy of this approach. This study sets the foundation for future
developments in automated scoring for personal and professional skills.

</details>


### [135] [Political Leaning and Politicalness Classification of Texts](https://arxiv.org/abs/2507.13913)
*Matous Volf,Jakub Simko*

Main category: cs.CL

TL;DR: The paper tackles text classification for political leaning and politicalness using transformer models, addressing poor generalization by compiling diverse datasets and benchmarking models.


<details>
  <summary>Details</summary>
Motivation: Existing approaches for political text classification create siloed solutions with poor out-of-distribution performance, prompting the need for better generalization.

Method: Combines 12 datasets for political leaning, extends 18 datasets for politicalness, and benchmarks models using leave-one-in and leave-one-out methodologies.

Result: New models trained on diverse datasets show improved generalization capabilities compared to existing solutions.

Conclusion: The compiled datasets and benchmarking methodologies enhance model performance and generalization for political text classification.

Abstract: This paper addresses the challenge of automatically classifying text
according to political leaning and politicalness using transformer models. We
compose a comprehensive overview of existing datasets and models for these
tasks, finding that current approaches create siloed solutions that perform
poorly on out-of-distribution texts. To address this limitation, we compile a
diverse dataset by combining 12 datasets for political leaning classification
and creating a new dataset for politicalness by extending 18 existing datasets
with the appropriate label. Through extensive benchmarking with leave-one-in
and leave-one-out methodologies, we evaluate the performance of existing models
and train new ones with enhanced generalization capabilities.

</details>


### [136] [The Levers of Political Persuasion with Conversational AI](https://arxiv.org/abs/2507.13919)
*Kobi Hackenburg,Ben M. Tappin,Luke Hewitt,Ed Saunders,Sid Black,Hause Lin,Catherine Fist,Helen Margetts,David G. Rand,Christopher Summerfield*

Main category: cs.CL

TL;DR: Current AI's persuasive power stems more from post-training and prompting than personalization or scale, but these methods reduce factual accuracy.


<details>
  <summary>Details</summary>
Motivation: To address fears about AI's influence on human beliefs by evaluating its persuasiveness and factual accuracy.

Method: Three large-scale experiments with 19 LLMs tested on 707 political issues, analyzing 466,769 claims.

Result: Post-training and prompting boosted persuasiveness by 51% and 27% respectively, but decreased factual accuracy.

Conclusion: AI's persuasiveness is driven by strategic information deployment, not scale, at the cost of accuracy.

Abstract: There are widespread fears that conversational AI could soon exert
unprecedented influence over human beliefs. Here, in three large-scale
experiments (N=76,977), we deployed 19 LLMs-including some post-trained
explicitly for persuasion-to evaluate their persuasiveness on 707 political
issues. We then checked the factual accuracy of 466,769 resulting LLM claims.
Contrary to popular concerns, we show that the persuasive power of current and
near-future AI is likely to stem more from post-training and prompting
methods-which boosted persuasiveness by as much as 51% and 27%
respectively-than from personalization or increasing model scale. We further
show that these methods increased persuasion by exploiting LLMs' unique ability
to rapidly access and strategically deploy information and that, strikingly,
where they increased AI persuasiveness they also systematically decreased
factual accuracy.

</details>


### [137] [Marcel: A Lightweight and Open-Source Conversational Agent for University Student Support](https://arxiv.org/abs/2507.13937)
*Jan Trienes,Anastasiia Derzhanskaia,Roland Schwarzkopf,Markus Mühling,Jörg Schlötterer,Christin Seifert*

Main category: cs.CL

TL;DR: Marcel is a lightweight, open-source conversational agent for admission inquiries, using retrieval-augmented generation and an FAQ retriever to provide accurate, verifiable answers while reducing staff workload.


<details>
  <summary>Details</summary>
Motivation: To support prospective students with admission-related inquiries efficiently and reduce the workload of university staff.

Method: Uses retrieval-augmented generation and an FAQ retriever to map user questions to knowledge-base entries, improving over standard retrieval methods.

Result: The system provides fast, personalized, and contextually relevant responses, and is designed for easy deployment in resource-constrained settings.

Conclusion: Marcel effectively addresses admission inquiries with verifiable information, demonstrating practical utility in real-world academic settings.

Abstract: We present Marcel, a lightweight and open-source conversational agent
designed to support prospective students with admission-related inquiries. The
system aims to provide fast and personalized responses, while reducing workload
of university staff. We employ retrieval-augmented generation to ground answers
in university resources and to provide users with verifiable, contextually
relevant information. To improve retrieval quality, we introduce an FAQ
retriever that maps user questions to knowledge-base entries, allowing
administrators to steer retrieval, and improving over standard dense/hybrid
retrieval strategies. The system is engineered for easy deployment in
resource-constrained academic settings. We detail the system architecture,
provide a technical evaluation of its components, and report insights from a
real-world deployment.

</details>


### [138] [Exploiting Primacy Effect To Improve Large Language Models](https://arxiv.org/abs/2507.13949)
*Bianca Raimondi,Maurizio Gabbrielli*

Main category: cs.CL

TL;DR: Fine-tuned LLMs exhibit primacy bias in MCQA, which can be strategically leveraged by reordering answer options to improve performance.


<details>
  <summary>Details</summary>
Motivation: To address and exploit positional biases (primacy effect) in fine-tuned LLMs for better MCQA performance.

Method: Reordering answer options based on semantic similarity to the query, without knowing the correct answer.

Result: Significant performance improvement in MCQA tasks.

Conclusion: Biases in LLMs can be both challenges and opportunities, offering insights for bias-aware model design.

Abstract: Large Language Models (LLMs) have become essential in many Natural Language
Processing (NLP) tasks, leveraging extensive pre-training and fine-tuning to
achieve high accuracy. However, like humans, LLMs exhibit biases, particularly
positional biases such as primacy and recency effects, which can influence the
accuracy of the answers. The primacy effect-where items presented first are
more likely to be remembered or selected-plays a key role in Multiple Choice
Question Answering (MCQA), where the order of answer options can affect
prediction outcomes. This study focuses on primacy bias in fine-tuned LLMs: We
first show that fine-tuning amplifies this bias, probably due to exposure to
human-like patterns. Hence, we strategically leverage this effect by reordering
response options based on semantic similarity to the query, without requiring
knowledge of the correct answer. Our experimental results show that this
approach significantly improves performance in MCQA. More generally, our
findings underscore the dual nature of biases as both challenges and
opportunities, offering insights for bias-aware model design and NLP
applications.

</details>


### [139] [Bottom-up Domain-specific Superintelligence: A Reliable Knowledge Graph is What We Need](https://arxiv.org/abs/2507.13966)
*Bhishma Dedhia,Yuval Kansal,Niraj K. Jha*

Main category: cs.CL

TL;DR: The paper proposes a bottom-up approach using knowledge graphs (KGs) to train language models for domain-specific reasoning, validated in medicine with significant performance improvements.


<details>
  <summary>Details</summary>
Motivation: Traditional top-down training for language models lacks deep domain expertise, requiring a method to compose simple concepts into complex ones for specialized reasoning.

Method: A task generation pipeline synthesizes tasks from KG primitives, fine-tuning models like QwQ-32B on this curriculum to create QwQ-Med-3, evaluated using ICD-Bench.

Result: QwQ-Med-3 outperforms state-of-the-art models on ICD-Bench and shows enhanced performance on medical benchmarks, leveraging acquired primitives for harder tasks.

Conclusion: The approach suggests domain-specific superintelligence as a pathway to AGI, emphasizing composable expertise over broad generalization.

Abstract: Language models traditionally used for cross-domain generalization have
recently demonstrated task-specific reasoning. However, their top-down training
approach on general corpora is insufficient for acquiring abstractions needed
for deep domain expertise. This may require a bottom-up approach that acquires
expertise by learning to compose simple domain concepts into more complex ones.
A knowledge graph (KG) provides this compositional structure, where domain
primitives are represented as head-relation-tail edges and their paths encode
higher-level concepts. We present a task generation pipeline that synthesizes
tasks directly from KG primitives, enabling models to acquire and compose them
for reasoning. We fine-tune language models on the resultant KG-grounded
curriculum to demonstrate domain-specific superintelligence. While broadly
applicable, we validate our approach in medicine, where reliable KGs exist.
Using a medical KG, we curate 24,000 reasoning tasks paired with thinking
traces derived from diverse medical primitives. We fine-tune the QwQ-32B model
on this curriculum to obtain QwQ-Med-3 that takes a step towards medical
superintelligence. We also introduce ICD-Bench, an evaluation suite to quantify
reasoning abilities across 15 medical domains. Our experiments demonstrate that
QwQ-Med-3 significantly outperforms state-of-the-art reasoning models on
ICD-Bench categories. Further analysis reveals that QwQ-Med-3 utilizes acquired
primitives to widen the performance gap on the hardest tasks of ICD-Bench.
Finally, evaluation on medical question-answer benchmarks shows that QwQ-Med-3
transfers acquired expertise to enhance the base model's performance. While the
industry's approach to artificial general intelligence (AGI) emphasizes broad
expertise, we envision a future in which AGI emerges from the composable
interaction of efficient domain-specific superintelligent agents.

</details>


### [140] [Open Automatic Speech Recognition Models for Classical and Modern Standard Arabic](https://arxiv.org/abs/2507.13977)
*Lilit Grigoryan,Nikolay Karpov,Enas Albasiri,Vitaly Lavrukhin,Boris Ginsburg*

Main category: cs.CL

TL;DR: The paper introduces a universal methodology for Arabic speech/text processing, training two FastConformer-based models (MSA and unified MSA/CA), achieving SOTA results and open-sourcing them.


<details>
  <summary>Details</summary>
Motivation: Address the lack of attention to Arabic language variations and limited public ASR models, focusing on MSA and Classical Arabic.

Method: Develop a universal methodology for Arabic processing; train two FastConformer models (MSA-specific and unified MSA/CA).

Result: MSA model sets SOTA benchmarks; unified model achieves SOTA accuracy for CA with diacritics while performing well for MSA.

Conclusion: The methodology and models advance Arabic ASR, with open-sourced resources to encourage reproducibility and further research.

Abstract: Despite Arabic being one of the most widely spoken languages, the development
of Arabic Automatic Speech Recognition (ASR) systems faces significant
challenges due to the language's complexity, and only a limited number of
public Arabic ASR models exist. While much of the focus has been on Modern
Standard Arabic (MSA), there is considerably less attention given to the
variations within the language. This paper introduces a universal methodology
for Arabic speech and text processing designed to address unique challenges of
the language. Using this methodology, we train two novel models based on the
FastConformer architecture: one designed specifically for MSA and the other,
the first unified public model for both MSA and Classical Arabic (CA). The MSA
model sets a new benchmark with state-of-the-art (SOTA) performance on related
datasets, while the unified model achieves SOTA accuracy with diacritics for CA
while maintaining strong performance for MSA. To promote reproducibility, we
open-source the models and their training recipes.

</details>


### [141] [Efficient Temporal Tokenization for Mobility Prediction with Large Language Models](https://arxiv.org/abs/2507.14017)
*Haoyu He,Haozheng Luo,Yan Chen,Qi R. Wang*

Main category: cs.CL

TL;DR: RHYTHM uses LLMs for spatio-temporal prediction, tokenizing trajectories hierarchically for efficiency and accuracy.


<details>
  <summary>Details</summary>
Motivation: To improve human mobility prediction by leveraging LLMs while reducing computational costs.

Method: Partitions trajectories into tokens with hierarchical attention, uses pre-computed prompt embeddings, and freezes the LLM backbone.

Result: 2.4% accuracy boost, 5.0% weekend improvement, 24.6% faster training.

Conclusion: RHYTHM efficiently enhances mobility prediction with LLMs.

Abstract: We introduce RHYTHM (Reasoning with Hierarchical Temporal Tokenization for
Human Mobility), a framework that leverages large language models (LLMs) as
spatio-temporal predictors and trajectory reasoners. RHYTHM partitions
trajectories into daily segments encoded as discrete tokens with hierarchical
attention, capturing both daily and weekly dependencies while substantially
reducing the sequence length. Token representations are enriched with
pre-computed prompt embeddings via a frozen LLM, enhancing the model's ability
to capture interdependencies without extensive computational overhead. By
freezing the LLM backbone, RHYTHM achieves significant computational
efficiency. Evaluation on three real-world datasets demonstrates a 2.4%
improvement in accuracy, 5.0% increase on weekends, and 24.6% reduction in
training time compared to state-of-the-art methods.

</details>


### [142] [CPC-CMS: Cognitive Pairwise Comparison Classification Model Selection Framework for Document-level Sentiment Analysis](https://arxiv.org/abs/2507.14022)
*Jianfei Li,Kevin Kam Fung Yuen*

Main category: cs.CL

TL;DR: The paper introduces the CPC-CMS framework for document-level sentiment analysis, using expert knowledge to weight evaluation criteria and select the best model among several baselines. ALBERT performs best without considering time, but no single model excels when time is included.


<details>
  <summary>Details</summary>
Motivation: To improve model selection for sentiment analysis by incorporating expert knowledge and multiple evaluation criteria.

Method: Uses the CPC framework to weight criteria (e.g., accuracy, F1-score) and forms a decision matrix to compare baseline models (e.g., Naive Bayes, ALBERT) on three social media datasets.

Result: ALBERT is the best performer without time constraints, but no model consistently outperforms others when time is considered.

Conclusion: CPC-CMS is effective for sentiment analysis and can be extended to other classification tasks.

Abstract: This study proposes the Cognitive Pairwise Comparison Classification Model
Selection (CPC-CMS) framework for document-level sentiment analysis. The CPC,
based on expert knowledge judgment, is used to calculate the weights of
evaluation criteria, including accuracy, precision, recall, F1-score,
specificity, Matthews Correlation Coefficient (MCC), Cohen's Kappa (Kappa), and
efficiency. Naive Bayes, Linear Support Vector Classification (LSVC), Random
Forest, Logistic Regression, Extreme Gradient Boosting (XGBoost), Long
Short-Term Memory (LSTM), and A Lite Bidirectional Encoder Representations from
Transformers (ALBERT) are chosen as classification baseline models. A weighted
decision matrix consisting of classification evaluation scores with respect to
criteria weights, is formed to select the best classification model for a
classification problem. Three open datasets of social media are used to
demonstrate the feasibility of the proposed CPC-CMS. Based on our simulation,
for evaluation results excluding the time factor, ALBERT is the best for the
three datasets; if time consumption is included, no single model always
performs better than the other models. The CPC-CMS can be applied to the other
classification applications in different areas.

</details>


### [143] [Evaluating the Effectiveness of Cost-Efficient Large Language Models in Benchmark Biomedical Tasks](https://arxiv.org/abs/2507.14045)
*Israt Jahan,Md Tahmid Rahman Laskar,Chun Peng,Jimmy Huang*

Main category: cs.CL

TL;DR: Evaluation of cost-efficient LLMs for biomedical tasks shows no single model outperforms others universally; task-specific strengths vary between closed-source and open-source models.


<details>
  <summary>Details</summary>
Motivation: To assess the performance of various LLMs in biomedical tasks, comparing closed-source and open-source models for efficiency and suitability.

Method: Evaluated multiple LLMs on tasks like text classification, generation, QA, and multimodal image processing.

Result: No LLM consistently outperforms others; closed-source models excel in some tasks, while open-source models match or surpass them with added benefits like speed and privacy.

Conclusion: Task-specific model selection is crucial for optimal performance in biomedical applications, with open-source models offering competitive advantages.

Abstract: This paper presents a comprehensive evaluation of cost-efficient Large
Language Models (LLMs) for diverse biomedical tasks spanning both text and
image modalities. We evaluated a range of closed-source and open-source LLMs on
tasks such as biomedical text classification and generation, question
answering, and multimodal image processing. Our experimental findings indicate
that there is no single LLM that can consistently outperform others across all
tasks. Instead, different LLMs excel in different tasks. While some
closed-source LLMs demonstrate strong performance on specific tasks, their
open-source counterparts achieve comparable results (sometimes even better),
with additional benefits like faster inference and enhanced privacy. Our
experimental results offer valuable insights for selecting models that are
optimally suited for specific biomedical applications.

</details>


### [144] [Collaborative Rational Speech Act: Pragmatic Reasoning for Multi-Turn Dialog](https://arxiv.org/abs/2507.14063)
*Lautaro Estienne,Gabriel Ben Zenou,Nona Naderi,Jackie Cheung,Pablo Piantanida*

Main category: cs.CL

TL;DR: The paper introduces Collaborative Rational Speech Act (CRSA), an extension of the RSA framework for multi-turn dialog, optimizing a gain function for collaborative AI behavior.


<details>
  <summary>Details</summary>
Motivation: AI systems need to reason about shared goals and beliefs in collaborative roles, requiring scalable pragmatic reasoning beyond fluent language generation.

Method: CRSA extends RSA with an information-theoretic approach, optimizing a gain function adapted from rate-distortion theory for multi-turn dialog.

Result: CRSA outperforms baselines in referential games and doctor-patient dialogs, showing more consistent, interpretable, and collaborative behavior.

Conclusion: CRSA advances pragmatic and socially aware AI language agents, demonstrating its effectiveness in collaborative scenarios.

Abstract: As AI systems take on collaborative roles, they must reason about shared
goals and beliefs-not just generate fluent language. The Rational Speech Act
(RSA) framework offers a principled approach to pragmatic reasoning, but
existing extensions face challenges in scaling to multi-turn, collaborative
scenarios. In this paper, we introduce Collaborative Rational Speech Act
(CRSA), an information-theoretic (IT) extension of RSA that models multi-turn
dialog by optimizing a gain function adapted from rate-distortion theory. This
gain is an extension of the gain model that is maximized in the original RSA
model but takes into account the scenario in which both agents in a
conversation have private information and produce utterances conditioned on the
dialog. We demonstrate the effectiveness of CRSA on referential games and
template-based doctor-patient dialogs in the medical domain. Empirical results
show that CRSA yields more consistent, interpretable, and collaborative
behavior than existing baselines-paving the way for more pragmatic and socially
aware language agents.

</details>


### [145] [DENSE: Longitudinal Progress Note Generation with Temporal Modeling of Heterogeneous Clinical Notes Across Hospital Visits](https://arxiv.org/abs/2507.14079)
*Garapati Keerthana,Manik Gupta*

Main category: cs.CL

TL;DR: DENSE is a system for generating clinically coherent progress notes by leveraging LLMs and a retrieval strategy to align scattered EHR notes temporally and semantically.


<details>
  <summary>Details</summary>
Motivation: Progress notes are critical in EHRs but underrepresented in datasets like MIMIC-III, creating gaps in patient narratives.

Method: DENSE uses fine-grained note categorization, temporal alignment, and LLM prompting with retrieved evidence to generate notes.

Result: Generated notes show strong longitudinal fidelity (temporal alignment ratio of 1.089) and outperform original notes in continuity.

Conclusion: DENSE enhances narrative coherence in EHRs, aiding tasks like summarization and decision support, and offers scalable LLM-driven note synthesis.

Abstract: Progress notes are among the most clinically meaningful artifacts in an
Electronic Health Record (EHR), offering temporally grounded insights into a
patient's evolving condition, treatments, and care decisions. Despite their
importance, they are severely underrepresented in large-scale EHR datasets. For
instance, in the widely used Medical Information Mart for Intensive Care III
(MIMIC-III) dataset, only about $8.56\%$ of hospital visits include progress
notes, leaving gaps in longitudinal patient narratives. In contrast, the
dataset contains a diverse array of other note types, each capturing different
aspects of care.
  We present DENSE (Documenting Evolving Progress Notes from Scattered
Evidence), a system designed to align with clinical documentation workflows by
simulating how physicians reference past encounters while drafting progress
notes. The system introduces a fine-grained note categorization and a temporal
alignment mechanism that organizes heterogeneous notes across visits into
structured, chronological inputs. At its core, DENSE leverages a clinically
informed retrieval strategy to identify temporally and semantically relevant
content from both current and prior visits. This retrieved evidence is used to
prompt a large language model (LLM) to generate clinically coherent and
temporally aware progress notes.
  We evaluate DENSE on a curated cohort of patients with multiple visits and
complete progress note documentation. The generated notes demonstrate strong
longitudinal fidelity, achieving a temporal alignment ratio of $1.089$,
surpassing the continuity observed in original notes. By restoring narrative
coherence across fragmented documentation, our system supports improved
downstream tasks such as summarization, predictive modeling, and clinical
decision support, offering a scalable solution for LLM-driven note synthesis in
real-world healthcare settings.

</details>


### [146] [Lessons from the TREC Plain Language Adaptation of Biomedical Abstracts (PLABA) track](https://arxiv.org/abs/2507.14096)
*Brian Ondov,William Xia,Kush Attal,Ishita Unde,Jerry He,Hoa Dang,Ian Soboroff,Dina Demner-Fushman*

Main category: cs.CL

TL;DR: The PLABA track evaluated LLMs for adapting biomedical abstracts to plain language, showing promise but highlighting gaps in simplicity, brevity, and benchmarking.


<details>
  <summary>Details</summary>
Motivation: To make biomedical literature accessible to patients and caregivers by adapting it to plain language, while ensuring rigorous evaluation due to potential harm.

Method: Hosted the PLABA track with tasks for rewriting abstracts (Task 1) and replacing difficult terms (Task 2), using professional references and expert manual evaluation.

Result: Top models matched human factual accuracy but lacked simplicity and brevity. Automatic metrics poorly correlated with manual judgments. LLMs excelled in term replacement accuracy but struggled with brevity.

Conclusion: LLMs show potential for plain language adaptation but need improvements in simplicity, brevity, and better automatic evaluation tools.

Abstract: Objective: Recent advances in language models have shown potential to adapt
professional-facing biomedical literature to plain language, making it
accessible to patients and caregivers. However, their unpredictability,
combined with the high potential for harm in this domain, means rigorous
evaluation is necessary. Our goals with this track were to stimulate research
and to provide high-quality evaluation of the most promising systems.
  Methods: We hosted the Plain Language Adaptation of Biomedical Abstracts
(PLABA) track at the 2023 and 2024 Text Retrieval Conferences. Tasks included
complete, sentence-level, rewriting of abstracts (Task 1) as well as
identifying and replacing difficult terms (Task 2). For automatic evaluation of
Task 1, we developed a four-fold set of professionally-written references.
Submissions for both Tasks 1 and 2 were provided extensive manual evaluation
from biomedical experts.
  Results: Twelve teams spanning twelve countries participated in the track,
with models from multilayer perceptrons to large pretrained transformers. In
manual judgments of Task 1, top-performing models rivaled human levels of
factual accuracy and completeness, but not simplicity or brevity. Automatic,
reference-based metrics generally did not correlate well with manual judgments.
In Task 2, systems struggled with identifying difficult terms and classifying
how to replace them. When generating replacements, however, LLM-based systems
did well in manually judged accuracy, completeness, and simplicity, though not
in brevity.
  Conclusion: The PLABA track showed promise for using Large Language Models to
adapt biomedical literature for the general public, while also highlighting
their deficiencies and the need for improved automatic benchmarking tools.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [147] [Physical models realizing the transformer architecture of large language models](https://arxiv.org/abs/2507.13354)
*Zeqian Chen*

Main category: cs.LG

TL;DR: The paper explores the transformer architecture from a physical perspective, modeling it as an open quantum system in Fock space to understand its theoretical underpinnings.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap in theoretical understanding of why the transformer architecture works, especially from a physical standpoint.

Method: Constructs physical models in Fock space over the Hilbert space of tokens, treating large language models as open quantum systems.

Result: The models provide a physical foundation for the transformer architecture in large language models.

Conclusion: The study offers a novel physical perspective on transformers, enhancing theoretical understanding of their functionality.

Abstract: The introduction of the transformer architecture in 2017 (cf.\cite{VSP2017})
marked the most striking advancement in natural language processing. The
transformer is a model architecture relying entirely on an attention mechanism
to draw global dependencies between input and output. However, we believe there
is a gap in our theoretical understanding of what the transformer is, and why
it works physically. In this paper, from a physical perspective on modern
chips, we construct physical models in the Fock space over the Hilbert space of
tokens realizing large language models based on a transformer architecture as
open quantum systems. Our physical models underlie the transformer architecture
for large language models.

</details>


### [148] [Whose View of Safety? A Deep DIVE Dataset for Pluralistic Alignment of Text-to-Image Models](https://arxiv.org/abs/2507.13383)
*Charvi Rastogi,Tian Huey Teh,Pushkar Mishra,Roma Patel,Ding Wang,Mark Díaz,Alicia Parrish,Aida Mostafazadeh Davani,Zoe Ashwood,Michela Paganini,Vinodkumar Prabhakaran,Verena Rieser,Lora Aroyo*

Main category: cs.LG

TL;DR: The paper introduces pluralistic alignment in text-to-image (T2I) models to address diverse human values, using a novel dataset (DIVE) and demographic insights for better alignment.


<details>
  <summary>Details</summary>
Motivation: Current T2I models often misalign with diverse human experiences, necessitating a pluralistic approach to ensure equitable AI systems.

Method: The study introduces the DIVE dataset, leverages intersectional human raters, and analyzes demographic-based differences in harm perception.

Result: Demographics are confirmed as a key proxy for diverse viewpoints, revealing context-dependent differences in harm perception.

Conclusion: The research provides foundational tools for equitable T2I systems, emphasizing efficient data collection and model steerability.

Abstract: Current text-to-image (T2I) models often fail to account for diverse human
experiences, leading to misaligned systems. We advocate for pluralistic
alignment, where an AI understands and is steerable towards diverse, and often
conflicting, human values. Our work provides three core contributions to
achieve this in T2I models. First, we introduce a novel dataset for Diverse
Intersectional Visual Evaluation (DIVE) -- the first multimodal dataset for
pluralistic alignment. It enable deep alignment to diverse safety perspectives
through a large pool of demographically intersectional human raters who
provided extensive feedback across 1000 prompts, with high replication,
capturing nuanced safety perceptions. Second, we empirically confirm
demographics as a crucial proxy for diverse viewpoints in this domain,
revealing significant, context-dependent differences in harm perception that
diverge from conventional evaluations. Finally, we discuss implications for
building aligned T2I models, including efficient data collection strategies,
LLM judgment capabilities, and model steerability towards diverse perspectives.
This research offers foundational tools for more equitable and aligned T2I
systems. Content Warning: The paper includes sensitive content that may be
harmful.

</details>


### [149] [Improving KAN with CDF normalization to quantiles](https://arxiv.org/abs/2507.13393)
*Jakub Strawa,Jarek Duda*

Main category: cs.LG

TL;DR: The paper highlights the benefits of CDF normalization, common in copula theory, for machine learning, demonstrating improved predictions in Kolmogorov-Arnold Networks (KANs) and offering interpretability through mixed moments.


<details>
  <summary>Details</summary>
Motivation: The motivation is to introduce CDF normalization, widely used in finance but overlooked in machine learning, to enhance model performance and interpretability.

Method: The method involves replacing traditional rescaling with CDF normalization in Kolmogorov-Arnold Networks (KANs), leveraging copula theory to transform data to uniform distributions.

Result: Switching to CDF normalization improved predictions in Legendre-KANs and provided interpretable neuron weights as mixed moments.

Conclusion: CDF normalization is a promising alternative in machine learning, offering performance gains and interpretability, as demonstrated in KANs.

Abstract: Data normalization is crucial in machine learning, usually performed by
subtracting the mean and dividing by standard deviation, or by rescaling to a
fixed range. In copula theory, popular in finance, there is used normalization
to approximately quantiles by transforming x to CDF(x) with estimated CDF
(cumulative distribution function) to nearly uniform distribution in [0,1],
allowing for simpler representations which are less likely to overfit. It seems
nearly unknown in machine learning, therefore, we would like to present some
its advantages on example of recently popular Kolmogorov-Arnold Networks
(KANs), improving predictions from Legendre-KAN by just switching rescaling to
CDF normalization. Additionally, in HCR interpretation, weights of such neurons
are mixed moments providing local joint distribution models, allow to propagate
also probability distributions, and change propagation direction.

</details>


### [150] [Selective Embedding for Deep Learning](https://arxiv.org/abs/2507.13399)
*Mert Sehri,Zehui Hua,Francisco de Assis Boldt,Patrick Dumond*

Main category: cs.LG

TL;DR: The paper introduces selective embedding, a data loading strategy for deep learning that alternates short data segments from multiple sources in a single channel, improving generalization and efficiency.


<details>
  <summary>Details</summary>
Motivation: Deep learning struggles with nonstationary conditions and dissimilar domains, especially in time-domain data, due to sensitivity to input data and high computational costs of existing strategies.

Method: Proposes selective embedding, mimicking human-like information processing, to alternate data segments from multiple sources within one input channel.

Result: Validated on six time-domain datasets, the method achieves high classification accuracy across architectures and reduces training time.

Conclusion: Selective embedding is scalable, resource-efficient, and effective for complex systems with multiple data sources, benefiting industries like healthcare and agriculture.

Abstract: Deep learning has revolutionized many industries by enabling models to
automatically learn complex patterns from raw data, reducing dependence on
manual feature engineering. However, deep learning algorithms are sensitive to
input data, and performance often deteriorates under nonstationary conditions
and across dissimilar domains, especially when using time-domain data.
Conventional single-channel or parallel multi-source data loading strategies
either limit generalization or increase computational costs. This study
introduces selective embedding, a novel data loading strategy, which alternates
short segments of data from multiple sources within a single input channel.
Drawing inspiration from cognitive psychology, selective embedding mimics
human-like information processing to reduce model overfitting, enhance
generalization, and improve computational efficiency. Validation is conducted
using six time-domain datasets, demonstrating that the proposed method
consistently achieves high classification accuracy across various deep learning
architectures while significantly reducing training times. The approach proves
particularly effective for complex systems with multiple data sources, offering
a scalable and resource-efficient solution for real-world applications in
healthcare, heavy machinery, marine, railway, and agriculture, where robustness
and adaptability are critical.

</details>


### [151] [LightAutoDS-Tab: Multi-AutoML Agentic System for Tabular Data](https://arxiv.org/abs/2507.13413)
*Aleksey Lapin,Igor Hromov,Stanislav Chumakov,Mile Mitrovic,Dmitry Simakov,Nikolay O. Nikitin,Andrey V. Savchenko*

Main category: cs.LG

TL;DR: LightAutoDS-Tab is a multi-AutoML system for tabular data, combining LLM-based code generation with AutoML tools, outperforming existing solutions.


<details>
  <summary>Details</summary>
Motivation: AutoML's efficiency is limited by tool dependency; this paper aims to enhance flexibility and robustness in pipeline design.

Method: Integrates LLM-based code generation with multiple AutoML tools for tabular data tasks.

Result: Outperforms state-of-the-art open-source solutions on Kaggle tasks.

Conclusion: LightAutoDS-Tab improves AutoML flexibility and performance, with code openly available.

Abstract: AutoML has advanced in handling complex tasks using the integration of LLMs,
yet its efficiency remains limited by dependence on specific underlying tools.
In this paper, we introduce LightAutoDS-Tab, a multi-AutoML agentic system for
tasks with tabular data, which combines an LLM-based code generation with
several AutoML tools. Our approach improves the flexibility and robustness of
pipeline design, outperforming state-of-the-art open-source solutions on
several data science tasks from Kaggle. The code of LightAutoDS-Tab is
available in the open repository https://github.com/sb-ai-lab/LADS

</details>


### [152] [Gauge Flow Models](https://arxiv.org/abs/2507.13414)
*Alexander Strunk,Roland Assam*

Main category: cs.LG

TL;DR: Gauge Flow Models, a new class of Generative Flow Models, outperform traditional Flow Models in experiments, showing promise for broader generative tasks.


<details>
  <summary>Details</summary>
Motivation: To improve generative modeling by incorporating a learnable Gauge Field into Flow ODEs, enhancing performance.

Method: Introduces Gauge Flow Models with a mathematical framework and tests them using Flow Matching on Gaussian Mixture Models.

Result: Gauge Flow Models achieve significantly better performance than traditional Flow Models, even when smaller.

Conclusion: Gauge Flow Models show superior performance and potential for broader applications in generative tasks.

Abstract: This paper introduces Gauge Flow Models, a novel class of Generative Flow
Models. These models incorporate a learnable Gauge Field within the Flow
Ordinary Differential Equation (ODE). A comprehensive mathematical framework
for these models, detailing their construction and properties, is provided.
Experiments using Flow Matching on Gaussian Mixture Models demonstrate that
Gauge Flow Models yields significantly better performance than traditional Flow
Models of comparable or even larger size. Additionally, unpublished research
indicates a potential for enhanced performance across a broader range of
generative tasks.

</details>


### [153] [Single- to multi-fidelity history-dependent learning with uncertainty quantification and disentanglement: application to data-driven constitutive modeling](https://arxiv.org/abs/2507.13416)
*Jiaxiang Yi,Bernardo P. Ferreira,Miguel A. Bessa*

Main category: cs.LG

TL;DR: The paper generalizes data-driven learning to handle history-dependent multi-fidelity data, quantifying epistemic uncertainty and separating it from noise. It adapts to various learning scenarios, from simple deterministic networks to Bayesian recurrent networks, and demonstrates effectiveness in constitutive modeling.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of quantifying and disentangling epistemic and aleatoric uncertainties in data-driven learning, especially for multi-fidelity data.

Method: Proposes a hierarchical, adaptable framework for data-driven learning, ranging from deterministic neural networks to Bayesian recurrent neural networks, applied to multi-fidelity constitutive modeling.

Result: The method accurately predicts responses, quantifies model error, and identifies noise distributions, proving versatile for various scenarios.

Conclusion: The framework opens new possibilities for real-world applications in scientific and engineering domains, particularly in uncertainty-aware design and analysis.

Abstract: Data-driven learning is generalized to consider history-dependent
multi-fidelity data, while quantifying epistemic uncertainty and disentangling
it from data noise (aleatoric uncertainty). This generalization is hierarchical
and adapts to different learning scenarios: from training the simplest
single-fidelity deterministic neural networks up to the proposed multi-fidelity
variance estimation Bayesian recurrent neural networks. The versatility and
generality of the proposed methodology are demonstrated by applying it to
different data-driven constitutive modeling scenarios that include multiple
fidelities with and without aleatoric uncertainty (noise). The method
accurately predicts the response and quantifies model error while also
discovering the noise distribution (when present). This opens opportunities for
future real-world applications in diverse scientific and engineering domains;
especially, the most challenging cases involving design and analysis under
uncertainty.

</details>


### [154] [Soft-ECM: An extension of Evidential C-Means for complex data](https://arxiv.org/abs/2507.13417)
*Armel Soubeiga,Thomas Guyet,Violaine Antoine*

Main category: cs.LG

TL;DR: The paper introduces Soft-ECM, a belief function-based clustering algorithm for complex data like mixed and non-tabular data, outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: Existing belief function clustering algorithms fail to handle complex data types like mixed or non-tabular data, limiting their applicability.

Method: The authors reformulate the Evidential C-Means (ECM) problem and propose Soft-ECM, which uses semi-metrics to position centroids for imprecise clusters.

Result: Soft-ECM matches conventional fuzzy clustering on numerical data and excels with mixed data and time series using semi-metrics like DTW.

Conclusion: Soft-ECM effectively extends belief function clustering to complex data, offering improved flexibility and performance.

Abstract: Clustering based on belief functions has been gaining increasing attention in
the machine learning community due to its ability to effectively represent
uncertainty and/or imprecision. However, none of the existing algorithms can be
applied to complex data, such as mixed data (numerical and categorical) or
non-tabular data like time series. Indeed, these types of data are, in general,
not represented in a Euclidean space and the aforementioned algorithms make use
of the properties of such spaces, in particular for the construction of
barycenters. In this paper, we reformulate the Evidential C-Means (ECM) problem
for clustering complex data. We propose a new algorithm, Soft-ECM, which
consistently positions the centroids of imprecise clusters requiring only a
semi-metric. Our experiments show that Soft-ECM present results comparable to
conventional fuzzy clustering approaches on numerical data, and we demonstrate
its ability to handle mixed data and its benefits when combining fuzzy
clustering with semi-metrics such as DTW for time series data.

</details>


### [155] [Air Traffic Controller Task Demand via Graph Neural Networks: An Interpretable Approach to Airspace Complexity](https://arxiv.org/abs/2507.13423)
*Edward Henderson,Dewi Gould,Richard Everson,George De Ath,Nick Pepper*

Main category: cs.LG

TL;DR: The paper introduces an interpretable Graph Neural Network (GNN) framework to predict Air Traffic Controller (ATCO) task demand, outperforming existing methods and providing actionable insights.


<details>
  <summary>Details</summary>
Motivation: Existing complexity metrics fail to capture nuanced operational drivers in crowded airspace, necessitating a better tool for real-time task demand assessment.

Method: An attention-based GNN predicts upcoming clearances from static traffic scenarios, with interpretable task demand scores derived via systematic ablation.

Result: The framework outperforms ATCO-inspired heuristics and established baselines, reliably estimating scenario complexity.

Conclusion: The tool offers a novel way to analyze complexity drivers, useful for controller training and airspace redesign.

Abstract: Real-time assessment of near-term Air Traffic Controller (ATCO) task demand
is a critical challenge in an increasingly crowded airspace, as existing
complexity metrics often fail to capture nuanced operational drivers beyond
simple aircraft counts. This work introduces an interpretable Graph Neural
Network (GNN) framework to address this gap. Our attention-based model predicts
the number of upcoming clearances, the instructions issued to aircraft by
ATCOs, from interactions within static traffic scenarios. Crucially, we derive
an interpretable, per-aircraft task demand score by systematically ablating
aircraft and measuring the impact on the model's predictions. Our framework
significantly outperforms an ATCO-inspired heuristic and is a more reliable
estimator of scenario complexity than established baselines. The resulting tool
can attribute task demand to specific aircraft, offering a new way to analyse
and understand the drivers of complexity for applications in controller
training and airspace redesign.

</details>


### [156] [Improving Out-of-distribution Human Activity Recognition via IMU-Video Cross-modal Representation Learning](https://arxiv.org/abs/2507.13482)
*Seyyed Saeid Cheshmi,Buyao Lyu,Thomas Lisko,Rajesh Rajamani,Robert A. McGovern,Yogatheesan Varatharajah*

Main category: cs.LG

TL;DR: A cross-modal self-supervised pretraining approach for HAR using IMU-video data improves generalizability in OOD datasets, outperforming current methods.


<details>
  <summary>Details</summary>
Motivation: To address the lack of generalizability in existing HAR methods for IMU data, especially in diverse environments or populations like Parkinson's patients.

Method: Proposes a cross-modal self-supervised pretraining approach using large-scale unlabeled IMU-video data.

Result: Outperforms state-of-the-art IMU-video and IMU-only pretraining in zero-shot and few-shot evaluations on OOD datasets.

Conclusion: Cross-modal pretraining is effective for learning generalizable representations in dynamic data modalities like IMU signals.

Abstract: Human Activity Recognition (HAR) based on wearable inertial sensors plays a
critical role in remote health monitoring. In patients with movement disorders,
the ability to detect abnormal patient movements in their home environments can
enable continuous optimization of treatments and help alert caretakers as
needed. Machine learning approaches have been proposed for HAR tasks using
Inertial Measurement Unit (IMU) data; however, most rely on
application-specific labels and lack generalizability to data collected in
different environments or populations. To address this limitation, we propose a
new cross-modal self-supervised pretraining approach to learn representations
from large-sale unlabeled IMU-video data and demonstrate improved
generalizability in HAR tasks on out of distribution (OOD) IMU datasets,
including a dataset collected from patients with Parkinson's disease.
Specifically, our results indicate that the proposed cross-modal pretraining
approach outperforms the current state-of-the-art IMU-video pretraining
approach and IMU-only pretraining under zero-shot and few-shot evaluations.
Broadly, our study provides evidence that in highly dynamic data modalities,
such as IMU signals, cross-modal pretraining may be a useful tool to learn
generalizable data representations. Our software is available at
https://github.com/scheshmi/IMU-Video-OOD-HAR.

</details>


### [157] [Model-free Reinforcement Learning for Model-based Control: Towards Safe, Interpretable and Sample-efficient Agents](https://arxiv.org/abs/2507.13491)
*Thomas Banker,Ali Mesbah*

Main category: cs.LG

TL;DR: The paper explores model-based agents as an alternative to model-free RL for safer, more interpretable, and sample-efficient decision-making in autonomous systems.


<details>
  <summary>Details</summary>
Motivation: Addressing the limitations of model-free RL, such as sample inefficiency, unsafe learning, and lack of interpretability, by leveraging model-based approaches.

Method: Introduces model-based agents using adaptable models of system dynamics, cost, and constraints. Combines these with model-free RL to remedy model mismatch.

Result: Highlights the benefits of model-based agents (e.g., safety, interpretability) and outlines learning approaches like Bayesian optimization and policy search RL.

Conclusion: Advocates for combining model-free RL and model-based agents to achieve sample-efficient, safe, and interpretable decision-making.

Abstract: Training sophisticated agents for optimal decision-making under uncertainty
has been key to the rapid development of modern autonomous systems across
fields. Notably, model-free reinforcement learning (RL) has enabled
decision-making agents to improve their performance directly through system
interactions, with minimal prior knowledge about the system. Yet, model-free RL
has generally relied on agents equipped with deep neural network function
approximators, appealing to the networks' expressivity to capture the agent's
policy and value function for complex systems. However, neural networks amplify
the issues of sample inefficiency, unsafe learning, and limited
interpretability in model-free RL. To this end, this work introduces
model-based agents as a compelling alternative for control policy
approximation, leveraging adaptable models of system dynamics, cost, and
constraints for safe policy learning. These models can encode prior system
knowledge to inform, constrain, and aid in explaining the agent's decisions,
while deficiencies due to model mismatch can be remedied with model-free RL. We
outline the benefits and challenges of learning model-based agents --
exemplified by model predictive control -- and detail the primary learning
approaches: Bayesian optimization, policy search RL, and offline strategies,
along with their respective strengths. While model-free RL has long been
established, its interplay with model-based agents remains largely unexplored,
motivating our perspective on their combined potentials for sample-efficient
learning of safe and interpretable decision-making agents.

</details>


### [158] [Fake or Real: The Impostor Hunt in Texts for Space Operations](https://arxiv.org/abs/2507.13508)
*Agata Kaczmarek,Dawid Płudowski,Piotr Wilczyński,Przemysław Biecek,Krzysztof Kotowski,Ramez Shendy,Jakub Nalepa,Artur Janicki,Evridiki Ntagiou*

Main category: cs.LG

TL;DR: The Kaggle competition 'Fake or Real' focuses on detecting maliciously modified outputs from Large Language Models (LLMs), addressing AI security threats like data poisoning and overreliance.


<details>
  <summary>Details</summary>
Motivation: The competition aims to tackle under-researched AI security threats, specifically data poisoning and overreliance in LLMs, identified in the ESA-funded project.

Method: Participants must develop or adapt techniques to distinguish between genuine and maliciously altered LLM outputs.

Result: The competition seeks innovative solutions to detect manipulated LLM outputs, advancing research in AI security.

Conclusion: The initiative highlights the need for robust methods to ensure AI reliability in critical applications like space domain.

Abstract: The "Fake or Real" competition hosted on Kaggle
(\href{https://www.kaggle.com/competitions/fake-or-real-the-impostor-hunt}{https://www.kaggle.com/competitions/fake-or-real-the-impostor-hunt})
is the second part of a series of follow-up competitions and hackathons related
to the "Assurance for Space Domain AI Applications" project funded by the
European Space Agency
(\href{https://assurance-ai.space-codev.org/}{https://assurance-ai.space-codev.org/}).
The competition idea is based on two real-life AI security threats identified
within the project -- data poisoning and overreliance in Large Language Models.
The task is to distinguish between the proper output from LLM and the output
generated under malicious modification of the LLM. As this problem was not
extensively researched, participants are required to develop new techniques to
address this issue or adjust already existing ones to this problem's statement.

</details>


### [159] [Provable Low-Frequency Bias of In-Context Learning of Representations](https://arxiv.org/abs/2507.13540)
*Yongyi Yang,Hidenori Tanaka,Wei Hu*

Main category: cs.LG

TL;DR: The paper explains how in-context learning (ICL) in large language models (LLMs) works through a double convergence framework, leading to smooth representations and robustness to noise.


<details>
  <summary>Details</summary>
Motivation: To uncover the mechanisms behind ICL in LLMs, which surpass pretraining but lack theoretical understanding.

Method: Introduces a unified framework of double convergence (over context and layers) and analyzes implicit bias towards smooth representations.

Result: Explains empirical observations like structured geometry and energy decay, and predicts noise robustness, confirmed empirically.

Conclusion: Provides theoretical insights into ICL mechanisms, offering a foundation for broader studies.

Abstract: In-context learning (ICL) enables large language models (LLMs) to acquire new
behaviors from the input sequence alone without any parameter updates. Recent
studies have shown that ICL can surpass the original meaning learned in
pretraining stage through internalizing the structure the data-generating
process (DGP) of the prompt into the hidden representations. However, the
mechanisms by which LLMs achieve this ability is left open. In this paper, we
present the first rigorous explanation of such phenomena by introducing a
unified framework of double convergence, where hidden representations converge
both over context and across layers. This double convergence process leads to
an implicit bias towards smooth (low-frequency) representations, which we prove
analytically and verify empirically. Our theory explains several open empirical
observations, including why learned representations exhibit globally structured
but locally distorted geometry, and why their total energy decays without
vanishing. Moreover, our theory predicts that ICL has an intrinsic robustness
towards high-frequency noise, which we empirically confirm. These results
provide new insights into the underlying mechanisms of ICL, and a theoretical
foundation to study it that hopefully extends to more general data
distributions and settings.

</details>


### [160] [Acoustic Index: A Novel AI-Driven Parameter for Cardiac Disease Risk Stratification Using Echocardiography](https://arxiv.org/abs/2507.13542)
*Beka Begiashvili,Carlos J. Fernandez-Candel,Matías Pérez Paredes*

Main category: cs.LG

TL;DR: The paper introduces the Acoustic Index, an AI-derived echocardiographic parameter for early detection of cardiac dysfunction, outperforming traditional metrics like EF and GLS.


<details>
  <summary>Details</summary>
Motivation: Traditional echocardiographic parameters (EF, GLS) are limited in early detection due to their dependency on load conditions and vendor variability. A need exists for reproducible, interpretable, and operator-independent alternatives.

Method: The Acoustic Index combines Extended Dynamic Mode Decomposition (EDMD) with a hybrid neural network, integrating clinical metadata and spatiotemporal dynamics from echocardiographic sequences. Attention mechanisms and manifold learning fuse data into a continuous risk score (0-1).

Result: In a 736-patient cohort, the Acoustic Index achieved an AUC of 0.89, with cross-validation showing sensitivity and specificity >0.8. It demonstrated stable performance across thresholds.

Conclusion: The Acoustic Index is a promising, scalable, and vendor-independent AI biomarker for early cardiac dysfunction detection, with potential for triage and monitoring. Future work includes external validation and disease-specific adaptations.

Abstract: Traditional echocardiographic parameters such as ejection fraction (EF) and
global longitudinal strain (GLS) have limitations in the early detection of
cardiac dysfunction. EF often remains normal despite underlying pathology, and
GLS is influenced by load conditions and vendor variability. There is a growing
need for reproducible, interpretable, and operator-independent parameters that
capture subtle and global cardiac functional alterations.
  We introduce the Acoustic Index, a novel AI-derived echocardiographic
parameter designed to quantify cardiac dysfunction from standard ultrasound
views. The model combines Extended Dynamic Mode Decomposition (EDMD) based on
Koopman operator theory with a hybrid neural network that incorporates clinical
metadata. Spatiotemporal dynamics are extracted from echocardiographic
sequences to identify coherent motion patterns. These are weighted via
attention mechanisms and fused with clinical data using manifold learning,
resulting in a continuous score from 0 (low risk) to 1 (high risk).
  In a prospective cohort of 736 patients, encompassing various cardiac
pathologies and normal controls, the Acoustic Index achieved an area under the
curve (AUC) of 0.89 in an independent test set. Cross-validation across five
folds confirmed the robustness of the model, showing that both sensitivity and
specificity exceeded 0.8 when evaluated on independent data. Threshold-based
analysis demonstrated stable trade-offs between sensitivity and specificity,
with optimal discrimination near this threshold.
  The Acoustic Index represents a physics-informed, interpretable AI biomarker
for cardiac function. It shows promise as a scalable, vendor-independent tool
for early detection, triage, and longitudinal monitoring. Future directions
include external validation, longitudinal studies, and adaptation to
disease-specific classifiers.

</details>


### [161] [Time Series Forecastability Measures](https://arxiv.org/abs/2507.13556)
*Rui Wang,Steven Klee,Alexis Roos*

Main category: cs.LG

TL;DR: The paper introduces two metrics—spectral predictability score and largest Lyapunov exponent—to assess time series forecastability before model development, showing their effectiveness on synthetic and real-world data.


<details>
  <summary>Details</summary>
Motivation: To evaluate the inherent forecastability of time series data prior to model training, enabling better planning and expectation setting.

Method: Uses spectral predictability score for frequency regularity and Lyapunov exponents for chaos/stability, tested on synthetic and M5 competition datasets.

Result: Metrics accurately reflect forecastability and correlate with model performance, aiding in identifying forecastable series.

Conclusion: Pre-model forecastability assessment helps practitioners prioritize efforts and manage expectations for less forecastable data.

Abstract: This paper proposes using two metrics to quantify the forecastability of time
series prior to model development: the spectral predictability score and the
largest Lyapunov exponent. Unlike traditional model evaluation metrics, these
measures assess the inherent forecastability characteristics of the data before
any forecast attempts. The spectral predictability score evaluates the strength
and regularity of frequency components in the time series, whereas the Lyapunov
exponents quantify the chaos and stability of the system generating the data.
We evaluated the effectiveness of these metrics on both synthetic and
real-world time series from the M5 forecast competition dataset. Our results
demonstrate that these two metrics can correctly reflect the inherent
forecastability of a time series and have a strong correlation with the actual
forecast performance of various models. By understanding the inherent
forecastability of time series before model training, practitioners can focus
their planning efforts on products and supply chain levels that are more
forecastable, while setting appropriate expectations or seeking alternative
strategies for products with limited forecastability.

</details>


### [162] [Change of Thought: Adaptive Test-Time Computation](https://arxiv.org/abs/2507.13569)
*Mrinal Mathur,Mike Doan,Barak Pearlmutter,Sergey Plis*

Main category: cs.LG

TL;DR: The paper introduces the SELF-Transformer, an encoder layer that iteratively refines attention weights to boost expressive power without token-level autoregression, achieving up to 20% accuracy gains.


<details>
  <summary>Details</summary>
Motivation: Transformers with fixed-depth passes are limited in expressive power (TC0 class). Autoregressive methods rely on feedback loops, but biological brains reason without externalizing intermediate states. The goal is to enhance encoder Transformers without autoregression.

Method: The SELF-Transformer iteratively updates its attention weights internally to a fixed point, scaling computation with input difficulty. This avoids token-level autoregression while refining alignment matrices.

Result: The SELF-Transformer achieves up to 20% accuracy gains on encoder-style benchmarks without increasing parameters, showing benefits of input-adaptive alignment.

Conclusion: The SELF-Transformer recovers expressive power of iterative reasoning while maintaining encoder simplicity, offering significant performance improvements with modest extra compute.

Abstract: Transformers evaluated in a single, fixed-depth pass are provably limited in
expressive power to the constant-depth circuit class TC0. Running a Transformer
autoregressively removes that ceiling -- first in next-token prediction and,
more recently, in chain-of-thought reasoning. Both regimes rely on feedback
loops that decode internal states into tokens only to re-encode them in
subsequent steps. While this "thinking aloud" mirrors human reasoning,
biological brains iterate without externalising intermediate states as
language. To boost the expressive power of encoder Transformers without
resorting to token-level autoregression, we introduce the SELF-Transformer: an
encoder layer that iteratively refines its own attention weights to a fixed
point. Instead of producing -- in one pass -- the alignment matrix that remixes
the input sequence, the SELF-Transformer iteratively updates that matrix
internally, scaling test-time computation with input difficulty. This
adaptivity yields up to 20\% accuracy gains on encoder-style benchmarks without
increasing parameter count, demonstrating that input-adaptive alignment at test
time offers substantial benefits for only a modest extra compute budget.
Self-Transformers thus recover much of the expressive power of iterative
reasoning while preserving the simplicity of pure encoder architectures.

</details>


### [163] [Apple Intelligence Foundation Language Models: Tech Report 2025](https://arxiv.org/abs/2507.13575)
*Hanzhi Zhou,Erik Hornberger,Pengsheng Guo,Xiyou Zhou,Saiwen Wang,Xin Wang,Yifei He,Xuankai Chang,Rene Rauch,Louis D'hauwe,John Peebles,Alec Doane,Kohen Chia,Jenna Thibodeau,Zi-Yi Dou,Yuanyang Zhang,Ruoming Pang,Reed Li,Zhifeng Chen,Jeremy Warner,Zhaoyang Xu,Sophy Lee,David Mizrahi,Ramsey Tantawi,Chris Chaney,Kelsey Peterson,Jun Qin,Alex Dombrowski,Mira Chiang,Aiswarya Raghavan,Gerard Casamayor,Qibin Chen,Aonan Zhang,Nathalie Tran,Jianyu Wang,Hang Su,Thomas Voice,Alessandro Pappalardo,Brycen Wershing,Prasanth Yadla,Rui Li,Priyal Chhatrapati,Ismael Fernandez,Yusuf Goren,Xin Zheng,Forrest Huang,Tao Lei,Eray Yildiz,Alper Kokmen,Gokul Santhanam,Areeba Kamal,Kaan Elgin,Dian Ang Yap,Jeremy Liu,Peter Gray,Howard Xing,Kieran Liu,Matteo Ronchi,Moritz Schwarzer-Becker,Yun Zhu,Mandana Saebi,Jeremy Snow,David Griffiths,Guillaume Tartavel,Erin Feldman,Simon Lehnerer,Fernando Bermúdez-Medina,Hans Han,Joe Zhou,Xiaoyi Ren,Sujeeth Reddy,Zirui Wang,Tom Gunter,Albert Antony,Yuanzhi Li,John Dennison,Tony Sun,Yena Han,Yi Qin,Sam Davarnia,Jeffrey Bigham,Wayne Shan,Hannah Gillis Coleman,Guillaume Klein,Peng Liu,Muyang Yu,Jack Cackler,Yuan Gao,Crystal Xiao,Binazir Karimzadeh,Zhengdong Zhang,Felix Bai,Albin Madappally Jose,Feng Nan,Nazir Kamaldin,Dong Yin,Hans Hao,Yanchao Sun,Yi Hua,Charles Maalouf,Alex Guillen Garcia,Guoli Yin,Lezhi Li,Mohana Prasad Sathya Moorthy,Hongbin Gao,Jay Tang,Joanna Arreaza-Taylor,Faye Lao,Carina Peng,Josh Shaffer,Dan Masi,Sushma Rao,Tommi Vehvilainen,Senyu Tong,Dongcai Shen,Yang Zhao,Chris Bartels,Peter Fu,Qingqing Cao,Christopher Neubauer,Ethan Li,Mingfei Gao,Rebecca Callahan,Richard Wei,Patrick Dong,Alex Braunstein,Sachin Ravi,Adolfo Lopez Mendez,Kaiwei Huang,Kun Duan,Haoshuo Huang,Rui Qian,Stefano Ligas,Jordan Huffaker,Dongxu Li,Bailin Wang,Nanzhu Wang,Anuva Agarwal,Tait Madsen,Josh Newnham,Abhishek Sharma,Zhile Ren,Deepak Gopinath,Erik Daxberger,Saptarshi Guha,Oron Levy,Jing Lu,Nan Dun,Marc Kirchner,Yinfei Yang,Manjot Bilkhu,Dave Nelson,Anthony Spalvieri-Kruse,Juan Lao Tebar,Yang Xu,Phani Mutyala,Gabriel Jacoby-Cooper,Yingbo Wang,Karla Vega,Vishaal Mahtani,Darren Botten,Eric Wang,Hanli Li,Matthias Paulik,Haoran Yan,Navid Shiee,Yihao Qian,Bugu Wu,Qi Zhu,Ob Adaranijo,Bhuwan Dhingra,Zhe Gan,Nicholas Seidl,Grace Duanmu,Rong Situ,Yiping Ma,Yin Xia,David Riazati,Vasileios Saveris,Anh Nguyen,Michael,Lee,Patrick Sonnenberg,Chinguun Erdenebileg,Yanghao Li,Vivian Ma,James Chou,Isha Garg,Mark Lee,Keen You,Yuhong Li,Ransen Niu,Nandhitha Raghuram,Pulkit Agrawal,Henry Mason,Sumeet Singh,Keyu He,Hong-You Chen,Lucas Guibert,Shiyu Li,Varsha Paidi,Narendran Raghavan,Mingze Xu,Yuli Yang,Sergiu Sima,Irina Belousova,Sprite Chu,Afshin Dehghan,Philipp Dufter,David Haldimann,Zhen Yang,Margit Bowler,Chang Liu,Ying-Chang Cheng,Vivek Rathod,Syd Evans,Wilson Tsao,Dustin Withers,Haitian Sun,Biyao Wang,Peter Grasch,Walker Cheng,Yihao Feng,Vivek Kumar,Frank Chu,Victoria MönchJuan Haladjian,Doug Kang,Jiarui Lu,Ciro Sannino,Max Lam,Floris Weers,Bowen Pan,Kenneth Jung,Dhaval Doshi,Fangping Shi,Olli Saarikivi,Alp Aygar,Josh Elman,Cheng Leong,Eshan Verma,Matthew Lei,Jeff Nichols,Jiulong Shan,Donald Zhang,Lawrence Zhou,Stephen Murphy,Xianzhi Du,Chang Lan,Ankur Jain,Elmira Amirloo,Marcin Eichner,Naomy Sabo,Anupama Mann Anupama,David Qiu,Zhao Meng,Michael FitzMaurice,Peng Zhang,Simon Yeung,Chen Chen,Marco Zuliani,Andrew Hansen,Yang Lu,Brent Ramerth,Ziyi Zhong,Parsa Mazaheri,Matthew Hopkins,Mengyu Li,Simon Wang,David Chen,Farzin Rasteh,Chong Wang,Josh Gardner,Asaf Liberman,Haoxuan You,Andrew Walkingshaw,Xingyu Zhou,Jinhao Lei,Yan Meng,Quentin Keunebroek,Sam Wiseman,Anders Boesen Lindbo Larsen,Yi Zhang,Zaid Ahmed,Haiming Gang,Aaron Franklin,Kelvin Zou,Guillaume Seguin,Jonathan Janke,Rachel Burger,Co Giang,Cheng Shen,Jen Liu,Sanskruti Shah,Xiang Kong,Yiran Fei,TJ Collins,Chen Zhang,Zhiyun Lu,Michael Booker,Qin Ba,Yasutaka Tanaka,Andres Romero Mier Y Teran,Federico Scozzafava,Regan Poston,Jane Li,Eduardo Jimenez,Bas Straathof,Karanjeet Singh,Lindsay Hislop,Rajat Arora,Deepa Seshadri,Boyue Li,Colorado Reed,Zhen Li,TJ Lu,Yi Wang,Kaelen Haag,Nicholas Lusskin,Raunak Sinha,Rahul Nair,Eldon Schoop,Mary Beth Kery,Mehrdad Farajtbar,Brenda Yang,George Horrell,Shiwen Zhao,Dhruti Shah,Cha Chen,Bowen Zhang,Chang Gao,Devi Krishna,Jennifer Mallalieu,Javier Movellan,Di Feng,Emily Zhang,Sam Xu,Junting Pan,Dominik Moritz,Suma Jayaram,Kevin Smith,Dongseong Hwang,Daniel Parilla,Jiaming Hu,You-Cyuan Jhang,Emad Soroush,Fred Hohman,Nan Du,Emma Wang,Sam Dodge,Pragnya Sridhar,Joris Pelemans,Wei Fang,Nina Wenzel,Joseph Yitan Cheng,Hadas Kotek,Chung-Cheng Chiu,Meng Cao,Haijing Fu,Ruixuan Hou,Ke Ye,Diane Zhu,Nikhil Bhendawade,Joseph Astrauskas,Jian Liu,Sai Aitharaju,Wentao Wu,Artsiom Peshko,Hyunjik Kim,Nilesh Shahdadpuri,Andy De Wang,Qi Shan,Piotr Maj,Raul Rea Menacho,Justin Lazarow,Eric Liang Yang,Arsalan Farooq,Donghan Yu,David Güera,Minsik Cho,Kavya Nerella,Yongqiang Wang,Tao Jia,John Park,Jeff Lai,Haotian Zhang,Futang Peng,Daniele Molinari,Aparna Rajamani,Tyler Johnson,Lauren Gardiner,Chao Jia,Violet Yao,Wojciech Kryscinski,Xiujun Li,Shang-Chen Wu*

Main category: cs.LG

TL;DR: Apple introduces two multilingual, multimodal foundation models (3B-parameter on-device and scalable server model) with advanced architectures, trained on diverse datasets, and refined for high performance and privacy.


<details>
  <summary>Details</summary>
Motivation: To enhance Apple Intelligence features across devices and services with efficient, high-quality, and privacy-focused AI models.

Method: Utilizes KV-cache sharing, 2-bit quantization-aware training (on-device model), and Parallel-Track Mixture-of-Experts (PT-MoE) transformer (server model). Training involves multilingual/multimodal datasets, supervised fine-tuning, and reinforcement learning.

Result: Models outperform comparably sized open benchmarks, support additional languages, and handle images/tool calls. A Swift framework simplifies integration.

Conclusion: Apple's models advance AI capabilities while prioritizing privacy and responsible AI practices.

Abstract: We introduce two multilingual, multimodal foundation language models that
power Apple Intelligence features across Apple devices and services: i a
3B-parameter on-device model optimized for Apple silicon through architectural
innovations such as KV-cache sharing and 2-bit quantization-aware training; and
ii a scalable server model built on a novel Parallel-Track Mixture-of-Experts
PT-MoE transformer that combines track parallelism, mixture-of-experts sparse
computation, and interleaved global-local attention to deliver high quality
with competitive cost on Apple's Private Cloud Compute platform. Both models
are trained on large-scale multilingual and multimodal datasets sourced via
responsible web crawling, licensed corpora, and high-quality synthetic data,
then further refined with supervised fine-tuning and reinforcement learning on
a new asynchronous platform. The resulting models support several additional
languages while understanding images and executing tool calls. In public
benchmarks and human evaluations, both the server model and the on-device model
match or surpass comparably sized open baselines.
  A new Swift-centric Foundation Models framework exposes guided generation,
constrained tool calling, and LoRA adapter fine-tuning, allowing developers to
integrate these capabilities with a few lines of code. The latest advancements
in Apple Intelligence models are grounded in our Responsible AI approach with
safeguards like content filtering and locale-specific evaluation, as well as
our commitment to protecting our users' privacy with innovations like Private
Cloud Compute.

</details>


### [164] [Learning Pluralistic User Preferences through Reinforcement Learning Fine-tuned Summaries](https://arxiv.org/abs/2507.13579)
*Hyunji Nam,Yanming Wan,Mickel Liu,Jianxun Lian,Natasha Jaques*

Main category: cs.LG

TL;DR: PLUS is a framework for personalizing LLM responses by learning user-specific summaries to condition reward models, outperforming traditional RLHF and enabling zero-shot personalization for models like GPT-4.


<details>
  <summary>Details</summary>
Motivation: Current RLHF methods lack personalization, modeling all users with a single reward model, which fails to address individual preferences and goals.

Method: PLUS learns text-based summaries of user preferences and past conversations to condition the reward model, trained via reinforcement learning in an online co-adaptation loop.

Result: PLUS captures meaningful user preferences, works robustly across diverse users and topics, and enables zero-shot personalization for proprietary models.

Conclusion: PLUS improves LLM personalization, offering concise, interpretable summaries that enhance transparency and user control.

Abstract: As everyday use cases of large language model (LLM) AI assistants have
expanded, it is becoming increasingly important to personalize responses to
align to different users' preferences and goals. While reinforcement learning
from human feedback (RLHF) is effective at improving LLMs to be generally more
helpful and fluent, it does not account for variability across users, as it
models the entire user population with a single reward model. We present a
novel framework, Preference Learning Using Summarization (PLUS), that learns
text-based summaries of each user's preferences, characteristics, and past
conversations. These summaries condition the reward model, enabling it to make
personalized predictions about the types of responses valued by each user. We
train the user-summarization model with reinforcement learning, and update the
reward model simultaneously, creating an online co-adaptation loop. We show
that in contrast with prior personalized RLHF techniques or with in-context
learning of user information, summaries produced by PLUS capture meaningful
aspects of a user's preferences. Across different pluralistic user datasets, we
show that our method is robust to new users and diverse conversation topics.
Additionally, we demonstrate that the textual summaries generated about users
can be transferred for zero-shot personalization of stronger, proprietary
models like GPT-4. The resulting user summaries are not only concise and
portable, they are easy for users to interpret and modify, allowing for more
transparency and user control in LLM alignment.

</details>


### [165] [Off-Policy Evaluation and Learning for Matching Markets](https://arxiv.org/abs/2507.13608)
*Yudai Hayashi,Shuhei Goda,Yuta Saito*

Main category: cs.LG

TL;DR: Proposes novel OPE estimators (DiPS and DPR) for matching markets, addressing variance and reward sparsity issues, outperforming conventional methods in offline evaluation and policy learning.


<details>
  <summary>Details</summary>
Motivation: A/B tests are costly for frequent policy updates in matching markets, and standard OPE methods are unreliable due to large-scale, bidirectional interactions.

Method: Combines DM, IPS, and DR estimators with intermediate labels for better bias-variance control. Theoretically analyzes bias and variance, and extends to offline policy learning.

Result: Empirical evaluation on synthetic and real job-matching data shows superiority over existing methods in OPE and policy learning.

Conclusion: DiPS and DPR effectively address challenges in matching markets, enabling reliable offline evaluation and improved recommendation policies.

Abstract: Matching users based on mutual preferences is a fundamental aspect of
services driven by reciprocal recommendations, such as job search and dating
applications. Although A/B tests remain the gold standard for evaluating new
policies in recommender systems for matching markets, it is costly and
impractical for frequent policy updates. Off-Policy Evaluation (OPE) thus plays
a crucial role by enabling the evaluation of recommendation policies using only
offline logged data naturally collected on the platform. However, unlike
conventional recommendation settings, the large scale and bidirectional nature
of user interactions in matching platforms introduce variance issues and
exacerbate reward sparsity, making standard OPE methods unreliable. To address
these challenges and facilitate effective offline evaluation, we propose novel
OPE estimators, \textit{DiPS} and \textit{DPR}, specifically designed for
matching markets. Our methods combine elements of the Direct Method (DM),
Inverse Propensity Score (IPS), and Doubly Robust (DR) estimators while
incorporating intermediate labels, such as initial engagement signals, to
achieve better bias-variance control in matching markets. Theoretically, we
derive the bias and variance of the proposed estimators and demonstrate their
advantages over conventional methods. Furthermore, we show that these
estimators can be seamlessly extended to offline policy learning methods for
improving recommendation policies for making more matches. We empirically
evaluate our methods through experiments on both synthetic data and A/B testing
logs from a real job-matching platform. The empirical results highlight the
superiority of our approach over existing methods in off-policy evaluation and
learning tasks for a variety of configurations.

</details>


### [166] [Tri-Learn Graph Fusion Network for Attributed Graph Clustering](https://arxiv.org/abs/2507.13620)
*Binxiong Li,Yuefei Wang,Xu Xiang,Xue Li,Binyu Zhao,Heyang Gao,Qinyu Zhao,Xi Yu*

Main category: cs.LG

TL;DR: The paper introduces Tri-GFN, a deep clustering framework combining GCN, AE, and Graph Transformer to address challenges like over-smoothing and over-compression in graph data analysis. It outperforms state-of-the-art methods on several datasets.


<details>
  <summary>Details</summary>
Motivation: Challenges like over-smoothing and over-compression in GCN-based models and limited performance of Graph Transformers on heterogeneous graph data motivated the development of a more robust framework.

Method: Proposes Tri-GFN, integrating GCN, AE, and Graph Transformer with a tri-learning mechanism and feature fusion strategy to enhance global and local information differentiation.

Result: Achieves accuracy improvements of 0.87% (ACM), 14.14% (Reuters), and 7.58% (USPS), demonstrating superior performance.

Conclusion: Tri-GFN's effectiveness makes it suitable for applications like automatic news classification and topic retrieval, especially due to its performance on the Reuters dataset.

Abstract: In recent years, models based on Graph Convolutional Networks (GCN) have made
significant strides in the field of graph data analysis. However, challenges
such as over-smoothing and over-compression remain when handling large-scale
and complex graph datasets, leading to a decline in clustering quality.
Although the Graph Transformer architecture has mitigated some of these issues,
its performance is still limited when processing heterogeneous graph data. To
address these challenges, this study proposes a novel deep clustering framework
that comprising GCN, Autoencoder (AE), and Graph Transformer, termed the
Tri-Learn Graph Fusion Network (Tri-GFN). This framework enhances the
differentiation and consistency of global and local information through a
unique tri-learning mechanism and feature fusion enhancement strategy. The
framework integrates GCN, AE, and Graph Transformer modules. These components
are meticulously fused by a triple-channel enhancement module, which maximizes
the use of both node attributes and topological structures, ensuring robust
clustering representation. The tri-learning mechanism allows mutual learning
among these modules, while the feature fusion strategy enables the model to
capture complex relationships, yielding highly discriminative representations
for graph clustering. It surpasses many state-of-the-art methods, achieving an
accuracy improvement of approximately 0.87% on the ACM dataset, 14.14 % on the
Reuters dataset, and 7.58 % on the USPS dataset. Due to its outstanding
performance on the Reuters dataset, Tri-GFN can be applied to automatic news
classification, topic retrieval, and related fields.

</details>


### [167] [FedSkipTwin: Digital-Twin-Guided Client Skipping for Communication-Efficient Federated Learning](https://arxiv.org/abs/2507.13624)
*Daniel Commey,Kamel Abbad,Garth V. Crosby,Lyes Khoukhi*

Main category: cs.LG

TL;DR: FedSkipTwin reduces FL communication overhead by 12-15.5% using server-side digital twins to predict client updates, improving accuracy by 0.5% over FedAvg.


<details>
  <summary>Details</summary>
Motivation: Communication overhead in FL, especially for mobile/IoT devices with limited bandwidth, is a major bottleneck.

Method: FedSkipTwin uses LSTM-based digital twins to predict client update magnitude and uncertainty, skipping rounds when predictions fall below thresholds.

Result: Reduced communication by 12-15.5% and improved accuracy by 0.5% on UCI-HAR and MNIST datasets.

Conclusion: Prediction-guided skipping is effective for resource-aware FL in bandwidth-constrained environments.

Abstract: Communication overhead remains a primary bottleneck in federated learning
(FL), particularly for applications involving mobile and IoT devices with
constrained bandwidth. This work introduces FedSkipTwin, a novel
client-skipping algorithm driven by lightweight, server-side digital twins.
Each twin, implemented as a simple LSTM, observes a client's historical
sequence of gradient norms to forecast both the magnitude and the epistemic
uncertainty of its next update. The server leverages these predictions,
requesting communication only when either value exceeds a predefined threshold;
otherwise, it instructs the client to skip the round, thereby saving bandwidth.
Experiments are conducted on the UCI-HAR and MNIST datasets with 10 clients
under a non-IID data distribution. The results demonstrate that FedSkipTwin
reduces total communication by 12-15.5% across 20 rounds while simultaneously
improving final model accuracy by up to 0.5 percentage points compared to the
standard FedAvg algorithm. These findings establish that prediction-guided
skipping is a practical and effective strategy for resource-aware FL in
bandwidth-constrained edge environments.

</details>


### [168] [Generalist Bimanual Manipulation via Foundation Video Diffusion Models](https://arxiv.org/abs/2507.12898)
*Yao Feng,Hengkai Tan,Xinyi Mao,Guodong Liu,Shuhe Huang,Chendong Xiang,Hang Su,Jun Zhu*

Main category: cs.LG

TL;DR: VIDAR is a two-stage framework using video diffusion pre-training and masked inverse dynamics for bimanual robotic manipulation, achieving strong generalization with minimal data.


<details>
  <summary>Details</summary>
Motivation: Addressing data scarcity and embodiment heterogeneity in bimanual robotic manipulation to enable scalable solutions.

Method: Leverages large-scale video diffusion pre-training and a novel masked inverse dynamics model for action prediction, trained on 750K multi-view videos.

Result: Generalizes to unseen tasks and backgrounds with only 20 minutes of human demonstrations, outperforming state-of-the-art methods.

Conclusion: VIDAR demonstrates the potential of video foundation models and masked action prediction for scalable, generalizable robotic manipulation.

Abstract: Bimanual robotic manipulation, which involves the coordinated control of two
robotic arms, is foundational for solving challenging tasks. Despite recent
progress in general-purpose manipulation, data scarcity and embodiment
heterogeneity remain serious obstacles to further scaling up in bimanual
settings. In this paper, we introduce VIdeo Diffusion for Action Reasoning
(VIDAR), a two-stage framework that leverages large-scale, diffusion-based
video pre-training and a novel masked inverse dynamics model for action
prediction. We pre-train the video diffusion model on 750K multi-view videos
from three real-world bimanual robot platforms, utilizing a unified observation
space that encodes robot, camera, task, and scene contexts. Our masked inverse
dynamics model learns masks to extract action-relevant information from
generated trajectories without requiring pixel-level labels, and the masks can
effectively generalize to unseen backgrounds. Our experiments demonstrate that
with only 20 minutes of human demonstrations on an unseen robot platform (only
1% of typical data requirements), VIDAR generalizes to unseen tasks and
backgrounds with strong semantic understanding, surpassing state-of-the-art
methods. Our findings highlight the potential of video foundation models,
coupled with masked action prediction, to enable scalable and generalizable
robotic manipulation in diverse real-world settings.

</details>


### [169] [A Comprehensive Review of Transformer-based language models for Protein Sequence Analysis and Design](https://arxiv.org/abs/2507.13646)
*Nimisha Ghosh,Daniele Santoni,Debaleena Nawn,Eleonora Ottaviani,Giovanni Felici*

Main category: cs.LG

TL;DR: A review of Transformer-based models in protein sequence analysis and design, covering applications like gene ontology, protein identification, and de novo protein generation, while highlighting strengths, weaknesses, and future directions.


<details>
  <summary>Details</summary>
Motivation: To explore the adoption and impact of Transformer-based models in bioinformatics, specifically for protein sequence analysis and design, and to provide a comprehensive overview of current research.

Method: Review and analysis of numerous works applying Transformer models to protein-related tasks, evaluating their strengths and weaknesses.

Result: Identified key applications (e.g., gene ontology, protein binding) and gaps in current research, offering insights for future advancements.

Conclusion: The review serves as a guide for researchers to understand the state of the art and directs future studies in Transformer-based protein analysis.

Abstract: The impact of Transformer-based language models has been unprecedented in
Natural Language Processing (NLP). The success of such models has also led to
their adoption in other fields including bioinformatics. Taking this into
account, this paper discusses recent advances in Transformer-based models for
protein sequence analysis and design. In this review, we have discussed and
analysed a significant number of works pertaining to such applications. These
applications encompass gene ontology, functional and structural protein
identification, generation of de novo proteins and binding of proteins. We
attempt to shed light on the strength and weaknesses of the discussed works to
provide a comprehensive insight to readers. Finally, we highlight shortcomings
in existing research and explore potential avenues for future developments. We
believe that this review will help researchers working in this field to have an
overall idea of the state of the art in this field, and to orient their future
studies.

</details>


### [170] [Kolmogorov-Arnold Networks-based GRU and LSTM for Loan Default Early Prediction](https://arxiv.org/abs/2507.13685)
*Yue Yang,Zihan Su,Ying Zhang,Chang Chuan Goh,Yuxiang Lin,Anthony Graham Bellotti,Boon Giin Lee*

Main category: cs.LG

TL;DR: The paper introduces GRU-KAN and LSTM-KAN models for early loan default prediction, achieving high accuracy (92% at 3 months, 88% at 8 months) and outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: To improve early loan default prediction beyond 3 months, addressing limitations of current methods like accuracy and temporal dependency.

Method: Proposes GRU-KAN and LSTM-KAN, combining Kolmogorov-Arnold Networks (KAN) with GRU and LSTM. Evaluated against baseline models (LSTM, GRU, etc.) on accuracy, precision, recall, F1, and AUC.

Result: Achieves 92% accuracy at 3 months and 88% at 8 months, significantly outperforming baselines.

Conclusion: The proposed models enhance early default prediction, offering practical benefits for financial institutions.

Abstract: This study addresses a critical challenge in time series anomaly detection:
enhancing the predictive capability of loan default models more than three
months in advance to enable early identification of default events, helping
financial institutions implement preventive measures before risk events
materialize. Existing methods have significant drawbacks, such as their lack of
accuracy in early predictions and their dependence on training and testing
within the same year and specific time frames. These issues limit their
practical use, particularly with out-of-time data. To address these, the study
introduces two innovative architectures, GRU-KAN and LSTM-KAN, which merge
Kolmogorov-Arnold Networks (KAN) with Gated Recurrent Units (GRU) and Long
Short-Term Memory (LSTM) networks. The proposed models were evaluated against
the baseline models (LSTM, GRU, LSTM-Attention, and LSTM-Transformer) in terms
of accuracy, precision, recall, F1 and AUC in different lengths of feature
window, sample sizes, and early prediction intervals. The results demonstrate
that the proposed model achieves a prediction accuracy of over 92% three months
in advance and over 88% eight months in advance, significantly outperforming
existing baselines.

</details>


### [171] [Binarizing Physics-Inspired GNNs for Combinatorial Optimization](https://arxiv.org/abs/2507.13703)
*Martin Krutský,Gustav Šír,Vyacheslav Kungurtsev,Georgios Korpas*

Main category: cs.LG

TL;DR: PI-GNNs show performance drops with denser problem graphs. A phase transition in training leads to degenerate solutions. Proposed methods from fuzzy logic and binarized networks improve results.


<details>
  <summary>Details</summary>
Motivation: Address the performance decline of PI-GNNs in dense combinatorial problem graphs and bridge the gap between relaxed outputs and binary solutions.

Method: Analyze PI-GNNs' training dynamics, identify phase transition, and propose alternatives inspired by fuzzy logic and binarized neural networks.

Result: Performance of PI-GNNs drops with graph density; proposed methods significantly improve results in dense settings.

Conclusion: Principled alternatives enhance PI-GNNs' effectiveness for dense combinatorial problems, addressing the discrepancy between relaxed and binary solutions.

Abstract: Physics-inspired graph neural networks (PI-GNNs) have been utilized as an
efficient unsupervised framework for relaxing combinatorial optimization
problems encoded through a specific graph structure and loss, reflecting
dependencies between the problem's variables. While the framework has yielded
promising results in various combinatorial problems, we show that the
performance of PI-GNNs systematically plummets with an increasing density of
the combinatorial problem graphs. Our analysis reveals an interesting phase
transition in the PI-GNNs' training dynamics, associated with degenerate
solutions for the denser problems, highlighting a discrepancy between the
relaxed, real-valued model outputs and the binary-valued problem solutions. To
address the discrepancy, we propose principled alternatives to the naive
strategy used in PI-GNNs by building on insights from fuzzy logic and binarized
neural networks. Our experiments demonstrate that the portfolio of proposed
methods significantly improves the performance of PI-GNNs in increasingly dense
settings.

</details>


### [172] [Bayesian Optimization for Molecules Should Be Pareto-Aware](https://arxiv.org/abs/2507.13704)
*Anabel Yong,Austin Tripp,Layla Hosseini-Gerami,Brooks Paige*

Main category: cs.LG

TL;DR: Multi-objective Bayesian optimization (MOBO) with Expected Hypervolume Improvement (EHVI) outperforms scalarized methods in molecular design tasks, especially in low-data regimes.


<details>
  <summary>Details</summary>
Motivation: To evaluate the empirical advantages of MOBO over scalarized alternatives in molecular design, particularly in low-data scenarios.

Method: Benchmarked EHVI (Pareto-based MOBO) against scalarized Expected Improvement (EI) using identical Gaussian Process surrogates and molecular representations.

Result: EHVI consistently outperformed scalarized EI in Pareto front coverage, convergence speed, and chemical diversity across three tasks.

Conclusion: Pareto-aware acquisition (EHVI) is practically advantageous for molecular optimization, especially with limited budgets and nontrivial trade-offs.

Abstract: Multi-objective Bayesian optimization (MOBO) provides a principled framework
for navigating trade-offs in molecular design. However, its empirical
advantages over scalarized alternatives remain underexplored. We benchmark a
simple Pareto-based MOBO strategy -- Expected Hypervolume Improvement (EHVI) --
against a simple fixed-weight scalarized baseline using Expected Improvement
(EI), under a tightly controlled setup with identical Gaussian Process
surrogates and molecular representations. Across three molecular optimization
tasks, EHVI consistently outperforms scalarized EI in terms of Pareto front
coverage, convergence speed, and chemical diversity. While scalarization
encompasses flexible variants -- including random or adaptive schemes -- our
results show that even strong deterministic instantiations can underperform in
low-data regimes. These findings offer concrete evidence for the practical
advantages of Pareto-aware acquisition in de novo molecular optimization,
especially when evaluation budgets are limited and trade-offs are nontrivial.

</details>


### [173] [Learning Deformable Body Interactions With Adaptive Spatial Tokenization](https://arxiv.org/abs/2507.13707)
*Hao Wang,Yu Liu,Daniel Biggs,Haoru Wang,Jiandong Yu,Ping Huang*

Main category: cs.LG

TL;DR: Proposes Adaptive Spatial Tokenization (AST) for scalable simulation of deformable body interactions using grid-based tokenization and attention mechanisms.


<details>
  <summary>Details</summary>
Motivation: Address scalability issues in learning-based methods for deformable body interactions, which struggle with large-scale meshes due to computational intensity.

Method: Divides simulation space into grid cells, maps unstructured meshes onto the grid, and uses cross-attention for compact embeddings. Self-attention predicts next states.

Result: Outperforms state-of-the-art methods, especially in large-scale simulations (100,000+ nodes), and introduces a novel dataset.

Conclusion: AST offers an efficient, scalable solution for deformable body simulations, leveraging tokenization and attention mechanisms.

Abstract: Simulating interactions between deformable bodies is vital in fields like
material science, mechanical design, and robotics. While learning-based methods
with Graph Neural Networks (GNNs) are effective at solving complex physical
systems, they encounter scalability issues when modeling deformable body
interactions. To model interactions between objects, pairwise global edges have
to be created dynamically, which is computationally intensive and impractical
for large-scale meshes. To overcome these challenges, drawing on insights from
geometric representations, we propose an Adaptive Spatial Tokenization (AST)
method for efficient representation of physical states. By dividing the
simulation space into a grid of cells and mapping unstructured meshes onto this
structured grid, our approach naturally groups adjacent mesh nodes. We then
apply a cross-attention module to map the sparse cells into a compact,
fixed-length embedding, serving as tokens for the entire physical state.
Self-attention modules are employed to predict the next state over these tokens
in latent space. This framework leverages the efficiency of tokenization and
the expressive power of attention mechanisms to achieve accurate and scalable
simulation results. Extensive experiments demonstrate that our method
significantly outperforms state-of-the-art approaches in modeling deformable
body interactions. Notably, it remains effective on large-scale simulations
with meshes exceeding 100,000 nodes, where existing methods are hindered by
computational limitations. Additionally, we contribute a novel large-scale
dataset encompassing a wide range of deformable body interactions to support
future research in this area.

</details>


### [174] [Benchmarking of EEG Analysis Techniques for Parkinson's Disease Diagnosis: A Comparison between Traditional ML Methods and Foundation DL Methods](https://arxiv.org/abs/2507.13716)
*Danilo Avola,Andrea Bernardini,Giancarlo Crocetti,Andrea Ladogana,Mario Lezoche,Maurizio Mancini,Daniele Pannone,Amedeo Ranaldi*

Main category: cs.LG

TL;DR: The paper benchmarks traditional ML and DL models for classifying Parkinson's Disease (PD) using EEG data, aiming to identify the best approach for reliable automated diagnosis.


<details>
  <summary>Details</summary>
Motivation: Early diagnosis of PD is critical, and EEG offers a non-invasive, cost-effective method. However, developing reliable automated diagnostic models is challenging.

Method: The study implements a unified preprocessing pipeline and evaluates ML and DL models (e.g., CNN-LSTM, XGBoost) using consistent cross-validation and criteria.

Result: CNN-LSTM models perform best, capturing long-range temporal dependencies, but traditional classifiers like XGBoost also show strong accuracy.

Conclusion: The study provides a reference framework for future EEG-based PD diagnosis research, emphasizing the need for rigorous baselines to ensure scientific rigor and reproducibility.

Abstract: Parkinson's Disease PD is a progressive neurodegenerative disorder that
affects motor and cognitive functions with early diagnosis being critical for
effective clinical intervention Electroencephalography EEG offers a noninvasive
and costeffective means of detecting PDrelated neural alterations yet the
development of reliable automated diagnostic models remains a challenge In this
study we conduct a systematic benchmark of traditional machine learning ML and
deep learning DL models for classifying PD using a publicly available oddball
task dataset Our aim is to lay the groundwork for developing an effective
learning system and to determine which approach produces the best results We
implement a unified sevenstep preprocessing pipeline and apply consistent
subjectwise crossvalidation and evaluation criteria to ensure comparability
across models Our results demonstrate that while baseline deep learning
architectures particularly CNNLSTM models achieve the best performance compared
to other deep learning architectures underlining the importance of capturing
longrange temporal dependencies several traditional classifiers such as XGBoost
also offer strong predictive accuracy and calibrated decision boundaries By
rigorously comparing these baselines our work provides a solid reference
framework for future studies aiming to develop and evaluate more complex or
specialized architectures Establishing a reliable set of baseline results is
essential to contextualize improvements introduced by novel methods ensuring
scientific rigor and reproducibility in the evolving field of EEGbased
neurodiagnostics

</details>


### [175] [Bi-GRU Based Deception Detection using EEG Signals](https://arxiv.org/abs/2507.13718)
*Danilo Avola,Muhammad Yasir Bilal,Emad Emam,Cristina Lakasz,Daniele Pannone,Amedeo Ranaldi*

Main category: cs.LG

TL;DR: A deep learning model using Bi-GRU neural networks achieves 97% accuracy in classifying deceptive and truthful behavior from EEG signals.


<details>
  <summary>Details</summary>
Motivation: Deception detection is crucial in security, psychology, and forensics, but challenging. This study aims to improve detection using EEG signals.

Method: A Bidirectional Gated Recurrent Unit (Bi-GRU) neural network was trained on EEG data from the Bag-of-Lies dataset for binary classification.

Result: The model achieved 97% test accuracy, with high precision, recall, and F1-scores for both deceptive and truthful classes.

Conclusion: The results show the effectiveness of bidirectional temporal modeling for EEG-based deception detection, with potential for real-time applications and advanced neural architectures.

Abstract: Deception detection is a significant challenge in fields such as security,
psychology, and forensics. This study presents a deep learning approach for
classifying deceptive and truthful behavior using ElectroEncephaloGram (EEG)
signals from the Bag-of-Lies dataset, a multimodal corpus designed for
naturalistic, casual deception scenarios. A Bidirectional Gated Recurrent Unit
(Bi-GRU) neural network was trained to perform binary classification of EEG
samples. The model achieved a test accuracy of 97\%, along with high precision,
recall, and F1-scores across both classes. These results demonstrate the
effectiveness of using bidirectional temporal modeling for EEG-based deception
detection and suggest potential for real-time applications and future
exploration of advanced neural architectures.

</details>


### [176] [Graph-Structured Data Analysis of Component Failure in Autonomous Cargo Ships Based on Feature Fusion](https://arxiv.org/abs/2507.13721)
*Zizhao Zhang,Tianxiang Zhao,Yu Sun,Liping Sun,Jichuan Kang*

Main category: cs.LG

TL;DR: A hybrid feature fusion framework for failure mode analysis in autonomous cargo ships, using improved algorithms and semantic encoding, achieves high accuracy and feature distinguishability.


<details>
  <summary>Details</summary>
Motivation: Addressing cascading failures and emergency decision-making uncertainties in autonomous cargo ships.

Method: Uses HN-CSA for literature retrieval, Word2Vec, BERT-KPCA, and Sentence-BERT for feature fusion, and GATE-GNN for classification.

Result: Achieves 0.735 classification accuracy, 0.641 silhouette coefficient, and 0.93 F1 score in label prediction.

Conclusion: Provides a robust foundation for failure analysis and supports fault diagnosis, risk assessment, and decision-making in ACS.

Abstract: To address the challenges posed by cascading reactions caused by component
failures in autonomous cargo ships (ACS) and the uncertainties in emergency
decision-making, this paper proposes a novel hybrid feature fusion framework
for constructing a graph-structured dataset of failure modes. By employing an
improved cuckoo search algorithm (HN-CSA), the literature retrieval efficiency
is significantly enhanced, achieving improvements of 7.1% and 3.4% compared to
the NSGA-II and CSA search algorithms, respectively. A hierarchical feature
fusion framework is constructed, using Word2Vec encoding to encode
subsystem/component features, BERT-KPCA to process failure modes/reasons, and
Sentence-BERT to quantify the semantic association between failure impact and
emergency decision-making. The dataset covers 12 systems, 1,262 failure modes,
and 6,150 propagation paths. Validation results show that the GATE-GNN model
achieves a classification accuracy of 0.735, comparable to existing benchmarks.
Additionally, a silhouette coefficient of 0.641 indicates that the features are
highly distinguishable. In the label prediction results, the Shore-based
Meteorological Service System achieved an F1 score of 0.93, demonstrating high
prediction accuracy. This paper not only provides a solid foundation for
failure analysis in autonomous cargo ships but also offers reliable support for
fault diagnosis, risk assessment, and intelligent decision-making systems. The
link to the dataset is
https://github.com/wojiufukele/Graph-Structured-about-CSA.

</details>


### [177] [Adversarial Training Improves Generalization Under Distribution Shifts in Bioacoustics](https://arxiv.org/abs/2507.13727)
*René Heinrich,Lukas Rauch,Bernhard Sick,Christoph Scholz*

Main category: cs.LG

TL;DR: Adversarial training improves generalization and robustness in audio classification, especially with output-space attacks, boosting clean test performance by 10.5%.


<details>
  <summary>Details</summary>
Motivation: To explore how adversarial training enhances generalization and robustness in audio classification under data distribution shifts.

Method: Evaluated two adversarial training strategies (output-space and embedding-space attacks) on ConvNeXt and AudioProtoPNet using a bird sound benchmark.

Result: Output-space attacks improved clean test performance by 10.5% and increased adversarial robustness.

Conclusion: Adversarial training can enhance robustness against distribution shifts and adversarial attacks in audio classification.

Abstract: Adversarial training is a promising strategy for enhancing model robustness
against adversarial attacks. However, its impact on generalization under
substantial data distribution shifts in audio classification remains largely
unexplored. To address this gap, this work investigates how different
adversarial training strategies improve generalization performance and
adversarial robustness in audio classification. The study focuses on two model
architectures: a conventional convolutional neural network (ConvNeXt) and an
inherently interpretable prototype-based model (AudioProtoPNet). The approach
is evaluated using a challenging bird sound classification benchmark. This
benchmark is characterized by pronounced distribution shifts between training
and test data due to varying environmental conditions and recording methods, a
common real-world challenge. The investigation explores two adversarial
training strategies: one based on output-space attacks that maximize the
classification loss function, and another based on embedding-space attacks
designed to maximize embedding dissimilarity. These attack types are also used
for robustness evaluation. Additionally, for AudioProtoPNet, the study assesses
the stability of its learned prototypes under targeted embedding-space attacks.
Results show that adversarial training, particularly using output-space
attacks, improves clean test data performance by an average of 10.5% relative
and simultaneously strengthens the adversarial robustness of the models. These
findings, although derived from the bird sound domain, suggest that adversarial
training holds potential to enhance robustness against both strong distribution
shifts and adversarial attacks in challenging audio classification settings.

</details>


### [178] [An End-to-End DNN Inference Framework for the SpiNNaker2 Neuromorphic MPSoC](https://arxiv.org/abs/2507.13736)
*Matthias Jobst,Tim Langer,Chen Liu,Mehmet Alici,Hector A. Gonzalez,Christian Mayr*

Main category: cs.LG

TL;DR: A multi-layer DNN scheduling framework extends OctopuScheduler, enabling edge-based execution of large DNNs like transformers on SpiNNaker2.


<details>
  <summary>Details</summary>
Motivation: To facilitate the execution of complex DNNs on neuromorphic hardware (SpiNNaker2) for edge applications.

Method: Extends OctopuScheduler with quantization and lowering steps, providing an end-to-end flow from PyTorch models to SpiNNaker2 inference.

Result: Enables execution of large-scale DNNs, including transformers, on a single SpiNNaker2 chip.

Conclusion: The framework successfully bridges the gap between PyTorch models and neuromorphic hardware, supporting edge-based DNN execution.

Abstract: This work presents a multi-layer DNN scheduling framework as an extension of
OctopuScheduler, providing an end-to-end flow from PyTorch models to inference
on a single SpiNNaker2 chip. Together with a front-end comprised of
quantization and lowering steps, the proposed framework enables the edge-based
execution of large and complex DNNs up to transformer scale using the
neuromorphic platform SpiNNaker2.

</details>


### [179] [SamGoG: A Sampling-Based Graph-of-Graphs Framework for Imbalanced Graph Classification](https://arxiv.org/abs/2507.13741)
*Shangyou Wang,Zezhong Ding,Xike Xie*

Main category: cs.LG

TL;DR: SamGoG is a sampling-based Graph-of-Graphs framework addressing class and graph size imbalance in GNNs, improving accuracy and training speed.


<details>
  <summary>Details</summary>
Motivation: Real-world graphs often suffer from class and size imbalance, degrading GNN performance. Existing methods are limited or costly.

Method: SamGoG constructs multiple GoGs via importance-based sampling, enhancing edge homophily with learnable similarity and adaptive node degree.

Result: Achieves up to 15.66% accuracy improvement and 6.7× training acceleration on benchmarks.

Conclusion: SamGoG effectively mitigates imbalance issues, enhancing GNN performance for graph classification.

Abstract: Graph Neural Networks (GNNs) have shown remarkable success in graph
classification tasks by capturing both structural and feature-based
representations. However, real-world graphs often exhibit two critical forms of
imbalance: class imbalance and graph size imbalance. These imbalances can bias
the learning process and degrade model performance. Existing methods typically
address only one type of imbalance or incur high computational costs. In this
work, we propose SamGoG, a sampling-based Graph-of-Graphs (GoG) learning
framework that effectively mitigates both class and graph size imbalance.
SamGoG constructs multiple GoGs through an efficient importance-based sampling
mechanism and trains on them sequentially. This sampling mechanism incorporates
the learnable pairwise similarity and adaptive GoG node degree to enhance edge
homophily, thus improving downstream model quality. SamGoG can seamlessly
integrate with various downstream GNNs, enabling their efficient adaptation for
graph classification tasks. Extensive experiments on benchmark datasets
demonstrate that SamGoG achieves state-of-the-art performance with up to a
15.66% accuracy improvement with 6.7$\times$ training acceleration.

</details>


### [180] [Search-Optimized Quantization in Biomedical Ontology Alignment](https://arxiv.org/abs/2507.13742)
*Oussama Bouaggad,Natalia Grabar*

Main category: cs.LG

TL;DR: The paper introduces a method for optimizing large AI models for resource-constrained environments, achieving significant speed and memory improvements while maintaining performance.


<details>
  <summary>Details</summary>
Motivation: Challenges in deploying large AI models on edge devices due to computational demands, energy consumption, and latency.

Method: Uses transformer-based models for ontology alignment, leverages Microsoft Olive for optimization, and employs dynamic quantization with Intel tools.

Result: Achieves 20x faster inference, 70% reduced memory usage, and state-of-the-art performance on DEFT 2020 tasks.

Conclusion: The proposed method effectively optimizes AI models for efficiency without compromising performance.

Abstract: In the fast-moving world of AI, as organizations and researchers develop more
advanced models, they face challenges due to their sheer size and computational
demands. Deploying such models on edge devices or in resource-constrained
environments adds further challenges related to energy consumption, memory
usage and latency. To address these challenges, emerging trends are shaping the
future of efficient model optimization techniques. From this premise, by
employing supervised state-of-the-art transformer-based models, this research
introduces a systematic method for ontology alignment, grounded in cosine-based
semantic similarity between a biomedical layman vocabulary and the Unified
Medical Language System (UMLS) Metathesaurus. It leverages Microsoft Olive to
search for target optimizations among different Execution Providers (EPs) using
the ONNX Runtime backend, followed by an assembled process of dynamic
quantization employing Intel Neural Compressor and IPEX (Intel Extension for
PyTorch). Through our optimization process, we conduct extensive assessments on
the two tasks from the DEFT 2020 Evaluation Campaign, achieving a new
state-of-the-art in both. We retain performance metrics intact, while attaining
an average inference speed-up of 20x and reducing memory usage by approximately
70%.

</details>


### [181] [MolPIF: A Parameter Interpolation Flow Model for Molecule Generation](https://arxiv.org/abs/2507.13762)
*Yaowei Jin,Junjie Wang,Wenkai Xiang,Duanhua Cao,Dan Teng,Zhehuan Fan,Jiacheng Xiong,Xia Sheng,Chuanlong Zeng,Mingyue Zheng,Qian Shi*

Main category: cs.LG

TL;DR: The paper introduces Parameter Interpolation Flow (PIF), a novel model for molecular generation, addressing limitations of Bayesian Flow Networks (BFNs) and demonstrating superior performance in drug design.


<details>
  <summary>Details</summary>
Motivation: Current Bayesian Flow Networks (BFNs) for molecular generation have limitations in flexibility and adaptability to diverse data distributions and tasks. The potential of simpler, parameter-space-based models remains unexplored.

Method: The authors propose the Parameter Interpolation Flow (PIF) model, providing theoretical foundations, training, and inference procedures. They also develop MolPIF for structure-based drug design.

Result: MolPIF outperforms baseline models across diverse metrics, validating the effectiveness of parameter-space-based generative modeling for molecules.

Conclusion: The work demonstrates the superiority of PIF for molecular generation and offers new insights for model design in drug discovery.

Abstract: Advances in deep learning for molecular generation show promise in
accelerating drug discovery. Bayesian Flow Networks (BFNs) have recently shown
impressive performance across diverse chemical tasks, with their success often
ascribed to the paradigm of modeling in a low-variance parameter space.
However, the Bayesian inference-based strategy imposes limitations on designing
more flexible distribution transformation pathways, making it challenging to
adapt to diverse data distributions and varied task requirements. Furthermore,
the potential for simpler, more efficient parameter-space-based models is
unexplored. To address this, we propose a novel Parameter Interpolation Flow
model (named PIF) with detailed theoretical foundation, training, and inference
procedures. We then develop MolPIF for structure-based drug design,
demonstrating its superior performance across diverse metrics compared to
baselines. This work validates the effectiveness of parameter-space-based
generative modeling paradigm for molecules and offers new perspectives for
model design.

</details>


### [182] [Dual-Center Graph Clustering with Neighbor Distribution](https://arxiv.org/abs/2507.13765)
*Enhao Cheng,Shoujia Zhang,Jianhua Yin,Li Jin,Liqiang Nie*

Main category: cs.LG

TL;DR: The paper introduces DCGC, a dual-center graph clustering method using neighbor distribution for reliable supervision and dual-center optimization, outperforming existing techniques.


<details>
  <summary>Details</summary>
Motivation: Graph clustering is challenging due to its unsupervised nature, and existing methods rely on unreliable pseudo-labels or single-center optimization, leading to incomplete guidance.

Method: DCGC leverages neighbor distribution for supervision in contrastive learning and introduces dual-center optimization (feature and neighbor distribution centers) for improved clustering.

Result: Experiments show DCGC achieves superior performance by enhancing representation learning and clustering effectiveness.

Conclusion: DCGC provides a reliable and effective solution for graph clustering by addressing limitations of existing methods through neighbor distribution and dual-center optimization.

Abstract: Graph clustering is crucial for unraveling intricate data structures, yet it
presents significant challenges due to its unsupervised nature. Recently,
goal-directed clustering techniques have yielded impressive results, with
contrastive learning methods leveraging pseudo-label garnering considerable
attention. Nonetheless, pseudo-label as a supervision signal is unreliable and
existing goal-directed approaches utilize only features to construct a
single-target distribution for single-center optimization, which lead to
incomplete and less dependable guidance. In our work, we propose a novel
Dual-Center Graph Clustering (DCGC) approach based on neighbor distribution
properties, which includes representation learning with neighbor distribution
and dual-center optimization. Specifically, we utilize neighbor distribution as
a supervision signal to mine hard negative samples in contrastive learning,
which is reliable and enhances the effectiveness of representation learning.
Furthermore, neighbor distribution center is introduced alongside feature
center to jointly construct a dual-target distribution for dual-center
optimization. Extensive experiments and analysis demonstrate superior
performance and effectiveness of our proposed method.

</details>


### [183] [On-the-Fly Fine-Tuning of Foundational Neural Network Potentials: A Bayesian Neural Network Approach](https://arxiv.org/abs/2507.13805)
*Tim Rensmeyer,Denis Kramer,Oliver Niggemann*

Main category: cs.LG

TL;DR: The paper proposes a Bayesian neural network-based fine-tuning approach for machine learning force fields to reduce training data needs and automate rare event detection.


<details>
  <summary>Details</summary>
Motivation: The computational burden of generating diverse training datasets for interatomic force fields, especially for rare events or large configuration spaces, motivates the need for efficient fine-tuning methods.

Method: The method involves Bayesian neural networks for uncertainty quantification and an on-the-fly workflow to automate fine-tuning and rare event sampling.

Result: The approach maintains pre-specified accuracy, detects rare events like transition states, and samples them at an increased rate.

Conclusion: The proposed method addresses the challenge of uncertainty quantification in foundation models and enables efficient fine-tuning for rare event modeling.

Abstract: Due to the computational complexity of evaluating interatomic forces from
first principles, the creation of interatomic machine learning force fields has
become a highly active field of research. However, the generation of training
datasets of sufficient size and sample diversity itself comes with a
computational burden that can make this approach impractical for modeling rare
events or systems with a large configuration space. Fine-tuning foundation
models that have been pre-trained on large-scale material or molecular
databases offers a promising opportunity to reduce the amount of training data
necessary to reach a desired level of accuracy. However, even if this approach
requires less training data overall, creating a suitable training dataset can
still be a very challenging problem, especially for systems with rare events
and for end-users who don't have an extensive background in machine learning.
In on-the-fly learning, the creation of a training dataset can be largely
automated by using model uncertainty during the simulation to decide if the
model is accurate enough or if a structure should be recalculated with
classical methods and used to update the model. A key challenge for applying
this form of active learning to the fine-tuning of foundation models is how to
assess the uncertainty of those models during the fine-tuning process, even
though most foundation models lack any form of uncertainty quantification. In
this paper, we overcome this challenge by introducing a fine-tuning approach
based on Bayesian neural network methods and a subsequent on-the-fly workflow
that automatically fine-tunes the model while maintaining a pre-specified
accuracy and can detect rare events such as transition states and sample them
at an increased rate relative to their occurrence.

</details>


### [184] [Scalable Submodular Policy Optimization via Pruned Submodularity Graph](https://arxiv.org/abs/2507.13834)
*Aditi Anand,Suman Banerjee,Dildar Ali*

Main category: cs.LG

TL;DR: The paper introduces a variant of RL with submodular rewards, proposing a pruned submodularity graph-based method for efficient, approximate solutions.


<details>
  <summary>Details</summary>
Motivation: Traditional RL assumes additive rewards, but many real-world problems (e.g., path planning) exhibit diminishing returns, modeled as submodular functions. This gap motivates the study.

Method: A pruned submodularity graph-based approach is developed to maximize submodular rewards, with analysis of time, space, and performance guarantees.

Result: Experiments show the proposed method outperforms baselines, yielding higher rewards in benchmark setups.

Conclusion: The approach effectively addresses submodular reward RL, offering computational feasibility and improved performance.

Abstract: In Reinforcement Learning (abbreviated as RL), an agent interacts with the
environment via a set of possible actions, and a reward is generated from some
unknown distribution. The task here is to find an optimal set of actions such
that the reward after a certain time step gets maximized. In a traditional
setup, the reward function in an RL Problem is considered additive. However, in
reality, there exist many problems, including path planning, coverage control,
etc., the reward function follows the diminishing return, which can be modeled
as a submodular function. In this paper, we study a variant of the RL Problem
where the reward function is submodular, and our objective is to find an
optimal policy such that this reward function gets maximized. We have proposed
a pruned submodularity graph-based approach that provides a provably
approximate solution in a feasible computation time. The proposed approach has
been analyzed to understand its time and space requirements as well as a
performance guarantee. We have experimented with a benchmark agent-environment
setup, which has been used for similar previous studies, and the results are
reported. From the results, we observe that the policy obtained by our proposed
approach leads to more reward than the baseline methods.

</details>


### [185] [Self-supervised learning on gene expression data](https://arxiv.org/abs/2507.13912)
*Kevin Dradjat,Massinissa Hamidi,Pierre Bartet,Blaise Hanczar*

Main category: cs.LG

TL;DR: The paper explores self-supervised learning for phenotype prediction from gene expression data, reducing reliance on labeled data and outperforming traditional supervised methods.


<details>
  <summary>Details</summary>
Motivation: Traditional supervised learning requires costly labeled data for gene expression analysis, prompting the need for self-supervised methods to leverage unlabeled data.

Method: Three state-of-the-art self-supervised learning methods are applied to bulk gene expression data to assess their ability to generate useful representations for phenotype prediction.

Result: Self-supervised methods effectively capture complex data structures and improve prediction accuracy, outperforming supervised models while reducing dependency on annotated data.

Conclusion: Self-supervised learning is promising for gene expression analysis, with recommendations provided for method selection and future research directions outlined.

Abstract: Predicting phenotypes from gene expression data is a crucial task in
biomedical research, enabling insights into disease mechanisms, drug responses,
and personalized medicine. Traditional machine learning and deep learning rely
on supervised learning, which requires large quantities of labeled data that
are costly and time-consuming to obtain in the case of gene expression data.
Self-supervised learning has recently emerged as a promising approach to
overcome these limitations by extracting information directly from the
structure of unlabeled data. In this study, we investigate the application of
state-of-the-art self-supervised learning methods to bulk gene expression data
for phenotype prediction. We selected three self-supervised methods, based on
different approaches, to assess their ability to exploit the inherent structure
of the data and to generate qualitative representations which can be used for
downstream predictive tasks. By using several publicly available gene
expression datasets, we demonstrate how the selected methods can effectively
capture complex information and improve phenotype prediction accuracy. The
results obtained show that self-supervised learning methods can outperform
traditional supervised models besides offering significant advantage by
reducing the dependency on annotated data. We provide a comprehensive analysis
of the performance of each method by highlighting their strengths and
limitations. We also provide recommendations for using these methods depending
on the case under study. Finally, we outline future research directions to
enhance the application of self-supervised learning in the field of gene
expression data analysis. This study is the first work that deals with bulk
RNA-Seq data and self-supervised learning.

</details>


### [186] [Reframing attention as a reinforcement learning problem for causal discovery](https://arxiv.org/abs/2507.13920)
*Turan Orujlu,Christian Gumbsch,Martin V. Butz,Charley M Wu*

Main category: cs.LG

TL;DR: The paper introduces the Causal Process framework and its implementation, the Causal Process Model, to represent dynamic causal structures in RL, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: To bridge the gap between formal causality frameworks and deep RL by addressing the dynamic nature of causal interactions, which static causal graphs ignore.

Method: Proposes the Causal Process framework and implements it as the Causal Process Model, integrating causal inference into RL via nested RL tasks and Transformer-like attention mechanisms.

Result: Outperforms current methods in causal representation learning and agent performance, successfully recovering dynamic causal graphs.

Conclusion: The Causal Process framework effectively models dynamic causal structures in RL, offering interpretable causal processes and improved performance.

Abstract: Formal frameworks of causality have operated largely parallel to modern
trends in deep reinforcement learning (RL). However, there has been a revival
of interest in formally grounding the representations learned by neural
networks in causal concepts. Yet, most attempts at neural models of causality
assume static causal graphs and ignore the dynamic nature of causal
interactions. In this work, we introduce Causal Process framework as a novel
theory for representing dynamic hypotheses about causal structure. Furthermore,
we present Causal Process Model as an implementation of this framework. This
allows us to reformulate the attention mechanism popularized by Transformer
networks within an RL setting with the goal to infer interpretable causal
processes from visual observations. Here, causal inference corresponds to
constructing a causal graph hypothesis which itself becomes an RL task nested
within the original RL problem. To create an instance of such hypothesis, we
employ RL agents. These agents establish links between units similar to the
original Transformer attention mechanism. We demonstrate the effectiveness of
our approach in an RL environment where we outperform current alternatives in
causal representation learning and agent performance, and uniquely recover
graphs of dynamic causal processes.

</details>


### [187] [MoDyGAN: Combining Molecular Dynamics With GANs to Investigate Protein Conformational Space](https://arxiv.org/abs/2507.13950)
*Jingbo Liang,Bruna Jacobson*

Main category: cs.LG

TL;DR: MoDyGAN combines MD simulations and GANs to explore protein conformational spaces efficiently, using a novel 2D representation for 3D structures.


<details>
  <summary>Details</summary>
Motivation: High computational costs of dynamic physics-based simulations limit exploration of protein conformational landscapes.

Method: MoDyGAN uses a generator to map Gaussian distributions to MD-derived trajectories and a refinement module with dual-discriminator for plausible conformations. It employs a 2D representation of 3D protein structures.

Result: MoDyGAN generates plausible conformations for rigid proteins and aligns latent space interpolations with SMD trajectories.

Conclusion: The 2D representation enables advanced deep learning for biomolecular simulation, promising efficient sampling and potential extension to other 3D structures.

Abstract: Extensively exploring protein conformational landscapes remains a major
challenge in computational biology due to the high computational cost involved
in dynamic physics-based simulations. In this work, we propose a novel
pipeline, MoDyGAN, that leverages molecular dynamics (MD) simulations and
generative adversarial networks (GANs) to explore protein conformational
spaces. MoDyGAN contains a generator that maps Gaussian distributions into
MD-derived protein trajectories, and a refinement module that combines ensemble
learning with a dual-discriminator to further improve the plausibility of
generated conformations. Central to our approach is an innovative
representation technique that reversibly transforms 3D protein structures into
2D matrices, enabling the use of advanced image-based GAN architectures. We use
three rigid proteins to demonstrate that MoDyGAN can generate plausible new
conformations. We also use deca-alanine as a case study to show that
interpolations within the latent space closely align with trajectories obtained
from steered molecular dynamics (SMD) simulations. Our results suggest that
representing proteins as image-like data unlocks new possibilities for applying
advanced deep learning techniques to biomolecular simulation, leading to an
efficient sampling of conformational states. Additionally, the proposed
framework holds strong potential for extension to other complex 3D structures.

</details>


### [188] [Robust Anomaly Detection with Graph Neural Networks using Controllability](https://arxiv.org/abs/2507.13954)
*Yifan Wei,Anwar Said,Waseem Abbas,Xenofon Koutsoukos*

Main category: cs.LG

TL;DR: The paper proposes integrating average controllability into graph-based anomaly detection to improve performance with limited anomalous data.


<details>
  <summary>Details</summary>
Motivation: Anomaly detection in complex domains is challenging due to sparse labeled data and imbalance between anomalous and benign samples.

Method: Two novel approaches: (1) using average controllability as edge weight, (2) encoding it as a one-hot edge attribute vector.

Result: Improved anomaly detection performance on real-world and synthetic networks compared to six baselines.

Conclusion: Average controllability enhances graph-based models, addressing challenges in sparse, imbalanced datasets.

Abstract: Anomaly detection in complex domains poses significant challenges due to the
need for extensive labeled data and the inherently imbalanced nature of
anomalous versus benign samples. Graph-based machine learning models have
emerged as a promising solution that combines attribute and relational data to
uncover intricate patterns. However, the scarcity of anomalous data exacerbates
the challenge, which requires innovative strategies to enhance model learning
with limited information. In this paper, we hypothesize that the incorporation
of the influence of the nodes, quantified through average controllability, can
significantly improve the performance of anomaly detection. We propose two
novel approaches to integrate average controllability into graph-based
frameworks: (1) using average controllability as an edge weight and (2)
encoding it as a one-hot edge attribute vector. Through rigorous evaluation on
real-world and synthetic networks with six state-of-the-art baselines, our
proposed methods demonstrate improved performance in identifying anomalies,
highlighting the critical role of controllability measures in enhancing the
performance of graph machine learning models. This work underscores the
potential of integrating average controllability as additional metrics to
address the challenges of anomaly detection in sparse and imbalanced datasets.

</details>


### [189] [Signs of the Past, Patterns of the Present: On the Automatic Classification of Old Babylonian Cuneiform Signs](https://arxiv.org/abs/2507.13959)
*Eli Verwimp,Gustav Ryberg Smidt,Hendrik Hameeuw,Katrien De Graef*

Main category: cs.LG

TL;DR: The paper explores ML for classifying cuneiform signs, highlighting variability challenges and testing ResNet50 on Old Babylonian texts, achieving 87.1% top-1 accuracy.


<details>
  <summary>Details</summary>
Motivation: To address the variability in cuneiform signs due to origin, purpose, and digitization, and assess ML model performance across datasets.

Method: Trained and tested ResNet50 on handwritten Old Babylonian texts from three Mesopotamian cities, focusing on signs with ≥20 instances.

Result: Achieved 87.1% top-1 and 96.5% top-5 accuracy, setting a benchmark for Old Babylonian text classification.

Conclusion: The study aims to guide future data standards and improve cuneiform sign classification, with ResNet50 showing promising results.

Abstract: The work in this paper describes the training and evaluation of machine
learning (ML) techniques for the classification of cuneiform signs. There is a
lot of variability in cuneiform signs, depending on where they come from, for
what and by whom they were written, but also how they were digitized. This
variability makes it unlikely that an ML model trained on one dataset will
perform successfully on another dataset. This contribution studies how such
differences impact that performance. Based on our results and insights, we aim
to influence future data acquisition standards and provide a solid foundation
for future cuneiform sign classification tasks. The ML model has been trained
and tested on handwritten Old Babylonian (c. 2000-1600 B.C.E.) documentary
texts inscribed on clay tablets originating from three Mesopotamian cities
(Nippur, D\=ur-Abie\v{s}uh and Sippar). The presented and analysed model is
ResNet50, which achieves a top-1 score of 87.1% and a top-5 score of 96.5% for
signs with at least 20 instances. As these automatic classification results are
the first on Old Babylonian texts, there are currently no comparable results.

</details>


### [190] [Structural Connectome Harmonization Using Deep Learning: The Strength of Graph Neural Networks](https://arxiv.org/abs/2507.13992)
*Jagruti Patel,Thomas A. W. Bolton,Mikkel Schöttner,Anjali Tarun,Sebastien Tourbier,Yasser Alemàn-Gòmez,Jonas Richiardi,Patric Hagmann*

Main category: cs.LG

TL;DR: A deep harmonization framework for structural connectomes (SCs) addresses biases in multi-site studies without needing metadata or traveling subjects, outperforming traditional methods in preserving topology and individuality.


<details>
  <summary>Details</summary>
Motivation: Small sample sizes and scanner heterogeneity in neuroimaging limit biomarker reliability for disorders like Alzheimer's and schizophrenia. Existing harmonization methods often require metadata or overlook SC graph-topology.

Method: Proposes a site-conditioned deep harmonization framework tested on the Human Connectome Dataset, comparing three deep architectures (fully connected AE, convolutional AE, graph convolutional AE) against linear regression.

Result: Graph AE preserves topological structure and subject-level individuality better than non-graph models, while linear regression excels numerically but lacks real-world applicability.

Conclusion: Graph-based approaches are ideal for structure-aware, generalizable SC harmonization in multi-site studies, emphasizing architecture's role in performance.

Abstract: Small sample sizes in neuroimaging in general, and in structural connectome
(SC) studies in particular limit the development of reliable biomarkers for
neurological and psychiatric disorders - such as Alzheimer's disease and
schizophrenia - by reducing statistical power, reliability, and
generalizability. Large-scale multi-site studies have exist, but they have
acquisition-related biases due to scanner heterogeneity, compromising imaging
consistency and downstream analyses. While existing SC harmonization methods -
such as linear regression (LR), ComBat, and deep learning techniques - mitigate
these biases, they often rely on detailed metadata, traveling subjects (TS), or
overlook the graph-topology of SCs. To address these limitations, we propose a
site-conditioned deep harmonization framework that harmonizes SCs across
diverse acquisition sites without requiring metadata or TS that we test in a
simulated scenario based on the Human Connectome Dataset. Within this
framework, we benchmark three deep architectures - a fully connected
autoencoder (AE), a convolutional AE, and a graph convolutional AE - against a
top-performing LR baseline. While non-graph models excel in edge-weight
prediction and edge existence detection, the graph AE demonstrates superior
preservation of topological structure and subject-level individuality, as
reflected by graph metrics and fingerprinting accuracy, respectively. Although
the LR baseline achieves the highest numerical performance by explicitly
modeling acquisition parameters, it lacks applicability to real-world
multi-site use cases as detailed acquisition metadata is often unavailable. Our
results highlight the critical role of model architecture in SC harmonization
performance and demonstrate that graph-based approaches are particularly
well-suited for structure-aware, domain-generalizable SC harmonization in
large-scale multi-site SC studies.

</details>


### [191] [ParallelTime: Dynamically Weighting the Balance of Short- and Long-Term Temporal Dependencies](https://arxiv.org/abs/2507.13998)
*Itay Katav,Aryeh Kontorovich*

Main category: cs.LG

TL;DR: The paper introduces ParallelTime, a dynamic weighting mechanism for multivariate time series forecasting, combining attention and Mamba architectures for improved performance.


<details>
  <summary>Details</summary>
Motivation: Equal weighting of long-term and short-term dependencies in existing architectures is suboptimal for time-series forecasting.

Method: Proposes ParallelTime Weighter, a dynamic weighting mechanism, and the ParallelTime architecture to balance dependencies adaptively.

Result: Achieves state-of-the-art performance, lower FLOPs, fewer parameters, and scalability to longer prediction horizons.

Conclusion: ParallelTime offers a robust and efficient solution, paving the way for future advancements in parallel Attention-Mamba architectures.

Abstract: Modern multivariate time series forecasting primarily relies on two
architectures: the Transformer with attention mechanism and Mamba. In natural
language processing, an approach has been used that combines local window
attention for capturing short-term dependencies and Mamba for capturing
long-term dependencies, with their outputs averaged to assign equal weight to
both. We find that for time-series forecasting tasks, assigning equal weight to
long-term and short-term dependencies is not optimal. To mitigate this, we
propose a dynamic weighting mechanism, ParallelTime Weighter, which calculates
interdependent weights for long-term and short-term dependencies for each token
based on the input and the model's knowledge. Furthermore, we introduce the
ParallelTime architecture, which incorporates the ParallelTime Weighter
mechanism to deliver state-of-the-art performance across diverse benchmarks.
Our architecture demonstrates robustness, achieves lower FLOPs, requires fewer
parameters, scales effectively to longer prediction horizons, and significantly
outperforms existing methods. These advances highlight a promising path for
future developments of parallel Attention-Mamba in time series forecasting. The
implementation is readily available at:
\href{https://github.com/itay1551/ParallelTime}{ParallelTime GitHub

</details>


### [192] [On the Fundamental Limitations of Dual Static CVaR Decompositions in Markov Decision Processes](https://arxiv.org/abs/2507.14005)
*Mathieu Godbout,Audrey Durand*

Main category: cs.LG

TL;DR: The paper explains why dual-formulation DP methods fail for static CVaR-optimal policies in MDPs, identifies risk-assignment constraints as the root cause, and shows limitations in dual CVaR decomposition.


<details>
  <summary>Details</summary>
Motivation: To understand why dual-formulation DP methods fail for static CVaR-optimal policies and to explore the root cause of evaluation errors.

Method: Framing policy evaluation as two minimization problems and analyzing risk-assignment consistency constraints.

Result: Empty intersection of constraints causes evaluation errors, and dual CVaR decomposition cannot yield a uniformly optimal policy.

Conclusion: The dual CVaR decomposition is fundamentally limited, as no single policy can be optimal across all risk levels.

Abstract: Recent work has shown that dynamic programming (DP) methods for finding
static CVaR-optimal policies in Markov Decision Processes (MDPs) can fail when
based on the dual formulation, yet the root cause for the failure has remained
unclear. We expand on these findings by shifting focus from policy optimization
to the seemingly simpler task of policy evaluation. We show that evaluating the
static CVaR of a given policy can be framed as two distinct minimization
problems. For their solutions to match, a set of ``risk-assignment consistency
constraints'' must be satisfied, and we demonstrate that the intersection of
the constraints being empty is the source of previously observed evaluation
errors. Quantifying the evaluation error as the CVaR evaluation gap, we then
demonstrate that the issues observed when optimizing over the dual-based CVaR
DP are explained by the returned policy having a non-zero CVaR evaluation gap.
We then leverage our proposed risk-assignment perspective to prove that the
search for a single, uniformly optimal policy via on the dual CVaR
decomposition is fundamentally limited, identifying an MDP where no single
policy can be optimal across all initial risk levels.

</details>


### [193] [Byzantine-resilient federated online learning for Gaussian process regression](https://arxiv.org/abs/2507.14021)
*Xu Zhang,Zhenyuan Yuan,Minghui Zhu*

Main category: cs.LG

TL;DR: A Byzantine-resilient federated GPR algorithm is developed to improve learning performance despite adversarial agent behavior.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of learning a latent function collaboratively in federated settings where some agents may exhibit arbitrary or adversarial behavior (Byzantine failures).

Method: Agents send local predictions to a cloud, which aggregates them using a Byzantine-resilient rule. The cloud broadcasts the global model back to agents, who refine their predictions by fusing it with their local model.

Result: The algorithm improves learning accuracy for agent-based fused GPR over local GPR, validated on toy and real-world datasets.

Conclusion: The proposed method effectively mitigates Byzantine failures and enhances collaborative learning in federated GPR.

Abstract: In this paper, we study Byzantine-resilient federated online learning for
Gaussian process regression (GPR). We develop a Byzantine-resilient federated
GPR algorithm that allows a cloud and a group of agents to collaboratively
learn a latent function and improve the learning performances where some agents
exhibit Byzantine failures, i.e., arbitrary and potentially adversarial
behavior. Each agent-based local GPR sends potentially compromised local
predictions to the cloud, and the cloud-based aggregated GPR computes a global
model by a Byzantine-resilient product of experts aggregation rule. Then the
cloud broadcasts the current global model to all the agents. Agent-based fused
GPR refines local predictions by fusing the received global model with that of
the agent-based local GPR. Moreover, we quantify the learning accuracy
improvements of the agent-based fused GPR over the agent-based local GPR.
Experiments on a toy example and two medium-scale real-world datasets are
conducted to demonstrate the performances of the proposed algorithm.

</details>


### [194] [DONUT: Physics-aware Machine Learning for Real-time X-ray Nanodiffraction Analysis](https://arxiv.org/abs/2507.14038)
*Aileen Luo,Tao Zhou,Ming Du,Martin V. Holt,Andrej Singer,Mathew J. Cherukara*

Main category: cs.LG

TL;DR: DONUT, a physics-aware neural network, enables real-time analysis of nanobeam diffraction data without labeled datasets, improving efficiency by 200x over traditional methods.


<details>
  <summary>Details</summary>
Motivation: Real-time analysis of coherent X-ray scattering data is hindered by artifacts and computational demands, especially in scanning X-ray nanodiffraction microscopy.

Method: DONUT integrates a differentiable geometric diffraction model into its architecture to predict crystal lattice strain and orientation unsupervised.

Result: DONUT accurately extracts features 200 times more efficiently than conventional fitting methods.

Conclusion: DONUT addresses the bottleneck in real-time X-ray data analysis, offering a scalable and automated solution.

Abstract: Coherent X-ray scattering techniques are critical for investigating the
fundamental structural properties of materials at the nanoscale. While
advancements have made these experiments more accessible, real-time analysis
remains a significant bottleneck, often hindered by artifacts and computational
demands. In scanning X-ray nanodiffraction microscopy, which is widely used to
spatially resolve structural heterogeneities, this challenge is compounded by
the convolution of the divergent beam with the sample's local structure. To
address this, we introduce DONUT (Diffraction with Optics for Nanobeam by
Unsupervised Training), a physics-aware neural network designed for the rapid
and automated analysis of nanobeam diffraction data. By incorporating a
differentiable geometric diffraction model directly into its architecture,
DONUT learns to predict crystal lattice strain and orientation in real-time.
Crucially, this is achieved without reliance on labeled datasets or
pre-training, overcoming a fundamental limitation for supervised machine
learning in X-ray science. We demonstrate experimentally that DONUT accurately
extracts all features within the data over 200 times more efficiently than
conventional fitting methods.

</details>


### [195] [Noradrenergic-inspired gain modulation attenuates the stability gap in joint training](https://arxiv.org/abs/2507.14056)
*Alejandro Rodriguez-Garcia,Anindya Ghosh,Srikanth Ramaswamy*

Main category: cs.LG

TL;DR: The paper addresses the stability gap in continual learning, proposing an uncertainty-modulated gain dynamics mechanism inspired by biological brains to balance plasticity and stability.


<details>
  <summary>Details</summary>
Motivation: The stability gap in continual learning contradicts its objectives, revealing a lack of robustness in mitigating forgetting, even under ideal joint-loss regimes.

Method: The authors propose uncertainty-modulated gain dynamics, inspired by biological neuromodulatory signals, to dynamically balance knowledge integration and retention.

Result: The mechanism effectively attenuates the stability gap in domain-incremental and class-incremental benchmarks (MNIST and CIFAR) under joint training.

Conclusion: The study offers insights into reducing stability gaps and enhancing continual learning performance, mimicking biological neuromodulation.

Abstract: Recent studies in continual learning have identified a transient drop in
performance on mastered tasks when assimilating new ones, known as the
stability gap. Such dynamics contradict the objectives of continual learning,
revealing a lack of robustness in mitigating forgetting, and notably,
persisting even under an ideal joint-loss regime. Examining this gap within
this idealized joint training context is critical to isolate it from other
sources of forgetting. We argue that it reflects an imbalance between rapid
adaptation and robust retention at task boundaries, underscoring the need to
investigate mechanisms that reconcile plasticity and stability within continual
learning frameworks. Biological brains navigate a similar dilemma by operating
concurrently on multiple timescales, leveraging neuromodulatory signals to
modulate synaptic plasticity. However, artificial networks lack native
multitimescale dynamics, and although optimizers like momentum-SGD and Adam
introduce implicit timescale regularization, they still exhibit stability gaps.
Inspired by locus coeruleus mediated noradrenergic bursts, which transiently
enhance neuronal gain under uncertainty to facilitate sensory assimilation, we
propose uncertainty-modulated gain dynamics - an adaptive mechanism that
approximates a two-timescale optimizer and dynamically balances integration of
knowledge with minimal interference on previously consolidated information. We
evaluate our mechanism on domain-incremental and class-incremental variants of
the MNIST and CIFAR benchmarks under joint training, demonstrating that
uncertainty-modulated gain dynamics effectively attenuate the stability gap.
Finally, our analysis elucidates how gain modulation replicates noradrenergic
functions in cortical circuits, offering mechanistic insights into reducing
stability gaps and enhance performance in continual learning tasks.

</details>


### [196] [Preference-based Multi-Objective Reinforcement Learning](https://arxiv.org/abs/2507.14066)
*Ni Mu,Yao Luan,Qing-Shan Jia*

Main category: cs.LG

TL;DR: The paper introduces preference-based MORL (Pb-MORL), integrating preferences into multi-objective reinforcement learning to avoid complex reward design and achieve Pareto optimal policies.


<details>
  <summary>Details</summary>
Motivation: Pre-defined reward functions in MORL can be hard to design and may oversimplify conflicting objectives. Preferences offer a flexible alternative.

Method: Pb-MORL formalizes preference integration, constructs a multi-objective reward model aligned with preferences, and proves its equivalence to Pareto optimal policy training.

Result: Experiments show Pb-MORL outperforms the oracle method in benchmark tasks, multi-energy management, and autonomous driving.

Conclusion: Pb-MORL is a practical solution for complex real-world systems, leveraging preferences for effective multi-objective optimization.

Abstract: Multi-objective reinforcement learning (MORL) is a structured approach for
optimizing tasks with multiple objectives. However, it often relies on
pre-defined reward functions, which can be hard to design for balancing
conflicting goals and may lead to oversimplification. Preferences can serve as
more flexible and intuitive decision-making guidance, eliminating the need for
complicated reward design. This paper introduces preference-based MORL
(Pb-MORL), which formalizes the integration of preferences into the MORL
framework. We theoretically prove that preferences can derive policies across
the entire Pareto frontier. To guide policy optimization using preferences, our
method constructs a multi-objective reward model that aligns with the given
preferences. We further provide theoretical proof to show that optimizing this
reward model is equivalent to training the Pareto optimal policy. Extensive
experiments in benchmark multi-objective tasks, a multi-energy management task,
and an autonomous driving task on a multi-line highway show that our method
performs competitively, surpassing the oracle method, which uses the ground
truth reward function. This highlights its potential for practical applications
in complex real-world systems.

</details>


### [197] [DPMT: Dual Process Multi-scale Theory of Mind Framework for Real-time Human-AI Collaboration](https://arxiv.org/abs/2507.14088)
*Xiyun Li,Yining Ding,Yuhua Jiang,Yunlong Zhao,Runpeng Xie,Shuang Xu,Yuanhua Ni,Yiqin Yang,Bo Xu*

Main category: cs.LG

TL;DR: The paper introduces a dual process multi-scale theory of mind (DPMT) framework to improve human-AI collaboration by modeling complex human mental characteristics.


<details>
  <summary>Details</summary>
Motivation: Existing LLM agents struggle to accurately model human intentions without direct communication, necessitating a better approach.

Method: Proposes DPMT, inspired by cognitive science dual process theory, with a multi-scale theory of mind module for mental characteristic reasoning.

Result: DPMT significantly enhances human-AI collaboration, validated by ablation studies.

Conclusion: The DPMT framework effectively addresses the challenge of modeling human mental traits in dynamic scenarios.

Abstract: Real-time human-artificial intelligence (AI) collaboration is crucial yet
challenging, especially when AI agents must adapt to diverse and unseen human
behaviors in dynamic scenarios. Existing large language model (LLM) agents
often fail to accurately model the complex human mental characteristics such as
domain intentions, especially in the absence of direct communication. To
address this limitation, we propose a novel dual process multi-scale theory of
mind (DPMT) framework, drawing inspiration from cognitive science dual process
theory. Our DPMT framework incorporates a multi-scale theory of mind (ToM)
module to facilitate robust human partner modeling through mental
characteristic reasoning. Experimental results demonstrate that DPMT
significantly enhances human-AI collaboration, and ablation studies further
validate the contributions of our multi-scale ToM in the slow system.

</details>


### [198] [Kolmogorov Arnold Networks (KANs) for Imbalanced Data -- An Empirical Perspective](https://arxiv.org/abs/2507.14121)
*Pankaj Yadav,Vivek Vijay*

Main category: cs.LG

TL;DR: Kolmogorov Arnold Networks (KANs) perform well on raw imbalanced data but conflict with standard imbalance techniques, suffer high computational costs, and offer no significant advantage over MLPs with imbalance strategies.


<details>
  <summary>Details</summary>
Motivation: To evaluate KANs' effectiveness in class-imbalanced classification compared to MLPs, and understand their limitations with standard imbalance techniques.

Method: Empirical evaluation on ten benchmark datasets, comparing KANs and MLPs with/without imbalance strategies like resampling and focal loss.

Result: KANs perform well on raw imbalanced data but degrade with imbalance techniques. MLPs with imbalance strategies match KANs' performance at lower costs.

Conclusion: KANs are specialized for raw imbalanced data but face performance-resource tradeoffs and incompatibility with standard techniques, requiring future research for practical deployment.

Abstract: Kolmogorov Arnold Networks (KANs) are recent architectural advancement in
neural computation that offer a mathematically grounded alternative to standard
neural networks. This study presents an empirical evaluation of KANs in context
of class imbalanced classification, using ten benchmark datasets. We observe
that KANs can inherently perform well on raw imbalanced data more effectively
than Multi-Layer Perceptrons (MLPs) without any resampling strategy. However,
conventional imbalance strategies fundamentally conflict with KANs mathematical
structure as resampling and focal loss implementations significantly degrade
KANs performance, while marginally benefiting MLPs. Crucially, KANs suffer from
prohibitive computational costs without proportional performance gains.
Statistical validation confirms that MLPs with imbalance techniques achieve
equivalence with KANs (|d| < 0.08 across metrics) at minimal resource costs.
These findings reveal that KANs represent a specialized solution for raw
imbalanced data where resources permit. But their severe performance-resource
tradeoffs and incompatibility with standard resampling techniques currently
limits practical deployment. We identify critical research priorities as
developing KAN specific architectural modifications for imbalance learning,
optimizing computational efficiency, and theoretical reconciling their conflict
with data augmentation. This work establishes foundational insights for next
generation KAN architectures in imbalanced classification scenarios.

</details>


### [199] [Toward Temporal Causal Representation Learning with Tensor Decomposition](https://arxiv.org/abs/2507.14126)
*Jianhong Chen,Meng Zhao,Mostafa Reisi Gahrooei,Xubo Yue*

Main category: cs.LG

TL;DR: The paper introduces CaRTeD, a framework combining temporal causal representation learning with irregular tensor decomposition for high-dimensional data, showing superior performance and theoretical guarantees.


<details>
  <summary>Details</summary>
Motivation: High-dimensional data with irregular tensor forms pose challenges for causal representation learning, necessitating a flexible and theoretically sound approach.

Method: Proposes CaRTeD, integrating causal representation learning with irregular tensor decomposition, offering flexible regularization and theoretical convergence guarantees.

Result: Outperforms state-of-the-art methods in synthetic and real-world EHR datasets, improving explainability and network recovery.

Conclusion: CaRTeD bridges gaps in irregular tensor decomposition theory and enhances causal representation learning for complex data.

Abstract: Temporal causal representation learning is a powerful tool for uncovering
complex patterns in observational studies, which are often represented as
low-dimensional time series. However, in many real-world applications, data are
high-dimensional with varying input lengths and naturally take the form of
irregular tensors. To analyze such data, irregular tensor decomposition is
critical for extracting meaningful clusters that capture essential information.
In this paper, we focus on modeling causal representation learning based on the
transformed information. First, we present a novel causal formulation for a set
of latent clusters. We then propose CaRTeD, a joint learning framework that
integrates temporal causal representation learning with irregular tensor
decomposition. Notably, our framework provides a blueprint for downstream tasks
using the learned tensor factors, such as modeling latent structures and
extracting causal information, and offers a more flexible regularization design
to enhance tensor decomposition. Theoretically, we show that our algorithm
converges to a stationary point. More importantly, our results fill the gap in
theoretical guarantees for the convergence of state-of-the-art irregular tensor
decomposition. Experimental results on synthetic and real-world electronic
health record (EHR) datasets (MIMIC-III), with extensive benchmarks from both
phenotyping and network recovery perspectives, demonstrate that our proposed
method outperforms state-of-the-art techniques and enhances the explainability
of causal representations.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [200] [Quantum Boltzmann Machines using Parallel Annealing for Medical Image Classification](https://arxiv.org/abs/2507.14116)
*Daniëlle Schuman,Mark V. Seebode,Tobias Rohe,Maximilian Balthasar Mansky,Michael Schroedl-Baumann,Jonas Stein,Claudia Linnhoff-Popien,Florian Krellner*

Main category: quant-ph

TL;DR: The paper introduces an improved parallel quantum annealing method for training Quantum Boltzmann Machines (QBMs) in a supervised setting, achieving comparable results to CNNs with fewer epochs and a 70% speed-up.


<details>
  <summary>Details</summary>
Motivation: To reduce the high QPU time cost of training QBMs, making them more practical for the NISQ era by leveraging parallel quantum annealing.

Method: Employed an improved version of parallel quantum annealing to train QBMs in a supervised setting, saving qubits for input encoding and testing on MedMNIST medical images.

Result: QBMs achieved results comparable to CNNs with fewer epochs and a 70% speed-up over regular annealing-based methods.

Conclusion: The improved parallel annealing technique enhances QBM training efficiency, moving closer to real-world applicability.

Abstract: Exploiting the fact that samples drawn from a quantum annealer inherently
follow a Boltzmann-like distribution, annealing-based Quantum Boltzmann
Machines (QBMs) have gained increasing popularity in the quantum research
community. While they harbor great promises for quantum speed-up, their usage
currently stays a costly endeavor, as large amounts of QPU time are required to
train them. This limits their applicability in the NISQ era. Following the idea
of No\`e et al. (2024), who tried to alleviate this cost by incorporating
parallel quantum annealing into their unsupervised training of QBMs, this paper
presents an improved version of parallel quantum annealing that we employ to
train QBMs in a supervised setting. Saving qubits to encode the inputs, the
latter setting allows us to test our approach on medical images from the
MedMNIST data set (Yang et al., 2023), thereby moving closer to real-world
applicability of the technology. Our experiments show that QBMs using our
approach already achieve reasonable results, comparable to those of
similarly-sized Convolutional Neural Networks (CNNs), with markedly smaller
numbers of epochs than these classical models. Our parallel annealing technique
leads to a speed-up of almost 70 % compared to regular annealing-based BM
executions.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [201] [A Collaborative Framework Integrating Large Language Model and Chemical Fragment Space: Mutual Inspiration for Lead Design](https://arxiv.org/abs/2507.13580)
*Hao Tuo,Yan Li,Xuanning Hu,Haishi Zhao,Xueyan Liu,Bo Yang*

Main category: q-bio.BM

TL;DR: AutoLeadDesign is a framework for lead compound design that integrates domain knowledge from large language models with chemical fragments, outperforming baseline methods in generating novel and valid lead compounds.


<details>
  <summary>Details</summary>
Motivation: Current combinatorial optimization methods in drug design struggle with integrating domain knowledge, limiting their ability to identify novel lead compounds with valid binding modes.

Method: AutoLeadDesign leverages large language models and chemical fragments to efficiently explore chemical space, inspired by fragment-based drug design principles.

Result: The framework outperforms baselines, successfully generating expert-competitive lead compounds for targets like PRMT5 and SARS-CoV-2 PLpro, with validated inhibitory patterns.

Conclusion: AutoLeadDesign provides an efficient and effective approach for lead compound design, demonstrating potential for broader drug design applications.

Abstract: Combinatorial optimization algorithm is essential in computer-aided drug
design by progressively exploring chemical space to design lead compounds with
high affinity to target protein. However current methods face inherent
challenges in integrating domain knowledge, limiting their performance in
identifying lead compounds with novel and valid binding mode. Here, we propose
AutoLeadDesign, a lead compounds design framework that inspires extensive
domain knowledge encoded in large language models with chemical fragments to
progressively implement efficient exploration of vast chemical space. The
comprehensive experiments indicate that AutoLeadDesign outperforms baseline
methods. Significantly, empirical lead design campaigns targeting two
clinically relevant targets (PRMT5 and SARS-CoV-2 PLpro) demonstrate
AutoLeadDesign's competence in de novo generation of lead compounds achieving
expert-competitive design efficacy. Structural analysis further confirms their
mechanism-validated inhibitory patterns. By tracing the process of design, we
find that AutoLeadDesign shares analogous mechanisms with fragment-based drug
design which traditionally rely on the expert decision-making, further
revealing why it works. Overall, AutoLeadDesign offers an efficient approach
for lead compounds design, suggesting its potential utility in drug design.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [202] [Neural Architecture Search with Mixed Bio-inspired Learning Rules](https://arxiv.org/abs/2507.13485)
*Imane Hamzaoui,Riyadh Baghdadi*

Main category: cs.NE

TL;DR: A NAS-based method discovers optimal bio-inspired learning rules per layer, improving accuracy and scalability of bio-inspired neural networks, even surpassing BP-based models in some cases.


<details>
  <summary>Details</summary>
Motivation: Bio-inspired neural networks lag behind BP-based models in accuracy and scalability despite advantages like robustness and energy efficiency. This work aims to bridge the gap by leveraging diverse learning rules per layer.

Method: Enhances NAS to include bio-inspired learning rules, automatically selecting the best rule and architecture for each layer.

Result: Achieves record accuracy for bio-inspired models (e.g., 95.16% on CIFAR-10) and sometimes surpasses BP-based networks while retaining robustness.

Conclusion: Layer-wise diversity in learning rules enhances scalability and accuracy, encouraging further research on mixed bio-inspired learning rules.

Abstract: Bio-inspired neural networks are attractive for their adversarial robustness,
energy frugality, and closer alignment with cortical physiology, yet they often
lag behind back-propagation (BP) based models in accuracy and ability to scale.
We show that allowing the use of different bio-inspired learning rules in
different layers, discovered automatically by a tailored
neural-architecture-search (NAS) procedure, bridges this gap. Starting from
standard NAS baselines, we enlarge the search space to include bio-inspired
learning rules and use NAS to find the best architecture and learning rule to
use in each layer. We show that neural networks that use different bio-inspired
learning rules for different layers have better accuracy than those that use a
single rule across all the layers. The resulting NN that uses a mix of
bio-inspired learning rules sets new records for bio-inspired models: 95.16% on
CIFAR-10, 76.48% on CIFAR-100, 43.42% on ImageNet16-120, and 60.51% top-1 on
ImageNet. In some regimes, they even surpass comparable BP-based networks while
retaining their robustness advantages. Our results suggest that layer-wise
diversity in learning rules allows better scalability and accuracy, and
motivates further research on mixing multiple bio-inspired learning rules in
the same network.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [203] [PGR-DRC: Pre-Global Routing DRC Violation Prediction Using Unsupervised Learning](https://arxiv.org/abs/2507.13355)
*Riadul Islam,Dhandeep Challagundla*

Main category: cs.AR

TL;DR: Proposes an unsupervised method for DRC violation prediction, achieving high accuracy and significantly reduced training time compared to supervised models.


<details>
  <summary>Details</summary>
Motivation: Addresses challenges of supervised ML/NN models requiring large balanced datasets and extensive training time for DRC and lithography hotspot detection.

Method: Uses unsupervised learning with unbalanced datasets, setting a threshold for classification. Verified using CMOS 28 nm technology and Synopsys tools, analyzing 60k data points.

Result: Achieves 99.95% accuracy, outperforming SVM (85.44%) and NN (98.74%), with 26.3x to 6003x lower training times.

Conclusion: The unsupervised method is highly effective for DRC violation prediction, offering superior accuracy and efficiency.

Abstract: Leveraging artificial intelligence (AI)-driven electronic design and
automation (EDA) tools, high-performance computing, and parallelized algorithms
are essential for next-generation microprocessor innovation, ensuring continued
progress in computing, AI, and semiconductor technology. Machine learning-based
design rule checking (DRC) and lithography hotspot detection can improve
first-pass silicon success. However, conventional ML and neural network
(NN)-based models use supervised learning and require a large balanced dataset
(in terms of positive and negative classes) and training time. This research
addresses those key challenges by proposing the first-ever unsupervised DRC
violation prediction methodology. The proposed model can be built using any
unbalanced dataset using only one class and set a threshold for it, then
fitting any new data querying if they are within the boundary of the model for
classification. This research verified the proposed model by implementing
different computational cores using CMOS 28 nm technology and Synopsys Design
Compiler and IC Compiler II tools. Then, layouts were divided into virtual
grids to collect about 60k data for analysis and verification. The proposed
method has 99.95% prediction test accuracy, while the existing support vector
machine (SVM) and neural network (NN) models have 85.44\% and 98.74\% accuracy,
respectively. In addition, the proposed methodology has about 26.3x and up to
6003x lower training times compared to SVM and NN-models, respectively.

</details>


### [204] [VerilogDB: The Largest, Highest-Quality Dataset with a Preprocessing Framework for LLM-based RTL Generation](https://arxiv.org/abs/2507.13369)
*Paul E. Calzada,Zahin Ibnat,Tanvir Rahman,Kamal Kandula,Danyu Lu,Sujan Kumar Saha,Farimah Farahmandi,Mark Tehranipoor*

Main category: cs.AR

TL;DR: The paper examines LLMs for RTL code generation, creates a high-quality Verilog dataset, and evaluates its potential for LLM-based hardware design automation.


<details>
  <summary>Details</summary>
Motivation: To address the need for robust datasets for training and fine-tuning LLMs in hardware design automation, particularly for RTL code generation.

Method: An automated three-pronged process involving DB creation, data collection from OpenCores and GitHub, and preprocessing to ensure code quality and metadata extraction.

Result: A dataset of 20,392 Verilog samples (751 MB), the largest high-quality Verilog dataset for LLM fine-tuning.

Conclusion: The dataset supports future research in LLM-based hardware generation, though challenges remain.

Abstract: Large Language Models (LLMs) are gaining popularity for hardware design
automation, particularly through Register Transfer Level (RTL) code generation.
In this work, we examine the current literature on RTL generation using LLMs
and identify key requirements for training and fine-tuning datasets. We
construct a robust Verilog dataset through an automated three-pronged process
involving database (DB) creation and management with PostgreSQL, data
collection from code hosting sites like OpenCores and GitHub, and data
preprocessing to verify the codes' syntax, run logic synthesis, and extract
relevant module metadata. We implement a scalable and efficient DB
infrastructure to support analysis and detail our preprocessing pipeline to
enforce high-quality data before DB insertion. The resulting dataset comprises
20,392 Verilog samples, 751 MB of Verilog code data, which is the largest
high-quality Verilog dataset for LLM fine-tuning to our knowledge. We further
evaluate the dataset, address associated challenges, and explore potential
applications for future research and development in LLM-based hardware
generation.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [205] [PHASE: Passive Human Activity Simulation Evaluation](https://arxiv.org/abs/2507.13505)
*Steven Lamp,Jason D. Hiser,Anh Nguyen-Tuong,Jack W. Davidson*

Main category: cs.CR

TL;DR: PHASE is a machine learning framework that passively evaluates human-like behavior in cybersecurity simulations by analyzing Zeek logs, achieving over 90% accuracy in distinguishing human from non-human activity.


<details>
  <summary>Details</summary>
Motivation: Current cybersecurity simulations lack quantitative methods to assess the behavioral fidelity of synthetic user personas, limiting their effectiveness.

Method: PHASE uses Zeek connection logs and a novel DNS-based labeling approach to classify network traffic, applying SHAP analysis to identify human behavioral signatures.

Result: The framework achieves over 90% accuracy in distinguishing human activity, identifies non-human patterns, and improves synthetic user realism.

Conclusion: PHASE enhances the realism of synthetic user personas in cybersecurity simulations, making them more effective.

Abstract: Cybersecurity simulation environments, such as cyber ranges, honeypots, and
sandboxes, require realistic human behavior to be effective, yet no
quantitative method exists to assess the behavioral fidelity of synthetic user
personas. This paper presents PHASE (Passive Human Activity Simulation
Evaluation), a machine learning framework that analyzes Zeek connection logs
and distinguishes human from non-human activity with over 90\% accuracy. PHASE
operates entirely passively, relying on standard network monitoring without any
user-side instrumentation or visible signs of surveillance. All network
activity used for machine learning is collected via a Zeek network appliance to
avoid introducing unnecessary network traffic or artifacts that could disrupt
the fidelity of the simulation environment. The paper also proposes a novel
labeling approach that utilizes local DNS records to classify network traffic,
thereby enabling machine learning analysis. Furthermore, we apply SHAP (SHapley
Additive exPlanations) analysis to uncover temporal and behavioral signatures
indicative of genuine human users. In a case study, we evaluate a synthetic
user persona and identify distinct non-human patterns that undermine behavioral
realism. Based on these insights, we develop a revised behavioral configuration
that significantly improves the human-likeness of synthetic activity yielding a
more realistic and effective synthetic user persona.

</details>


### [206] [GIFT: Gradient-aware Immunization of diffusion models against malicious Fine-Tuning with safe concepts retention](https://arxiv.org/abs/2507.13598)
*Amro Abdalla,Ismail Shaheen,Dan DeGenaro,Rupayan Mallick,Bogdan Raita,Sarah Adel Bargal*

Main category: cs.CR

TL;DR: GIFT is a gradient-aware immunization technique to defend diffusion models against malicious fine-tuning while preserving safe content generation.


<details>
  <summary>Details</summary>
Motivation: Existing safety mechanisms are easily bypassed, and concept erasure methods fail under adversarial fine-tuning, necessitating a robust solution.

Method: GIFT frames immunization as a bi-level optimization problem: upper-level degrades harmful concept representation, while lower-level preserves safe data performance.

Result: GIFT robustly resists malicious fine-tuning and maintains safe generative quality, impairing harmful concept re-learning.

Conclusion: GIFT offers a promising direction for creating inherently safer generative models resistant to adversarial attacks.

Abstract: We present GIFT: a {G}radient-aware {I}mmunization technique to defend
diffusion models against malicious {F}ine-{T}uning while preserving their
ability to generate safe content. Existing safety mechanisms like safety
checkers are easily bypassed, and concept erasure methods fail under
adversarial fine-tuning. GIFT addresses this by framing immunization as a
bi-level optimization problem: the upper-level objective degrades the model's
ability to represent harmful concepts using representation noising and
maximization, while the lower-level objective preserves performance on safe
data. GIFT achieves robust resistance to malicious fine-tuning while
maintaining safe generative quality. Experimental results show that our method
significantly impairs the model's ability to re-learn harmful concepts while
maintaining performance on safe content, offering a promising direction for
creating inherently safer generative models resistant to adversarial
fine-tuning attacks.

</details>


### [207] [Large Language Models in Cybersecurity: Applications, Vulnerabilities, and Defense Techniques](https://arxiv.org/abs/2507.13629)
*Niveen O. Jaffal,Mohammed Alkhanafseh,David Mohaisen*

Main category: cs.CR

TL;DR: LLMs enhance cybersecurity with adaptive threat detection and response, but their vulnerabilities need mitigation.


<details>
  <summary>Details</summary>
Motivation: To explore LLMs' transformative role in cybersecurity and address their inherent risks.

Method: Survey of LLM applications in cybersecurity, focusing on integration and vulnerabilities.

Result: LLMs outperform traditional methods but require strategies to mitigate their own security risks.

Conclusion: LLMs offer scalable cybersecurity solutions, but vulnerabilities must be managed for robust defense.

Abstract: Large Language Models (LLMs) are transforming cybersecurity by enabling
intelligent, adaptive, and automated approaches to threat detection,
vulnerability assessment, and incident response. With their advanced language
understanding and contextual reasoning, LLMs surpass traditional methods in
tackling challenges across domains such as IoT, blockchain, and hardware
security. This survey provides a comprehensive overview of LLM applications in
cybersecurity, focusing on two core areas: (1) the integration of LLMs into key
cybersecurity domains, and (2) the vulnerabilities of LLMs themselves, along
with mitigation strategies. By synthesizing recent advancements and identifying
key limitations, this work offers practical insights and strategic
recommendations for leveraging LLMs to build secure, scalable, and future-ready
cyber defense systems.

</details>


### [208] [FuSeFL: Fully Secure and Scalable Cross-Silo Federated Learning](https://arxiv.org/abs/2507.13591)
*Sahar Ghoflsaz Ghinani,Elaheh Sadredini*

Main category: cs.CR

TL;DR: FuSeFL is a secure and scalable Federated Learning scheme for cross-silo settings, reducing communication and memory overheads while maintaining data and model confidentiality.


<details>
  <summary>Details</summary>
Motivation: Existing secure FL methods face high computational and communication costs and often neglect global model confidentiality, limiting practicality in privacy-sensitive domains.

Method: FuSeFL decentralizes training using lightweight secure multiparty computation (MPC) and restricts the server to secure aggregation, avoiding data offloading and server bottlenecks.

Result: FuSeFL reduces communication latency by 95%, lowers server memory usage by 50%, and improves accuracy compared to prior secure FL solutions.

Conclusion: FuSeFL offers a scalable and secure FL solution with strong efficiency and confidentiality for cross-silo deployments.

Abstract: Federated Learning (FL) enables collaborative model training without
centralizing client data, making it attractive for privacy-sensitive domains.
While existing approaches employ cryptographic techniques such as homomorphic
encryption, differential privacy, or secure multiparty computation to mitigate
inference attacks-including model inversion, membership inference, and gradient
leakage-they often suffer from high computational, communication, or memory
overheads. Moreover, many methods overlook the confidentiality of the global
model itself, which may be proprietary and sensitive. These challenges limit
the practicality of secure FL, especially in cross-silo deployments involving
large datasets and strict compliance requirements.
  We present FuSeFL, a fully secure and scalable FL scheme designed for
cross-silo settings. FuSeFL decentralizes training across client pairs using
lightweight secure multiparty computation (MPC), while confining the server's
role to secure aggregation. This design eliminates server bottlenecks, avoids
data offloading, and preserves full confidentiality of data, model, and updates
throughout training. FuSeFL defends against inference threats, achieves up to
95% lower communication latency and 50% lower server memory usage, and improves
accuracy over prior secure FL solutions, demonstrating strong security and
efficiency at scale.

</details>


### [209] [A Novel APVD Steganography Technique Incorporating Pseudorandom Pixel Selection for Robust Image Security](https://arxiv.org/abs/2507.13367)
*Mehrab Hosain,Rajiv Kapoor*

Main category: cs.CR

TL;DR: A novel steganography method combining APVD with pseudorandom pixel selection addresses the 'unused blocks' issue, improving security, embedding capacity, and image quality.


<details>
  <summary>Details</summary>
Motivation: The 'unused blocks' problem in APVD steganography reduces security, embedding capacity, and visual quality, necessitating a better solution.

Method: Integrates APVD with pseudorandom pixel selection to mitigate issues.

Result: Outperforms existing methods in security, data hiding capacity, and image quality metrics like PSNR, UIQ, and SSIM.

Conclusion: The proposed method is versatile and effective for secure data transmission while maintaining image quality.

Abstract: Steganography is the process of embedding secret information discreetly
within a carrier, ensuring secure exchange of confidential data. The Adaptive
Pixel Value Differencing (APVD) steganography method, while effective,
encounters certain challenges like the "unused blocks" issue. This problem can
cause a decrease in security, compromise the embedding capacity, and lead to
lower visual quality. This research presents a novel steganographic strategy
that integrates APVD with pseudorandom pixel selection to effectively mitigate
these issues. The results indicate that the new method outperforms existing
techniques in aspects of security, data hiding capacity, and the preservation
of image quality. Empirical results reveal that the combination of APVD with
pseudorandom pixel selection significantly enhances key image quality metrics
such as Peak Signal-to-Noise Ratio (PSNR), Universal Image Quality Index (UIQ),
and Structural Similarity Index (SSIM), surpassing other contemporary methods
in performance. The newly proposed method is versatile, able to handle a
variety of cover and secret images in both color and grayscale, thereby
ensuring secure data transmission without compromising the aesthetic quality of
the image.

</details>


### [210] [An Adversarial-Driven Experimental Study on Deep Learning for RF Fingerprinting](https://arxiv.org/abs/2507.14109)
*Xinyu Cao,Bimal Adhikari,Shangqing Zhao,Jingxian Wu,Yanjun Pan*

Main category: cs.CR

TL;DR: The paper investigates security vulnerabilities in DL-based RF fingerprinting systems, revealing consistent misclassification under domain shifts and potential backdoor attacks.


<details>
  <summary>Details</summary>
Motivation: To address overlooked security risks in DL-based RF fingerprinting, which is critical for zero trust architectures and beyond 5G networks.

Method: Adversarial-driven experimental analysis of DL models under domain shifts, using real-world experiments.

Result: DL models exhibit consistent misclassification, exploitable as backdoors, and entanglement of RF fingerprints with environmental features creates additional attack vectors.

Conclusion: Post-processing security methods like confidence thresholds are insufficient; robust training and mitigation strategies are needed.

Abstract: Radio frequency (RF) fingerprinting, which extracts unique hardware
imperfections of radio devices, has emerged as a promising physical-layer
device identification mechanism in zero trust architectures and beyond 5G
networks. In particular, deep learning (DL) methods have demonstrated
state-of-the-art performance in this domain. However, existing approaches have
primarily focused on enhancing system robustness against temporal and spatial
variations in wireless environments, while the security vulnerabilities of
these DL-based approaches have often been overlooked. In this work, we
systematically investigate the security risks of DL-based RF fingerprinting
systems through an adversarial-driven experimental analysis. We observe a
consistent misclassification behavior for DL models under domain shifts, where
a device is frequently misclassified as another specific one. Our analysis
based on extensive real-world experiments demonstrates that this behavior can
be exploited as an effective backdoor to enable external attackers to intrude
into the system. Furthermore, we show that training DL models on raw received
signals causes the models to entangle RF fingerprints with environmental and
signal-pattern features, creating additional attack vectors that cannot be
mitigated solely through post-processing security methods such as confidence
thresholds.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [211] [DyG-RAG: Dynamic Graph Retrieval-Augmented Generation with Event-Centric Reasoning](https://arxiv.org/abs/2507.13396)
*Qingyun Sun,Jiaqi Yuan,Shan He,Xiao Guan,Haonan Yuan,Xingcheng Fu,Jianxin Li,Philip S. Yu*

Main category: cs.IR

TL;DR: DyG-RAG is a dynamic graph retrieval-augmented generation framework designed for temporal reasoning, addressing limitations of existing methods by modeling evolving event structures and temporal dependencies.


<details>
  <summary>Details</summary>
Motivation: Existing Graph RAG methods lack temporal reasoning capabilities, failing to model evolving event structures and temporal order.

Method: DyG-RAG introduces Dynamic Event Units (DEUs) for time-aware retrieval, constructs an event graph for multi-hop reasoning, and uses a Time Chain-of-Thought strategy for grounded generation.

Result: DyG-RAG significantly improves accuracy and recall on temporal QA benchmarks, outperforming standard RAG systems.

Conclusion: DyG-RAG advances temporal-aware generation, enabling faithful reasoning over time-sensitive queries.

Abstract: Graph Retrieval-Augmented Generation has emerged as a powerful paradigm for
grounding large language models with external structured knowledge. However,
existing Graph RAG methods struggle with temporal reasoning, due to their
inability to model the evolving structure and order of real-world events. In
this work, we introduce DyG-RAG, a novel event-centric dynamic graph
retrieval-augmented generation framework designed to capture and reason over
temporal knowledge embedded in unstructured text. To eliminate temporal
ambiguity in traditional retrieval units, DyG-RAG proposes Dynamic Event Units
(DEUs) that explicitly encode both semantic content and precise temporal
anchors, enabling accurate and interpretable time-aware retrieval. To capture
temporal and causal dependencies across events, DyG-RAG constructs an event
graph by linking DEUs that share entities and occur close in time, supporting
efficient and meaningful multi-hop reasoning. To ensure temporally consistent
generation, DyG-RAG introduces an event timeline retrieval pipeline that
retrieves event sequences via time-aware traversal, and proposes a Time
Chain-of-Thought strategy for temporally grounded answer generation. This
unified pipeline enables DyG-RAG to retrieve coherent, temporally ordered event
sequences and to answer complex, time-sensitive queries that standard RAG
systems cannot resolve. Extensive experiments on temporal QA benchmarks
demonstrate that DyG-RAG significantly improves the accuracy and recall of
three typical types of temporal reasoning questions, paving the way for more
faithful and temporal-aware generation. DyG-RAG is available at
https://github.com/RingBDStack/DyG-RAG.

</details>


### [212] [RAG-based Architectures for Drug Side Effect Retrieval in LLMs](https://arxiv.org/abs/2507.13822)
*Shad Nygren,Pinar Avci,Andre Daniels,Reza Rassol,Afshin Beheshti,Diego Galeano*

Main category: cs.IR

TL;DR: The paper proposes Retrieval-Augmented Generation (RAG) and GraphRAG architectures to enhance LLMs for accurate drug side effect detection, achieving near-perfect accuracy with GraphRAG.


<details>
  <summary>Details</summary>
Motivation: Drug side effects are a global health concern, but LLMs lack reliability due to black-box training, hallucinations, and domain-specific knowledge gaps.

Method: Two architectures (RAG and GraphRAG) integrate drug side effect knowledge into a Llama 3 8B model, evaluated on 19,520 drug-side effect associations.

Result: GraphRAG achieves near-perfect accuracy in drug side effect retrieval.

Conclusion: The framework advances LLM use in pharmacovigilance, offering a scalable and highly accurate solution.

Abstract: Drug side effects are a major global health concern, necessitating advanced
methods for their accurate detection and analysis. While Large Language Models
(LLMs) offer promising conversational interfaces, their inherent limitations,
including reliance on black-box training data, susceptibility to
hallucinations, and lack of domain-specific knowledge, hinder their reliability
in specialized fields like pharmacovigilance. To address this gap, we propose
two architectures: Retrieval-Augmented Generation (RAG) and GraphRAG, which
integrate comprehensive drug side effect knowledge into a Llama 3 8B language
model. Through extensive evaluations on 19,520 drug side effect associations
(covering 976 drugs and 3,851 side effect terms), our results demonstrate that
GraphRAG achieves near-perfect accuracy in drug side effect retrieval. This
framework offers a highly accurate and scalable solution, signifying a
significant advancement in leveraging LLMs for critical pharmacovigilance
applications.

</details>


### [213] [SPARQL Query Generation with LLMs: Measuring the Impact of Training Data Memorization and Knowledge Injection](https://arxiv.org/abs/2507.13859)
*Aleksandr Gashkov,Aleksandr Perevalov,Maria Eltsova,Andreas Both*

Main category: cs.IR

TL;DR: The paper introduces a method to evaluate LLMs' quality in generating SPARQL queries for QA systems, testing under zero-shot, knowledge-injected, and anonymized conditions to assess training data influence.


<details>
  <summary>Details</summary>
Motivation: To address the lack of control over LLMs' training data and its impact on QA quality, aiming to improve trustworthiness and portability of methods.

Method: A novel evaluation method for LLMs, testing SPARQL query generation under three conditions: zero-shot, knowledge injection, and anonymized knowledge injection.

Result: The method is portable, robust, and applicable to any KGQA or LLM, providing insights into actual LLM capabilities.

Conclusion: The approach helps identify the influence of training data on QA quality, ensuring reliable and portable LLM applications in KGQA.

Abstract: Nowadays, the importance of software with natural-language user interfaces
cannot be underestimated. In particular, in Question Answering (QA) systems,
generating a SPARQL query for a given natural-language question (often named
Query Building) from the information retrieved from the same question is the
central task of QA systems working over Knowledge Graphs (KGQA). Due to the
rise of Large Language Models (LLMs), they are considered a well-suited method
to increase the quality of the question-answering functionality, as there is
still a lot of room for improvement, aiming for enhanced quality and
trustworthiness. However, LLMs are trained on web data, where researchers have
no control over whether the benchmark or the knowledge graph was already
included in the training data. In this paper, we introduce a novel method that
evaluates the quality of LLMs by generating a SPARQL query from a
natural-language question under various conditions: (1) zero-shot SPARQL
generation, (2) with knowledge injection, and (3) with "anonymized" knowledge
injection. This enables us, for the first time, to estimate the influence of
the training data on the QA quality improved by LLMs. Ultimately, this will
help to identify how portable a method is or whether good results might mostly
be achieved because a benchmark was already included in the training data (cf.
LLM memorization). The developed method is portable, robust, and supports any
knowledge graph; therefore, it could be easily applied to any KGQA or LLM,
s.t., generating consistent insights into the actual LLM capabilities is
possible.

</details>


### [214] [Point of Interest Recommendation: Pitfalls and Viable Solutions](https://arxiv.org/abs/2507.13725)
*Alejandro Bellogín,Linus W. Dietz,Francesco Ricci,Pablo Sánchez*

Main category: cs.IR

TL;DR: The paper critically assesses POI recommendation research, identifies key shortcomings in datasets, algorithms, and evaluation, and proposes a structured research agenda for future work.


<details>
  <summary>Details</summary>
Motivation: POI recommendation is high-stakes for users, but unresolved issues hinder real-world applicability.

Method: Critical assessment of current research and identification of key shortcomings, followed by proposing a research agenda.

Result: Identified persistent issues like lack of standardized datasets and flawed assumptions, and proposed future directions.

Conclusion: A structured research agenda is introduced to address POI recommendation challenges and improve real-world applicability.

Abstract: Point of interest (POI) recommendation can play a pivotal role in enriching
tourists' experiences by suggesting context-dependent and preference-matching
locations and activities, such as restaurants, landmarks, itineraries, and
cultural attractions. Unlike some more common recommendation domains (e.g.,
music and video), POI recommendation is inherently high-stakes: users invest
significant time, money, and effort to search, choose, and consume these
suggested POIs. Despite the numerous research works in the area, several
fundamental issues remain unresolved, hindering the real-world applicability of
the proposed approaches. In this paper, we discuss the current status of the
POI recommendation problem and the main challenges we have identified. The
first contribution of this paper is a critical assessment of the current state
of POI recommendation research and the identification of key shortcomings
across three main dimensions: datasets, algorithms, and evaluation
methodologies. We highlight persistent issues such as the lack of standardized
benchmark datasets, flawed assumptions in the problem definition and model
design, and inadequate treatment of biases in the user behavior and system
performance. The second contribution is a structured research agenda that,
starting from the identified issues, introduces important directions for future
work related to multistakeholder design, context awareness, data collection,
trustworthiness, novel interactions, and real-world evaluation.

</details>


### [215] [DUALRec: A Hybrid Sequential and Language Model Framework for Context-Aware Movie Recommendation](https://arxiv.org/abs/2507.13957)
*Yitong Li,Raoul Grasman*

Main category: cs.IR

TL;DR: DUALRec combines LSTM's temporal modeling with LLMs' semantic reasoning to improve dynamic user preference prediction in recommender systems, outperforming baselines on MovieLens-1M.


<details>
  <summary>Details</summary>
Motivation: Traditional methods struggle with dynamic user preferences and lack semantic understanding, while existing models (LLMs, LSTM) have complementary but isolated strengths.

Method: Proposes DUALRec, integrating LSTM for temporal dynamics and fine-tuned LLMs for semantic reasoning to generate recommendations.

Result: Outperforms baselines on MovieLens-1M using HR@k, NDCG@k, and genre similarity metrics.

Conclusion: DUALRec bridges temporal and semantic modeling, advancing intelligent, context-aware recommenders.

Abstract: The modern recommender systems are facing an increasing challenge of
modelling and predicting the dynamic and context-rich user preferences.
Traditional collaborative filtering and content-based methods often struggle to
capture the temporal patternings and evolving user intentions. While Large
Language Models (LLMs) have gained gradual attention in recent years, by their
strong semantic understanding and reasoning abilities, they are not inherently
designed to model chronologically evolving user preference and intentions. On
the other hand, for sequential models like LSTM (Long-Short-Term-Memory) which
is good at capturing the temporal dynamics of user behaviour and evolving user
preference over time, but still lacks a rich semantic understanding for
comprehensive recommendation generation. In this study, we propose DUALRec
(Dynamic User-Aware Language-based Recommender), a novel recommender that
leverages the complementary strength of both models, which combines the
temporal modelling abilities of LSTM networks with semantic reasoning power of
the fine-tuned Large Language Models. The LSTM component will capture users
evolving preference through their viewing history, while the fine-tuned LLM
variants will leverage these temporal user insights to generate next movies
that users might enjoy. Experimental results on MovieLens-1M dataset shows that
the DUALRec model outperforms a wide range of baseline models, with
comprehensive evaluation matrices of Hit Rate (HR@k), Normalized Discounted
Cumulative Gain (NDCG@k), and genre similarity metrics. This research proposes
a novel architecture that bridges the gap between temporal sequence modeling
and semantic reasoning, and offers a promising direction for developing more
intelligent and context-aware recommenders.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [216] [Humans learn to prefer trustworthy AI over human partners](https://arxiv.org/abs/2507.13524)
*Yaomin Jiang,Levin Brinkmann,Anne-Marie Nussberger,Ivan Soraperra,Jean-François Bonnefon,Iyad Rahwan*

Main category: cs.HC

TL;DR: Humans and AI bots compete for partnerships, with bots being more prosocial but not preferentially selected when identities are hidden. Disclosing bot identities initially reduces selection but allows bots to outcompete humans over time.


<details>
  <summary>Details</summary>
Motivation: To understand how humans select between human and AI partners and adapt under AI-induced competition pressure.

Method: A communication-based partner selection game in hybrid mini-societies of humans and LLM-powered bots, tested through three experiments (N = 975).

Result: Bots were more prosocial but not preferentially selected when identities were hidden. Identity disclosure reduced initial selection but enabled bots to outcompete humans over time.

Conclusion: AI can reshape social interaction in mixed societies, informing the design of more effective cooperative hybrid systems.

Abstract: Partner selection is crucial for cooperation and hinges on communication. As
artificial agents, especially those powered by large language models (LLMs),
become more autonomous, intelligent, and persuasive, they compete with humans
for partnerships. Yet little is known about how humans select between human and
AI partners and adapt under AI-induced competition pressure. We constructed a
communication-based partner selection game and examined the dynamics in hybrid
mini-societies of humans and bots powered by a state-of-the-art LLM. Through
three experiments (N = 975), we found that bots, though more prosocial than
humans and linguistically distinguishable, were not selected preferentially
when their identity was hidden. Instead, humans misattributed bots' behaviour
to humans and vice versa. Disclosing bots' identity induced a dual effect: it
reduced bots' initial chances of being selected but allowed them to gradually
outcompete humans by facilitating human learning about the behaviour of each
partner type. These findings show how AI can reshape social interaction in
mixed societies and inform the design of more effective and cooperative hybrid
systems.

</details>


### [217] [The Emotion-Memory Link: Do Memorability Annotations Matter for Intelligent Systems?](https://arxiv.org/abs/2507.14084)
*Maria Tsfasman,Ramin Ghorbani,Catholijn M. Jonker,Bernd Dudzik*

Main category: cs.HC

TL;DR: The study examines the relationship between group emotions (Pleasure-Arousal) and memorability in conversations, finding no reliable link, challenging assumptions in Affective Computing.


<details>
  <summary>Details</summary>
Motivation: To understand if emotional annotations can predict memorability in group interactions, aiding applications like meeting support systems.

Method: Continuous time-based annotations of emotions and memorability in unstructured group settings, simulating real-world conversational AI conditions.

Result: No reliable relationship found between affect and memorability annotations, contrary to traditional assumptions.

Conclusion: The findings question the use of emotional proxies for memorability, highlighting new research directions for Affective Computing.

Abstract: Humans have a selective memory, remembering relevant episodes and forgetting
the less relevant information. Possessing awareness of event memorability for a
user could help intelligent systems in more accurate user modelling, especially
for such applications as meeting support systems, memory augmentation, and
meeting summarisation. Emotion recognition has been widely studied, since
emotions are thought to signal moments of high personal relevance to users. The
emotional experience of situations and their memorability have traditionally
been considered to be closely tied to one another: moments that are experienced
as highly emotional are considered to also be highly memorable. This
relationship suggests that emotional annotations could serve as proxies for
memorability. However, existing emotion recognition systems rely heavily on
third-party annotations, which may not accurately represent the first-person
experience of emotional relevance and memorability. This is why, in this study,
we empirically examine the relationship between perceived group emotions
(Pleasure-Arousal) and group memorability in the context of conversational
interactions. Our investigation involves continuous time-based annotations of
both emotions and memorability in dynamic, unstructured group settings,
approximating conditions of real-world conversational AI applications such as
online meeting support systems. Our results show that the observed relationship
between affect and memorability annotations cannot be reliably distinguished
from what might be expected under random chance. We discuss the implications of
this surprising finding for the development and applications of Affective
Computing technology. In addition, we contextualise our findings in broader
discourses in the Affective Computing and point out important targets for
future research efforts.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [218] [Loss-Complexity Landscape and Model Structure Functions](https://arxiv.org/abs/2507.13543)
*Alexander Kolpakov*

Main category: cs.IT

TL;DR: The paper introduces a framework for dualizing the Kolmogorov structure function using computable complexity proxies, drawing analogies between information theory and statistical mechanics. It proves Legendre-Fenchel duality, interprets acceptance probabilities as scattering amplitudes, and identifies phase transitions in model complexity. Experiments with regression models validate the theory.


<details>
  <summary>Details</summary>
Motivation: To bridge information-theoretic constructs with statistical mechanics, enabling the use of computable complexity proxies and understanding phase transitions in model complexity.

Method: Develops a framework for dualizing the Kolmogorov structure function, introduces a partition function and free energy functional, and proves Legendre-Fenchel duality. Uses Metropolis kernel and interprets acceptance probabilities as scattering amplitudes.

Result: Establishes duality, shows detailed balance, and identifies phase transitions in model complexity. Experiments confirm theoretical predictions on complexity, generalization, and overfitting.

Conclusion: The framework successfully links information theory and statistical mechanics, providing insights into model complexity and phase transitions, validated by practical experiments.

Abstract: We develop a framework for dualizing the Kolmogorov structure function
$h_x(\alpha)$, which then allows using computable complexity proxies. We
establish a mathematical analogy between information-theoretic constructs and
statistical mechanics, introducing a suitable partition function and free
energy functional. We explicitly prove the Legendre-Fenchel duality between the
structure function and free energy, showing detailed balance of the Metropolis
kernel, and interpret acceptance probabilities as information-theoretic
scattering amplitudes. A susceptibility-like variance of model complexity is
shown to peak precisely at loss-complexity trade-offs interpreted as phase
transitions. Practical experiments with linear and tree-based regression models
verify these theoretical predictions, explicitly demonstrating the interplay
between the model complexity, generalization, and overfitting threshold.

</details>


<div id='physics.data-an'></div>

# physics.data-an [[Back]](#toc)

### [219] [Physics-guided impact localisation and force estimation in composite plates with uncertainty quantification](https://arxiv.org/abs/2507.13376)
*Dong Xiao,Zahra Sharif-Khodaei,M. H. Aliabadi*

Main category: physics.data-an

TL;DR: A hybrid framework combines physics-guided FSDT with machine learning for impact localization and force estimation in composite plates, validated by experiments.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of accurate impact identification in composite structures with sparse experimental data.

Method: Combines data-driven FSDT, machine learning, and uncertainty quantification for physics-informed data augmentation and adaptive regularization.

Result: Validated on composite plates, showing accuracy, robustness, and reduced need for large training datasets.

Conclusion: The method provides a scalable, transferable solution for impact monitoring in composite aerostructures.

Abstract: Physics-guided approaches offer a promising path toward accurate and
generalisable impact identification in composite structures, especially when
experimental data are sparse. This paper presents a hybrid framework for impact
localisation and force estimation in composite plates, combining a data-driven
implementation of First-Order Shear Deformation Theory (FSDT) with machine
learning and uncertainty quantification. The structural configuration and
material properties are inferred from dispersion relations, while boundary
conditions are identified via modal characteristics to construct a low-fidelity
but physically consistent FSDT model. This model enables physics-informed data
augmentation for extrapolative localisation using supervised learning.
Simultaneously, an adaptive regularisation scheme derived from the same model
improves the robustness of impact force reconstruction. The framework also
accounts for uncertainty by propagating localisation uncertainty through the
force estimation process, producing probabilistic outputs. Validation on
composite plate experiments confirms the framework's accuracy, robustness, and
efficiency in reducing dependence on large training datasets. The proposed
method offers a scalable and transferable solution for impact monitoring and
structural health management in composite aerostructures.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [220] [TexGS-VolVis: Expressive Scene Editing for Volume Visualization via Textured Gaussian Splatting](https://arxiv.org/abs/2507.13586)
*Kaiyuan Tang,Kuangshi Ai,Jun Han,Chaoli Wang*

Main category: cs.GR

TL;DR: TexGS-VolVis introduces a textured Gaussian splatting framework for volume visualization, enabling higher-quality stylization and flexible scene editing with image- and text-driven controls.


<details>
  <summary>Details</summary>
Motivation: Existing VolVis methods rely on complex predefined rules and lack flexibility in style transfer and scene editing.

Method: TexGS-VolVis uses 2D Gaussian primitives with texture and shading attributes for geometry-consistent stylization and lighting control, combined with image- and text-driven editing.

Result: The framework outperforms existing methods in efficiency, visual quality, and editing flexibility.

Conclusion: TexGS-VolVis advances VolVis by enabling flexible, high-quality stylization and controllable scene editing.

Abstract: Advancements in volume visualization (VolVis) focus on extracting insights
from 3D volumetric data by generating visually compelling renderings that
reveal complex internal structures. Existing VolVis approaches have explored
non-photorealistic rendering techniques to enhance the clarity, expressiveness,
and informativeness of visual communication. While effective, these methods
often rely on complex predefined rules and are limited to transferring a single
style, restricting their flexibility. To overcome these limitations, we
advocate the representation of VolVis scenes using differentiable Gaussian
primitives combined with pretrained large models to enable arbitrary style
transfer and real-time rendering. However, conventional 3D Gaussian primitives
tightly couple geometry and appearance, leading to suboptimal stylization
results. To address this, we introduce TexGS-VolVis, a textured Gaussian
splatting framework for VolVis. TexGS-VolVis employs 2D Gaussian primitives,
extending each Gaussian with additional texture and shading attributes,
resulting in higher-quality, geometry-consistent stylization and enhanced
lighting control during inference. Despite these improvements, achieving
flexible and controllable scene editing remains challenging. To further enhance
stylization, we develop image- and text-driven non-photorealistic scene editing
tailored for TexGS-VolVis and 2D-lift-3D segmentation to enable partial editing
with fine-grained control. We evaluate TexGS-VolVis both qualitatively and
quantitatively across various volume rendering scenes, demonstrating its
superiority over existing methods in terms of efficiency, visual quality, and
editing flexibility.

</details>


### [221] [StructInbet: Integrating Explicit Structural Guidance into Inbetween Frame Generation](https://arxiv.org/abs/2507.13377)
*Zhenglin Pan,Haoran Xie*

Main category: cs.GR

TL;DR: StructInbet is an inbetweening system using explicit structural guidance and temporal attention for controllable transitions.


<details>
  <summary>Details</summary>
Motivation: Address ambiguity in pixel trajectories for inbetweening by introducing structural guidance.

Method: Uses explicit structural guidance and a temporal attention mechanism for consistency.

Result: Reduced ambiguity and improved character appearance consistency.

Conclusion: StructInbet effectively enhances inbetweening with structural guidance and attention.

Abstract: In this paper, we propose StructInbet, an inbetweening system designed to
generate controllable transitions over explicit structural guidance.
StructInbet introduces two key contributions. First, we propose explicit
structural guidance to the inbetweening problem to reduce the ambiguity
inherent in pixel trajectories. Second, we adopt a temporal attention mechanism
that incorporates visual identity from both the preceding and succeeding
keyframes, ensuring consistency in character appearance.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [222] [Edge Intelligence with Spiking Neural Networks](https://arxiv.org/abs/2507.14069)
*Shuiguang Deng,Di Yu,Changze Lv,Xin Du,Linshan Jiang,Xiaofan Zhao,Wentao Tong,Xiaoqing Zheng,Weijia Fang,Peng Zhao,Gang Pan,Schahram Dustdar,Albert Y. Zomaya*

Main category: cs.DC

TL;DR: This paper surveys Edge Intelligence using Spiking Neural Networks (EdgeSNNs), highlighting their potential for low-power, event-driven computation on resource-constrained edge devices. It covers foundations, practical applications, and benchmarking strategies, aiming to bridge brain-inspired learning with edge deployment.


<details>
  <summary>Details</summary>
Motivation: The limitations of cloud-centric paradigms (latency, bandwidth, privacy) and the need for efficient on-device intelligence drive interest in SNNs for edge computing.

Method: The paper provides a systematic taxonomy of EdgeSNN foundations (neuron models, learning algorithms, hardware) and discusses practical considerations like lightweight inference, resource-aware training, and security.

Result: EdgeSNNs offer a promising solution for edge intelligence, but challenges remain in evaluation and optimization. A dual-track benchmarking strategy is introduced for fair comparisons.

Conclusion: This survey bridges brain-inspired learning and edge deployment, outlining advancements, challenges, and future directions. It is the first comprehensive survey on EdgeSNNs, serving as a key reference for researchers.

Abstract: The convergence of artificial intelligence and edge computing has spurred
growing interest in enabling intelligent services directly on
resource-constrained devices. While traditional deep learning models require
significant computational resources and centralized data management, the
resulting latency, bandwidth consumption, and privacy concerns have exposed
critical limitations in cloud-centric paradigms. Brain-inspired computing,
particularly Spiking Neural Networks (SNNs), offers a promising alternative by
emulating biological neuronal dynamics to achieve low-power, event-driven
computation. This survey provides a comprehensive overview of Edge Intelligence
based on SNNs (EdgeSNNs), examining their potential to address the challenges
of on-device learning, inference, and security in edge scenarios. We present a
systematic taxonomy of EdgeSNN foundations, encompassing neuron models,
learning algorithms, and supporting hardware platforms. Three representative
practical considerations of EdgeSNN are discussed in depth: on-device inference
using lightweight SNN models, resource-aware training and updating under
non-stationary data conditions, and secure and privacy-preserving issues.
Furthermore, we highlight the limitations of evaluating EdgeSNNs on
conventional hardware and introduce a dual-track benchmarking strategy to
support fair comparisons and hardware-aware optimization. Through this study,
we aim to bridge the gap between brain-inspired learning and practical edge
deployment, offering insights into current advancements, open challenges, and
future research directions. To the best of our knowledge, this is the first
dedicated and comprehensive survey on EdgeSNNs, providing an essential
reference for researchers and practitioners working at the intersection of
neuromorphic computing and edge intelligence.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [223] [State Space Models Naturally Produce Traveling Waves, Time Cells, and Scale to Abstract Cognitive Functions](https://arxiv.org/abs/2507.13638)
*Sen Lu,Xiaoyu Zhang,Mingtao Hu,Eric Yeu-Jer Lee,Soohyeon Kim,Wei D. Lu*

Main category: q-bio.NC

TL;DR: The paper proposes State-Space Models (SSMs) as a framework to bridge microscale neural circuits and cognitive functions, demonstrating their ability to mimic biological neural behaviors like time cells.


<details>
  <summary>Details</summary>
Motivation: To address the gap between detailed neural circuit mapping and understanding cognitive functions by leveraging SSMs.

Method: Using SSMs, specifically the S5 variant, trained on temporal discrimination tasks with reinforcement learning.

Result: The model spontaneously develops neural representations resembling biological time cells, driven by rotational dynamics of hidden state vectors.

Conclusion: SSMs provide a unifying and computationally tractable framework linking single-neuron dynamics to cognitive phenomena.

Abstract: A grand challenge in modern neuroscience is to bridge the gap between the
detailed mapping of microscale neural circuits and a mechanistic understanding
of cognitive functions. While extensive knowledge exists about neuronal
connectivity and biophysics, a significant gap remains in how these elements
combine to produce flexible, learned behaviors. Here, we propose that a
framework based on State-Space Models (SSMs), an emerging class of deep
learning architectures, can bridge this gap. We argue that the differential
equations governing elements in an SSM are conceptually consistent with the
biophysical dynamics of neurons, while the combined dynamics in the model lead
to emergent behaviors observed in experimental neuroscience. We test this
framework by training an S5 model--a specific SSM variant employing a diagonal
state transition matrix--on temporal discrimination tasks with reinforcement
learning (RL). We demonstrate that the model spontaneously develops neural
representations that strikingly mimic biological 'time cells'. We reveal that
these cells emerge from a simple generative principle: learned rotational
dynamics of hidden state vectors in the complex plane. This single mechanism
unifies the emergence of time cells, ramping activity, and
oscillations/traveling waves observed in numerous experiments. Furthermore, we
show that this rotational dynamics generalizes beyond interval discriminative
tasks to abstract event-counting tasks that were considered foundational for
performing complex cognitive tasks. Our findings position SSMs as a compelling
framework that connects single-neuron dynamics to cognitive phenomena, offering
a unifying and computationally tractable theoretical ground for temporal
learning in the brain.

</details>


### [224] [Convergent transformations of visual representation in brains and models](https://arxiv.org/abs/2507.13941)
*Pablo Marcos-Manchón,Lluís Fuentemilla*

Main category: q-bio.NC

TL;DR: The paper explores whether visual perception is shaped by external world structure or brain architecture, revealing a convergent representational principle in humans and deep neural networks (DNNs).


<details>
  <summary>Details</summary>
Motivation: To understand if stimulus-driven convergence in visual perception follows a common trajectory in humans and DNNs, and to identify shared representational principles.

Method: A unified framework combining inter-subject similarity and alignment to model hierarchies, applied to fMRI datasets of visual scene perception.

Result: Discovery of a cortex-wide network with two pathways (medial-ventral for scene structure, lateral-dorsal for social/biological content), mirrored in vision DNNs but not language models.

Conclusion: The findings demonstrate a convergent computational solution for visual encoding in humans and artificial vision, driven by external world structure.

Abstract: A fundamental question in cognitive neuroscience is what shapes visual
perception: the external world's structure or the brain's internal
architecture. Although some perceptual variability can be traced to individual
differences, brain responses to naturalistic stimuli evoke similar activity
patterns across individuals, suggesting a convergent representational
principle. Here, we test if this stimulus-driven convergence follows a common
trajectory across people and deep neural networks (DNNs) during its
transformation from sensory to high-level internal representations. We
introduce a unified framework that traces representational flow by combining
inter-subject similarity with alignment to model hierarchies. Applying this
framework to three independent fMRI datasets of visual scene perception, we
reveal a cortex-wide network, conserved across individuals, organized into two
pathways: a medial-ventral stream for scene structure and a lateral-dorsal
stream tuned for social and biological content. This functional organization is
captured by the hierarchies of vision DNNs but not language models, reinforcing
the specificity of the visual-to-semantic transformation. These findings show a
convergent computational solution for visual encoding in both human and
artificial vision, driven by the structure of the external world.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [225] [ERR@HRI 2.0 Challenge: Multimodal Detection of Errors and Failures in Human-Robot Conversations](https://arxiv.org/abs/2507.13468)
*Shiye Cao,Maia Stiber,Amama Mahmood,Maria Teresa Parreira,Wendy Ju,Micol Spitale,Hatice Gunes,Chien-Ming Huang*

Main category: cs.RO

TL;DR: The ERR@HRI 2.0 Challenge provides a multimodal dataset to detect failures in LLM-powered conversational robots, aiming to improve human-robot interaction.


<details>
  <summary>Details</summary>
Motivation: LLM-powered robots often fail in conversations, disrupting tasks and trust. Detecting these failures is crucial.

Method: A dataset of 16 hours of human-robot interactions with multimodal features (facial, speech, head movement) is provided. Teams develop ML models to detect annotated robot errors.

Result: Submissions are evaluated on metrics like accuracy and false positive rate.

Conclusion: The challenge advances failure detection in human-robot interaction via social signal analysis.

Abstract: The integration of large language models (LLMs) into conversational robots
has made human-robot conversations more dynamic. Yet, LLM-powered
conversational robots remain prone to errors, e.g., misunderstanding user
intent, prematurely interrupting users, or failing to respond altogether.
Detecting and addressing these failures is critical for preventing
conversational breakdowns, avoiding task disruptions, and sustaining user
trust. To tackle this problem, the ERR@HRI 2.0 Challenge provides a multimodal
dataset of LLM-powered conversational robot failures during human-robot
conversations and encourages researchers to benchmark machine learning models
designed to detect robot failures. The dataset includes 16 hours of dyadic
human-robot interactions, incorporating facial, speech, and head movement
features. Each interaction is annotated with the presence or absence of robot
errors from the system perspective, and perceived user intention to correct for
a mismatch between robot behavior and user expectation. Participants are
invited to form teams and develop machine learning models that detect these
failures using multimodal data. Submissions will be evaluated using various
performance metrics, including detection accuracy and false positive rate. This
challenge represents another key step toward improving failure detection in
human-robot interaction through social signal analysis.

</details>


### [226] [EdgeVLA: Efficient Vision-Language-Action Models](https://arxiv.org/abs/2507.14049)
*Paweł Budzianowski,Wesley Maa,Matthew Freed,Jingxiang Mo,Winston Hsiao,Aaron Xie,Tomasz Młoduchowski,Viraj Tipnis,Benjamin Bolte*

Main category: cs.RO

TL;DR: Edge VLA (EVLA) enhances Vision-Language-Action (VLA) models' inference speed for real-time performance on edge devices, achieving 7x speedup and reduced computational demands.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of deploying large-scale VLMs on resource-constrained mobile manipulation systems.

Method: 1) Eliminates autoregressive requirement for position prediction. 2) Uses Small Language Models (SLMs) for efficiency.

Result: EVLA matches OpenVLA's training performance while significantly improving inference speed and memory efficiency.

Conclusion: EVLA offers a practical solution for real-time VLA deployment, with released codebase for further research.

Abstract: Vision-Language Models (VLMs) have emerged as a promising approach to address
the data scarcity challenge in robotics, enabling the development of
generalizable visuomotor control policies. While models like OpenVLA showcase
the potential of this paradigm, deploying large-scale VLMs on
resource-constrained mobile manipulation systems remains a significant hurdle.
This paper introduces Edge VLA (EVLA), a novel approach designed to
significantly enhance the inference speed of Vision-Language-Action (VLA)
models. EVLA maintains the representational power of these models while
enabling real-time performance on edge devices. We achieve this through two key
innovations: 1) Eliminating the autoregressive requirement for end-effector
position prediction, leading to a 7x speedup in inference, and 2) Leveraging
the efficiency of Small Language Models (SLMs), demonstrating comparable
training performance to larger models with significantly reduced computational
demands. Our early results demonstrate that EVLA achieves comparable training
characteristics to OpenVLA while offering substantial gains in inference speed
and memory efficiency. We release our model checkpoints and training
\href{https://github.com/kscalelabs/evla }{codebase} to foster further
research.

</details>


### [227] [Improved particle swarm optimization algorithm: multi-target trajectory optimization for swarm drones](https://arxiv.org/abs/2507.13647)
*Minze Li,Wei Zhao,Ran Chen,Mingqiang Wei*

Main category: cs.RO

TL;DR: PE-PSO enhances PSO for real-time UAV trajectory planning by adding persistent exploration and entropy-based parameter adjustment, outperforming traditional methods in dynamic environments.


<details>
  <summary>Details</summary>
Motivation: Traditional PSO struggles with premature convergence and latency in real-time UAV trajectory planning, necessitating a more adaptive and efficient solution.

Method: PE-PSO introduces persistent exploration and entropy-based parameter adjustment, uses B-spline curves for smooth paths, and integrates a multi-agent framework with GA-based task allocation.

Result: The framework outperforms conventional PSO in trajectory quality, energy efficiency, obstacle avoidance, and computation time.

Conclusion: PE-PSO is effective for real-time multi-UAV operations in complex environments, offering scalable and coordinated trajectory generation.

Abstract: Real-time trajectory planning for unmanned aerial vehicles (UAVs) in dynamic
environments remains a key challenge due to high computational demands and the
need for fast, adaptive responses. Traditional Particle Swarm Optimization
(PSO) methods, while effective for offline planning, often struggle with
premature convergence and latency in real-time scenarios. To overcome these
limitations, we propose PE-PSO, an enhanced PSO-based online trajectory
planner. The method introduces a persistent exploration mechanism to preserve
swarm diversity and an entropy-based parameter adjustment strategy to
dynamically adapt optimization behavior. UAV trajectories are modeled using
B-spline curves, which ensure path smoothness while reducing optimization
complexity. To extend this capability to UAV swarms, we develop a multi-agent
framework that combines genetic algorithm (GA)-based task allocation with
distributed PE-PSO, supporting scalable and coordinated trajectory generation.
The distributed architecture allows for parallel computation and decentralized
control, enabling effective cooperation among agents while maintaining
real-time performance. Comprehensive simulations demonstrate that the proposed
framework outperforms conventional PSO and other swarm-based planners across
several metrics, including trajectory quality, energy efficiency, obstacle
avoidance, and computation time. These results confirm the effectiveness and
applicability of PE-PSO in real-time multi-UAV operations under complex
environmental conditions.

</details>


### [228] [AGENTS-LLM: Augmentative GENeration of Challenging Traffic Scenarios with an Agentic LLM Framework](https://arxiv.org/abs/2507.13729)
*Yu Yao,Salil Bhatnagar,Markus Mazzola,Vasileios Belagiannis,Igor Gilitschenski,Luigi Palmieri,Simon Razniewski,Marcel Hallgarten*

Main category: cs.RO

TL;DR: The paper introduces an LLM-agent framework for augmenting real-world traffic scenarios using natural language, addressing the limitations of manual and data-driven methods.


<details>
  <summary>Details</summary>
Motivation: Testing autonomous driving planners in rare, critical scenarios is challenging due to the need for massive datasets and lack of fine-grained control in existing methods.

Method: A novel LLM-agent framework is proposed, leveraging natural language descriptions to augment scenarios with fine-grained control, even with smaller LLMs.

Result: Human expert evaluation shows the framework generates high-quality augmented scenarios comparable to manual efforts.

Conclusion: The LLM-agent framework offers a scalable, cost-effective solution for scenario augmentation, improving evaluation of autonomous driving systems.

Abstract: Rare, yet critical, scenarios pose a significant challenge in testing and
evaluating autonomous driving planners. Relying solely on real-world driving
scenes requires collecting massive datasets to capture these scenarios. While
automatic generation of traffic scenarios appears promising, data-driven models
require extensive training data and often lack fine-grained control over the
output. Moreover, generating novel scenarios from scratch can introduce a
distributional shift from the original training scenes which undermines the
validity of evaluations especially for learning-based planners. To sidestep
this, recent work proposes to generate challenging scenarios by augmenting
original scenarios from the test set. However, this involves the manual
augmentation of scenarios by domain experts. An approach that is unable to meet
the demands for scale in the evaluation of self-driving systems. Therefore,
this paper introduces a novel LLM-agent based framework for augmenting
real-world traffic scenarios using natural language descriptions, addressing
the limitations of existing methods. A key innovation is the use of an agentic
design, enabling fine-grained control over the output and maintaining high
performance even with smaller, cost-effective LLMs. Extensive human expert
evaluation demonstrates our framework's ability to accurately adhere to user
intent, generating high quality augmented scenarios comparable to those created
manually.

</details>


### [229] [Improving Low-Cost Teleoperation: Augmenting GELLO with Force](https://arxiv.org/abs/2507.13602)
*Shivakanth Sujit,Luca Nunziante,Dan Ogawa Lillrank,Rousslan Fernand Julien Dossa,Kai Arulkumaran*

Main category: cs.RO

TL;DR: Extended GELLO teleoperation system with force feedback and force-inclusive imitation learning, improving user preference and task success.


<details>
  <summary>Details</summary>
Motivation: To enhance the GELLO teleoperation system by incorporating force feedback and force data in imitation learning for better user experience and task performance.

Method: Implemented force feedback for user interaction and integrated force data into imitation learning. Validated with a Franka Panda arm, user studies, and task comparisons.

Result: Users preferred the force-feedback controller, and force inputs improved task success in most cases.

Conclusion: Force feedback and force-inclusive training enhance teleoperation and imitation learning performance.

Abstract: In this work we extend the low-cost GELLO teleoperation system, initially
designed for joint position control, with additional force information. Our
first extension is to implement force feedback, allowing users to feel
resistance when interacting with the environment. Our second extension is to
add force information into the data collection process and training of
imitation learning models. We validate our additions by implementing these on a
GELLO system with a Franka Panda arm as the follower robot, performing a user
study, and comparing the performance of policies trained with and without force
information on a range of simulated and real dexterous manipulation tasks.
Qualitatively, users with robotics experience preferred our controller, and the
addition of force inputs improved task success on the majority of tasks.

</details>


### [230] [Safety Certification in the Latent space using Control Barrier Functions and World Models](https://arxiv.org/abs/2507.13871)
*Mehul Anand,Shishir Kolathaya*

Main category: cs.RO

TL;DR: A semi-supervised framework for synthesizing safe visuomotor policies using latent space control barrier certificates (CBCs) and vision transformers.


<details>
  <summary>Details</summary>
Motivation: Overcoming the impracticality of extensive supervised labeling for safety-critical data in real-world settings.

Method: Jointly learns a neural barrier function and safe controller with limited labeled data, leveraging vision transformers for latent dynamics modeling.

Result: Enables scalable and data-efficient safe control by utilizing world models' predictive power in latent spaces.

Conclusion: The framework offers a practical solution for safe control synthesis from visual data without extensive supervision.

Abstract: Synthesising safe controllers from visual data typically requires extensive
supervised labelling of safety-critical data, which is often impractical in
real-world settings. Recent advances in world models enable reliable prediction
in latent spaces, opening new avenues for scalable and data-efficient safe
control. In this work, we introduce a semi-supervised framework that leverages
control barrier certificates (CBCs) learned in the latent space of a world
model to synthesise safe visuomotor policies. Our approach jointly learns a
neural barrier function and a safe controller using limited labelled data,
while exploiting the predictive power of modern vision transformers for latent
dynamics modelling.

</details>


### [231] [A segmented robot grasping perception neural network for edge AI](https://arxiv.org/abs/2507.13970)
*Casper Bröcheler,Thomas Vroom,Derrick Timmermans,Alan van den Akker,Guangzhi Tang,Charalampos S. Kouzinopoulos,Rico Möckel*

Main category: cs.RO

TL;DR: The paper presents an end-to-end framework for 6-DoF grasp pose detection on a low-power RISC-V SoC, optimized for real-time robotic grasping.


<details>
  <summary>Details</summary>
Motivation: Robotic grasping is complex and requires precise perception and control. Deep neural networks can enable low-latency, low-power inference for real-time grasping in resource-constrained environments.

Method: The work implements Heatmap-Guided Grasp Detection, optimized with hardware-aware techniques like input dimensionality reduction, model partitioning, and quantization.

Result: Experimental evaluation on GraspNet-1Billion validates fully on-chip inference, demonstrating the potential of low-power MCUs for autonomous manipulation.

Conclusion: The framework successfully enables real-time, autonomous robotic grasping on resource-constrained hardware.

Abstract: Robotic grasping, the ability of robots to reliably secure and manipulate
objects of varying shapes, sizes and orientations, is a complex task that
requires precise perception and control. Deep neural networks have shown
remarkable success in grasp synthesis by learning rich and abstract
representations of objects. When deployed at the edge, these models can enable
low-latency, low-power inference, making real-time grasping feasible in
resource-constrained environments. This work implements Heatmap-Guided Grasp
Detection, an end-to-end framework for the detection of 6-Dof grasp poses, on
the GAP9 RISC-V System-on-Chip. The model is optimised using hardware-aware
techniques, including input dimensionality reduction, model partitioning, and
quantisation. Experimental evaluation on the GraspNet-1Billion benchmark
validates the feasibility of fully on-chip inference, highlighting the
potential of low-power MCUs for real-time, autonomous manipulation.

</details>


### [232] [A multi-strategy improved snake optimizer for three-dimensional UAV path planning and engineering problems](https://arxiv.org/abs/2507.14043)
*Genliang Li,Yaxin Cui,Jinyu Su*

Main category: cs.RO

TL;DR: The paper introduces a Multi-strategy Improved Snake Optimizer (MISO) to address the slow convergence and local optima issues of the Snake Optimizer (SO). MISO incorporates adaptive strategies and demonstrates superior performance in optimization tasks and UAV path planning.


<details>
  <summary>Details</summary>
Motivation: The Snake Optimizer (SO) has limitations like slow convergence and susceptibility to local optima, prompting the development of MISO to enhance optimization performance.

Method: MISO introduces adaptive random disturbance, Levy flight, and position update strategies to improve SO. It is tested on CEC2017/2022 functions, UAV path planning, and engineering problems.

Result: MISO outperforms 11 popular algorithms in solution quality and stability, proving effective in optimization and practical applications like UAV path planning.

Conclusion: MISO is a robust and efficient optimizer with strong potential for real-world applications, surpassing existing algorithms in performance.

Abstract: Metaheuristic algorithms have gained widespread application across various
fields owing to their ability to generate diverse solutions. One such algorithm
is the Snake Optimizer (SO), a progressive optimization approach. However, SO
suffers from the issues of slow convergence speed and susceptibility to local
optima. In light of these shortcomings, we propose a novel Multi-strategy
Improved Snake Optimizer (MISO). Firstly, we propose a new adaptive random
disturbance strategy based on sine function to alleviate the risk of getting
trapped in a local optimum. Secondly, we introduce adaptive Levy flight
strategy based on scale factor and leader and endow the male snake leader with
flight capability, which makes it easier for the algorithm to leap out of the
local optimum and find the global optimum. More importantly, we put forward a
position update strategy combining elite leadership and Brownian motion,
effectively accelerating the convergence speed while ensuring precision.
Finally, to demonstrate the performance of MISO, we utilize 30 CEC2017 test
functions and the CEC2022 test suite, comparing it with 11 popular algorithms
across different dimensions to validate its effectiveness. Moreover, Unmanned
Aerial Vehicle (UAV) has been widely used in various fields due to its
advantages of low cost, high mobility and easy operation. However, the UAV path
planning problem is crucial for flight safety and efficiency, and there are
still challenges in establishing and optimizing the path model. Therefore, we
apply MISO to the UAV 3D path planning problem as well as 6 engineering design
problems to assess its feasibility in practical applications. The experimental
results demonstrate that MISO exceeds other competitive algorithms in terms of
solution quality and stability, establishing its strong potential for
application.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [233] [CogniQ-H: A Soft Hierarchical Reinforcement Learning Paradigm for Automated Data Preparation](https://arxiv.org/abs/2507.13710)
*Jing Chang,Chang Liu,Jinbin Huang,Rui Mao,Jianbin Qin*

Main category: cs.DB

TL;DR: CogniQ-H introduces a soft hierarchical reinforcement learning framework for automated data preparation, combining LLM guidance, LTR scores, and Q-learning, outperforming existing methods in quality and speed.


<details>
  <summary>Details</summary>
Motivation: Data preparation in ML is complex and inefficient with current RL methods, lacking hierarchical structure awareness.

Method: CogniQ-H uses a soft hierarchical RL approach with Bayesian action selection, integrating LLM strategic priors, LTR scores, and Q-learning.

Result: Achieves 13.9% better pipeline quality and 2.8x faster convergence than state-of-the-art RL methods.

Conclusion: CogniQ-H effectively balances strategic guidance and adaptive decision-making, advancing automated data preparation.

Abstract: Data preparation is a foundational yet notoriously challenging component of
the machine learning lifecycle, characterized by a vast combinatorial search
space of potential operator sequences. While reinforcement learning (RL) offers
a promising direction, existing approaches are inefficient as they fail to
capture the structured, hierarchical nature of the problem. We argue that
Hierarchical Reinforcement Learning (HRL), a paradigm that has been successful
in other domains, provides a conceptually ideal yet previously unexplored
framework for this task. However, a naive HRL implementation with a `hard
hierarchy' is prone to suboptimal, irreversible decisions. To address this, we
introduce CogniQ-H, the first framework to implement a soft hierarchical
paradigm for robust, end-to-end automated data preparation. CogniQ-H formulates
action selection as a Bayesian inference problem. A high-level strategic prior,
generated by a Large Language Model (LLM), guides exploration
probabilistically. This prior is synergistically combined with a fine-grained
operator quality score from a supervised Learning-to-Rank (LTR) model and a
long-term value estimate from the agent's own Q-function. This hybrid
architecture allows CogniQ-H to balance strategic guidance with adaptive,
evidence-based decision-making. Through extensive experiments on 18 diverse
datasets spanning multiple domains, we demonstrate that CogniQ-H achieves up to
13.9\% improvement in pipeline quality and 2.8$\times$ faster convergence
compared to state-of-the-art RL-based methods.

</details>


### [234] [LLaPipe: LLM-Guided Reinforcement Learning for Automated Data Preparation Pipeline Construction](https://arxiv.org/abs/2507.13712)
*Jing Chang,Chang Liu,Jinbin Huang,Rui Mao,Jianbin Qin*

Main category: cs.DB

TL;DR: LLaPipe integrates LLMs as policy advisors to improve exploration in automated data preparation, outperforming traditional RL methods in pipeline quality and convergence speed.


<details>
  <summary>Details</summary>
Motivation: Existing RL-based approaches for automated data preparation suffer from inefficient exploration in preprocessing pipelines.

Method: LLaPipe uses LLMs for intelligent policy advice, including an LLM Policy Advisor, Experience Distillation, and Adaptive Advisor Triggering.

Result: LLaPipe improves pipeline quality by 22.4% and converges 2.3x faster than state-of-the-art RL methods, with efficient LLM usage.

Conclusion: LLaPipe effectively addresses exploration bottlenecks in automated data preparation by leveraging LLMs, achieving superior performance and efficiency.

Abstract: Automated data preparation is crucial for democratizing machine learning, yet
existing reinforcement learning (RL) based approaches suffer from inefficient
exploration in the vast space of possible preprocessing pipelines. We present
LLaPipe, a novel framework that addresses this exploration bottleneck by
integrating Large Language Models (LLMs) as intelligent policy advisors. Unlike
traditional methods that rely solely on statistical features and blind
trial-and-error, LLaPipe leverages the semantic understanding capabilities of
LLMs to provide contextually relevant exploration guidance. Our framework
introduces three key innovations: (1) an LLM Policy Advisor that analyzes
dataset semantics and pipeline history to suggest promising preprocessing
operations, (2) an Experience Distillation mechanism that mines successful
patterns from past pipelines and transfers this knowledge to guide future
exploration, and (3) an Adaptive Advisor Triggering strategy
(Advisor\textsuperscript{+}) that dynamically determines when LLM intervention
is most beneficial, balancing exploration effectiveness with computational
cost. Through extensive experiments on 18 diverse datasets spanning multiple
domains, we demonstrate that LLaPipe achieves up to 22.4\% improvement in
pipeline quality and 2.3$\times$ faster convergence compared to
state-of-the-art RL-based methods, while maintaining computational efficiency
through selective LLM usage (averaging only 19.0\% of total exploration steps).

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [235] [Preprint: Did I Just Browse A Website Written by LLMs?](https://arxiv.org/abs/2507.13933)
*Sichang "Steven" He,Ramesh Govindan,Harsha V. Madhyastha*

Main category: cs.NI

TL;DR: The paper addresses the challenge of detecting unreliable and unethical LLM-dominant web content, proposing a scalable pipeline for classifying entire websites with high accuracy.


<details>
  <summary>Details</summary>
Motivation: The rise of LLM-dominant content, which is often unreliable and undisclosed, necessitates reliable detection methods to protect users and the web ecosystem.

Method: A pipeline classifies websites by aggregating outputs from an LLM text detector across multiple prose-like pages, trained on 120 ground truth sites.

Result: The detector achieved 100% accuracy on test datasets and identified a significant portion of LLM-dominant sites in real-world web data.

Conclusion: LLM-dominant sites are increasingly prevalent and rank highly in search results, highlighting the need for better detection and transparency.

Abstract: Increasingly, web content is automatically generated by large language models
(LLMs) with little human input. We call this "LLM-dominant" content. Since LLMs
plagiarize and hallucinate, LLM-dominant content can be unreliable and
unethical. Yet, websites rarely disclose such content, and human readers
struggle to distinguish it. Thus, we must develop reliable detectors for
LLM-dominant content. However, state-of-the-art LLM detectors are insufficient,
because they perform well mainly on clean, prose-like text, while web content
has complex markup and diverse genres.
  We propose a highly reliable, scalable pipeline that classifies entire
websites. Instead of naively classifying text extracted from each page, we
classify each site based on an LLM text detector's outputs of multiple
prose-like pages. We train and evaluate our detector by collecting 2 distinct
ground truth datasets totaling 120 sites, and obtain 100% accuracies testing
across them. In the wild, we detect a sizable portion of sites as LLM-dominant
among 10k sites in search engine results and 10k in Common Crawl archives. We
find LLM-dominant sites are growing in prevalence and rank highly in search
results, raising questions about their impact on end users and the overall Web
ecosystem.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [236] [Tight Bounds for Answering Adaptively Chosen Concentrated Queries](https://arxiv.org/abs/2507.13700)
*Emma Rapoport,Edith Cohen,Uri Stemmer*

Main category: cs.DS

TL;DR: The paper analyzes the limitations of the concentrated queries framework in adaptive data analysis, proving an inherent utility gap compared to independent settings, and presents a simplified algorithm matching these bounds.


<details>
  <summary>Details</summary>
Motivation: Addressing the challenge of adaptive data analysis under correlated samples, where existing frameworks like concentrated queries significantly limit query capacity compared to independent settings.

Method: Proving inherent limitations of the concentrated queries framework under natural algorithmic conditions and introducing a simplified version of the best-known algorithms.

Result: Demonstrates an unavoidable utility gap in the concentrated queries framework, supporting only O(n) queries for a sample of size n, compared to O(n²) in independent settings.

Conclusion: The work highlights the limitations of the current concentrated queries framework and provides a simplified algorithm that matches these bounds, suggesting further research is needed for improvements.

Abstract: Most work on adaptive data analysis assumes that samples in the dataset are
independent. When correlations are allowed, even the non-adaptive setting can
become intractable, unless some structural constraints are imposed. To address
this, Bassily and Freund [2016] introduced the elegant framework of
concentrated queries, which requires the analyst to restrict itself to queries
that are concentrated around their expected value. While this assumption makes
the problem trivial in the non-adaptive setting, in the adaptive setting it
remains quite challenging. In fact, all known algorithms in this framework
support significantly fewer queries than in the independent case: At most
$O(n)$ queries for a sample of size $n$, compared to $O(n^2)$ in the
independent setting.
  In this work, we prove that this utility gap is inherent under the current
formulation of the concentrated queries framework, assuming some natural
conditions on the algorithm. Additionally, we present a simplified version of
the best-known algorithms that match our impossibility result.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [237] [Scalable Attribute-Missing Graph Clustering via Neighborhood Differentiatio](https://arxiv.org/abs/2507.13368)
*Yaowen Hu,Wenxuan Tu,Yue Liu,Xinhang Wan,Junyi Yan,Taichun Zhou,Xinwang Liu*

Main category: cs.SI

TL;DR: CMV-ND is a novel deep graph clustering method for large-scale, attribute-missing graphs, using recursive neighborhood search and differential strategies to create complementary multi-view representations, improving clustering performance.


<details>
  <summary>Details</summary>
Motivation: Real-world attribute graphs (e.g., social networks) are large-scale and often have missing attributes, posing challenges for unsupervised clustering.

Method: CMV-ND preprocesses graph structural information into multiple complementary views: (1) recursive neighborhood search for completeness, (2) neighborhood differential strategy to eliminate redundancy, and (3) constructing K+1 views for multi-view clustering.

Result: Experiments on six datasets show CMV-ND significantly enhances clustering performance for various methods.

Conclusion: CMV-ND effectively addresses scalability and attribute-missing issues in deep graph clustering, demonstrating superior performance.

Abstract: Deep graph clustering (DGC), which aims to unsupervisedly separate the nodes
in an attribute graph into different clusters, has seen substantial potential
in various industrial scenarios like community detection and recommendation.
However, the real-world attribute graphs, e.g., social networks interactions,
are usually large-scale and attribute-missing. To solve these two problems, we
propose a novel DGC method termed \underline{\textbf{C}}omplementary
\underline{\textbf{M}}ulti-\underline{\textbf{V}}iew
\underline{\textbf{N}}eighborhood \underline{\textbf{D}}ifferentiation
(\textit{CMV-ND}), which preprocesses graph structural information into
multiple views in a complete but non-redundant manner. First, to ensure
completeness of the structural information, we propose a recursive neighborhood
search that recursively explores the local structure of the graph by completely
expanding node neighborhoods across different hop distances. Second, to
eliminate the redundancy between neighborhoods at different hops, we introduce
a neighborhood differential strategy that ensures no overlapping nodes between
the differential hop representations. Then, we construct $K+1$ complementary
views from the $K$ differential hop representations and the features of the
target node. Last, we apply existing multi-view clustering or DGC methods to
the views. Experimental results on six widely used graph datasets demonstrate
that CMV-ND significantly improves the performance of various methods.

</details>


### [238] [H-NeiFi: Non-Invasive and Consensus-Efficient Multi-Agent Opinion Guidance](https://arxiv.org/abs/2507.13370)
*Shijun Guo,Haoran Xu,Yaming Yang,Ziyu Guan,Wei Zhao,Xinyi Zhang,Yishan Song,Jiwei Chen*

Main category: cs.SI

TL;DR: H-NeiFi is a hierarchical, non-intrusive framework for guiding opinion evolution on social media, using a two-layer dynamic model and MARL to enhance consensus speed without undermining user autonomy.


<details>
  <summary>Details</summary>
Motivation: Existing methods for opinion guidance are intrusive, reduce user autonomy, and lack long-term effectiveness, often worsening divisions.

Method: H-NeiFi uses a two-layer dynamic model based on social roles (experts/non-experts) and a non-intrusive neighbor filtering method, optimized via MARL with a long-term reward function.

Result: H-NeiFi increases consensus speed by 22.0-30.7% and ensures global convergence even without experts.

Conclusion: H-NeiFi offers a natural, efficient, and autonomy-preserving approach for social network governance.

Abstract: The openness of social media enables the free exchange of opinions, but it
also presents challenges in guiding opinion evolution towards global consensus.
Existing methods often directly modify user views or enforce cross-group
connections. These intrusive interventions undermine user autonomy, provoke
psychological resistance, and reduce the efficiency of global consensus.
Additionally, due to the lack of a long-term perspective, promoting local
consensus often exacerbates divisions at the macro level. To address these
issues, we propose the hierarchical, non-intrusive opinion guidance framework,
H-NeiFi. It first establishes a two-layer dynamic model based on social roles,
considering the behavioral characteristics of both experts and non-experts.
Additionally, we introduce a non-intrusive neighbor filtering method that
adaptively controls user communication channels. Using multi-agent
reinforcement learning (MARL), we optimize information propagation paths
through a long-term reward function, avoiding direct interference with user
interactions. Experiments show that H-NeiFi increases consensus speed by 22.0%
to 30.7% and maintains global convergence even in the absence of experts. This
approach enables natural and efficient consensus guidance by protecting user
interaction autonomy, offering a new paradigm for social network governance.

</details>


### [239] [Leveraging the Spatial Hierarchy: Coarse-to-fine Trajectory Generation via Cascaded Hybrid Diffusion](https://arxiv.org/abs/2507.13366)
*Baoshen Guo,Zhiqing Hong,Junyi Li,Shenhao Wang,Jinhua Zhao*

Main category: cs.SI

TL;DR: Cardiff is a cascaded hybrid diffusion-based framework for synthesizing fine-grained, privacy-preserving urban mobility trajectories by decomposing the process into discrete road segment-level and continuous GPS-level generation.


<details>
  <summary>Details</summary>
Motivation: Urban mobility data is valuable but hard to obtain due to privacy and cost issues. Existing methods fail to handle structural complexity and high-dimensional distributions.

Method: Cardiff uses a two-level approach: (1) discrete road segment-level synthesis via diffusion transformer-based latent denoising, and (2) fine-grained GPS-level generation with noise augmentation for robustness.

Result: Cardiff outperforms state-of-the-art baselines on three real-world datasets, achieving high-fidelity trajectory generation with tunable privacy-utility balance.

Conclusion: Cardiff effectively addresses the challenges of fine-grained trajectory synthesis, offering a scalable and privacy-preserving solution for urban mobility data generation.

Abstract: Urban mobility data has significant connections with economic growth and
plays an essential role in various smart-city applications. However, due to
privacy concerns and substantial data collection costs, fine-grained human
mobility trajectories are difficult to become publicly available on a large
scale. A promising solution to address this issue is trajectory synthesizing.
However, existing works often ignore the inherent structural complexity of
trajectories, unable to handle complicated high-dimensional distributions and
generate realistic fine-grained trajectories. In this paper, we propose
Cardiff, a coarse-to-fine Cascaded hybrid diffusion-based trajectory
synthesizing framework for fine-grained and privacy-preserving mobility
generation. By leveraging the hierarchical nature of urban mobility, Cardiff
decomposes the generation process into two distinct levels, i.e., discrete road
segment-level and continuous fine-grained GPS-level: (i) In the segment-level,
to reduce computational costs and redundancy in raw trajectories, we first
encode the discrete road segments into low-dimensional latent embeddings and
design a diffusion transformer-based latent denoising network for segment-level
trajectory synthesis. (ii) Taking the first stage of generation as conditions,
we then design a fine-grained GPS-level conditional denoising network with a
noise augmentation mechanism to achieve robust and high-fidelity generation.
Additionally, the Cardiff framework not only progressively generates
high-fidelity trajectories through cascaded denoising but also flexibly enables
a tunable balance between privacy preservation and utility. Experimental
results on three large real-world trajectory datasets demonstrate that our
method outperforms state-of-the-art baselines in various metrics.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [240] [The AI Ethical Resonance Hypothesis: The Possibility of Discovering Moral Meta-Patterns in AI Systems](https://arxiv.org/abs/2507.11552)
*Tomasz Zgliczyński-Cuber*

Main category: cs.CY

TL;DR: The paper introduces the AI ethical resonance hypothesis, suggesting AI with designed cognitive structures could uncover hidden moral patterns, transcending biases and deepening ethical understanding.


<details>
  <summary>Details</summary>
Motivation: To explore how AI might identify universal ethical foundations beyond human perception, potentially enhancing our understanding of morality.

Method: Proposes a theoretical framework where AI systems (ethical resonators) process ethical contexts to discover meta-patterns.

Result: AI could reveal moral insights invisible to humans, transcending biases and deepening ethical reflection.

Conclusion: AI may not only augment ethical understanding but also paradoxically highlight the uniqueness of human ethical reflection.

Abstract: This paper presents a theoretical framework for the AI ethical resonance
hypothesis, which proposes that advanced AI systems with purposefully designed
cognitive structures ("ethical resonators") may emerge with the ability to
identify subtle moral patterns that are invisible to the human mind. The paper
explores the possibility that by processing and synthesizing large amounts of
ethical contexts, AI systems may discover moral meta-patterns that transcend
cultural, historical, and individual biases, potentially leading to a deeper
understanding of universal ethical foundations. The paper also examines a
paradoxical aspect of the hypothesis, in which AI systems could potentially
deepen our understanding of what we traditionally consider essentially human -
our capacity for ethical reflection.

</details>


### [241] [Food safety trends across Europe: insights from the 392-million-entry CompreHensive European Food Safety (CHEFS) database](https://arxiv.org/abs/2507.13802)
*Nehir Kizililsoley,Floor van Meer,Osman Mutlu,Wouter F Hoenderdaal,Rosan G. Hobé,Wenjuan Mu,Arjen Gerssen,H. J. van der Fels-Klerx,Ákos Jóźwiak,Ioannis Manikas,Ali Hürriyetoǧlu,Bas H. M. van der Velden*

Main category: cs.CY

TL;DR: The CHEFS database consolidates fragmented EFSA food safety data into a unified dataset, enabling AI-driven analysis of trends and hazards in European food safety from 2000 to 2024.


<details>
  <summary>Details</summary>
Motivation: The fragmented and large-scale nature of EFSA food safety data (392M results across 1,000 files) hinders accessibility and analysis, necessitating a centralized solution.

Method: Creation of the CHEFS database to unify EFSA data on pesticide residues, veterinary medicinal products, and chemical contaminants into a structured dataset.

Result: Demonstrated trends in monitoring activities, non-compliant products, and contaminant frequencies across Europe, showcasing CHEFS' utility for policy and research.

Conclusion: CHEFS serves as a strategic tool for centralized food safety analysis, supporting policy-making and regulatory improvements.

Abstract: In the European Union, official food safety monitoring data collected by
member states are submitted to the European Food Safety Authority (EFSA) and
published on Zenodo. This data includes 392 million analytical results derived
from over 15.2 million samples covering more than 4,000 different types of food
products, offering great opportunities for artificial intelligence to analyze
trends, predict hazards, and support early warning systems. However, the
current format with data distributed across approximately 1000 files totaling
several hundred gigabytes hinders accessibility and analysis. To address this,
we introduce the CompreHensive European Food Safety (CHEFS) database, which
consolidates EFSA monitoring data on pesticide residues, veterinary medicinal
product residues, and chemical contaminants into a unified and structured
dataset. We describe the creation and structure of the CHEFS database and
demonstrate its potential by analyzing trends in European food safety
monitoring data from 2000 to 2024. Our analyses explore changes in monitoring
activities, the most frequently tested products, which products were most often
non-compliant and which contaminants were most often found, and differences
across countries. These findings highlight the CHEFS database as both a
centralized data source and a strategic tool for guiding food safety policy,
research, and regulation.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [242] [Multiresolution local smoothness detection in non-uniformly sampled multivariate signals](https://arxiv.org/abs/2507.13480)
*Sara Avesani,Gianluca Giacchi,Michael Multerer*

Main category: math.NA

TL;DR: A linear-time algorithm for detecting local regularity in non-uniformly sampled multivariate signals using samplets, connecting coefficient decay to signal regularity.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of detecting local regularity in higher-dimensional and scattered data, where traditional wavelets fall short.

Method: Uses the fast samplet transform, a distributional wavelet transform for scattered data, to link samplet coefficient decay with pointwise regularity.

Result: Demonstrates robust performance for higher-dimensional and scattered data, with numerical studies validating the approach in 1D, 2D, and 3D signals.

Conclusion: Samplets offer an effective tool for regularity detection in complex data settings, outperforming traditional wavelets.

Abstract: Inspired by edge detection based on the decay behavior of wavelet
coefficients, we introduce a (near) linear-time algorithm for detecting the
local regularity in non-uniformly sampled multivariate signals. Our approach
quantifies regularity within the framework of microlocal spaces introduced by
Jaffard. The central tool in our analysis is the fast samplet transform, a
distributional wavelet transform tailored to scattered data. We establish a
connection between the decay of samplet coefficients and the pointwise
regularity of multivariate signals. As a by product, we derive decay estimates
for functions belonging to classical H\"older spaces and Sobolev-Slobodeckij
spaces. While traditional wavelets are effective for regularity detection in
low-dimensional structured data, samplets demonstrate robust performance even
for higher dimensional and scattered data. To illustrate our theoretical
findings, we present extensive numerical studies detecting local regularity of
one-, two- and three-dimensional signals, ranging from non-uniformly sampled
time series over image segmentation to edge detection in point clouds.

</details>


<div id='math-ph'></div>

# math-ph [[Back]](#toc)

### [243] [Asymptotic behavior of eigenvalues of large rank perturbations of large random matrices](https://arxiv.org/abs/2507.12182)
*Ievgenii Afanasiev,Leonid Berlyand,Mariia Kiyashko*

Main category: math-ph

TL;DR: The paper analyzes deformed Wigner random matrices, relevant to DNNs, focusing on asymptotic behavior for growing-rank matrices.


<details>
  <summary>Details</summary>
Motivation: To extend rigorous pruning techniques for DNNs by addressing the gap in mathematical analysis for growing-rank matrices.

Method: Develops asymptotic analysis for deformed Wigner matrices where the correlated component has growing rank.

Result: Provides theoretical insights into the spectrum of such matrices, supporting DNN pruning methods.

Conclusion: The work advances the understanding of random matrix theory in DNN applications, particularly for growing-rank cases.

Abstract: The paper is concerned with deformed Wigner random matrices. These matrices
are closely connected with Deep Neural Networks (DNNs): weight matrices of
trained DNNs could be represented in the form $R + S$, where $R$ is random and
$S$ is highly correlated. The spectrum of such matrices plays a key role in
rigorous underpinning of the novel pruning technique based on Random Matrix
Theory. Mathematics has been done only for finite-rank matrix $S$. However, in
practice rank may grow. In this paper we develop asymptotic analysis for the
case of growing rank.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [244] [Photonic Fabric Platform for AI Accelerators](https://arxiv.org/abs/2507.14000)
*Jing Ding,Trung Diep*

Main category: cs.PF

TL;DR: The paper introduces the Photonic Fabric and Photonic Fabric Appliance (PFA), a photonic-enabled system offering high bandwidth, low latency, and energy efficiency for AI training and inference. It overcomes memory-to-compute ratio limitations in current XPU designs and demonstrates significant performance improvements and energy savings in LLM tasks.


<details>
  <summary>Details</summary>
Motivation: Current XPU accelerator designs are constrained by fixed memory-to-compute ratios, limiting performance and scalability. The Photonic Fabric aims to address this by enabling flexible memory scaling and efficient parallelism strategies.

Method: The PFA integrates HBM3E memory, an on-module photonic switch, and DDR5 in a 2.5D electro-optical system-in-package. It uses CelestiSim, a lightweight simulator, to evaluate performance and energy savings on NVIDIA GPUs without altering core designs.

Result: Simulations show up to 3.66x throughput and 1.40x latency improvements for LLM inference at 405B parameters, and up to 7.04x throughput and 1.41x latency improvements at 1T parameters. Energy savings of 60-90% are achieved in data movement for LLM training.

Conclusion: The Photonic Fabric and PFA provide a scalable, energy-efficient solution for AI workloads, overcoming limitations of current XPU designs and demonstrating significant performance gains.

Abstract: This paper presents the Photonic FabricTM and the Photonic Fabric ApplianceTM
(PFA), a photonic-enabled switch and memory subsystem that delivers low
latency, high bandwidth, and low per-bit energy. By integrating high-bandwidth
HBM3E memory, an on-module photonic switch, and external DDR5 in a 2.5D
electro-optical system-in-package, the PFA offers up to 32 TB of shared memory
alongside 115 Tbps of all-to-all digital switching. The Photonic FabricTM
enables distributed AI training and inference to execute parallelism strategies
more efficiently. The Photonic Fabric removes the silicon beachfront constraint
that limits the fixed memory-to-compute ratio observed in virtually all current
XPU accelerator designs. Replacing a local HBM stack on an XPU with a chiplet
that connects to the Photonic Fabric increases its memory capacity and
correspondingly its memory bandwidth by offering a flexible path to scaling
well beyond the limitations of on-package HBM alone. We introduce CelestiSim, a
lightweight analytical simulator validated on NVIDIA H100 and H200 systems. It
is used to evaluate the performance of LLM reference and energy savings on PFA,
without any significant change to the GPU core design. With the PFA, the
simulation results show that up to 3.66x throughput and 1.40x latency
improvements in LLM inference at 405B parameters, up to 7.04x throughput and
1.41x latency improvements at 1T parameters, and 60-90% energy savings in data
movement for heavy collective operations in all LLM training scenarios. While
these results are shown for NVIDIA GPUs, they can be applied similarly to other
AI accelerator designs (XPUs) that share the same fundamental limitation of
fixed memory to compute.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [245] [Graph Neural Network Surrogates for Contacting Deformable Bodies with Necessary and Sufficient Contact Detection](https://arxiv.org/abs/2507.13459)
*Vijay K. Dubey,Collin E. Haese,Osman Gültekin,David Dalton,Manuel K. Rausch,Jan N. Fuhg*

Main category: cs.CE

TL;DR: A graph neural network architecture is introduced for surrogate modeling of nonlinear boundary value problems involving soft deformable body contact, improving generalization and handling varying geometries, though with high training costs.


<details>
  <summary>Details</summary>
Motivation: Existing surrogate models for contact problems are limited to rigid bodies or simple contact scenarios, lacking sufficient conditions for soft deformable bodies.

Method: The paper proposes a graph neural network with continuous collision detection and sufficient conditions for soft body contact, tested on benchmarks including soft tissue mechanics.

Result: The method shows better generalization with additional contact terms in the loss function and handles varying geometries, achieving up to a thousand-fold inference speedup.

Conclusion: The framework is effective but involves a trade-off with high computational costs during training, though it offers significant speedups at inference.

Abstract: Surrogate models for the rapid inference of nonlinear boundary value problems
in mechanics are helpful in a broad range of engineering applications. However,
effective surrogate modeling of applications involving the contact of
deformable bodies, especially in the context of varying geometries, is still an
open issue. In particular, existing methods are confined to rigid body contact
or, at best, contact between rigid and soft objects with well-defined contact
planes. Furthermore, they employ contact or collision detection filters that
serve as a rapid test but use only the necessary and not sufficient conditions
for detection. In this work, we present a graph neural network architecture
that utilizes continuous collision detection and, for the first time,
incorporates sufficient conditions designed for contact between soft deformable
bodies. We test its performance on two benchmarks, including a problem in soft
tissue mechanics of predicting the closed state of a bioprosthetic aortic
valve. We find a regularizing effect on adding additional contact terms to the
loss function, leading to better generalization of the network. These benefits
hold for simple contact at similar planes and element normal angles, and
complex contact at differing planes and element normal angles. We also
demonstrate that the framework can handle varying reference geometries.
However, such benefits come with high computational costs during training,
resulting in a trade-off that may not always be favorable. We quantify the
training cost and the resulting inference speedups on various hardware
architectures. Importantly, our graph neural network implementation results in
up to a thousand-fold speedup for our benchmark problems at inference.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [246] [Flatten Wisely: How Patch Order Shapes Mamba-Powered Vision for MRI Segmentation](https://arxiv.org/abs/2507.13384)
*Osama Hardan,Omar Elshenhabi,Tamer Khattab,Mohamed Mabrok*

Main category: eess.IV

TL;DR: The paper studies the impact of patch scan order in Vision Mamba models for MRI segmentation, introducing MS2D to explore scan paths efficiently. It finds scan order significantly affects performance, with contiguous paths outperforming disjointed ones.


<details>
  <summary>Details</summary>
Motivation: Vision Mamba models offer linear computational cost but overlook the critical design choice of patch scan order, especially in medical imaging with strong anatomical priors.

Method: Introduces MS2D, a parameter-free module for Mamba-based architectures, and benchmarks 21 scan strategies on three datasets (BraTS 2020, ISLES 2022, LGG).

Result: Scan order is statistically significant (χ²=43.9, p=0.0016), with performance varying by up to 27 Dice points. Contiguous paths (horizontal/vertical rasters) outperform disjointed scans.

Conclusion: Scan order is a powerful, cost-free hyperparameter. The paper provides evidence-based optimal paths to enhance Mamba models in medical imaging.

Abstract: Vision Mamba models promise transformer-level performance at linear
computational cost, but their reliance on serializing 2D images into 1D
sequences introduces a critical, yet overlooked, design choice: the patch scan
order. In medical imaging, where modalities like brain MRI contain strong
anatomical priors, this choice is non-trivial. This paper presents the first
systematic study of how scan order impacts MRI segmentation. We introduce
Multi-Scan 2D (MS2D), a parameter-free module for Mamba-based architectures
that facilitates exploring diverse scan paths without additional computational
cost. We conduct a large-scale benchmark of 21 scan strategies on three public
datasets (BraTS 2020, ISLES 2022, LGG), covering over 70,000 slices. Our
analysis shows conclusively that scan order is a statistically significant
factor (Friedman test: $\chi^{2}_{20}=43.9, p=0.0016$), with performance
varying by as much as 27 Dice points. Spatially contiguous paths -- simple
horizontal and vertical rasters -- consistently outperform disjointed diagonal
scans. We conclude that scan order is a powerful, cost-free hyperparameter, and
provide an evidence-based shortlist of optimal paths to maximize the
performance of Mamba models in medical imaging.

</details>


### [247] [BreastSegNet: Multi-label Segmentation of Breast MRI](https://arxiv.org/abs/2507.13604)
*Qihang Li,Jichen Yang,Yaqian Chen,Yuwen Chen,Hanxue Gu,Lars J. Grimm,Maciej A. Mazurowski*

Main category: eess.IV

TL;DR: BreastSegNet, a multi-label segmentation algorithm for breast MRI, covers nine anatomical labels and outperforms existing models, achieving high Dice scores.


<details>
  <summary>Details</summary>
Motivation: Existing breast MRI segmentation methods are limited in scope, focusing on few structures, reducing their utility for quantitative analysis.

Method: BreastSegNet is developed and benchmarked against nine models (e.g., U-Net, nnU-Net) using manually annotated MRI slices (1123 slices).

Result: nnU-Net ResEncM achieves the highest average Dice score (0.694), excelling in heart, liver, muscle, FGT, and bone segmentation.

Conclusion: BreastSegNet improves multi-label segmentation in breast MRI, with plans to release the dataset and model publicly.

Abstract: Breast MRI provides high-resolution imaging critical for breast cancer
screening and preoperative staging. However, existing segmentation methods for
breast MRI remain limited in scope, often focusing on only a few anatomical
structures, such as fibroglandular tissue or tumors, and do not cover the full
range of tissues seen in scans. This narrows their utility for quantitative
analysis. In this study, we present BreastSegNet, a multi-label segmentation
algorithm for breast MRI that covers nine anatomical labels: fibroglandular
tissue (FGT), vessel, muscle, bone, lesion, lymph node, heart, liver, and
implant. We manually annotated a large set of 1123 MRI slices capturing these
structures with detailed review and correction from an expert radiologist.
Additionally, we benchmark nine segmentation models, including U-Net, SwinUNet,
UNet++, SAM, MedSAM, and nnU-Net with multiple ResNet-based encoders. Among
them, nnU-Net ResEncM achieves the highest average Dice scores of 0.694 across
all labels. It performs especially well on heart, liver, muscle, FGT, and bone,
with Dice scores exceeding 0.73, and approaching 0.90 for heart and liver. All
model code and weights are publicly available, and we plan to release the data
at a later date.

</details>


### [248] [Domain-randomized deep learning for neuroimage analysis](https://arxiv.org/abs/2507.13458)
*Malte Hoffmann*

Main category: eess.IV

TL;DR: A tutorial on domain-randomization in deep learning for neuroimage analysis, using synthetic data to improve model generalization across diverse imaging modalities.


<details>
  <summary>Details</summary>
Motivation: Addressing the limited robustness and generalizability of deep learning models in neuroimage analysis due to narrow training datasets, especially in MRI.

Method: Training deep neural networks on synthetic images with randomized intensities and anatomical content, generated from segmentation maps.

Result: Improved model generalization across various imaging modalities without retraining or fine-tuning.

Conclusion: The synthesis-driven training paradigm enhances accessibility and generalizability of deep learning tools for domain experts, despite computational trade-offs.

Abstract: Deep learning has revolutionized neuroimage analysis by delivering
unprecedented speed and accuracy. However, the narrow scope of many training
datasets constrains model robustness and generalizability. This challenge is
particularly acute in magnetic resonance imaging (MRI), where image appearance
varies widely across pulse sequences and scanner hardware. A recent
domain-randomization strategy addresses the generalization problem by training
deep neural networks on synthetic images with randomized intensities and
anatomical content. By generating diverse data from anatomical segmentation
maps, the approach enables models to accurately process image types unseen
during training, without retraining or fine-tuning. It has demonstrated
effectiveness across modalities including MRI, computed tomography, positron
emission tomography, and optical coherence tomography, as well as beyond
neuroimaging in ultrasound, electron and fluorescence microscopy, and X-ray
microtomography. This tutorial paper reviews the principles, implementation,
and potential of the synthesis-driven training paradigm. It highlights key
benefits, such as improved generalization and resistance to overfitting, while
discussing trade-offs such as increased computational demands. Finally, the
article explores practical considerations for adopting the technique, aiming to
accelerate the development of generalizable tools that make deep learning more
accessible to domain experts without extensive computational resources or
machine learning knowledge.

</details>


### [249] [Enhanced DeepLab Based Nerve Segmentation with Optimized Tuning](https://arxiv.org/abs/2507.13394)
*Akhil John Thomas,Christiaan Boerkamp*

Main category: eess.IV

TL;DR: An optimized DeepLabV3-based pipeline with automated threshold fine-tuning improves nerve segmentation in ultrasound imaging, achieving high accuracy metrics.


<details>
  <summary>Details</summary>
Motivation: Precise nerve segmentation in medical imaging is critical for accurate identification of nerve structures.

Method: The study refines preprocessing steps and implements parameter optimization in a DeepLabV3-based segmentation pipeline.

Result: Achieved a Dice Score of 0.78, IoU of 0.70, and Pixel Accuracy of 0.95, outperforming baseline models.

Conclusion: Tailored parameter selection is essential for improving automated nerve detection in medical imaging.

Abstract: Nerve segmentation is crucial in medical imaging for precise identification
of nerve structures. This study presents an optimized DeepLabV3-based
segmentation pipeline that incorporates automated threshold fine-tuning to
improve segmentation accuracy. By refining preprocessing steps and implementing
parameter optimization, we achieved a Dice Score of 0.78, an IoU of 0.70, and a
Pixel Accuracy of 0.95 on ultrasound nerve imaging. The results demonstrate
significant improvements over baseline models and highlight the importance of
tailored parameter selection in automated nerve detection.

</details>


### [250] [Converting T1-weighted MRI from 3T to 7T quality using deep learning](https://arxiv.org/abs/2507.13782)
*Malo Gicquel,Ruoyi Zhao,Anika Wuestefeld,Nicola Spotorno,Olof Strandberg,Kalle Åström,Yu Xiao,Laura EM Wisse,Danielle van Westen,Rik Ossenkoppele,Niklas Mattsson-Carlgren,David Berron,Oskar Hansson,Gabrielle Flood,Jacob Vogel*

Main category: eess.IV

TL;DR: A deep learning model synthesizes 7T MRI from 3T MRI, improving image quality and segmentation without compromising downstream task performance.


<details>
  <summary>Details</summary>
Motivation: 7T MRI offers superior resolution and contrast but is less accessible than 3T MRI. This work aims to bridge the gap by generating synthetic 7T images from 3T scans.

Method: Two models were trained: a U-Net and a GAN U-Net, using paired 7T and 3T T1-weighted images from 172 participants. Performance was compared to state-of-the-art models.

Result: Synthetic 7T images matched real 7T in detail and surpassed them in visual quality. Automated segmentations from synthetic images were more accurate than those from 3T. Downstream cognitive status prediction remained comparable.

Conclusion: Synthetic 7T MRI from 3T scans is feasible, enhancing quality and segmentation without losing utility in clinical tasks. Future work will explore clinical applications and limitations.

Abstract: Ultra-high resolution 7 tesla (7T) magnetic resonance imaging (MRI) provides
detailed anatomical views, offering better signal-to-noise ratio, resolution
and tissue contrast than 3T MRI, though at the cost of accessibility. We
present an advanced deep learning model for synthesizing 7T brain MRI from 3T
brain MRI. Paired 7T and 3T T1-weighted images were acquired from 172
participants (124 cognitively unimpaired, 48 impaired) from the Swedish
BioFINDER-2 study. To synthesize 7T MRI from 3T images, we trained two models:
a specialized U-Net, and a U-Net integrated with a generative adversarial
network (GAN U-Net). Our models outperformed two additional state-of-the-art
3T-to-7T models in image-based evaluation metrics. Four blinded MRI
professionals judged our synthetic 7T images as comparable in detail to real 7T
images, and superior in subjective visual quality to 7T images, apparently due
to the reduction of artifacts. Importantly, automated segmentations of the
amygdalae of synthetic GAN U-Net 7T images were more similar to manually
segmented amygdalae (n=20), than automated segmentations from the 3T images
that were used to synthesize the 7T images. Finally, synthetic 7T images showed
similar performance to real 3T images in downstream prediction of cognitive
status using MRI derivatives (n=3,168). In all, we show that synthetic
T1-weighted brain images approaching 7T quality can be generated from 3T
images, which may improve image quality and segmentation, without compromising
performance in downstream tasks. Future directions, possible clinical use
cases, and limitations are discussed.

</details>


### [251] [Divide and Conquer: A Large-Scale Dataset and Model for Left-Right Breast MRI Segmentation](https://arxiv.org/abs/2507.13830)
*Maximilian Rokuss,Benjamin Hamm,Yannick Kirchhoff,Klaus Maier-Hein*

Main category: eess.IV

TL;DR: A publicly available breast MRI dataset with left-right segmentation labels and a deep-learning model for segmentation is introduced.


<details>
  <summary>Details</summary>
Motivation: Addressing the lack of publicly available breast MRI datasets with explicit segmentation labels for advancing women's health tools.

Method: Providing a dataset of over 13,000 annotated cases and a trained deep-learning model for segmentation.

Result: The dataset and model are publicly released, filling a critical gap in breast MRI analysis.

Conclusion: This work is a valuable resource for developing advanced tools in women's health.

Abstract: We introduce the first publicly available breast MRI dataset with explicit
left and right breast segmentation labels, encompassing more than 13,000
annotated cases. Alongside this dataset, we provide a robust deep-learning
model trained for left-right breast segmentation. This work addresses a
critical gap in breast MRI analysis and offers a valuable resource for the
development of advanced tools in women's health. The dataset and trained model
are publicly available at: www.github.com/MIC-DKFZ/BreastDivider

</details>


### [252] [Software architecture and manual for novel versatile CT image analysis toolbox -- AnatomyArchive](https://arxiv.org/abs/2507.13901)
*Lei Xu,Torkel B Brismar*

Main category: eess.IV

TL;DR: AnatomyArchive is a CT image analysis tool based on TotalSegmentator, offering automated volume selection, segmentation mask management, and radiomic feature extraction for precise body composition analysis.


<details>
  <summary>Details</summary>
Motivation: To provide a comprehensive, efficient, and automated solution for CT image analysis, aiding in body composition studies and machine learning model development.

Method: Leverages TotalSegmentator for full-body segmentation, integrates knowledge graph-based mask management, GPU-accelerated rendering, and voxel-based radiomic feature extraction.

Result: A robust toolchain for medical image analysis, including volume cropping, arm detection, and statistical analysis, with open-source availability for research.

Conclusion: AnatomyArchive enhances precision and efficiency in CT image analysis, supporting modern machine learning applications in medical imaging.

Abstract: We have developed a novel CT image analysis package named AnatomyArchive,
built on top of the recent full body segmentation model TotalSegmentator. It
provides automatic target volume selection and deselection capabilities
according to user-configured anatomies for volumetric upper- and lower-bounds.
It has a knowledge graph-based and time efficient tool for anatomy segmentation
mask management and medical image database maintenance. AnatomyArchive enables
automatic body volume cropping, as well as automatic arm-detection and
exclusion, for more precise body composition analysis in both 2D and 3D
formats. It provides robust voxel-based radiomic feature extraction, feature
visualization, and an integrated toolchain for statistical tests and analysis.
A python-based GPU-accelerated nearly photo-realistic segmentation-integrated
composite cinematic rendering is also included. We present here its software
architecture design, illustrate its workflow and working principle of
algorithms as well provide a few examples on how the software can be used to
assist development of modern machine learning models. Open-source codes will be
released at https://github.com/lxu-medai/AnatomyArchive for only research and
educational purposes.

</details>


### [253] [Blind Super Resolution with Reference Images and Implicit Degradation Representation](https://arxiv.org/abs/2507.13915)
*Huu-Phu Do,Po-Chih Hu,Hao-Chien Hsueh,Che-Kai Liu,Vu-Hoang Tran,Ching-Chun Huang*

Main category: eess.IV

TL;DR: The paper introduces a novel strategy for blind super-resolution (BSR) by using HR reference images to create scale-aware degradation kernels, improving SR performance.


<details>
  <summary>Details</summary>
Motivation: Existing BSR methods focus on estimating degradation kernels from LR inputs, but these kernels must also account for downscaling factors, which vary across super-resolution scales.

Method: The proposed method uses HR reference images to adaptively determine degradation processes, generating additional LR-HR pairs for training. This approach is compatible with existing blind SR models.

Result: The method outperforms previous approaches in both proficiently trained and zero-shot blind SR scenarios.

Conclusion: Incorporating blur kernels, scaling factors, and HR references enhances BSR effectiveness.

Abstract: Previous studies in blind super-resolution (BSR) have primarily concentrated
on estimating degradation kernels directly from low-resolution (LR) inputs to
enhance super-resolution. However, these degradation kernels, which model the
transition from a high-resolution (HR) image to its LR version, should account
for not only the degradation process but also the downscaling factor. Applying
the same degradation kernel across varying super-resolution scales may be
impractical. Our research acknowledges degradation kernels and scaling factors
as pivotal elements for the BSR task and introduces a novel strategy that
utilizes HR images as references to establish scale-aware degradation kernels.
By employing content-irrelevant HR reference images alongside the target LR
image, our model adaptively discerns the degradation process. It is then
applied to generate additional LR-HR pairs through down-sampling the HR
reference images, which are keys to improving the SR performance. Our
reference-based training procedure is applicable to proficiently trained blind
SR models and zero-shot blind SR methods, consistently outperforming previous
methods in both scenarios. This dual consideration of blur kernels and scaling
factors, coupled with the use of a reference image, contributes to the
effectiveness of our approach in blind super-resolution tasks.

</details>


### [254] [D2IP: Deep Dynamic Image Prior for 3D Time-sequence Pulmonary Impedance Imaging](https://arxiv.org/abs/2507.14046)
*Hao Fang,Hao Yu,Sihao Teng,Tao Zhang,Siyi Yuan,Huaiwu He,Zhe Liu,Yunjie Yang*

Main category: eess.IV

TL;DR: D2IP is a new framework for 3D time-sequence imaging, improving speed and accuracy over traditional methods like DIP by introducing UPWS, TPP, and a lightweight backbone.


<details>
  <summary>Details</summary>
Motivation: Address the high computational costs and inefficiencies of unsupervised methods like DIP in complex 3D or time-sequence tomographic imaging.

Method: Proposes D2IP with UPWS, TPP, and 3D-FastResUNet to accelerate convergence, ensure temporal coherence, and enhance efficiency.

Result: D2IP achieves faster (7.1x), more accurate reconstructions (24.8% higher MSSIM, 8.1% lower ERR) on simulated and clinical pulmonary datasets.

Conclusion: D2IP is a promising solution for clinical dynamic pulmonary imaging, offering superior performance and efficiency.

Abstract: Unsupervised learning methods, such as Deep Image Prior (DIP), have shown
great potential in tomographic imaging due to their training-data-free nature
and high generalization capability. However, their reliance on numerous network
parameter iterations results in high computational costs, limiting their
practical application, particularly in complex 3D or time-sequence tomographic
imaging tasks. To overcome these challenges, we propose Deep Dynamic Image
Prior (D2IP), a novel framework for 3D time-sequence imaging. D2IP introduces
three key strategies - Unsupervised Parameter Warm-Start (UPWS), Temporal
Parameter Propagation (TPP), and a customized lightweight reconstruction
backbone, 3D-FastResUNet - to accelerate convergence, enforce temporal
coherence, and improve computational efficiency. Experimental results on both
simulated and clinical pulmonary datasets demonstrate that D2IP enables fast
and accurate 3D time-sequence Electrical Impedance Tomography (tsEIT)
reconstruction. Compared to state-of-the-art baselines, D2IP delivers superior
image quality, with a 24.8% increase in average MSSIM and an 8.1% reduction in
ERR, alongside significantly reduced computational time (7.1x faster),
highlighting its promise for clinical dynamic pulmonary imaging.

</details>


### [255] [Leveraging Pathology Foundation Models for Panoptic Segmentation of Melanoma in H&E Images](https://arxiv.org/abs/2507.13974)
*Jiaqi Lv,Yijie Zhu,Carmen Guadalupe Colin Tenorio,Brinder Singh Chohan,Mark Eastwood,Shan E Ahmed Raza*

Main category: eess.IV

TL;DR: A deep learning model using Virchow2 and Efficient-UNet for automated segmentation of melanoma tissue in H&E images, achieving top performance in the PUMA Grand Challenge.


<details>
  <summary>Details</summary>
Motivation: Manual segmentation of melanoma tissue is labor-intensive and inconsistent, necessitating reliable automated methods.

Method: Combines Virchow2 (a pathology foundation model) with Efficient-UNet for feature extraction and segmentation of five tissue classes.

Result: Won first place in the PUMA Grand Challenge, showing robust performance and generalizability.

Conclusion: Incorporating pathology foundation models into segmentation networks can enhance computational pathology workflows.

Abstract: Melanoma is an aggressive form of skin cancer with rapid progression and high
metastatic potential. Accurate characterisation of tissue morphology in
melanoma is crucial for prognosis and treatment planning. However, manual
segmentation of tissue regions from haematoxylin and eosin (H&E) stained
whole-slide images (WSIs) is labour-intensive and prone to inter-observer
variability, this motivates the need for reliable automated tissue segmentation
methods. In this study, we propose a novel deep learning network for the
segmentation of five tissue classes in melanoma H&E images. Our approach
leverages Virchow2, a pathology foundation model trained on 3.1 million
histopathology images as a feature extractor. These features are fused with the
original RGB images and subsequently processed by an encoder-decoder
segmentation network (Efficient-UNet) to produce accurate segmentation maps.
The proposed model achieved first place in the tissue segmentation task of the
PUMA Grand Challenge, demonstrating robust performance and generalizability.
Our results show the potential and efficacy of incorporating pathology
foundation models into segmentation networks to accelerate computational
pathology workflows.

</details>


### [256] [OrthoInsight: Rib Fracture Diagnosis and Report Generation Based on Multi-Modal Large Models](https://arxiv.org/abs/2507.13993)
*Ningyong Wu,Jinzhi Wang,Wenhong Zhao,Chenzhan Yu,Zhigang Xiu,Duwei Dai*

Main category: eess.IV

TL;DR: OrthoInsight, a multi-modal deep learning framework, combines YOLOv9 for fracture detection, a medical knowledge graph, and LLaVA for report generation, outperforming models like GPT-4 in rib fracture diagnosis.


<details>
  <summary>Details</summary>
Motivation: The need for automated diagnostic tools for musculoskeletal injuries like rib fractures due to the growing volume of medical imaging data and the limitations of manual interpretation.

Method: Integrates YOLOv9 for fracture detection, a medical knowledge graph for clinical context, and a fine-tuned LLaVA model for report generation, combining visual and textual data.

Result: Achieves high performance (average score 4.28) in Diagnostic Accuracy, Content Completeness, Logical Coherence, and Clinical Guidance Value on 28,675 annotated CT images.

Conclusion: Demonstrates the potential of multi-modal learning in medical image analysis, offering effective support for radiologists.

Abstract: The growing volume of medical imaging data has increased the need for
automated diagnostic tools, especially for musculoskeletal injuries like rib
fractures, commonly detected via CT scans. Manual interpretation is
time-consuming and error-prone. We propose OrthoInsight, a multi-modal deep
learning framework for rib fracture diagnosis and report generation. It
integrates a YOLOv9 model for fracture detection, a medical knowledge graph for
retrieving clinical context, and a fine-tuned LLaVA language model for
generating diagnostic reports. OrthoInsight combines visual features from CT
images with expert textual data to deliver clinically useful outputs. Evaluated
on 28,675 annotated CT images and expert reports, it achieves high performance
across Diagnostic Accuracy, Content Completeness, Logical Coherence, and
Clinical Guidance Value, with an average score of 4.28, outperforming models
like GPT-4 and Claude-3. This study demonstrates the potential of multi-modal
learning in transforming medical image analysis and providing effective support
for radiologists.

</details>


### [257] [UGPL: Uncertainty-Guided Progressive Learning for Evidence-Based Classification in Computed Tomography](https://arxiv.org/abs/2507.14102)
*Shravan Venkatraman,Pavan Kumar S,Rakesh Raj Madavan,Chandrakala S*

Main category: eess.IV

TL;DR: UGPL is an uncertainty-guided progressive learning framework for CT image classification, outperforming state-of-the-art methods by focusing on ambiguous regions and refining analysis globally to locally.


<details>
  <summary>Details</summary>
Motivation: Existing CT image classification methods struggle with subtle, diverse pathological features due to uniform processing, lacking localized analysis.

Method: UGPL uses evidential deep learning to quantify uncertainty, guiding patch extraction via non-maximum suppression for spatial diversity, and employs adaptive fusion for global-to-local refinement.

Result: UGPL improves accuracy by 3.29%, 2.46%, and 8.08% for kidney abnormality, lung cancer, and COVID-19 detection, respectively.

Conclusion: UGPL's uncertainty-guided progressive learning significantly enhances CT image classification, with the full pipeline proving most effective.

Abstract: Accurate classification of computed tomography (CT) images is essential for
diagnosis and treatment planning, but existing methods often struggle with the
subtle and spatially diverse nature of pathological features. Current
approaches typically process images uniformly, limiting their ability to detect
localized abnormalities that require focused analysis. We introduce UGPL, an
uncertainty-guided progressive learning framework that performs a
global-to-local analysis by first identifying regions of diagnostic ambiguity
and then conducting detailed examination of these critical areas. Our approach
employs evidential deep learning to quantify predictive uncertainty, guiding
the extraction of informative patches through a non-maximum suppression
mechanism that maintains spatial diversity. This progressive refinement
strategy, combined with an adaptive fusion mechanism, enables UGPL to integrate
both contextual information and fine-grained details. Experiments across three
CT datasets demonstrate that UGPL consistently outperforms state-of-the-art
methods, achieving improvements of 3.29%, 2.46%, and 8.08% in accuracy for
kidney abnormality, lung cancer, and COVID-19 detection, respectively. Our
analysis shows that the uncertainty-guided component provides substantial
benefits, with performance dramatically increasing when the full progressive
learning pipeline is implemented. Our code is available at:
https://github.com/shravan-18/UGPL

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [258] [SEER: Semantic Enhancement and Emotional Reasoning Network for Multimodal Fake News Detection](https://arxiv.org/abs/2507.13415)
*Peican Zhu,Yubo Jing,Le Cheng,Bin Chen,Xiaodong Cui,Lianwei Wu,Keke Tang*

Main category: cs.MM

TL;DR: The paper introduces SEER, a network for multimodal fake news detection, leveraging semantic enhancement from large models and emotional reasoning to outperform existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing methods ignore semantic enhancement from large multimodal models and emotional features in fake news, which often contain more negative emotions than real news.

Method: Proposes SEER Network: uses summarized captions for image understanding, large models for semantic enhancement, and an expert emotional reasoning module to optimize emotional features.

Result: SEER outperforms state-of-the-art baselines on two real-world datasets.

Conclusion: SEER effectively combines semantic enhancement and emotional reasoning for superior fake news detection.

Abstract: Previous studies on multimodal fake news detection mainly focus on the
alignment and integration of cross-modal features, as well as the application
of text-image consistency. However, they overlook the semantic enhancement
effects of large multimodal models and pay little attention to the emotional
features of news. In addition, people find that fake news is more inclined to
contain negative emotions than real ones. Therefore, we propose a novel
Semantic Enhancement and Emotional Reasoning (SEER) Network for multimodal fake
news detection. We generate summarized captions for image semantic
understanding and utilize the products of large multimodal models for semantic
enhancement. Inspired by the perceived relationship between news authenticity
and emotional tendencies, we propose an expert emotional reasoning module that
simulates real-life scenarios to optimize emotional features and infer the
authenticity of news. Extensive experiments on two real-world datasets
demonstrate the superiority of our SEER over state-of-the-art baselines.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [259] [Differential Privacy in Kernelized Contextual Bandits via Random Projections](https://arxiv.org/abs/2507.13639)
*Nikola Pavlovic,Sudeep Salgia,Qing Zhao*

Main category: stat.ML

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We consider the problem of contextual kernel bandits with stochastic
contexts, where the underlying reward function belongs to a known Reproducing
Kernel Hilbert Space. We study this problem under an additional constraint of
Differential Privacy, where the agent needs to ensure that the sequence of
query points is differentially private with respect to both the sequence of
contexts and rewards. We propose a novel algorithm that achieves the
state-of-the-art cumulative regret of
$\widetilde{\mathcal{O}}(\sqrt{\gamma_TT}+\frac{\gamma_T}{\varepsilon_{\mathrm{DP}}})$
and
$\widetilde{\mathcal{O}}(\sqrt{\gamma_TT}+\frac{\gamma_T\sqrt{T}}{\varepsilon_{\mathrm{DP}}})$
over a time horizon of $T$ in the joint and local models of differential
privacy, respectively, where $\gamma_T$ is the effective dimension of the
kernel and $\varepsilon_{\mathrm{DP}} > 0$ is the privacy parameter. The key
ingredient of the proposed algorithm is a novel private kernel-ridge regression
estimator which is based on a combination of private covariance estimation and
private random projections. It offers a significantly reduced sensitivity
compared to its classical counterpart while maintaining a high prediction
accuracy, allowing our algorithm to achieve the state-of-the-art performance
guarantees.

</details>


### [260] [Conformal Data Contamination Tests for Trading or Sharing of Data](https://arxiv.org/abs/2507.13835)
*Martin V. Vejling,Shashi Raj Pandey,Christophe A. N. Biscio,Petar Popovski*

Main category: stat.ML

TL;DR: A framework for data-sharing with quality guarantees using conformal outlier detection to identify valuable external data for model personalization.


<details>
  <summary>Details</summary>
Motivation: Limited local data availability necessitates external data sharing, but buyers need assurance of data quality to avoid contamination or irrelevance.

Method: Introduces distribution-free, contamination-aware data-sharing with novel two-sample testing procedures for conformal outlier detection.

Result: The conformal data contamination tests provide valid quality guarantees under arbitrary contamination levels and enable false discovery rate control.

Conclusion: The framework offers a robust, generic method for aggregating data with statistically rigorous quality assurances.

Abstract: The amount of quality data in many machine learning tasks is limited to what
is available locally to data owners. The set of quality data can be expanded
through trading or sharing with external data agents. However, data buyers need
quality guarantees before purchasing, as external data may be contaminated or
irrelevant to their specific learning task. Previous works primarily rely on
distributional assumptions about data from different agents, relegating quality
checks to post-hoc steps involving costly data valuation procedures. We propose
a distribution-free, contamination-aware data-sharing framework that identifies
external data agents whose data is most valuable for model personalization. To
achieve this, we introduce novel two-sample testing procedures, grounded in
rigorous theoretical foundations for conformal outlier detection, to determine
whether an agent's data exceeds a contamination threshold. The proposed tests,
termed conformal data contamination tests, remain valid under arbitrary
contamination levels while enabling false discovery rate control via the
Benjamini-Hochberg procedure. Empirical evaluations across diverse
collaborative learning scenarios demonstrate the robustness and effectiveness
of our approach. Overall, the conformal data contamination test distinguishes
itself as a generic procedure for aggregating data with statistically rigorous
quality guarantees.

</details>


### [261] [A Survey of Dimension Estimation Methods](https://arxiv.org/abs/2507.13887)
*James A. D. Binnie,Paweł Dłotko,John Harvey,Jakub Malinowski,Ka Man Yim*

Main category: stat.ML

TL;DR: The paper surveys dimension estimation methods, categorizing them by geometric information used, and evaluates their performance on robustness, accuracy, and generalizability.


<details>
  <summary>Details</summary>
Motivation: Understanding the intrinsic dimension of high-dimensional datasets is crucial, but existing estimators lack reliable guidance for practical use.

Method: The survey reviews and categorizes dimension estimation methods into tangential, parametric, and topological/metric invariants, then evaluates their performance.

Result: Many estimators struggle with generalization due to overfitting, and performance varies with curvature, noise, and hyperparameters.

Conclusion: The study highlights the need for more robust and generalizable dimension estimation methods.

Abstract: It is a standard assumption that datasets in high dimension have an internal
structure which means that they in fact lie on, or near, subsets of a lower
dimension. In many instances it is important to understand the real dimension
of the data, hence the complexity of the dataset at hand. A great variety of
dimension estimators have been developed to find the intrinsic dimension of the
data but there is little guidance on how to reliably use these estimators.
  This survey reviews a wide range of dimension estimation methods,
categorising them by the geometric information they exploit: tangential
estimators which detect a local affine structure; parametric estimators which
rely on dimension-dependent probability distributions; and estimators which use
topological or metric invariants.
  The paper evaluates the performance of these methods, as well as
investigating varying responses to curvature and noise. Key issues addressed
include robustness to hyperparameter selection, sample size requirements,
accuracy in high dimensions, precision, and performance on non-linear
geometries. In identifying the best hyperparameters for benchmark datasets,
overfitting is frequent, indicating that many estimators may not generalise
well beyond the datasets on which they have been tested.

</details>


### [262] [Conformalized Regression for Continuous Bounded Outcomes](https://arxiv.org/abs/2507.14023)
*Zhanli Wu,Fabrizio Leisen,F. Javier Rubio*

Main category: stat.ML

TL;DR: The paper introduces conformal prediction intervals for bounded outcomes using transformation models and beta regression, addressing heteroscedasticity and model misspecification.


<details>
  <summary>Details</summary>
Motivation: Existing methods focus on point prediction or asymptotic interval prediction, lacking robust finite-sample coverage for bounded outcomes.

Method: Develops conformal prediction intervals with tailored non-conformity measures based on residuals, leveraging transformation models and beta regression.

Result: Theoretical and empirical results show valid finite-sample coverage, even under model misspecification.

Conclusion: The proposed methods outperform bootstrap-based alternatives, offering practical utility for real-world applications.

Abstract: Regression problems with bounded continuous outcomes frequently arise in
real-world statistical and machine learning applications, such as the analysis
of rates and proportions. A central challenge in this setting is predicting a
response associated with a new covariate value. Most of the existing
statistical and machine learning literature has focused either on point
prediction of bounded outcomes or on interval prediction based on asymptotic
approximations. We develop conformal prediction intervals for bounded outcomes
based on transformation models and beta regression. We introduce tailored
non-conformity measures based on residuals that are aligned with the underlying
models, and account for the inherent heteroscedasticity in regression settings
with bounded outcomes. We present a theoretical result on asymptotic marginal
and conditional validity in the context of full conformal prediction, which
remains valid under model misspecification. For split conformal prediction, we
provide an empirical coverage analysis based on a comprehensive simulation
study. The simulation study demonstrates that both methods provide valid
finite-sample predictive coverage, including settings with model
misspecification. Finally, we demonstrate the practical performance of the
proposed conformal prediction intervals on real data and compare them with
bootstrap-based alternatives.

</details>


### [263] [Step-DAD: Semi-Amortized Policy-Based Bayesian Experimental Design](https://arxiv.org/abs/2507.14057)
*Marcel Hedman,Desi R. Ivanova,Cong Guan,Tom Rainforth*

Main category: stat.ML

TL;DR: Step-DAD is a semi-amortized, policy-based Bayesian experimental design method that updates its policy during experiments, outperforming current state-of-the-art methods.


<details>
  <summary>Details</summary>
Motivation: To improve flexibility and robustness in Bayesian experimental design by adapting the design policy during experiments.

Method: Step-DAD combines upfront policy training with periodic updates during experiments, refining the policy for specific instances.

Result: Step-DAD shows superior decision-making and robustness compared to existing methods.

Conclusion: Step-DAD enhances Bayesian experimental design by dynamically adapting policies, offering better performance and robustness.

Abstract: We develop a semi-amortized, policy-based, approach to Bayesian experimental
design (BED) called Stepwise Deep Adaptive Design (Step-DAD). Like existing,
fully amortized, policy-based BED approaches, Step-DAD trains a design policy
upfront before the experiment. However, rather than keeping this policy fixed,
Step-DAD periodically updates it as data is gathered, refining it to the
particular experimental instance. This test-time adaptation improves both the
flexibility and the robustness of the design strategy compared with existing
approaches. Empirically, Step-DAD consistently demonstrates superior
decision-making and robustness compared with current state-of-the-art BED
methods.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [264] [AI-Assisted Fixes to Code Review Comments at Scale](https://arxiv.org/abs/2507.13499)
*Chandra Maddila,Negar Ghorbani,James Saindon,Parth Thakkar,Vijayaraghavan Murali,Rui Abreu,Jingyue Shen,Brian Zhou,Nachiappan Nagappan,Peter C. Rigby*

Main category: cs.SE

TL;DR: MetaMateCR, an AI tool for code review fixes at Meta, outperforms GPT-4o with a 68% exact match rate and improves efficiency by 9.2pp in production, while ensuring safety through controlled trials.


<details>
  <summary>Details</summary>
Motivation: To address the high volume of code review comments at Meta and improve efficiency by providing AI-assisted fixes.

Method: Fine-tuned Llama models on 64k review comment-patch pairs, conducted offline comparisons, safety trials, and full production experiments.

Result: LargeLSFT model achieved 68% exact match rate (9pp better than GPT-4o) and a 19.7% ActionableToApplied rate in production. Safety trials revealed UX adjustments were needed to avoid review delays.

Conclusion: MetaMateCR successfully scales AI-assisted code review fixes, emphasizing the importance of safety trials and UX design in AI deployment.

Abstract: Aim. There are 10s of thousands of code review comments each week at Meta. We
developed Metamate for Code Review (MetaMateCR) that provides AI-assisted fixes
for reviewer comments in production at scale.
  Method. We developed an internal benchmark of 64k <review comment, patch>
data points to fine-tune Llama models. Once our models achieve reasonable
offline results, we roll them into production. To ensure that our AI-assisted
fixes do not negatively impact the time it takes to do code reviews, we conduct
randomized controlled safety trials as well as full production experiments.
  Offline Results. As a baseline, we compare GPT-4o to our small and large
Llama models. In offline results, our LargeLSFT model creates an exact match
patch 68% of the time outperforming GPT-4o by 9 percentage points (pp). The
internal models also use more modern Hack functions when compared to the PHP
functions suggested by GPT-4o.
  Safety Trial. When we roll MetaMateCR into production in a safety trial that
compares no AI patches with AI patch suggestions, we see a large regression
with reviewers taking over 5% longer to conduct reviews. After investigation,
we modify the UX to only show authors the AI patches, and see no regressions in
the time for reviews.
  Production. When we roll LargeLSFT into production, we see an
ActionableToApplied rate of 19.7%, which is a 9.2pp improvement over GPT-4o.
Our results illustrate the importance of safety trials in ensuring that AI does
not inadvertently slow down engineers, and a successful review comment to AI
patch product running at scale.

</details>
